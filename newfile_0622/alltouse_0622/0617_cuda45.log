nohup: ignoring input
multi 16shot downstream for 64shot upstream
Task: superglue-cb, Checkpoint: models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt, Identifier: T5-large-multitask-cls2cls-5e-1-4-10-up64shot
Output directory () already exists and is not empty.
06/17/2022 16:48:58 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-large-multitask-cls2cls-5e-1-4-10-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/17/2022 16:48:58 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb
06/17/2022 16:48:58 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-large-multitask-cls2cls-5e-1-4-10-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/17/2022 16:48:58 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb
06/17/2022 16:48:59 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/17/2022 16:48:59 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/17/2022 16:48:59 - INFO - __main__ - args.device: cuda:0
06/17/2022 16:48:59 - INFO - __main__ - args.device: cuda:1
06/17/2022 16:48:59 - INFO - __main__ - Using 2 gpus
06/17/2022 16:48:59 - INFO - __main__ - Using 2 gpus
06/17/2022 16:48:59 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
06/17/2022 16:48:59 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
06/17/2022 16:49:04 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.5, bsz=8 ...
06/17/2022 16:49:05 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 16:49:05 - INFO - __main__ - Printing 3 examples
06/17/2022 16:49:05 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 16:49:05 - INFO - __main__ - ['contradiction']
06/17/2022 16:49:05 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/17/2022 16:49:05 - INFO - __main__ - ['contradiction']
06/17/2022 16:49:05 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/17/2022 16:49:05 - INFO - __main__ - ['contradiction']
06/17/2022 16:49:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 16:49:05 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 16:49:05 - INFO - __main__ - Printing 3 examples
06/17/2022 16:49:05 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 16:49:05 - INFO - __main__ - ['contradiction']
06/17/2022 16:49:05 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/17/2022 16:49:05 - INFO - __main__ - ['contradiction']
06/17/2022 16:49:05 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/17/2022 16:49:05 - INFO - __main__ - ['contradiction']
06/17/2022 16:49:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 16:49:05 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:49:05 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:49:05 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 16:49:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 16:49:05 - INFO - __main__ - Printing 3 examples
06/17/2022 16:49:05 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
06/17/2022 16:49:05 - INFO - __main__ - ['contradiction']
06/17/2022 16:49:05 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 16:49:05 - INFO - __main__ - ['contradiction']
06/17/2022 16:49:05 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
06/17/2022 16:49:05 - INFO - __main__ - ['contradiction']
06/17/2022 16:49:05 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:49:05 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 16:49:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 16:49:05 - INFO - __main__ - Printing 3 examples
06/17/2022 16:49:05 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
06/17/2022 16:49:05 - INFO - __main__ - ['contradiction']
06/17/2022 16:49:05 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 16:49:05 - INFO - __main__ - ['contradiction']
06/17/2022 16:49:05 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
06/17/2022 16:49:05 - INFO - __main__ - ['contradiction']
06/17/2022 16:49:05 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:49:05 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:49:05 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:49:05 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 16:49:05 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 16:49:22 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 16:49:25 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 16:49:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 16:49:26 - INFO - __main__ - Starting training!
06/17/2022 16:49:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 16:49:26 - INFO - __main__ - Starting training!
06/17/2022 16:49:31 - INFO - __main__ - Step 10 Global step 10 Train loss 1.30 on epoch=3
06/17/2022 16:49:33 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=6
06/17/2022 16:49:36 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=9
06/17/2022 16:49:39 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=13
06/17/2022 16:49:42 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=16
06/17/2022 16:49:43 - INFO - __main__ - Global step 50 Train loss 0.66 ACC 0.53125 on epoch=16
06/17/2022 16:49:43 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.53125 on epoch=16, global_step=50
06/17/2022 16:49:46 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=19
06/17/2022 16:49:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.41 on epoch=23
06/17/2022 16:49:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=26
06/17/2022 16:49:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=29
06/17/2022 16:49:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=33
06/17/2022 16:49:58 - INFO - __main__ - Global step 100 Train loss 0.43 ACC 0.5 on epoch=33
06/17/2022 16:50:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=36
06/17/2022 16:50:04 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=39
06/17/2022 16:50:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.39 on epoch=43
06/17/2022 16:50:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=46
06/17/2022 16:50:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=49
06/17/2022 16:50:13 - INFO - __main__ - Global step 150 Train loss 0.41 ACC 0.5 on epoch=49
06/17/2022 16:50:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=53
06/17/2022 16:50:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=56
06/17/2022 16:50:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=59
06/17/2022 16:50:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=63
06/17/2022 16:50:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.33 on epoch=66
06/17/2022 16:50:28 - INFO - __main__ - Global step 200 Train loss 0.38 ACC 0.5 on epoch=66
06/17/2022 16:50:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=69
06/17/2022 16:50:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.33 on epoch=73
06/17/2022 16:50:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.34 on epoch=76
06/17/2022 16:50:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=79
06/17/2022 16:50:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=83
06/17/2022 16:50:43 - INFO - __main__ - Global step 250 Train loss 0.31 ACC 0.34375 on epoch=83
06/17/2022 16:50:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=86
06/17/2022 16:50:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=89
06/17/2022 16:50:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=93
06/17/2022 16:50:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.33 on epoch=96
06/17/2022 16:50:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=99
06/17/2022 16:50:58 - INFO - __main__ - Global step 300 Train loss 0.27 ACC 0.40625 on epoch=99
06/17/2022 16:51:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=103
06/17/2022 16:51:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.22 on epoch=106
06/17/2022 16:51:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.19 on epoch=109
06/17/2022 16:51:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.16 on epoch=113
06/17/2022 16:51:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.19 on epoch=116
06/17/2022 16:51:13 - INFO - __main__ - Global step 350 Train loss 0.20 ACC 0.4375 on epoch=116
06/17/2022 16:51:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.17 on epoch=119
06/17/2022 16:51:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.17 on epoch=123
06/17/2022 16:51:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=126
06/17/2022 16:51:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.16 on epoch=129
06/17/2022 16:51:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.13 on epoch=133
06/17/2022 16:51:28 - INFO - __main__ - Global step 400 Train loss 0.17 ACC 0.4375 on epoch=133
06/17/2022 16:51:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.14 on epoch=136
06/17/2022 16:51:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.12 on epoch=139
06/17/2022 16:51:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.10 on epoch=143
06/17/2022 16:51:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.14 on epoch=146
06/17/2022 16:51:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=149
06/17/2022 16:51:43 - INFO - __main__ - Global step 450 Train loss 0.13 ACC 0.34375 on epoch=149
06/17/2022 16:51:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.10 on epoch=153
06/17/2022 16:51:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.12 on epoch=156
06/17/2022 16:51:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.08 on epoch=159
06/17/2022 16:51:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.09 on epoch=163
06/17/2022 16:51:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=166
06/17/2022 16:51:58 - INFO - __main__ - Global step 500 Train loss 0.10 ACC 0.46875 on epoch=166
06/17/2022 16:52:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=169
06/17/2022 16:52:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.05 on epoch=173
06/17/2022 16:52:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=176
06/17/2022 16:52:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=179
06/17/2022 16:52:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.08 on epoch=183
06/17/2022 16:52:13 - INFO - __main__ - Global step 550 Train loss 0.08 ACC 0.4375 on epoch=183
06/17/2022 16:52:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=186
06/17/2022 16:52:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.04 on epoch=189
06/17/2022 16:52:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=193
06/17/2022 16:52:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=196
06/17/2022 16:52:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=199
06/17/2022 16:52:28 - INFO - __main__ - Global step 600 Train loss 0.05 ACC 0.5 on epoch=199
06/17/2022 16:52:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=203
06/17/2022 16:52:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=206
06/17/2022 16:52:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.07 on epoch=209
06/17/2022 16:52:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.09 on epoch=213
06/17/2022 16:52:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=216
06/17/2022 16:52:43 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.59375 on epoch=216
06/17/2022 16:52:43 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=216, global_step=650
06/17/2022 16:52:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=219
06/17/2022 16:52:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=223
06/17/2022 16:52:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=226
06/17/2022 16:52:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=229
06/17/2022 16:52:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=233
06/17/2022 16:52:58 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.53125 on epoch=233
06/17/2022 16:53:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=236
06/17/2022 16:53:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=239
06/17/2022 16:53:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=243
06/17/2022 16:53:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=246
06/17/2022 16:53:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=249
06/17/2022 16:53:13 - INFO - __main__ - Global step 750 Train loss 0.04 ACC 0.53125 on epoch=249
06/17/2022 16:53:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=253
06/17/2022 16:53:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=256
06/17/2022 16:53:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=259
06/17/2022 16:53:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=263
06/17/2022 16:53:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=266
06/17/2022 16:53:28 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.53125 on epoch=266
06/17/2022 16:53:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=269
06/17/2022 16:53:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=273
06/17/2022 16:53:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=276
06/17/2022 16:53:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=279
06/17/2022 16:53:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=283
06/17/2022 16:53:43 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.46875 on epoch=283
06/17/2022 16:53:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=286
06/17/2022 16:53:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=289
06/17/2022 16:53:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=293
06/17/2022 16:53:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=296
06/17/2022 16:53:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=299
06/17/2022 16:53:59 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.5625 on epoch=299
06/17/2022 16:54:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=303
06/17/2022 16:54:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=306
06/17/2022 16:54:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=309
06/17/2022 16:54:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=313
06/17/2022 16:54:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=316
06/17/2022 16:54:14 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.40625 on epoch=316
06/17/2022 16:54:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=319
06/17/2022 16:54:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=323
06/17/2022 16:54:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=326
06/17/2022 16:54:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=329
06/17/2022 16:54:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=333
06/17/2022 16:54:29 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.5 on epoch=333
06/17/2022 16:54:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=336
06/17/2022 16:54:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=339
06/17/2022 16:54:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
06/17/2022 16:54:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=346
06/17/2022 16:54:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=349
06/17/2022 16:54:44 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.46875 on epoch=349
06/17/2022 16:54:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=353
06/17/2022 16:54:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
06/17/2022 16:54:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=359
06/17/2022 16:54:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=363
06/17/2022 16:54:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=366
06/17/2022 16:54:59 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.46875 on epoch=366
06/17/2022 16:55:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=369
06/17/2022 16:55:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
06/17/2022 16:55:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=376
06/17/2022 16:55:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
06/17/2022 16:55:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=383
06/17/2022 16:55:14 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.46875 on epoch=383
06/17/2022 16:55:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=386
06/17/2022 16:55:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
06/17/2022 16:55:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=393
06/17/2022 16:55:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=396
06/17/2022 16:55:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
06/17/2022 16:55:29 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.46875 on epoch=399
06/17/2022 16:55:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=403
06/17/2022 16:55:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
06/17/2022 16:55:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=409
06/17/2022 16:55:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=413
06/17/2022 16:55:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=416
06/17/2022 16:55:44 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.59375 on epoch=416
06/17/2022 16:55:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=419
06/17/2022 16:55:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=423
06/17/2022 16:55:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
06/17/2022 16:55:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
06/17/2022 16:55:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=433
06/17/2022 16:55:59 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.4375 on epoch=433
06/17/2022 16:56:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
06/17/2022 16:56:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/17/2022 16:56:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=443
06/17/2022 16:56:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
06/17/2022 16:56:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
06/17/2022 16:56:14 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.625 on epoch=449
06/17/2022 16:56:14 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=449, global_step=1350
06/17/2022 16:56:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
06/17/2022 16:56:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
06/17/2022 16:56:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=459
06/17/2022 16:56:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
06/17/2022 16:56:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
06/17/2022 16:56:29 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.65625 on epoch=466
06/17/2022 16:56:29 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=466, global_step=1400
06/17/2022 16:56:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
06/17/2022 16:56:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
06/17/2022 16:56:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
06/17/2022 16:56:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
06/17/2022 16:56:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
06/17/2022 16:56:44 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.59375 on epoch=483
06/17/2022 16:56:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
06/17/2022 16:56:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
06/17/2022 16:56:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
06/17/2022 16:56:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
06/17/2022 16:56:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
06/17/2022 16:56:59 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.5 on epoch=499
06/17/2022 16:57:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
06/17/2022 16:57:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
06/17/2022 16:57:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
06/17/2022 16:57:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
06/17/2022 16:57:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
06/17/2022 16:57:14 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.5625 on epoch=516
06/17/2022 16:57:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/17/2022 16:57:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
06/17/2022 16:57:23 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
06/17/2022 16:57:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/17/2022 16:57:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
06/17/2022 16:57:29 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.5625 on epoch=533
06/17/2022 16:57:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
06/17/2022 16:57:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
06/17/2022 16:57:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
06/17/2022 16:57:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
06/17/2022 16:57:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/17/2022 16:57:44 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.59375 on epoch=549
06/17/2022 16:57:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
06/17/2022 16:57:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
06/17/2022 16:57:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/17/2022 16:57:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
06/17/2022 16:57:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
06/17/2022 16:57:59 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.625 on epoch=566
06/17/2022 16:58:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
06/17/2022 16:58:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
06/17/2022 16:58:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/17/2022 16:58:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/17/2022 16:58:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/17/2022 16:58:14 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.5625 on epoch=583
06/17/2022 16:58:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/17/2022 16:58:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/17/2022 16:58:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/17/2022 16:58:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
06/17/2022 16:58:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
06/17/2022 16:58:30 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.625 on epoch=599
06/17/2022 16:58:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
06/17/2022 16:58:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/17/2022 16:58:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/17/2022 16:58:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/17/2022 16:58:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
06/17/2022 16:58:45 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.5625 on epoch=616
06/17/2022 16:58:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/17/2022 16:58:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
06/17/2022 16:58:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/17/2022 16:58:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/17/2022 16:58:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
06/17/2022 16:59:00 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.5 on epoch=633
06/17/2022 16:59:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
06/17/2022 16:59:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/17/2022 16:59:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/17/2022 16:59:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/17/2022 16:59:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/17/2022 16:59:15 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.6875 on epoch=649
06/17/2022 16:59:15 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=649, global_step=1950
06/17/2022 16:59:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
06/17/2022 16:59:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/17/2022 16:59:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/17/2022 16:59:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/17/2022 16:59:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/17/2022 16:59:30 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.65625 on epoch=666
06/17/2022 16:59:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/17/2022 16:59:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/17/2022 16:59:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/17/2022 16:59:41 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/17/2022 16:59:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/17/2022 16:59:45 - INFO - __main__ - Global step 2050 Train loss 0.00 ACC 0.625 on epoch=683
06/17/2022 16:59:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/17/2022 16:59:51 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/17/2022 16:59:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/17/2022 16:59:56 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/17/2022 16:59:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/17/2022 17:00:00 - INFO - __main__ - Global step 2100 Train loss 0.00 ACC 0.625 on epoch=699
06/17/2022 17:00:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/17/2022 17:00:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/17/2022 17:00:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/17/2022 17:00:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/17/2022 17:00:14 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/17/2022 17:00:15 - INFO - __main__ - Global step 2150 Train loss 0.00 ACC 0.59375 on epoch=716
06/17/2022 17:00:18 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/17/2022 17:00:21 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 17:00:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
06/17/2022 17:00:27 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
06/17/2022 17:00:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/17/2022 17:00:30 - INFO - __main__ - Global step 2200 Train loss 0.01 ACC 0.59375 on epoch=733
06/17/2022 17:00:33 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/17/2022 17:00:36 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/17/2022 17:00:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/17/2022 17:00:42 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/17/2022 17:00:45 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/17/2022 17:00:46 - INFO - __main__ - Global step 2250 Train loss 0.00 ACC 0.53125 on epoch=749
06/17/2022 17:00:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/17/2022 17:00:51 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/17/2022 17:00:54 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/17/2022 17:00:57 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/17/2022 17:01:00 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/17/2022 17:01:01 - INFO - __main__ - Global step 2300 Train loss 0.00 ACC 0.53125 on epoch=766
06/17/2022 17:01:04 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/17/2022 17:01:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/17/2022 17:01:09 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=776
06/17/2022 17:01:12 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=779
06/17/2022 17:01:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/17/2022 17:01:16 - INFO - __main__ - Global step 2350 Train loss 0.01 ACC 0.5625 on epoch=783
06/17/2022 17:01:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
06/17/2022 17:01:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/17/2022 17:01:24 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 17:01:27 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 17:01:30 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/17/2022 17:01:31 - INFO - __main__ - Global step 2400 Train loss 0.00 ACC 0.65625 on epoch=799
06/17/2022 17:01:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/17/2022 17:01:37 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 17:01:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/17/2022 17:01:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/17/2022 17:01:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/17/2022 17:01:46 - INFO - __main__ - Global step 2450 Train loss 0.00 ACC 0.6875 on epoch=816
06/17/2022 17:01:49 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/17/2022 17:01:52 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/17/2022 17:01:55 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/17/2022 17:01:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/17/2022 17:02:00 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 17:02:01 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.625 on epoch=833
06/17/2022 17:02:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 17:02:07 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/17/2022 17:02:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
06/17/2022 17:02:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 17:02:15 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/17/2022 17:02:16 - INFO - __main__ - Global step 2550 Train loss 0.01 ACC 0.65625 on epoch=849
06/17/2022 17:02:19 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/17/2022 17:02:22 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=856
06/17/2022 17:02:25 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/17/2022 17:02:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 17:02:31 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 17:02:32 - INFO - __main__ - Global step 2600 Train loss 0.01 ACC 0.5625 on epoch=866
06/17/2022 17:02:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/17/2022 17:02:37 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/17/2022 17:02:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/17/2022 17:02:43 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/17/2022 17:02:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 17:02:47 - INFO - __main__ - Global step 2650 Train loss 0.00 ACC 0.625 on epoch=883
06/17/2022 17:02:49 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 17:02:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 17:02:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
06/17/2022 17:02:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 17:03:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 17:03:02 - INFO - __main__ - Global step 2700 Train loss 0.00 ACC 0.625 on epoch=899
06/17/2022 17:03:05 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 17:03:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 17:03:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 17:03:13 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
06/17/2022 17:03:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/17/2022 17:03:17 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.6875 on epoch=916
06/17/2022 17:03:20 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 17:03:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
06/17/2022 17:03:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/17/2022 17:03:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/17/2022 17:03:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 17:03:32 - INFO - __main__ - Global step 2800 Train loss 0.00 ACC 0.6875 on epoch=933
06/17/2022 17:03:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 17:03:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/17/2022 17:03:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 17:03:43 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 17:03:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 17:03:47 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.78125 on epoch=949
06/17/2022 17:03:47 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.78125 on epoch=949, global_step=2850
06/17/2022 17:03:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
06/17/2022 17:03:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
06/17/2022 17:03:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 17:03:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 17:04:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 17:04:02 - INFO - __main__ - Global step 2900 Train loss 0.01 ACC 0.65625 on epoch=966
06/17/2022 17:04:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/17/2022 17:04:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 17:04:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/17/2022 17:04:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 17:04:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 17:04:17 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.6875 on epoch=983
06/17/2022 17:04:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 17:04:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/17/2022 17:04:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 17:04:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 17:04:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 17:04:32 - INFO - __main__ - Global step 3000 Train loss 0.00 ACC 0.75 on epoch=999
06/17/2022 17:04:32 - INFO - __main__ - save last model!
06/17/2022 17:04:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 17:04:32 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 17:04:32 - INFO - __main__ - Printing 3 examples
06/17/2022 17:04:32 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 17:04:32 - INFO - __main__ - ['contradiction']
06/17/2022 17:04:32 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 17:04:32 - INFO - __main__ - ['neutral']
06/17/2022 17:04:32 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 17:04:32 - INFO - __main__ - ['entailment']
06/17/2022 17:04:32 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:04:32 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:04:32 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 17:04:33 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 17:04:33 - INFO - __main__ - Printing 3 examples
06/17/2022 17:04:33 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 17:04:33 - INFO - __main__ - ['contradiction']
06/17/2022 17:04:33 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/17/2022 17:04:33 - INFO - __main__ - ['contradiction']
06/17/2022 17:04:33 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/17/2022 17:04:33 - INFO - __main__ - ['contradiction']
06/17/2022 17:04:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 17:04:33 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:04:33 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 17:04:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 17:04:33 - INFO - __main__ - Printing 3 examples
06/17/2022 17:04:33 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
06/17/2022 17:04:33 - INFO - __main__ - ['contradiction']
06/17/2022 17:04:33 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 17:04:33 - INFO - __main__ - ['contradiction']
06/17/2022 17:04:33 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
06/17/2022 17:04:33 - INFO - __main__ - ['contradiction']
06/17/2022 17:04:33 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:04:33 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:04:33 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 17:04:35 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_100_0.5_8_predictions.txt
06/17/2022 17:04:35 - INFO - __main__ - ACC on test data: 0.6250
06/17/2022 17:04:35 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.5, bsz=8, dev_performance=0.78125, test_performance=0.625
06/17/2022 17:04:35 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.4, bsz=8 ...
06/17/2022 17:04:36 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 17:04:36 - INFO - __main__ - Printing 3 examples
06/17/2022 17:04:36 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 17:04:36 - INFO - __main__ - ['contradiction']
06/17/2022 17:04:36 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/17/2022 17:04:36 - INFO - __main__ - ['contradiction']
06/17/2022 17:04:36 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/17/2022 17:04:36 - INFO - __main__ - ['contradiction']
06/17/2022 17:04:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 17:04:36 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:04:36 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 17:04:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 17:04:36 - INFO - __main__ - Printing 3 examples
06/17/2022 17:04:36 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
06/17/2022 17:04:36 - INFO - __main__ - ['contradiction']
06/17/2022 17:04:36 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 17:04:36 - INFO - __main__ - ['contradiction']
06/17/2022 17:04:36 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
06/17/2022 17:04:36 - INFO - __main__ - ['contradiction']
06/17/2022 17:04:36 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:04:36 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:04:36 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 17:04:51 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:04:52 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:04:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:04:52 - INFO - __main__ - Starting training!
06/17/2022 17:04:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:04:53 - INFO - __main__ - Starting training!
06/17/2022 17:04:57 - INFO - __main__ - Step 10 Global step 10 Train loss 1.30 on epoch=3
06/17/2022 17:04:59 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=6
06/17/2022 17:05:02 - INFO - __main__ - Step 30 Global step 30 Train loss 0.46 on epoch=9
06/17/2022 17:05:05 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=13
06/17/2022 17:05:08 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=16
06/17/2022 17:05:09 - INFO - __main__ - Global step 50 Train loss 0.65 ACC 0.5 on epoch=16
06/17/2022 17:05:09 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 17:05:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=19
06/17/2022 17:05:15 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=23
06/17/2022 17:05:17 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=26
06/17/2022 17:05:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=29
06/17/2022 17:05:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=33
06/17/2022 17:05:24 - INFO - __main__ - Global step 100 Train loss 0.45 ACC 0.5 on epoch=33
06/17/2022 17:05:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=36
06/17/2022 17:05:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=39
06/17/2022 17:05:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=43
06/17/2022 17:05:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=46
06/17/2022 17:05:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=49
06/17/2022 17:05:39 - INFO - __main__ - Global step 150 Train loss 0.44 ACC 0.5 on epoch=49
06/17/2022 17:05:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=53
06/17/2022 17:05:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=56
06/17/2022 17:05:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=59
06/17/2022 17:05:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=63
06/17/2022 17:05:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=66
06/17/2022 17:05:55 - INFO - __main__ - Global step 200 Train loss 0.40 ACC 0.4375 on epoch=66
06/17/2022 17:05:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=69
06/17/2022 17:06:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=73
06/17/2022 17:06:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=76
06/17/2022 17:06:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=79
06/17/2022 17:06:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=83
06/17/2022 17:06:10 - INFO - __main__ - Global step 250 Train loss 0.37 ACC 0.46875 on epoch=83
06/17/2022 17:06:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.30 on epoch=86
06/17/2022 17:06:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=89
06/17/2022 17:06:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=93
06/17/2022 17:06:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=96
06/17/2022 17:06:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=99
06/17/2022 17:06:25 - INFO - __main__ - Global step 300 Train loss 0.33 ACC 0.46875 on epoch=99
06/17/2022 17:06:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=103
06/17/2022 17:06:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.30 on epoch=106
06/17/2022 17:06:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=109
06/17/2022 17:06:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=113
06/17/2022 17:06:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=116
06/17/2022 17:06:40 - INFO - __main__ - Global step 350 Train loss 0.27 ACC 0.46875 on epoch=116
06/17/2022 17:06:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=119
06/17/2022 17:06:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=123
06/17/2022 17:06:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=126
06/17/2022 17:06:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=129
06/17/2022 17:06:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.21 on epoch=133
06/17/2022 17:06:56 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.46875 on epoch=133
06/17/2022 17:06:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=136
06/17/2022 17:07:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=139
06/17/2022 17:07:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.19 on epoch=143
06/17/2022 17:07:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.18 on epoch=146
06/17/2022 17:07:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=149
06/17/2022 17:07:11 - INFO - __main__ - Global step 450 Train loss 0.19 ACC 0.40625 on epoch=149
06/17/2022 17:07:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.18 on epoch=153
06/17/2022 17:07:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=156
06/17/2022 17:07:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=159
06/17/2022 17:07:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.17 on epoch=163
06/17/2022 17:07:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=166
06/17/2022 17:07:26 - INFO - __main__ - Global step 500 Train loss 0.18 ACC 0.375 on epoch=166
06/17/2022 17:07:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.16 on epoch=169
06/17/2022 17:07:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.12 on epoch=173
06/17/2022 17:07:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.14 on epoch=176
06/17/2022 17:07:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=179
06/17/2022 17:07:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.11 on epoch=183
06/17/2022 17:07:41 - INFO - __main__ - Global step 550 Train loss 0.13 ACC 0.40625 on epoch=183
06/17/2022 17:07:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=186
06/17/2022 17:07:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=189
06/17/2022 17:07:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.14 on epoch=193
06/17/2022 17:07:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.10 on epoch=196
06/17/2022 17:07:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.13 on epoch=199
06/17/2022 17:07:56 - INFO - __main__ - Global step 600 Train loss 0.13 ACC 0.40625 on epoch=199
06/17/2022 17:07:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.09 on epoch=203
06/17/2022 17:08:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=206
06/17/2022 17:08:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=209
06/17/2022 17:08:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.09 on epoch=213
06/17/2022 17:08:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=216
06/17/2022 17:08:11 - INFO - __main__ - Global step 650 Train loss 0.10 ACC 0.34375 on epoch=216
06/17/2022 17:08:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.09 on epoch=219
06/17/2022 17:08:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.08 on epoch=223
06/17/2022 17:08:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=226
06/17/2022 17:08:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=229
06/17/2022 17:08:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=233
06/17/2022 17:08:27 - INFO - __main__ - Global step 700 Train loss 0.07 ACC 0.375 on epoch=233
06/17/2022 17:08:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=236
06/17/2022 17:08:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=239
06/17/2022 17:08:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=243
06/17/2022 17:08:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=246
06/17/2022 17:08:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=249
06/17/2022 17:08:42 - INFO - __main__ - Global step 750 Train loss 0.06 ACC 0.4375 on epoch=249
06/17/2022 17:08:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=253
06/17/2022 17:08:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=256
06/17/2022 17:08:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=259
06/17/2022 17:08:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=263
06/17/2022 17:08:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=266
06/17/2022 17:08:57 - INFO - __main__ - Global step 800 Train loss 0.07 ACC 0.46875 on epoch=266
06/17/2022 17:09:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=269
06/17/2022 17:09:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=273
06/17/2022 17:09:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=276
06/17/2022 17:09:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=279
06/17/2022 17:09:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=283
06/17/2022 17:09:12 - INFO - __main__ - Global step 850 Train loss 0.05 ACC 0.4375 on epoch=283
06/17/2022 17:09:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=286
06/17/2022 17:09:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=289
06/17/2022 17:09:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=293
06/17/2022 17:09:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=296
06/17/2022 17:09:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=299
06/17/2022 17:09:27 - INFO - __main__ - Global step 900 Train loss 0.05 ACC 0.53125 on epoch=299
06/17/2022 17:09:27 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=299, global_step=900
06/17/2022 17:09:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=303
06/17/2022 17:09:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=306
06/17/2022 17:09:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=309
06/17/2022 17:09:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=313
06/17/2022 17:09:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=316
06/17/2022 17:09:42 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.5625 on epoch=316
06/17/2022 17:09:42 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=316, global_step=950
06/17/2022 17:09:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=319
06/17/2022 17:09:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=323
06/17/2022 17:09:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=326
06/17/2022 17:09:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=329
06/17/2022 17:09:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=333
06/17/2022 17:09:58 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.53125 on epoch=333
06/17/2022 17:10:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=336
06/17/2022 17:10:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=339
06/17/2022 17:10:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=343
06/17/2022 17:10:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=346
06/17/2022 17:10:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=349
06/17/2022 17:10:13 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.5 on epoch=349
06/17/2022 17:10:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=353
06/17/2022 17:10:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=356
06/17/2022 17:10:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=359
06/17/2022 17:10:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=363
06/17/2022 17:10:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
06/17/2022 17:10:28 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.5 on epoch=366
06/17/2022 17:10:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
06/17/2022 17:10:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
06/17/2022 17:10:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=376
06/17/2022 17:10:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=379
06/17/2022 17:10:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=383
06/17/2022 17:10:43 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.53125 on epoch=383
06/17/2022 17:10:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=386
06/17/2022 17:10:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
06/17/2022 17:10:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
06/17/2022 17:10:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
06/17/2022 17:10:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
06/17/2022 17:10:59 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.5 on epoch=399
06/17/2022 17:11:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
06/17/2022 17:11:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=406
06/17/2022 17:11:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=409
06/17/2022 17:11:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
06/17/2022 17:11:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
06/17/2022 17:11:14 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.46875 on epoch=416
06/17/2022 17:11:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=419
06/17/2022 17:11:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=423
06/17/2022 17:11:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=426
06/17/2022 17:11:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
06/17/2022 17:11:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
06/17/2022 17:11:29 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.46875 on epoch=433
06/17/2022 17:11:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
06/17/2022 17:11:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=439
06/17/2022 17:11:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
06/17/2022 17:11:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
06/17/2022 17:11:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
06/17/2022 17:11:45 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.46875 on epoch=449
06/17/2022 17:11:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
06/17/2022 17:11:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
06/17/2022 17:11:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
06/17/2022 17:11:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
06/17/2022 17:11:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=466
06/17/2022 17:12:00 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.5 on epoch=466
06/17/2022 17:12:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
06/17/2022 17:12:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
06/17/2022 17:12:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
06/17/2022 17:12:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=479
06/17/2022 17:12:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
06/17/2022 17:12:15 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.65625 on epoch=483
06/17/2022 17:12:15 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.65625 on epoch=483, global_step=1450
06/17/2022 17:12:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
06/17/2022 17:12:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
06/17/2022 17:12:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
06/17/2022 17:12:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
06/17/2022 17:12:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
06/17/2022 17:12:30 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.59375 on epoch=499
06/17/2022 17:12:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
06/17/2022 17:12:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
06/17/2022 17:12:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
06/17/2022 17:12:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
06/17/2022 17:12:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
06/17/2022 17:12:46 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.53125 on epoch=516
06/17/2022 17:12:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=519
06/17/2022 17:12:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
06/17/2022 17:12:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
06/17/2022 17:12:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
06/17/2022 17:13:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=533
06/17/2022 17:13:01 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.53125 on epoch=533
06/17/2022 17:13:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
06/17/2022 17:13:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
06/17/2022 17:13:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
06/17/2022 17:13:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
06/17/2022 17:13:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/17/2022 17:13:16 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.53125 on epoch=549
06/17/2022 17:13:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
06/17/2022 17:13:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
06/17/2022 17:13:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
06/17/2022 17:13:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
06/17/2022 17:13:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
06/17/2022 17:13:32 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.5 on epoch=566
06/17/2022 17:13:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
06/17/2022 17:13:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
06/17/2022 17:13:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/17/2022 17:13:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
06/17/2022 17:13:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
06/17/2022 17:13:47 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.5 on epoch=583
06/17/2022 17:13:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/17/2022 17:13:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/17/2022 17:13:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
06/17/2022 17:13:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
06/17/2022 17:14:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
06/17/2022 17:14:03 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.5 on epoch=599
06/17/2022 17:14:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/17/2022 17:14:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/17/2022 17:14:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=609
06/17/2022 17:14:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/17/2022 17:14:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/17/2022 17:14:18 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.53125 on epoch=616
06/17/2022 17:14:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/17/2022 17:14:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
06/17/2022 17:14:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
06/17/2022 17:14:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/17/2022 17:14:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/17/2022 17:14:33 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.5 on epoch=633
06/17/2022 17:14:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
06/17/2022 17:14:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/17/2022 17:14:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/17/2022 17:14:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/17/2022 17:14:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/17/2022 17:14:49 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.5625 on epoch=649
06/17/2022 17:14:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/17/2022 17:14:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/17/2022 17:14:57 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/17/2022 17:15:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/17/2022 17:15:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
06/17/2022 17:15:04 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.53125 on epoch=666
06/17/2022 17:15:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
06/17/2022 17:15:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/17/2022 17:15:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/17/2022 17:15:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
06/17/2022 17:15:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/17/2022 17:15:19 - INFO - __main__ - Global step 2050 Train loss 0.01 ACC 0.53125 on epoch=683
06/17/2022 17:15:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
06/17/2022 17:15:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/17/2022 17:15:27 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/17/2022 17:15:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=696
06/17/2022 17:15:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/17/2022 17:15:34 - INFO - __main__ - Global step 2100 Train loss 0.02 ACC 0.4375 on epoch=699
06/17/2022 17:15:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/17/2022 17:15:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/17/2022 17:15:42 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/17/2022 17:15:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/17/2022 17:15:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/17/2022 17:15:49 - INFO - __main__ - Global step 2150 Train loss 0.00 ACC 0.5 on epoch=716
06/17/2022 17:15:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/17/2022 17:15:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 17:15:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/17/2022 17:16:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/17/2022 17:16:03 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/17/2022 17:16:04 - INFO - __main__ - Global step 2200 Train loss 0.00 ACC 0.5 on epoch=733
06/17/2022 17:16:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/17/2022 17:16:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/17/2022 17:16:12 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/17/2022 17:16:15 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/17/2022 17:16:18 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
06/17/2022 17:16:19 - INFO - __main__ - Global step 2250 Train loss 0.01 ACC 0.5 on epoch=749
06/17/2022 17:16:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/17/2022 17:16:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/17/2022 17:16:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/17/2022 17:16:30 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/17/2022 17:16:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/17/2022 17:16:34 - INFO - __main__ - Global step 2300 Train loss 0.00 ACC 0.53125 on epoch=766
06/17/2022 17:16:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/17/2022 17:16:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/17/2022 17:16:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/17/2022 17:16:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/17/2022 17:16:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/17/2022 17:16:49 - INFO - __main__ - Global step 2350 Train loss 0.00 ACC 0.53125 on epoch=783
06/17/2022 17:16:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/17/2022 17:16:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/17/2022 17:16:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 17:17:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 17:17:03 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/17/2022 17:17:04 - INFO - __main__ - Global step 2400 Train loss 0.00 ACC 0.5625 on epoch=799
06/17/2022 17:17:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/17/2022 17:17:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 17:17:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/17/2022 17:17:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/17/2022 17:17:18 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/17/2022 17:17:19 - INFO - __main__ - Global step 2450 Train loss 0.00 ACC 0.5625 on epoch=816
06/17/2022 17:17:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/17/2022 17:17:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/17/2022 17:17:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/17/2022 17:17:30 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/17/2022 17:17:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 17:17:34 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.5625 on epoch=833
06/17/2022 17:17:36 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
06/17/2022 17:17:39 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/17/2022 17:17:42 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/17/2022 17:17:45 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 17:17:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/17/2022 17:17:49 - INFO - __main__ - Global step 2550 Train loss 0.00 ACC 0.53125 on epoch=849
06/17/2022 17:17:51 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/17/2022 17:17:54 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 17:17:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/17/2022 17:18:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 17:18:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
06/17/2022 17:18:03 - INFO - __main__ - Global step 2600 Train loss 0.00 ACC 0.5625 on epoch=866
06/17/2022 17:18:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/17/2022 17:18:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/17/2022 17:18:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/17/2022 17:18:15 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
06/17/2022 17:18:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 17:18:18 - INFO - __main__ - Global step 2650 Train loss 0.00 ACC 0.5625 on epoch=883
06/17/2022 17:18:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
06/17/2022 17:18:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=889
06/17/2022 17:18:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 17:18:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 17:18:33 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 17:18:33 - INFO - __main__ - Global step 2700 Train loss 0.01 ACC 0.5625 on epoch=899
06/17/2022 17:18:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 17:18:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 17:18:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
06/17/2022 17:18:45 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 17:18:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/17/2022 17:18:48 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.5625 on epoch=916
06/17/2022 17:18:51 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=919
06/17/2022 17:18:54 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 17:18:57 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/17/2022 17:19:00 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/17/2022 17:19:03 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 17:19:03 - INFO - __main__ - Global step 2800 Train loss 0.01 ACC 0.5625 on epoch=933
06/17/2022 17:19:06 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 17:19:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/17/2022 17:19:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 17:19:15 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 17:19:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 17:19:18 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.59375 on epoch=949
06/17/2022 17:19:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/17/2022 17:19:24 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/17/2022 17:19:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 17:19:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 17:19:32 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=966
06/17/2022 17:19:33 - INFO - __main__ - Global step 2900 Train loss 0.01 ACC 0.5 on epoch=966
06/17/2022 17:19:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/17/2022 17:19:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 17:19:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/17/2022 17:19:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 17:19:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 17:19:48 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.5 on epoch=983
06/17/2022 17:19:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=986
06/17/2022 17:19:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/17/2022 17:19:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 17:19:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 17:20:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 17:20:03 - INFO - __main__ - Global step 3000 Train loss 0.01 ACC 0.5625 on epoch=999
06/17/2022 17:20:03 - INFO - __main__ - save last model!
06/17/2022 17:20:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 17:20:03 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 17:20:03 - INFO - __main__ - Printing 3 examples
06/17/2022 17:20:03 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 17:20:03 - INFO - __main__ - ['contradiction']
06/17/2022 17:20:03 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 17:20:03 - INFO - __main__ - ['neutral']
06/17/2022 17:20:03 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 17:20:03 - INFO - __main__ - ['entailment']
06/17/2022 17:20:03 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:20:03 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:20:03 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 17:20:04 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 17:20:04 - INFO - __main__ - Printing 3 examples
06/17/2022 17:20:04 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 17:20:04 - INFO - __main__ - ['contradiction']
06/17/2022 17:20:04 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/17/2022 17:20:04 - INFO - __main__ - ['contradiction']
06/17/2022 17:20:04 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/17/2022 17:20:04 - INFO - __main__ - ['contradiction']
06/17/2022 17:20:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 17:20:04 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:20:04 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 17:20:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 17:20:04 - INFO - __main__ - Printing 3 examples
06/17/2022 17:20:04 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
06/17/2022 17:20:04 - INFO - __main__ - ['contradiction']
06/17/2022 17:20:04 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 17:20:04 - INFO - __main__ - ['contradiction']
06/17/2022 17:20:04 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
06/17/2022 17:20:04 - INFO - __main__ - ['contradiction']
06/17/2022 17:20:04 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:20:04 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:20:04 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 17:20:05 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_100_0.4_8_predictions.txt
06/17/2022 17:20:05 - INFO - __main__ - ACC on test data: 0.6250
06/17/2022 17:20:06 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.4, bsz=8, dev_performance=0.65625, test_performance=0.625
06/17/2022 17:20:06 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.3, bsz=8 ...
06/17/2022 17:20:07 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 17:20:07 - INFO - __main__ - Printing 3 examples
06/17/2022 17:20:07 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 17:20:07 - INFO - __main__ - ['contradiction']
06/17/2022 17:20:07 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/17/2022 17:20:07 - INFO - __main__ - ['contradiction']
06/17/2022 17:20:07 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/17/2022 17:20:07 - INFO - __main__ - ['contradiction']
06/17/2022 17:20:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 17:20:07 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:20:07 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 17:20:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 17:20:07 - INFO - __main__ - Printing 3 examples
06/17/2022 17:20:07 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
06/17/2022 17:20:07 - INFO - __main__ - ['contradiction']
06/17/2022 17:20:07 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 17:20:07 - INFO - __main__ - ['contradiction']
06/17/2022 17:20:07 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
06/17/2022 17:20:07 - INFO - __main__ - ['contradiction']
06/17/2022 17:20:07 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:20:07 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:20:07 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 17:20:22 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:20:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:20:23 - INFO - __main__ - Starting training!
06/17/2022 17:20:23 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:20:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:20:24 - INFO - __main__ - Starting training!
06/17/2022 17:20:28 - INFO - __main__ - Step 10 Global step 10 Train loss 1.23 on epoch=3
06/17/2022 17:20:31 - INFO - __main__ - Step 20 Global step 20 Train loss 0.65 on epoch=6
06/17/2022 17:20:34 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=9
06/17/2022 17:20:37 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=13
06/17/2022 17:20:39 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=16
06/17/2022 17:20:40 - INFO - __main__ - Global step 50 Train loss 0.66 ACC 0.5 on epoch=16
06/17/2022 17:20:40 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 17:20:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=19
06/17/2022 17:20:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.41 on epoch=23
06/17/2022 17:20:49 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=26
06/17/2022 17:20:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=29
06/17/2022 17:20:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=33
06/17/2022 17:20:56 - INFO - __main__ - Global step 100 Train loss 0.45 ACC 0.5 on epoch=33
06/17/2022 17:20:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.38 on epoch=36
06/17/2022 17:21:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.39 on epoch=39
06/17/2022 17:21:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=43
06/17/2022 17:21:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=46
06/17/2022 17:21:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
06/17/2022 17:21:11 - INFO - __main__ - Global step 150 Train loss 0.42 ACC 0.5 on epoch=49
06/17/2022 17:21:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=53
06/17/2022 17:21:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=56
06/17/2022 17:21:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=59
06/17/2022 17:21:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=63
06/17/2022 17:21:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=66
06/17/2022 17:21:26 - INFO - __main__ - Global step 200 Train loss 0.38 ACC 0.5 on epoch=66
06/17/2022 17:21:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=69
06/17/2022 17:21:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.33 on epoch=73
06/17/2022 17:21:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=76
06/17/2022 17:21:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=79
06/17/2022 17:21:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=83
06/17/2022 17:21:41 - INFO - __main__ - Global step 250 Train loss 0.39 ACC 0.5 on epoch=83
06/17/2022 17:21:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.35 on epoch=86
06/17/2022 17:21:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=89
06/17/2022 17:21:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.36 on epoch=93
06/17/2022 17:21:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=96
06/17/2022 17:21:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=99
06/17/2022 17:21:56 - INFO - __main__ - Global step 300 Train loss 0.36 ACC 0.5 on epoch=99
06/17/2022 17:21:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=103
06/17/2022 17:22:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=106
06/17/2022 17:22:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=109
06/17/2022 17:22:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=113
06/17/2022 17:22:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=116
06/17/2022 17:22:11 - INFO - __main__ - Global step 350 Train loss 0.35 ACC 0.5 on epoch=116
06/17/2022 17:22:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.31 on epoch=119
06/17/2022 17:22:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=123
06/17/2022 17:22:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=126
06/17/2022 17:22:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=129
06/17/2022 17:22:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=133
06/17/2022 17:22:27 - INFO - __main__ - Global step 400 Train loss 0.33 ACC 0.4375 on epoch=133
06/17/2022 17:22:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=136
06/17/2022 17:22:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=139
06/17/2022 17:22:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=143
06/17/2022 17:22:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=146
06/17/2022 17:22:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=149
06/17/2022 17:22:42 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.5 on epoch=149
06/17/2022 17:22:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.19 on epoch=153
06/17/2022 17:22:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=156
06/17/2022 17:22:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=159
06/17/2022 17:22:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=163
06/17/2022 17:22:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=166
06/17/2022 17:22:57 - INFO - __main__ - Global step 500 Train loss 0.22 ACC 0.4375 on epoch=166
06/17/2022 17:23:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=169
06/17/2022 17:23:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.19 on epoch=173
06/17/2022 17:23:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=176
06/17/2022 17:23:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=179
06/17/2022 17:23:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=183
06/17/2022 17:23:12 - INFO - __main__ - Global step 550 Train loss 0.20 ACC 0.40625 on epoch=183
06/17/2022 17:23:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=186
06/17/2022 17:23:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=189
06/17/2022 17:23:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=193
06/17/2022 17:23:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=196
06/17/2022 17:23:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=199
06/17/2022 17:23:27 - INFO - __main__ - Global step 600 Train loss 0.20 ACC 0.375 on epoch=199
06/17/2022 17:23:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=203
06/17/2022 17:23:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=206
06/17/2022 17:23:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=209
06/17/2022 17:23:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.13 on epoch=213
06/17/2022 17:23:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=216
06/17/2022 17:23:42 - INFO - __main__ - Global step 650 Train loss 0.15 ACC 0.375 on epoch=216
06/17/2022 17:23:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=219
06/17/2022 17:23:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=223
06/17/2022 17:23:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=226
06/17/2022 17:23:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=229
06/17/2022 17:23:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=233
06/17/2022 17:23:58 - INFO - __main__ - Global step 700 Train loss 0.17 ACC 0.46875 on epoch=233
06/17/2022 17:24:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=236
06/17/2022 17:24:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=239
06/17/2022 17:24:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=243
06/17/2022 17:24:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=246
06/17/2022 17:24:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=249
06/17/2022 17:24:13 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.4375 on epoch=249
06/17/2022 17:24:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=253
06/17/2022 17:24:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=256
06/17/2022 17:24:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=259
06/17/2022 17:24:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=263
06/17/2022 17:24:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=266
06/17/2022 17:24:28 - INFO - __main__ - Global step 800 Train loss 0.10 ACC 0.4375 on epoch=266
06/17/2022 17:24:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=269
06/17/2022 17:24:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=273
06/17/2022 17:24:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=276
06/17/2022 17:24:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=279
06/17/2022 17:24:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=283
06/17/2022 17:24:43 - INFO - __main__ - Global step 850 Train loss 0.08 ACC 0.46875 on epoch=283
06/17/2022 17:24:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=286
06/17/2022 17:24:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=289
06/17/2022 17:24:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=293
06/17/2022 17:24:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=296
06/17/2022 17:24:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
06/17/2022 17:24:58 - INFO - __main__ - Global step 900 Train loss 0.07 ACC 0.46875 on epoch=299
06/17/2022 17:25:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=303
06/17/2022 17:25:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=306
06/17/2022 17:25:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=309
06/17/2022 17:25:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=313
06/17/2022 17:25:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=316
06/17/2022 17:25:14 - INFO - __main__ - Global step 950 Train loss 0.06 ACC 0.4375 on epoch=316
06/17/2022 17:25:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=319
06/17/2022 17:25:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=323
06/17/2022 17:25:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=326
06/17/2022 17:25:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=329
06/17/2022 17:25:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=333
06/17/2022 17:25:29 - INFO - __main__ - Global step 1000 Train loss 0.06 ACC 0.46875 on epoch=333
06/17/2022 17:25:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=336
06/17/2022 17:25:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=339
06/17/2022 17:25:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=343
06/17/2022 17:25:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=346
06/17/2022 17:25:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=349
06/17/2022 17:25:44 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.46875 on epoch=349
06/17/2022 17:25:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=353
06/17/2022 17:25:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=356
06/17/2022 17:25:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
06/17/2022 17:25:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=363
06/17/2022 17:25:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=366
06/17/2022 17:25:59 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.40625 on epoch=366
06/17/2022 17:26:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=369
06/17/2022 17:26:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
06/17/2022 17:26:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=376
06/17/2022 17:26:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=379
06/17/2022 17:26:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
06/17/2022 17:26:15 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.4375 on epoch=383
06/17/2022 17:26:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=386
06/17/2022 17:26:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=389
06/17/2022 17:26:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
06/17/2022 17:26:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
06/17/2022 17:26:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
06/17/2022 17:26:30 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.46875 on epoch=399
06/17/2022 17:26:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
06/17/2022 17:26:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=406
06/17/2022 17:26:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=409
06/17/2022 17:26:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=413
06/17/2022 17:26:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
06/17/2022 17:26:45 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.4375 on epoch=416
06/17/2022 17:26:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
06/17/2022 17:26:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=423
06/17/2022 17:26:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
06/17/2022 17:26:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
06/17/2022 17:26:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=433
06/17/2022 17:27:00 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.4375 on epoch=433
06/17/2022 17:27:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
06/17/2022 17:27:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
06/17/2022 17:27:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=443
06/17/2022 17:27:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
06/17/2022 17:27:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
06/17/2022 17:27:16 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.46875 on epoch=449
06/17/2022 17:27:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
06/17/2022 17:27:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
06/17/2022 17:27:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
06/17/2022 17:27:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
06/17/2022 17:27:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=466
06/17/2022 17:27:31 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.4375 on epoch=466
06/17/2022 17:27:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
06/17/2022 17:27:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
06/17/2022 17:27:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
06/17/2022 17:27:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
06/17/2022 17:27:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
06/17/2022 17:27:46 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.4375 on epoch=483
06/17/2022 17:27:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
06/17/2022 17:27:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
06/17/2022 17:27:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
06/17/2022 17:27:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
06/17/2022 17:28:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
06/17/2022 17:28:01 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.4375 on epoch=499
06/17/2022 17:28:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
06/17/2022 17:28:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
06/17/2022 17:28:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
06/17/2022 17:28:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
06/17/2022 17:28:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
06/17/2022 17:28:17 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.5 on epoch=516
06/17/2022 17:28:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
06/17/2022 17:28:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
06/17/2022 17:28:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=526
06/17/2022 17:28:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
06/17/2022 17:28:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
06/17/2022 17:28:32 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.5 on epoch=533
06/17/2022 17:28:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
06/17/2022 17:28:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
06/17/2022 17:28:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
06/17/2022 17:28:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
06/17/2022 17:28:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=549
06/17/2022 17:28:47 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.5 on epoch=549
06/17/2022 17:28:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
06/17/2022 17:28:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
06/17/2022 17:28:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/17/2022 17:28:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
06/17/2022 17:29:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
06/17/2022 17:29:02 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.46875 on epoch=566
06/17/2022 17:29:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
06/17/2022 17:29:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
06/17/2022 17:29:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
06/17/2022 17:29:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/17/2022 17:29:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/17/2022 17:29:18 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.4375 on epoch=583
06/17/2022 17:29:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/17/2022 17:29:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
06/17/2022 17:29:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/17/2022 17:29:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
06/17/2022 17:29:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=599
06/17/2022 17:29:33 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.4375 on epoch=599
06/17/2022 17:29:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/17/2022 17:29:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=606
06/17/2022 17:29:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/17/2022 17:29:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/17/2022 17:29:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=616
06/17/2022 17:29:48 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.46875 on epoch=616
06/17/2022 17:29:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/17/2022 17:29:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
06/17/2022 17:29:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
06/17/2022 17:29:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
06/17/2022 17:30:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/17/2022 17:30:03 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.46875 on epoch=633
06/17/2022 17:30:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
06/17/2022 17:30:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/17/2022 17:30:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/17/2022 17:30:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
06/17/2022 17:30:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/17/2022 17:30:19 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.40625 on epoch=649
06/17/2022 17:30:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/17/2022 17:30:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/17/2022 17:30:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/17/2022 17:30:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/17/2022 17:30:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/17/2022 17:30:34 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.4375 on epoch=666
06/17/2022 17:30:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
06/17/2022 17:30:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
06/17/2022 17:30:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/17/2022 17:30:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/17/2022 17:30:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/17/2022 17:30:49 - INFO - __main__ - Global step 2050 Train loss 0.01 ACC 0.375 on epoch=683
06/17/2022 17:30:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/17/2022 17:30:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
06/17/2022 17:30:58 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/17/2022 17:31:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
06/17/2022 17:31:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/17/2022 17:31:04 - INFO - __main__ - Global step 2100 Train loss 0.01 ACC 0.5 on epoch=699
06/17/2022 17:31:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/17/2022 17:31:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/17/2022 17:31:13 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
06/17/2022 17:31:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/17/2022 17:31:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/17/2022 17:31:20 - INFO - __main__ - Global step 2150 Train loss 0.01 ACC 0.59375 on epoch=716
06/17/2022 17:31:20 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=716, global_step=2150
06/17/2022 17:31:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/17/2022 17:31:25 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 17:31:28 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
06/17/2022 17:31:31 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/17/2022 17:31:34 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
06/17/2022 17:31:35 - INFO - __main__ - Global step 2200 Train loss 0.01 ACC 0.625 on epoch=733
06/17/2022 17:31:35 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=733, global_step=2200
06/17/2022 17:31:38 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=736
06/17/2022 17:31:41 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/17/2022 17:31:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=743
06/17/2022 17:31:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/17/2022 17:31:49 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/17/2022 17:31:50 - INFO - __main__ - Global step 2250 Train loss 0.03 ACC 0.4375 on epoch=749
06/17/2022 17:31:53 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
06/17/2022 17:31:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/17/2022 17:31:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
06/17/2022 17:32:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
06/17/2022 17:32:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/17/2022 17:32:05 - INFO - __main__ - Global step 2300 Train loss 0.01 ACC 0.625 on epoch=766
06/17/2022 17:32:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
06/17/2022 17:32:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=773
06/17/2022 17:32:14 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
06/17/2022 17:32:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/17/2022 17:32:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/17/2022 17:32:21 - INFO - __main__ - Global step 2350 Train loss 0.01 ACC 0.6875 on epoch=783
06/17/2022 17:32:21 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.6875 on epoch=783, global_step=2350
06/17/2022 17:32:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/17/2022 17:32:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/17/2022 17:32:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 17:32:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
06/17/2022 17:32:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/17/2022 17:32:36 - INFO - __main__ - Global step 2400 Train loss 0.01 ACC 0.4375 on epoch=799
06/17/2022 17:32:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/17/2022 17:32:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 17:32:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/17/2022 17:32:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/17/2022 17:32:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=816
06/17/2022 17:32:51 - INFO - __main__ - Global step 2450 Train loss 0.01 ACC 0.5625 on epoch=816
06/17/2022 17:32:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/17/2022 17:32:57 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
06/17/2022 17:33:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/17/2022 17:33:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/17/2022 17:33:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 17:33:06 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.625 on epoch=833
06/17/2022 17:33:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 17:33:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/17/2022 17:33:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
06/17/2022 17:33:18 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 17:33:21 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/17/2022 17:33:22 - INFO - __main__ - Global step 2550 Train loss 0.01 ACC 0.6875 on epoch=849
06/17/2022 17:33:25 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=853
06/17/2022 17:33:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 17:33:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/17/2022 17:33:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 17:33:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 17:33:37 - INFO - __main__ - Global step 2600 Train loss 0.01 ACC 0.53125 on epoch=866
06/17/2022 17:33:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/17/2022 17:33:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/17/2022 17:33:46 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/17/2022 17:33:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/17/2022 17:33:51 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 17:33:52 - INFO - __main__ - Global step 2650 Train loss 0.00 ACC 0.5 on epoch=883
06/17/2022 17:33:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 17:33:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
06/17/2022 17:34:01 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 17:34:04 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 17:34:07 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 17:34:08 - INFO - __main__ - Global step 2700 Train loss 0.00 ACC 0.5 on epoch=899
06/17/2022 17:34:11 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 17:34:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 17:34:16 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 17:34:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 17:34:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/17/2022 17:34:23 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.5 on epoch=916
06/17/2022 17:34:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 17:34:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 17:34:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
06/17/2022 17:34:35 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/17/2022 17:34:38 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 17:34:39 - INFO - __main__ - Global step 2800 Train loss 0.00 ACC 0.59375 on epoch=933
06/17/2022 17:34:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 17:34:44 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
06/17/2022 17:34:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 17:34:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 17:34:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 17:34:54 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.5625 on epoch=949
06/17/2022 17:34:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/17/2022 17:35:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/17/2022 17:35:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 17:35:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/17/2022 17:35:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 17:35:09 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.53125 on epoch=966
06/17/2022 17:35:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/17/2022 17:35:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 17:35:18 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/17/2022 17:35:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 17:35:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 17:35:24 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.5625 on epoch=983
06/17/2022 17:35:27 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 17:35:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
06/17/2022 17:35:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 17:35:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 17:35:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 17:35:40 - INFO - __main__ - Global step 3000 Train loss 0.00 ACC 0.625 on epoch=999
06/17/2022 17:35:40 - INFO - __main__ - save last model!
06/17/2022 17:35:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 17:35:40 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 17:35:40 - INFO - __main__ - Printing 3 examples
06/17/2022 17:35:40 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 17:35:40 - INFO - __main__ - ['contradiction']
06/17/2022 17:35:40 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 17:35:40 - INFO - __main__ - ['neutral']
06/17/2022 17:35:40 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 17:35:40 - INFO - __main__ - ['entailment']
06/17/2022 17:35:40 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:35:40 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:35:40 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 17:35:41 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 17:35:41 - INFO - __main__ - Printing 3 examples
06/17/2022 17:35:41 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 17:35:41 - INFO - __main__ - ['contradiction']
06/17/2022 17:35:41 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/17/2022 17:35:41 - INFO - __main__ - ['contradiction']
06/17/2022 17:35:41 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/17/2022 17:35:41 - INFO - __main__ - ['contradiction']
06/17/2022 17:35:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 17:35:41 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:35:41 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 17:35:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 17:35:41 - INFO - __main__ - Printing 3 examples
06/17/2022 17:35:41 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
06/17/2022 17:35:41 - INFO - __main__ - ['contradiction']
06/17/2022 17:35:41 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 17:35:41 - INFO - __main__ - ['contradiction']
06/17/2022 17:35:41 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
06/17/2022 17:35:41 - INFO - __main__ - ['contradiction']
06/17/2022 17:35:41 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:35:41 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:35:41 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 17:35:42 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_100_0.3_8_predictions.txt
06/17/2022 17:35:42 - INFO - __main__ - ACC on test data: 0.6429
06/17/2022 17:35:42 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.3, bsz=8, dev_performance=0.6875, test_performance=0.6428571428571429
06/17/2022 17:35:42 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.2, bsz=8 ...
06/17/2022 17:35:43 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 17:35:43 - INFO - __main__ - Printing 3 examples
06/17/2022 17:35:43 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 17:35:43 - INFO - __main__ - ['contradiction']
06/17/2022 17:35:43 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/17/2022 17:35:43 - INFO - __main__ - ['contradiction']
06/17/2022 17:35:43 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/17/2022 17:35:43 - INFO - __main__ - ['contradiction']
06/17/2022 17:35:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 17:35:43 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:35:43 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 17:35:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 17:35:43 - INFO - __main__ - Printing 3 examples
06/17/2022 17:35:43 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
06/17/2022 17:35:43 - INFO - __main__ - ['contradiction']
06/17/2022 17:35:43 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 17:35:43 - INFO - __main__ - ['contradiction']
06/17/2022 17:35:43 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
06/17/2022 17:35:43 - INFO - __main__ - ['contradiction']
06/17/2022 17:35:43 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:35:43 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:35:44 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 17:35:56 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:35:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:35:57 - INFO - __main__ - Starting training!
06/17/2022 17:36:01 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:36:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:36:02 - INFO - __main__ - Starting training!
06/17/2022 17:36:05 - INFO - __main__ - Step 10 Global step 10 Train loss 1.72 on epoch=3
06/17/2022 17:36:08 - INFO - __main__ - Step 20 Global step 20 Train loss 0.66 on epoch=6
06/17/2022 17:36:11 - INFO - __main__ - Step 30 Global step 30 Train loss 0.45 on epoch=9
06/17/2022 17:36:14 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=13
06/17/2022 17:36:17 - INFO - __main__ - Step 50 Global step 50 Train loss 0.56 on epoch=16
06/17/2022 17:36:18 - INFO - __main__ - Global step 50 Train loss 0.78 ACC 0.5 on epoch=16
06/17/2022 17:36:18 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 17:36:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=19
06/17/2022 17:36:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=23
06/17/2022 17:36:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=26
06/17/2022 17:36:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=29
06/17/2022 17:36:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=33
06/17/2022 17:36:33 - INFO - __main__ - Global step 100 Train loss 0.47 ACC 0.5 on epoch=33
06/17/2022 17:36:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=36
06/17/2022 17:36:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=39
06/17/2022 17:36:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=43
06/17/2022 17:36:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=46
06/17/2022 17:36:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
06/17/2022 17:36:48 - INFO - __main__ - Global step 150 Train loss 0.46 ACC 0.5 on epoch=49
06/17/2022 17:36:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
06/17/2022 17:36:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=56
06/17/2022 17:36:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=59
06/17/2022 17:36:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=63
06/17/2022 17:37:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=66
06/17/2022 17:37:03 - INFO - __main__ - Global step 200 Train loss 0.44 ACC 0.5 on epoch=66
06/17/2022 17:37:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=69
06/17/2022 17:37:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
06/17/2022 17:37:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=76
06/17/2022 17:37:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
06/17/2022 17:37:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=83
06/17/2022 17:37:18 - INFO - __main__ - Global step 250 Train loss 0.43 ACC 0.53125 on epoch=83
06/17/2022 17:37:18 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=83, global_step=250
06/17/2022 17:37:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.35 on epoch=86
06/17/2022 17:37:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=89
06/17/2022 17:37:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=93
06/17/2022 17:37:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=96
06/17/2022 17:37:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=99
06/17/2022 17:37:33 - INFO - __main__ - Global step 300 Train loss 0.41 ACC 0.53125 on epoch=99
06/17/2022 17:37:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=103
06/17/2022 17:37:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=106
06/17/2022 17:37:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
06/17/2022 17:37:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=113
06/17/2022 17:37:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=116
06/17/2022 17:37:48 - INFO - __main__ - Global step 350 Train loss 0.38 ACC 0.5 on epoch=116
06/17/2022 17:37:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=119
06/17/2022 17:37:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=123
06/17/2022 17:37:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=126
06/17/2022 17:38:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=129
06/17/2022 17:38:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=133
06/17/2022 17:38:04 - INFO - __main__ - Global step 400 Train loss 0.38 ACC 0.5 on epoch=133
06/17/2022 17:38:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=136
06/17/2022 17:38:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=139
06/17/2022 17:38:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=143
06/17/2022 17:38:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=146
06/17/2022 17:38:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=149
06/17/2022 17:38:19 - INFO - __main__ - Global step 450 Train loss 0.36 ACC 0.4375 on epoch=149
06/17/2022 17:38:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=153
06/17/2022 17:38:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=156
06/17/2022 17:38:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=159
06/17/2022 17:38:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=163
06/17/2022 17:38:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=166
06/17/2022 17:38:34 - INFO - __main__ - Global step 500 Train loss 0.35 ACC 0.4375 on epoch=166
06/17/2022 17:38:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=169
06/17/2022 17:38:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=173
06/17/2022 17:38:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=176
06/17/2022 17:38:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.34 on epoch=179
06/17/2022 17:38:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=183
06/17/2022 17:38:49 - INFO - __main__ - Global step 550 Train loss 0.34 ACC 0.5 on epoch=183
06/17/2022 17:38:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.31 on epoch=186
06/17/2022 17:38:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.29 on epoch=189
06/17/2022 17:38:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=193
06/17/2022 17:39:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=196
06/17/2022 17:39:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=199
06/17/2022 17:39:05 - INFO - __main__ - Global step 600 Train loss 0.32 ACC 0.4375 on epoch=199
06/17/2022 17:39:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=203
06/17/2022 17:39:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=206
06/17/2022 17:39:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=209
06/17/2022 17:39:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.28 on epoch=213
06/17/2022 17:39:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=216
06/17/2022 17:39:20 - INFO - __main__ - Global step 650 Train loss 0.29 ACC 0.3125 on epoch=216
06/17/2022 17:39:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=219
06/17/2022 17:39:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=223
06/17/2022 17:39:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=226
06/17/2022 17:39:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=229
06/17/2022 17:39:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=233
06/17/2022 17:39:35 - INFO - __main__ - Global step 700 Train loss 0.27 ACC 0.4375 on epoch=233
06/17/2022 17:39:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=236
06/17/2022 17:39:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=239
06/17/2022 17:39:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=243
06/17/2022 17:39:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=246
06/17/2022 17:39:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=249
06/17/2022 17:39:50 - INFO - __main__ - Global step 750 Train loss 0.25 ACC 0.4375 on epoch=249
06/17/2022 17:39:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=253
06/17/2022 17:39:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.26 on epoch=256
06/17/2022 17:39:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=259
06/17/2022 17:40:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=263
06/17/2022 17:40:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=266
06/17/2022 17:40:05 - INFO - __main__ - Global step 800 Train loss 0.24 ACC 0.34375 on epoch=266
06/17/2022 17:40:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=269
06/17/2022 17:40:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=273
06/17/2022 17:40:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=276
06/17/2022 17:40:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=279
06/17/2022 17:40:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=283
06/17/2022 17:40:20 - INFO - __main__ - Global step 850 Train loss 0.20 ACC 0.375 on epoch=283
06/17/2022 17:40:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=286
06/17/2022 17:40:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.17 on epoch=289
06/17/2022 17:40:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=293
06/17/2022 17:40:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=296
06/17/2022 17:40:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=299
06/17/2022 17:40:35 - INFO - __main__ - Global step 900 Train loss 0.19 ACC 0.4375 on epoch=299
06/17/2022 17:40:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=303
06/17/2022 17:40:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=306
06/17/2022 17:40:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=309
06/17/2022 17:40:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.14 on epoch=313
06/17/2022 17:40:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=316
06/17/2022 17:40:50 - INFO - __main__ - Global step 950 Train loss 0.18 ACC 0.375 on epoch=316
06/17/2022 17:40:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=319
06/17/2022 17:40:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=323
06/17/2022 17:40:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=326
06/17/2022 17:41:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=329
06/17/2022 17:41:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.12 on epoch=333
06/17/2022 17:41:06 - INFO - __main__ - Global step 1000 Train loss 0.15 ACC 0.28125 on epoch=333
06/17/2022 17:41:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.13 on epoch=336
06/17/2022 17:41:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.18 on epoch=339
06/17/2022 17:41:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.13 on epoch=343
06/17/2022 17:41:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=346
06/17/2022 17:41:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=349
06/17/2022 17:41:21 - INFO - __main__ - Global step 1050 Train loss 0.13 ACC 0.375 on epoch=349
06/17/2022 17:41:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=353
06/17/2022 17:41:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=356
06/17/2022 17:41:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=359
06/17/2022 17:41:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=363
06/17/2022 17:41:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=366
06/17/2022 17:41:36 - INFO - __main__ - Global step 1100 Train loss 0.12 ACC 0.375 on epoch=366
06/17/2022 17:41:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.13 on epoch=369
06/17/2022 17:41:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=373
06/17/2022 17:41:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=376
06/17/2022 17:41:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=379
06/17/2022 17:41:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=383
06/17/2022 17:41:51 - INFO - __main__ - Global step 1150 Train loss 0.11 ACC 0.375 on epoch=383
06/17/2022 17:41:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=386
06/17/2022 17:41:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=389
06/17/2022 17:41:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=393
06/17/2022 17:42:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=396
06/17/2022 17:42:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=399
06/17/2022 17:42:06 - INFO - __main__ - Global step 1200 Train loss 0.10 ACC 0.3125 on epoch=399
06/17/2022 17:42:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=403
06/17/2022 17:42:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=406
06/17/2022 17:42:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=409
06/17/2022 17:42:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=413
06/17/2022 17:42:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=416
06/17/2022 17:42:21 - INFO - __main__ - Global step 1250 Train loss 0.09 ACC 0.34375 on epoch=416
06/17/2022 17:42:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=419
06/17/2022 17:42:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.10 on epoch=423
06/17/2022 17:42:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=426
06/17/2022 17:42:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=429
06/17/2022 17:42:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=433
06/17/2022 17:42:36 - INFO - __main__ - Global step 1300 Train loss 0.08 ACC 0.3125 on epoch=433
06/17/2022 17:42:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=436
06/17/2022 17:42:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=439
06/17/2022 17:42:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=443
06/17/2022 17:42:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=446
06/17/2022 17:42:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=449
06/17/2022 17:42:51 - INFO - __main__ - Global step 1350 Train loss 0.07 ACC 0.34375 on epoch=449
06/17/2022 17:42:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=453
06/17/2022 17:42:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=456
06/17/2022 17:43:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=459
06/17/2022 17:43:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=463
06/17/2022 17:43:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=466
06/17/2022 17:43:06 - INFO - __main__ - Global step 1400 Train loss 0.06 ACC 0.375 on epoch=466
06/17/2022 17:43:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=469
06/17/2022 17:43:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=473
06/17/2022 17:43:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
06/17/2022 17:43:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=479
06/17/2022 17:43:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=483
06/17/2022 17:43:22 - INFO - __main__ - Global step 1450 Train loss 0.04 ACC 0.34375 on epoch=483
06/17/2022 17:43:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=486
06/17/2022 17:43:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=489
06/17/2022 17:43:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
06/17/2022 17:43:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=496
06/17/2022 17:43:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=499
06/17/2022 17:43:37 - INFO - __main__ - Global step 1500 Train loss 0.05 ACC 0.375 on epoch=499
06/17/2022 17:43:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=503
06/17/2022 17:43:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=506
06/17/2022 17:43:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=509
06/17/2022 17:43:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=513
06/17/2022 17:43:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=516
06/17/2022 17:43:52 - INFO - __main__ - Global step 1550 Train loss 0.05 ACC 0.375 on epoch=516
06/17/2022 17:43:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
06/17/2022 17:43:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
06/17/2022 17:44:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
06/17/2022 17:44:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
06/17/2022 17:44:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=533
06/17/2022 17:44:07 - INFO - __main__ - Global step 1600 Train loss 0.04 ACC 0.40625 on epoch=533
06/17/2022 17:44:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=536
06/17/2022 17:44:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=539
06/17/2022 17:44:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
06/17/2022 17:44:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=546
06/17/2022 17:44:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=549
06/17/2022 17:44:22 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.40625 on epoch=549
06/17/2022 17:44:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=553
06/17/2022 17:44:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=556
06/17/2022 17:44:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
06/17/2022 17:44:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=563
06/17/2022 17:44:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=566
06/17/2022 17:44:37 - INFO - __main__ - Global step 1700 Train loss 0.04 ACC 0.40625 on epoch=566
06/17/2022 17:44:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=569
06/17/2022 17:44:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
06/17/2022 17:44:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=576
06/17/2022 17:44:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=579
06/17/2022 17:44:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
06/17/2022 17:44:52 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.40625 on epoch=583
06/17/2022 17:44:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
06/17/2022 17:44:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=589
06/17/2022 17:45:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=593
06/17/2022 17:45:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
06/17/2022 17:45:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=599
06/17/2022 17:45:08 - INFO - __main__ - Global step 1800 Train loss 0.04 ACC 0.40625 on epoch=599
06/17/2022 17:45:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
06/17/2022 17:45:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=606
06/17/2022 17:45:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=609
06/17/2022 17:45:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
06/17/2022 17:45:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=616
06/17/2022 17:45:23 - INFO - __main__ - Global step 1850 Train loss 0.03 ACC 0.40625 on epoch=616
06/17/2022 17:45:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=619
06/17/2022 17:45:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
06/17/2022 17:45:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/17/2022 17:45:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
06/17/2022 17:45:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/17/2022 17:45:38 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.40625 on epoch=633
06/17/2022 17:45:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
06/17/2022 17:45:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=639
06/17/2022 17:45:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/17/2022 17:45:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
06/17/2022 17:45:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
06/17/2022 17:45:53 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.40625 on epoch=649
06/17/2022 17:45:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
06/17/2022 17:45:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=656
06/17/2022 17:46:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
06/17/2022 17:46:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=663
06/17/2022 17:46:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=666
06/17/2022 17:46:08 - INFO - __main__ - Global step 2000 Train loss 0.04 ACC 0.40625 on epoch=666
06/17/2022 17:46:11 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
06/17/2022 17:46:14 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=673
06/17/2022 17:46:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
06/17/2022 17:46:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
06/17/2022 17:46:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/17/2022 17:46:23 - INFO - __main__ - Global step 2050 Train loss 0.02 ACC 0.40625 on epoch=683
06/17/2022 17:46:26 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
06/17/2022 17:46:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=689
06/17/2022 17:46:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/17/2022 17:46:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
06/17/2022 17:46:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/17/2022 17:46:38 - INFO - __main__ - Global step 2100 Train loss 0.02 ACC 0.40625 on epoch=699
06/17/2022 17:46:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
06/17/2022 17:46:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=706
06/17/2022 17:46:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/17/2022 17:46:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
06/17/2022 17:46:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/17/2022 17:46:54 - INFO - __main__ - Global step 2150 Train loss 0.02 ACC 0.46875 on epoch=716
06/17/2022 17:46:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
06/17/2022 17:46:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
06/17/2022 17:47:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
06/17/2022 17:47:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=729
06/17/2022 17:47:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
06/17/2022 17:47:09 - INFO - __main__ - Global step 2200 Train loss 0.02 ACC 0.46875 on epoch=733
06/17/2022 17:47:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/17/2022 17:47:14 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
06/17/2022 17:47:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/17/2022 17:47:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/17/2022 17:47:23 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
06/17/2022 17:47:24 - INFO - __main__ - Global step 2250 Train loss 0.01 ACC 0.46875 on epoch=749
06/17/2022 17:47:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
06/17/2022 17:47:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/17/2022 17:47:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=759
06/17/2022 17:47:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
06/17/2022 17:47:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/17/2022 17:47:39 - INFO - __main__ - Global step 2300 Train loss 0.02 ACC 0.4375 on epoch=766
06/17/2022 17:47:42 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
06/17/2022 17:47:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=773
06/17/2022 17:47:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
06/17/2022 17:47:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/17/2022 17:47:53 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=783
06/17/2022 17:47:54 - INFO - __main__ - Global step 2350 Train loss 0.02 ACC 0.46875 on epoch=783
06/17/2022 17:47:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/17/2022 17:48:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/17/2022 17:48:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
06/17/2022 17:48:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
06/17/2022 17:48:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/17/2022 17:48:09 - INFO - __main__ - Global step 2400 Train loss 0.01 ACC 0.46875 on epoch=799
06/17/2022 17:48:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
06/17/2022 17:48:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
06/17/2022 17:48:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=809
06/17/2022 17:48:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/17/2022 17:48:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/17/2022 17:48:25 - INFO - __main__ - Global step 2450 Train loss 0.02 ACC 0.46875 on epoch=816
06/17/2022 17:48:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/17/2022 17:48:30 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/17/2022 17:48:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=826
06/17/2022 17:48:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/17/2022 17:48:39 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=833
06/17/2022 17:48:40 - INFO - __main__ - Global step 2500 Train loss 0.02 ACC 0.46875 on epoch=833
06/17/2022 17:48:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
06/17/2022 17:48:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/17/2022 17:48:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/17/2022 17:48:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=846
06/17/2022 17:48:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/17/2022 17:48:55 - INFO - __main__ - Global step 2550 Train loss 0.01 ACC 0.4375 on epoch=849
06/17/2022 17:48:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=853
06/17/2022 17:49:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
06/17/2022 17:49:04 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/17/2022 17:49:06 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
06/17/2022 17:49:09 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=866
06/17/2022 17:49:10 - INFO - __main__ - Global step 2600 Train loss 0.02 ACC 0.46875 on epoch=866
06/17/2022 17:49:13 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
06/17/2022 17:49:16 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/17/2022 17:49:19 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
06/17/2022 17:49:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=879
06/17/2022 17:49:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
06/17/2022 17:49:26 - INFO - __main__ - Global step 2650 Train loss 0.01 ACC 0.46875 on epoch=883
06/17/2022 17:49:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 17:49:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
06/17/2022 17:49:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
06/17/2022 17:49:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
06/17/2022 17:49:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
06/17/2022 17:49:41 - INFO - __main__ - Global step 2700 Train loss 0.01 ACC 0.375 on epoch=899
06/17/2022 17:49:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
06/17/2022 17:49:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=906
06/17/2022 17:49:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 17:49:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 17:49:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/17/2022 17:49:56 - INFO - __main__ - Global step 2750 Train loss 0.01 ACC 0.4375 on epoch=916
06/17/2022 17:49:59 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
06/17/2022 17:50:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 17:50:05 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/17/2022 17:50:07 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
06/17/2022 17:50:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
06/17/2022 17:50:11 - INFO - __main__ - Global step 2800 Train loss 0.01 ACC 0.40625 on epoch=933
06/17/2022 17:50:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
06/17/2022 17:50:17 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/17/2022 17:50:20 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
06/17/2022 17:50:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 17:50:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 17:50:26 - INFO - __main__ - Global step 2850 Train loss 0.01 ACC 0.40625 on epoch=949
06/17/2022 17:50:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/17/2022 17:50:32 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
06/17/2022 17:50:35 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=959
06/17/2022 17:50:38 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 17:50:41 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 17:50:42 - INFO - __main__ - Global step 2900 Train loss 0.01 ACC 0.4375 on epoch=966
06/17/2022 17:50:45 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/17/2022 17:50:47 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 17:50:50 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/17/2022 17:50:53 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 17:50:56 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 17:50:57 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.4375 on epoch=983
06/17/2022 17:51:00 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 17:51:03 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
06/17/2022 17:51:05 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=993
06/17/2022 17:51:08 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
06/17/2022 17:51:11 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 17:51:12 - INFO - __main__ - Global step 3000 Train loss 0.01 ACC 0.4375 on epoch=999
06/17/2022 17:51:12 - INFO - __main__ - save last model!
06/17/2022 17:51:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 17:51:12 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 17:51:12 - INFO - __main__ - Printing 3 examples
06/17/2022 17:51:12 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 17:51:12 - INFO - __main__ - ['contradiction']
06/17/2022 17:51:12 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 17:51:12 - INFO - __main__ - ['neutral']
06/17/2022 17:51:12 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 17:51:12 - INFO - __main__ - ['entailment']
06/17/2022 17:51:12 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:51:12 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:51:12 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 17:51:13 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 17:51:13 - INFO - __main__ - Printing 3 examples
06/17/2022 17:51:13 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/17/2022 17:51:13 - INFO - __main__ - ['contradiction']
06/17/2022 17:51:13 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/17/2022 17:51:13 - INFO - __main__ - ['contradiction']
06/17/2022 17:51:13 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/17/2022 17:51:13 - INFO - __main__ - ['contradiction']
06/17/2022 17:51:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 17:51:13 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:51:13 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 17:51:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 17:51:13 - INFO - __main__ - Printing 3 examples
06/17/2022 17:51:13 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
06/17/2022 17:51:13 - INFO - __main__ - ['contradiction']
06/17/2022 17:51:13 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 17:51:13 - INFO - __main__ - ['contradiction']
06/17/2022 17:51:13 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 17:51:13 - INFO - __main__ - ['contradiction']
06/17/2022 17:51:13 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:51:13 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:51:13 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 17:51:14 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_100_0.2_8_predictions.txt
06/17/2022 17:51:14 - INFO - __main__ - ACC on test data: 0.5000
06/17/2022 17:51:15 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.2, bsz=8, dev_performance=0.53125, test_performance=0.5
06/17/2022 17:51:15 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.5, bsz=8 ...
06/17/2022 17:51:16 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 17:51:16 - INFO - __main__ - Printing 3 examples
06/17/2022 17:51:16 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/17/2022 17:51:16 - INFO - __main__ - ['contradiction']
06/17/2022 17:51:16 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/17/2022 17:51:16 - INFO - __main__ - ['contradiction']
06/17/2022 17:51:16 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/17/2022 17:51:16 - INFO - __main__ - ['contradiction']
06/17/2022 17:51:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 17:51:16 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:51:16 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 17:51:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 17:51:16 - INFO - __main__ - Printing 3 examples
06/17/2022 17:51:16 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
06/17/2022 17:51:16 - INFO - __main__ - ['contradiction']
06/17/2022 17:51:16 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 17:51:16 - INFO - __main__ - ['contradiction']
06/17/2022 17:51:16 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 17:51:16 - INFO - __main__ - ['contradiction']
06/17/2022 17:51:16 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:51:16 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:51:16 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 17:51:28 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:51:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:51:29 - INFO - __main__ - Starting training!
06/17/2022 17:51:33 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:51:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:51:33 - INFO - __main__ - Starting training!
06/17/2022 17:51:37 - INFO - __main__ - Step 10 Global step 10 Train loss 1.01 on epoch=3
06/17/2022 17:51:40 - INFO - __main__ - Step 20 Global step 20 Train loss 0.46 on epoch=6
06/17/2022 17:51:42 - INFO - __main__ - Step 30 Global step 30 Train loss 0.41 on epoch=9
06/17/2022 17:51:45 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=13
06/17/2022 17:51:48 - INFO - __main__ - Step 50 Global step 50 Train loss 0.43 on epoch=16
06/17/2022 17:51:49 - INFO - __main__ - Global step 50 Train loss 0.56 ACC 0.5 on epoch=16
06/17/2022 17:51:49 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 17:51:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=19
06/17/2022 17:51:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.41 on epoch=23
06/17/2022 17:51:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=26
06/17/2022 17:52:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=29
06/17/2022 17:52:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=33
06/17/2022 17:52:03 - INFO - __main__ - Global step 100 Train loss 0.44 ACC 0.4375 on epoch=33
06/17/2022 17:52:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=36
06/17/2022 17:52:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.37 on epoch=39
06/17/2022 17:52:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.38 on epoch=43
06/17/2022 17:52:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.31 on epoch=46
06/17/2022 17:52:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.39 on epoch=49
06/17/2022 17:52:18 - INFO - __main__ - Global step 150 Train loss 0.38 ACC 0.46875 on epoch=49
06/17/2022 17:52:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.33 on epoch=53
06/17/2022 17:52:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=56
06/17/2022 17:52:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.35 on epoch=59
06/17/2022 17:52:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=63
06/17/2022 17:52:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=66
06/17/2022 17:52:33 - INFO - __main__ - Global step 200 Train loss 0.30 ACC 0.46875 on epoch=66
06/17/2022 17:52:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.28 on epoch=69
06/17/2022 17:52:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.31 on epoch=73
06/17/2022 17:52:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=76
06/17/2022 17:52:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=79
06/17/2022 17:52:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=83
06/17/2022 17:52:47 - INFO - __main__ - Global step 250 Train loss 0.28 ACC 0.40625 on epoch=83
06/17/2022 17:52:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=86
06/17/2022 17:52:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=89
06/17/2022 17:52:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=93
06/17/2022 17:52:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=96
06/17/2022 17:53:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.20 on epoch=99
06/17/2022 17:53:02 - INFO - __main__ - Global step 300 Train loss 0.24 ACC 0.4375 on epoch=99
06/17/2022 17:53:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.19 on epoch=103
06/17/2022 17:53:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.19 on epoch=106
06/17/2022 17:53:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.17 on epoch=109
06/17/2022 17:53:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=113
06/17/2022 17:53:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.19 on epoch=116
06/17/2022 17:53:17 - INFO - __main__ - Global step 350 Train loss 0.19 ACC 0.40625 on epoch=116
06/17/2022 17:53:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=119
06/17/2022 17:53:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=123
06/17/2022 17:53:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.19 on epoch=126
06/17/2022 17:53:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.12 on epoch=129
06/17/2022 17:53:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=133
06/17/2022 17:53:31 - INFO - __main__ - Global step 400 Train loss 0.17 ACC 0.3125 on epoch=133
06/17/2022 17:53:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.19 on epoch=136
06/17/2022 17:53:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.13 on epoch=139
06/17/2022 17:53:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=143
06/17/2022 17:53:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.10 on epoch=146
06/17/2022 17:53:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.11 on epoch=149
06/17/2022 17:53:46 - INFO - __main__ - Global step 450 Train loss 0.13 ACC 0.46875 on epoch=149
06/17/2022 17:53:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=153
06/17/2022 17:53:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.09 on epoch=156
06/17/2022 17:53:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=159
06/17/2022 17:53:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.10 on epoch=163
06/17/2022 17:54:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=166
06/17/2022 17:54:01 - INFO - __main__ - Global step 500 Train loss 0.11 ACC 0.46875 on epoch=166
06/17/2022 17:54:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.06 on epoch=169
06/17/2022 17:54:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=173
06/17/2022 17:54:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.09 on epoch=176
06/17/2022 17:54:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=179
06/17/2022 17:54:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=183
06/17/2022 17:54:15 - INFO - __main__ - Global step 550 Train loss 0.07 ACC 0.46875 on epoch=183
06/17/2022 17:54:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=186
06/17/2022 17:54:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=189
06/17/2022 17:54:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=193
06/17/2022 17:54:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=196
06/17/2022 17:54:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=199
06/17/2022 17:54:30 - INFO - __main__ - Global step 600 Train loss 0.06 ACC 0.46875 on epoch=199
06/17/2022 17:54:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.09 on epoch=203
06/17/2022 17:54:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=206
06/17/2022 17:54:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.07 on epoch=209
06/17/2022 17:54:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.05 on epoch=213
06/17/2022 17:54:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=216
06/17/2022 17:54:45 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.5 on epoch=216
06/17/2022 17:54:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=219
06/17/2022 17:54:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=223
06/17/2022 17:54:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=226
06/17/2022 17:54:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=229
06/17/2022 17:54:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=233
06/17/2022 17:55:00 - INFO - __main__ - Global step 700 Train loss 0.05 ACC 0.46875 on epoch=233
06/17/2022 17:55:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=236
06/17/2022 17:55:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=239
06/17/2022 17:55:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=243
06/17/2022 17:55:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=246
06/17/2022 17:55:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=249
06/17/2022 17:55:14 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.53125 on epoch=249
06/17/2022 17:55:14 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=249, global_step=750
06/17/2022 17:55:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=253
06/17/2022 17:55:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.05 on epoch=256
06/17/2022 17:55:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=259
06/17/2022 17:55:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=263
06/17/2022 17:55:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=266
06/17/2022 17:55:29 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.59375 on epoch=266
06/17/2022 17:55:29 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=266, global_step=800
06/17/2022 17:55:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=269
06/17/2022 17:55:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=273
06/17/2022 17:55:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=276
06/17/2022 17:55:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=279
06/17/2022 17:55:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=283
06/17/2022 17:55:44 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.53125 on epoch=283
06/17/2022 17:55:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=286
06/17/2022 17:55:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=289
06/17/2022 17:55:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=293
06/17/2022 17:55:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=296
06/17/2022 17:55:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=299
06/17/2022 17:55:59 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.5625 on epoch=299
06/17/2022 17:56:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=303
06/17/2022 17:56:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=306
06/17/2022 17:56:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=309
06/17/2022 17:56:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=313
06/17/2022 17:56:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=316
06/17/2022 17:56:13 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.5 on epoch=316
06/17/2022 17:56:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=319
06/17/2022 17:56:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=323
06/17/2022 17:56:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=326
06/17/2022 17:56:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=329
06/17/2022 17:56:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=333
06/17/2022 17:56:28 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.53125 on epoch=333
06/17/2022 17:56:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=336
06/17/2022 17:56:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=339
06/17/2022 17:56:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=343
06/17/2022 17:56:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=346
06/17/2022 17:56:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=349
06/17/2022 17:56:43 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.5 on epoch=349
06/17/2022 17:56:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=353
06/17/2022 17:56:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=356
06/17/2022 17:56:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=359
06/17/2022 17:56:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=363
06/17/2022 17:56:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
06/17/2022 17:56:57 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.53125 on epoch=366
06/17/2022 17:57:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=369
06/17/2022 17:57:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=373
06/17/2022 17:57:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=376
06/17/2022 17:57:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
06/17/2022 17:57:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
06/17/2022 17:57:12 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.59375 on epoch=383
06/17/2022 17:57:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
06/17/2022 17:57:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
06/17/2022 17:57:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
06/17/2022 17:57:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
06/17/2022 17:57:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
06/17/2022 17:57:27 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.5625 on epoch=399
06/17/2022 17:57:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
06/17/2022 17:57:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=406
06/17/2022 17:57:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
06/17/2022 17:57:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=413
06/17/2022 17:57:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
06/17/2022 17:57:42 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.46875 on epoch=416
06/17/2022 17:57:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
06/17/2022 17:57:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=423
06/17/2022 17:57:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
06/17/2022 17:57:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
06/17/2022 17:57:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
06/17/2022 17:57:57 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.5 on epoch=433
06/17/2022 17:57:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
06/17/2022 17:58:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/17/2022 17:58:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=443
06/17/2022 17:58:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
06/17/2022 17:58:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
06/17/2022 17:58:11 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.53125 on epoch=449
06/17/2022 17:58:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
06/17/2022 17:58:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=456
06/17/2022 17:58:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=459
06/17/2022 17:58:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=463
06/17/2022 17:58:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
06/17/2022 17:58:26 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.53125 on epoch=466
06/17/2022 17:58:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=469
06/17/2022 17:58:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
06/17/2022 17:58:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
06/17/2022 17:58:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=479
06/17/2022 17:58:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
06/17/2022 17:58:41 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.53125 on epoch=483
06/17/2022 17:58:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
06/17/2022 17:58:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
06/17/2022 17:58:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
06/17/2022 17:58:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
06/17/2022 17:58:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
06/17/2022 17:58:56 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.59375 on epoch=499
06/17/2022 17:58:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
06/17/2022 17:59:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
06/17/2022 17:59:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
06/17/2022 17:59:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
06/17/2022 17:59:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=516
06/17/2022 17:59:11 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.59375 on epoch=516
06/17/2022 17:59:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/17/2022 17:59:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
06/17/2022 17:59:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
06/17/2022 17:59:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
06/17/2022 17:59:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
06/17/2022 17:59:26 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.59375 on epoch=533
06/17/2022 17:59:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=536
06/17/2022 17:59:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=539
06/17/2022 17:59:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
06/17/2022 17:59:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
06/17/2022 17:59:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/17/2022 17:59:41 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.53125 on epoch=549
06/17/2022 17:59:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
06/17/2022 17:59:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
06/17/2022 17:59:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/17/2022 17:59:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
06/17/2022 17:59:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
06/17/2022 17:59:56 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.5625 on epoch=566
06/17/2022 17:59:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
06/17/2022 18:00:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
06/17/2022 18:00:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
06/17/2022 18:00:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/17/2022 18:00:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/17/2022 18:00:10 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.5625 on epoch=583
06/17/2022 18:00:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/17/2022 18:00:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/17/2022 18:00:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/17/2022 18:00:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
06/17/2022 18:00:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=599
06/17/2022 18:00:25 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.5625 on epoch=599
06/17/2022 18:00:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/17/2022 18:00:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
06/17/2022 18:00:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/17/2022 18:00:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/17/2022 18:00:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
06/17/2022 18:00:40 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.5625 on epoch=616
06/17/2022 18:00:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/17/2022 18:00:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
06/17/2022 18:00:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/17/2022 18:00:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/17/2022 18:00:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/17/2022 18:00:54 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.59375 on epoch=633
06/17/2022 18:00:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/17/2022 18:01:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
06/17/2022 18:01:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/17/2022 18:01:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/17/2022 18:01:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/17/2022 18:01:09 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.625 on epoch=649
06/17/2022 18:01:09 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=649, global_step=1950
06/17/2022 18:01:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/17/2022 18:01:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/17/2022 18:01:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/17/2022 18:01:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/17/2022 18:01:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
06/17/2022 18:01:24 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.59375 on epoch=666
06/17/2022 18:01:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/17/2022 18:01:29 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/17/2022 18:01:32 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/17/2022 18:01:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/17/2022 18:01:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/17/2022 18:01:39 - INFO - __main__ - Global step 2050 Train loss 0.00 ACC 0.65625 on epoch=683
06/17/2022 18:01:39 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=683, global_step=2050
06/17/2022 18:01:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/17/2022 18:01:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
06/17/2022 18:01:47 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/17/2022 18:01:50 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/17/2022 18:01:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/17/2022 18:01:53 - INFO - __main__ - Global step 2100 Train loss 0.00 ACC 0.65625 on epoch=699
06/17/2022 18:01:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/17/2022 18:01:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/17/2022 18:02:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/17/2022 18:02:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/17/2022 18:02:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/17/2022 18:02:08 - INFO - __main__ - Global step 2150 Train loss 0.00 ACC 0.625 on epoch=716
06/17/2022 18:02:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/17/2022 18:02:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 18:02:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/17/2022 18:02:19 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/17/2022 18:02:22 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/17/2022 18:02:23 - INFO - __main__ - Global step 2200 Train loss 0.00 ACC 0.65625 on epoch=733
06/17/2022 18:02:26 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/17/2022 18:02:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
06/17/2022 18:02:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/17/2022 18:02:34 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/17/2022 18:02:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=749
06/17/2022 18:02:38 - INFO - __main__ - Global step 2250 Train loss 0.01 ACC 0.625 on epoch=749
06/17/2022 18:02:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/17/2022 18:02:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/17/2022 18:02:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/17/2022 18:02:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/17/2022 18:02:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/17/2022 18:02:52 - INFO - __main__ - Global step 2300 Train loss 0.00 ACC 0.65625 on epoch=766
06/17/2022 18:02:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/17/2022 18:02:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/17/2022 18:03:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/17/2022 18:03:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/17/2022 18:03:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/17/2022 18:03:07 - INFO - __main__ - Global step 2350 Train loss 0.00 ACC 0.65625 on epoch=783
06/17/2022 18:03:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/17/2022 18:03:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/17/2022 18:03:15 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 18:03:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 18:03:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/17/2022 18:03:22 - INFO - __main__ - Global step 2400 Train loss 0.00 ACC 0.65625 on epoch=799
06/17/2022 18:03:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/17/2022 18:03:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 18:03:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/17/2022 18:03:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/17/2022 18:03:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/17/2022 18:03:37 - INFO - __main__ - Global step 2450 Train loss 0.00 ACC 0.6875 on epoch=816
06/17/2022 18:03:37 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=816, global_step=2450
06/17/2022 18:03:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/17/2022 18:03:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/17/2022 18:03:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/17/2022 18:03:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/17/2022 18:03:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 18:03:51 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.6875 on epoch=833
06/17/2022 18:03:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 18:03:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/17/2022 18:04:00 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/17/2022 18:04:02 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 18:04:05 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/17/2022 18:04:06 - INFO - __main__ - Global step 2550 Train loss 0.00 ACC 0.6875 on epoch=849
06/17/2022 18:04:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/17/2022 18:04:12 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 18:04:14 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/17/2022 18:04:17 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 18:04:20 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 18:04:21 - INFO - __main__ - Global step 2600 Train loss 0.00 ACC 0.65625 on epoch=866
06/17/2022 18:04:23 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/17/2022 18:04:26 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/17/2022 18:04:29 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/17/2022 18:04:32 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=879
06/17/2022 18:04:34 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 18:04:35 - INFO - __main__ - Global step 2650 Train loss 0.00 ACC 0.6875 on epoch=883
06/17/2022 18:04:38 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 18:04:41 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 18:04:44 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 18:04:46 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 18:04:49 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 18:04:50 - INFO - __main__ - Global step 2700 Train loss 0.00 ACC 0.6875 on epoch=899
06/17/2022 18:04:53 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 18:04:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 18:04:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 18:05:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
06/17/2022 18:05:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/17/2022 18:05:04 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.6875 on epoch=916
06/17/2022 18:05:07 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 18:05:10 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
06/17/2022 18:05:13 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/17/2022 18:05:16 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/17/2022 18:05:18 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 18:05:19 - INFO - __main__ - Global step 2800 Train loss 0.00 ACC 0.65625 on epoch=933
06/17/2022 18:05:22 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 18:05:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/17/2022 18:05:27 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 18:05:30 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 18:05:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 18:05:34 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.71875 on epoch=949
06/17/2022 18:05:34 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=949, global_step=2850
06/17/2022 18:05:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/17/2022 18:05:39 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/17/2022 18:05:42 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 18:05:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 18:05:48 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 18:05:49 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.6875 on epoch=966
06/17/2022 18:05:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=969
06/17/2022 18:05:54 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 18:05:57 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/17/2022 18:06:00 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 18:06:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 18:06:03 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.6875 on epoch=983
06/17/2022 18:06:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 18:06:09 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/17/2022 18:06:12 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
06/17/2022 18:06:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 18:06:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 18:06:18 - INFO - __main__ - Global step 3000 Train loss 0.00 ACC 0.65625 on epoch=999
06/17/2022 18:06:18 - INFO - __main__ - save last model!
06/17/2022 18:06:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 18:06:18 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 18:06:18 - INFO - __main__ - Printing 3 examples
06/17/2022 18:06:18 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 18:06:18 - INFO - __main__ - ['contradiction']
06/17/2022 18:06:18 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 18:06:18 - INFO - __main__ - ['neutral']
06/17/2022 18:06:18 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 18:06:18 - INFO - __main__ - ['entailment']
06/17/2022 18:06:18 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:06:18 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:06:18 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 18:06:18 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 18:06:18 - INFO - __main__ - Printing 3 examples
06/17/2022 18:06:18 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/17/2022 18:06:18 - INFO - __main__ - ['contradiction']
06/17/2022 18:06:18 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/17/2022 18:06:18 - INFO - __main__ - ['contradiction']
06/17/2022 18:06:18 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/17/2022 18:06:18 - INFO - __main__ - ['contradiction']
06/17/2022 18:06:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 18:06:18 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:06:18 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 18:06:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 18:06:18 - INFO - __main__ - Printing 3 examples
06/17/2022 18:06:18 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
06/17/2022 18:06:18 - INFO - __main__ - ['contradiction']
06/17/2022 18:06:18 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 18:06:18 - INFO - __main__ - ['contradiction']
06/17/2022 18:06:18 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 18:06:18 - INFO - __main__ - ['contradiction']
06/17/2022 18:06:18 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:06:19 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:06:19 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 18:06:20 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_13_0.5_8_predictions.txt
06/17/2022 18:06:20 - INFO - __main__ - ACC on test data: 0.6964
06/17/2022 18:06:21 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.5, bsz=8, dev_performance=0.71875, test_performance=0.6964285714285714
06/17/2022 18:06:21 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.4, bsz=8 ...
06/17/2022 18:06:21 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 18:06:21 - INFO - __main__ - Printing 3 examples
06/17/2022 18:06:21 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/17/2022 18:06:21 - INFO - __main__ - ['contradiction']
06/17/2022 18:06:21 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/17/2022 18:06:21 - INFO - __main__ - ['contradiction']
06/17/2022 18:06:21 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/17/2022 18:06:21 - INFO - __main__ - ['contradiction']
06/17/2022 18:06:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 18:06:21 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:06:22 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 18:06:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 18:06:22 - INFO - __main__ - Printing 3 examples
06/17/2022 18:06:22 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
06/17/2022 18:06:22 - INFO - __main__ - ['contradiction']
06/17/2022 18:06:22 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 18:06:22 - INFO - __main__ - ['contradiction']
06/17/2022 18:06:22 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 18:06:22 - INFO - __main__ - ['contradiction']
06/17/2022 18:06:22 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:06:22 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:06:22 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 18:06:37 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 18:06:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 18:06:38 - INFO - __main__ - Starting training!
06/17/2022 18:06:38 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 18:06:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 18:06:39 - INFO - __main__ - Starting training!
06/17/2022 18:06:43 - INFO - __main__ - Step 10 Global step 10 Train loss 1.44 on epoch=3
06/17/2022 18:06:45 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=6
06/17/2022 18:06:48 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=9
06/17/2022 18:06:51 - INFO - __main__ - Step 40 Global step 40 Train loss 0.46 on epoch=13
06/17/2022 18:06:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=16
06/17/2022 18:06:55 - INFO - __main__ - Global step 50 Train loss 0.67 ACC 0.5 on epoch=16
06/17/2022 18:06:55 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 18:06:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=19
06/17/2022 18:07:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=23
06/17/2022 18:07:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
06/17/2022 18:07:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=29
06/17/2022 18:07:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=33
06/17/2022 18:07:09 - INFO - __main__ - Global step 100 Train loss 0.45 ACC 0.5 on epoch=33
06/17/2022 18:07:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=36
06/17/2022 18:07:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=39
06/17/2022 18:07:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.38 on epoch=43
06/17/2022 18:07:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=46
06/17/2022 18:07:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
06/17/2022 18:07:24 - INFO - __main__ - Global step 150 Train loss 0.42 ACC 0.46875 on epoch=49
06/17/2022 18:07:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.34 on epoch=53
06/17/2022 18:07:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=56
06/17/2022 18:07:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=59
06/17/2022 18:07:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=63
06/17/2022 18:07:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=66
06/17/2022 18:07:39 - INFO - __main__ - Global step 200 Train loss 0.37 ACC 0.5 on epoch=66
06/17/2022 18:07:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=69
06/17/2022 18:07:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=73
06/17/2022 18:07:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.30 on epoch=76
06/17/2022 18:07:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=79
06/17/2022 18:07:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=83
06/17/2022 18:07:54 - INFO - __main__ - Global step 250 Train loss 0.32 ACC 0.53125 on epoch=83
06/17/2022 18:07:54 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=83, global_step=250
06/17/2022 18:07:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.29 on epoch=86
06/17/2022 18:07:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.34 on epoch=89
06/17/2022 18:08:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=93
06/17/2022 18:08:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=96
06/17/2022 18:08:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=99
06/17/2022 18:08:09 - INFO - __main__ - Global step 300 Train loss 0.29 ACC 0.46875 on epoch=99
06/17/2022 18:08:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=103
06/17/2022 18:08:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.20 on epoch=106
06/17/2022 18:08:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=109
06/17/2022 18:08:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=113
06/17/2022 18:08:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=116
06/17/2022 18:08:24 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.3125 on epoch=116
06/17/2022 18:08:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=119
06/17/2022 18:08:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=123
06/17/2022 18:08:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=126
06/17/2022 18:08:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=129
06/17/2022 18:08:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=133
06/17/2022 18:08:39 - INFO - __main__ - Global step 400 Train loss 0.22 ACC 0.28125 on epoch=133
06/17/2022 18:08:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.19 on epoch=136
06/17/2022 18:08:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=139
06/17/2022 18:08:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.20 on epoch=143
06/17/2022 18:08:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=146
06/17/2022 18:08:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.20 on epoch=149
06/17/2022 18:08:54 - INFO - __main__ - Global step 450 Train loss 0.20 ACC 0.28125 on epoch=149
06/17/2022 18:08:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.18 on epoch=153
06/17/2022 18:08:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=156
06/17/2022 18:09:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=159
06/17/2022 18:09:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.14 on epoch=163
06/17/2022 18:09:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=166
06/17/2022 18:09:08 - INFO - __main__ - Global step 500 Train loss 0.17 ACC 0.25 on epoch=166
06/17/2022 18:09:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.15 on epoch=169
06/17/2022 18:09:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.16 on epoch=173
06/17/2022 18:09:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=176
06/17/2022 18:09:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=179
06/17/2022 18:09:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.15 on epoch=183
06/17/2022 18:09:23 - INFO - __main__ - Global step 550 Train loss 0.15 ACC 0.40625 on epoch=183
06/17/2022 18:09:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.14 on epoch=186
06/17/2022 18:09:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=189
06/17/2022 18:09:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=193
06/17/2022 18:09:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=196
06/17/2022 18:09:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.09 on epoch=199
06/17/2022 18:09:38 - INFO - __main__ - Global step 600 Train loss 0.12 ACC 0.40625 on epoch=199
06/17/2022 18:09:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=203
06/17/2022 18:09:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=206
06/17/2022 18:09:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=209
06/17/2022 18:09:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=213
06/17/2022 18:09:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=216
06/17/2022 18:09:53 - INFO - __main__ - Global step 650 Train loss 0.10 ACC 0.4375 on epoch=216
06/17/2022 18:09:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=219
06/17/2022 18:09:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=223
06/17/2022 18:10:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=226
06/17/2022 18:10:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=229
06/17/2022 18:10:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=233
06/17/2022 18:10:08 - INFO - __main__ - Global step 700 Train loss 0.07 ACC 0.28125 on epoch=233
06/17/2022 18:10:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=236
06/17/2022 18:10:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=239
06/17/2022 18:10:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=243
06/17/2022 18:10:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=246
06/17/2022 18:10:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=249
06/17/2022 18:10:23 - INFO - __main__ - Global step 750 Train loss 0.06 ACC 0.5 on epoch=249
06/17/2022 18:10:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.06 on epoch=253
06/17/2022 18:10:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=256
06/17/2022 18:10:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=259
06/17/2022 18:10:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=263
06/17/2022 18:10:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=266
06/17/2022 18:10:38 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.46875 on epoch=266
06/17/2022 18:10:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=269
06/17/2022 18:10:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=273
06/17/2022 18:10:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=276
06/17/2022 18:10:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=279
06/17/2022 18:10:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=283
06/17/2022 18:10:53 - INFO - __main__ - Global step 850 Train loss 0.05 ACC 0.53125 on epoch=283
06/17/2022 18:10:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=286
06/17/2022 18:10:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=289
06/17/2022 18:11:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=293
06/17/2022 18:11:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=296
06/17/2022 18:11:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
06/17/2022 18:11:08 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.4375 on epoch=299
06/17/2022 18:11:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=303
06/17/2022 18:11:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=306
06/17/2022 18:11:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=309
06/17/2022 18:11:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=313
06/17/2022 18:11:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=316
06/17/2022 18:11:23 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.40625 on epoch=316
06/17/2022 18:11:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=319
06/17/2022 18:11:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=323
06/17/2022 18:11:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=326
06/17/2022 18:11:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=329
06/17/2022 18:11:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=333
06/17/2022 18:11:38 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.53125 on epoch=333
06/17/2022 18:11:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=336
06/17/2022 18:11:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=339
06/17/2022 18:11:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=343
06/17/2022 18:11:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=346
06/17/2022 18:11:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=349
06/17/2022 18:11:53 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.5 on epoch=349
06/17/2022 18:11:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=353
06/17/2022 18:11:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=356
06/17/2022 18:12:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=359
06/17/2022 18:12:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=363
06/17/2022 18:12:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=366
06/17/2022 18:12:08 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.40625 on epoch=366
06/17/2022 18:12:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
06/17/2022 18:12:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
06/17/2022 18:12:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=376
06/17/2022 18:12:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
06/17/2022 18:12:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
06/17/2022 18:12:23 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.5 on epoch=383
06/17/2022 18:12:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=386
06/17/2022 18:12:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
06/17/2022 18:12:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=393
06/17/2022 18:12:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=396
06/17/2022 18:12:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
06/17/2022 18:12:38 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.59375 on epoch=399
06/17/2022 18:12:38 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=399, global_step=1200
06/17/2022 18:12:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
06/17/2022 18:12:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
06/17/2022 18:12:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=409
06/17/2022 18:12:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
06/17/2022 18:12:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
06/17/2022 18:12:53 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.53125 on epoch=416
06/17/2022 18:12:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
06/17/2022 18:12:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
06/17/2022 18:13:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
06/17/2022 18:13:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=429
06/17/2022 18:13:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
06/17/2022 18:13:08 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.5625 on epoch=433
06/17/2022 18:13:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
06/17/2022 18:13:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/17/2022 18:13:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
06/17/2022 18:13:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
06/17/2022 18:13:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
06/17/2022 18:13:23 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.46875 on epoch=449
06/17/2022 18:13:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
06/17/2022 18:13:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
06/17/2022 18:13:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
06/17/2022 18:13:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=463
06/17/2022 18:13:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
06/17/2022 18:13:38 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.53125 on epoch=466
06/17/2022 18:13:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
06/17/2022 18:13:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
06/17/2022 18:13:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
06/17/2022 18:13:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
06/17/2022 18:13:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
06/17/2022 18:13:53 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.4375 on epoch=483
06/17/2022 18:13:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
06/17/2022 18:13:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
06/17/2022 18:14:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
06/17/2022 18:14:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
06/17/2022 18:14:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
06/17/2022 18:14:08 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.53125 on epoch=499
06/17/2022 18:14:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
06/17/2022 18:14:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
06/17/2022 18:14:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
06/17/2022 18:14:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
06/17/2022 18:14:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
06/17/2022 18:14:23 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.53125 on epoch=516
06/17/2022 18:14:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/17/2022 18:14:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
06/17/2022 18:14:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
06/17/2022 18:14:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
06/17/2022 18:14:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=533
06/17/2022 18:14:38 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.5625 on epoch=533
06/17/2022 18:14:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
06/17/2022 18:14:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
06/17/2022 18:14:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
06/17/2022 18:14:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
06/17/2022 18:14:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/17/2022 18:14:53 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.5 on epoch=549
06/17/2022 18:14:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
06/17/2022 18:14:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
06/17/2022 18:15:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
06/17/2022 18:15:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
06/17/2022 18:15:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=566
06/17/2022 18:15:08 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.5625 on epoch=566
06/17/2022 18:15:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
06/17/2022 18:15:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
06/17/2022 18:15:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/17/2022 18:15:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/17/2022 18:15:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
06/17/2022 18:15:23 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.5625 on epoch=583
06/17/2022 18:15:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/17/2022 18:15:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/17/2022 18:15:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/17/2022 18:15:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/17/2022 18:15:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
06/17/2022 18:15:38 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.59375 on epoch=599
06/17/2022 18:15:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/17/2022 18:15:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/17/2022 18:15:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/17/2022 18:15:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/17/2022 18:15:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
06/17/2022 18:15:53 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.59375 on epoch=616
06/17/2022 18:15:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/17/2022 18:15:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
06/17/2022 18:16:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/17/2022 18:16:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/17/2022 18:16:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
06/17/2022 18:16:08 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.53125 on epoch=633
06/17/2022 18:16:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/17/2022 18:16:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/17/2022 18:16:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/17/2022 18:16:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
06/17/2022 18:16:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/17/2022 18:16:23 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.5625 on epoch=649
06/17/2022 18:16:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/17/2022 18:16:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/17/2022 18:16:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/17/2022 18:16:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/17/2022 18:16:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/17/2022 18:16:38 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.625 on epoch=666
06/17/2022 18:16:38 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=666, global_step=2000
06/17/2022 18:16:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/17/2022 18:16:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/17/2022 18:16:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/17/2022 18:16:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/17/2022 18:16:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/17/2022 18:16:53 - INFO - __main__ - Global step 2050 Train loss 0.00 ACC 0.625 on epoch=683
06/17/2022 18:16:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
06/17/2022 18:16:58 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/17/2022 18:17:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/17/2022 18:17:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/17/2022 18:17:07 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/17/2022 18:17:08 - INFO - __main__ - Global step 2100 Train loss 0.00 ACC 0.6875 on epoch=699
06/17/2022 18:17:08 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.6875 on epoch=699, global_step=2100
06/17/2022 18:17:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/17/2022 18:17:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/17/2022 18:17:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=709
06/17/2022 18:17:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
06/17/2022 18:17:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/17/2022 18:17:23 - INFO - __main__ - Global step 2150 Train loss 0.01 ACC 0.6875 on epoch=716
06/17/2022 18:17:26 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/17/2022 18:17:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 18:17:31 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/17/2022 18:17:34 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/17/2022 18:17:37 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/17/2022 18:17:38 - INFO - __main__ - Global step 2200 Train loss 0.00 ACC 0.71875 on epoch=733
06/17/2022 18:17:38 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=733, global_step=2200
06/17/2022 18:17:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/17/2022 18:17:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/17/2022 18:17:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/17/2022 18:17:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/17/2022 18:17:52 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/17/2022 18:17:53 - INFO - __main__ - Global step 2250 Train loss 0.00 ACC 0.625 on epoch=749
06/17/2022 18:17:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/17/2022 18:17:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/17/2022 18:18:01 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/17/2022 18:18:04 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/17/2022 18:18:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=766
06/17/2022 18:18:08 - INFO - __main__ - Global step 2300 Train loss 0.01 ACC 0.65625 on epoch=766
06/17/2022 18:18:11 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=769
06/17/2022 18:18:14 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/17/2022 18:18:16 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/17/2022 18:18:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/17/2022 18:18:22 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/17/2022 18:18:23 - INFO - __main__ - Global step 2350 Train loss 0.01 ACC 0.65625 on epoch=783
06/17/2022 18:18:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/17/2022 18:18:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/17/2022 18:18:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 18:18:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 18:18:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/17/2022 18:18:38 - INFO - __main__ - Global step 2400 Train loss 0.00 ACC 0.625 on epoch=799
06/17/2022 18:18:41 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/17/2022 18:18:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 18:18:46 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/17/2022 18:18:49 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/17/2022 18:18:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/17/2022 18:18:53 - INFO - __main__ - Global step 2450 Train loss 0.00 ACC 0.625 on epoch=816
06/17/2022 18:18:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/17/2022 18:18:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
06/17/2022 18:19:01 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/17/2022 18:19:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/17/2022 18:19:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 18:19:08 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.71875 on epoch=833
06/17/2022 18:19:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 18:19:13 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/17/2022 18:19:16 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/17/2022 18:19:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 18:19:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/17/2022 18:19:23 - INFO - __main__ - Global step 2550 Train loss 0.00 ACC 0.625 on epoch=849
06/17/2022 18:19:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/17/2022 18:19:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 18:19:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/17/2022 18:19:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 18:19:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 18:19:38 - INFO - __main__ - Global step 2600 Train loss 0.00 ACC 0.65625 on epoch=866
06/17/2022 18:19:41 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/17/2022 18:19:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/17/2022 18:19:46 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/17/2022 18:19:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/17/2022 18:19:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 18:19:53 - INFO - __main__ - Global step 2650 Train loss 0.00 ACC 0.625 on epoch=883
06/17/2022 18:19:56 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=886
06/17/2022 18:19:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 18:20:01 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 18:20:04 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 18:20:07 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 18:20:08 - INFO - __main__ - Global step 2700 Train loss 0.01 ACC 0.625 on epoch=899
06/17/2022 18:20:11 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 18:20:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 18:20:16 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 18:20:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 18:20:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/17/2022 18:20:23 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.625 on epoch=916
06/17/2022 18:20:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 18:20:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 18:20:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/17/2022 18:20:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
06/17/2022 18:20:37 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=933
06/17/2022 18:20:38 - INFO - __main__ - Global step 2800 Train loss 0.01 ACC 0.65625 on epoch=933
06/17/2022 18:20:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 18:20:43 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
06/17/2022 18:20:46 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 18:20:49 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 18:20:52 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 18:20:53 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.5625 on epoch=949
06/17/2022 18:20:56 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/17/2022 18:20:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/17/2022 18:21:01 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 18:21:04 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 18:21:07 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 18:21:08 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.625 on epoch=966
06/17/2022 18:21:11 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/17/2022 18:21:13 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 18:21:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/17/2022 18:21:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 18:21:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
06/17/2022 18:21:23 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.59375 on epoch=983
06/17/2022 18:21:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
06/17/2022 18:21:28 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/17/2022 18:21:31 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 18:21:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 18:21:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
06/17/2022 18:21:38 - INFO - __main__ - Global step 3000 Train loss 0.00 ACC 0.625 on epoch=999
06/17/2022 18:21:38 - INFO - __main__ - save last model!
06/17/2022 18:21:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 18:21:38 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 18:21:38 - INFO - __main__ - Printing 3 examples
06/17/2022 18:21:38 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 18:21:38 - INFO - __main__ - ['contradiction']
06/17/2022 18:21:38 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 18:21:38 - INFO - __main__ - ['neutral']
06/17/2022 18:21:38 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 18:21:38 - INFO - __main__ - ['entailment']
06/17/2022 18:21:38 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:21:38 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:21:38 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 18:21:38 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 18:21:38 - INFO - __main__ - Printing 3 examples
06/17/2022 18:21:38 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/17/2022 18:21:38 - INFO - __main__ - ['contradiction']
06/17/2022 18:21:38 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/17/2022 18:21:38 - INFO - __main__ - ['contradiction']
06/17/2022 18:21:38 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/17/2022 18:21:38 - INFO - __main__ - ['contradiction']
06/17/2022 18:21:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 18:21:38 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:21:38 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 18:21:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 18:21:38 - INFO - __main__ - Printing 3 examples
06/17/2022 18:21:38 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
06/17/2022 18:21:38 - INFO - __main__ - ['contradiction']
06/17/2022 18:21:38 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 18:21:38 - INFO - __main__ - ['contradiction']
06/17/2022 18:21:38 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 18:21:38 - INFO - __main__ - ['contradiction']
06/17/2022 18:21:38 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:21:38 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:21:38 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 18:21:40 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_13_0.4_8_predictions.txt
06/17/2022 18:21:40 - INFO - __main__ - ACC on test data: 0.7143
06/17/2022 18:21:40 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.4, bsz=8, dev_performance=0.71875, test_performance=0.7142857142857143
06/17/2022 18:21:40 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.3, bsz=8 ...
06/17/2022 18:21:41 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 18:21:41 - INFO - __main__ - Printing 3 examples
06/17/2022 18:21:41 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/17/2022 18:21:41 - INFO - __main__ - ['contradiction']
06/17/2022 18:21:41 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/17/2022 18:21:41 - INFO - __main__ - ['contradiction']
06/17/2022 18:21:41 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/17/2022 18:21:41 - INFO - __main__ - ['contradiction']
06/17/2022 18:21:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 18:21:41 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:21:41 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 18:21:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 18:21:41 - INFO - __main__ - Printing 3 examples
06/17/2022 18:21:41 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
06/17/2022 18:21:41 - INFO - __main__ - ['contradiction']
06/17/2022 18:21:41 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 18:21:41 - INFO - __main__ - ['contradiction']
06/17/2022 18:21:41 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 18:21:41 - INFO - __main__ - ['contradiction']
06/17/2022 18:21:41 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:21:41 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:21:41 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 18:21:56 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 18:21:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 18:21:57 - INFO - __main__ - Starting training!
06/17/2022 18:21:58 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 18:21:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 18:21:58 - INFO - __main__ - Starting training!
06/17/2022 18:22:02 - INFO - __main__ - Step 10 Global step 10 Train loss 1.28 on epoch=3
06/17/2022 18:22:05 - INFO - __main__ - Step 20 Global step 20 Train loss 0.59 on epoch=6
06/17/2022 18:22:08 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=9
06/17/2022 18:22:10 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=13
06/17/2022 18:22:13 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=16
06/17/2022 18:22:14 - INFO - __main__ - Global step 50 Train loss 0.67 ACC 0.53125 on epoch=16
06/17/2022 18:22:14 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.53125 on epoch=16, global_step=50
06/17/2022 18:22:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=19
06/17/2022 18:22:20 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=23
06/17/2022 18:22:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=26
06/17/2022 18:22:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=29
06/17/2022 18:22:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=33
06/17/2022 18:22:29 - INFO - __main__ - Global step 100 Train loss 0.46 ACC 0.5 on epoch=33
06/17/2022 18:22:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=36
06/17/2022 18:22:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=39
06/17/2022 18:22:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=43
06/17/2022 18:22:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=46
06/17/2022 18:22:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.35 on epoch=49
06/17/2022 18:22:44 - INFO - __main__ - Global step 150 Train loss 0.46 ACC 0.46875 on epoch=49
06/17/2022 18:22:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
06/17/2022 18:22:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=56
06/17/2022 18:22:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=59
06/17/2022 18:22:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=63
06/17/2022 18:22:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=66
06/17/2022 18:22:59 - INFO - __main__ - Global step 200 Train loss 0.42 ACC 0.5 on epoch=66
06/17/2022 18:23:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=69
06/17/2022 18:23:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=73
06/17/2022 18:23:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.36 on epoch=76
06/17/2022 18:23:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.36 on epoch=79
06/17/2022 18:23:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=83
06/17/2022 18:23:14 - INFO - __main__ - Global step 250 Train loss 0.36 ACC 0.5 on epoch=83
06/17/2022 18:23:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.35 on epoch=86
06/17/2022 18:23:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=89
06/17/2022 18:23:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=93
06/17/2022 18:23:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=96
06/17/2022 18:23:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.31 on epoch=99
06/17/2022 18:23:28 - INFO - __main__ - Global step 300 Train loss 0.34 ACC 0.5 on epoch=99
06/17/2022 18:23:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=103
06/17/2022 18:23:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=106
06/17/2022 18:23:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=109
06/17/2022 18:23:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=113
06/17/2022 18:23:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.29 on epoch=116
06/17/2022 18:23:43 - INFO - __main__ - Global step 350 Train loss 0.31 ACC 0.4375 on epoch=116
06/17/2022 18:23:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.30 on epoch=119
06/17/2022 18:23:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=123
06/17/2022 18:23:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.27 on epoch=126
06/17/2022 18:23:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=129
06/17/2022 18:23:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=133
06/17/2022 18:23:58 - INFO - __main__ - Global step 400 Train loss 0.28 ACC 0.4375 on epoch=133
06/17/2022 18:24:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=136
06/17/2022 18:24:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=139
06/17/2022 18:24:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=143
06/17/2022 18:24:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=146
06/17/2022 18:24:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=149
06/17/2022 18:24:13 - INFO - __main__ - Global step 450 Train loss 0.25 ACC 0.40625 on epoch=149
06/17/2022 18:24:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=153
06/17/2022 18:24:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=156
06/17/2022 18:24:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=159
06/17/2022 18:24:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=163
06/17/2022 18:24:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=166
06/17/2022 18:24:27 - INFO - __main__ - Global step 500 Train loss 0.21 ACC 0.5 on epoch=166
06/17/2022 18:24:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=169
06/17/2022 18:24:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=173
06/17/2022 18:24:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.17 on epoch=176
06/17/2022 18:24:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=179
06/17/2022 18:24:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=183
06/17/2022 18:24:42 - INFO - __main__ - Global step 550 Train loss 0.19 ACC 0.40625 on epoch=183
06/17/2022 18:24:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=186
06/17/2022 18:24:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=189
06/17/2022 18:24:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=193
06/17/2022 18:24:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.18 on epoch=196
06/17/2022 18:24:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.16 on epoch=199
06/17/2022 18:24:57 - INFO - __main__ - Global step 600 Train loss 0.16 ACC 0.46875 on epoch=199
06/17/2022 18:25:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.16 on epoch=203
06/17/2022 18:25:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=206
06/17/2022 18:25:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=209
06/17/2022 18:25:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.14 on epoch=213
06/17/2022 18:25:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.15 on epoch=216
06/17/2022 18:25:12 - INFO - __main__ - Global step 650 Train loss 0.16 ACC 0.4375 on epoch=216
06/17/2022 18:25:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=219
06/17/2022 18:25:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=223
06/17/2022 18:25:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.11 on epoch=226
06/17/2022 18:25:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=229
06/17/2022 18:25:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=233
06/17/2022 18:25:26 - INFO - __main__ - Global step 700 Train loss 0.13 ACC 0.5 on epoch=233
06/17/2022 18:25:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=236
06/17/2022 18:25:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=239
06/17/2022 18:25:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=243
06/17/2022 18:25:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=246
06/17/2022 18:25:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=249
06/17/2022 18:25:41 - INFO - __main__ - Global step 750 Train loss 0.10 ACC 0.53125 on epoch=249
06/17/2022 18:25:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=253
06/17/2022 18:25:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=256
06/17/2022 18:25:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=259
06/17/2022 18:25:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=263
06/17/2022 18:25:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=266
06/17/2022 18:25:56 - INFO - __main__ - Global step 800 Train loss 0.10 ACC 0.53125 on epoch=266
06/17/2022 18:25:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=269
06/17/2022 18:26:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=273
06/17/2022 18:26:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=276
06/17/2022 18:26:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=279
06/17/2022 18:26:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=283
06/17/2022 18:26:11 - INFO - __main__ - Global step 850 Train loss 0.09 ACC 0.5 on epoch=283
06/17/2022 18:26:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=286
06/17/2022 18:26:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=289
06/17/2022 18:26:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=293
06/17/2022 18:26:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=296
06/17/2022 18:26:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=299
06/17/2022 18:26:26 - INFO - __main__ - Global step 900 Train loss 0.08 ACC 0.46875 on epoch=299
06/17/2022 18:26:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=303
06/17/2022 18:26:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=306
06/17/2022 18:26:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=309
06/17/2022 18:26:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=313
06/17/2022 18:26:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=316
06/17/2022 18:26:40 - INFO - __main__ - Global step 950 Train loss 0.09 ACC 0.5 on epoch=316
06/17/2022 18:26:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=319
06/17/2022 18:26:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=323
06/17/2022 18:26:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=326
06/17/2022 18:26:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=329
06/17/2022 18:26:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=333
06/17/2022 18:26:55 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.4375 on epoch=333
06/17/2022 18:26:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=336
06/17/2022 18:27:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=339
06/17/2022 18:27:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=343
06/17/2022 18:27:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=346
06/17/2022 18:27:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=349
06/17/2022 18:27:10 - INFO - __main__ - Global step 1050 Train loss 0.05 ACC 0.375 on epoch=349
06/17/2022 18:27:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=353
06/17/2022 18:27:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=356
06/17/2022 18:27:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=359
06/17/2022 18:27:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=363
06/17/2022 18:27:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=366
06/17/2022 18:27:24 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.40625 on epoch=366
06/17/2022 18:27:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
06/17/2022 18:27:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
06/17/2022 18:27:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
06/17/2022 18:27:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=379
06/17/2022 18:27:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
06/17/2022 18:27:39 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.40625 on epoch=383
06/17/2022 18:27:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=386
06/17/2022 18:27:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=389
06/17/2022 18:27:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=393
06/17/2022 18:27:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=396
06/17/2022 18:27:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
06/17/2022 18:27:53 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.4375 on epoch=399
06/17/2022 18:27:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
06/17/2022 18:27:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=406
06/17/2022 18:28:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=409
06/17/2022 18:28:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=413
06/17/2022 18:28:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
06/17/2022 18:28:08 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.5 on epoch=416
06/17/2022 18:28:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=419
06/17/2022 18:28:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=423
06/17/2022 18:28:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=426
06/17/2022 18:28:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=429
06/17/2022 18:28:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
06/17/2022 18:28:22 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.5 on epoch=433
06/17/2022 18:28:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=436
06/17/2022 18:28:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/17/2022 18:28:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
06/17/2022 18:28:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
06/17/2022 18:28:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
06/17/2022 18:28:37 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.46875 on epoch=449
06/17/2022 18:28:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
06/17/2022 18:28:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=456
06/17/2022 18:28:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
06/17/2022 18:28:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
06/17/2022 18:28:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
06/17/2022 18:28:51 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.46875 on epoch=466
06/17/2022 18:28:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=469
06/17/2022 18:28:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
06/17/2022 18:29:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=476
06/17/2022 18:29:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
06/17/2022 18:29:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
06/17/2022 18:29:06 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.53125 on epoch=483
06/17/2022 18:29:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
06/17/2022 18:29:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=489
06/17/2022 18:29:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
06/17/2022 18:29:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
06/17/2022 18:29:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
06/17/2022 18:29:21 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.53125 on epoch=499
06/17/2022 18:29:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=503
06/17/2022 18:29:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
06/17/2022 18:29:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
06/17/2022 18:29:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
06/17/2022 18:29:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
06/17/2022 18:29:35 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.5 on epoch=516
06/17/2022 18:29:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/17/2022 18:29:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
06/17/2022 18:29:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
06/17/2022 18:29:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
06/17/2022 18:29:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/17/2022 18:29:50 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.5625 on epoch=533
06/17/2022 18:29:50 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=533, global_step=1600
06/17/2022 18:29:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
06/17/2022 18:29:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
06/17/2022 18:29:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
06/17/2022 18:30:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/17/2022 18:30:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=549
06/17/2022 18:30:04 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.5625 on epoch=549
06/17/2022 18:30:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
06/17/2022 18:30:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
06/17/2022 18:30:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
06/17/2022 18:30:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
06/17/2022 18:30:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
06/17/2022 18:30:19 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.59375 on epoch=566
06/17/2022 18:30:19 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=566, global_step=1700
06/17/2022 18:30:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=569
06/17/2022 18:30:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
06/17/2022 18:30:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
06/17/2022 18:30:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/17/2022 18:30:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
06/17/2022 18:30:34 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.5625 on epoch=583
06/17/2022 18:30:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
06/17/2022 18:30:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/17/2022 18:30:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/17/2022 18:30:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/17/2022 18:30:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
06/17/2022 18:30:49 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.53125 on epoch=599
06/17/2022 18:30:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
06/17/2022 18:30:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/17/2022 18:30:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/17/2022 18:31:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/17/2022 18:31:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/17/2022 18:31:03 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.625 on epoch=616
06/17/2022 18:31:03 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=616, global_step=1850
06/17/2022 18:31:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/17/2022 18:31:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
06/17/2022 18:31:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/17/2022 18:31:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/17/2022 18:31:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/17/2022 18:31:18 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.65625 on epoch=633
06/17/2022 18:31:18 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=633, global_step=1900
06/17/2022 18:31:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
06/17/2022 18:31:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
06/17/2022 18:31:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/17/2022 18:31:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/17/2022 18:31:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/17/2022 18:31:33 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.6875 on epoch=649
06/17/2022 18:31:33 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=649, global_step=1950
06/17/2022 18:31:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/17/2022 18:31:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/17/2022 18:31:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/17/2022 18:31:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/17/2022 18:31:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/17/2022 18:31:47 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.6875 on epoch=666
06/17/2022 18:31:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
06/17/2022 18:31:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/17/2022 18:31:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/17/2022 18:31:58 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/17/2022 18:32:01 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/17/2022 18:32:02 - INFO - __main__ - Global step 2050 Train loss 0.01 ACC 0.59375 on epoch=683
06/17/2022 18:32:05 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/17/2022 18:32:07 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/17/2022 18:32:10 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/17/2022 18:32:13 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
06/17/2022 18:32:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/17/2022 18:32:17 - INFO - __main__ - Global step 2100 Train loss 0.01 ACC 0.6875 on epoch=699
06/17/2022 18:32:19 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/17/2022 18:32:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/17/2022 18:32:25 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=709
06/17/2022 18:32:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/17/2022 18:32:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/17/2022 18:32:31 - INFO - __main__ - Global step 2150 Train loss 0.01 ACC 0.71875 on epoch=716
06/17/2022 18:32:31 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=716, global_step=2150
06/17/2022 18:32:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=719
06/17/2022 18:32:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 18:32:39 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
06/17/2022 18:32:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/17/2022 18:32:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
06/17/2022 18:32:46 - INFO - __main__ - Global step 2200 Train loss 0.01 ACC 0.625 on epoch=733
06/17/2022 18:32:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/17/2022 18:32:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/17/2022 18:32:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/17/2022 18:32:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/17/2022 18:32:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/17/2022 18:33:00 - INFO - __main__ - Global step 2250 Train loss 0.00 ACC 0.53125 on epoch=749
06/17/2022 18:33:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/17/2022 18:33:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/17/2022 18:33:09 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
06/17/2022 18:33:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
06/17/2022 18:33:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/17/2022 18:33:15 - INFO - __main__ - Global step 2300 Train loss 0.01 ACC 0.59375 on epoch=766
06/17/2022 18:33:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
06/17/2022 18:33:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/17/2022 18:33:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/17/2022 18:33:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/17/2022 18:33:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/17/2022 18:33:29 - INFO - __main__ - Global step 2350 Train loss 0.01 ACC 0.59375 on epoch=783
06/17/2022 18:33:32 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
06/17/2022 18:33:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/17/2022 18:33:38 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 18:33:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 18:33:43 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/17/2022 18:33:44 - INFO - __main__ - Global step 2400 Train loss 0.01 ACC 0.71875 on epoch=799
06/17/2022 18:33:47 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/17/2022 18:33:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 18:33:52 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/17/2022 18:33:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/17/2022 18:33:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/17/2022 18:33:59 - INFO - __main__ - Global step 2450 Train loss 0.00 ACC 0.59375 on epoch=816
06/17/2022 18:34:01 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/17/2022 18:34:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/17/2022 18:34:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/17/2022 18:34:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/17/2022 18:34:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 18:34:13 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.625 on epoch=833
06/17/2022 18:34:16 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 18:34:19 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
06/17/2022 18:34:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/17/2022 18:34:24 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 18:34:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/17/2022 18:34:28 - INFO - __main__ - Global step 2550 Train loss 0.00 ACC 0.6875 on epoch=849
06/17/2022 18:34:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/17/2022 18:34:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
06/17/2022 18:34:36 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/17/2022 18:34:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 18:34:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 18:34:42 - INFO - __main__ - Global step 2600 Train loss 0.00 ACC 0.625 on epoch=866
06/17/2022 18:34:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/17/2022 18:34:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/17/2022 18:34:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/17/2022 18:34:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
06/17/2022 18:34:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 18:34:57 - INFO - __main__ - Global step 2650 Train loss 0.00 ACC 0.59375 on epoch=883
06/17/2022 18:35:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 18:35:03 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 18:35:05 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 18:35:08 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 18:35:11 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 18:35:12 - INFO - __main__ - Global step 2700 Train loss 0.00 ACC 0.59375 on epoch=899
06/17/2022 18:35:14 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=903
06/17/2022 18:35:17 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 18:35:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 18:35:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 18:35:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/17/2022 18:35:26 - INFO - __main__ - Global step 2750 Train loss 0.01 ACC 0.59375 on epoch=916
06/17/2022 18:35:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 18:35:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 18:35:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
06/17/2022 18:35:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/17/2022 18:35:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 18:35:41 - INFO - __main__ - Global step 2800 Train loss 0.00 ACC 0.625 on epoch=933
06/17/2022 18:35:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 18:35:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/17/2022 18:35:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 18:35:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 18:35:55 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 18:35:56 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.59375 on epoch=949
06/17/2022 18:35:58 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/17/2022 18:36:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/17/2022 18:36:04 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 18:36:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 18:36:09 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 18:36:10 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.5625 on epoch=966
06/17/2022 18:36:13 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/17/2022 18:36:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 18:36:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
06/17/2022 18:36:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 18:36:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 18:36:25 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.65625 on epoch=983
06/17/2022 18:36:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 18:36:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/17/2022 18:36:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 18:36:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 18:36:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 18:36:39 - INFO - __main__ - Global step 3000 Train loss 0.00 ACC 0.65625 on epoch=999
06/17/2022 18:36:39 - INFO - __main__ - save last model!
06/17/2022 18:36:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 18:36:40 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 18:36:40 - INFO - __main__ - Printing 3 examples
06/17/2022 18:36:40 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 18:36:40 - INFO - __main__ - ['contradiction']
06/17/2022 18:36:40 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 18:36:40 - INFO - __main__ - ['neutral']
06/17/2022 18:36:40 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 18:36:40 - INFO - __main__ - ['entailment']
06/17/2022 18:36:40 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:36:40 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:36:40 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 18:36:40 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 18:36:40 - INFO - __main__ - Printing 3 examples
06/17/2022 18:36:40 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/17/2022 18:36:40 - INFO - __main__ - ['contradiction']
06/17/2022 18:36:40 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/17/2022 18:36:40 - INFO - __main__ - ['contradiction']
06/17/2022 18:36:40 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/17/2022 18:36:40 - INFO - __main__ - ['contradiction']
06/17/2022 18:36:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 18:36:40 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:36:40 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 18:36:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 18:36:40 - INFO - __main__ - Printing 3 examples
06/17/2022 18:36:40 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
06/17/2022 18:36:40 - INFO - __main__ - ['contradiction']
06/17/2022 18:36:40 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 18:36:40 - INFO - __main__ - ['contradiction']
06/17/2022 18:36:40 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 18:36:40 - INFO - __main__ - ['contradiction']
06/17/2022 18:36:40 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:36:40 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:36:40 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 18:36:42 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_13_0.3_8_predictions.txt
06/17/2022 18:36:42 - INFO - __main__ - ACC on test data: 0.7143
06/17/2022 18:36:42 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.3, bsz=8, dev_performance=0.71875, test_performance=0.7142857142857143
06/17/2022 18:36:42 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.2, bsz=8 ...
06/17/2022 18:36:43 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 18:36:43 - INFO - __main__ - Printing 3 examples
06/17/2022 18:36:43 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/17/2022 18:36:43 - INFO - __main__ - ['contradiction']
06/17/2022 18:36:43 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/17/2022 18:36:43 - INFO - __main__ - ['contradiction']
06/17/2022 18:36:43 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/17/2022 18:36:43 - INFO - __main__ - ['contradiction']
06/17/2022 18:36:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 18:36:43 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:36:43 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 18:36:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 18:36:43 - INFO - __main__ - Printing 3 examples
06/17/2022 18:36:43 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
06/17/2022 18:36:43 - INFO - __main__ - ['contradiction']
06/17/2022 18:36:43 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
06/17/2022 18:36:43 - INFO - __main__ - ['contradiction']
06/17/2022 18:36:43 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/17/2022 18:36:43 - INFO - __main__ - ['contradiction']
06/17/2022 18:36:43 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:36:43 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:36:43 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 18:36:59 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 18:36:59 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 18:36:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 18:36:59 - INFO - __main__ - Starting training!
06/17/2022 18:37:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 18:37:00 - INFO - __main__ - Starting training!
06/17/2022 18:37:04 - INFO - __main__ - Step 10 Global step 10 Train loss 1.70 on epoch=3
06/17/2022 18:37:06 - INFO - __main__ - Step 20 Global step 20 Train loss 0.70 on epoch=6
06/17/2022 18:37:09 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=9
06/17/2022 18:37:12 - INFO - __main__ - Step 40 Global step 40 Train loss 0.59 on epoch=13
06/17/2022 18:37:15 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=16
06/17/2022 18:37:15 - INFO - __main__ - Global step 50 Train loss 0.81 ACC 0.5 on epoch=16
06/17/2022 18:37:16 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 18:37:18 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=19
06/17/2022 18:37:21 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=23
06/17/2022 18:37:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.36 on epoch=26
06/17/2022 18:37:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=29
06/17/2022 18:37:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=33
06/17/2022 18:37:30 - INFO - __main__ - Global step 100 Train loss 0.44 ACC 0.5 on epoch=33
06/17/2022 18:37:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=36
06/17/2022 18:37:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=39
06/17/2022 18:37:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=43
06/17/2022 18:37:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=46
06/17/2022 18:37:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=49
06/17/2022 18:37:45 - INFO - __main__ - Global step 150 Train loss 0.47 ACC 0.5 on epoch=49
06/17/2022 18:37:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=53
06/17/2022 18:37:50 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=56
06/17/2022 18:37:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=59
06/17/2022 18:37:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=63
06/17/2022 18:37:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=66
06/17/2022 18:37:59 - INFO - __main__ - Global step 200 Train loss 0.44 ACC 0.46875 on epoch=66
06/17/2022 18:38:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=69
06/17/2022 18:38:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=73
06/17/2022 18:38:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=76
06/17/2022 18:38:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
06/17/2022 18:38:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=83
06/17/2022 18:38:14 - INFO - __main__ - Global step 250 Train loss 0.45 ACC 0.46875 on epoch=83
06/17/2022 18:38:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=86
06/17/2022 18:38:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=89
06/17/2022 18:38:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.58 on epoch=93
06/17/2022 18:38:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=96
06/17/2022 18:38:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=99
06/17/2022 18:38:29 - INFO - __main__ - Global step 300 Train loss 0.45 ACC 0.4375 on epoch=99
06/17/2022 18:38:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=103
06/17/2022 18:38:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=106
06/17/2022 18:38:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=109
06/17/2022 18:38:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=113
06/17/2022 18:38:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.34 on epoch=116
06/17/2022 18:38:43 - INFO - __main__ - Global step 350 Train loss 0.37 ACC 0.46875 on epoch=116
06/17/2022 18:38:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=119
06/17/2022 18:38:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=123
06/17/2022 18:38:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=126
06/17/2022 18:38:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=129
06/17/2022 18:38:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=133
06/17/2022 18:38:58 - INFO - __main__ - Global step 400 Train loss 0.38 ACC 0.53125 on epoch=133
06/17/2022 18:38:58 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=133, global_step=400
06/17/2022 18:39:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=136
06/17/2022 18:39:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=139
06/17/2022 18:39:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=143
06/17/2022 18:39:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=146
06/17/2022 18:39:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=149
06/17/2022 18:39:13 - INFO - __main__ - Global step 450 Train loss 0.34 ACC 0.46875 on epoch=149
06/17/2022 18:39:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=153
06/17/2022 18:39:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.30 on epoch=156
06/17/2022 18:39:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=159
06/17/2022 18:39:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=163
06/17/2022 18:39:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=166
06/17/2022 18:39:28 - INFO - __main__ - Global step 500 Train loss 0.31 ACC 0.5 on epoch=166
06/17/2022 18:39:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=169
06/17/2022 18:39:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=173
06/17/2022 18:39:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.31 on epoch=176
06/17/2022 18:39:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=179
06/17/2022 18:39:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=183
06/17/2022 18:39:43 - INFO - __main__ - Global step 550 Train loss 0.29 ACC 0.53125 on epoch=183
06/17/2022 18:39:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=186
06/17/2022 18:39:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=189
06/17/2022 18:39:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=193
06/17/2022 18:39:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=196
06/17/2022 18:39:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=199
06/17/2022 18:39:58 - INFO - __main__ - Global step 600 Train loss 0.28 ACC 0.4375 on epoch=199
06/17/2022 18:40:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=203
06/17/2022 18:40:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=206
06/17/2022 18:40:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=209
06/17/2022 18:40:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=213
06/17/2022 18:40:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=216
06/17/2022 18:40:13 - INFO - __main__ - Global step 650 Train loss 0.28 ACC 0.40625 on epoch=216
06/17/2022 18:40:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=219
06/17/2022 18:40:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=223
06/17/2022 18:40:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.28 on epoch=226
06/17/2022 18:40:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=229
06/17/2022 18:40:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=233
06/17/2022 18:40:28 - INFO - __main__ - Global step 700 Train loss 0.27 ACC 0.375 on epoch=233
06/17/2022 18:40:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=236
06/17/2022 18:40:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=239
06/17/2022 18:40:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=243
06/17/2022 18:40:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=246
06/17/2022 18:40:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=249
06/17/2022 18:40:43 - INFO - __main__ - Global step 750 Train loss 0.24 ACC 0.46875 on epoch=249
06/17/2022 18:40:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=253
06/17/2022 18:40:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=256
06/17/2022 18:40:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=259
06/17/2022 18:40:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=263
06/17/2022 18:40:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=266
06/17/2022 18:40:57 - INFO - __main__ - Global step 800 Train loss 0.20 ACC 0.40625 on epoch=266
06/17/2022 18:41:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=269
06/17/2022 18:41:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=273
06/17/2022 18:41:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=276
06/17/2022 18:41:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=279
06/17/2022 18:41:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=283
06/17/2022 18:41:12 - INFO - __main__ - Global step 850 Train loss 0.20 ACC 0.34375 on epoch=283
06/17/2022 18:41:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=286
06/17/2022 18:41:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=289
06/17/2022 18:41:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=293
06/17/2022 18:41:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=296
06/17/2022 18:41:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=299
06/17/2022 18:41:26 - INFO - __main__ - Global step 900 Train loss 0.19 ACC 0.46875 on epoch=299
06/17/2022 18:41:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=303
06/17/2022 18:41:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=306
06/17/2022 18:41:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=309
06/17/2022 18:41:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=313
06/17/2022 18:41:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=316
06/17/2022 18:41:41 - INFO - __main__ - Global step 950 Train loss 0.18 ACC 0.5 on epoch=316
06/17/2022 18:41:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=319
06/17/2022 18:41:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=323
06/17/2022 18:41:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=326
06/17/2022 18:41:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=329
06/17/2022 18:41:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.17 on epoch=333
06/17/2022 18:41:56 - INFO - __main__ - Global step 1000 Train loss 0.16 ACC 0.5 on epoch=333
06/17/2022 18:41:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=336
06/17/2022 18:42:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=339
06/17/2022 18:42:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=343
06/17/2022 18:42:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=346
06/17/2022 18:42:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=349
06/17/2022 18:42:11 - INFO - __main__ - Global step 1050 Train loss 0.15 ACC 0.40625 on epoch=349
06/17/2022 18:42:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=353
06/17/2022 18:42:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=356
06/17/2022 18:42:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=359
06/17/2022 18:42:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=363
06/17/2022 18:42:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.11 on epoch=366
06/17/2022 18:42:25 - INFO - __main__ - Global step 1100 Train loss 0.12 ACC 0.34375 on epoch=366
06/17/2022 18:42:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=369
06/17/2022 18:42:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=373
06/17/2022 18:42:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=376
06/17/2022 18:42:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=379
06/17/2022 18:42:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=383
06/17/2022 18:42:40 - INFO - __main__ - Global step 1150 Train loss 0.12 ACC 0.46875 on epoch=383
06/17/2022 18:42:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=386
06/17/2022 18:42:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=389
06/17/2022 18:42:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=393
06/17/2022 18:42:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=396
06/17/2022 18:42:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=399
06/17/2022 18:42:55 - INFO - __main__ - Global step 1200 Train loss 0.11 ACC 0.46875 on epoch=399
06/17/2022 18:42:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=403
06/17/2022 18:43:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=406
06/17/2022 18:43:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=409
06/17/2022 18:43:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=413
06/17/2022 18:43:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=416
06/17/2022 18:43:09 - INFO - __main__ - Global step 1250 Train loss 0.09 ACC 0.34375 on epoch=416
06/17/2022 18:43:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.10 on epoch=419
06/17/2022 18:43:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=423
06/17/2022 18:43:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=426
06/17/2022 18:43:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.09 on epoch=429
06/17/2022 18:43:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=433
06/17/2022 18:43:24 - INFO - __main__ - Global step 1300 Train loss 0.08 ACC 0.53125 on epoch=433
06/17/2022 18:43:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=436
06/17/2022 18:43:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=439
06/17/2022 18:43:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=443
06/17/2022 18:43:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=446
06/17/2022 18:43:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=449
06/17/2022 18:43:39 - INFO - __main__ - Global step 1350 Train loss 0.08 ACC 0.46875 on epoch=449
06/17/2022 18:43:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=453
06/17/2022 18:43:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=456
06/17/2022 18:43:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=459
06/17/2022 18:43:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=463
06/17/2022 18:43:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.09 on epoch=466
06/17/2022 18:43:53 - INFO - __main__ - Global step 1400 Train loss 0.08 ACC 0.46875 on epoch=466
06/17/2022 18:43:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=469
06/17/2022 18:43:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=473
06/17/2022 18:44:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=476
06/17/2022 18:44:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=479
06/17/2022 18:44:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=483
06/17/2022 18:44:08 - INFO - __main__ - Global step 1450 Train loss 0.06 ACC 0.5 on epoch=483
06/17/2022 18:44:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=486
06/17/2022 18:44:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
06/17/2022 18:44:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=493
06/17/2022 18:44:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=496
06/17/2022 18:44:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=499
06/17/2022 18:44:23 - INFO - __main__ - Global step 1500 Train loss 0.06 ACC 0.53125 on epoch=499
06/17/2022 18:44:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=503
06/17/2022 18:44:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=506
06/17/2022 18:44:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=509
06/17/2022 18:44:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=513
06/17/2022 18:44:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=516
06/17/2022 18:44:38 - INFO - __main__ - Global step 1550 Train loss 0.05 ACC 0.46875 on epoch=516
06/17/2022 18:44:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=519
06/17/2022 18:44:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
06/17/2022 18:44:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=526
06/17/2022 18:44:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=529
06/17/2022 18:44:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=533
06/17/2022 18:44:52 - INFO - __main__ - Global step 1600 Train loss 0.05 ACC 0.5 on epoch=533
06/17/2022 18:44:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=536
06/17/2022 18:44:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=539
06/17/2022 18:45:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=543
06/17/2022 18:45:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=546
06/17/2022 18:45:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=549
06/17/2022 18:45:07 - INFO - __main__ - Global step 1650 Train loss 0.05 ACC 0.59375 on epoch=549
06/17/2022 18:45:07 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=549, global_step=1650
06/17/2022 18:45:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
06/17/2022 18:45:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
06/17/2022 18:45:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
06/17/2022 18:45:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=563
06/17/2022 18:45:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=566
06/17/2022 18:45:22 - INFO - __main__ - Global step 1700 Train loss 0.03 ACC 0.53125 on epoch=566
06/17/2022 18:45:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=569
06/17/2022 18:45:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=573
06/17/2022 18:45:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=576
06/17/2022 18:45:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=579
06/17/2022 18:45:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
06/17/2022 18:45:37 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.5625 on epoch=583
06/17/2022 18:45:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
06/17/2022 18:45:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
06/17/2022 18:45:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
06/17/2022 18:45:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=596
06/17/2022 18:45:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=599
06/17/2022 18:45:51 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.53125 on epoch=599
06/17/2022 18:45:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=603
06/17/2022 18:45:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
06/17/2022 18:46:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
06/17/2022 18:46:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
06/17/2022 18:46:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=616
06/17/2022 18:46:06 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.53125 on epoch=616
06/17/2022 18:46:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=619
06/17/2022 18:46:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=623
06/17/2022 18:46:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/17/2022 18:46:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
06/17/2022 18:46:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
06/17/2022 18:46:21 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.5625 on epoch=633
06/17/2022 18:46:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
06/17/2022 18:46:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
06/17/2022 18:46:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=643
06/17/2022 18:46:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
06/17/2022 18:46:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
06/17/2022 18:46:36 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.5 on epoch=649
06/17/2022 18:46:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/17/2022 18:46:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
06/17/2022 18:46:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
06/17/2022 18:46:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/17/2022 18:46:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/17/2022 18:46:51 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.5 on epoch=666
06/17/2022 18:46:53 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
06/17/2022 18:46:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
06/17/2022 18:46:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
06/17/2022 18:47:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
06/17/2022 18:47:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=683
06/17/2022 18:47:05 - INFO - __main__ - Global step 2050 Train loss 0.02 ACC 0.5 on epoch=683
06/17/2022 18:47:08 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
06/17/2022 18:47:11 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=689
06/17/2022 18:47:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/17/2022 18:47:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
06/17/2022 18:47:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/17/2022 18:47:20 - INFO - __main__ - Global step 2100 Train loss 0.02 ACC 0.59375 on epoch=699
06/17/2022 18:47:23 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
06/17/2022 18:47:26 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=706
06/17/2022 18:47:29 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/17/2022 18:47:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/17/2022 18:47:34 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/17/2022 18:47:35 - INFO - __main__ - Global step 2150 Train loss 0.02 ACC 0.5 on epoch=716
06/17/2022 18:47:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/17/2022 18:47:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 18:47:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/17/2022 18:47:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
06/17/2022 18:47:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=733
06/17/2022 18:47:50 - INFO - __main__ - Global step 2200 Train loss 0.01 ACC 0.5625 on epoch=733
06/17/2022 18:47:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=736
06/17/2022 18:47:55 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
06/17/2022 18:47:58 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=743
06/17/2022 18:48:01 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=746
06/17/2022 18:48:04 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
06/17/2022 18:48:05 - INFO - __main__ - Global step 2250 Train loss 0.02 ACC 0.53125 on epoch=749
06/17/2022 18:48:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=753
06/17/2022 18:48:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/17/2022 18:48:13 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
06/17/2022 18:48:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=763
06/17/2022 18:48:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/17/2022 18:48:19 - INFO - __main__ - Global step 2300 Train loss 0.02 ACC 0.5 on epoch=766
06/17/2022 18:48:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=769
06/17/2022 18:48:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=773
06/17/2022 18:48:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
06/17/2022 18:48:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=779
06/17/2022 18:48:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
06/17/2022 18:48:34 - INFO - __main__ - Global step 2350 Train loss 0.02 ACC 0.4375 on epoch=783
06/17/2022 18:48:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
06/17/2022 18:48:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/17/2022 18:48:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=793
06/17/2022 18:48:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
06/17/2022 18:48:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/17/2022 18:48:49 - INFO - __main__ - Global step 2400 Train loss 0.02 ACC 0.46875 on epoch=799
06/17/2022 18:48:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
06/17/2022 18:48:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=806
06/17/2022 18:48:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/17/2022 18:49:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/17/2022 18:49:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/17/2022 18:49:04 - INFO - __main__ - Global step 2450 Train loss 0.01 ACC 0.5 on epoch=816
06/17/2022 18:49:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=819
06/17/2022 18:49:09 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
06/17/2022 18:49:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/17/2022 18:49:15 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/17/2022 18:49:18 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=833
06/17/2022 18:49:19 - INFO - __main__ - Global step 2500 Train loss 0.02 ACC 0.59375 on epoch=833
06/17/2022 18:49:21 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
06/17/2022 18:49:24 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
06/17/2022 18:49:27 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
06/17/2022 18:49:30 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=846
06/17/2022 18:49:32 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/17/2022 18:49:33 - INFO - __main__ - Global step 2550 Train loss 0.01 ACC 0.59375 on epoch=849
06/17/2022 18:49:36 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
06/17/2022 18:49:39 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 18:49:42 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/17/2022 18:49:44 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 18:49:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
06/17/2022 18:49:48 - INFO - __main__ - Global step 2600 Train loss 0.01 ACC 0.59375 on epoch=866
06/17/2022 18:49:51 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
06/17/2022 18:49:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=873
06/17/2022 18:49:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
06/17/2022 18:49:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
06/17/2022 18:50:02 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 18:50:03 - INFO - __main__ - Global step 2650 Train loss 0.01 ACC 0.5625 on epoch=883
06/17/2022 18:50:06 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
06/17/2022 18:50:09 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=889
06/17/2022 18:50:11 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 18:50:14 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 18:50:17 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 18:50:18 - INFO - __main__ - Global step 2700 Train loss 0.01 ACC 0.53125 on epoch=899
06/17/2022 18:50:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=903
06/17/2022 18:50:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
06/17/2022 18:50:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
06/17/2022 18:50:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
06/17/2022 18:50:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/17/2022 18:50:33 - INFO - __main__ - Global step 2750 Train loss 0.01 ACC 0.5625 on epoch=916
06/17/2022 18:50:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 18:50:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 18:50:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
06/17/2022 18:50:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
06/17/2022 18:50:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 18:50:48 - INFO - __main__ - Global step 2800 Train loss 0.01 ACC 0.5 on epoch=933
06/17/2022 18:50:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=936
06/17/2022 18:50:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/17/2022 18:50:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
06/17/2022 18:50:59 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
06/17/2022 18:51:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
06/17/2022 18:51:02 - INFO - __main__ - Global step 2850 Train loss 0.01 ACC 0.53125 on epoch=949
06/17/2022 18:51:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/17/2022 18:51:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
06/17/2022 18:51:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 18:51:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/17/2022 18:51:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=966
06/17/2022 18:51:17 - INFO - __main__ - Global step 2900 Train loss 0.01 ACC 0.53125 on epoch=966
06/17/2022 18:51:20 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=969
06/17/2022 18:51:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 18:51:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
06/17/2022 18:51:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
06/17/2022 18:51:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 18:51:32 - INFO - __main__ - Global step 2950 Train loss 0.01 ACC 0.625 on epoch=983
06/17/2022 18:51:32 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=983, global_step=2950
06/17/2022 18:51:35 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 18:51:38 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
06/17/2022 18:51:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 18:51:43 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 18:51:46 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=999
06/17/2022 18:51:47 - INFO - __main__ - Global step 3000 Train loss 0.01 ACC 0.59375 on epoch=999
06/17/2022 18:51:47 - INFO - __main__ - save last model!
06/17/2022 18:51:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 18:51:47 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 18:51:47 - INFO - __main__ - Printing 3 examples
06/17/2022 18:51:47 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 18:51:47 - INFO - __main__ - ['contradiction']
06/17/2022 18:51:47 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 18:51:47 - INFO - __main__ - ['neutral']
06/17/2022 18:51:47 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 18:51:47 - INFO - __main__ - ['entailment']
06/17/2022 18:51:47 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:51:47 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:51:47 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 18:51:47 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 18:51:47 - INFO - __main__ - Printing 3 examples
06/17/2022 18:51:47 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/17/2022 18:51:47 - INFO - __main__ - ['contradiction']
06/17/2022 18:51:47 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/17/2022 18:51:47 - INFO - __main__ - ['contradiction']
06/17/2022 18:51:47 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/17/2022 18:51:47 - INFO - __main__ - ['contradiction']
06/17/2022 18:51:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 18:51:47 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:51:47 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 18:51:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 18:51:47 - INFO - __main__ - Printing 3 examples
06/17/2022 18:51:47 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
06/17/2022 18:51:47 - INFO - __main__ - ['contradiction']
06/17/2022 18:51:47 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
06/17/2022 18:51:47 - INFO - __main__ - ['contradiction']
06/17/2022 18:51:47 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
06/17/2022 18:51:47 - INFO - __main__ - ['contradiction']
06/17/2022 18:51:47 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:51:47 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:51:47 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 18:51:49 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_13_0.2_8_predictions.txt
06/17/2022 18:51:49 - INFO - __main__ - ACC on test data: 0.6964
06/17/2022 18:51:50 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.2, bsz=8, dev_performance=0.625, test_performance=0.6964285714285714
06/17/2022 18:51:50 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.5, bsz=8 ...
06/17/2022 18:51:50 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 18:51:50 - INFO - __main__ - Printing 3 examples
06/17/2022 18:51:50 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/17/2022 18:51:50 - INFO - __main__ - ['contradiction']
06/17/2022 18:51:50 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/17/2022 18:51:50 - INFO - __main__ - ['contradiction']
06/17/2022 18:51:50 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/17/2022 18:51:50 - INFO - __main__ - ['contradiction']
06/17/2022 18:51:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 18:51:50 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:51:50 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 18:51:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 18:51:50 - INFO - __main__ - Printing 3 examples
06/17/2022 18:51:50 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
06/17/2022 18:51:50 - INFO - __main__ - ['contradiction']
06/17/2022 18:51:50 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
06/17/2022 18:51:50 - INFO - __main__ - ['contradiction']
06/17/2022 18:51:50 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
06/17/2022 18:51:50 - INFO - __main__ - ['contradiction']
06/17/2022 18:51:50 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:51:51 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:51:51 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 18:52:03 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 18:52:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 18:52:04 - INFO - __main__ - Starting training!
06/17/2022 18:52:08 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 18:52:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 18:52:09 - INFO - __main__ - Starting training!
06/17/2022 18:52:13 - INFO - __main__ - Step 10 Global step 10 Train loss 0.96 on epoch=3
06/17/2022 18:52:16 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=6
06/17/2022 18:52:19 - INFO - __main__ - Step 30 Global step 30 Train loss 0.42 on epoch=9
06/17/2022 18:52:22 - INFO - __main__ - Step 40 Global step 40 Train loss 0.43 on epoch=13
06/17/2022 18:52:25 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=16
06/17/2022 18:52:26 - INFO - __main__ - Global step 50 Train loss 0.56 ACC 0.5 on epoch=16
06/17/2022 18:52:26 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 18:52:29 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=19
06/17/2022 18:52:32 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=23
06/17/2022 18:52:35 - INFO - __main__ - Step 80 Global step 80 Train loss 0.41 on epoch=26
06/17/2022 18:52:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=29
06/17/2022 18:52:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=33
06/17/2022 18:52:42 - INFO - __main__ - Global step 100 Train loss 0.42 ACC 0.5 on epoch=33
06/17/2022 18:52:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.41 on epoch=36
06/17/2022 18:52:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=39
06/17/2022 18:52:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.40 on epoch=43
06/17/2022 18:52:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=46
06/17/2022 18:52:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.39 on epoch=49
06/17/2022 18:52:58 - INFO - __main__ - Global step 150 Train loss 0.41 ACC 0.6875 on epoch=49
06/17/2022 18:52:58 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.6875 on epoch=49, global_step=150
06/17/2022 18:53:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.33 on epoch=53
06/17/2022 18:53:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.35 on epoch=56
06/17/2022 18:53:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=59
06/17/2022 18:53:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.30 on epoch=63
06/17/2022 18:53:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=66
06/17/2022 18:53:15 - INFO - __main__ - Global step 200 Train loss 0.34 ACC 0.59375 on epoch=66
06/17/2022 18:53:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=69
06/17/2022 18:53:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.30 on epoch=73
06/17/2022 18:53:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.30 on epoch=76
06/17/2022 18:53:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=79
06/17/2022 18:53:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=83
06/17/2022 18:53:31 - INFO - __main__ - Global step 250 Train loss 0.30 ACC 0.53125 on epoch=83
06/17/2022 18:53:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.29 on epoch=86
06/17/2022 18:53:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=89
06/17/2022 18:53:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=93
06/17/2022 18:53:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=96
06/17/2022 18:53:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=99
06/17/2022 18:53:47 - INFO - __main__ - Global step 300 Train loss 0.25 ACC 0.5 on epoch=99
06/17/2022 18:53:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=103
06/17/2022 18:53:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=106
06/17/2022 18:53:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.20 on epoch=109
06/17/2022 18:53:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=113
06/17/2022 18:54:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.20 on epoch=116
06/17/2022 18:54:03 - INFO - __main__ - Global step 350 Train loss 0.21 ACC 0.59375 on epoch=116
06/17/2022 18:54:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.16 on epoch=119
06/17/2022 18:54:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.18 on epoch=123
06/17/2022 18:54:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.17 on epoch=126
06/17/2022 18:54:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.12 on epoch=129
06/17/2022 18:54:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.14 on epoch=133
06/17/2022 18:54:20 - INFO - __main__ - Global step 400 Train loss 0.15 ACC 0.53125 on epoch=133
06/17/2022 18:54:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=136
06/17/2022 18:54:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.13 on epoch=139
06/17/2022 18:54:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.17 on epoch=143
06/17/2022 18:54:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.15 on epoch=146
06/17/2022 18:54:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.11 on epoch=149
06/17/2022 18:54:36 - INFO - __main__ - Global step 450 Train loss 0.14 ACC 0.53125 on epoch=149
06/17/2022 18:54:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=153
06/17/2022 18:54:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=156
06/17/2022 18:54:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=159
06/17/2022 18:54:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.06 on epoch=163
06/17/2022 18:54:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=166
06/17/2022 18:54:52 - INFO - __main__ - Global step 500 Train loss 0.09 ACC 0.5625 on epoch=166
06/17/2022 18:54:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=169
06/17/2022 18:54:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=173
06/17/2022 18:55:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=176
06/17/2022 18:55:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=179
06/17/2022 18:55:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.08 on epoch=183
06/17/2022 18:55:08 - INFO - __main__ - Global step 550 Train loss 0.09 ACC 0.59375 on epoch=183
06/17/2022 18:55:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=186
06/17/2022 18:55:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=189
06/17/2022 18:55:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.09 on epoch=193
06/17/2022 18:55:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=196
06/17/2022 18:55:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=199
06/17/2022 18:55:24 - INFO - __main__ - Global step 600 Train loss 0.07 ACC 0.5 on epoch=199
06/17/2022 18:55:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=203
06/17/2022 18:55:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=206
06/17/2022 18:55:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=209
06/17/2022 18:55:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=213
06/17/2022 18:55:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=216
06/17/2022 18:55:40 - INFO - __main__ - Global step 650 Train loss 0.05 ACC 0.59375 on epoch=216
06/17/2022 18:55:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=219
06/17/2022 18:55:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=223
06/17/2022 18:55:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=226
06/17/2022 18:55:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=229
06/17/2022 18:55:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=233
06/17/2022 18:55:57 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.59375 on epoch=233
06/17/2022 18:56:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=236
06/17/2022 18:56:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=239
06/17/2022 18:56:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=243
06/17/2022 18:56:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=246
06/17/2022 18:56:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=249
06/17/2022 18:56:13 - INFO - __main__ - Global step 750 Train loss 0.05 ACC 0.59375 on epoch=249
06/17/2022 18:56:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.06 on epoch=253
06/17/2022 18:56:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=256
06/17/2022 18:56:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=259
06/17/2022 18:56:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=263
06/17/2022 18:56:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=266
06/17/2022 18:56:29 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.6875 on epoch=266
06/17/2022 18:56:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=269
06/17/2022 18:56:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=273
06/17/2022 18:56:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=276
06/17/2022 18:56:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=279
06/17/2022 18:56:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=283
06/17/2022 18:56:45 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.5625 on epoch=283
06/17/2022 18:56:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=286
06/17/2022 18:56:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=289
06/17/2022 18:56:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=293
06/17/2022 18:56:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=296
06/17/2022 18:57:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=299
06/17/2022 18:57:01 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.53125 on epoch=299
06/17/2022 18:57:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=303
06/17/2022 18:57:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=306
06/17/2022 18:57:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=309
06/17/2022 18:57:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=313
06/17/2022 18:57:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=316
06/17/2022 18:57:18 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.5625 on epoch=316
06/17/2022 18:57:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=319
06/17/2022 18:57:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=323
06/17/2022 18:57:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=326
06/17/2022 18:57:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=329
06/17/2022 18:57:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=333
06/17/2022 18:57:34 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.5625 on epoch=333
06/17/2022 18:57:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=336
06/17/2022 18:57:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=339
06/17/2022 18:57:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=343
06/17/2022 18:57:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=346
06/17/2022 18:57:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=349
06/17/2022 18:57:50 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.65625 on epoch=349
06/17/2022 18:57:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=353
06/17/2022 18:57:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
06/17/2022 18:58:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=359
06/17/2022 18:58:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=363
06/17/2022 18:58:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=366
06/17/2022 18:58:07 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.65625 on epoch=366
06/17/2022 18:58:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
06/17/2022 18:58:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=373
06/17/2022 18:58:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
06/17/2022 18:58:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=379
06/17/2022 18:58:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=383
06/17/2022 18:58:23 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.625 on epoch=383
06/17/2022 18:58:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
06/17/2022 18:58:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=389
06/17/2022 18:58:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=393
06/17/2022 18:58:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
06/17/2022 18:58:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=399
06/17/2022 18:58:39 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.625 on epoch=399
06/17/2022 18:58:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
06/17/2022 18:58:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=406
06/17/2022 18:58:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
06/17/2022 18:58:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=413
06/17/2022 18:58:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
06/17/2022 18:58:56 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.65625 on epoch=416
06/17/2022 18:58:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
06/17/2022 18:59:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
06/17/2022 18:59:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
06/17/2022 18:59:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=429
06/17/2022 18:59:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=433
06/17/2022 18:59:12 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.65625 on epoch=433
06/17/2022 18:59:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
06/17/2022 18:59:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=439
06/17/2022 18:59:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
06/17/2022 18:59:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
06/17/2022 18:59:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
06/17/2022 18:59:29 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.6875 on epoch=449
06/17/2022 18:59:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
06/17/2022 18:59:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=456
06/17/2022 18:59:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
06/17/2022 18:59:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=463
06/17/2022 18:59:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
06/17/2022 18:59:45 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.65625 on epoch=466
06/17/2022 18:59:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
06/17/2022 18:59:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
06/17/2022 18:59:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
06/17/2022 18:59:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
06/17/2022 19:00:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
06/17/2022 19:00:02 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.6875 on epoch=483
06/17/2022 19:00:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
06/17/2022 19:00:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
06/17/2022 19:00:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
06/17/2022 19:00:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
06/17/2022 19:00:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
06/17/2022 19:00:18 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.75 on epoch=499
06/17/2022 19:00:18 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.75 on epoch=499, global_step=1500
06/17/2022 19:00:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
06/17/2022 19:00:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
06/17/2022 19:00:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
06/17/2022 19:00:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
06/17/2022 19:00:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
06/17/2022 19:00:34 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.65625 on epoch=516
06/17/2022 19:00:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/17/2022 19:00:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
06/17/2022 19:00:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
06/17/2022 19:00:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/17/2022 19:00:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
06/17/2022 19:00:51 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.6875 on epoch=533
06/17/2022 19:00:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
06/17/2022 19:00:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
06/17/2022 19:01:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
06/17/2022 19:01:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/17/2022 19:01:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/17/2022 19:01:07 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.6875 on epoch=549
06/17/2022 19:01:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
06/17/2022 19:01:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
06/17/2022 19:01:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/17/2022 19:01:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
06/17/2022 19:01:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
06/17/2022 19:01:23 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.6875 on epoch=566
06/17/2022 19:01:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
06/17/2022 19:01:29 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
06/17/2022 19:01:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
06/17/2022 19:01:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/17/2022 19:01:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/17/2022 19:01:40 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.6875 on epoch=583
06/17/2022 19:01:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/17/2022 19:01:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/17/2022 19:01:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/17/2022 19:01:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/17/2022 19:01:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
06/17/2022 19:01:56 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.6875 on epoch=599
06/17/2022 19:01:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/17/2022 19:02:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
06/17/2022 19:02:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/17/2022 19:02:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/17/2022 19:02:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/17/2022 19:02:12 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.6875 on epoch=616
06/17/2022 19:02:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/17/2022 19:02:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
06/17/2022 19:02:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/17/2022 19:02:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/17/2022 19:02:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/17/2022 19:02:28 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.6875 on epoch=633
06/17/2022 19:02:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/17/2022 19:02:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
06/17/2022 19:02:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=643
06/17/2022 19:02:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
06/17/2022 19:02:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/17/2022 19:02:44 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.6875 on epoch=649
06/17/2022 19:02:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/17/2022 19:02:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/17/2022 19:02:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
06/17/2022 19:02:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/17/2022 19:02:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/17/2022 19:03:00 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.75 on epoch=666
06/17/2022 19:03:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/17/2022 19:03:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/17/2022 19:03:10 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/17/2022 19:03:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/17/2022 19:03:16 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/17/2022 19:03:17 - INFO - __main__ - Global step 2050 Train loss 0.00 ACC 0.75 on epoch=683
06/17/2022 19:03:20 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
06/17/2022 19:03:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/17/2022 19:03:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/17/2022 19:03:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
06/17/2022 19:03:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/17/2022 19:03:33 - INFO - __main__ - Global step 2100 Train loss 0.01 ACC 0.6875 on epoch=699
06/17/2022 19:03:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/17/2022 19:03:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/17/2022 19:03:42 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/17/2022 19:03:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/17/2022 19:03:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/17/2022 19:03:49 - INFO - __main__ - Global step 2150 Train loss 0.00 ACC 0.75 on epoch=716
06/17/2022 19:03:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/17/2022 19:03:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 19:03:58 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
06/17/2022 19:04:01 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/17/2022 19:04:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/17/2022 19:04:05 - INFO - __main__ - Global step 2200 Train loss 0.00 ACC 0.75 on epoch=733
06/17/2022 19:04:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/17/2022 19:04:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
06/17/2022 19:04:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/17/2022 19:04:17 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/17/2022 19:04:20 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/17/2022 19:04:21 - INFO - __main__ - Global step 2250 Train loss 0.01 ACC 0.6875 on epoch=749
06/17/2022 19:04:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/17/2022 19:04:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/17/2022 19:04:31 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/17/2022 19:04:34 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/17/2022 19:04:37 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/17/2022 19:04:38 - INFO - __main__ - Global step 2300 Train loss 0.00 ACC 0.625 on epoch=766
06/17/2022 19:04:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/17/2022 19:04:44 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/17/2022 19:04:47 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
06/17/2022 19:04:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/17/2022 19:04:53 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/17/2022 19:04:54 - INFO - __main__ - Global step 2350 Train loss 0.00 ACC 0.71875 on epoch=783
06/17/2022 19:04:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/17/2022 19:05:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/17/2022 19:05:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 19:05:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 19:05:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/17/2022 19:05:10 - INFO - __main__ - Global step 2400 Train loss 0.00 ACC 0.6875 on epoch=799
06/17/2022 19:05:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/17/2022 19:05:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 19:05:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/17/2022 19:05:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/17/2022 19:05:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/17/2022 19:05:26 - INFO - __main__ - Global step 2450 Train loss 0.00 ACC 0.65625 on epoch=816
06/17/2022 19:05:29 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/17/2022 19:05:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/17/2022 19:05:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/17/2022 19:05:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/17/2022 19:05:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 19:05:42 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.71875 on epoch=833
06/17/2022 19:05:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 19:05:48 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=839
06/17/2022 19:05:51 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/17/2022 19:05:54 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
06/17/2022 19:05:57 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/17/2022 19:05:58 - INFO - __main__ - Global step 2550 Train loss 0.01 ACC 0.6875 on epoch=849
06/17/2022 19:06:01 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/17/2022 19:06:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 19:06:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=859
06/17/2022 19:06:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 19:06:13 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 19:06:14 - INFO - __main__ - Global step 2600 Train loss 0.00 ACC 0.6875 on epoch=866
06/17/2022 19:06:17 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/17/2022 19:06:20 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/17/2022 19:06:23 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/17/2022 19:06:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/17/2022 19:06:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 19:06:31 - INFO - __main__ - Global step 2650 Train loss 0.00 ACC 0.6875 on epoch=883
06/17/2022 19:06:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 19:06:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 19:06:40 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 19:06:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 19:06:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 19:06:47 - INFO - __main__ - Global step 2700 Train loss 0.00 ACC 0.71875 on epoch=899
06/17/2022 19:06:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 19:06:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 19:06:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 19:06:59 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 19:07:02 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/17/2022 19:07:03 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.71875 on epoch=916
06/17/2022 19:07:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 19:07:09 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 19:07:12 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
06/17/2022 19:07:15 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/17/2022 19:07:18 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=933
06/17/2022 19:07:19 - INFO - __main__ - Global step 2800 Train loss 0.01 ACC 0.8125 on epoch=933
06/17/2022 19:07:19 - INFO - __main__ - Saving model with best ACC: 0.75 -> 0.8125 on epoch=933, global_step=2800
06/17/2022 19:07:22 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 19:07:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/17/2022 19:07:28 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 19:07:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 19:07:35 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 19:07:35 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.6875 on epoch=949
06/17/2022 19:07:39 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/17/2022 19:07:42 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/17/2022 19:07:45 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 19:07:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 19:07:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 19:07:52 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.71875 on epoch=966
06/17/2022 19:07:55 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=969
06/17/2022 19:07:58 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 19:08:01 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/17/2022 19:08:04 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 19:08:07 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 19:08:08 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.71875 on epoch=983
06/17/2022 19:08:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 19:08:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/17/2022 19:08:17 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 19:08:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 19:08:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 19:08:24 - INFO - __main__ - Global step 3000 Train loss 0.00 ACC 0.75 on epoch=999
06/17/2022 19:08:24 - INFO - __main__ - save last model!
06/17/2022 19:08:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 19:08:24 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 19:08:24 - INFO - __main__ - Printing 3 examples
06/17/2022 19:08:24 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 19:08:24 - INFO - __main__ - ['contradiction']
06/17/2022 19:08:24 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 19:08:24 - INFO - __main__ - ['neutral']
06/17/2022 19:08:24 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 19:08:24 - INFO - __main__ - ['entailment']
06/17/2022 19:08:24 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:08:24 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:08:24 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 19:08:25 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 19:08:25 - INFO - __main__ - Printing 3 examples
06/17/2022 19:08:25 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/17/2022 19:08:25 - INFO - __main__ - ['contradiction']
06/17/2022 19:08:25 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/17/2022 19:08:25 - INFO - __main__ - ['contradiction']
06/17/2022 19:08:25 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/17/2022 19:08:25 - INFO - __main__ - ['contradiction']
06/17/2022 19:08:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 19:08:25 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:08:25 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 19:08:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 19:08:25 - INFO - __main__ - Printing 3 examples
06/17/2022 19:08:25 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
06/17/2022 19:08:25 - INFO - __main__ - ['contradiction']
06/17/2022 19:08:25 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
06/17/2022 19:08:25 - INFO - __main__ - ['contradiction']
06/17/2022 19:08:25 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
06/17/2022 19:08:25 - INFO - __main__ - ['contradiction']
06/17/2022 19:08:25 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:08:25 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:08:25 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 19:08:26 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_21_0.5_8_predictions.txt
06/17/2022 19:08:26 - INFO - __main__ - ACC on test data: 0.6250
06/17/2022 19:08:27 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.5, bsz=8, dev_performance=0.8125, test_performance=0.625
06/17/2022 19:08:27 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.4, bsz=8 ...
06/17/2022 19:08:27 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 19:08:27 - INFO - __main__ - Printing 3 examples
06/17/2022 19:08:27 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/17/2022 19:08:27 - INFO - __main__ - ['contradiction']
06/17/2022 19:08:27 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/17/2022 19:08:27 - INFO - __main__ - ['contradiction']
06/17/2022 19:08:27 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/17/2022 19:08:27 - INFO - __main__ - ['contradiction']
06/17/2022 19:08:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 19:08:27 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:08:28 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 19:08:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 19:08:28 - INFO - __main__ - Printing 3 examples
06/17/2022 19:08:28 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
06/17/2022 19:08:28 - INFO - __main__ - ['contradiction']
06/17/2022 19:08:28 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
06/17/2022 19:08:28 - INFO - __main__ - ['contradiction']
06/17/2022 19:08:28 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
06/17/2022 19:08:28 - INFO - __main__ - ['contradiction']
06/17/2022 19:08:28 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:08:28 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:08:28 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 19:08:40 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 19:08:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 19:08:41 - INFO - __main__ - Starting training!
06/17/2022 19:08:45 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 19:08:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 19:08:46 - INFO - __main__ - Starting training!
06/17/2022 19:08:50 - INFO - __main__ - Step 10 Global step 10 Train loss 1.09 on epoch=3
06/17/2022 19:08:53 - INFO - __main__ - Step 20 Global step 20 Train loss 0.48 on epoch=6
06/17/2022 19:08:56 - INFO - __main__ - Step 30 Global step 30 Train loss 0.43 on epoch=9
06/17/2022 19:08:59 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=13
06/17/2022 19:09:02 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=16
06/17/2022 19:09:03 - INFO - __main__ - Global step 50 Train loss 0.59 ACC 0.5 on epoch=16
06/17/2022 19:09:03 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 19:09:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.47 on epoch=19
06/17/2022 19:09:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=23
06/17/2022 19:09:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=26
06/17/2022 19:09:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=29
06/17/2022 19:09:18 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=33
06/17/2022 19:09:19 - INFO - __main__ - Global step 100 Train loss 0.44 ACC 0.5 on epoch=33
06/17/2022 19:09:22 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=36
06/17/2022 19:09:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=39
06/17/2022 19:09:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=43
06/17/2022 19:09:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.38 on epoch=46
06/17/2022 19:09:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.36 on epoch=49
06/17/2022 19:09:35 - INFO - __main__ - Global step 150 Train loss 0.40 ACC 0.59375 on epoch=49
06/17/2022 19:09:35 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=49, global_step=150
06/17/2022 19:09:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=53
06/17/2022 19:09:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=56
06/17/2022 19:09:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=59
06/17/2022 19:09:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=63
06/17/2022 19:09:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=66
06/17/2022 19:09:51 - INFO - __main__ - Global step 200 Train loss 0.38 ACC 0.65625 on epoch=66
06/17/2022 19:09:51 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=66, global_step=200
06/17/2022 19:09:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=69
06/17/2022 19:09:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=73
06/17/2022 19:10:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.32 on epoch=76
06/17/2022 19:10:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=79
06/17/2022 19:10:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.32 on epoch=83
06/17/2022 19:10:08 - INFO - __main__ - Global step 250 Train loss 0.31 ACC 0.65625 on epoch=83
06/17/2022 19:10:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=86
06/17/2022 19:10:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=89
06/17/2022 19:10:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=93
06/17/2022 19:10:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=96
06/17/2022 19:10:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=99
06/17/2022 19:10:24 - INFO - __main__ - Global step 300 Train loss 0.28 ACC 0.5625 on epoch=99
06/17/2022 19:10:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=103
06/17/2022 19:10:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=106
06/17/2022 19:10:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=109
06/17/2022 19:10:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=113
06/17/2022 19:10:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.18 on epoch=116
06/17/2022 19:10:40 - INFO - __main__ - Global step 350 Train loss 0.23 ACC 0.53125 on epoch=116
06/17/2022 19:10:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=119
06/17/2022 19:10:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=123
06/17/2022 19:10:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=126
06/17/2022 19:10:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=129
06/17/2022 19:10:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.20 on epoch=133
06/17/2022 19:10:56 - INFO - __main__ - Global step 400 Train loss 0.23 ACC 0.5 on epoch=133
06/17/2022 19:10:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=136
06/17/2022 19:11:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=139
06/17/2022 19:11:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.20 on epoch=143
06/17/2022 19:11:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.19 on epoch=146
06/17/2022 19:11:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=149
06/17/2022 19:11:12 - INFO - __main__ - Global step 450 Train loss 0.19 ACC 0.5 on epoch=149
06/17/2022 19:11:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.15 on epoch=153
06/17/2022 19:11:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=156
06/17/2022 19:11:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.15 on epoch=159
06/17/2022 19:11:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=163
06/17/2022 19:11:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.18 on epoch=166
06/17/2022 19:11:28 - INFO - __main__ - Global step 500 Train loss 0.17 ACC 0.53125 on epoch=166
06/17/2022 19:11:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=169
06/17/2022 19:11:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=173
06/17/2022 19:11:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.16 on epoch=176
06/17/2022 19:11:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.14 on epoch=179
06/17/2022 19:11:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.11 on epoch=183
06/17/2022 19:11:44 - INFO - __main__ - Global step 550 Train loss 0.13 ACC 0.46875 on epoch=183
06/17/2022 19:11:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.10 on epoch=186
06/17/2022 19:11:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=189
06/17/2022 19:11:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.11 on epoch=193
06/17/2022 19:11:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.09 on epoch=196
06/17/2022 19:11:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=199
06/17/2022 19:12:00 - INFO - __main__ - Global step 600 Train loss 0.11 ACC 0.5 on epoch=199
06/17/2022 19:12:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.11 on epoch=203
06/17/2022 19:12:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=206
06/17/2022 19:12:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=209
06/17/2022 19:12:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=213
06/17/2022 19:12:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=216
06/17/2022 19:12:16 - INFO - __main__ - Global step 650 Train loss 0.11 ACC 0.5 on epoch=216
06/17/2022 19:12:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=219
06/17/2022 19:12:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=223
06/17/2022 19:12:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=226
06/17/2022 19:12:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=229
06/17/2022 19:12:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=233
06/17/2022 19:12:32 - INFO - __main__ - Global step 700 Train loss 0.10 ACC 0.53125 on epoch=233
06/17/2022 19:12:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=236
06/17/2022 19:12:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=239
06/17/2022 19:12:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=243
06/17/2022 19:12:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=246
06/17/2022 19:12:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=249
06/17/2022 19:12:48 - INFO - __main__ - Global step 750 Train loss 0.06 ACC 0.5 on epoch=249
06/17/2022 19:12:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=253
06/17/2022 19:12:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.09 on epoch=256
06/17/2022 19:12:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=259
06/17/2022 19:13:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=263
06/17/2022 19:13:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=266
06/17/2022 19:13:04 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.59375 on epoch=266
06/17/2022 19:13:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=269
06/17/2022 19:13:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=273
06/17/2022 19:13:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=276
06/17/2022 19:13:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=279
06/17/2022 19:13:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=283
06/17/2022 19:13:20 - INFO - __main__ - Global step 850 Train loss 0.05 ACC 0.5 on epoch=283
06/17/2022 19:13:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=286
06/17/2022 19:13:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=289
06/17/2022 19:13:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=293
06/17/2022 19:13:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=296
06/17/2022 19:13:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
06/17/2022 19:13:36 - INFO - __main__ - Global step 900 Train loss 0.05 ACC 0.59375 on epoch=299
06/17/2022 19:13:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=303
06/17/2022 19:13:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=306
06/17/2022 19:13:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=309
06/17/2022 19:13:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=313
06/17/2022 19:13:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=316
06/17/2022 19:13:52 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.59375 on epoch=316
06/17/2022 19:13:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=319
06/17/2022 19:13:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=323
06/17/2022 19:14:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=326
06/17/2022 19:14:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=329
06/17/2022 19:14:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=333
06/17/2022 19:14:09 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.5625 on epoch=333
06/17/2022 19:14:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
06/17/2022 19:14:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=339
06/17/2022 19:14:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
06/17/2022 19:14:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=346
06/17/2022 19:14:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=349
06/17/2022 19:14:25 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.59375 on epoch=349
06/17/2022 19:14:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=353
06/17/2022 19:14:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
06/17/2022 19:14:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=359
06/17/2022 19:14:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=363
06/17/2022 19:14:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=366
06/17/2022 19:14:41 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.59375 on epoch=366
06/17/2022 19:14:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=369
06/17/2022 19:14:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
06/17/2022 19:14:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=376
06/17/2022 19:14:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=379
06/17/2022 19:14:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=383
06/17/2022 19:14:57 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.59375 on epoch=383
06/17/2022 19:15:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
06/17/2022 19:15:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=389
06/17/2022 19:15:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=393
06/17/2022 19:15:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=396
06/17/2022 19:15:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
06/17/2022 19:15:13 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.59375 on epoch=399
06/17/2022 19:15:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=403
06/17/2022 19:15:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=406
06/17/2022 19:15:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=409
06/17/2022 19:15:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=413
06/17/2022 19:15:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
06/17/2022 19:15:29 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.53125 on epoch=416
06/17/2022 19:15:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
06/17/2022 19:15:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=423
06/17/2022 19:15:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
06/17/2022 19:15:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=429
06/17/2022 19:15:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=433
06/17/2022 19:15:45 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.5625 on epoch=433
06/17/2022 19:15:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
06/17/2022 19:15:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/17/2022 19:15:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
06/17/2022 19:15:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
06/17/2022 19:16:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
06/17/2022 19:16:01 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.625 on epoch=449
06/17/2022 19:16:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
06/17/2022 19:16:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=456
06/17/2022 19:16:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
06/17/2022 19:16:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
06/17/2022 19:16:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
06/17/2022 19:16:17 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.6875 on epoch=466
06/17/2022 19:16:17 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=466, global_step=1400
06/17/2022 19:16:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
06/17/2022 19:16:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
06/17/2022 19:16:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
06/17/2022 19:16:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
06/17/2022 19:16:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
06/17/2022 19:16:33 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.75 on epoch=483
06/17/2022 19:16:33 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.75 on epoch=483, global_step=1450
06/17/2022 19:16:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
06/17/2022 19:16:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
06/17/2022 19:16:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
06/17/2022 19:16:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
06/17/2022 19:16:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
06/17/2022 19:16:50 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.65625 on epoch=499
06/17/2022 19:16:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
06/17/2022 19:16:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
06/17/2022 19:16:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
06/17/2022 19:17:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
06/17/2022 19:17:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
06/17/2022 19:17:06 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.59375 on epoch=516
06/17/2022 19:17:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
06/17/2022 19:17:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
06/17/2022 19:17:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=526
06/17/2022 19:17:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/17/2022 19:17:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
06/17/2022 19:17:22 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.625 on epoch=533
06/17/2022 19:17:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
06/17/2022 19:17:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
06/17/2022 19:17:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
06/17/2022 19:17:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/17/2022 19:17:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/17/2022 19:17:38 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.65625 on epoch=549
06/17/2022 19:17:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
06/17/2022 19:17:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=556
06/17/2022 19:17:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
06/17/2022 19:17:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
06/17/2022 19:17:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
06/17/2022 19:17:55 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.6875 on epoch=566
06/17/2022 19:17:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
06/17/2022 19:18:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
06/17/2022 19:18:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
06/17/2022 19:18:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/17/2022 19:18:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/17/2022 19:18:11 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.6875 on epoch=583
06/17/2022 19:18:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
06/17/2022 19:18:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/17/2022 19:18:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/17/2022 19:18:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=596
06/17/2022 19:18:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
06/17/2022 19:18:27 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.59375 on epoch=599
06/17/2022 19:18:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/17/2022 19:18:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/17/2022 19:18:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/17/2022 19:18:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/17/2022 19:18:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/17/2022 19:18:43 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.625 on epoch=616
06/17/2022 19:18:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/17/2022 19:18:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
06/17/2022 19:18:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/17/2022 19:18:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/17/2022 19:18:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/17/2022 19:18:59 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.65625 on epoch=633
06/17/2022 19:19:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
06/17/2022 19:19:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
06/17/2022 19:19:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/17/2022 19:19:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
06/17/2022 19:19:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/17/2022 19:19:16 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.6875 on epoch=649
06/17/2022 19:19:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/17/2022 19:19:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/17/2022 19:19:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/17/2022 19:19:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/17/2022 19:19:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/17/2022 19:19:32 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.71875 on epoch=666
06/17/2022 19:19:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/17/2022 19:19:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
06/17/2022 19:19:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=676
06/17/2022 19:19:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
06/17/2022 19:19:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/17/2022 19:19:48 - INFO - __main__ - Global step 2050 Train loss 0.01 ACC 0.71875 on epoch=683
06/17/2022 19:19:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=686
06/17/2022 19:19:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/17/2022 19:19:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/17/2022 19:20:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/17/2022 19:20:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/17/2022 19:20:04 - INFO - __main__ - Global step 2100 Train loss 0.01 ACC 0.6875 on epoch=699
06/17/2022 19:20:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/17/2022 19:20:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/17/2022 19:20:13 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/17/2022 19:20:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/17/2022 19:20:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/17/2022 19:20:20 - INFO - __main__ - Global step 2150 Train loss 0.01 ACC 0.625 on epoch=716
06/17/2022 19:20:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/17/2022 19:20:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 19:20:29 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
06/17/2022 19:20:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/17/2022 19:20:36 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
06/17/2022 19:20:36 - INFO - __main__ - Global step 2200 Train loss 0.00 ACC 0.625 on epoch=733
06/17/2022 19:20:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/17/2022 19:20:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/17/2022 19:20:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/17/2022 19:20:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/17/2022 19:20:52 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/17/2022 19:20:53 - INFO - __main__ - Global step 2250 Train loss 0.00 ACC 0.65625 on epoch=749
06/17/2022 19:20:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
06/17/2022 19:20:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/17/2022 19:21:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/17/2022 19:21:05 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/17/2022 19:21:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/17/2022 19:21:09 - INFO - __main__ - Global step 2300 Train loss 0.00 ACC 0.65625 on epoch=766
06/17/2022 19:21:12 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/17/2022 19:21:15 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/17/2022 19:21:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/17/2022 19:21:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/17/2022 19:21:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
06/17/2022 19:21:25 - INFO - __main__ - Global step 2350 Train loss 0.01 ACC 0.6875 on epoch=783
06/17/2022 19:21:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/17/2022 19:21:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/17/2022 19:21:34 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 19:21:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 19:21:40 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/17/2022 19:21:41 - INFO - __main__ - Global step 2400 Train loss 0.00 ACC 0.6875 on epoch=799
06/17/2022 19:21:44 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/17/2022 19:21:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 19:21:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/17/2022 19:21:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/17/2022 19:21:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/17/2022 19:21:58 - INFO - __main__ - Global step 2450 Train loss 0.00 ACC 0.65625 on epoch=816
06/17/2022 19:22:01 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/17/2022 19:22:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/17/2022 19:22:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/17/2022 19:22:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/17/2022 19:22:13 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 19:22:14 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.6875 on epoch=833
06/17/2022 19:22:17 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 19:22:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/17/2022 19:22:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/17/2022 19:22:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 19:22:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/17/2022 19:22:30 - INFO - __main__ - Global step 2550 Train loss 0.00 ACC 0.71875 on epoch=849
06/17/2022 19:22:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/17/2022 19:22:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
06/17/2022 19:22:39 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/17/2022 19:22:42 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
06/17/2022 19:22:45 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=866
06/17/2022 19:22:46 - INFO - __main__ - Global step 2600 Train loss 0.01 ACC 0.65625 on epoch=866
06/17/2022 19:22:49 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/17/2022 19:22:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=873
06/17/2022 19:22:55 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
06/17/2022 19:22:58 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=879
06/17/2022 19:23:01 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 19:23:02 - INFO - __main__ - Global step 2650 Train loss 0.02 ACC 0.6875 on epoch=883
06/17/2022 19:23:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 19:23:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 19:23:11 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 19:23:14 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
06/17/2022 19:23:17 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 19:23:18 - INFO - __main__ - Global step 2700 Train loss 0.00 ACC 0.6875 on epoch=899
06/17/2022 19:23:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 19:23:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 19:23:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 19:23:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 19:23:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/17/2022 19:23:35 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.71875 on epoch=916
06/17/2022 19:23:38 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 19:23:41 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=923
06/17/2022 19:23:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/17/2022 19:23:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/17/2022 19:23:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
06/17/2022 19:23:51 - INFO - __main__ - Global step 2800 Train loss 0.01 ACC 0.6875 on epoch=933
06/17/2022 19:23:54 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 19:23:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=939
06/17/2022 19:24:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 19:24:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 19:24:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 19:24:07 - INFO - __main__ - Global step 2850 Train loss 0.01 ACC 0.65625 on epoch=949
06/17/2022 19:24:10 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/17/2022 19:24:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/17/2022 19:24:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 19:24:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 19:24:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 19:24:23 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.6875 on epoch=966
06/17/2022 19:24:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/17/2022 19:24:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 19:24:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=976
06/17/2022 19:24:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 19:24:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 19:24:40 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.65625 on epoch=983
06/17/2022 19:24:43 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
06/17/2022 19:24:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/17/2022 19:24:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 19:24:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 19:24:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 19:24:56 - INFO - __main__ - Global step 3000 Train loss 0.00 ACC 0.71875 on epoch=999
06/17/2022 19:24:56 - INFO - __main__ - save last model!
06/17/2022 19:24:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 19:24:56 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 19:24:56 - INFO - __main__ - Printing 3 examples
06/17/2022 19:24:56 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 19:24:56 - INFO - __main__ - ['contradiction']
06/17/2022 19:24:56 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 19:24:56 - INFO - __main__ - ['neutral']
06/17/2022 19:24:56 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 19:24:56 - INFO - __main__ - ['entailment']
06/17/2022 19:24:56 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:24:56 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:24:56 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 19:24:56 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 19:24:56 - INFO - __main__ - Printing 3 examples
06/17/2022 19:24:56 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/17/2022 19:24:56 - INFO - __main__ - ['contradiction']
06/17/2022 19:24:56 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/17/2022 19:24:56 - INFO - __main__ - ['contradiction']
06/17/2022 19:24:56 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/17/2022 19:24:56 - INFO - __main__ - ['contradiction']
06/17/2022 19:24:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 19:24:56 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:24:56 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 19:24:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 19:24:56 - INFO - __main__ - Printing 3 examples
06/17/2022 19:24:56 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
06/17/2022 19:24:56 - INFO - __main__ - ['contradiction']
06/17/2022 19:24:56 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
06/17/2022 19:24:56 - INFO - __main__ - ['contradiction']
06/17/2022 19:24:56 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
06/17/2022 19:24:56 - INFO - __main__ - ['contradiction']
06/17/2022 19:24:56 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:24:56 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:24:56 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 19:24:58 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_21_0.4_8_predictions.txt
06/17/2022 19:24:58 - INFO - __main__ - ACC on test data: 0.6429
06/17/2022 19:24:58 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.4, bsz=8, dev_performance=0.75, test_performance=0.6428571428571429
06/17/2022 19:24:58 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.3, bsz=8 ...
06/17/2022 19:24:59 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 19:24:59 - INFO - __main__ - Printing 3 examples
06/17/2022 19:24:59 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/17/2022 19:24:59 - INFO - __main__ - ['contradiction']
06/17/2022 19:24:59 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/17/2022 19:24:59 - INFO - __main__ - ['contradiction']
06/17/2022 19:24:59 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/17/2022 19:24:59 - INFO - __main__ - ['contradiction']
06/17/2022 19:24:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 19:24:59 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:24:59 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 19:24:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 19:24:59 - INFO - __main__ - Printing 3 examples
06/17/2022 19:24:59 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
06/17/2022 19:24:59 - INFO - __main__ - ['contradiction']
06/17/2022 19:24:59 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
06/17/2022 19:24:59 - INFO - __main__ - ['contradiction']
06/17/2022 19:24:59 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
06/17/2022 19:24:59 - INFO - __main__ - ['contradiction']
06/17/2022 19:24:59 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:24:59 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:24:59 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 19:25:12 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 19:25:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 19:25:13 - INFO - __main__ - Starting training!
06/17/2022 19:25:17 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 19:25:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 19:25:18 - INFO - __main__ - Starting training!
06/17/2022 19:25:21 - INFO - __main__ - Step 10 Global step 10 Train loss 1.38 on epoch=3
06/17/2022 19:25:24 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=6
06/17/2022 19:25:28 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=9
06/17/2022 19:25:31 - INFO - __main__ - Step 40 Global step 40 Train loss 0.41 on epoch=13
06/17/2022 19:25:34 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=16
06/17/2022 19:25:34 - INFO - __main__ - Global step 50 Train loss 0.67 ACC 0.5 on epoch=16
06/17/2022 19:25:35 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 19:25:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=19
06/17/2022 19:25:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=23
06/17/2022 19:25:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.41 on epoch=26
06/17/2022 19:25:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=29
06/17/2022 19:25:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=33
06/17/2022 19:25:51 - INFO - __main__ - Global step 100 Train loss 0.46 ACC 0.53125 on epoch=33
06/17/2022 19:25:51 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=33, global_step=100
06/17/2022 19:25:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=36
06/17/2022 19:25:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=39
06/17/2022 19:26:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.36 on epoch=43
06/17/2022 19:26:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=46
06/17/2022 19:26:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=49
06/17/2022 19:26:07 - INFO - __main__ - Global step 150 Train loss 0.39 ACC 0.625 on epoch=49
06/17/2022 19:26:07 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.625 on epoch=49, global_step=150
06/17/2022 19:26:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=53
06/17/2022 19:26:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=56
06/17/2022 19:26:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=59
06/17/2022 19:26:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=63
06/17/2022 19:26:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.31 on epoch=66
06/17/2022 19:26:23 - INFO - __main__ - Global step 200 Train loss 0.38 ACC 0.65625 on epoch=66
06/17/2022 19:26:23 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=66, global_step=200
06/17/2022 19:26:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
06/17/2022 19:26:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=73
06/17/2022 19:26:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=76
06/17/2022 19:26:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
06/17/2022 19:26:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=83
06/17/2022 19:26:39 - INFO - __main__ - Global step 250 Train loss 0.39 ACC 0.6875 on epoch=83
06/17/2022 19:26:39 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=83, global_step=250
06/17/2022 19:26:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=86
06/17/2022 19:26:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=89
06/17/2022 19:26:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.31 on epoch=93
06/17/2022 19:26:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=96
06/17/2022 19:26:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=99
06/17/2022 19:26:55 - INFO - __main__ - Global step 300 Train loss 0.35 ACC 0.6875 on epoch=99
06/17/2022 19:26:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=103
06/17/2022 19:27:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.29 on epoch=106
06/17/2022 19:27:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=109
06/17/2022 19:27:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=113
06/17/2022 19:27:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.29 on epoch=116
06/17/2022 19:27:11 - INFO - __main__ - Global step 350 Train loss 0.32 ACC 0.6875 on epoch=116
06/17/2022 19:27:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=119
06/17/2022 19:27:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=123
06/17/2022 19:27:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=126
06/17/2022 19:27:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=129
06/17/2022 19:27:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=133
06/17/2022 19:27:27 - INFO - __main__ - Global step 400 Train loss 0.28 ACC 0.625 on epoch=133
06/17/2022 19:27:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=136
06/17/2022 19:27:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=139
06/17/2022 19:27:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=143
06/17/2022 19:27:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=146
06/17/2022 19:27:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=149
06/17/2022 19:27:44 - INFO - __main__ - Global step 450 Train loss 0.25 ACC 0.625 on epoch=149
06/17/2022 19:27:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=153
06/17/2022 19:27:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=156
06/17/2022 19:27:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=159
06/17/2022 19:27:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=163
06/17/2022 19:27:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=166
06/17/2022 19:28:00 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.53125 on epoch=166
06/17/2022 19:28:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=169
06/17/2022 19:28:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.19 on epoch=173
06/17/2022 19:28:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=176
06/17/2022 19:28:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=179
06/17/2022 19:28:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=183
06/17/2022 19:28:16 - INFO - __main__ - Global step 550 Train loss 0.20 ACC 0.625 on epoch=183
06/17/2022 19:28:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=186
06/17/2022 19:28:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=189
06/17/2022 19:28:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=193
06/17/2022 19:28:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.17 on epoch=196
06/17/2022 19:28:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=199
06/17/2022 19:28:32 - INFO - __main__ - Global step 600 Train loss 0.17 ACC 0.53125 on epoch=199
06/17/2022 19:28:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=203
06/17/2022 19:28:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=206
06/17/2022 19:28:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=209
06/17/2022 19:28:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=213
06/17/2022 19:28:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=216
06/17/2022 19:28:48 - INFO - __main__ - Global step 650 Train loss 0.19 ACC 0.40625 on epoch=216
06/17/2022 19:28:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=219
06/17/2022 19:28:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=223
06/17/2022 19:28:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=226
06/17/2022 19:29:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=229
06/17/2022 19:29:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=233
06/17/2022 19:29:04 - INFO - __main__ - Global step 700 Train loss 0.16 ACC 0.53125 on epoch=233
06/17/2022 19:29:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=236
06/17/2022 19:29:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=239
06/17/2022 19:29:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=243
06/17/2022 19:29:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=246
06/17/2022 19:29:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.13 on epoch=249
06/17/2022 19:29:21 - INFO - __main__ - Global step 750 Train loss 0.14 ACC 0.5625 on epoch=249
06/17/2022 19:29:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.12 on epoch=253
06/17/2022 19:29:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=256
06/17/2022 19:29:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=259
06/17/2022 19:29:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=263
06/17/2022 19:29:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=266
06/17/2022 19:29:37 - INFO - __main__ - Global step 800 Train loss 0.12 ACC 0.46875 on epoch=266
06/17/2022 19:29:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=269
06/17/2022 19:29:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=273
06/17/2022 19:29:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=276
06/17/2022 19:29:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=279
06/17/2022 19:29:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=283
06/17/2022 19:29:53 - INFO - __main__ - Global step 850 Train loss 0.10 ACC 0.53125 on epoch=283
06/17/2022 19:29:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=286
06/17/2022 19:29:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=289
06/17/2022 19:30:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=293
06/17/2022 19:30:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=296
06/17/2022 19:30:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=299
06/17/2022 19:30:09 - INFO - __main__ - Global step 900 Train loss 0.10 ACC 0.375 on epoch=299
06/17/2022 19:30:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=303
06/17/2022 19:30:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=306
06/17/2022 19:30:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=309
06/17/2022 19:30:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=313
06/17/2022 19:30:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=316
06/17/2022 19:30:25 - INFO - __main__ - Global step 950 Train loss 0.09 ACC 0.5625 on epoch=316
06/17/2022 19:30:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=319
06/17/2022 19:30:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=323
06/17/2022 19:30:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=326
06/17/2022 19:30:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=329
06/17/2022 19:30:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=333
06/17/2022 19:30:41 - INFO - __main__ - Global step 1000 Train loss 0.07 ACC 0.5 on epoch=333
06/17/2022 19:30:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=336
06/17/2022 19:30:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=339
06/17/2022 19:30:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=343
06/17/2022 19:30:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
06/17/2022 19:30:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=349
06/17/2022 19:30:57 - INFO - __main__ - Global step 1050 Train loss 0.07 ACC 0.46875 on epoch=349
06/17/2022 19:31:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=353
06/17/2022 19:31:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=356
06/17/2022 19:31:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=359
06/17/2022 19:31:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=363
06/17/2022 19:31:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=366
06/17/2022 19:31:13 - INFO - __main__ - Global step 1100 Train loss 0.07 ACC 0.5625 on epoch=366
06/17/2022 19:31:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=369
06/17/2022 19:31:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=373
06/17/2022 19:31:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
06/17/2022 19:31:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=379
06/17/2022 19:31:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=383
06/17/2022 19:31:29 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.53125 on epoch=383
06/17/2022 19:31:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=386
06/17/2022 19:31:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=389
06/17/2022 19:31:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
06/17/2022 19:31:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=396
06/17/2022 19:31:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=399
06/17/2022 19:31:46 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.53125 on epoch=399
06/17/2022 19:31:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=403
06/17/2022 19:31:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=406
06/17/2022 19:31:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=409
06/17/2022 19:31:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=413
06/17/2022 19:32:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=416
06/17/2022 19:32:02 - INFO - __main__ - Global step 1250 Train loss 0.04 ACC 0.59375 on epoch=416
06/17/2022 19:32:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=419
06/17/2022 19:32:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=423
06/17/2022 19:32:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=426
06/17/2022 19:32:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=429
06/17/2022 19:32:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=433
06/17/2022 19:32:18 - INFO - __main__ - Global step 1300 Train loss 0.04 ACC 0.5625 on epoch=433
06/17/2022 19:32:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=436
06/17/2022 19:32:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
06/17/2022 19:32:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=443
06/17/2022 19:32:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=446
06/17/2022 19:32:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=449
06/17/2022 19:32:34 - INFO - __main__ - Global step 1350 Train loss 0.05 ACC 0.53125 on epoch=449
06/17/2022 19:32:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=453
06/17/2022 19:32:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=456
06/17/2022 19:32:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
06/17/2022 19:32:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
06/17/2022 19:32:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
06/17/2022 19:32:50 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.53125 on epoch=466
06/17/2022 19:32:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
06/17/2022 19:32:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=473
06/17/2022 19:32:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
06/17/2022 19:33:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=479
06/17/2022 19:33:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=483
06/17/2022 19:33:06 - INFO - __main__ - Global step 1450 Train loss 0.04 ACC 0.53125 on epoch=483
06/17/2022 19:33:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
06/17/2022 19:33:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
06/17/2022 19:33:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
06/17/2022 19:33:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
06/17/2022 19:33:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
06/17/2022 19:33:22 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.625 on epoch=499
06/17/2022 19:33:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
06/17/2022 19:33:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
06/17/2022 19:33:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
06/17/2022 19:33:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=513
06/17/2022 19:33:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
06/17/2022 19:33:38 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.46875 on epoch=516
06/17/2022 19:33:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
06/17/2022 19:33:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
06/17/2022 19:33:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
06/17/2022 19:33:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
06/17/2022 19:33:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/17/2022 19:33:54 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.625 on epoch=533
06/17/2022 19:33:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=536
06/17/2022 19:34:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=539
06/17/2022 19:34:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
06/17/2022 19:34:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/17/2022 19:34:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/17/2022 19:34:10 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.59375 on epoch=549
06/17/2022 19:34:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=553
06/17/2022 19:34:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=556
06/17/2022 19:34:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=559
06/17/2022 19:34:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=563
06/17/2022 19:34:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
06/17/2022 19:34:27 - INFO - __main__ - Global step 1700 Train loss 0.04 ACC 0.59375 on epoch=566
06/17/2022 19:34:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
06/17/2022 19:34:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
06/17/2022 19:34:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/17/2022 19:34:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
06/17/2022 19:34:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/17/2022 19:34:43 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.5625 on epoch=583
06/17/2022 19:34:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
06/17/2022 19:34:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/17/2022 19:34:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=593
06/17/2022 19:34:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
06/17/2022 19:34:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=599
06/17/2022 19:34:59 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.625 on epoch=599
06/17/2022 19:35:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
06/17/2022 19:35:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
06/17/2022 19:35:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=609
06/17/2022 19:35:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/17/2022 19:35:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
06/17/2022 19:35:15 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.6875 on epoch=616
06/17/2022 19:35:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=619
06/17/2022 19:35:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
06/17/2022 19:35:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
06/17/2022 19:35:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
06/17/2022 19:35:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/17/2022 19:35:31 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.625 on epoch=633
06/17/2022 19:35:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=636
06/17/2022 19:35:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
06/17/2022 19:35:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/17/2022 19:35:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
06/17/2022 19:35:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
06/17/2022 19:35:48 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.6875 on epoch=649
06/17/2022 19:35:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=653
06/17/2022 19:35:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
06/17/2022 19:35:57 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/17/2022 19:36:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
06/17/2022 19:36:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/17/2022 19:36:04 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.6875 on epoch=666
06/17/2022 19:36:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
06/17/2022 19:36:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=673
06/17/2022 19:36:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
06/17/2022 19:36:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
06/17/2022 19:36:19 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
06/17/2022 19:36:20 - INFO - __main__ - Global step 2050 Train loss 0.02 ACC 0.75 on epoch=683
06/17/2022 19:36:20 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.75 on epoch=683, global_step=2050
06/17/2022 19:36:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
06/17/2022 19:36:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
06/17/2022 19:36:29 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/17/2022 19:36:32 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
06/17/2022 19:36:35 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/17/2022 19:36:36 - INFO - __main__ - Global step 2100 Train loss 0.01 ACC 0.71875 on epoch=699
06/17/2022 19:36:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
06/17/2022 19:36:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/17/2022 19:36:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
06/17/2022 19:36:48 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/17/2022 19:36:51 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/17/2022 19:36:52 - INFO - __main__ - Global step 2150 Train loss 0.01 ACC 0.6875 on epoch=716
06/17/2022 19:36:55 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/17/2022 19:36:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 19:37:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/17/2022 19:37:04 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
06/17/2022 19:37:07 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/17/2022 19:37:08 - INFO - __main__ - Global step 2200 Train loss 0.00 ACC 0.71875 on epoch=733
06/17/2022 19:37:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/17/2022 19:37:14 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=739
06/17/2022 19:37:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/17/2022 19:37:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/17/2022 19:37:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
06/17/2022 19:37:25 - INFO - __main__ - Global step 2250 Train loss 0.01 ACC 0.65625 on epoch=749
06/17/2022 19:37:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/17/2022 19:37:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/17/2022 19:37:34 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/17/2022 19:37:37 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
06/17/2022 19:37:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/17/2022 19:37:41 - INFO - __main__ - Global step 2300 Train loss 0.01 ACC 0.71875 on epoch=766
06/17/2022 19:37:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
06/17/2022 19:37:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/17/2022 19:37:50 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
06/17/2022 19:37:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/17/2022 19:37:56 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/17/2022 19:37:57 - INFO - __main__ - Global step 2350 Train loss 0.01 ACC 0.6875 on epoch=783
06/17/2022 19:38:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/17/2022 19:38:03 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/17/2022 19:38:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=793
06/17/2022 19:38:09 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 19:38:12 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
06/17/2022 19:38:13 - INFO - __main__ - Global step 2400 Train loss 0.01 ACC 0.6875 on epoch=799
06/17/2022 19:38:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/17/2022 19:38:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 19:38:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=809
06/17/2022 19:38:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/17/2022 19:38:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/17/2022 19:38:29 - INFO - __main__ - Global step 2450 Train loss 0.01 ACC 0.6875 on epoch=816
06/17/2022 19:38:32 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/17/2022 19:38:35 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
06/17/2022 19:38:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/17/2022 19:38:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/17/2022 19:38:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
06/17/2022 19:38:46 - INFO - __main__ - Global step 2500 Train loss 0.01 ACC 0.65625 on epoch=833
06/17/2022 19:38:49 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 19:38:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/17/2022 19:38:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
06/17/2022 19:38:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 19:39:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/17/2022 19:39:02 - INFO - __main__ - Global step 2550 Train loss 0.00 ACC 0.6875 on epoch=849
06/17/2022 19:39:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/17/2022 19:39:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
06/17/2022 19:39:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/17/2022 19:39:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 19:39:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 19:39:18 - INFO - __main__ - Global step 2600 Train loss 0.00 ACC 0.71875 on epoch=866
06/17/2022 19:39:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=869
06/17/2022 19:39:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/17/2022 19:39:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/17/2022 19:39:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/17/2022 19:39:33 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=883
06/17/2022 19:39:34 - INFO - __main__ - Global step 2650 Train loss 0.02 ACC 0.65625 on epoch=883
06/17/2022 19:39:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 19:39:40 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 19:39:43 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 19:39:46 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=896
06/17/2022 19:39:50 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 19:39:51 - INFO - __main__ - Global step 2700 Train loss 0.01 ACC 0.65625 on epoch=899
06/17/2022 19:39:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 19:39:57 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 19:40:00 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
06/17/2022 19:40:03 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 19:40:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/17/2022 19:40:07 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.6875 on epoch=916
06/17/2022 19:40:10 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=919
06/17/2022 19:40:13 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
06/17/2022 19:40:16 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/17/2022 19:40:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
06/17/2022 19:40:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 19:40:23 - INFO - __main__ - Global step 2800 Train loss 0.01 ACC 0.6875 on epoch=933
06/17/2022 19:40:26 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 19:40:29 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/17/2022 19:40:32 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 19:40:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 19:40:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 19:40:39 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.6875 on epoch=949
06/17/2022 19:40:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/17/2022 19:40:45 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/17/2022 19:40:48 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 19:40:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 19:40:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 19:40:55 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.71875 on epoch=966
06/17/2022 19:40:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/17/2022 19:41:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=973
06/17/2022 19:41:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/17/2022 19:41:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 19:41:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 19:41:11 - INFO - __main__ - Global step 2950 Train loss 0.01 ACC 0.6875 on epoch=983
06/17/2022 19:41:14 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 19:41:17 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
06/17/2022 19:41:20 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
06/17/2022 19:41:23 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 19:41:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 19:41:27 - INFO - __main__ - Global step 3000 Train loss 0.01 ACC 0.6875 on epoch=999
06/17/2022 19:41:27 - INFO - __main__ - save last model!
06/17/2022 19:41:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 19:41:28 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 19:41:28 - INFO - __main__ - Printing 3 examples
06/17/2022 19:41:28 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 19:41:28 - INFO - __main__ - ['contradiction']
06/17/2022 19:41:28 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 19:41:28 - INFO - __main__ - ['neutral']
06/17/2022 19:41:28 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 19:41:28 - INFO - __main__ - ['entailment']
06/17/2022 19:41:28 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:41:28 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:41:28 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 19:41:28 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 19:41:28 - INFO - __main__ - Printing 3 examples
06/17/2022 19:41:28 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/17/2022 19:41:28 - INFO - __main__ - ['contradiction']
06/17/2022 19:41:28 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/17/2022 19:41:28 - INFO - __main__ - ['contradiction']
06/17/2022 19:41:28 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/17/2022 19:41:28 - INFO - __main__ - ['contradiction']
06/17/2022 19:41:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 19:41:28 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:41:28 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 19:41:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 19:41:28 - INFO - __main__ - Printing 3 examples
06/17/2022 19:41:28 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
06/17/2022 19:41:28 - INFO - __main__ - ['contradiction']
06/17/2022 19:41:28 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
06/17/2022 19:41:28 - INFO - __main__ - ['contradiction']
06/17/2022 19:41:28 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
06/17/2022 19:41:28 - INFO - __main__ - ['contradiction']
06/17/2022 19:41:28 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:41:28 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:41:28 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 19:41:30 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_21_0.3_8_predictions.txt
06/17/2022 19:41:30 - INFO - __main__ - ACC on test data: 0.6071
06/17/2022 19:41:30 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.3, bsz=8, dev_performance=0.75, test_performance=0.6071428571428571
06/17/2022 19:41:30 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.2, bsz=8 ...
06/17/2022 19:41:31 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 19:41:31 - INFO - __main__ - Printing 3 examples
06/17/2022 19:41:31 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/17/2022 19:41:31 - INFO - __main__ - ['contradiction']
06/17/2022 19:41:31 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/17/2022 19:41:31 - INFO - __main__ - ['contradiction']
06/17/2022 19:41:31 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/17/2022 19:41:31 - INFO - __main__ - ['contradiction']
06/17/2022 19:41:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 19:41:31 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:41:31 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 19:41:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 19:41:31 - INFO - __main__ - Printing 3 examples
06/17/2022 19:41:31 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
06/17/2022 19:41:31 - INFO - __main__ - ['contradiction']
06/17/2022 19:41:31 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
06/17/2022 19:41:31 - INFO - __main__ - ['contradiction']
06/17/2022 19:41:31 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
06/17/2022 19:41:31 - INFO - __main__ - ['contradiction']
06/17/2022 19:41:31 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:41:31 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:41:31 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 19:41:43 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 19:41:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 19:41:44 - INFO - __main__ - Starting training!
06/17/2022 19:41:47 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 19:41:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 19:41:48 - INFO - __main__ - Starting training!
06/17/2022 19:41:52 - INFO - __main__ - Step 10 Global step 10 Train loss 1.77 on epoch=3
06/17/2022 19:41:55 - INFO - __main__ - Step 20 Global step 20 Train loss 0.63 on epoch=6
06/17/2022 19:41:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=9
06/17/2022 19:42:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=13
06/17/2022 19:42:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=16
06/17/2022 19:42:05 - INFO - __main__ - Global step 50 Train loss 0.79 ACC 0.5 on epoch=16
06/17/2022 19:42:05 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 19:42:08 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=19
06/17/2022 19:42:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=23
06/17/2022 19:42:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=26
06/17/2022 19:42:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=29
06/17/2022 19:42:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=33
06/17/2022 19:42:21 - INFO - __main__ - Global step 100 Train loss 0.46 ACC 0.5 on epoch=33
06/17/2022 19:42:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=36
06/17/2022 19:42:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=39
06/17/2022 19:42:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=43
06/17/2022 19:42:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=46
06/17/2022 19:42:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=49
06/17/2022 19:42:38 - INFO - __main__ - Global step 150 Train loss 0.44 ACC 0.5 on epoch=49
06/17/2022 19:42:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=53
06/17/2022 19:42:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=56
06/17/2022 19:42:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=59
06/17/2022 19:42:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=63
06/17/2022 19:42:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=66
06/17/2022 19:42:54 - INFO - __main__ - Global step 200 Train loss 0.45 ACC 0.53125 on epoch=66
06/17/2022 19:42:54 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=66, global_step=200
06/17/2022 19:42:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=69
06/17/2022 19:43:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=73
06/17/2022 19:43:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=76
06/17/2022 19:43:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=79
06/17/2022 19:43:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=83
06/17/2022 19:43:10 - INFO - __main__ - Global step 250 Train loss 0.42 ACC 0.5625 on epoch=83
06/17/2022 19:43:10 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=83, global_step=250
06/17/2022 19:43:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=86
06/17/2022 19:43:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=89
06/17/2022 19:43:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=93
06/17/2022 19:43:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=96
06/17/2022 19:43:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=99
06/17/2022 19:43:26 - INFO - __main__ - Global step 300 Train loss 0.39 ACC 0.5625 on epoch=99
06/17/2022 19:43:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=103
06/17/2022 19:43:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=106
06/17/2022 19:43:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=109
06/17/2022 19:43:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=113
06/17/2022 19:43:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=116
06/17/2022 19:43:43 - INFO - __main__ - Global step 350 Train loss 0.38 ACC 0.625 on epoch=116
06/17/2022 19:43:43 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.625 on epoch=116, global_step=350
06/17/2022 19:43:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=119
06/17/2022 19:43:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=123
06/17/2022 19:43:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=126
06/17/2022 19:43:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=129
06/17/2022 19:43:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=133
06/17/2022 19:43:59 - INFO - __main__ - Global step 400 Train loss 0.37 ACC 0.65625 on epoch=133
06/17/2022 19:43:59 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=133, global_step=400
06/17/2022 19:44:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=136
06/17/2022 19:44:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=139
06/17/2022 19:44:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=143
06/17/2022 19:44:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=146
06/17/2022 19:44:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=149
06/17/2022 19:44:15 - INFO - __main__ - Global step 450 Train loss 0.36 ACC 0.75 on epoch=149
06/17/2022 19:44:15 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.75 on epoch=149, global_step=450
06/17/2022 19:44:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.29 on epoch=153
06/17/2022 19:44:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=156
06/17/2022 19:44:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=159
06/17/2022 19:44:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=163
06/17/2022 19:44:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=166
06/17/2022 19:44:32 - INFO - __main__ - Global step 500 Train loss 0.29 ACC 0.75 on epoch=166
06/17/2022 19:44:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=169
06/17/2022 19:44:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.28 on epoch=173
06/17/2022 19:44:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=176
06/17/2022 19:44:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.31 on epoch=179
06/17/2022 19:44:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=183
06/17/2022 19:44:48 - INFO - __main__ - Global step 550 Train loss 0.30 ACC 0.78125 on epoch=183
06/17/2022 19:44:48 - INFO - __main__ - Saving model with best ACC: 0.75 -> 0.78125 on epoch=183, global_step=550
06/17/2022 19:44:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=186
06/17/2022 19:44:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=189
06/17/2022 19:44:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=193
06/17/2022 19:45:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.31 on epoch=196
06/17/2022 19:45:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=199
06/17/2022 19:45:04 - INFO - __main__ - Global step 600 Train loss 0.29 ACC 0.71875 on epoch=199
06/17/2022 19:45:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=203
06/17/2022 19:45:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=206
06/17/2022 19:45:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=209
06/17/2022 19:45:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=213
06/17/2022 19:45:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=216
06/17/2022 19:45:20 - INFO - __main__ - Global step 650 Train loss 0.25 ACC 0.6875 on epoch=216
06/17/2022 19:45:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=219
06/17/2022 19:45:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=223
06/17/2022 19:45:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=226
06/17/2022 19:45:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=229
06/17/2022 19:45:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=233
06/17/2022 19:45:37 - INFO - __main__ - Global step 700 Train loss 0.23 ACC 0.65625 on epoch=233
06/17/2022 19:45:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=236
06/17/2022 19:45:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=239
06/17/2022 19:45:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=243
06/17/2022 19:45:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=246
06/17/2022 19:45:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.18 on epoch=249
06/17/2022 19:45:53 - INFO - __main__ - Global step 750 Train loss 0.22 ACC 0.625 on epoch=249
06/17/2022 19:45:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=253
06/17/2022 19:45:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=256
06/17/2022 19:46:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=259
06/17/2022 19:46:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=263
06/17/2022 19:46:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=266
06/17/2022 19:46:09 - INFO - __main__ - Global step 800 Train loss 0.21 ACC 0.6875 on epoch=266
06/17/2022 19:46:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=269
06/17/2022 19:46:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=273
06/17/2022 19:46:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=276
06/17/2022 19:46:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.17 on epoch=279
06/17/2022 19:46:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=283
06/17/2022 19:46:26 - INFO - __main__ - Global step 850 Train loss 0.18 ACC 0.53125 on epoch=283
06/17/2022 19:46:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=286
06/17/2022 19:46:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.15 on epoch=289
06/17/2022 19:46:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=293
06/17/2022 19:46:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=296
06/17/2022 19:46:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=299
06/17/2022 19:46:42 - INFO - __main__ - Global step 900 Train loss 0.17 ACC 0.5625 on epoch=299
06/17/2022 19:46:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=303
06/17/2022 19:46:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=306
06/17/2022 19:46:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=309
06/17/2022 19:46:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.14 on epoch=313
06/17/2022 19:46:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=316
06/17/2022 19:46:58 - INFO - __main__ - Global step 950 Train loss 0.16 ACC 0.65625 on epoch=316
06/17/2022 19:47:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=319
06/17/2022 19:47:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=323
06/17/2022 19:47:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=326
06/17/2022 19:47:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=329
06/17/2022 19:47:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=333
06/17/2022 19:47:14 - INFO - __main__ - Global step 1000 Train loss 0.16 ACC 0.5625 on epoch=333
06/17/2022 19:47:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.16 on epoch=336
06/17/2022 19:47:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=339
06/17/2022 19:47:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=343
06/17/2022 19:47:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=346
06/17/2022 19:47:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=349
06/17/2022 19:47:31 - INFO - __main__ - Global step 1050 Train loss 0.15 ACC 0.5625 on epoch=349
06/17/2022 19:47:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=353
06/17/2022 19:47:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.15 on epoch=356
06/17/2022 19:47:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=359
06/17/2022 19:47:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=363
06/17/2022 19:47:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=366
06/17/2022 19:47:47 - INFO - __main__ - Global step 1100 Train loss 0.12 ACC 0.5 on epoch=366
06/17/2022 19:47:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=369
06/17/2022 19:47:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=373
06/17/2022 19:47:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=376
06/17/2022 19:47:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=379
06/17/2022 19:48:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=383
06/17/2022 19:48:04 - INFO - __main__ - Global step 1150 Train loss 0.11 ACC 0.5625 on epoch=383
06/17/2022 19:48:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=386
06/17/2022 19:48:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=389
06/17/2022 19:48:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=393
06/17/2022 19:48:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=396
06/17/2022 19:48:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=399
06/17/2022 19:48:20 - INFO - __main__ - Global step 1200 Train loss 0.13 ACC 0.46875 on epoch=399
06/17/2022 19:48:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=403
06/17/2022 19:48:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=406
06/17/2022 19:48:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.10 on epoch=409
06/17/2022 19:48:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=413
06/17/2022 19:48:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=416
06/17/2022 19:48:37 - INFO - __main__ - Global step 1250 Train loss 0.09 ACC 0.59375 on epoch=416
06/17/2022 19:48:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.09 on epoch=419
06/17/2022 19:48:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=423
06/17/2022 19:48:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=426
06/17/2022 19:48:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=429
06/17/2022 19:48:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=433
06/17/2022 19:48:53 - INFO - __main__ - Global step 1300 Train loss 0.09 ACC 0.59375 on epoch=433
06/17/2022 19:48:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=436
06/17/2022 19:48:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=439
06/17/2022 19:49:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=443
06/17/2022 19:49:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=446
06/17/2022 19:49:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=449
06/17/2022 19:49:10 - INFO - __main__ - Global step 1350 Train loss 0.08 ACC 0.5 on epoch=449
06/17/2022 19:49:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=453
06/17/2022 19:49:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=456
06/17/2022 19:49:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=459
06/17/2022 19:49:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=463
06/17/2022 19:49:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=466
06/17/2022 19:49:26 - INFO - __main__ - Global step 1400 Train loss 0.09 ACC 0.4375 on epoch=466
06/17/2022 19:49:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=469
06/17/2022 19:49:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=473
06/17/2022 19:49:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=476
06/17/2022 19:49:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=479
06/17/2022 19:49:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=483
06/17/2022 19:49:42 - INFO - __main__ - Global step 1450 Train loss 0.08 ACC 0.53125 on epoch=483
06/17/2022 19:49:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=486
06/17/2022 19:49:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
06/17/2022 19:49:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=493
06/17/2022 19:49:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=496
06/17/2022 19:49:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=499
06/17/2022 19:49:59 - INFO - __main__ - Global step 1500 Train loss 0.06 ACC 0.5 on epoch=499
06/17/2022 19:50:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=503
06/17/2022 19:50:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
06/17/2022 19:50:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=509
06/17/2022 19:50:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=513
06/17/2022 19:50:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=516
06/17/2022 19:50:15 - INFO - __main__ - Global step 1550 Train loss 0.05 ACC 0.4375 on epoch=516
06/17/2022 19:50:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
06/17/2022 19:50:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=523
06/17/2022 19:50:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
06/17/2022 19:50:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=529
06/17/2022 19:50:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
06/17/2022 19:50:32 - INFO - __main__ - Global step 1600 Train loss 0.05 ACC 0.5 on epoch=533
06/17/2022 19:50:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
06/17/2022 19:50:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=539
06/17/2022 19:50:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=543
06/17/2022 19:50:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=546
06/17/2022 19:50:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=549
06/17/2022 19:50:48 - INFO - __main__ - Global step 1650 Train loss 0.06 ACC 0.5 on epoch=549
06/17/2022 19:50:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=553
06/17/2022 19:50:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=556
06/17/2022 19:50:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
06/17/2022 19:51:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=563
06/17/2022 19:51:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=566
06/17/2022 19:51:05 - INFO - __main__ - Global step 1700 Train loss 0.04 ACC 0.5625 on epoch=566
06/17/2022 19:51:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=569
06/17/2022 19:51:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=573
06/17/2022 19:51:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=576
06/17/2022 19:51:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/17/2022 19:51:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=583
06/17/2022 19:51:21 - INFO - __main__ - Global step 1750 Train loss 0.04 ACC 0.53125 on epoch=583
06/17/2022 19:51:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
06/17/2022 19:51:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=589
06/17/2022 19:51:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
06/17/2022 19:51:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=596
06/17/2022 19:51:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=599
06/17/2022 19:51:38 - INFO - __main__ - Global step 1800 Train loss 0.05 ACC 0.53125 on epoch=599
06/17/2022 19:51:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
06/17/2022 19:51:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
06/17/2022 19:51:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=609
06/17/2022 19:51:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=613
06/17/2022 19:51:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=616
06/17/2022 19:51:54 - INFO - __main__ - Global step 1850 Train loss 0.04 ACC 0.5625 on epoch=616
06/17/2022 19:51:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
06/17/2022 19:52:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
06/17/2022 19:52:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=626
06/17/2022 19:52:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
06/17/2022 19:52:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/17/2022 19:52:11 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.59375 on epoch=633
06/17/2022 19:52:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
06/17/2022 19:52:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=639
06/17/2022 19:52:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=643
06/17/2022 19:52:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=646
06/17/2022 19:52:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
06/17/2022 19:52:27 - INFO - __main__ - Global step 1950 Train loss 0.03 ACC 0.59375 on epoch=649
06/17/2022 19:52:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
06/17/2022 19:52:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
06/17/2022 19:52:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
06/17/2022 19:52:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
06/17/2022 19:52:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
06/17/2022 19:52:44 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.59375 on epoch=666
06/17/2022 19:52:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=669
06/17/2022 19:52:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
06/17/2022 19:52:53 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
06/17/2022 19:52:56 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
06/17/2022 19:52:59 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=683
06/17/2022 19:53:00 - INFO - __main__ - Global step 2050 Train loss 0.03 ACC 0.59375 on epoch=683
06/17/2022 19:53:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=686
06/17/2022 19:53:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
06/17/2022 19:53:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/17/2022 19:53:12 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
06/17/2022 19:53:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/17/2022 19:53:17 - INFO - __main__ - Global step 2100 Train loss 0.03 ACC 0.59375 on epoch=699
06/17/2022 19:53:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
06/17/2022 19:53:23 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=706
06/17/2022 19:53:26 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/17/2022 19:53:29 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/17/2022 19:53:32 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=716
06/17/2022 19:53:33 - INFO - __main__ - Global step 2150 Train loss 0.02 ACC 0.625 on epoch=716
06/17/2022 19:53:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/17/2022 19:53:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
06/17/2022 19:53:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
06/17/2022 19:53:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
06/17/2022 19:53:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
06/17/2022 19:53:50 - INFO - __main__ - Global step 2200 Train loss 0.02 ACC 0.59375 on epoch=733
06/17/2022 19:53:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=736
06/17/2022 19:53:56 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=739
06/17/2022 19:53:59 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/17/2022 19:54:02 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/17/2022 19:54:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
06/17/2022 19:54:06 - INFO - __main__ - Global step 2250 Train loss 0.02 ACC 0.65625 on epoch=749
06/17/2022 19:54:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=753
06/17/2022 19:54:12 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/17/2022 19:54:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=759
06/17/2022 19:54:19 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=763
06/17/2022 19:54:22 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/17/2022 19:54:23 - INFO - __main__ - Global step 2300 Train loss 0.02 ACC 0.625 on epoch=766
06/17/2022 19:54:26 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
06/17/2022 19:54:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/17/2022 19:54:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
06/17/2022 19:54:35 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/17/2022 19:54:38 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/17/2022 19:54:39 - INFO - __main__ - Global step 2350 Train loss 0.02 ACC 0.59375 on epoch=783
06/17/2022 19:54:42 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
06/17/2022 19:54:45 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/17/2022 19:54:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=793
06/17/2022 19:54:51 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
06/17/2022 19:54:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=799
06/17/2022 19:54:56 - INFO - __main__ - Global step 2400 Train loss 0.02 ACC 0.625 on epoch=799
06/17/2022 19:54:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
06/17/2022 19:55:02 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=806
06/17/2022 19:55:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/17/2022 19:55:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/17/2022 19:55:11 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=816
06/17/2022 19:55:12 - INFO - __main__ - Global step 2450 Train loss 0.02 ACC 0.625 on epoch=816
06/17/2022 19:55:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=819
06/17/2022 19:55:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
06/17/2022 19:55:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/17/2022 19:55:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/17/2022 19:55:28 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
06/17/2022 19:55:29 - INFO - __main__ - Global step 2500 Train loss 0.02 ACC 0.6875 on epoch=833
06/17/2022 19:55:32 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 19:55:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
06/17/2022 19:55:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
06/17/2022 19:55:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=846
06/17/2022 19:55:44 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/17/2022 19:55:45 - INFO - __main__ - Global step 2550 Train loss 0.01 ACC 0.65625 on epoch=849
06/17/2022 19:55:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
06/17/2022 19:55:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 19:55:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
06/17/2022 19:55:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/17/2022 19:56:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=866
06/17/2022 19:56:02 - INFO - __main__ - Global step 2600 Train loss 0.01 ACC 0.625 on epoch=866
06/17/2022 19:56:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
06/17/2022 19:56:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=873
06/17/2022 19:56:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/17/2022 19:56:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/17/2022 19:56:17 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
06/17/2022 19:56:18 - INFO - __main__ - Global step 2650 Train loss 0.01 ACC 0.6875 on epoch=883
06/17/2022 19:56:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
06/17/2022 19:56:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
06/17/2022 19:56:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
06/17/2022 19:56:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 19:56:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
06/17/2022 19:56:35 - INFO - __main__ - Global step 2700 Train loss 0.01 ACC 0.625 on epoch=899
06/17/2022 19:56:38 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 19:56:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 19:56:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
06/17/2022 19:56:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 19:56:50 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/17/2022 19:56:51 - INFO - __main__ - Global step 2750 Train loss 0.01 ACC 0.59375 on epoch=916
06/17/2022 19:56:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
06/17/2022 19:56:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 19:57:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
06/17/2022 19:57:04 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
06/17/2022 19:57:07 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 19:57:08 - INFO - __main__ - Global step 2800 Train loss 0.01 ACC 0.65625 on epoch=933
06/17/2022 19:57:11 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 19:57:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
06/17/2022 19:57:17 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
06/17/2022 19:57:20 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 19:57:23 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
06/17/2022 19:57:24 - INFO - __main__ - Global step 2850 Train loss 0.01 ACC 0.625 on epoch=949
06/17/2022 19:57:27 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
06/17/2022 19:57:30 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/17/2022 19:57:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 19:57:37 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
06/17/2022 19:57:40 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
06/17/2022 19:57:41 - INFO - __main__ - Global step 2900 Train loss 0.01 ACC 0.625 on epoch=966
06/17/2022 19:57:44 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/17/2022 19:57:47 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 19:57:50 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
06/17/2022 19:57:53 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=979
06/17/2022 19:57:56 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 19:57:57 - INFO - __main__ - Global step 2950 Train loss 0.01 ACC 0.65625 on epoch=983
06/17/2022 19:58:00 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 19:58:03 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
06/17/2022 19:58:06 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=993
06/17/2022 19:58:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=996
06/17/2022 19:58:12 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 19:58:13 - INFO - __main__ - Global step 3000 Train loss 0.01 ACC 0.65625 on epoch=999
06/17/2022 19:58:13 - INFO - __main__ - save last model!
06/17/2022 19:58:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 19:58:13 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 19:58:13 - INFO - __main__ - Printing 3 examples
06/17/2022 19:58:13 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 19:58:13 - INFO - __main__ - ['contradiction']
06/17/2022 19:58:13 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 19:58:13 - INFO - __main__ - ['neutral']
06/17/2022 19:58:13 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 19:58:13 - INFO - __main__ - ['entailment']
06/17/2022 19:58:13 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:58:13 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:58:14 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 19:58:14 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 19:58:14 - INFO - __main__ - Printing 3 examples
06/17/2022 19:58:14 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/17/2022 19:58:14 - INFO - __main__ - ['contradiction']
06/17/2022 19:58:14 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/17/2022 19:58:14 - INFO - __main__ - ['contradiction']
06/17/2022 19:58:14 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/17/2022 19:58:14 - INFO - __main__ - ['contradiction']
06/17/2022 19:58:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 19:58:14 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:58:14 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 19:58:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 19:58:14 - INFO - __main__ - Printing 3 examples
06/17/2022 19:58:14 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
06/17/2022 19:58:14 - INFO - __main__ - ['contradiction']
06/17/2022 19:58:14 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
06/17/2022 19:58:14 - INFO - __main__ - ['contradiction']
06/17/2022 19:58:14 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
06/17/2022 19:58:14 - INFO - __main__ - ['contradiction']
06/17/2022 19:58:14 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:58:14 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:58:14 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 19:58:16 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_21_0.2_8_predictions.txt
06/17/2022 19:58:16 - INFO - __main__ - ACC on test data: 0.6250
06/17/2022 19:58:16 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.2, bsz=8, dev_performance=0.78125, test_performance=0.625
06/17/2022 19:58:16 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.5, bsz=8 ...
06/17/2022 19:58:17 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 19:58:17 - INFO - __main__ - Printing 3 examples
06/17/2022 19:58:17 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/17/2022 19:58:17 - INFO - __main__ - ['contradiction']
06/17/2022 19:58:17 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/17/2022 19:58:17 - INFO - __main__ - ['contradiction']
06/17/2022 19:58:17 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/17/2022 19:58:17 - INFO - __main__ - ['contradiction']
06/17/2022 19:58:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 19:58:17 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:58:17 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 19:58:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 19:58:17 - INFO - __main__ - Printing 3 examples
06/17/2022 19:58:17 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
06/17/2022 19:58:17 - INFO - __main__ - ['contradiction']
06/17/2022 19:58:17 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
06/17/2022 19:58:17 - INFO - __main__ - ['contradiction']
06/17/2022 19:58:17 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
06/17/2022 19:58:17 - INFO - __main__ - ['contradiction']
06/17/2022 19:58:17 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:58:17 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:58:17 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 19:58:29 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 19:58:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 19:58:30 - INFO - __main__ - Starting training!
06/17/2022 19:58:35 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 19:58:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 19:58:35 - INFO - __main__ - Starting training!
06/17/2022 19:58:39 - INFO - __main__ - Step 10 Global step 10 Train loss 1.08 on epoch=3
06/17/2022 19:58:42 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=6
06/17/2022 19:58:45 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=9
06/17/2022 19:58:48 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=13
06/17/2022 19:58:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=16
06/17/2022 19:58:52 - INFO - __main__ - Global step 50 Train loss 0.62 ACC 0.5625 on epoch=16
06/17/2022 19:58:53 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5625 on epoch=16, global_step=50
06/17/2022 19:58:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.39 on epoch=19
06/17/2022 19:58:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.41 on epoch=23
06/17/2022 19:59:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=26
06/17/2022 19:59:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=29
06/17/2022 19:59:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=33
06/17/2022 19:59:09 - INFO - __main__ - Global step 100 Train loss 0.41 ACC 0.65625 on epoch=33
06/17/2022 19:59:09 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.65625 on epoch=33, global_step=100
06/17/2022 19:59:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=36
06/17/2022 19:59:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=39
06/17/2022 19:59:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.38 on epoch=43
06/17/2022 19:59:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.34 on epoch=46
06/17/2022 19:59:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=49
06/17/2022 19:59:25 - INFO - __main__ - Global step 150 Train loss 0.39 ACC 0.34375 on epoch=49
06/17/2022 19:59:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.31 on epoch=53
06/17/2022 19:59:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=56
06/17/2022 19:59:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.35 on epoch=59
06/17/2022 19:59:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=63
06/17/2022 19:59:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.31 on epoch=66
06/17/2022 19:59:41 - INFO - __main__ - Global step 200 Train loss 0.33 ACC 0.34375 on epoch=66
06/17/2022 19:59:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.29 on epoch=69
06/17/2022 19:59:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.25 on epoch=73
06/17/2022 19:59:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=76
06/17/2022 19:59:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=79
06/17/2022 19:59:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=83
06/17/2022 19:59:57 - INFO - __main__ - Global step 250 Train loss 0.27 ACC 0.375 on epoch=83
06/17/2022 20:00:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=86
06/17/2022 20:00:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.19 on epoch=89
06/17/2022 20:00:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=93
06/17/2022 20:00:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.20 on epoch=96
06/17/2022 20:00:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=99
06/17/2022 20:00:13 - INFO - __main__ - Global step 300 Train loss 0.22 ACC 0.28125 on epoch=99
06/17/2022 20:00:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=103
06/17/2022 20:00:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.20 on epoch=106
06/17/2022 20:00:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.15 on epoch=109
06/17/2022 20:00:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=113
06/17/2022 20:00:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.14 on epoch=116
06/17/2022 20:00:29 - INFO - __main__ - Global step 350 Train loss 0.20 ACC 0.34375 on epoch=116
06/17/2022 20:00:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.18 on epoch=119
06/17/2022 20:00:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.20 on epoch=123
06/17/2022 20:00:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.15 on epoch=126
06/17/2022 20:00:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.14 on epoch=129
06/17/2022 20:00:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.15 on epoch=133
06/17/2022 20:00:45 - INFO - __main__ - Global step 400 Train loss 0.16 ACC 0.3125 on epoch=133
06/17/2022 20:00:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=136
06/17/2022 20:00:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.14 on epoch=139
06/17/2022 20:00:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.19 on epoch=143
06/17/2022 20:00:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.12 on epoch=146
06/17/2022 20:01:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.08 on epoch=149
06/17/2022 20:01:01 - INFO - __main__ - Global step 450 Train loss 0.13 ACC 0.34375 on epoch=149
06/17/2022 20:01:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=153
06/17/2022 20:01:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.12 on epoch=156
06/17/2022 20:01:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.08 on epoch=159
06/17/2022 20:01:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=163
06/17/2022 20:01:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.14 on epoch=166
06/17/2022 20:01:17 - INFO - __main__ - Global step 500 Train loss 0.09 ACC 0.3125 on epoch=166
06/17/2022 20:01:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=169
06/17/2022 20:01:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=173
06/17/2022 20:01:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.10 on epoch=176
06/17/2022 20:01:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=179
06/17/2022 20:01:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=183
06/17/2022 20:01:33 - INFO - __main__ - Global step 550 Train loss 0.07 ACC 0.3125 on epoch=183
06/17/2022 20:01:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=186
06/17/2022 20:01:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=189
06/17/2022 20:01:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=193
06/17/2022 20:01:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=196
06/17/2022 20:01:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=199
06/17/2022 20:01:49 - INFO - __main__ - Global step 600 Train loss 0.05 ACC 0.40625 on epoch=199
06/17/2022 20:01:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=203
06/17/2022 20:01:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=206
06/17/2022 20:01:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=209
06/17/2022 20:02:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.05 on epoch=213
06/17/2022 20:02:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=216
06/17/2022 20:02:05 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.375 on epoch=216
06/17/2022 20:02:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=219
06/17/2022 20:02:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.07 on epoch=223
06/17/2022 20:02:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=226
06/17/2022 20:02:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=229
06/17/2022 20:02:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=233
06/17/2022 20:02:21 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.40625 on epoch=233
06/17/2022 20:02:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=236
06/17/2022 20:02:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=239
06/17/2022 20:02:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=243
06/17/2022 20:02:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=246
06/17/2022 20:02:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=249
06/17/2022 20:02:37 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.40625 on epoch=249
06/17/2022 20:02:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=253
06/17/2022 20:02:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=256
06/17/2022 20:02:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=259
06/17/2022 20:02:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=263
06/17/2022 20:02:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=266
06/17/2022 20:02:53 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.5 on epoch=266
06/17/2022 20:02:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=269
06/17/2022 20:02:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=273
06/17/2022 20:03:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=276
06/17/2022 20:03:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=279
06/17/2022 20:03:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=283
06/17/2022 20:03:09 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.53125 on epoch=283
06/17/2022 20:03:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=286
06/17/2022 20:03:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=289
06/17/2022 20:03:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=293
06/17/2022 20:03:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=296
06/17/2022 20:03:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=299
06/17/2022 20:03:25 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.40625 on epoch=299
06/17/2022 20:03:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=303
06/17/2022 20:03:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=306
06/17/2022 20:03:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=309
06/17/2022 20:03:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=313
06/17/2022 20:03:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=316
06/17/2022 20:03:41 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.46875 on epoch=316
06/17/2022 20:03:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=319
06/17/2022 20:03:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=323
06/17/2022 20:03:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=326
06/17/2022 20:03:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=329
06/17/2022 20:03:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=333
06/17/2022 20:03:57 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.46875 on epoch=333
06/17/2022 20:04:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=336
06/17/2022 20:04:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=339
06/17/2022 20:04:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=343
06/17/2022 20:04:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=346
06/17/2022 20:04:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=349
06/17/2022 20:04:14 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.5 on epoch=349
06/17/2022 20:04:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=353
06/17/2022 20:04:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
06/17/2022 20:04:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
06/17/2022 20:04:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=363
06/17/2022 20:04:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=366
06/17/2022 20:04:30 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.53125 on epoch=366
06/17/2022 20:04:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
06/17/2022 20:04:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
06/17/2022 20:04:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
06/17/2022 20:04:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=379
06/17/2022 20:04:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
06/17/2022 20:04:46 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.5625 on epoch=383
06/17/2022 20:04:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=386
06/17/2022 20:04:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=389
06/17/2022 20:04:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=393
06/17/2022 20:04:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=396
06/17/2022 20:05:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
06/17/2022 20:05:02 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.40625 on epoch=399
06/17/2022 20:05:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
06/17/2022 20:05:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
06/17/2022 20:05:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=409
06/17/2022 20:05:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=413
06/17/2022 20:05:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=416
06/17/2022 20:05:18 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.5 on epoch=416
06/17/2022 20:05:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=419
06/17/2022 20:05:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=423
06/17/2022 20:05:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
06/17/2022 20:05:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
06/17/2022 20:05:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
06/17/2022 20:05:34 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.5625 on epoch=433
06/17/2022 20:05:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
06/17/2022 20:05:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=439
06/17/2022 20:05:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=443
06/17/2022 20:05:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
06/17/2022 20:05:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
06/17/2022 20:05:50 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.53125 on epoch=449
06/17/2022 20:05:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
06/17/2022 20:05:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=456
06/17/2022 20:05:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=459
06/17/2022 20:06:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=463
06/17/2022 20:06:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=466
06/17/2022 20:06:06 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.59375 on epoch=466
06/17/2022 20:06:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
06/17/2022 20:06:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
06/17/2022 20:06:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
06/17/2022 20:06:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
06/17/2022 20:06:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
06/17/2022 20:06:22 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.5625 on epoch=483
06/17/2022 20:06:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
06/17/2022 20:06:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
06/17/2022 20:06:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
06/17/2022 20:06:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
06/17/2022 20:06:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
06/17/2022 20:06:38 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.625 on epoch=499
06/17/2022 20:06:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
06/17/2022 20:06:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
06/17/2022 20:06:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=509
06/17/2022 20:06:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
06/17/2022 20:06:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
06/17/2022 20:06:53 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.5625 on epoch=516
06/17/2022 20:06:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/17/2022 20:06:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
06/17/2022 20:07:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
06/17/2022 20:07:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/17/2022 20:07:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
06/17/2022 20:07:09 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.59375 on epoch=533
06/17/2022 20:07:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
06/17/2022 20:07:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
06/17/2022 20:07:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
06/17/2022 20:07:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
06/17/2022 20:07:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/17/2022 20:07:26 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.59375 on epoch=549
06/17/2022 20:07:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
06/17/2022 20:07:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
06/17/2022 20:07:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
06/17/2022 20:07:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
06/17/2022 20:07:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
06/17/2022 20:07:42 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.59375 on epoch=566
06/17/2022 20:07:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
06/17/2022 20:07:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=573
06/17/2022 20:07:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
06/17/2022 20:07:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/17/2022 20:07:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/17/2022 20:07:57 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.5625 on epoch=583
06/17/2022 20:08:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/17/2022 20:08:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/17/2022 20:08:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/17/2022 20:08:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/17/2022 20:08:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
06/17/2022 20:08:13 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.625 on epoch=599
06/17/2022 20:08:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/17/2022 20:08:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/17/2022 20:08:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/17/2022 20:08:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/17/2022 20:08:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
06/17/2022 20:08:29 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.59375 on epoch=616
06/17/2022 20:08:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/17/2022 20:08:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
06/17/2022 20:08:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/17/2022 20:08:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/17/2022 20:08:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/17/2022 20:08:45 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.625 on epoch=633
06/17/2022 20:08:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/17/2022 20:08:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
06/17/2022 20:08:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/17/2022 20:08:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/17/2022 20:09:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/17/2022 20:09:01 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.5625 on epoch=649
06/17/2022 20:09:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/17/2022 20:09:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/17/2022 20:09:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/17/2022 20:09:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/17/2022 20:09:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/17/2022 20:09:17 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.59375 on epoch=666
06/17/2022 20:09:20 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/17/2022 20:09:23 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/17/2022 20:09:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/17/2022 20:09:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/17/2022 20:09:32 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/17/2022 20:09:33 - INFO - __main__ - Global step 2050 Train loss 0.00 ACC 0.625 on epoch=683
06/17/2022 20:09:36 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/17/2022 20:09:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/17/2022 20:09:42 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/17/2022 20:09:45 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/17/2022 20:09:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/17/2022 20:09:49 - INFO - __main__ - Global step 2100 Train loss 0.00 ACC 0.53125 on epoch=699
06/17/2022 20:09:52 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
06/17/2022 20:09:55 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=706
06/17/2022 20:09:58 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/17/2022 20:10:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/17/2022 20:10:04 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/17/2022 20:10:05 - INFO - __main__ - Global step 2150 Train loss 0.01 ACC 0.5625 on epoch=716
06/17/2022 20:10:08 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/17/2022 20:10:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 20:10:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/17/2022 20:10:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/17/2022 20:10:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/17/2022 20:10:20 - INFO - __main__ - Global step 2200 Train loss 0.00 ACC 0.5 on epoch=733
06/17/2022 20:10:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/17/2022 20:10:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
06/17/2022 20:10:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/17/2022 20:10:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/17/2022 20:10:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/17/2022 20:10:36 - INFO - __main__ - Global step 2250 Train loss 0.00 ACC 0.59375 on epoch=749
06/17/2022 20:10:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/17/2022 20:10:42 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/17/2022 20:10:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/17/2022 20:10:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/17/2022 20:10:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/17/2022 20:10:52 - INFO - __main__ - Global step 2300 Train loss 0.00 ACC 0.59375 on epoch=766
06/17/2022 20:10:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/17/2022 20:10:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/17/2022 20:11:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/17/2022 20:11:04 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/17/2022 20:11:07 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=783
06/17/2022 20:11:08 - INFO - __main__ - Global step 2350 Train loss 0.01 ACC 0.46875 on epoch=783
06/17/2022 20:11:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/17/2022 20:11:14 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/17/2022 20:11:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 20:11:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 20:11:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/17/2022 20:11:24 - INFO - __main__ - Global step 2400 Train loss 0.00 ACC 0.5 on epoch=799
06/17/2022 20:11:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/17/2022 20:11:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 20:11:33 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/17/2022 20:11:36 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/17/2022 20:11:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/17/2022 20:11:40 - INFO - __main__ - Global step 2450 Train loss 0.00 ACC 0.53125 on epoch=816
06/17/2022 20:11:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/17/2022 20:11:46 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/17/2022 20:11:49 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/17/2022 20:11:51 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=829
06/17/2022 20:11:54 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 20:11:55 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.59375 on epoch=833
06/17/2022 20:11:58 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 20:12:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
06/17/2022 20:12:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/17/2022 20:12:07 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 20:12:10 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/17/2022 20:12:11 - INFO - __main__ - Global step 2550 Train loss 0.01 ACC 0.5 on epoch=849
06/17/2022 20:12:14 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=853
06/17/2022 20:12:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
06/17/2022 20:12:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=859
06/17/2022 20:12:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 20:12:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 20:12:27 - INFO - __main__ - Global step 2600 Train loss 0.04 ACC 0.5625 on epoch=866
06/17/2022 20:12:30 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/17/2022 20:12:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/17/2022 20:12:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/17/2022 20:12:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/17/2022 20:12:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 20:12:43 - INFO - __main__ - Global step 2650 Train loss 0.00 ACC 0.59375 on epoch=883
06/17/2022 20:12:46 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 20:12:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 20:12:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 20:12:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 20:12:58 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 20:12:59 - INFO - __main__ - Global step 2700 Train loss 0.00 ACC 0.625 on epoch=899
06/17/2022 20:13:02 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 20:13:05 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 20:13:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 20:13:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 20:13:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/17/2022 20:13:15 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.46875 on epoch=916
06/17/2022 20:13:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 20:13:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=923
06/17/2022 20:13:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/17/2022 20:13:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/17/2022 20:13:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 20:13:30 - INFO - __main__ - Global step 2800 Train loss 0.01 ACC 0.59375 on epoch=933
06/17/2022 20:13:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 20:13:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/17/2022 20:13:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 20:13:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 20:13:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 20:13:46 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.59375 on epoch=949
06/17/2022 20:13:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/17/2022 20:13:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
06/17/2022 20:13:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 20:13:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 20:14:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 20:14:02 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.625 on epoch=966
06/17/2022 20:14:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/17/2022 20:14:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 20:14:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=976
06/17/2022 20:14:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 20:14:17 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 20:14:18 - INFO - __main__ - Global step 2950 Train loss 0.01 ACC 0.59375 on epoch=983
06/17/2022 20:14:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 20:14:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/17/2022 20:14:27 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 20:14:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 20:14:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 20:14:34 - INFO - __main__ - Global step 3000 Train loss 0.00 ACC 0.625 on epoch=999
06/17/2022 20:14:34 - INFO - __main__ - save last model!
06/17/2022 20:14:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 20:14:34 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 20:14:34 - INFO - __main__ - Printing 3 examples
06/17/2022 20:14:34 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 20:14:34 - INFO - __main__ - ['contradiction']
06/17/2022 20:14:34 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 20:14:34 - INFO - __main__ - ['neutral']
06/17/2022 20:14:34 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 20:14:34 - INFO - __main__ - ['entailment']
06/17/2022 20:14:34 - INFO - __main__ - Tokenizing Input ...
06/17/2022 20:14:34 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:14:34 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 20:14:34 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 20:14:34 - INFO - __main__ - Printing 3 examples
06/17/2022 20:14:34 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/17/2022 20:14:34 - INFO - __main__ - ['contradiction']
06/17/2022 20:14:34 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/17/2022 20:14:34 - INFO - __main__ - ['contradiction']
06/17/2022 20:14:34 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/17/2022 20:14:34 - INFO - __main__ - ['contradiction']
06/17/2022 20:14:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 20:14:34 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:14:34 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 20:14:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 20:14:34 - INFO - __main__ - Printing 3 examples
06/17/2022 20:14:34 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
06/17/2022 20:14:34 - INFO - __main__ - ['contradiction']
06/17/2022 20:14:34 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
06/17/2022 20:14:34 - INFO - __main__ - ['contradiction']
06/17/2022 20:14:34 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
06/17/2022 20:14:34 - INFO - __main__ - ['contradiction']
06/17/2022 20:14:34 - INFO - __main__ - Tokenizing Input ...
06/17/2022 20:14:34 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:14:35 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 20:14:36 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_42_0.5_8_predictions.txt
06/17/2022 20:14:36 - INFO - __main__ - ACC on test data: 0.6250
06/17/2022 20:14:37 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.5, bsz=8, dev_performance=0.65625, test_performance=0.625
06/17/2022 20:14:37 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.4, bsz=8 ...
06/17/2022 20:14:38 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 20:14:38 - INFO - __main__ - Printing 3 examples
06/17/2022 20:14:38 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/17/2022 20:14:38 - INFO - __main__ - ['contradiction']
06/17/2022 20:14:38 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/17/2022 20:14:38 - INFO - __main__ - ['contradiction']
06/17/2022 20:14:38 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/17/2022 20:14:38 - INFO - __main__ - ['contradiction']
06/17/2022 20:14:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 20:14:38 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:14:38 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 20:14:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 20:14:38 - INFO - __main__ - Printing 3 examples
06/17/2022 20:14:38 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
06/17/2022 20:14:38 - INFO - __main__ - ['contradiction']
06/17/2022 20:14:38 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
06/17/2022 20:14:38 - INFO - __main__ - ['contradiction']
06/17/2022 20:14:38 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
06/17/2022 20:14:38 - INFO - __main__ - ['contradiction']
06/17/2022 20:14:38 - INFO - __main__ - Tokenizing Input ...
06/17/2022 20:14:38 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:14:38 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 20:14:53 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 20:14:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 20:14:54 - INFO - __main__ - Starting training!
06/17/2022 20:14:54 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 20:14:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 20:14:55 - INFO - __main__ - Starting training!
06/17/2022 20:14:59 - INFO - __main__ - Step 10 Global step 10 Train loss 1.19 on epoch=3
06/17/2022 20:15:02 - INFO - __main__ - Step 20 Global step 20 Train loss 0.57 on epoch=6
06/17/2022 20:15:05 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=9
06/17/2022 20:15:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=13
06/17/2022 20:15:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.40 on epoch=16
06/17/2022 20:15:13 - INFO - __main__ - Global step 50 Train loss 0.65 ACC 0.5 on epoch=16
06/17/2022 20:15:13 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 20:15:16 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=19
06/17/2022 20:15:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=23
06/17/2022 20:15:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=26
06/17/2022 20:15:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=29
06/17/2022 20:15:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=33
06/17/2022 20:15:29 - INFO - __main__ - Global step 100 Train loss 0.45 ACC 0.625 on epoch=33
06/17/2022 20:15:29 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.625 on epoch=33, global_step=100
06/17/2022 20:15:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=36
06/17/2022 20:15:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=39
06/17/2022 20:15:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=43
06/17/2022 20:15:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=46
06/17/2022 20:15:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=49
06/17/2022 20:15:45 - INFO - __main__ - Global step 150 Train loss 0.41 ACC 0.59375 on epoch=49
06/17/2022 20:15:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=53
06/17/2022 20:15:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.33 on epoch=56
06/17/2022 20:15:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.35 on epoch=59
06/17/2022 20:15:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=63
06/17/2022 20:16:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=66
06/17/2022 20:16:01 - INFO - __main__ - Global step 200 Train loss 0.35 ACC 0.5 on epoch=66
06/17/2022 20:16:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=69
06/17/2022 20:16:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.33 on epoch=73
06/17/2022 20:16:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.33 on epoch=76
06/17/2022 20:16:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.30 on epoch=79
06/17/2022 20:16:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=83
06/17/2022 20:16:17 - INFO - __main__ - Global step 250 Train loss 0.31 ACC 0.4375 on epoch=83
06/17/2022 20:16:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.23 on epoch=86
06/17/2022 20:16:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=89
06/17/2022 20:16:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.31 on epoch=93
06/17/2022 20:16:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=96
06/17/2022 20:16:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.20 on epoch=99
06/17/2022 20:16:33 - INFO - __main__ - Global step 300 Train loss 0.27 ACC 0.375 on epoch=99
06/17/2022 20:16:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=103
06/17/2022 20:16:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=106
06/17/2022 20:16:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=109
06/17/2022 20:16:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=113
06/17/2022 20:16:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=116
06/17/2022 20:16:50 - INFO - __main__ - Global step 350 Train loss 0.23 ACC 0.34375 on epoch=116
06/17/2022 20:16:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=119
06/17/2022 20:16:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=123
06/17/2022 20:16:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=126
06/17/2022 20:17:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.19 on epoch=129
06/17/2022 20:17:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.20 on epoch=133
06/17/2022 20:17:06 - INFO - __main__ - Global step 400 Train loss 0.22 ACC 0.375 on epoch=133
06/17/2022 20:17:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.17 on epoch=136
06/17/2022 20:17:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.19 on epoch=139
06/17/2022 20:17:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.15 on epoch=143
06/17/2022 20:17:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.13 on epoch=146
06/17/2022 20:17:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=149
06/17/2022 20:17:22 - INFO - __main__ - Global step 450 Train loss 0.16 ACC 0.375 on epoch=149
06/17/2022 20:17:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.16 on epoch=153
06/17/2022 20:17:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.16 on epoch=156
06/17/2022 20:17:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.17 on epoch=159
06/17/2022 20:17:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.10 on epoch=163
06/17/2022 20:17:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=166
06/17/2022 20:17:38 - INFO - __main__ - Global step 500 Train loss 0.15 ACC 0.40625 on epoch=166
06/17/2022 20:17:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=169
06/17/2022 20:17:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.16 on epoch=173
06/17/2022 20:17:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=176
06/17/2022 20:17:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.12 on epoch=179
06/17/2022 20:17:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.13 on epoch=183
06/17/2022 20:17:54 - INFO - __main__ - Global step 550 Train loss 0.14 ACC 0.375 on epoch=183
06/17/2022 20:17:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.14 on epoch=186
06/17/2022 20:18:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.11 on epoch=189
06/17/2022 20:18:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=193
06/17/2022 20:18:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.09 on epoch=196
06/17/2022 20:18:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=199
06/17/2022 20:18:11 - INFO - __main__ - Global step 600 Train loss 0.12 ACC 0.375 on epoch=199
06/17/2022 20:18:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=203
06/17/2022 20:18:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=206
06/17/2022 20:18:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=209
06/17/2022 20:18:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.10 on epoch=213
06/17/2022 20:18:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=216
06/17/2022 20:18:27 - INFO - __main__ - Global step 650 Train loss 0.11 ACC 0.34375 on epoch=216
06/17/2022 20:18:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=219
06/17/2022 20:18:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=223
06/17/2022 20:18:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=226
06/17/2022 20:18:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=229
06/17/2022 20:18:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=233
06/17/2022 20:18:43 - INFO - __main__ - Global step 700 Train loss 0.09 ACC 0.3125 on epoch=233
06/17/2022 20:18:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=236
06/17/2022 20:18:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=239
06/17/2022 20:18:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=243
06/17/2022 20:18:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=246
06/17/2022 20:18:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=249
06/17/2022 20:18:59 - INFO - __main__ - Global step 750 Train loss 0.07 ACC 0.34375 on epoch=249
06/17/2022 20:19:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=253
06/17/2022 20:19:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=256
06/17/2022 20:19:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=259
06/17/2022 20:19:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=263
06/17/2022 20:19:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=266
06/17/2022 20:19:15 - INFO - __main__ - Global step 800 Train loss 0.04 ACC 0.34375 on epoch=266
06/17/2022 20:19:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=269
06/17/2022 20:19:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=273
06/17/2022 20:19:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=276
06/17/2022 20:19:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=279
06/17/2022 20:19:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=283
06/17/2022 20:19:31 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.40625 on epoch=283
06/17/2022 20:19:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=286
06/17/2022 20:19:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=289
06/17/2022 20:19:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=293
06/17/2022 20:19:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=296
06/17/2022 20:19:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
06/17/2022 20:19:47 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.4375 on epoch=299
06/17/2022 20:19:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=303
06/17/2022 20:19:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=306
06/17/2022 20:19:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=309
06/17/2022 20:19:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=313
06/17/2022 20:20:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=316
06/17/2022 20:20:03 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.40625 on epoch=316
06/17/2022 20:20:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=319
06/17/2022 20:20:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=323
06/17/2022 20:20:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=326
06/17/2022 20:20:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=329
06/17/2022 20:20:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=333
06/17/2022 20:20:20 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.46875 on epoch=333
06/17/2022 20:20:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=336
06/17/2022 20:20:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=339
06/17/2022 20:20:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=343
06/17/2022 20:20:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=346
06/17/2022 20:20:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=349
06/17/2022 20:20:36 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.46875 on epoch=349
06/17/2022 20:20:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=353
06/17/2022 20:20:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=356
06/17/2022 20:20:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
06/17/2022 20:20:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=363
06/17/2022 20:20:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
06/17/2022 20:20:52 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.46875 on epoch=366
06/17/2022 20:20:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=369
06/17/2022 20:20:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
06/17/2022 20:21:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
06/17/2022 20:21:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=379
06/17/2022 20:21:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
06/17/2022 20:21:08 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.46875 on epoch=383
06/17/2022 20:21:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=386
06/17/2022 20:21:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
06/17/2022 20:21:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
06/17/2022 20:21:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
06/17/2022 20:21:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
06/17/2022 20:21:24 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.40625 on epoch=399
06/17/2022 20:21:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
06/17/2022 20:21:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=406
06/17/2022 20:21:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
06/17/2022 20:21:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
06/17/2022 20:21:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
06/17/2022 20:21:40 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.375 on epoch=416
06/17/2022 20:21:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=419
06/17/2022 20:21:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=423
06/17/2022 20:21:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
06/17/2022 20:21:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=429
06/17/2022 20:21:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
06/17/2022 20:21:56 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.4375 on epoch=433
06/17/2022 20:22:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
06/17/2022 20:22:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/17/2022 20:22:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
06/17/2022 20:22:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
06/17/2022 20:22:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
06/17/2022 20:22:13 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.4375 on epoch=449
06/17/2022 20:22:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=453
06/17/2022 20:22:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
06/17/2022 20:22:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
06/17/2022 20:22:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
06/17/2022 20:22:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
06/17/2022 20:22:29 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.375 on epoch=466
06/17/2022 20:22:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
06/17/2022 20:22:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
06/17/2022 20:22:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
06/17/2022 20:22:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
06/17/2022 20:22:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
06/17/2022 20:22:45 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.375 on epoch=483
06/17/2022 20:22:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
06/17/2022 20:22:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
06/17/2022 20:22:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
06/17/2022 20:22:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
06/17/2022 20:23:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
06/17/2022 20:23:01 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.40625 on epoch=499
06/17/2022 20:23:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
06/17/2022 20:23:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
06/17/2022 20:23:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
06/17/2022 20:23:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
06/17/2022 20:23:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
06/17/2022 20:23:17 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.4375 on epoch=516
06/17/2022 20:23:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/17/2022 20:23:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
06/17/2022 20:23:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
06/17/2022 20:23:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/17/2022 20:23:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
06/17/2022 20:23:33 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.46875 on epoch=533
06/17/2022 20:23:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
06/17/2022 20:23:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=539
06/17/2022 20:23:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
06/17/2022 20:23:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
06/17/2022 20:23:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
06/17/2022 20:23:50 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.4375 on epoch=549
06/17/2022 20:23:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
06/17/2022 20:23:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
06/17/2022 20:23:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/17/2022 20:24:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
06/17/2022 20:24:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
06/17/2022 20:24:06 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.4375 on epoch=566
06/17/2022 20:24:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
06/17/2022 20:24:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
06/17/2022 20:24:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
06/17/2022 20:24:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
06/17/2022 20:24:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/17/2022 20:24:22 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.4375 on epoch=583
06/17/2022 20:24:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/17/2022 20:24:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/17/2022 20:24:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/17/2022 20:24:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/17/2022 20:24:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
06/17/2022 20:24:38 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.375 on epoch=599
06/17/2022 20:24:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/17/2022 20:24:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
06/17/2022 20:24:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/17/2022 20:24:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=613
06/17/2022 20:24:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/17/2022 20:24:54 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.4375 on epoch=616
06/17/2022 20:24:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/17/2022 20:25:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
06/17/2022 20:25:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/17/2022 20:25:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/17/2022 20:25:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/17/2022 20:25:10 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.3125 on epoch=633
06/17/2022 20:25:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/17/2022 20:25:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/17/2022 20:25:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=643
06/17/2022 20:25:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
06/17/2022 20:25:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/17/2022 20:25:27 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.375 on epoch=649
06/17/2022 20:25:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/17/2022 20:25:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/17/2022 20:25:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/17/2022 20:25:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/17/2022 20:25:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/17/2022 20:25:43 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.375 on epoch=666
06/17/2022 20:25:46 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
06/17/2022 20:25:49 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/17/2022 20:25:52 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/17/2022 20:25:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
06/17/2022 20:25:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/17/2022 20:25:59 - INFO - __main__ - Global step 2050 Train loss 0.01 ACC 0.375 on epoch=683
06/17/2022 20:26:02 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/17/2022 20:26:05 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/17/2022 20:26:08 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/17/2022 20:26:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/17/2022 20:26:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=699
06/17/2022 20:26:15 - INFO - __main__ - Global step 2100 Train loss 0.01 ACC 0.40625 on epoch=699
06/17/2022 20:26:18 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/17/2022 20:26:21 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/17/2022 20:26:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/17/2022 20:26:27 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/17/2022 20:26:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/17/2022 20:26:31 - INFO - __main__ - Global step 2150 Train loss 0.00 ACC 0.40625 on epoch=716
06/17/2022 20:26:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/17/2022 20:26:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 20:26:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/17/2022 20:26:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/17/2022 20:26:47 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/17/2022 20:26:48 - INFO - __main__ - Global step 2200 Train loss 0.00 ACC 0.46875 on epoch=733
06/17/2022 20:26:51 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=736
06/17/2022 20:26:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/17/2022 20:26:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/17/2022 20:27:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/17/2022 20:27:03 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/17/2022 20:27:04 - INFO - __main__ - Global step 2250 Train loss 0.01 ACC 0.4375 on epoch=749
06/17/2022 20:27:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/17/2022 20:27:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/17/2022 20:27:13 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/17/2022 20:27:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
06/17/2022 20:27:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/17/2022 20:27:20 - INFO - __main__ - Global step 2300 Train loss 0.00 ACC 0.4375 on epoch=766
06/17/2022 20:27:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/17/2022 20:27:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/17/2022 20:27:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/17/2022 20:27:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/17/2022 20:27:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/17/2022 20:27:37 - INFO - __main__ - Global step 2350 Train loss 0.00 ACC 0.4375 on epoch=783
06/17/2022 20:27:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/17/2022 20:27:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/17/2022 20:27:46 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 20:27:49 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 20:27:52 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/17/2022 20:27:53 - INFO - __main__ - Global step 2400 Train loss 0.00 ACC 0.375 on epoch=799
06/17/2022 20:27:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
06/17/2022 20:27:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 20:28:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/17/2022 20:28:05 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/17/2022 20:28:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/17/2022 20:28:09 - INFO - __main__ - Global step 2450 Train loss 0.00 ACC 0.40625 on epoch=816
06/17/2022 20:28:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/17/2022 20:28:15 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/17/2022 20:28:18 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/17/2022 20:28:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/17/2022 20:28:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 20:28:25 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.375 on epoch=833
06/17/2022 20:28:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 20:28:31 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/17/2022 20:28:34 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
06/17/2022 20:28:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 20:28:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/17/2022 20:28:41 - INFO - __main__ - Global step 2550 Train loss 0.00 ACC 0.40625 on epoch=849
06/17/2022 20:28:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/17/2022 20:28:48 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 20:28:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/17/2022 20:28:54 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 20:28:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 20:28:58 - INFO - __main__ - Global step 2600 Train loss 0.00 ACC 0.40625 on epoch=866
06/17/2022 20:29:01 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/17/2022 20:29:04 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/17/2022 20:29:07 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
06/17/2022 20:29:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/17/2022 20:29:13 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 20:29:14 - INFO - __main__ - Global step 2650 Train loss 0.00 ACC 0.40625 on epoch=883
06/17/2022 20:29:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 20:29:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 20:29:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=893
06/17/2022 20:29:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 20:29:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 20:29:30 - INFO - __main__ - Global step 2700 Train loss 0.01 ACC 0.375 on epoch=899
06/17/2022 20:29:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 20:29:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 20:29:39 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 20:29:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 20:29:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/17/2022 20:29:46 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.40625 on epoch=916
06/17/2022 20:29:49 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 20:29:52 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 20:29:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/17/2022 20:29:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/17/2022 20:30:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 20:30:02 - INFO - __main__ - Global step 2800 Train loss 0.00 ACC 0.375 on epoch=933
06/17/2022 20:30:06 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 20:30:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/17/2022 20:30:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=943
06/17/2022 20:30:15 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 20:30:18 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 20:30:19 - INFO - __main__ - Global step 2850 Train loss 0.01 ACC 0.375 on epoch=949
06/17/2022 20:30:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/17/2022 20:30:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/17/2022 20:30:28 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
06/17/2022 20:30:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 20:30:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 20:30:35 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.40625 on epoch=966
06/17/2022 20:30:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=969
06/17/2022 20:30:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 20:30:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
06/17/2022 20:30:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 20:30:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 20:30:51 - INFO - __main__ - Global step 2950 Train loss 0.02 ACC 0.34375 on epoch=983
06/17/2022 20:30:54 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 20:30:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/17/2022 20:31:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 20:31:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 20:31:06 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 20:31:07 - INFO - __main__ - Global step 3000 Train loss 0.00 ACC 0.34375 on epoch=999
06/17/2022 20:31:07 - INFO - __main__ - save last model!
06/17/2022 20:31:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 20:31:07 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 20:31:07 - INFO - __main__ - Printing 3 examples
06/17/2022 20:31:07 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 20:31:07 - INFO - __main__ - ['contradiction']
06/17/2022 20:31:07 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 20:31:07 - INFO - __main__ - ['neutral']
06/17/2022 20:31:07 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 20:31:07 - INFO - __main__ - ['entailment']
06/17/2022 20:31:07 - INFO - __main__ - Tokenizing Input ...
06/17/2022 20:31:07 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:31:07 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 20:31:07 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 20:31:07 - INFO - __main__ - Printing 3 examples
06/17/2022 20:31:07 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/17/2022 20:31:07 - INFO - __main__ - ['contradiction']
06/17/2022 20:31:07 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/17/2022 20:31:07 - INFO - __main__ - ['contradiction']
06/17/2022 20:31:07 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/17/2022 20:31:07 - INFO - __main__ - ['contradiction']
06/17/2022 20:31:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 20:31:08 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:31:08 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 20:31:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 20:31:08 - INFO - __main__ - Printing 3 examples
06/17/2022 20:31:08 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
06/17/2022 20:31:08 - INFO - __main__ - ['contradiction']
06/17/2022 20:31:08 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
06/17/2022 20:31:08 - INFO - __main__ - ['contradiction']
06/17/2022 20:31:08 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
06/17/2022 20:31:08 - INFO - __main__ - ['contradiction']
06/17/2022 20:31:08 - INFO - __main__ - Tokenizing Input ...
06/17/2022 20:31:08 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:31:08 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 20:31:09 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_42_0.4_8_predictions.txt
06/17/2022 20:31:09 - INFO - __main__ - ACC on test data: 0.5000
06/17/2022 20:31:10 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.4, bsz=8, dev_performance=0.625, test_performance=0.5
06/17/2022 20:31:10 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.3, bsz=8 ...
06/17/2022 20:31:11 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 20:31:11 - INFO - __main__ - Printing 3 examples
06/17/2022 20:31:11 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/17/2022 20:31:11 - INFO - __main__ - ['contradiction']
06/17/2022 20:31:11 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/17/2022 20:31:11 - INFO - __main__ - ['contradiction']
06/17/2022 20:31:11 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/17/2022 20:31:11 - INFO - __main__ - ['contradiction']
06/17/2022 20:31:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 20:31:11 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:31:11 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 20:31:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 20:31:11 - INFO - __main__ - Printing 3 examples
06/17/2022 20:31:11 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
06/17/2022 20:31:11 - INFO - __main__ - ['contradiction']
06/17/2022 20:31:11 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
06/17/2022 20:31:11 - INFO - __main__ - ['contradiction']
06/17/2022 20:31:11 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
06/17/2022 20:31:11 - INFO - __main__ - ['contradiction']
06/17/2022 20:31:11 - INFO - __main__ - Tokenizing Input ...
06/17/2022 20:31:11 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:31:11 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 20:31:26 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 20:31:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 20:31:27 - INFO - __main__ - Starting training!
06/17/2022 20:31:27 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 20:31:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 20:31:28 - INFO - __main__ - Starting training!
06/17/2022 20:31:32 - INFO - __main__ - Step 10 Global step 10 Train loss 1.31 on epoch=3
06/17/2022 20:31:35 - INFO - __main__ - Step 20 Global step 20 Train loss 0.65 on epoch=6
06/17/2022 20:31:38 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=9
06/17/2022 20:31:41 - INFO - __main__ - Step 40 Global step 40 Train loss 0.54 on epoch=13
06/17/2022 20:31:44 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=16
06/17/2022 20:31:45 - INFO - __main__ - Global step 50 Train loss 0.71 ACC 0.5 on epoch=16
06/17/2022 20:31:45 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 20:31:48 - INFO - __main__ - Step 60 Global step 60 Train loss 0.47 on epoch=19
06/17/2022 20:31:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=23
06/17/2022 20:31:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=26
06/17/2022 20:31:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=29
06/17/2022 20:32:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=33
06/17/2022 20:32:01 - INFO - __main__ - Global step 100 Train loss 0.46 ACC 0.53125 on epoch=33
06/17/2022 20:32:01 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=33, global_step=100
06/17/2022 20:32:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=36
06/17/2022 20:32:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.41 on epoch=39
06/17/2022 20:32:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.38 on epoch=43
06/17/2022 20:32:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=46
06/17/2022 20:32:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
06/17/2022 20:32:17 - INFO - __main__ - Global step 150 Train loss 0.42 ACC 0.59375 on epoch=49
06/17/2022 20:32:17 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=49, global_step=150
06/17/2022 20:32:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=53
06/17/2022 20:32:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=56
06/17/2022 20:32:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=59
06/17/2022 20:32:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=63
06/17/2022 20:32:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=66
06/17/2022 20:32:33 - INFO - __main__ - Global step 200 Train loss 0.39 ACC 0.5625 on epoch=66
06/17/2022 20:32:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=69
06/17/2022 20:32:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=73
06/17/2022 20:32:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=76
06/17/2022 20:32:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=79
06/17/2022 20:32:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=83
06/17/2022 20:32:49 - INFO - __main__ - Global step 250 Train loss 0.38 ACC 0.4375 on epoch=83
06/17/2022 20:32:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.34 on epoch=86
06/17/2022 20:32:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.32 on epoch=89
06/17/2022 20:32:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.35 on epoch=93
06/17/2022 20:33:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=96
06/17/2022 20:33:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.35 on epoch=99
06/17/2022 20:33:05 - INFO - __main__ - Global step 300 Train loss 0.34 ACC 0.34375 on epoch=99
06/17/2022 20:33:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=103
06/17/2022 20:33:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=106
06/17/2022 20:33:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=109
06/17/2022 20:33:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=113
06/17/2022 20:33:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=116
06/17/2022 20:33:21 - INFO - __main__ - Global step 350 Train loss 0.29 ACC 0.34375 on epoch=116
06/17/2022 20:33:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=119
06/17/2022 20:33:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=123
06/17/2022 20:33:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=126
06/17/2022 20:33:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.31 on epoch=129
06/17/2022 20:33:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=133
06/17/2022 20:33:37 - INFO - __main__ - Global step 400 Train loss 0.26 ACC 0.34375 on epoch=133
06/17/2022 20:33:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=136
06/17/2022 20:33:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=139
06/17/2022 20:33:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=143
06/17/2022 20:33:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=146
06/17/2022 20:33:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.17 on epoch=149
06/17/2022 20:33:53 - INFO - __main__ - Global step 450 Train loss 0.22 ACC 0.375 on epoch=149
06/17/2022 20:33:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=153
06/17/2022 20:33:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=156
06/17/2022 20:34:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=159
06/17/2022 20:34:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.17 on epoch=163
06/17/2022 20:34:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=166
06/17/2022 20:34:09 - INFO - __main__ - Global step 500 Train loss 0.21 ACC 0.34375 on epoch=166
06/17/2022 20:34:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=169
06/17/2022 20:34:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=173
06/17/2022 20:34:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.16 on epoch=176
06/17/2022 20:34:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=179
06/17/2022 20:34:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.12 on epoch=183
06/17/2022 20:34:24 - INFO - __main__ - Global step 550 Train loss 0.18 ACC 0.34375 on epoch=183
06/17/2022 20:34:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=186
06/17/2022 20:34:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=189
06/17/2022 20:34:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=193
06/17/2022 20:34:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.16 on epoch=196
06/17/2022 20:34:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.16 on epoch=199
06/17/2022 20:34:40 - INFO - __main__ - Global step 600 Train loss 0.16 ACC 0.28125 on epoch=199
06/17/2022 20:34:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=203
06/17/2022 20:34:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=206
06/17/2022 20:34:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.14 on epoch=209
06/17/2022 20:34:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=213
06/17/2022 20:34:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=216
06/17/2022 20:34:56 - INFO - __main__ - Global step 650 Train loss 0.15 ACC 0.28125 on epoch=216
06/17/2022 20:34:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=219
06/17/2022 20:35:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=223
06/17/2022 20:35:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.15 on epoch=226
06/17/2022 20:35:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=229
06/17/2022 20:35:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.10 on epoch=233
06/17/2022 20:35:12 - INFO - __main__ - Global step 700 Train loss 0.11 ACC 0.375 on epoch=233
06/17/2022 20:35:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=236
06/17/2022 20:35:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=239
06/17/2022 20:35:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=243
06/17/2022 20:35:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=246
06/17/2022 20:35:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=249
06/17/2022 20:35:28 - INFO - __main__ - Global step 750 Train loss 0.12 ACC 0.375 on epoch=249
06/17/2022 20:35:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=253
06/17/2022 20:35:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=256
06/17/2022 20:35:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.11 on epoch=259
06/17/2022 20:35:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=263
06/17/2022 20:35:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=266
06/17/2022 20:35:43 - INFO - __main__ - Global step 800 Train loss 0.10 ACC 0.25 on epoch=266
06/17/2022 20:35:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=269
06/17/2022 20:35:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=273
06/17/2022 20:35:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=276
06/17/2022 20:35:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=279
06/17/2022 20:35:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=283
06/17/2022 20:35:59 - INFO - __main__ - Global step 850 Train loss 0.08 ACC 0.3125 on epoch=283
06/17/2022 20:36:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=286
06/17/2022 20:36:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=289
06/17/2022 20:36:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=293
06/17/2022 20:36:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=296
06/17/2022 20:36:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=299
06/17/2022 20:36:15 - INFO - __main__ - Global step 900 Train loss 0.07 ACC 0.3125 on epoch=299
06/17/2022 20:36:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=303
06/17/2022 20:36:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=306
06/17/2022 20:36:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=309
06/17/2022 20:36:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=313
06/17/2022 20:36:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=316
06/17/2022 20:36:31 - INFO - __main__ - Global step 950 Train loss 0.07 ACC 0.3125 on epoch=316
06/17/2022 20:36:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=319
06/17/2022 20:36:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=323
06/17/2022 20:36:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
06/17/2022 20:36:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=329
06/17/2022 20:36:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=333
06/17/2022 20:36:47 - INFO - __main__ - Global step 1000 Train loss 0.06 ACC 0.34375 on epoch=333
06/17/2022 20:36:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
06/17/2022 20:36:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=339
06/17/2022 20:36:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=343
06/17/2022 20:36:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=346
06/17/2022 20:37:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=349
06/17/2022 20:37:03 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.3125 on epoch=349
06/17/2022 20:37:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=353
06/17/2022 20:37:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=356
06/17/2022 20:37:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=359
06/17/2022 20:37:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=363
06/17/2022 20:37:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=366
06/17/2022 20:37:19 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.34375 on epoch=366
06/17/2022 20:37:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=369
06/17/2022 20:37:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=373
06/17/2022 20:37:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=376
06/17/2022 20:37:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=379
06/17/2022 20:37:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=383
06/17/2022 20:37:35 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.34375 on epoch=383
06/17/2022 20:37:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=386
06/17/2022 20:37:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
06/17/2022 20:37:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=393
06/17/2022 20:37:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=396
06/17/2022 20:37:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
06/17/2022 20:37:51 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.375 on epoch=399
06/17/2022 20:37:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
06/17/2022 20:37:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=406
06/17/2022 20:38:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=409
06/17/2022 20:38:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=413
06/17/2022 20:38:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
06/17/2022 20:38:07 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.375 on epoch=416
06/17/2022 20:38:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
06/17/2022 20:38:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
06/17/2022 20:38:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=426
06/17/2022 20:38:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
06/17/2022 20:38:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
06/17/2022 20:38:23 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.40625 on epoch=433
06/17/2022 20:38:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
06/17/2022 20:38:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
06/17/2022 20:38:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
06/17/2022 20:38:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
06/17/2022 20:38:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=449
06/17/2022 20:38:39 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.40625 on epoch=449
06/17/2022 20:38:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
06/17/2022 20:38:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=456
06/17/2022 20:38:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
06/17/2022 20:38:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
06/17/2022 20:38:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=466
06/17/2022 20:38:54 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.46875 on epoch=466
06/17/2022 20:38:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
06/17/2022 20:39:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=473
06/17/2022 20:39:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=476
06/17/2022 20:39:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
06/17/2022 20:39:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
06/17/2022 20:39:10 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.40625 on epoch=483
06/17/2022 20:39:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
06/17/2022 20:39:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
06/17/2022 20:39:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
06/17/2022 20:39:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
06/17/2022 20:39:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=499
06/17/2022 20:39:26 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.46875 on epoch=499
06/17/2022 20:39:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=503
06/17/2022 20:39:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
06/17/2022 20:39:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
06/17/2022 20:39:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=513
06/17/2022 20:39:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=516
06/17/2022 20:39:42 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.4375 on epoch=516
06/17/2022 20:39:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
06/17/2022 20:39:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
06/17/2022 20:39:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
06/17/2022 20:39:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
06/17/2022 20:39:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/17/2022 20:39:58 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.46875 on epoch=533
06/17/2022 20:40:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
06/17/2022 20:40:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
06/17/2022 20:40:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
06/17/2022 20:40:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/17/2022 20:40:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
06/17/2022 20:40:14 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.4375 on epoch=549
06/17/2022 20:40:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
06/17/2022 20:40:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
06/17/2022 20:40:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
06/17/2022 20:40:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
06/17/2022 20:40:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
06/17/2022 20:40:30 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.4375 on epoch=566
06/17/2022 20:40:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
06/17/2022 20:40:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
06/17/2022 20:40:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
06/17/2022 20:40:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/17/2022 20:40:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
06/17/2022 20:40:47 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.5 on epoch=583
06/17/2022 20:40:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/17/2022 20:40:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
06/17/2022 20:40:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/17/2022 20:40:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/17/2022 20:41:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
06/17/2022 20:41:03 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.5 on epoch=599
06/17/2022 20:41:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/17/2022 20:41:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
06/17/2022 20:41:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/17/2022 20:41:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/17/2022 20:41:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/17/2022 20:41:18 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.53125 on epoch=616
06/17/2022 20:41:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/17/2022 20:41:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
06/17/2022 20:41:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=626
06/17/2022 20:41:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=629
06/17/2022 20:41:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/17/2022 20:41:34 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.46875 on epoch=633
06/17/2022 20:41:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
06/17/2022 20:41:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/17/2022 20:41:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/17/2022 20:41:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
06/17/2022 20:41:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
06/17/2022 20:41:50 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.46875 on epoch=649
06/17/2022 20:41:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/17/2022 20:41:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/17/2022 20:41:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/17/2022 20:42:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/17/2022 20:42:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/17/2022 20:42:06 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.46875 on epoch=666
06/17/2022 20:42:09 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/17/2022 20:42:12 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=673
06/17/2022 20:42:15 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/17/2022 20:42:18 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
06/17/2022 20:42:21 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
06/17/2022 20:42:22 - INFO - __main__ - Global step 2050 Train loss 0.01 ACC 0.5 on epoch=683
06/17/2022 20:42:25 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
06/17/2022 20:42:28 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/17/2022 20:42:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/17/2022 20:42:34 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/17/2022 20:42:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/17/2022 20:42:38 - INFO - __main__ - Global step 2100 Train loss 0.00 ACC 0.53125 on epoch=699
06/17/2022 20:42:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/17/2022 20:42:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/17/2022 20:42:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/17/2022 20:42:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/17/2022 20:42:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/17/2022 20:42:54 - INFO - __main__ - Global step 2150 Train loss 0.01 ACC 0.4375 on epoch=716
06/17/2022 20:42:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/17/2022 20:43:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
06/17/2022 20:43:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/17/2022 20:43:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/17/2022 20:43:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/17/2022 20:43:10 - INFO - __main__ - Global step 2200 Train loss 0.01 ACC 0.46875 on epoch=733
06/17/2022 20:43:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/17/2022 20:43:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/17/2022 20:43:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/17/2022 20:43:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/17/2022 20:43:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/17/2022 20:43:26 - INFO - __main__ - Global step 2250 Train loss 0.00 ACC 0.4375 on epoch=749
06/17/2022 20:43:29 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/17/2022 20:43:32 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/17/2022 20:43:35 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
06/17/2022 20:43:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/17/2022 20:43:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/17/2022 20:43:42 - INFO - __main__ - Global step 2300 Train loss 0.01 ACC 0.5 on epoch=766
06/17/2022 20:43:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/17/2022 20:43:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/17/2022 20:43:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=776
06/17/2022 20:43:54 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/17/2022 20:43:57 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/17/2022 20:43:58 - INFO - __main__ - Global step 2350 Train loss 0.02 ACC 0.53125 on epoch=783
06/17/2022 20:44:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
06/17/2022 20:44:04 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/17/2022 20:44:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 20:44:10 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 20:44:13 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/17/2022 20:44:14 - INFO - __main__ - Global step 2400 Train loss 0.01 ACC 0.46875 on epoch=799
06/17/2022 20:44:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/17/2022 20:44:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 20:44:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/17/2022 20:44:26 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=813
06/17/2022 20:44:29 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/17/2022 20:44:30 - INFO - __main__ - Global step 2450 Train loss 0.01 ACC 0.4375 on epoch=816
06/17/2022 20:44:33 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
06/17/2022 20:44:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/17/2022 20:44:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/17/2022 20:44:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/17/2022 20:44:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 20:44:46 - INFO - __main__ - Global step 2500 Train loss 0.01 ACC 0.46875 on epoch=833
06/17/2022 20:44:49 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=836
06/17/2022 20:44:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/17/2022 20:44:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/17/2022 20:44:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 20:45:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/17/2022 20:45:02 - INFO - __main__ - Global step 2550 Train loss 0.02 ACC 0.5 on epoch=849
06/17/2022 20:45:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/17/2022 20:45:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 20:45:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
06/17/2022 20:45:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/17/2022 20:45:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 20:45:18 - INFO - __main__ - Global step 2600 Train loss 0.01 ACC 0.5 on epoch=866
06/17/2022 20:45:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=869
06/17/2022 20:45:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/17/2022 20:45:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/17/2022 20:45:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=879
06/17/2022 20:45:33 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 20:45:34 - INFO - __main__ - Global step 2650 Train loss 0.01 ACC 0.5 on epoch=883
06/17/2022 20:45:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 20:45:40 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 20:45:43 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 20:45:46 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=896
06/17/2022 20:45:49 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 20:45:50 - INFO - __main__ - Global step 2700 Train loss 0.01 ACC 0.46875 on epoch=899
06/17/2022 20:45:53 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 20:45:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 20:45:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
06/17/2022 20:46:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 20:46:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/17/2022 20:46:06 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.5 on epoch=916
06/17/2022 20:46:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
06/17/2022 20:46:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 20:46:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
06/17/2022 20:46:18 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/17/2022 20:46:21 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 20:46:22 - INFO - __main__ - Global step 2800 Train loss 0.01 ACC 0.5 on epoch=933
06/17/2022 20:46:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 20:46:28 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/17/2022 20:46:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 20:46:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 20:46:37 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 20:46:38 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.40625 on epoch=949
06/17/2022 20:46:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/17/2022 20:46:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
06/17/2022 20:46:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 20:46:50 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 20:46:53 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 20:46:54 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.5 on epoch=966
06/17/2022 20:46:57 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/17/2022 20:47:00 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
06/17/2022 20:47:03 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/17/2022 20:47:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 20:47:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 20:47:10 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.46875 on epoch=983
06/17/2022 20:47:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
06/17/2022 20:47:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/17/2022 20:47:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=993
06/17/2022 20:47:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 20:47:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 20:47:26 - INFO - __main__ - Global step 3000 Train loss 0.01 ACC 0.40625 on epoch=999
06/17/2022 20:47:26 - INFO - __main__ - save last model!
06/17/2022 20:47:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 20:47:26 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 20:47:26 - INFO - __main__ - Printing 3 examples
06/17/2022 20:47:26 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 20:47:26 - INFO - __main__ - ['contradiction']
06/17/2022 20:47:26 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 20:47:26 - INFO - __main__ - ['neutral']
06/17/2022 20:47:26 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 20:47:26 - INFO - __main__ - ['entailment']
06/17/2022 20:47:26 - INFO - __main__ - Tokenizing Input ...
06/17/2022 20:47:26 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:47:27 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 20:47:27 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 20:47:27 - INFO - __main__ - Printing 3 examples
06/17/2022 20:47:27 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/17/2022 20:47:27 - INFO - __main__ - ['contradiction']
06/17/2022 20:47:27 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/17/2022 20:47:27 - INFO - __main__ - ['contradiction']
06/17/2022 20:47:27 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/17/2022 20:47:27 - INFO - __main__ - ['contradiction']
06/17/2022 20:47:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 20:47:27 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:47:27 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 20:47:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 20:47:27 - INFO - __main__ - Printing 3 examples
06/17/2022 20:47:27 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
06/17/2022 20:47:27 - INFO - __main__ - ['contradiction']
06/17/2022 20:47:27 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
06/17/2022 20:47:27 - INFO - __main__ - ['contradiction']
06/17/2022 20:47:27 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
06/17/2022 20:47:27 - INFO - __main__ - ['contradiction']
06/17/2022 20:47:27 - INFO - __main__ - Tokenizing Input ...
06/17/2022 20:47:27 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:47:27 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 20:47:29 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_42_0.3_8_predictions.txt
06/17/2022 20:47:29 - INFO - __main__ - ACC on test data: 0.5357
06/17/2022 20:47:29 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.3, bsz=8, dev_performance=0.59375, test_performance=0.5357142857142857
06/17/2022 20:47:29 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.2, bsz=8 ...
06/17/2022 20:47:30 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 20:47:30 - INFO - __main__ - Printing 3 examples
06/17/2022 20:47:30 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/17/2022 20:47:30 - INFO - __main__ - ['contradiction']
06/17/2022 20:47:30 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/17/2022 20:47:30 - INFO - __main__ - ['contradiction']
06/17/2022 20:47:30 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/17/2022 20:47:30 - INFO - __main__ - ['contradiction']
06/17/2022 20:47:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 20:47:30 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:47:30 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 20:47:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 20:47:30 - INFO - __main__ - Printing 3 examples
06/17/2022 20:47:30 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
06/17/2022 20:47:30 - INFO - __main__ - ['contradiction']
06/17/2022 20:47:30 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
06/17/2022 20:47:30 - INFO - __main__ - ['contradiction']
06/17/2022 20:47:30 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
06/17/2022 20:47:30 - INFO - __main__ - ['contradiction']
06/17/2022 20:47:30 - INFO - __main__ - Tokenizing Input ...
06/17/2022 20:47:30 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:47:30 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 20:47:45 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 20:47:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 20:47:46 - INFO - __main__ - Starting training!
06/17/2022 20:47:46 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 20:47:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 20:47:47 - INFO - __main__ - Starting training!
06/17/2022 20:47:51 - INFO - __main__ - Step 10 Global step 10 Train loss 1.73 on epoch=3
06/17/2022 20:47:54 - INFO - __main__ - Step 20 Global step 20 Train loss 0.70 on epoch=6
06/17/2022 20:47:57 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=9
06/17/2022 20:48:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=13
06/17/2022 20:48:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.45 on epoch=16
06/17/2022 20:48:04 - INFO - __main__ - Global step 50 Train loss 0.77 ACC 0.5 on epoch=16
06/17/2022 20:48:04 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 20:48:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=19
06/17/2022 20:48:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=23
06/17/2022 20:48:13 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
06/17/2022 20:48:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=29
06/17/2022 20:48:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=33
06/17/2022 20:48:20 - INFO - __main__ - Global step 100 Train loss 0.48 ACC 0.5 on epoch=33
06/17/2022 20:48:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=36
06/17/2022 20:48:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=39
06/17/2022 20:48:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
06/17/2022 20:48:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=46
06/17/2022 20:48:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
06/17/2022 20:48:36 - INFO - __main__ - Global step 150 Train loss 0.45 ACC 0.5625 on epoch=49
06/17/2022 20:48:36 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=49, global_step=150
06/17/2022 20:48:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=53
06/17/2022 20:48:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=56
06/17/2022 20:48:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=59
06/17/2022 20:48:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=63
06/17/2022 20:48:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=66
06/17/2022 20:48:51 - INFO - __main__ - Global step 200 Train loss 0.41 ACC 0.59375 on epoch=66
06/17/2022 20:48:51 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=66, global_step=200
06/17/2022 20:48:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=69
06/17/2022 20:48:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=73
06/17/2022 20:49:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=76
06/17/2022 20:49:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=79
06/17/2022 20:49:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=83
06/17/2022 20:49:07 - INFO - __main__ - Global step 250 Train loss 0.42 ACC 0.53125 on epoch=83
06/17/2022 20:49:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=86
06/17/2022 20:49:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=89
06/17/2022 20:49:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.34 on epoch=93
06/17/2022 20:49:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=96
06/17/2022 20:49:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=99
06/17/2022 20:49:23 - INFO - __main__ - Global step 300 Train loss 0.39 ACC 0.5 on epoch=99
06/17/2022 20:49:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=103
06/17/2022 20:49:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=106
06/17/2022 20:49:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=109
06/17/2022 20:49:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=113
06/17/2022 20:49:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.34 on epoch=116
06/17/2022 20:49:39 - INFO - __main__ - Global step 350 Train loss 0.35 ACC 0.4375 on epoch=116
06/17/2022 20:49:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=119
06/17/2022 20:49:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=123
06/17/2022 20:49:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=126
06/17/2022 20:49:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=129
06/17/2022 20:49:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.32 on epoch=133
06/17/2022 20:49:55 - INFO - __main__ - Global step 400 Train loss 0.33 ACC 0.375 on epoch=133
06/17/2022 20:49:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=136
06/17/2022 20:50:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=139
06/17/2022 20:50:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.29 on epoch=143
06/17/2022 20:50:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.29 on epoch=146
06/17/2022 20:50:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.28 on epoch=149
06/17/2022 20:50:11 - INFO - __main__ - Global step 450 Train loss 0.28 ACC 0.40625 on epoch=149
06/17/2022 20:50:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=153
06/17/2022 20:50:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=156
06/17/2022 20:50:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=159
06/17/2022 20:50:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=163
06/17/2022 20:50:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=166
06/17/2022 20:50:27 - INFO - __main__ - Global step 500 Train loss 0.26 ACC 0.375 on epoch=166
06/17/2022 20:50:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=169
06/17/2022 20:50:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.30 on epoch=173
06/17/2022 20:50:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.28 on epoch=176
06/17/2022 20:50:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=179
06/17/2022 20:50:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=183
06/17/2022 20:50:43 - INFO - __main__ - Global step 550 Train loss 0.27 ACC 0.3125 on epoch=183
06/17/2022 20:50:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=186
06/17/2022 20:50:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=189
06/17/2022 20:50:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=193
06/17/2022 20:50:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=196
06/17/2022 20:50:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=199
06/17/2022 20:50:59 - INFO - __main__ - Global step 600 Train loss 0.22 ACC 0.34375 on epoch=199
06/17/2022 20:51:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=203
06/17/2022 20:51:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=206
06/17/2022 20:51:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=209
06/17/2022 20:51:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=213
06/17/2022 20:51:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=216
06/17/2022 20:51:15 - INFO - __main__ - Global step 650 Train loss 0.23 ACC 0.3125 on epoch=216
06/17/2022 20:51:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=219
06/17/2022 20:51:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=223
06/17/2022 20:51:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=226
06/17/2022 20:51:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=229
06/17/2022 20:51:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=233
06/17/2022 20:51:31 - INFO - __main__ - Global step 700 Train loss 0.19 ACC 0.34375 on epoch=233
06/17/2022 20:51:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.18 on epoch=236
06/17/2022 20:51:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=239
06/17/2022 20:51:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=243
06/17/2022 20:51:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=246
06/17/2022 20:51:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=249
06/17/2022 20:51:47 - INFO - __main__ - Global step 750 Train loss 0.16 ACC 0.34375 on epoch=249
06/17/2022 20:51:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=253
06/17/2022 20:51:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=256
06/17/2022 20:51:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=259
06/17/2022 20:51:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=263
06/17/2022 20:52:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=266
06/17/2022 20:52:02 - INFO - __main__ - Global step 800 Train loss 0.18 ACC 0.3125 on epoch=266
06/17/2022 20:52:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=269
06/17/2022 20:52:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.15 on epoch=273
06/17/2022 20:52:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=276
06/17/2022 20:52:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=279
06/17/2022 20:52:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.16 on epoch=283
06/17/2022 20:52:18 - INFO - __main__ - Global step 850 Train loss 0.15 ACC 0.34375 on epoch=283
06/17/2022 20:52:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.15 on epoch=286
06/17/2022 20:52:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.15 on epoch=289
06/17/2022 20:52:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=293
06/17/2022 20:52:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=296
06/17/2022 20:52:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=299
06/17/2022 20:52:34 - INFO - __main__ - Global step 900 Train loss 0.15 ACC 0.34375 on epoch=299
06/17/2022 20:52:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=303
06/17/2022 20:52:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=306
06/17/2022 20:52:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=309
06/17/2022 20:52:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.15 on epoch=313
06/17/2022 20:52:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=316
06/17/2022 20:52:50 - INFO - __main__ - Global step 950 Train loss 0.13 ACC 0.3125 on epoch=316
06/17/2022 20:52:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=319
06/17/2022 20:52:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=323
06/17/2022 20:52:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=326
06/17/2022 20:53:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.13 on epoch=329
06/17/2022 20:53:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.12 on epoch=333
06/17/2022 20:53:06 - INFO - __main__ - Global step 1000 Train loss 0.12 ACC 0.34375 on epoch=333
06/17/2022 20:53:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=336
06/17/2022 20:53:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=339
06/17/2022 20:53:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=343
06/17/2022 20:53:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=346
06/17/2022 20:53:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=349
06/17/2022 20:53:21 - INFO - __main__ - Global step 1050 Train loss 0.13 ACC 0.3125 on epoch=349
06/17/2022 20:53:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=353
06/17/2022 20:53:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=356
06/17/2022 20:53:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=359
06/17/2022 20:53:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=363
06/17/2022 20:53:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=366
06/17/2022 20:53:37 - INFO - __main__ - Global step 1100 Train loss 0.11 ACC 0.3125 on epoch=366
06/17/2022 20:53:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=369
06/17/2022 20:53:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=373
06/17/2022 20:53:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.12 on epoch=376
06/17/2022 20:53:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=379
06/17/2022 20:53:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=383
06/17/2022 20:53:53 - INFO - __main__ - Global step 1150 Train loss 0.11 ACC 0.34375 on epoch=383
06/17/2022 20:53:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=386
06/17/2022 20:53:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=389
06/17/2022 20:54:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=393
06/17/2022 20:54:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=396
06/17/2022 20:54:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=399
06/17/2022 20:54:09 - INFO - __main__ - Global step 1200 Train loss 0.08 ACC 0.34375 on epoch=399
06/17/2022 20:54:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=403
06/17/2022 20:54:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=406
06/17/2022 20:54:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=409
06/17/2022 20:54:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=413
06/17/2022 20:54:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=416
06/17/2022 20:54:25 - INFO - __main__ - Global step 1250 Train loss 0.08 ACC 0.375 on epoch=416
06/17/2022 20:54:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=419
06/17/2022 20:54:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=423
06/17/2022 20:54:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=426
06/17/2022 20:54:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.09 on epoch=429
06/17/2022 20:54:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=433
06/17/2022 20:54:41 - INFO - __main__ - Global step 1300 Train loss 0.08 ACC 0.40625 on epoch=433
06/17/2022 20:54:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=436
06/17/2022 20:54:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=439
06/17/2022 20:54:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=443
06/17/2022 20:54:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=446
06/17/2022 20:54:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=449
06/17/2022 20:54:57 - INFO - __main__ - Global step 1350 Train loss 0.06 ACC 0.40625 on epoch=449
06/17/2022 20:55:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=453
06/17/2022 20:55:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=456
06/17/2022 20:55:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=459
06/17/2022 20:55:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=463
06/17/2022 20:55:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=466
06/17/2022 20:55:12 - INFO - __main__ - Global step 1400 Train loss 0.05 ACC 0.40625 on epoch=466
06/17/2022 20:55:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=469
06/17/2022 20:55:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
06/17/2022 20:55:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=476
06/17/2022 20:55:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=479
06/17/2022 20:55:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=483
06/17/2022 20:55:28 - INFO - __main__ - Global step 1450 Train loss 0.05 ACC 0.40625 on epoch=483
06/17/2022 20:55:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
06/17/2022 20:55:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
06/17/2022 20:55:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=493
06/17/2022 20:55:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=496
06/17/2022 20:55:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=499
06/17/2022 20:55:44 - INFO - __main__ - Global step 1500 Train loss 0.05 ACC 0.40625 on epoch=499
06/17/2022 20:55:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=503
06/17/2022 20:55:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
06/17/2022 20:55:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=509
06/17/2022 20:55:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=513
06/17/2022 20:55:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
06/17/2022 20:56:00 - INFO - __main__ - Global step 1550 Train loss 0.05 ACC 0.375 on epoch=516
06/17/2022 20:56:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
06/17/2022 20:56:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
06/17/2022 20:56:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=526
06/17/2022 20:56:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=529
06/17/2022 20:56:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=533
06/17/2022 20:56:16 - INFO - __main__ - Global step 1600 Train loss 0.04 ACC 0.375 on epoch=533
06/17/2022 20:56:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=536
06/17/2022 20:56:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
06/17/2022 20:56:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
06/17/2022 20:56:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=546
06/17/2022 20:56:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=549
06/17/2022 20:56:32 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.40625 on epoch=549
06/17/2022 20:56:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=553
06/17/2022 20:56:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
06/17/2022 20:56:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
06/17/2022 20:56:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
06/17/2022 20:56:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=566
06/17/2022 20:56:48 - INFO - __main__ - Global step 1700 Train loss 0.03 ACC 0.4375 on epoch=566
06/17/2022 20:56:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=569
06/17/2022 20:56:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=573
06/17/2022 20:56:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=576
06/17/2022 20:57:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=579
06/17/2022 20:57:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
06/17/2022 20:57:04 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.46875 on epoch=583
06/17/2022 20:57:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=586
06/17/2022 20:57:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
06/17/2022 20:57:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=593
06/17/2022 20:57:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=596
06/17/2022 20:57:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
06/17/2022 20:57:20 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.46875 on epoch=599
06/17/2022 20:57:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=603
06/17/2022 20:57:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
06/17/2022 20:57:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
06/17/2022 20:57:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
06/17/2022 20:57:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
06/17/2022 20:57:36 - INFO - __main__ - Global step 1850 Train loss 0.03 ACC 0.46875 on epoch=616
06/17/2022 20:57:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/17/2022 20:57:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
06/17/2022 20:57:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
06/17/2022 20:57:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
06/17/2022 20:57:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
06/17/2022 20:57:52 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.4375 on epoch=633
06/17/2022 20:57:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
06/17/2022 20:57:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
06/17/2022 20:58:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=643
06/17/2022 20:58:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
06/17/2022 20:58:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
06/17/2022 20:58:08 - INFO - __main__ - Global step 1950 Train loss 0.03 ACC 0.4375 on epoch=649
06/17/2022 20:58:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=653
06/17/2022 20:58:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/17/2022 20:58:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/17/2022 20:58:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/17/2022 20:58:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/17/2022 20:58:24 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.46875 on epoch=666
06/17/2022 20:58:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=669
06/17/2022 20:58:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
06/17/2022 20:58:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
06/17/2022 20:58:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=679
06/17/2022 20:58:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/17/2022 20:58:40 - INFO - __main__ - Global step 2050 Train loss 0.02 ACC 0.4375 on epoch=683
06/17/2022 20:58:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
06/17/2022 20:58:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
06/17/2022 20:58:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/17/2022 20:58:52 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=696
06/17/2022 20:58:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/17/2022 20:58:56 - INFO - __main__ - Global step 2100 Train loss 0.02 ACC 0.46875 on epoch=699
06/17/2022 20:58:59 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
06/17/2022 20:59:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/17/2022 20:59:05 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/17/2022 20:59:08 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
06/17/2022 20:59:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=716
06/17/2022 20:59:11 - INFO - __main__ - Global step 2150 Train loss 0.02 ACC 0.4375 on epoch=716
06/17/2022 20:59:14 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/17/2022 20:59:17 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
06/17/2022 20:59:20 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/17/2022 20:59:23 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
06/17/2022 20:59:26 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/17/2022 20:59:27 - INFO - __main__ - Global step 2200 Train loss 0.01 ACC 0.46875 on epoch=733
06/17/2022 20:59:30 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=736
06/17/2022 20:59:33 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/17/2022 20:59:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/17/2022 20:59:39 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
06/17/2022 20:59:42 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
06/17/2022 20:59:43 - INFO - __main__ - Global step 2250 Train loss 0.01 ACC 0.4375 on epoch=749
06/17/2022 20:59:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
06/17/2022 20:59:49 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/17/2022 20:59:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
06/17/2022 20:59:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
06/17/2022 20:59:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=766
06/17/2022 20:59:59 - INFO - __main__ - Global step 2300 Train loss 0.01 ACC 0.4375 on epoch=766
06/17/2022 21:00:02 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
06/17/2022 21:00:05 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/17/2022 21:00:08 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
06/17/2022 21:00:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/17/2022 21:00:14 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/17/2022 21:00:15 - INFO - __main__ - Global step 2350 Train loss 0.01 ACC 0.4375 on epoch=783
06/17/2022 21:00:18 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
06/17/2022 21:00:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/17/2022 21:00:24 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 21:00:27 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=796
06/17/2022 21:00:30 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/17/2022 21:00:31 - INFO - __main__ - Global step 2400 Train loss 0.01 ACC 0.4375 on epoch=799
06/17/2022 21:00:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
06/17/2022 21:00:37 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
06/17/2022 21:00:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=809
06/17/2022 21:00:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/17/2022 21:00:46 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/17/2022 21:00:47 - INFO - __main__ - Global step 2450 Train loss 0.01 ACC 0.4375 on epoch=816
06/17/2022 21:00:50 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/17/2022 21:00:53 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=823
06/17/2022 21:00:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=826
06/17/2022 21:00:59 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/17/2022 21:01:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 21:01:03 - INFO - __main__ - Global step 2500 Train loss 0.01 ACC 0.46875 on epoch=833
06/17/2022 21:01:06 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
06/17/2022 21:01:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
06/17/2022 21:01:12 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/17/2022 21:01:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 21:01:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=849
06/17/2022 21:01:19 - INFO - __main__ - Global step 2550 Train loss 0.01 ACC 0.46875 on epoch=849
06/17/2022 21:01:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
06/17/2022 21:01:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 21:01:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
06/17/2022 21:01:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 21:01:34 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
06/17/2022 21:01:34 - INFO - __main__ - Global step 2600 Train loss 0.01 ACC 0.4375 on epoch=866
06/17/2022 21:01:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=869
06/17/2022 21:01:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
06/17/2022 21:01:43 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
06/17/2022 21:01:46 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
06/17/2022 21:01:49 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
06/17/2022 21:01:50 - INFO - __main__ - Global step 2650 Train loss 0.02 ACC 0.46875 on epoch=883
06/17/2022 21:01:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
06/17/2022 21:01:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
06/17/2022 21:01:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=893
06/17/2022 21:02:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 21:02:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
06/17/2022 21:02:06 - INFO - __main__ - Global step 2700 Train loss 0.01 ACC 0.46875 on epoch=899
06/17/2022 21:02:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 21:02:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=906
06/17/2022 21:02:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 21:02:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 21:02:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/17/2022 21:02:22 - INFO - __main__ - Global step 2750 Train loss 0.01 ACC 0.4375 on epoch=916
06/17/2022 21:02:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 21:02:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
06/17/2022 21:02:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
06/17/2022 21:02:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
06/17/2022 21:02:37 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
06/17/2022 21:02:38 - INFO - __main__ - Global step 2800 Train loss 0.01 ACC 0.4375 on epoch=933
06/17/2022 21:02:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 21:02:44 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
06/17/2022 21:02:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 21:02:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 21:02:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
06/17/2022 21:02:54 - INFO - __main__ - Global step 2850 Train loss 0.01 ACC 0.46875 on epoch=949
06/17/2022 21:02:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/17/2022 21:03:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
06/17/2022 21:03:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
06/17/2022 21:03:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 21:03:09 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 21:03:10 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.46875 on epoch=966
06/17/2022 21:03:13 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/17/2022 21:03:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 21:03:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
06/17/2022 21:03:22 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 21:03:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 21:03:26 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.46875 on epoch=983
06/17/2022 21:03:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 21:03:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=989
06/17/2022 21:03:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 21:03:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
06/17/2022 21:03:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 21:03:42 - INFO - __main__ - Global step 3000 Train loss 0.01 ACC 0.46875 on epoch=999
06/17/2022 21:03:42 - INFO - __main__ - save last model!
06/17/2022 21:03:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 21:03:42 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 21:03:42 - INFO - __main__ - Printing 3 examples
06/17/2022 21:03:42 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 21:03:42 - INFO - __main__ - ['contradiction']
06/17/2022 21:03:42 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 21:03:42 - INFO - __main__ - ['neutral']
06/17/2022 21:03:42 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 21:03:42 - INFO - __main__ - ['entailment']
06/17/2022 21:03:42 - INFO - __main__ - Tokenizing Input ...
06/17/2022 21:03:42 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:03:42 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 21:03:42 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 21:03:42 - INFO - __main__ - Printing 3 examples
06/17/2022 21:03:42 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/17/2022 21:03:42 - INFO - __main__ - ['contradiction']
06/17/2022 21:03:42 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/17/2022 21:03:42 - INFO - __main__ - ['contradiction']
06/17/2022 21:03:42 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/17/2022 21:03:42 - INFO - __main__ - ['contradiction']
06/17/2022 21:03:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 21:03:42 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:03:42 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 21:03:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 21:03:42 - INFO - __main__ - Printing 3 examples
06/17/2022 21:03:42 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
06/17/2022 21:03:42 - INFO - __main__ - ['contradiction']
06/17/2022 21:03:42 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
06/17/2022 21:03:42 - INFO - __main__ - ['contradiction']
06/17/2022 21:03:42 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
06/17/2022 21:03:42 - INFO - __main__ - ['contradiction']
06/17/2022 21:03:42 - INFO - __main__ - Tokenizing Input ...
06/17/2022 21:03:42 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:03:42 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 21:03:44 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_42_0.2_8_predictions.txt
06/17/2022 21:03:44 - INFO - __main__ - ACC on test data: 0.5714
06/17/2022 21:03:45 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.2, bsz=8, dev_performance=0.59375, test_performance=0.5714285714285714
06/17/2022 21:03:45 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.5, bsz=8 ...
06/17/2022 21:03:46 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 21:03:46 - INFO - __main__ - Printing 3 examples
06/17/2022 21:03:46 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/17/2022 21:03:46 - INFO - __main__ - ['contradiction']
06/17/2022 21:03:46 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/17/2022 21:03:46 - INFO - __main__ - ['contradiction']
06/17/2022 21:03:46 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/17/2022 21:03:46 - INFO - __main__ - ['contradiction']
06/17/2022 21:03:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 21:03:46 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:03:46 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 21:03:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 21:03:46 - INFO - __main__ - Printing 3 examples
06/17/2022 21:03:46 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
06/17/2022 21:03:46 - INFO - __main__ - ['contradiction']
06/17/2022 21:03:46 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
06/17/2022 21:03:46 - INFO - __main__ - ['contradiction']
06/17/2022 21:03:46 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
06/17/2022 21:03:46 - INFO - __main__ - ['contradiction']
06/17/2022 21:03:46 - INFO - __main__ - Tokenizing Input ...
06/17/2022 21:03:46 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:03:46 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 21:03:58 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 21:03:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 21:03:59 - INFO - __main__ - Starting training!
06/17/2022 21:04:03 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 21:04:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 21:04:04 - INFO - __main__ - Starting training!
06/17/2022 21:04:07 - INFO - __main__ - Step 10 Global step 10 Train loss 1.05 on epoch=3
06/17/2022 21:04:10 - INFO - __main__ - Step 20 Global step 20 Train loss 0.48 on epoch=6
06/17/2022 21:04:13 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=9
06/17/2022 21:04:15 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=13
06/17/2022 21:04:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.43 on epoch=16
06/17/2022 21:04:19 - INFO - __main__ - Global step 50 Train loss 0.60 ACC 0.5 on epoch=16
06/17/2022 21:04:19 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 21:04:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=19
06/17/2022 21:04:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=23
06/17/2022 21:04:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=26
06/17/2022 21:04:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=29
06/17/2022 21:04:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=33
06/17/2022 21:04:33 - INFO - __main__ - Global step 100 Train loss 0.43 ACC 0.53125 on epoch=33
06/17/2022 21:04:33 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=33, global_step=100
06/17/2022 21:04:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=36
06/17/2022 21:04:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=39
06/17/2022 21:04:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=43
06/17/2022 21:04:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.36 on epoch=46
06/17/2022 21:04:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.34 on epoch=49
06/17/2022 21:04:47 - INFO - __main__ - Global step 150 Train loss 0.36 ACC 0.5 on epoch=49
06/17/2022 21:04:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=53
06/17/2022 21:04:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=56
06/17/2022 21:04:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.34 on epoch=59
06/17/2022 21:04:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.31 on epoch=63
06/17/2022 21:05:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.30 on epoch=66
06/17/2022 21:05:01 - INFO - __main__ - Global step 200 Train loss 0.32 ACC 0.53125 on epoch=66
06/17/2022 21:05:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.32 on epoch=69
06/17/2022 21:05:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=73
06/17/2022 21:05:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=76
06/17/2022 21:05:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=79
06/17/2022 21:05:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=83
06/17/2022 21:05:16 - INFO - __main__ - Global step 250 Train loss 0.27 ACC 0.53125 on epoch=83
06/17/2022 21:05:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=86
06/17/2022 21:05:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=89
06/17/2022 21:05:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=93
06/17/2022 21:05:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.20 on epoch=96
06/17/2022 21:05:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.20 on epoch=99
06/17/2022 21:05:30 - INFO - __main__ - Global step 300 Train loss 0.23 ACC 0.5 on epoch=99
06/17/2022 21:05:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=103
06/17/2022 21:05:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.22 on epoch=106
06/17/2022 21:05:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.14 on epoch=109
06/17/2022 21:05:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.18 on epoch=113
06/17/2022 21:05:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.11 on epoch=116
06/17/2022 21:05:44 - INFO - __main__ - Global step 350 Train loss 0.17 ACC 0.53125 on epoch=116
06/17/2022 21:05:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.13 on epoch=119
06/17/2022 21:05:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=123
06/17/2022 21:05:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.16 on epoch=126
06/17/2022 21:05:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.15 on epoch=129
06/17/2022 21:05:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.11 on epoch=133
06/17/2022 21:05:58 - INFO - __main__ - Global step 400 Train loss 0.13 ACC 0.53125 on epoch=133
06/17/2022 21:06:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.14 on epoch=136
06/17/2022 21:06:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=139
06/17/2022 21:06:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=143
06/17/2022 21:06:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.14 on epoch=146
06/17/2022 21:06:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.08 on epoch=149
06/17/2022 21:06:12 - INFO - __main__ - Global step 450 Train loss 0.13 ACC 0.53125 on epoch=149
06/17/2022 21:06:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.10 on epoch=153
06/17/2022 21:06:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.10 on epoch=156
06/17/2022 21:06:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.09 on epoch=159
06/17/2022 21:06:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=163
06/17/2022 21:06:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=166
06/17/2022 21:06:27 - INFO - __main__ - Global step 500 Train loss 0.10 ACC 0.5625 on epoch=166
06/17/2022 21:06:27 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=166, global_step=500
06/17/2022 21:06:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=169
06/17/2022 21:06:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.09 on epoch=173
06/17/2022 21:06:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=176
06/17/2022 21:06:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=179
06/17/2022 21:06:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=183
06/17/2022 21:06:41 - INFO - __main__ - Global step 550 Train loss 0.08 ACC 0.625 on epoch=183
06/17/2022 21:06:41 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.625 on epoch=183, global_step=550
06/17/2022 21:06:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.09 on epoch=186
06/17/2022 21:06:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=189
06/17/2022 21:06:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.08 on epoch=193
06/17/2022 21:06:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=196
06/17/2022 21:06:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=199
06/17/2022 21:06:55 - INFO - __main__ - Global step 600 Train loss 0.07 ACC 0.5625 on epoch=199
06/17/2022 21:06:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=203
06/17/2022 21:07:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=206
06/17/2022 21:07:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=209
06/17/2022 21:07:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.05 on epoch=213
06/17/2022 21:07:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=216
06/17/2022 21:07:09 - INFO - __main__ - Global step 650 Train loss 0.05 ACC 0.5625 on epoch=216
06/17/2022 21:07:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=219
06/17/2022 21:07:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=223
06/17/2022 21:07:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=226
06/17/2022 21:07:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=229
06/17/2022 21:07:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=233
06/17/2022 21:07:23 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.65625 on epoch=233
06/17/2022 21:07:23 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=233, global_step=700
06/17/2022 21:07:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=236
06/17/2022 21:07:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=239
06/17/2022 21:07:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=243
06/17/2022 21:07:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=246
06/17/2022 21:07:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=249
06/17/2022 21:07:38 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.59375 on epoch=249
06/17/2022 21:07:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=253
06/17/2022 21:07:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=256
06/17/2022 21:07:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=259
06/17/2022 21:07:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=263
06/17/2022 21:07:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=266
06/17/2022 21:07:52 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.53125 on epoch=266
06/17/2022 21:07:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=269
06/17/2022 21:07:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=273
06/17/2022 21:08:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=276
06/17/2022 21:08:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=279
06/17/2022 21:08:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=283
06/17/2022 21:08:06 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.5 on epoch=283
06/17/2022 21:08:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=286
06/17/2022 21:08:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=289
06/17/2022 21:08:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=293
06/17/2022 21:08:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=296
06/17/2022 21:08:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=299
06/17/2022 21:08:20 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.53125 on epoch=299
06/17/2022 21:08:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=303
06/17/2022 21:08:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=306
06/17/2022 21:08:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=309
06/17/2022 21:08:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=313
06/17/2022 21:08:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=316
06/17/2022 21:08:34 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.5625 on epoch=316
06/17/2022 21:08:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=319
06/17/2022 21:08:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=323
06/17/2022 21:08:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=326
06/17/2022 21:08:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=329
06/17/2022 21:08:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=333
06/17/2022 21:08:48 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.59375 on epoch=333
06/17/2022 21:08:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=336
06/17/2022 21:08:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=339
06/17/2022 21:08:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
06/17/2022 21:08:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=346
06/17/2022 21:09:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=349
06/17/2022 21:09:02 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.59375 on epoch=349
06/17/2022 21:09:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=353
06/17/2022 21:09:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=356
06/17/2022 21:09:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=359
06/17/2022 21:09:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=363
06/17/2022 21:09:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=366
06/17/2022 21:09:17 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.59375 on epoch=366
06/17/2022 21:09:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
06/17/2022 21:09:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
06/17/2022 21:09:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=376
06/17/2022 21:09:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
06/17/2022 21:09:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
06/17/2022 21:09:31 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.65625 on epoch=383
06/17/2022 21:09:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
06/17/2022 21:09:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
06/17/2022 21:09:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=393
06/17/2022 21:09:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
06/17/2022 21:09:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
06/17/2022 21:09:45 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.71875 on epoch=399
06/17/2022 21:09:45 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.71875 on epoch=399, global_step=1200
06/17/2022 21:09:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
06/17/2022 21:09:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
06/17/2022 21:09:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=409
06/17/2022 21:09:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=413
06/17/2022 21:09:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=416
06/17/2022 21:09:59 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.71875 on epoch=416
06/17/2022 21:10:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
06/17/2022 21:10:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=423
06/17/2022 21:10:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=426
06/17/2022 21:10:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
06/17/2022 21:10:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
06/17/2022 21:10:13 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.75 on epoch=433
06/17/2022 21:10:13 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.75 on epoch=433, global_step=1300
06/17/2022 21:10:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
06/17/2022 21:10:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/17/2022 21:10:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
06/17/2022 21:10:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
06/17/2022 21:10:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
06/17/2022 21:10:28 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.6875 on epoch=449
06/17/2022 21:10:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
06/17/2022 21:10:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
06/17/2022 21:10:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
06/17/2022 21:10:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=463
06/17/2022 21:10:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
06/17/2022 21:10:42 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.65625 on epoch=466
06/17/2022 21:10:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=469
06/17/2022 21:10:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
06/17/2022 21:10:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
06/17/2022 21:10:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
06/17/2022 21:10:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
06/17/2022 21:10:56 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.6875 on epoch=483
06/17/2022 21:10:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
06/17/2022 21:11:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
06/17/2022 21:11:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
06/17/2022 21:11:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
06/17/2022 21:11:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
06/17/2022 21:11:10 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.6875 on epoch=499
06/17/2022 21:11:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
06/17/2022 21:11:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
06/17/2022 21:11:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
06/17/2022 21:11:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
06/17/2022 21:11:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
06/17/2022 21:11:25 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.65625 on epoch=516
06/17/2022 21:11:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/17/2022 21:11:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
06/17/2022 21:11:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
06/17/2022 21:11:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/17/2022 21:11:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
06/17/2022 21:11:39 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.65625 on epoch=533
06/17/2022 21:11:42 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
06/17/2022 21:11:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
06/17/2022 21:11:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
06/17/2022 21:11:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/17/2022 21:11:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/17/2022 21:11:53 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.71875 on epoch=549
06/17/2022 21:11:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
06/17/2022 21:11:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
06/17/2022 21:12:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/17/2022 21:12:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
06/17/2022 21:12:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=566
06/17/2022 21:12:07 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.71875 on epoch=566
06/17/2022 21:12:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
06/17/2022 21:12:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
06/17/2022 21:12:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
06/17/2022 21:12:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
06/17/2022 21:12:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/17/2022 21:12:22 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.625 on epoch=583
06/17/2022 21:12:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/17/2022 21:12:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/17/2022 21:12:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/17/2022 21:12:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/17/2022 21:12:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
06/17/2022 21:12:36 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.71875 on epoch=599
06/17/2022 21:12:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/17/2022 21:12:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/17/2022 21:12:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/17/2022 21:12:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
06/17/2022 21:12:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/17/2022 21:12:50 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.71875 on epoch=616
06/17/2022 21:12:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/17/2022 21:12:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
06/17/2022 21:12:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/17/2022 21:13:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/17/2022 21:13:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=633
06/17/2022 21:13:04 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.625 on epoch=633
06/17/2022 21:13:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
06/17/2022 21:13:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=639
06/17/2022 21:13:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/17/2022 21:13:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/17/2022 21:13:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/17/2022 21:13:19 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.6875 on epoch=649
06/17/2022 21:13:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/17/2022 21:13:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/17/2022 21:13:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/17/2022 21:13:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/17/2022 21:13:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=666
06/17/2022 21:13:33 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.71875 on epoch=666
06/17/2022 21:13:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/17/2022 21:13:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/17/2022 21:13:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
06/17/2022 21:13:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.09 on epoch=679
06/17/2022 21:13:46 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/17/2022 21:13:47 - INFO - __main__ - Global step 2050 Train loss 0.02 ACC 0.71875 on epoch=683
06/17/2022 21:13:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/17/2022 21:13:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/17/2022 21:13:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/17/2022 21:13:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
06/17/2022 21:14:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/17/2022 21:14:01 - INFO - __main__ - Global step 2100 Train loss 0.01 ACC 0.71875 on epoch=699
06/17/2022 21:14:04 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/17/2022 21:14:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/17/2022 21:14:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/17/2022 21:14:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/17/2022 21:14:14 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/17/2022 21:14:15 - INFO - __main__ - Global step 2150 Train loss 0.00 ACC 0.71875 on epoch=716
06/17/2022 21:14:18 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/17/2022 21:14:21 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 21:14:23 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/17/2022 21:14:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
06/17/2022 21:14:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/17/2022 21:14:30 - INFO - __main__ - Global step 2200 Train loss 0.00 ACC 0.71875 on epoch=733
06/17/2022 21:14:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/17/2022 21:14:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/17/2022 21:14:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/17/2022 21:14:40 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/17/2022 21:14:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/17/2022 21:14:44 - INFO - __main__ - Global step 2250 Train loss 0.00 ACC 0.71875 on epoch=749
06/17/2022 21:14:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/17/2022 21:14:49 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/17/2022 21:14:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/17/2022 21:14:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/17/2022 21:14:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/17/2022 21:14:58 - INFO - __main__ - Global step 2300 Train loss 0.00 ACC 0.65625 on epoch=766
06/17/2022 21:15:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/17/2022 21:15:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/17/2022 21:15:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/17/2022 21:15:09 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/17/2022 21:15:11 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/17/2022 21:15:12 - INFO - __main__ - Global step 2350 Train loss 0.00 ACC 0.71875 on epoch=783
06/17/2022 21:15:15 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/17/2022 21:15:18 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/17/2022 21:15:20 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 21:15:23 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 21:15:26 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/17/2022 21:15:27 - INFO - __main__ - Global step 2400 Train loss 0.00 ACC 0.71875 on epoch=799
06/17/2022 21:15:29 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/17/2022 21:15:32 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 21:15:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/17/2022 21:15:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/17/2022 21:15:40 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/17/2022 21:15:41 - INFO - __main__ - Global step 2450 Train loss 0.00 ACC 0.6875 on epoch=816
06/17/2022 21:15:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/17/2022 21:15:46 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/17/2022 21:15:49 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/17/2022 21:15:51 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/17/2022 21:15:54 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 21:15:55 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.65625 on epoch=833
06/17/2022 21:15:58 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 21:16:00 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/17/2022 21:16:03 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/17/2022 21:16:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 21:16:09 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/17/2022 21:16:09 - INFO - __main__ - Global step 2550 Train loss 0.00 ACC 0.625 on epoch=849
06/17/2022 21:16:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/17/2022 21:16:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 21:16:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/17/2022 21:16:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 21:16:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 21:16:24 - INFO - __main__ - Global step 2600 Train loss 0.00 ACC 0.65625 on epoch=866
06/17/2022 21:16:27 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/17/2022 21:16:29 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/17/2022 21:16:32 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/17/2022 21:16:35 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/17/2022 21:16:37 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 21:16:38 - INFO - __main__ - Global step 2650 Train loss 0.00 ACC 0.65625 on epoch=883
06/17/2022 21:16:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 21:16:44 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 21:16:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 21:16:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 21:16:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 21:16:52 - INFO - __main__ - Global step 2700 Train loss 0.00 ACC 0.6875 on epoch=899
06/17/2022 21:16:55 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 21:16:58 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 21:17:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 21:17:03 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
06/17/2022 21:17:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/17/2022 21:17:07 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.75 on epoch=916
06/17/2022 21:17:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 21:17:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 21:17:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/17/2022 21:17:17 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/17/2022 21:17:20 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 21:17:21 - INFO - __main__ - Global step 2800 Train loss 0.00 ACC 0.71875 on epoch=933
06/17/2022 21:17:24 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 21:17:26 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/17/2022 21:17:29 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 21:17:32 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 21:17:34 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 21:17:35 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.6875 on epoch=949
06/17/2022 21:17:38 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/17/2022 21:17:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/17/2022 21:17:43 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 21:17:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
06/17/2022 21:17:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 21:17:50 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.6875 on epoch=966
06/17/2022 21:17:52 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/17/2022 21:17:55 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 21:17:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/17/2022 21:18:00 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 21:18:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 21:18:04 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.625 on epoch=983
06/17/2022 21:18:07 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 21:18:09 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/17/2022 21:18:12 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 21:18:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 21:18:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 21:18:18 - INFO - __main__ - Global step 3000 Train loss 0.00 ACC 0.6875 on epoch=999
06/17/2022 21:18:18 - INFO - __main__ - save last model!
06/17/2022 21:18:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 21:18:18 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 21:18:18 - INFO - __main__ - Printing 3 examples
06/17/2022 21:18:18 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 21:18:18 - INFO - __main__ - ['contradiction']
06/17/2022 21:18:18 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 21:18:18 - INFO - __main__ - ['neutral']
06/17/2022 21:18:18 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 21:18:18 - INFO - __main__ - ['entailment']
06/17/2022 21:18:18 - INFO - __main__ - Tokenizing Input ...
06/17/2022 21:18:18 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:18:18 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 21:18:19 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 21:18:19 - INFO - __main__ - Printing 3 examples
06/17/2022 21:18:19 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/17/2022 21:18:19 - INFO - __main__ - ['contradiction']
06/17/2022 21:18:19 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/17/2022 21:18:19 - INFO - __main__ - ['contradiction']
06/17/2022 21:18:19 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/17/2022 21:18:19 - INFO - __main__ - ['contradiction']
06/17/2022 21:18:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 21:18:19 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:18:19 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 21:18:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 21:18:19 - INFO - __main__ - Printing 3 examples
06/17/2022 21:18:19 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
06/17/2022 21:18:19 - INFO - __main__ - ['contradiction']
06/17/2022 21:18:19 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
06/17/2022 21:18:19 - INFO - __main__ - ['contradiction']
06/17/2022 21:18:19 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
06/17/2022 21:18:19 - INFO - __main__ - ['contradiction']
06/17/2022 21:18:19 - INFO - __main__ - Tokenizing Input ...
06/17/2022 21:18:19 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:18:19 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 21:18:20 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_87_0.5_8_predictions.txt
06/17/2022 21:18:20 - INFO - __main__ - ACC on test data: 0.6786
06/17/2022 21:18:21 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.5, bsz=8, dev_performance=0.75, test_performance=0.6785714285714286
06/17/2022 21:18:21 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.4, bsz=8 ...
06/17/2022 21:18:22 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 21:18:22 - INFO - __main__ - Printing 3 examples
06/17/2022 21:18:22 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/17/2022 21:18:22 - INFO - __main__ - ['contradiction']
06/17/2022 21:18:22 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/17/2022 21:18:22 - INFO - __main__ - ['contradiction']
06/17/2022 21:18:22 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/17/2022 21:18:22 - INFO - __main__ - ['contradiction']
06/17/2022 21:18:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 21:18:22 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:18:22 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 21:18:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 21:18:22 - INFO - __main__ - Printing 3 examples
06/17/2022 21:18:22 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
06/17/2022 21:18:22 - INFO - __main__ - ['contradiction']
06/17/2022 21:18:22 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
06/17/2022 21:18:22 - INFO - __main__ - ['contradiction']
06/17/2022 21:18:22 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
06/17/2022 21:18:22 - INFO - __main__ - ['contradiction']
06/17/2022 21:18:22 - INFO - __main__ - Tokenizing Input ...
06/17/2022 21:18:22 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:18:22 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 21:18:34 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 21:18:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 21:18:35 - INFO - __main__ - Starting training!
06/17/2022 21:18:39 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 21:18:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 21:18:40 - INFO - __main__ - Starting training!
06/17/2022 21:18:43 - INFO - __main__ - Step 10 Global step 10 Train loss 1.27 on epoch=3
06/17/2022 21:18:46 - INFO - __main__ - Step 20 Global step 20 Train loss 0.54 on epoch=6
06/17/2022 21:18:49 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=9
06/17/2022 21:18:51 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=13
06/17/2022 21:18:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=16
06/17/2022 21:18:55 - INFO - __main__ - Global step 50 Train loss 0.65 ACC 0.5 on epoch=16
06/17/2022 21:18:55 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 21:18:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.43 on epoch=19
06/17/2022 21:19:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=23
06/17/2022 21:19:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=26
06/17/2022 21:19:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=29
06/17/2022 21:19:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=33
06/17/2022 21:19:09 - INFO - __main__ - Global step 100 Train loss 0.45 ACC 0.5 on epoch=33
06/17/2022 21:19:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=36
06/17/2022 21:19:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=39
06/17/2022 21:19:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.39 on epoch=43
06/17/2022 21:19:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.37 on epoch=46
06/17/2022 21:19:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.39 on epoch=49
06/17/2022 21:19:24 - INFO - __main__ - Global step 150 Train loss 0.40 ACC 0.5 on epoch=49
06/17/2022 21:19:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.36 on epoch=53
06/17/2022 21:19:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=56
06/17/2022 21:19:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=59
06/17/2022 21:19:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=63
06/17/2022 21:19:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.33 on epoch=66
06/17/2022 21:19:38 - INFO - __main__ - Global step 200 Train loss 0.37 ACC 0.5 on epoch=66
06/17/2022 21:19:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.34 on epoch=69
06/17/2022 21:19:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=73
06/17/2022 21:19:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.36 on epoch=76
06/17/2022 21:19:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.29 on epoch=79
06/17/2022 21:19:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.30 on epoch=83
06/17/2022 21:19:53 - INFO - __main__ - Global step 250 Train loss 0.33 ACC 0.59375 on epoch=83
06/17/2022 21:19:53 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=83, global_step=250
06/17/2022 21:19:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=86
06/17/2022 21:19:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=89
06/17/2022 21:20:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=93
06/17/2022 21:20:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=96
06/17/2022 21:20:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.28 on epoch=99
06/17/2022 21:20:07 - INFO - __main__ - Global step 300 Train loss 0.28 ACC 0.5625 on epoch=99
06/17/2022 21:20:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.33 on epoch=103
06/17/2022 21:20:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=106
06/17/2022 21:20:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=109
06/17/2022 21:20:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.21 on epoch=113
06/17/2022 21:20:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=116
06/17/2022 21:20:21 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.625 on epoch=116
06/17/2022 21:20:21 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=116, global_step=350
06/17/2022 21:20:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=119
06/17/2022 21:20:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=123
06/17/2022 21:20:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.18 on epoch=126
06/17/2022 21:20:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.19 on epoch=129
06/17/2022 21:20:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.18 on epoch=133
06/17/2022 21:20:36 - INFO - __main__ - Global step 400 Train loss 0.20 ACC 0.5 on epoch=133
06/17/2022 21:20:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=136
06/17/2022 21:20:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.16 on epoch=139
06/17/2022 21:20:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.16 on epoch=143
06/17/2022 21:20:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.18 on epoch=146
06/17/2022 21:20:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.17 on epoch=149
06/17/2022 21:20:50 - INFO - __main__ - Global step 450 Train loss 0.18 ACC 0.5625 on epoch=149
06/17/2022 21:20:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.14 on epoch=153
06/17/2022 21:20:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.11 on epoch=156
06/17/2022 21:20:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.15 on epoch=159
06/17/2022 21:21:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.13 on epoch=163
06/17/2022 21:21:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.14 on epoch=166
06/17/2022 21:21:04 - INFO - __main__ - Global step 500 Train loss 0.13 ACC 0.5 on epoch=166
06/17/2022 21:21:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.15 on epoch=169
06/17/2022 21:21:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=173
06/17/2022 21:21:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=176
06/17/2022 21:21:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=179
06/17/2022 21:21:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.11 on epoch=183
06/17/2022 21:21:19 - INFO - __main__ - Global step 550 Train loss 0.13 ACC 0.5 on epoch=183
06/17/2022 21:21:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=186
06/17/2022 21:21:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=189
06/17/2022 21:21:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.11 on epoch=193
06/17/2022 21:21:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.12 on epoch=196
06/17/2022 21:21:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=199
06/17/2022 21:21:33 - INFO - __main__ - Global step 600 Train loss 0.12 ACC 0.46875 on epoch=199
06/17/2022 21:21:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=203
06/17/2022 21:21:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=206
06/17/2022 21:21:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=209
06/17/2022 21:21:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.10 on epoch=213
06/17/2022 21:21:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=216
06/17/2022 21:21:47 - INFO - __main__ - Global step 650 Train loss 0.08 ACC 0.46875 on epoch=216
06/17/2022 21:21:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=219
06/17/2022 21:21:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.08 on epoch=223
06/17/2022 21:21:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=226
06/17/2022 21:21:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.07 on epoch=229
06/17/2022 21:22:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=233
06/17/2022 21:22:01 - INFO - __main__ - Global step 700 Train loss 0.06 ACC 0.53125 on epoch=233
06/17/2022 21:22:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=236
06/17/2022 21:22:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=239
06/17/2022 21:22:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=243
06/17/2022 21:22:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=246
06/17/2022 21:22:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=249
06/17/2022 21:22:15 - INFO - __main__ - Global step 750 Train loss 0.06 ACC 0.53125 on epoch=249
06/17/2022 21:22:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=253
06/17/2022 21:22:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=256
06/17/2022 21:22:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=259
06/17/2022 21:22:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=263
06/17/2022 21:22:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=266
06/17/2022 21:22:30 - INFO - __main__ - Global step 800 Train loss 0.04 ACC 0.46875 on epoch=266
06/17/2022 21:22:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=269
06/17/2022 21:22:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=273
06/17/2022 21:22:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=276
06/17/2022 21:22:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=279
06/17/2022 21:22:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=283
06/17/2022 21:22:44 - INFO - __main__ - Global step 850 Train loss 0.08 ACC 0.46875 on epoch=283
06/17/2022 21:22:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=286
06/17/2022 21:22:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=289
06/17/2022 21:22:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=293
06/17/2022 21:22:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=296
06/17/2022 21:22:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=299
06/17/2022 21:22:58 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.46875 on epoch=299
06/17/2022 21:23:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=303
06/17/2022 21:23:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=306
06/17/2022 21:23:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=309
06/17/2022 21:23:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=313
06/17/2022 21:23:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=316
06/17/2022 21:23:12 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.46875 on epoch=316
06/17/2022 21:23:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=319
06/17/2022 21:23:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=323
06/17/2022 21:23:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=326
06/17/2022 21:23:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=329
06/17/2022 21:23:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=333
06/17/2022 21:23:27 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.46875 on epoch=333
06/17/2022 21:23:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=336
06/17/2022 21:23:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=339
06/17/2022 21:23:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=343
06/17/2022 21:23:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=346
06/17/2022 21:23:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=349
06/17/2022 21:23:41 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.46875 on epoch=349
06/17/2022 21:23:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=353
06/17/2022 21:23:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=356
06/17/2022 21:23:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
06/17/2022 21:23:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=363
06/17/2022 21:23:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
06/17/2022 21:23:55 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.5 on epoch=366
06/17/2022 21:23:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=369
06/17/2022 21:24:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
06/17/2022 21:24:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=376
06/17/2022 21:24:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=379
06/17/2022 21:24:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
06/17/2022 21:24:09 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.5625 on epoch=383
06/17/2022 21:24:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=386
06/17/2022 21:24:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=389
06/17/2022 21:24:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
06/17/2022 21:24:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
06/17/2022 21:24:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
06/17/2022 21:24:24 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.65625 on epoch=399
06/17/2022 21:24:24 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=399, global_step=1200
06/17/2022 21:24:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
06/17/2022 21:24:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
06/17/2022 21:24:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
06/17/2022 21:24:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=413
06/17/2022 21:24:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
06/17/2022 21:24:38 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.65625 on epoch=416
06/17/2022 21:24:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=419
06/17/2022 21:24:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
06/17/2022 21:24:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
06/17/2022 21:24:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
06/17/2022 21:24:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
06/17/2022 21:24:53 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.6875 on epoch=433
06/17/2022 21:24:53 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=433, global_step=1300
06/17/2022 21:24:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
06/17/2022 21:24:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/17/2022 21:25:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=443
06/17/2022 21:25:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
06/17/2022 21:25:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=449
06/17/2022 21:25:07 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.6875 on epoch=449
06/17/2022 21:25:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
06/17/2022 21:25:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
06/17/2022 21:25:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
06/17/2022 21:25:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
06/17/2022 21:25:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
06/17/2022 21:25:21 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.71875 on epoch=466
06/17/2022 21:25:22 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=466, global_step=1400
06/17/2022 21:25:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
06/17/2022 21:25:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
06/17/2022 21:25:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
06/17/2022 21:25:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
06/17/2022 21:25:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
06/17/2022 21:25:36 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.71875 on epoch=483
06/17/2022 21:25:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
06/17/2022 21:25:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
06/17/2022 21:25:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
06/17/2022 21:25:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
06/17/2022 21:25:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
06/17/2022 21:25:50 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.75 on epoch=499
06/17/2022 21:25:50 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.75 on epoch=499, global_step=1500
06/17/2022 21:25:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=503
06/17/2022 21:25:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
06/17/2022 21:25:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
06/17/2022 21:26:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
06/17/2022 21:26:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
06/17/2022 21:26:05 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.65625 on epoch=516
06/17/2022 21:26:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
06/17/2022 21:26:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
06/17/2022 21:26:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
06/17/2022 21:26:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/17/2022 21:26:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/17/2022 21:26:19 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.75 on epoch=533
06/17/2022 21:26:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=536
06/17/2022 21:26:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
06/17/2022 21:26:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
06/17/2022 21:26:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/17/2022 21:26:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
06/17/2022 21:26:34 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.71875 on epoch=549
06/17/2022 21:26:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
06/17/2022 21:26:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
06/17/2022 21:26:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/17/2022 21:26:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
06/17/2022 21:26:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
06/17/2022 21:26:48 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.65625 on epoch=566
06/17/2022 21:26:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
06/17/2022 21:26:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
06/17/2022 21:26:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
06/17/2022 21:26:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/17/2022 21:27:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/17/2022 21:27:02 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.71875 on epoch=583
06/17/2022 21:27:05 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/17/2022 21:27:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
06/17/2022 21:27:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/17/2022 21:27:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/17/2022 21:27:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
06/17/2022 21:27:17 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.6875 on epoch=599
06/17/2022 21:27:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=603
06/17/2022 21:27:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
06/17/2022 21:27:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/17/2022 21:27:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=613
06/17/2022 21:27:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
06/17/2022 21:27:31 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.71875 on epoch=616
06/17/2022 21:27:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/17/2022 21:27:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
06/17/2022 21:27:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/17/2022 21:27:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
06/17/2022 21:27:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/17/2022 21:27:45 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.71875 on epoch=633
06/17/2022 21:27:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
06/17/2022 21:27:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/17/2022 21:27:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/17/2022 21:27:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
06/17/2022 21:27:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/17/2022 21:28:00 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.71875 on epoch=649
06/17/2022 21:28:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/17/2022 21:28:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/17/2022 21:28:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/17/2022 21:28:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/17/2022 21:28:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/17/2022 21:28:14 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.6875 on epoch=666
06/17/2022 21:28:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
06/17/2022 21:28:19 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=673
06/17/2022 21:28:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/17/2022 21:28:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/17/2022 21:28:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/17/2022 21:28:28 - INFO - __main__ - Global step 2050 Train loss 0.02 ACC 0.6875 on epoch=683
06/17/2022 21:28:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
06/17/2022 21:28:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/17/2022 21:28:36 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/17/2022 21:28:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=696
06/17/2022 21:28:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/17/2022 21:28:43 - INFO - __main__ - Global step 2100 Train loss 0.01 ACC 0.6875 on epoch=699
06/17/2022 21:28:45 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/17/2022 21:28:48 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=706
06/17/2022 21:28:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/17/2022 21:28:53 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/17/2022 21:28:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/17/2022 21:28:57 - INFO - __main__ - Global step 2150 Train loss 0.01 ACC 0.65625 on epoch=716
06/17/2022 21:29:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/17/2022 21:29:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/17/2022 21:29:05 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/17/2022 21:29:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/17/2022 21:29:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
06/17/2022 21:29:11 - INFO - __main__ - Global step 2200 Train loss 0.00 ACC 0.6875 on epoch=733
06/17/2022 21:29:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/17/2022 21:29:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/17/2022 21:29:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/17/2022 21:29:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
06/17/2022 21:29:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/17/2022 21:29:26 - INFO - __main__ - Global step 2250 Train loss 0.01 ACC 0.65625 on epoch=749
06/17/2022 21:29:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/17/2022 21:29:31 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
06/17/2022 21:29:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/17/2022 21:29:36 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/17/2022 21:29:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/17/2022 21:29:40 - INFO - __main__ - Global step 2300 Train loss 0.00 ACC 0.65625 on epoch=766
06/17/2022 21:29:43 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
06/17/2022 21:29:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/17/2022 21:29:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/17/2022 21:29:51 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/17/2022 21:29:53 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/17/2022 21:29:54 - INFO - __main__ - Global step 2350 Train loss 0.00 ACC 0.65625 on epoch=783
06/17/2022 21:29:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/17/2022 21:30:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/17/2022 21:30:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/17/2022 21:30:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 21:30:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/17/2022 21:30:09 - INFO - __main__ - Global step 2400 Train loss 0.00 ACC 0.71875 on epoch=799
06/17/2022 21:30:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
06/17/2022 21:30:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 21:30:17 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/17/2022 21:30:19 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
06/17/2022 21:30:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/17/2022 21:30:23 - INFO - __main__ - Global step 2450 Train loss 0.01 ACC 0.6875 on epoch=816
06/17/2022 21:30:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/17/2022 21:30:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/17/2022 21:30:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/17/2022 21:30:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/17/2022 21:30:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 21:30:37 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.75 on epoch=833
06/17/2022 21:30:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 21:30:43 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/17/2022 21:30:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
06/17/2022 21:30:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 21:30:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/17/2022 21:30:52 - INFO - __main__ - Global step 2550 Train loss 0.00 ACC 0.71875 on epoch=849
06/17/2022 21:30:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/17/2022 21:30:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 21:31:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/17/2022 21:31:02 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/17/2022 21:31:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 21:31:06 - INFO - __main__ - Global step 2600 Train loss 0.00 ACC 0.65625 on epoch=866
06/17/2022 21:31:09 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/17/2022 21:31:11 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/17/2022 21:31:14 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/17/2022 21:31:17 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/17/2022 21:31:19 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 21:31:20 - INFO - __main__ - Global step 2650 Train loss 0.00 ACC 0.75 on epoch=883
06/17/2022 21:31:23 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 21:31:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 21:31:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 21:31:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/17/2022 21:31:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=899
06/17/2022 21:31:35 - INFO - __main__ - Global step 2700 Train loss 0.01 ACC 0.71875 on epoch=899
06/17/2022 21:31:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=903
06/17/2022 21:31:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 21:31:43 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 21:31:45 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
06/17/2022 21:31:48 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/17/2022 21:31:49 - INFO - __main__ - Global step 2750 Train loss 0.01 ACC 0.75 on epoch=916
06/17/2022 21:31:52 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 21:31:54 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 21:31:57 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/17/2022 21:32:00 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/17/2022 21:32:02 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 21:32:03 - INFO - __main__ - Global step 2800 Train loss 0.00 ACC 0.75 on epoch=933
06/17/2022 21:32:06 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 21:32:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/17/2022 21:32:11 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
06/17/2022 21:32:14 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 21:32:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/17/2022 21:32:18 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.75 on epoch=949
06/17/2022 21:32:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/17/2022 21:32:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/17/2022 21:32:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 21:32:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 21:32:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 21:32:32 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.75 on epoch=966
06/17/2022 21:32:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/17/2022 21:32:37 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 21:32:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/17/2022 21:32:43 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 21:32:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 21:32:46 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.75 on epoch=983
06/17/2022 21:32:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 21:32:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/17/2022 21:32:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 21:32:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/17/2022 21:33:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 21:33:01 - INFO - __main__ - Global step 3000 Train loss 0.00 ACC 0.75 on epoch=999
06/17/2022 21:33:01 - INFO - __main__ - save last model!
06/17/2022 21:33:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 21:33:01 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 21:33:01 - INFO - __main__ - Printing 3 examples
06/17/2022 21:33:01 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 21:33:01 - INFO - __main__ - ['contradiction']
06/17/2022 21:33:01 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 21:33:01 - INFO - __main__ - ['neutral']
06/17/2022 21:33:01 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 21:33:01 - INFO - __main__ - ['entailment']
06/17/2022 21:33:01 - INFO - __main__ - Tokenizing Input ...
06/17/2022 21:33:01 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:33:01 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 21:33:01 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 21:33:01 - INFO - __main__ - Printing 3 examples
06/17/2022 21:33:01 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/17/2022 21:33:01 - INFO - __main__ - ['contradiction']
06/17/2022 21:33:01 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/17/2022 21:33:01 - INFO - __main__ - ['contradiction']
06/17/2022 21:33:01 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/17/2022 21:33:01 - INFO - __main__ - ['contradiction']
06/17/2022 21:33:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 21:33:01 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:33:01 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 21:33:01 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 21:33:01 - INFO - __main__ - Printing 3 examples
06/17/2022 21:33:01 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
06/17/2022 21:33:01 - INFO - __main__ - ['contradiction']
06/17/2022 21:33:01 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
06/17/2022 21:33:01 - INFO - __main__ - ['contradiction']
06/17/2022 21:33:01 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
06/17/2022 21:33:01 - INFO - __main__ - ['contradiction']
06/17/2022 21:33:01 - INFO - __main__ - Tokenizing Input ...
06/17/2022 21:33:01 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:33:01 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 21:33:03 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_87_0.4_8_predictions.txt
06/17/2022 21:33:03 - INFO - __main__ - ACC on test data: 0.6250
06/17/2022 21:33:03 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.4, bsz=8, dev_performance=0.75, test_performance=0.625
06/17/2022 21:33:03 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.3, bsz=8 ...
06/17/2022 21:33:04 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 21:33:04 - INFO - __main__ - Printing 3 examples
06/17/2022 21:33:04 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/17/2022 21:33:04 - INFO - __main__ - ['contradiction']
06/17/2022 21:33:04 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/17/2022 21:33:04 - INFO - __main__ - ['contradiction']
06/17/2022 21:33:04 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/17/2022 21:33:04 - INFO - __main__ - ['contradiction']
06/17/2022 21:33:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 21:33:04 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:33:04 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 21:33:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 21:33:04 - INFO - __main__ - Printing 3 examples
06/17/2022 21:33:04 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
06/17/2022 21:33:04 - INFO - __main__ - ['contradiction']
06/17/2022 21:33:04 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
06/17/2022 21:33:04 - INFO - __main__ - ['contradiction']
06/17/2022 21:33:04 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
06/17/2022 21:33:04 - INFO - __main__ - ['contradiction']
06/17/2022 21:33:04 - INFO - __main__ - Tokenizing Input ...
06/17/2022 21:33:04 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:33:04 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 21:33:20 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 21:33:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 21:33:20 - INFO - __main__ - Starting training!
06/17/2022 21:33:21 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 21:33:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 21:33:22 - INFO - __main__ - Starting training!
06/17/2022 21:33:25 - INFO - __main__ - Step 10 Global step 10 Train loss 1.33 on epoch=3
06/17/2022 21:33:28 - INFO - __main__ - Step 20 Global step 20 Train loss 0.62 on epoch=6
06/17/2022 21:33:31 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=9
06/17/2022 21:33:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=13
06/17/2022 21:33:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=16
06/17/2022 21:33:37 - INFO - __main__ - Global step 50 Train loss 0.68 ACC 0.5 on epoch=16
06/17/2022 21:33:37 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 21:33:39 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=19
06/17/2022 21:33:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=23
06/17/2022 21:33:45 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=26
06/17/2022 21:33:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=29
06/17/2022 21:33:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.41 on epoch=33
06/17/2022 21:33:51 - INFO - __main__ - Global step 100 Train loss 0.46 ACC 0.5 on epoch=33
06/17/2022 21:33:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=36
06/17/2022 21:33:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=39
06/17/2022 21:33:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=43
06/17/2022 21:34:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=46
06/17/2022 21:34:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=49
06/17/2022 21:34:05 - INFO - __main__ - Global step 150 Train loss 0.46 ACC 0.5 on epoch=49
06/17/2022 21:34:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=53
06/17/2022 21:34:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=56
06/17/2022 21:34:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=59
06/17/2022 21:34:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=63
06/17/2022 21:34:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=66
06/17/2022 21:34:19 - INFO - __main__ - Global step 200 Train loss 0.42 ACC 0.5 on epoch=66
06/17/2022 21:34:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=69
06/17/2022 21:34:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=73
06/17/2022 21:34:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=76
06/17/2022 21:34:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=79
06/17/2022 21:34:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=83
06/17/2022 21:34:33 - INFO - __main__ - Global step 250 Train loss 0.37 ACC 0.5 on epoch=83
06/17/2022 21:34:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.32 on epoch=86
06/17/2022 21:34:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.31 on epoch=89
06/17/2022 21:34:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.35 on epoch=93
06/17/2022 21:34:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=96
06/17/2022 21:34:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=99
06/17/2022 21:34:47 - INFO - __main__ - Global step 300 Train loss 0.33 ACC 0.46875 on epoch=99
06/17/2022 21:34:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=103
06/17/2022 21:34:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.30 on epoch=106
06/17/2022 21:34:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=109
06/17/2022 21:34:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=113
06/17/2022 21:35:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=116
06/17/2022 21:35:01 - INFO - __main__ - Global step 350 Train loss 0.29 ACC 0.5 on epoch=116
06/17/2022 21:35:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=119
06/17/2022 21:35:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.28 on epoch=123
06/17/2022 21:35:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=126
06/17/2022 21:35:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=129
06/17/2022 21:35:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=133
06/17/2022 21:35:15 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.53125 on epoch=133
06/17/2022 21:35:15 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=133, global_step=400
06/17/2022 21:35:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=136
06/17/2022 21:35:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=139
06/17/2022 21:35:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=143
06/17/2022 21:35:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=146
06/17/2022 21:35:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.20 on epoch=149
06/17/2022 21:35:30 - INFO - __main__ - Global step 450 Train loss 0.23 ACC 0.5625 on epoch=149
06/17/2022 21:35:30 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=149, global_step=450
06/17/2022 21:35:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=153
06/17/2022 21:35:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.18 on epoch=156
06/17/2022 21:35:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=159
06/17/2022 21:35:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=163
06/17/2022 21:35:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=166
06/17/2022 21:35:44 - INFO - __main__ - Global step 500 Train loss 0.21 ACC 0.6875 on epoch=166
06/17/2022 21:35:44 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.6875 on epoch=166, global_step=500
06/17/2022 21:35:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.18 on epoch=169
06/17/2022 21:35:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=173
06/17/2022 21:35:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=176
06/17/2022 21:35:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=179
06/17/2022 21:35:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.14 on epoch=183
06/17/2022 21:35:58 - INFO - __main__ - Global step 550 Train loss 0.18 ACC 0.65625 on epoch=183
06/17/2022 21:36:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.14 on epoch=186
06/17/2022 21:36:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=189
06/17/2022 21:36:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.14 on epoch=193
06/17/2022 21:36:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=196
06/17/2022 21:36:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=199
06/17/2022 21:36:13 - INFO - __main__ - Global step 600 Train loss 0.15 ACC 0.5625 on epoch=199
06/17/2022 21:36:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=203
06/17/2022 21:36:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=206
06/17/2022 21:36:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=209
06/17/2022 21:36:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=213
06/17/2022 21:36:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=216
06/17/2022 21:36:27 - INFO - __main__ - Global step 650 Train loss 0.14 ACC 0.5 on epoch=216
06/17/2022 21:36:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=219
06/17/2022 21:36:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=223
06/17/2022 21:36:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=226
06/17/2022 21:36:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=229
06/17/2022 21:36:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=233
06/17/2022 21:36:41 - INFO - __main__ - Global step 700 Train loss 0.12 ACC 0.53125 on epoch=233
06/17/2022 21:36:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=236
06/17/2022 21:36:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=239
06/17/2022 21:36:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=243
06/17/2022 21:36:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.12 on epoch=246
06/17/2022 21:36:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=249
06/17/2022 21:36:55 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.5 on epoch=249
06/17/2022 21:36:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=253
06/17/2022 21:37:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=256
06/17/2022 21:37:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=259
06/17/2022 21:37:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=263
06/17/2022 21:37:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=266
06/17/2022 21:37:09 - INFO - __main__ - Global step 800 Train loss 0.10 ACC 0.5 on epoch=266
06/17/2022 21:37:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=269
06/17/2022 21:37:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=273
06/17/2022 21:37:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=276
06/17/2022 21:37:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=279
06/17/2022 21:37:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=283
06/17/2022 21:37:23 - INFO - __main__ - Global step 850 Train loss 0.07 ACC 0.5 on epoch=283
06/17/2022 21:37:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=286
06/17/2022 21:37:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=289
06/17/2022 21:37:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=293
06/17/2022 21:37:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=296
06/17/2022 21:37:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
06/17/2022 21:37:37 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.5625 on epoch=299
06/17/2022 21:37:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=303
06/17/2022 21:37:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=306
06/17/2022 21:37:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=309
06/17/2022 21:37:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=313
06/17/2022 21:37:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=316
06/17/2022 21:37:51 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.625 on epoch=316
06/17/2022 21:37:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=319
06/17/2022 21:37:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.14 on epoch=323
06/17/2022 21:37:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=326
06/17/2022 21:38:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=329
06/17/2022 21:38:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=333
06/17/2022 21:38:05 - INFO - __main__ - Global step 1000 Train loss 0.08 ACC 0.625 on epoch=333
06/17/2022 21:38:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=336
06/17/2022 21:38:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=339
06/17/2022 21:38:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=343
06/17/2022 21:38:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
06/17/2022 21:38:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=349
06/17/2022 21:38:19 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.71875 on epoch=349
06/17/2022 21:38:19 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=349, global_step=1050
06/17/2022 21:38:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=353
06/17/2022 21:38:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=356
06/17/2022 21:38:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=359
06/17/2022 21:38:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=363
06/17/2022 21:38:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=366
06/17/2022 21:38:34 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.75 on epoch=366
06/17/2022 21:38:34 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.75 on epoch=366, global_step=1100
06/17/2022 21:38:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=369
06/17/2022 21:38:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=373
06/17/2022 21:38:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=376
06/17/2022 21:38:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
06/17/2022 21:38:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=383
06/17/2022 21:38:48 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.65625 on epoch=383
06/17/2022 21:38:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=386
06/17/2022 21:38:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=389
06/17/2022 21:38:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=393
06/17/2022 21:38:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
06/17/2022 21:39:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=399
06/17/2022 21:39:02 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.6875 on epoch=399
06/17/2022 21:39:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=403
06/17/2022 21:39:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=406
06/17/2022 21:39:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=409
06/17/2022 21:39:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=413
06/17/2022 21:39:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
06/17/2022 21:39:16 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.6875 on epoch=416
06/17/2022 21:39:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=419
06/17/2022 21:39:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
06/17/2022 21:39:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
06/17/2022 21:39:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=429
06/17/2022 21:39:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=433
06/17/2022 21:39:30 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.6875 on epoch=433
06/17/2022 21:39:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
06/17/2022 21:39:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/17/2022 21:39:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=443
06/17/2022 21:39:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=446
06/17/2022 21:39:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
06/17/2022 21:39:44 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.6875 on epoch=449
06/17/2022 21:39:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=453
06/17/2022 21:39:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=456
06/17/2022 21:39:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
06/17/2022 21:39:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=463
06/17/2022 21:39:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
06/17/2022 21:39:58 - INFO - __main__ - Global step 1400 Train loss 0.04 ACC 0.6875 on epoch=466
06/17/2022 21:40:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
06/17/2022 21:40:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
06/17/2022 21:40:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
06/17/2022 21:40:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
06/17/2022 21:40:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
06/17/2022 21:40:13 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.6875 on epoch=483
06/17/2022 21:40:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
06/17/2022 21:40:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
06/17/2022 21:40:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
06/17/2022 21:40:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=496
06/17/2022 21:40:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
06/17/2022 21:40:27 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.75 on epoch=499
06/17/2022 21:40:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
06/17/2022 21:40:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
06/17/2022 21:40:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=509
06/17/2022 21:40:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
06/17/2022 21:40:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
06/17/2022 21:40:41 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.71875 on epoch=516
06/17/2022 21:40:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=519
06/17/2022 21:40:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
06/17/2022 21:40:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
06/17/2022 21:40:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
06/17/2022 21:40:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/17/2022 21:40:55 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.75 on epoch=533
06/17/2022 21:40:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
06/17/2022 21:41:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
06/17/2022 21:41:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
06/17/2022 21:41:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
06/17/2022 21:41:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/17/2022 21:41:09 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.71875 on epoch=549
06/17/2022 21:41:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
06/17/2022 21:41:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
06/17/2022 21:41:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
06/17/2022 21:41:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
06/17/2022 21:41:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=566
06/17/2022 21:41:23 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.75 on epoch=566
06/17/2022 21:41:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
06/17/2022 21:41:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
06/17/2022 21:41:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/17/2022 21:41:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=579
06/17/2022 21:41:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
06/17/2022 21:41:37 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.71875 on epoch=583
06/17/2022 21:41:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
06/17/2022 21:41:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/17/2022 21:41:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/17/2022 21:41:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/17/2022 21:41:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
06/17/2022 21:41:52 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.71875 on epoch=599
06/17/2022 21:41:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
06/17/2022 21:41:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
06/17/2022 21:41:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/17/2022 21:42:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/17/2022 21:42:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=616
06/17/2022 21:42:06 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.71875 on epoch=616
06/17/2022 21:42:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/17/2022 21:42:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
06/17/2022 21:42:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/17/2022 21:42:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
06/17/2022 21:42:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/17/2022 21:42:20 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.71875 on epoch=633
06/17/2022 21:42:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
06/17/2022 21:42:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
06/17/2022 21:42:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/17/2022 21:42:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=646
06/17/2022 21:42:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/17/2022 21:42:34 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.71875 on epoch=649
06/17/2022 21:42:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/17/2022 21:42:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/17/2022 21:42:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=659
06/17/2022 21:42:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=663
06/17/2022 21:42:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/17/2022 21:42:48 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.71875 on epoch=666
06/17/2022 21:42:51 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
06/17/2022 21:42:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
06/17/2022 21:42:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
06/17/2022 21:42:59 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/17/2022 21:43:01 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/17/2022 21:43:02 - INFO - __main__ - Global step 2050 Train loss 0.01 ACC 0.75 on epoch=683
06/17/2022 21:43:05 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/17/2022 21:43:08 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/17/2022 21:43:10 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=693
06/17/2022 21:43:13 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
06/17/2022 21:43:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=699
06/17/2022 21:43:17 - INFO - __main__ - Global step 2100 Train loss 0.01 ACC 0.71875 on epoch=699
06/17/2022 21:43:19 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/17/2022 21:43:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/17/2022 21:43:25 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=709
06/17/2022 21:43:27 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/17/2022 21:43:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=716
06/17/2022 21:43:31 - INFO - __main__ - Global step 2150 Train loss 0.01 ACC 0.71875 on epoch=716
06/17/2022 21:43:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/17/2022 21:43:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
06/17/2022 21:43:39 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/17/2022 21:43:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=729
06/17/2022 21:43:44 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
06/17/2022 21:43:45 - INFO - __main__ - Global step 2200 Train loss 0.02 ACC 0.75 on epoch=733
06/17/2022 21:43:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/17/2022 21:43:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
06/17/2022 21:43:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/17/2022 21:43:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/17/2022 21:43:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/17/2022 21:43:59 - INFO - __main__ - Global step 2250 Train loss 0.00 ACC 0.78125 on epoch=749
06/17/2022 21:43:59 - INFO - __main__ - Saving model with best ACC: 0.75 -> 0.78125 on epoch=749, global_step=2250
06/17/2022 21:44:02 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
06/17/2022 21:44:04 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/17/2022 21:44:07 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/17/2022 21:44:10 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/17/2022 21:44:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=766
06/17/2022 21:44:13 - INFO - __main__ - Global step 2300 Train loss 0.01 ACC 0.75 on epoch=766
06/17/2022 21:44:16 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/17/2022 21:44:18 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/17/2022 21:44:21 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/17/2022 21:44:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/17/2022 21:44:26 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/17/2022 21:44:27 - INFO - __main__ - Global step 2350 Train loss 0.00 ACC 0.78125 on epoch=783
06/17/2022 21:44:30 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/17/2022 21:44:33 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/17/2022 21:44:35 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=793
06/17/2022 21:44:38 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/17/2022 21:44:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/17/2022 21:44:41 - INFO - __main__ - Global step 2400 Train loss 0.01 ACC 0.71875 on epoch=799
06/17/2022 21:44:44 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
06/17/2022 21:44:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/17/2022 21:44:49 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/17/2022 21:44:52 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/17/2022 21:44:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/17/2022 21:44:56 - INFO - __main__ - Global step 2450 Train loss 0.01 ACC 0.75 on epoch=816
06/17/2022 21:44:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/17/2022 21:45:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/17/2022 21:45:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/17/2022 21:45:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/17/2022 21:45:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/17/2022 21:45:10 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.71875 on epoch=833
06/17/2022 21:45:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/17/2022 21:45:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/17/2022 21:45:18 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/17/2022 21:45:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/17/2022 21:45:23 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/17/2022 21:45:24 - INFO - __main__ - Global step 2550 Train loss 0.00 ACC 0.75 on epoch=849
06/17/2022 21:45:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
06/17/2022 21:45:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 21:45:32 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
06/17/2022 21:45:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
06/17/2022 21:45:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 21:45:38 - INFO - __main__ - Global step 2600 Train loss 0.01 ACC 0.75 on epoch=866
06/17/2022 21:45:41 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=869
06/17/2022 21:45:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/17/2022 21:45:46 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=876
06/17/2022 21:45:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/17/2022 21:45:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/17/2022 21:45:53 - INFO - __main__ - Global step 2650 Train loss 0.01 ACC 0.6875 on epoch=883
06/17/2022 21:45:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
06/17/2022 21:45:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 21:46:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 21:46:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
06/17/2022 21:46:06 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/17/2022 21:46:07 - INFO - __main__ - Global step 2700 Train loss 0.00 ACC 0.75 on epoch=899
06/17/2022 21:46:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/17/2022 21:46:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/17/2022 21:46:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/17/2022 21:46:17 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/17/2022 21:46:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/17/2022 21:46:21 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.71875 on epoch=916
06/17/2022 21:46:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/17/2022 21:46:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 21:46:29 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/17/2022 21:46:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/17/2022 21:46:34 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 21:46:35 - INFO - __main__ - Global step 2800 Train loss 0.00 ACC 0.71875 on epoch=933
06/17/2022 21:46:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/17/2022 21:46:40 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
06/17/2022 21:46:43 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/17/2022 21:46:45 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/17/2022 21:46:48 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=949
06/17/2022 21:46:49 - INFO - __main__ - Global step 2850 Train loss 0.02 ACC 0.75 on epoch=949
06/17/2022 21:46:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/17/2022 21:46:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
06/17/2022 21:46:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/17/2022 21:47:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/17/2022 21:47:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/17/2022 21:47:03 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.71875 on epoch=966
06/17/2022 21:47:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/17/2022 21:47:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 21:47:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/17/2022 21:47:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/17/2022 21:47:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/17/2022 21:47:17 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.71875 on epoch=983
06/17/2022 21:47:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/17/2022 21:47:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/17/2022 21:47:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=993
06/17/2022 21:47:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
06/17/2022 21:47:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/17/2022 21:47:31 - INFO - __main__ - Global step 3000 Train loss 0.01 ACC 0.78125 on epoch=999
06/17/2022 21:47:31 - INFO - __main__ - save last model!
06/17/2022 21:47:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 21:47:31 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 21:47:31 - INFO - __main__ - Printing 3 examples
06/17/2022 21:47:31 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 21:47:31 - INFO - __main__ - ['contradiction']
06/17/2022 21:47:31 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 21:47:31 - INFO - __main__ - ['neutral']
06/17/2022 21:47:31 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 21:47:31 - INFO - __main__ - ['entailment']
06/17/2022 21:47:31 - INFO - __main__ - Tokenizing Input ...
06/17/2022 21:47:32 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:47:32 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 21:47:32 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 21:47:32 - INFO - __main__ - Printing 3 examples
06/17/2022 21:47:32 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/17/2022 21:47:32 - INFO - __main__ - ['contradiction']
06/17/2022 21:47:32 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/17/2022 21:47:32 - INFO - __main__ - ['contradiction']
06/17/2022 21:47:32 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/17/2022 21:47:32 - INFO - __main__ - ['contradiction']
06/17/2022 21:47:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 21:47:32 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:47:32 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 21:47:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 21:47:32 - INFO - __main__ - Printing 3 examples
06/17/2022 21:47:32 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
06/17/2022 21:47:32 - INFO - __main__ - ['contradiction']
06/17/2022 21:47:32 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
06/17/2022 21:47:32 - INFO - __main__ - ['contradiction']
06/17/2022 21:47:32 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
06/17/2022 21:47:32 - INFO - __main__ - ['contradiction']
06/17/2022 21:47:32 - INFO - __main__ - Tokenizing Input ...
06/17/2022 21:47:32 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:47:32 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 21:47:34 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_87_0.3_8_predictions.txt
06/17/2022 21:47:34 - INFO - __main__ - ACC on test data: 0.5893
06/17/2022 21:47:34 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.3, bsz=8, dev_performance=0.78125, test_performance=0.5892857142857143
06/17/2022 21:47:34 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.2, bsz=8 ...
06/17/2022 21:47:35 - INFO - __main__ - Start tokenizing ... 48 instances
06/17/2022 21:47:35 - INFO - __main__ - Printing 3 examples
06/17/2022 21:47:35 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/17/2022 21:47:35 - INFO - __main__ - ['contradiction']
06/17/2022 21:47:35 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/17/2022 21:47:35 - INFO - __main__ - ['contradiction']
06/17/2022 21:47:35 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/17/2022 21:47:35 - INFO - __main__ - ['contradiction']
06/17/2022 21:47:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 21:47:35 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:47:35 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/17/2022 21:47:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/17/2022 21:47:35 - INFO - __main__ - Printing 3 examples
06/17/2022 21:47:35 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
06/17/2022 21:47:35 - INFO - __main__ - ['contradiction']
06/17/2022 21:47:35 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
06/17/2022 21:47:35 - INFO - __main__ - ['contradiction']
06/17/2022 21:47:35 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
06/17/2022 21:47:35 - INFO - __main__ - ['contradiction']
06/17/2022 21:47:35 - INFO - __main__ - Tokenizing Input ...
06/17/2022 21:47:35 - INFO - __main__ - Tokenizing Output ...
06/17/2022 21:47:35 - INFO - __main__ - Loaded 32 examples from dev data
06/17/2022 21:47:50 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 21:47:51 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 21:47:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 21:47:51 - INFO - __main__ - Starting training!
06/17/2022 21:47:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 21:47:52 - INFO - __main__ - Starting training!
06/17/2022 21:47:56 - INFO - __main__ - Step 10 Global step 10 Train loss 1.72 on epoch=3
06/17/2022 21:47:58 - INFO - __main__ - Step 20 Global step 20 Train loss 0.82 on epoch=6
06/17/2022 21:48:01 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=9
06/17/2022 21:48:03 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=13
06/17/2022 21:48:06 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=16
06/17/2022 21:48:07 - INFO - __main__ - Global step 50 Train loss 0.82 ACC 0.5 on epoch=16
06/17/2022 21:48:07 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
06/17/2022 21:48:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=19
06/17/2022 21:48:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=23
06/17/2022 21:48:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=26
06/17/2022 21:48:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=29
06/17/2022 21:48:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=33
06/17/2022 21:48:21 - INFO - __main__ - Global step 100 Train loss 0.47 ACC 0.5 on epoch=33
06/17/2022 21:48:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=36
06/17/2022 21:48:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=39
06/17/2022 21:48:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=43
06/17/2022 21:48:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=46
06/17/2022 21:48:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
06/17/2022 21:48:35 - INFO - __main__ - Global step 150 Train loss 0.45 ACC 0.5 on epoch=49
06/17/2022 21:48:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=53
06/17/2022 21:48:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=56
06/17/2022 21:48:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=59
06/17/2022 21:48:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=63
06/17/2022 21:48:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=66
06/17/2022 21:48:49 - INFO - __main__ - Global step 200 Train loss 0.44 ACC 0.5 on epoch=66
06/17/2022 21:48:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=69
06/17/2022 21:48:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=73
06/17/2022 21:48:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=76
06/17/2022 21:49:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=79
06/17/2022 21:49:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=83
06/17/2022 21:49:04 - INFO - __main__ - Global step 250 Train loss 0.44 ACC 0.53125 on epoch=83
06/17/2022 21:49:04 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=83, global_step=250
06/17/2022 21:49:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=86
06/17/2022 21:49:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=89
06/17/2022 21:49:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=93
06/17/2022 21:49:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=96
06/17/2022 21:49:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=99
06/17/2022 21:49:18 - INFO - __main__ - Global step 300 Train loss 0.41 ACC 0.53125 on epoch=99
06/17/2022 21:49:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=103
06/17/2022 21:49:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
06/17/2022 21:49:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=109
06/17/2022 21:49:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=113
06/17/2022 21:49:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
06/17/2022 21:49:33 - INFO - __main__ - Global step 350 Train loss 0.40 ACC 0.53125 on epoch=116
06/17/2022 21:49:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=119
06/17/2022 21:49:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=123
06/17/2022 21:49:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.33 on epoch=126
06/17/2022 21:49:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.31 on epoch=129
06/17/2022 21:49:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=133
06/17/2022 21:49:47 - INFO - __main__ - Global step 400 Train loss 0.35 ACC 0.5625 on epoch=133
06/17/2022 21:49:47 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=133, global_step=400
06/17/2022 21:49:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.29 on epoch=136
06/17/2022 21:49:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=139
06/17/2022 21:49:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=143
06/17/2022 21:49:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.32 on epoch=146
06/17/2022 21:50:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.28 on epoch=149
06/17/2022 21:50:02 - INFO - __main__ - Global step 450 Train loss 0.30 ACC 0.59375 on epoch=149
06/17/2022 21:50:02 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=149, global_step=450
06/17/2022 21:50:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=153
06/17/2022 21:50:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=156
06/17/2022 21:50:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=159
06/17/2022 21:50:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=163
06/17/2022 21:50:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=166
06/17/2022 21:50:16 - INFO - __main__ - Global step 500 Train loss 0.28 ACC 0.625 on epoch=166
06/17/2022 21:50:16 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=166, global_step=500
06/17/2022 21:50:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=169
06/17/2022 21:50:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.28 on epoch=173
06/17/2022 21:50:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=176
06/17/2022 21:50:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=179
06/17/2022 21:50:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=183
06/17/2022 21:50:31 - INFO - __main__ - Global step 550 Train loss 0.26 ACC 0.65625 on epoch=183
06/17/2022 21:50:31 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=183, global_step=550
06/17/2022 21:50:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.26 on epoch=186
06/17/2022 21:50:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=189
06/17/2022 21:50:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=193
06/17/2022 21:50:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=196
06/17/2022 21:50:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=199
06/17/2022 21:50:45 - INFO - __main__ - Global step 600 Train loss 0.26 ACC 0.6875 on epoch=199
06/17/2022 21:50:45 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=199, global_step=600
06/17/2022 21:50:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=203
06/17/2022 21:50:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.26 on epoch=206
06/17/2022 21:50:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=209
06/17/2022 21:50:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=213
06/17/2022 21:50:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=216
06/17/2022 21:51:00 - INFO - __main__ - Global step 650 Train loss 0.23 ACC 0.53125 on epoch=216
06/17/2022 21:51:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=219
06/17/2022 21:51:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=223
06/17/2022 21:51:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=226
06/17/2022 21:51:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=229
06/17/2022 21:51:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=233
06/17/2022 21:51:14 - INFO - __main__ - Global step 700 Train loss 0.21 ACC 0.59375 on epoch=233
06/17/2022 21:51:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=236
06/17/2022 21:51:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=239
06/17/2022 21:51:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=243
06/17/2022 21:51:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=246
06/17/2022 21:51:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=249
06/17/2022 21:51:29 - INFO - __main__ - Global step 750 Train loss 0.21 ACC 0.59375 on epoch=249
06/17/2022 21:51:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=253
06/17/2022 21:51:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=256
06/17/2022 21:51:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=259
06/17/2022 21:51:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=263
06/17/2022 21:51:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=266
06/17/2022 21:51:43 - INFO - __main__ - Global step 800 Train loss 0.19 ACC 0.5 on epoch=266
06/17/2022 21:51:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=269
06/17/2022 21:51:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=273
06/17/2022 21:51:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=276
06/17/2022 21:51:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=279
06/17/2022 21:51:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.16 on epoch=283
06/17/2022 21:51:57 - INFO - __main__ - Global step 850 Train loss 0.18 ACC 0.5 on epoch=283
06/17/2022 21:52:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=286
06/17/2022 21:52:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.15 on epoch=289
06/17/2022 21:52:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=293
06/17/2022 21:52:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.17 on epoch=296
06/17/2022 21:52:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=299
06/17/2022 21:52:12 - INFO - __main__ - Global step 900 Train loss 0.17 ACC 0.625 on epoch=299
06/17/2022 21:52:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=303
06/17/2022 21:52:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=306
06/17/2022 21:52:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.15 on epoch=309
06/17/2022 21:52:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=313
06/17/2022 21:52:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=316
06/17/2022 21:52:26 - INFO - __main__ - Global step 950 Train loss 0.15 ACC 0.65625 on epoch=316
06/17/2022 21:52:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=319
06/17/2022 21:52:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.14 on epoch=323
06/17/2022 21:52:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=326
06/17/2022 21:52:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=329
06/17/2022 21:52:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=333
06/17/2022 21:52:41 - INFO - __main__ - Global step 1000 Train loss 0.13 ACC 0.625 on epoch=333
06/17/2022 21:52:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.13 on epoch=336
06/17/2022 21:52:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=339
06/17/2022 21:52:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.13 on epoch=343
06/17/2022 21:52:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=346
06/17/2022 21:52:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=349
06/17/2022 21:52:55 - INFO - __main__ - Global step 1050 Train loss 0.12 ACC 0.6875 on epoch=349
06/17/2022 21:52:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=353
06/17/2022 21:53:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=356
06/17/2022 21:53:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.11 on epoch=359
06/17/2022 21:53:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=363
06/17/2022 21:53:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=366
06/17/2022 21:53:10 - INFO - __main__ - Global step 1100 Train loss 0.12 ACC 0.5 on epoch=366
06/17/2022 21:53:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=369
06/17/2022 21:53:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.13 on epoch=373
06/17/2022 21:53:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=376
06/17/2022 21:53:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=379
06/17/2022 21:53:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=383
06/17/2022 21:53:24 - INFO - __main__ - Global step 1150 Train loss 0.12 ACC 0.46875 on epoch=383
06/17/2022 21:53:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=386
06/17/2022 21:53:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.14 on epoch=389
06/17/2022 21:53:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=393
06/17/2022 21:53:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=396
06/17/2022 21:53:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=399
06/17/2022 21:53:39 - INFO - __main__ - Global step 1200 Train loss 0.11 ACC 0.5625 on epoch=399
06/17/2022 21:53:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=403
06/17/2022 21:53:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=406
06/17/2022 21:53:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=409
06/17/2022 21:53:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=413
06/17/2022 21:53:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=416
06/17/2022 21:53:53 - INFO - __main__ - Global step 1250 Train loss 0.07 ACC 0.59375 on epoch=416
06/17/2022 21:53:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=419
06/17/2022 21:53:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=423
06/17/2022 21:54:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=426
06/17/2022 21:54:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=429
06/17/2022 21:54:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=433
06/17/2022 21:54:07 - INFO - __main__ - Global step 1300 Train loss 0.06 ACC 0.65625 on epoch=433
06/17/2022 21:54:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=436
06/17/2022 21:54:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=439
06/17/2022 21:54:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=443
06/17/2022 21:54:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=446
06/17/2022 21:54:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=449
06/17/2022 21:54:22 - INFO - __main__ - Global step 1350 Train loss 0.08 ACC 0.625 on epoch=449
06/17/2022 21:54:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=453
06/17/2022 21:54:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=456
06/17/2022 21:54:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=459
06/17/2022 21:54:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=463
06/17/2022 21:54:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=466
06/17/2022 21:54:36 - INFO - __main__ - Global step 1400 Train loss 0.06 ACC 0.5625 on epoch=466
06/17/2022 21:54:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=469
06/17/2022 21:54:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=473
06/17/2022 21:54:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=476
06/17/2022 21:54:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=479
06/17/2022 21:54:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=483
06/17/2022 21:54:51 - INFO - __main__ - Global step 1450 Train loss 0.07 ACC 0.625 on epoch=483
06/17/2022 21:54:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
06/17/2022 21:54:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
06/17/2022 21:54:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=493
06/17/2022 21:55:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=496
06/17/2022 21:55:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=499
06/17/2022 21:55:05 - INFO - __main__ - Global step 1500 Train loss 0.04 ACC 0.53125 on epoch=499
06/17/2022 21:55:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=503
06/17/2022 21:55:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=506
06/17/2022 21:55:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=509
06/17/2022 21:55:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=513
06/17/2022 21:55:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=516
06/17/2022 21:55:20 - INFO - __main__ - Global step 1550 Train loss 0.07 ACC 0.625 on epoch=516
06/17/2022 21:55:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=519
06/17/2022 21:55:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=523
06/17/2022 21:55:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=526
06/17/2022 21:55:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=529
06/17/2022 21:55:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=533
06/17/2022 21:55:34 - INFO - __main__ - Global step 1600 Train loss 0.06 ACC 0.53125 on epoch=533
06/17/2022 21:55:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=536
06/17/2022 21:55:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=539
06/17/2022 21:55:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
06/17/2022 21:55:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=546
06/17/2022 21:55:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=549
06/17/2022 21:55:48 - INFO - __main__ - Global step 1650 Train loss 0.04 ACC 0.625 on epoch=549
06/17/2022 21:55:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
06/17/2022 21:55:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=556
06/17/2022 21:55:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=559
06/17/2022 21:55:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
06/17/2022 21:56:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=566
06/17/2022 21:56:02 - INFO - __main__ - Global step 1700 Train loss 0.04 ACC 0.59375 on epoch=566
06/17/2022 21:56:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
06/17/2022 21:56:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
06/17/2022 21:56:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=576
06/17/2022 21:56:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=579
06/17/2022 21:56:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=583
06/17/2022 21:56:17 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.625 on epoch=583
06/17/2022 21:56:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
06/17/2022 21:56:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=589
06/17/2022 21:56:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/17/2022 21:56:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
06/17/2022 21:56:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=599
06/17/2022 21:56:31 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.65625 on epoch=599
06/17/2022 21:56:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
06/17/2022 21:56:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=606
06/17/2022 21:56:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/17/2022 21:56:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=613
06/17/2022 21:56:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=616
06/17/2022 21:56:45 - INFO - __main__ - Global step 1850 Train loss 0.03 ACC 0.6875 on epoch=616
06/17/2022 21:56:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
06/17/2022 21:56:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
06/17/2022 21:56:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=626
06/17/2022 21:56:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
06/17/2022 21:56:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=633
06/17/2022 21:56:59 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.5625 on epoch=633
06/17/2022 21:57:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
06/17/2022 21:57:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=639
06/17/2022 21:57:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=643
06/17/2022 21:57:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=646
06/17/2022 21:57:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
06/17/2022 21:57:14 - INFO - __main__ - Global step 1950 Train loss 0.03 ACC 0.65625 on epoch=649
06/17/2022 21:57:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/17/2022 21:57:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
06/17/2022 21:57:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
06/17/2022 21:57:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
06/17/2022 21:57:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=666
06/17/2022 21:57:28 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.625 on epoch=666
06/17/2022 21:57:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=669
06/17/2022 21:57:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
06/17/2022 21:57:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=676
06/17/2022 21:57:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
06/17/2022 21:57:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/17/2022 21:57:42 - INFO - __main__ - Global step 2050 Train loss 0.02 ACC 0.6875 on epoch=683
06/17/2022 21:57:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=686
06/17/2022 21:57:47 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
06/17/2022 21:57:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=693
06/17/2022 21:57:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
06/17/2022 21:57:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=699
06/17/2022 21:57:56 - INFO - __main__ - Global step 2100 Train loss 0.03 ACC 0.6875 on epoch=699
06/17/2022 21:57:59 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=703
06/17/2022 21:58:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/17/2022 21:58:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/17/2022 21:58:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=713
06/17/2022 21:58:10 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=716
06/17/2022 21:58:11 - INFO - __main__ - Global step 2150 Train loss 0.03 ACC 0.6875 on epoch=716
06/17/2022 21:58:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=719
06/17/2022 21:58:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
06/17/2022 21:58:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
06/17/2022 21:58:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
06/17/2022 21:58:24 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=733
06/17/2022 21:58:25 - INFO - __main__ - Global step 2200 Train loss 0.02 ACC 0.6875 on epoch=733
06/17/2022 21:58:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/17/2022 21:58:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
06/17/2022 21:58:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/17/2022 21:58:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
06/17/2022 21:58:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
06/17/2022 21:58:39 - INFO - __main__ - Global step 2250 Train loss 0.02 ACC 0.65625 on epoch=749
06/17/2022 21:58:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
06/17/2022 21:58:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/17/2022 21:58:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=759
06/17/2022 21:58:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
06/17/2022 21:58:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=766
06/17/2022 21:58:53 - INFO - __main__ - Global step 2300 Train loss 0.02 ACC 0.59375 on epoch=766
06/17/2022 21:58:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
06/17/2022 21:58:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/17/2022 21:59:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
06/17/2022 21:59:04 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/17/2022 21:59:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=783
06/17/2022 21:59:07 - INFO - __main__ - Global step 2350 Train loss 0.02 ACC 0.625 on epoch=783
06/17/2022 21:59:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
06/17/2022 21:59:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/17/2022 21:59:15 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
06/17/2022 21:59:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
06/17/2022 21:59:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/17/2022 21:59:22 - INFO - __main__ - Global step 2400 Train loss 0.01 ACC 0.75 on epoch=799
06/17/2022 21:59:22 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.75 on epoch=799, global_step=2400
06/17/2022 21:59:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
06/17/2022 21:59:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=806
06/17/2022 21:59:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/17/2022 21:59:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/17/2022 21:59:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/17/2022 21:59:36 - INFO - __main__ - Global step 2450 Train loss 0.01 ACC 0.65625 on epoch=816
06/17/2022 21:59:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/17/2022 21:59:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
06/17/2022 21:59:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/17/2022 21:59:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/17/2022 21:59:49 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
06/17/2022 21:59:50 - INFO - __main__ - Global step 2500 Train loss 0.01 ACC 0.625 on epoch=833
06/17/2022 21:59:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=836
06/17/2022 21:59:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
06/17/2022 21:59:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
06/17/2022 22:00:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
06/17/2022 22:00:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
06/17/2022 22:00:05 - INFO - __main__ - Global step 2550 Train loss 0.02 ACC 0.65625 on epoch=849
06/17/2022 22:00:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/17/2022 22:00:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/17/2022 22:00:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
06/17/2022 22:00:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/17/2022 22:00:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/17/2022 22:00:19 - INFO - __main__ - Global step 2600 Train loss 0.01 ACC 0.65625 on epoch=866
06/17/2022 22:00:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
06/17/2022 22:00:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/17/2022 22:00:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
06/17/2022 22:00:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
06/17/2022 22:00:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
06/17/2022 22:00:33 - INFO - __main__ - Global step 2650 Train loss 0.01 ACC 0.53125 on epoch=883
06/17/2022 22:00:36 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/17/2022 22:00:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/17/2022 22:00:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/17/2022 22:00:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=896
06/17/2022 22:00:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=899
06/17/2022 22:00:48 - INFO - __main__ - Global step 2700 Train loss 0.02 ACC 0.625 on epoch=899
06/17/2022 22:00:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=903
06/17/2022 22:00:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
06/17/2022 22:00:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
06/17/2022 22:00:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
06/17/2022 22:01:01 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/17/2022 22:01:02 - INFO - __main__ - Global step 2750 Train loss 0.02 ACC 0.59375 on epoch=916
06/17/2022 22:01:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=919
06/17/2022 22:01:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/17/2022 22:01:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
06/17/2022 22:01:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=929
06/17/2022 22:01:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/17/2022 22:01:16 - INFO - __main__ - Global step 2800 Train loss 0.01 ACC 0.65625 on epoch=933
06/17/2022 22:01:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
06/17/2022 22:01:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
06/17/2022 22:01:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
06/17/2022 22:01:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
06/17/2022 22:01:30 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
06/17/2022 22:01:31 - INFO - __main__ - Global step 2850 Train loss 0.01 ACC 0.6875 on epoch=949
06/17/2022 22:01:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/17/2022 22:01:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=956
06/17/2022 22:01:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=959
06/17/2022 22:01:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/17/2022 22:01:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
06/17/2022 22:01:45 - INFO - __main__ - Global step 2900 Train loss 0.02 ACC 0.625 on epoch=966
06/17/2022 22:01:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/17/2022 22:01:50 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/17/2022 22:01:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/17/2022 22:01:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
06/17/2022 22:01:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=983
06/17/2022 22:02:00 - INFO - __main__ - Global step 2950 Train loss 0.01 ACC 0.625 on epoch=983
06/17/2022 22:02:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=986
06/17/2022 22:02:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
06/17/2022 22:02:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/17/2022 22:02:10 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=996
06/17/2022 22:02:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=999
06/17/2022 22:02:14 - INFO - __main__ - Global step 3000 Train loss 0.03 ACC 0.65625 on epoch=999
06/17/2022 22:02:14 - INFO - __main__ - save last model!
06/17/2022 22:02:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 22:02:14 - INFO - __main__ - Start tokenizing ... 56 instances
06/17/2022 22:02:14 - INFO - __main__ - Printing 3 examples
06/17/2022 22:02:14 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/17/2022 22:02:14 - INFO - __main__ - ['contradiction']
06/17/2022 22:02:14 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/17/2022 22:02:14 - INFO - __main__ - ['neutral']
06/17/2022 22:02:14 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/17/2022 22:02:14 - INFO - __main__ - ['entailment']
06/17/2022 22:02:14 - INFO - __main__ - Tokenizing Input ...
06/17/2022 22:02:14 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:02:14 - INFO - __main__ - Loaded 56 examples from test data
06/17/2022 22:02:16 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-superglue-cb/superglue-cb_16_87_0.2_8_predictions.txt
06/17/2022 22:02:16 - INFO - __main__ - ACC on test data: 0.5357
06/17/2022 22:02:17 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.2, bsz=8, dev_performance=0.75, test_performance=0.5357142857142857
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (85189): No such process
Task: dbpedia_14, Checkpoint: models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt, Identifier: T5-large-multitask-cls2cls-5e-1-4-10-up64shot
Output directory () already exists and is not empty.
06/17/2022 22:02:23 - INFO - __main__ - Namespace(task_dir='data/dbpedia_14/', task_name='dbpedia_14', identifier='T5-large-multitask-cls2cls-5e-1-4-10-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/17/2022 22:02:23 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14
06/17/2022 22:02:23 - INFO - __main__ - Namespace(task_dir='data/dbpedia_14/', task_name='dbpedia_14', identifier='T5-large-multitask-cls2cls-5e-1-4-10-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/17/2022 22:02:23 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14
06/17/2022 22:02:23 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/17/2022 22:02:23 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/17/2022 22:02:23 - INFO - __main__ - args.device: cuda:0
06/17/2022 22:02:23 - INFO - __main__ - Using 2 gpus
06/17/2022 22:02:23 - INFO - __main__ - args.device: cuda:1
06/17/2022 22:02:23 - INFO - __main__ - Using 2 gpus
06/17/2022 22:02:23 - INFO - __main__ - Fine-tuning the following samples: ['dbpedia_14_16_100', 'dbpedia_14_16_13', 'dbpedia_14_16_21', 'dbpedia_14_16_42', 'dbpedia_14_16_87']
06/17/2022 22:02:23 - INFO - __main__ - Fine-tuning the following samples: ['dbpedia_14_16_100', 'dbpedia_14_16_13', 'dbpedia_14_16_21', 'dbpedia_14_16_42', 'dbpedia_14_16_87']
06/17/2022 22:02:28 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.5, bsz=8 ...
06/17/2022 22:02:29 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 22:02:29 - INFO - __main__ - Printing 3 examples
06/17/2022 22:02:29 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
06/17/2022 22:02:29 - INFO - __main__ - ['Animal']
06/17/2022 22:02:29 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
06/17/2022 22:02:29 - INFO - __main__ - ['Animal']
06/17/2022 22:02:29 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
06/17/2022 22:02:29 - INFO - __main__ - ['Animal']
06/17/2022 22:02:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 22:02:29 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 22:02:29 - INFO - __main__ - Printing 3 examples
06/17/2022 22:02:29 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
06/17/2022 22:02:29 - INFO - __main__ - ['Animal']
06/17/2022 22:02:29 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
06/17/2022 22:02:29 - INFO - __main__ - ['Animal']
06/17/2022 22:02:29 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
06/17/2022 22:02:29 - INFO - __main__ - ['Animal']
06/17/2022 22:02:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 22:02:29 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:02:29 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:02:29 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/17/2022 22:02:29 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 22:02:29 - INFO - __main__ - Printing 3 examples
06/17/2022 22:02:29 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
06/17/2022 22:02:29 - INFO - __main__ - ['Animal']
06/17/2022 22:02:29 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
06/17/2022 22:02:29 - INFO - __main__ - ['Animal']
06/17/2022 22:02:29 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
06/17/2022 22:02:29 - INFO - __main__ - ['Animal']
06/17/2022 22:02:29 - INFO - __main__ - Tokenizing Input ...
06/17/2022 22:02:29 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/17/2022 22:02:29 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 22:02:29 - INFO - __main__ - Printing 3 examples
06/17/2022 22:02:29 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
06/17/2022 22:02:29 - INFO - __main__ - ['Animal']
06/17/2022 22:02:29 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
06/17/2022 22:02:29 - INFO - __main__ - ['Animal']
06/17/2022 22:02:29 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
06/17/2022 22:02:29 - INFO - __main__ - ['Animal']
06/17/2022 22:02:29 - INFO - __main__ - Tokenizing Input ...
06/17/2022 22:02:29 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:02:29 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:02:29 - INFO - __main__ - Loaded 224 examples from dev data
06/17/2022 22:02:30 - INFO - __main__ - Loaded 224 examples from dev data
06/17/2022 22:02:47 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 22:02:47 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 22:02:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 22:02:48 - INFO - __main__ - Starting training!
06/17/2022 22:02:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 22:02:52 - INFO - __main__ - Starting training!
06/17/2022 22:02:56 - INFO - __main__ - Step 10 Global step 10 Train loss 6.48 on epoch=0
06/17/2022 22:02:59 - INFO - __main__ - Step 20 Global step 20 Train loss 4.64 on epoch=1
06/17/2022 22:03:02 - INFO - __main__ - Step 30 Global step 30 Train loss 3.91 on epoch=2
06/17/2022 22:03:04 - INFO - __main__ - Step 40 Global step 40 Train loss 3.55 on epoch=2
06/17/2022 22:03:07 - INFO - __main__ - Step 50 Global step 50 Train loss 3.06 on epoch=3
06/17/2022 22:03:12 - INFO - __main__ - Global step 50 Train loss 4.33 Classification-F1 0.09449715410666175 on epoch=3
06/17/2022 22:03:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.09449715410666175 on epoch=3, global_step=50
06/17/2022 22:03:14 - INFO - __main__ - Step 60 Global step 60 Train loss 2.91 on epoch=4
06/17/2022 22:03:17 - INFO - __main__ - Step 70 Global step 70 Train loss 2.21 on epoch=4
06/17/2022 22:03:19 - INFO - __main__ - Step 80 Global step 80 Train loss 2.37 on epoch=5
06/17/2022 22:03:22 - INFO - __main__ - Step 90 Global step 90 Train loss 1.99 on epoch=6
06/17/2022 22:03:24 - INFO - __main__ - Step 100 Global step 100 Train loss 1.79 on epoch=7
06/17/2022 22:03:30 - INFO - __main__ - Global step 100 Train loss 2.26 Classification-F1 0.12245518216089807 on epoch=7
06/17/2022 22:03:30 - INFO - __main__ - Saving model with best Classification-F1: 0.09449715410666175 -> 0.12245518216089807 on epoch=7, global_step=100
06/17/2022 22:03:32 - INFO - __main__ - Step 110 Global step 110 Train loss 1.59 on epoch=7
06/17/2022 22:03:35 - INFO - __main__ - Step 120 Global step 120 Train loss 1.50 on epoch=8
06/17/2022 22:03:37 - INFO - __main__ - Step 130 Global step 130 Train loss 1.53 on epoch=9
06/17/2022 22:03:40 - INFO - __main__ - Step 140 Global step 140 Train loss 1.24 on epoch=9
06/17/2022 22:03:42 - INFO - __main__ - Step 150 Global step 150 Train loss 1.10 on epoch=10
06/17/2022 22:03:48 - INFO - __main__ - Global step 150 Train loss 1.39 Classification-F1 0.35281495838727894 on epoch=10
06/17/2022 22:03:48 - INFO - __main__ - Saving model with best Classification-F1: 0.12245518216089807 -> 0.35281495838727894 on epoch=10, global_step=150
06/17/2022 22:03:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.99 on epoch=11
06/17/2022 22:03:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.97 on epoch=12
06/17/2022 22:03:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.72 on epoch=12
06/17/2022 22:03:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.87 on epoch=13
06/17/2022 22:04:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.68 on epoch=14
06/17/2022 22:04:07 - INFO - __main__ - Global step 200 Train loss 0.85 Classification-F1 0.546381314268775 on epoch=14
06/17/2022 22:04:07 - INFO - __main__ - Saving model with best Classification-F1: 0.35281495838727894 -> 0.546381314268775 on epoch=14, global_step=200
06/17/2022 22:04:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.76 on epoch=14
06/17/2022 22:04:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.61 on epoch=15
06/17/2022 22:04:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=16
06/17/2022 22:04:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.60 on epoch=17
06/17/2022 22:04:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=17
06/17/2022 22:04:26 - INFO - __main__ - Global step 250 Train loss 0.58 Classification-F1 0.5899627675718758 on epoch=17
06/17/2022 22:04:26 - INFO - __main__ - Saving model with best Classification-F1: 0.546381314268775 -> 0.5899627675718758 on epoch=17, global_step=250
06/17/2022 22:04:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=18
06/17/2022 22:04:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=19
06/17/2022 22:04:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=19
06/17/2022 22:04:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=20
06/17/2022 22:04:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=21
06/17/2022 22:04:46 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.7283117917526519 on epoch=21
06/17/2022 22:04:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5899627675718758 -> 0.7283117917526519 on epoch=21, global_step=300
06/17/2022 22:04:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=22
06/17/2022 22:04:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=22
06/17/2022 22:04:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=23
06/17/2022 22:04:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=24
06/17/2022 22:04:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.32 on epoch=24
06/17/2022 22:05:05 - INFO - __main__ - Global step 350 Train loss 0.33 Classification-F1 0.6562202470122175 on epoch=24
06/17/2022 22:05:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.31 on epoch=25
06/17/2022 22:05:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=26
06/17/2022 22:05:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.22 on epoch=27
06/17/2022 22:05:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=27
06/17/2022 22:05:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=28
06/17/2022 22:05:25 - INFO - __main__ - Global step 400 Train loss 0.28 Classification-F1 0.7081702845143705 on epoch=28
06/17/2022 22:05:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=29
06/17/2022 22:05:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=29
06/17/2022 22:05:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.29 on epoch=30
06/17/2022 22:05:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=31
06/17/2022 22:05:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.17 on epoch=32
06/17/2022 22:05:44 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.7097366545184383 on epoch=32
06/17/2022 22:05:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.17 on epoch=32
06/17/2022 22:05:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=33
06/17/2022 22:05:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.17 on epoch=34
06/17/2022 22:05:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.18 on epoch=34
06/17/2022 22:05:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=35
06/17/2022 22:06:04 - INFO - __main__ - Global step 500 Train loss 0.19 Classification-F1 0.7686420036836301 on epoch=35
06/17/2022 22:06:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7283117917526519 -> 0.7686420036836301 on epoch=35, global_step=500
06/17/2022 22:06:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=36
06/17/2022 22:06:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.19 on epoch=37
06/17/2022 22:06:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.17 on epoch=37
06/17/2022 22:06:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.16 on epoch=38
06/17/2022 22:06:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.11 on epoch=39
06/17/2022 22:06:23 - INFO - __main__ - Global step 550 Train loss 0.15 Classification-F1 0.8109388819066239 on epoch=39
06/17/2022 22:06:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7686420036836301 -> 0.8109388819066239 on epoch=39, global_step=550
06/17/2022 22:06:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.18 on epoch=39
06/17/2022 22:06:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=40
06/17/2022 22:06:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=41
06/17/2022 22:06:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.11 on epoch=42
06/17/2022 22:06:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=42
06/17/2022 22:06:42 - INFO - __main__ - Global step 600 Train loss 0.15 Classification-F1 0.7722198931050953 on epoch=42
06/17/2022 22:06:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=43
06/17/2022 22:06:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.09 on epoch=44
06/17/2022 22:06:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=44
06/17/2022 22:06:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.14 on epoch=45
06/17/2022 22:06:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=46
06/17/2022 22:07:01 - INFO - __main__ - Global step 650 Train loss 0.12 Classification-F1 0.7736941110755152 on epoch=46
06/17/2022 22:07:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=47
06/17/2022 22:07:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=47
06/17/2022 22:07:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=48
06/17/2022 22:07:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=49
06/17/2022 22:07:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.10 on epoch=49
06/17/2022 22:07:20 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.6683720686916532 on epoch=49
06/17/2022 22:07:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=50
06/17/2022 22:07:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=51
06/17/2022 22:07:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=52
06/17/2022 22:07:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=52
06/17/2022 22:07:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=53
06/17/2022 22:07:39 - INFO - __main__ - Global step 750 Train loss 0.10 Classification-F1 0.6532420984033888 on epoch=53
06/17/2022 22:07:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=54
06/17/2022 22:07:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=54
06/17/2022 22:07:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=55
06/17/2022 22:07:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=56
06/17/2022 22:07:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=57
06/17/2022 22:07:57 - INFO - __main__ - Global step 800 Train loss 0.08 Classification-F1 0.6503905399171555 on epoch=57
06/17/2022 22:08:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=57
06/17/2022 22:08:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=58
06/17/2022 22:08:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=59
06/17/2022 22:08:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=59
06/17/2022 22:08:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=60
06/17/2022 22:08:16 - INFO - __main__ - Global step 850 Train loss 0.09 Classification-F1 0.740393718794841 on epoch=60
06/17/2022 22:08:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.11 on epoch=61
06/17/2022 22:08:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=62
06/17/2022 22:08:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=62
06/17/2022 22:08:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=63
06/17/2022 22:08:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=64
06/17/2022 22:08:35 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.7399929315341502 on epoch=64
06/17/2022 22:08:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=64
06/17/2022 22:08:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=65
06/17/2022 22:08:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=66
06/17/2022 22:08:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=67
06/17/2022 22:08:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=67
06/17/2022 22:08:54 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.79288137542407 on epoch=67
06/17/2022 22:08:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=68
06/17/2022 22:08:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=69
06/17/2022 22:09:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=69
06/17/2022 22:09:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=70
06/17/2022 22:09:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=71
06/17/2022 22:09:13 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.7857512506468864 on epoch=71
06/17/2022 22:09:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=72
06/17/2022 22:09:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=72
06/17/2022 22:09:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=73
06/17/2022 22:09:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=74
06/17/2022 22:09:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=74
06/17/2022 22:09:32 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.79338953266847 on epoch=74
06/17/2022 22:09:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=75
06/17/2022 22:09:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=76
06/17/2022 22:09:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=77
06/17/2022 22:09:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=77
06/17/2022 22:09:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=78
06/17/2022 22:09:50 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.7479393908550489 on epoch=78
06/17/2022 22:09:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=79
06/17/2022 22:09:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=79
06/17/2022 22:09:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=80
06/17/2022 22:10:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=81
06/17/2022 22:10:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=82
06/17/2022 22:10:09 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.8006970891221366 on epoch=82
06/17/2022 22:10:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=82
06/17/2022 22:10:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=83
06/17/2022 22:10:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=84
06/17/2022 22:10:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=84
06/17/2022 22:10:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=85
06/17/2022 22:10:28 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.7860009608991687 on epoch=85
06/17/2022 22:10:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=86
06/17/2022 22:10:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=87
06/17/2022 22:10:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=87
06/17/2022 22:10:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=88
06/17/2022 22:10:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=89
06/17/2022 22:10:47 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.7743999721675802 on epoch=89
06/17/2022 22:10:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=89
06/17/2022 22:10:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=90
06/17/2022 22:10:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=91
06/17/2022 22:10:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=92
06/17/2022 22:10:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=92
06/17/2022 22:11:06 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.7420503033406258 on epoch=92
06/17/2022 22:11:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=93
06/17/2022 22:11:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=94
06/17/2022 22:11:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=94
06/17/2022 22:11:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=95
06/17/2022 22:11:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=96
06/17/2022 22:11:24 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.7685201157686926 on epoch=96
06/17/2022 22:11:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=97
06/17/2022 22:11:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=97
06/17/2022 22:11:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=98
06/17/2022 22:11:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=99
06/17/2022 22:11:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=99
06/17/2022 22:11:43 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.8258812029210512 on epoch=99
06/17/2022 22:11:43 - INFO - __main__ - Saving model with best Classification-F1: 0.8109388819066239 -> 0.8258812029210512 on epoch=99, global_step=1400
06/17/2022 22:11:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=100
06/17/2022 22:11:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=101
06/17/2022 22:11:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=102
06/17/2022 22:11:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=102
06/17/2022 22:11:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=103
06/17/2022 22:12:02 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.801595972373961 on epoch=103
06/17/2022 22:12:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=104
06/17/2022 22:12:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=104
06/17/2022 22:12:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=105
06/17/2022 22:12:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=106
06/17/2022 22:12:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=107
06/17/2022 22:12:21 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.849679863147605 on epoch=107
06/17/2022 22:12:21 - INFO - __main__ - Saving model with best Classification-F1: 0.8258812029210512 -> 0.849679863147605 on epoch=107, global_step=1500
06/17/2022 22:12:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=107
06/17/2022 22:12:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=108
06/17/2022 22:12:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=109
06/17/2022 22:12:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=109
06/17/2022 22:12:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=110
06/17/2022 22:12:40 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.8457697947214076 on epoch=110
06/17/2022 22:12:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=111
06/17/2022 22:12:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=112
06/17/2022 22:12:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=112
06/17/2022 22:12:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=113
06/17/2022 22:12:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=114
06/17/2022 22:12:59 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.855605789073531 on epoch=114
06/17/2022 22:12:59 - INFO - __main__ - Saving model with best Classification-F1: 0.849679863147605 -> 0.855605789073531 on epoch=114, global_step=1600
06/17/2022 22:13:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=114
06/17/2022 22:13:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=115
06/17/2022 22:13:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=116
06/17/2022 22:13:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=117
06/17/2022 22:13:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=117
06/17/2022 22:13:18 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.8544526314924797 on epoch=117
06/17/2022 22:13:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=118
06/17/2022 22:13:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=119
06/17/2022 22:13:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=119
06/17/2022 22:13:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=120
06/17/2022 22:13:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=121
06/17/2022 22:13:37 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.849679863147605 on epoch=121
06/17/2022 22:13:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=122
06/17/2022 22:13:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=122
06/17/2022 22:13:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=123
06/17/2022 22:13:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=124
06/17/2022 22:13:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=124
06/17/2022 22:13:56 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.8488467177983308 on epoch=124
06/17/2022 22:13:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=125
06/17/2022 22:14:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=126
06/17/2022 22:14:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=127
06/17/2022 22:14:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=127
06/17/2022 22:14:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=128
06/17/2022 22:14:15 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.805276036775088 on epoch=128
06/17/2022 22:14:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=129
06/17/2022 22:14:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=129
06/17/2022 22:14:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=130
06/17/2022 22:14:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=131
06/17/2022 22:14:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=132
06/17/2022 22:14:34 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.855605789073531 on epoch=132
06/17/2022 22:14:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=132
06/17/2022 22:14:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=133
06/17/2022 22:14:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=134
06/17/2022 22:14:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=134
06/17/2022 22:14:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=135
06/17/2022 22:14:53 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.8489581259979742 on epoch=135
06/17/2022 22:14:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=136
06/17/2022 22:14:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=137
06/17/2022 22:15:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=137
06/17/2022 22:15:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=138
06/17/2022 22:15:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=139
06/17/2022 22:15:12 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.8527567862245282 on epoch=139
06/17/2022 22:15:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=139
06/17/2022 22:15:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=140
06/17/2022 22:15:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=141
06/17/2022 22:15:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=142
06/17/2022 22:15:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=142
06/17/2022 22:15:31 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.8607143459062259 on epoch=142
06/17/2022 22:15:31 - INFO - __main__ - Saving model with best Classification-F1: 0.855605789073531 -> 0.8607143459062259 on epoch=142, global_step=2000
06/17/2022 22:15:34 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=143
06/17/2022 22:15:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=144
06/17/2022 22:15:39 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=144
06/17/2022 22:15:41 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=145
06/17/2022 22:15:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=146
06/17/2022 22:15:50 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.8607143459062259 on epoch=146
06/17/2022 22:15:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=147
06/17/2022 22:15:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=147
06/17/2022 22:15:58 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=148
06/17/2022 22:16:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=149
06/17/2022 22:16:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=149
06/17/2022 22:16:10 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.8569156856796718 on epoch=149
06/17/2022 22:16:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=150
06/17/2022 22:16:15 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=151
06/17/2022 22:16:17 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=152
06/17/2022 22:16:20 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=152
06/17/2022 22:16:23 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=153
06/17/2022 22:16:29 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.8630131964809384 on epoch=153
06/17/2022 22:16:29 - INFO - __main__ - Saving model with best Classification-F1: 0.8607143459062259 -> 0.8630131964809384 on epoch=153, global_step=2150
06/17/2022 22:16:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=154
06/17/2022 22:16:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=154
06/17/2022 22:16:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=155
06/17/2022 22:16:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=156
06/17/2022 22:16:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=157
06/17/2022 22:16:48 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.8609970674486804 on epoch=157
06/17/2022 22:16:50 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=157
06/17/2022 22:16:53 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=158
06/17/2022 22:16:55 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=159
06/17/2022 22:16:58 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=159
06/17/2022 22:17:01 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=160
06/17/2022 22:17:07 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=160
06/17/2022 22:17:07 - INFO - __main__ - Saving model with best Classification-F1: 0.8630131964809384 -> 0.9910627007401202 on epoch=160, global_step=2250
06/17/2022 22:17:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=161
06/17/2022 22:17:12 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=162
06/17/2022 22:17:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=162
06/17/2022 22:17:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=163
06/17/2022 22:17:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=164
06/17/2022 22:17:26 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.9228413163897036 on epoch=164
06/17/2022 22:17:29 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=164
06/17/2022 22:17:31 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=165
06/17/2022 22:17:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=166
06/17/2022 22:17:36 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=167
06/17/2022 22:17:39 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=167
06/17/2022 22:17:45 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.855605789073531 on epoch=167
06/17/2022 22:17:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=168
06/17/2022 22:17:51 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=169
06/17/2022 22:17:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=169
06/17/2022 22:17:56 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=170
06/17/2022 22:17:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=171
06/17/2022 22:18:05 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.9867213747669157 on epoch=171
06/17/2022 22:18:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=172
06/17/2022 22:18:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=172
06/17/2022 22:18:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=173
06/17/2022 22:18:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=174
06/17/2022 22:18:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=174
06/17/2022 22:18:24 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=174
06/17/2022 22:18:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=175
06/17/2022 22:18:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=176
06/17/2022 22:18:32 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=177
06/17/2022 22:18:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=177
06/17/2022 22:18:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=178
06/17/2022 22:18:43 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=178
06/17/2022 22:18:46 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=179
06/17/2022 22:18:48 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
06/17/2022 22:18:51 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=180
06/17/2022 22:18:53 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=181
06/17/2022 22:18:56 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=182
06/17/2022 22:19:03 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=182
06/17/2022 22:19:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=182
06/17/2022 22:19:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
06/17/2022 22:19:10 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=184
06/17/2022 22:19:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
06/17/2022 22:19:15 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=185
06/17/2022 22:19:22 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=185
06/17/2022 22:19:24 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=186
06/17/2022 22:19:27 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=187
06/17/2022 22:19:30 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
06/17/2022 22:19:32 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=188
06/17/2022 22:19:35 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=189
06/17/2022 22:19:41 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=189
06/17/2022 22:19:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
06/17/2022 22:19:46 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=190
06/17/2022 22:19:49 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
06/17/2022 22:19:51 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
06/17/2022 22:19:54 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=192
06/17/2022 22:20:01 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.9865940511101802 on epoch=192
06/17/2022 22:20:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=193
06/17/2022 22:20:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=194
06/17/2022 22:20:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=194
06/17/2022 22:20:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=195
06/17/2022 22:20:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=196
06/17/2022 22:20:20 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.9228413163897036 on epoch=196
06/17/2022 22:20:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=197
06/17/2022 22:20:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=197
06/17/2022 22:20:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=198
06/17/2022 22:20:30 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=199
06/17/2022 22:20:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=199
06/17/2022 22:20:40 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=199
06/17/2022 22:20:42 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=200
06/17/2022 22:20:45 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=201
06/17/2022 22:20:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=202
06/17/2022 22:20:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=202
06/17/2022 22:20:52 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=203
06/17/2022 22:20:59 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=203
06/17/2022 22:21:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=204
06/17/2022 22:21:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=204
06/17/2022 22:21:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=205
06/17/2022 22:21:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=206
06/17/2022 22:21:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=207
06/17/2022 22:21:18 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.9228413163897036 on epoch=207
06/17/2022 22:21:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=207
06/17/2022 22:21:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=208
06/17/2022 22:21:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=209
06/17/2022 22:21:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=209
06/17/2022 22:21:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=210
06/17/2022 22:21:38 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=210
06/17/2022 22:21:40 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=211
06/17/2022 22:21:43 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
06/17/2022 22:21:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=212
06/17/2022 22:21:48 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=213
06/17/2022 22:21:50 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=214
06/17/2022 22:21:52 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 22:21:52 - INFO - __main__ - Printing 3 examples
06/17/2022 22:21:52 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
06/17/2022 22:21:52 - INFO - __main__ - ['Animal']
06/17/2022 22:21:52 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
06/17/2022 22:21:52 - INFO - __main__ - ['Animal']
06/17/2022 22:21:52 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
06/17/2022 22:21:52 - INFO - __main__ - ['Animal']
06/17/2022 22:21:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 22:21:52 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:21:52 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/17/2022 22:21:52 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 22:21:52 - INFO - __main__ - Printing 3 examples
06/17/2022 22:21:52 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
06/17/2022 22:21:52 - INFO - __main__ - ['Animal']
06/17/2022 22:21:52 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
06/17/2022 22:21:52 - INFO - __main__ - ['Animal']
06/17/2022 22:21:52 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
06/17/2022 22:21:52 - INFO - __main__ - ['Animal']
06/17/2022 22:21:52 - INFO - __main__ - Tokenizing Input ...
06/17/2022 22:21:52 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:21:53 - INFO - __main__ - Loaded 224 examples from dev data
06/17/2022 22:21:57 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=214
06/17/2022 22:21:57 - INFO - __main__ - save last model!
06/17/2022 22:21:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 22:21:58 - INFO - __main__ - Start tokenizing ... 3500 instances
06/17/2022 22:21:58 - INFO - __main__ - Printing 3 examples
06/17/2022 22:21:58 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/17/2022 22:21:58 - INFO - __main__ - ['Animal']
06/17/2022 22:21:58 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/17/2022 22:21:58 - INFO - __main__ - ['Animal']
06/17/2022 22:21:58 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/17/2022 22:21:58 - INFO - __main__ - ['Village']
06/17/2022 22:21:58 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 22:21:59 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:22:03 - INFO - __main__ - Loaded 3500 examples from test data
06/17/2022 22:22:08 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 22:22:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 22:22:08 - INFO - __main__ - Starting training!
06/17/2022 22:24:35 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_100_0.5_8_predictions.txt
06/17/2022 22:24:35 - INFO - __main__ - Classification-F1 on test data: 0.6503
06/17/2022 22:24:35 - INFO - __main__ - prefix=dbpedia_14_16_100, lr=0.5, bsz=8, dev_performance=0.9910627007401202, test_performance=0.6503113726227004
06/17/2022 22:24:35 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.4, bsz=8 ...
06/17/2022 22:24:36 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 22:24:36 - INFO - __main__ - Printing 3 examples
06/17/2022 22:24:36 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
06/17/2022 22:24:36 - INFO - __main__ - ['Animal']
06/17/2022 22:24:36 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
06/17/2022 22:24:36 - INFO - __main__ - ['Animal']
06/17/2022 22:24:36 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
06/17/2022 22:24:36 - INFO - __main__ - ['Animal']
06/17/2022 22:24:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 22:24:36 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:24:36 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/17/2022 22:24:36 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 22:24:36 - INFO - __main__ - Printing 3 examples
06/17/2022 22:24:36 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
06/17/2022 22:24:36 - INFO - __main__ - ['Animal']
06/17/2022 22:24:36 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
06/17/2022 22:24:36 - INFO - __main__ - ['Animal']
06/17/2022 22:24:36 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
06/17/2022 22:24:36 - INFO - __main__ - ['Animal']
06/17/2022 22:24:36 - INFO - __main__ - Tokenizing Input ...
06/17/2022 22:24:37 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:24:37 - INFO - __main__ - Loaded 224 examples from dev data
06/17/2022 22:24:55 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 22:24:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 22:24:56 - INFO - __main__ - Starting training!
06/17/2022 22:25:00 - INFO - __main__ - Step 10 Global step 10 Train loss 6.59 on epoch=0
06/17/2022 22:25:02 - INFO - __main__ - Step 20 Global step 20 Train loss 4.79 on epoch=1
06/17/2022 22:25:05 - INFO - __main__ - Step 30 Global step 30 Train loss 4.03 on epoch=2
06/17/2022 22:25:07 - INFO - __main__ - Step 40 Global step 40 Train loss 3.63 on epoch=2
06/17/2022 22:25:10 - INFO - __main__ - Step 50 Global step 50 Train loss 3.29 on epoch=3
06/17/2022 22:25:16 - INFO - __main__ - Global step 50 Train loss 4.47 Classification-F1 0.07886064255828339 on epoch=3
06/17/2022 22:25:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.07886064255828339 on epoch=3, global_step=50
06/17/2022 22:25:18 - INFO - __main__ - Step 60 Global step 60 Train loss 3.29 on epoch=4
06/17/2022 22:25:21 - INFO - __main__ - Step 70 Global step 70 Train loss 2.50 on epoch=4
06/17/2022 22:25:24 - INFO - __main__ - Step 80 Global step 80 Train loss 2.46 on epoch=5
06/17/2022 22:25:26 - INFO - __main__ - Step 90 Global step 90 Train loss 2.16 on epoch=6
06/17/2022 22:25:29 - INFO - __main__ - Step 100 Global step 100 Train loss 1.97 on epoch=7
06/17/2022 22:25:35 - INFO - __main__ - Global step 100 Train loss 2.48 Classification-F1 0.10455398842495615 on epoch=7
06/17/2022 22:25:35 - INFO - __main__ - Saving model with best Classification-F1: 0.07886064255828339 -> 0.10455398842495615 on epoch=7, global_step=100
06/17/2022 22:25:37 - INFO - __main__ - Step 110 Global step 110 Train loss 1.85 on epoch=7
06/17/2022 22:25:40 - INFO - __main__ - Step 120 Global step 120 Train loss 1.77 on epoch=8
06/17/2022 22:25:42 - INFO - __main__ - Step 130 Global step 130 Train loss 1.81 on epoch=9
06/17/2022 22:25:45 - INFO - __main__ - Step 140 Global step 140 Train loss 1.37 on epoch=9
06/17/2022 22:25:48 - INFO - __main__ - Step 150 Global step 150 Train loss 1.47 on epoch=10
06/17/2022 22:25:54 - INFO - __main__ - Global step 150 Train loss 1.65 Classification-F1 0.17848438152564527 on epoch=10
06/17/2022 22:25:54 - INFO - __main__ - Saving model with best Classification-F1: 0.10455398842495615 -> 0.17848438152564527 on epoch=10, global_step=150
06/17/2022 22:25:56 - INFO - __main__ - Step 160 Global step 160 Train loss 1.23 on epoch=11
06/17/2022 22:25:59 - INFO - __main__ - Step 170 Global step 170 Train loss 1.19 on epoch=12
06/17/2022 22:26:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.99 on epoch=12
06/17/2022 22:26:04 - INFO - __main__ - Step 190 Global step 190 Train loss 1.07 on epoch=13
06/17/2022 22:26:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.97 on epoch=14
06/17/2022 22:26:13 - INFO - __main__ - Global step 200 Train loss 1.09 Classification-F1 0.36594944133290835 on epoch=14
06/17/2022 22:26:13 - INFO - __main__ - Saving model with best Classification-F1: 0.17848438152564527 -> 0.36594944133290835 on epoch=14, global_step=200
06/17/2022 22:26:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.87 on epoch=14
06/17/2022 22:26:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.82 on epoch=15
06/17/2022 22:26:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.80 on epoch=16
06/17/2022 22:26:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.66 on epoch=17
06/17/2022 22:26:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.63 on epoch=17
06/17/2022 22:26:33 - INFO - __main__ - Global step 250 Train loss 0.76 Classification-F1 0.48846904958767806 on epoch=17
06/17/2022 22:26:33 - INFO - __main__ - Saving model with best Classification-F1: 0.36594944133290835 -> 0.48846904958767806 on epoch=17, global_step=250
06/17/2022 22:26:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=18
06/17/2022 22:26:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.64 on epoch=19
06/17/2022 22:26:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=19
06/17/2022 22:26:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=20
06/17/2022 22:26:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=21
06/17/2022 22:26:53 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.5596950777852538 on epoch=21
06/17/2022 22:26:53 - INFO - __main__ - Saving model with best Classification-F1: 0.48846904958767806 -> 0.5596950777852538 on epoch=21, global_step=300
06/17/2022 22:26:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=22
06/17/2022 22:26:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=22
06/17/2022 22:27:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.54 on epoch=23
06/17/2022 22:27:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=24
06/17/2022 22:27:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=24
06/17/2022 22:27:12 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.5460094837373939 on epoch=24
06/17/2022 22:27:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=25
06/17/2022 22:27:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=26
06/17/2022 22:27:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=27
06/17/2022 22:27:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=27
06/17/2022 22:27:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=28
06/17/2022 22:27:31 - INFO - __main__ - Global step 400 Train loss 0.32 Classification-F1 0.6050891275376231 on epoch=28
06/17/2022 22:27:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5596950777852538 -> 0.6050891275376231 on epoch=28, global_step=400
06/17/2022 22:27:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=29
06/17/2022 22:27:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=29
06/17/2022 22:27:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.20 on epoch=30
06/17/2022 22:27:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=31
06/17/2022 22:27:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=32
06/17/2022 22:27:51 - INFO - __main__ - Global step 450 Train loss 0.26 Classification-F1 0.6471412570404507 on epoch=32
06/17/2022 22:27:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6050891275376231 -> 0.6471412570404507 on epoch=32, global_step=450
06/17/2022 22:27:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.18 on epoch=32
06/17/2022 22:27:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.18 on epoch=33
06/17/2022 22:27:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=34
06/17/2022 22:28:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.12 on epoch=34
06/17/2022 22:28:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=35
06/17/2022 22:28:10 - INFO - __main__ - Global step 500 Train loss 0.20 Classification-F1 0.6560433557587257 on epoch=35
06/17/2022 22:28:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6471412570404507 -> 0.6560433557587257 on epoch=35, global_step=500
06/17/2022 22:28:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=36
06/17/2022 22:28:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.18 on epoch=37
06/17/2022 22:28:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=37
06/17/2022 22:28:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=38
06/17/2022 22:28:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=39
06/17/2022 22:28:29 - INFO - __main__ - Global step 550 Train loss 0.19 Classification-F1 0.6893386922359784 on epoch=39
06/17/2022 22:28:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6560433557587257 -> 0.6893386922359784 on epoch=39, global_step=550
06/17/2022 22:28:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=39
06/17/2022 22:28:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=40
06/17/2022 22:28:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=41
06/17/2022 22:28:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=42
06/17/2022 22:28:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=42
06/17/2022 22:28:48 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.742334240849215 on epoch=42
06/17/2022 22:28:48 - INFO - __main__ - Saving model with best Classification-F1: 0.6893386922359784 -> 0.742334240849215 on epoch=42, global_step=600
06/17/2022 22:28:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=43
06/17/2022 22:28:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=44
06/17/2022 22:28:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=44
06/17/2022 22:28:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=45
06/17/2022 22:29:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=46
06/17/2022 22:29:08 - INFO - __main__ - Global step 650 Train loss 0.14 Classification-F1 0.8292795123914669 on epoch=46
06/17/2022 22:29:08 - INFO - __main__ - Saving model with best Classification-F1: 0.742334240849215 -> 0.8292795123914669 on epoch=46, global_step=650
06/17/2022 22:29:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=47
06/17/2022 22:29:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=47
06/17/2022 22:29:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.15 on epoch=48
06/17/2022 22:29:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=49
06/17/2022 22:29:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=49
06/17/2022 22:29:27 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.8273885610520506 on epoch=49
06/17/2022 22:29:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=50
06/17/2022 22:29:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=51
06/17/2022 22:29:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=52
06/17/2022 22:29:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=52
06/17/2022 22:29:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.13 on epoch=53
06/17/2022 22:29:46 - INFO - __main__ - Global step 750 Train loss 0.14 Classification-F1 0.839312126835846 on epoch=53
06/17/2022 22:29:46 - INFO - __main__ - Saving model with best Classification-F1: 0.8292795123914669 -> 0.839312126835846 on epoch=53, global_step=750
06/17/2022 22:29:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.15 on epoch=54
06/17/2022 22:29:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=54
06/17/2022 22:29:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=55
06/17/2022 22:29:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=56
06/17/2022 22:29:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=57
06/17/2022 22:30:05 - INFO - __main__ - Global step 800 Train loss 0.12 Classification-F1 0.8905791994217043 on epoch=57
06/17/2022 22:30:05 - INFO - __main__ - Saving model with best Classification-F1: 0.839312126835846 -> 0.8905791994217043 on epoch=57, global_step=800
06/17/2022 22:30:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=57
06/17/2022 22:30:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.15 on epoch=58
06/17/2022 22:30:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.13 on epoch=59
06/17/2022 22:30:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=59
06/17/2022 22:30:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=60
06/17/2022 22:30:24 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.8312948110520506 on epoch=60
06/17/2022 22:30:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=61
06/17/2022 22:30:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.13 on epoch=62
06/17/2022 22:30:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=62
06/17/2022 22:30:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=63
06/17/2022 22:30:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=64
06/17/2022 22:30:43 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.838251332791181 on epoch=64
06/17/2022 22:30:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=64
06/17/2022 22:30:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=65
06/17/2022 22:30:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=66
06/17/2022 22:30:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=67
06/17/2022 22:30:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=67
06/17/2022 22:31:02 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.7404226518408517 on epoch=67
06/17/2022 22:31:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=68
06/17/2022 22:31:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=69
06/17/2022 22:31:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=69
06/17/2022 22:31:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=70
06/17/2022 22:31:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=71
06/17/2022 22:31:21 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.7255424033412647 on epoch=71
06/17/2022 22:31:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=72
06/17/2022 22:31:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=72
06/17/2022 22:31:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=73
06/17/2022 22:31:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=74
06/17/2022 22:31:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=74
06/17/2022 22:31:40 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.6988595322796293 on epoch=74
06/17/2022 22:31:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=75
06/17/2022 22:31:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=76
06/17/2022 22:31:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=77
06/17/2022 22:31:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=77
06/17/2022 22:31:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=78
06/17/2022 22:31:59 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.8488237173958237 on epoch=78
06/17/2022 22:32:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=79
06/17/2022 22:32:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=79
06/17/2022 22:32:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=80
06/17/2022 22:32:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=81
06/17/2022 22:32:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=82
06/17/2022 22:32:19 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.7512170212294533 on epoch=82
06/17/2022 22:32:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=82
06/17/2022 22:32:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=83
06/17/2022 22:32:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=84
06/17/2022 22:32:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=84
06/17/2022 22:32:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=85
06/17/2022 22:32:39 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.8517190747266649 on epoch=85
06/17/2022 22:32:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=86
06/17/2022 22:32:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=87
06/17/2022 22:32:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=87
06/17/2022 22:32:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=88
06/17/2022 22:32:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=89
06/17/2022 22:32:58 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.9063408481672239 on epoch=89
06/17/2022 22:32:58 - INFO - __main__ - Saving model with best Classification-F1: 0.8905791994217043 -> 0.9063408481672239 on epoch=89, global_step=1250
06/17/2022 22:33:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=89
06/17/2022 22:33:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=90
06/17/2022 22:33:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=91
06/17/2022 22:33:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=92
06/17/2022 22:33:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=92
06/17/2022 22:33:18 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.9187894121480459 on epoch=92
06/17/2022 22:33:18 - INFO - __main__ - Saving model with best Classification-F1: 0.9063408481672239 -> 0.9187894121480459 on epoch=92, global_step=1300
06/17/2022 22:33:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=93
06/17/2022 22:33:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=94
06/17/2022 22:33:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=94
06/17/2022 22:33:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=95
06/17/2022 22:33:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=96
06/17/2022 22:33:38 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.9187894121480459 on epoch=96
06/17/2022 22:33:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=97
06/17/2022 22:33:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=97
06/17/2022 22:33:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=98
06/17/2022 22:33:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=99
06/17/2022 22:33:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=99
06/17/2022 22:33:57 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.9682345730732829 on epoch=99
06/17/2022 22:33:57 - INFO - __main__ - Saving model with best Classification-F1: 0.9187894121480459 -> 0.9682345730732829 on epoch=99, global_step=1400
06/17/2022 22:33:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=100
06/17/2022 22:34:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=101
06/17/2022 22:34:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=102
06/17/2022 22:34:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=102
06/17/2022 22:34:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=103
06/17/2022 22:34:16 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.9867213747669157 on epoch=103
06/17/2022 22:34:16 - INFO - __main__ - Saving model with best Classification-F1: 0.9682345730732829 -> 0.9867213747669157 on epoch=103, global_step=1450
06/17/2022 22:34:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=104
06/17/2022 22:34:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=104
06/17/2022 22:34:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=105
06/17/2022 22:34:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=106
06/17/2022 22:34:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=107
06/17/2022 22:34:36 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.9779157630794254 on epoch=107
06/17/2022 22:34:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=107
06/17/2022 22:34:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=108
06/17/2022 22:34:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=109
06/17/2022 22:34:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=109
06/17/2022 22:34:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=110
06/17/2022 22:34:55 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.9106508840095179 on epoch=110
06/17/2022 22:34:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=111
06/17/2022 22:35:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=112
06/17/2022 22:35:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=112
06/17/2022 22:35:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=113
06/17/2022 22:35:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=114
06/17/2022 22:35:15 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.9106508840095179 on epoch=114
06/17/2022 22:35:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=114
06/17/2022 22:35:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=115
06/17/2022 22:35:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=116
06/17/2022 22:35:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=117
06/17/2022 22:35:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=117
06/17/2022 22:35:35 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.9780015231899212 on epoch=117
06/17/2022 22:35:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=118
06/17/2022 22:35:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=119
06/17/2022 22:35:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=119
06/17/2022 22:35:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=120
06/17/2022 22:35:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=121
06/17/2022 22:35:54 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.9099914938166591 on epoch=121
06/17/2022 22:35:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=122
06/17/2022 22:36:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=122
06/17/2022 22:36:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=123
06/17/2022 22:36:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=124
06/17/2022 22:36:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=124
06/17/2022 22:36:14 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.7981256175251041 on epoch=124
06/17/2022 22:36:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=125
06/17/2022 22:36:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=126
06/17/2022 22:36:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=127
06/17/2022 22:36:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=127
06/17/2022 22:36:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=128
06/17/2022 22:36:34 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.9147375079063884 on epoch=128
06/17/2022 22:36:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=129
06/17/2022 22:36:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=129
06/17/2022 22:36:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=130
06/17/2022 22:36:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=131
06/17/2022 22:36:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=132
06/17/2022 22:36:53 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.9187894121480459 on epoch=132
06/17/2022 22:36:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=132
06/17/2022 22:36:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=133
06/17/2022 22:37:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=134
06/17/2022 22:37:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=134
06/17/2022 22:37:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=135
06/17/2022 22:37:13 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.9867213747669157 on epoch=135
06/17/2022 22:37:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=136
06/17/2022 22:37:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=137
06/17/2022 22:37:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=137
06/17/2022 22:37:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=138
06/17/2022 22:37:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=139
06/17/2022 22:37:33 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.9867213747669157 on epoch=139
06/17/2022 22:37:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=139
06/17/2022 22:37:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=140
06/17/2022 22:37:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=141
06/17/2022 22:37:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=142
06/17/2022 22:37:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=142
06/17/2022 22:37:53 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.8592145362543845 on epoch=142
06/17/2022 22:37:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=143
06/17/2022 22:37:58 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=144
06/17/2022 22:38:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=144
06/17/2022 22:38:03 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=145
06/17/2022 22:38:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=146
06/17/2022 22:38:13 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.9187894121480461 on epoch=146
06/17/2022 22:38:15 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=147
06/17/2022 22:38:18 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=147
06/17/2022 22:38:20 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=148
06/17/2022 22:38:23 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=149
06/17/2022 22:38:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=149
06/17/2022 22:38:32 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.863147605083089 on epoch=149
06/17/2022 22:38:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=150
06/17/2022 22:38:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=151
06/17/2022 22:38:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=152
06/17/2022 22:38:43 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=152
06/17/2022 22:38:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=153
06/17/2022 22:38:52 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.9228413163897036 on epoch=153
06/17/2022 22:38:55 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=154
06/17/2022 22:38:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=154
06/17/2022 22:39:00 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=155
06/17/2022 22:39:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=156
06/17/2022 22:39:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=157
06/17/2022 22:39:12 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.9186746497230369 on epoch=157
06/17/2022 22:39:15 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=157
06/17/2022 22:39:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=158
06/17/2022 22:39:20 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=159
06/17/2022 22:39:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=159
06/17/2022 22:39:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=160
06/17/2022 22:39:32 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.9144753033178079 on epoch=160
06/17/2022 22:39:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=161
06/17/2022 22:39:38 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=162
06/17/2022 22:39:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=162
06/17/2022 22:39:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=163
06/17/2022 22:39:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=164
06/17/2022 22:39:52 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.9820991153059465 on epoch=164
06/17/2022 22:39:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=164
06/17/2022 22:39:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
06/17/2022 22:40:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=166
06/17/2022 22:40:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=167
06/17/2022 22:40:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=167
06/17/2022 22:40:13 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.9867213747669157 on epoch=167
06/17/2022 22:40:15 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=168
06/17/2022 22:40:18 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=169
06/17/2022 22:40:20 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=169
06/17/2022 22:40:23 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=170
06/17/2022 22:40:26 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=171
06/17/2022 22:40:33 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.9867213747669157 on epoch=171
06/17/2022 22:40:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=172
06/17/2022 22:40:38 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=172
06/17/2022 22:40:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=173
06/17/2022 22:40:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=174
06/17/2022 22:40:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=174
06/17/2022 22:40:52 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.9867213747669157 on epoch=174
06/17/2022 22:40:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=175
06/17/2022 22:40:58 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=176
06/17/2022 22:41:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=177
06/17/2022 22:41:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
06/17/2022 22:41:05 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=178
06/17/2022 22:41:13 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.9867213747669157 on epoch=178
06/17/2022 22:41:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=179
06/17/2022 22:41:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
06/17/2022 22:41:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=180
06/17/2022 22:41:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=181
06/17/2022 22:41:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=182
06/17/2022 22:41:33 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.9867213747669157 on epoch=182
06/17/2022 22:41:36 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
06/17/2022 22:41:38 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
06/17/2022 22:41:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=184
06/17/2022 22:41:43 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=184
06/17/2022 22:41:46 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=185
06/17/2022 22:41:53 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=185
06/17/2022 22:41:53 - INFO - __main__ - Saving model with best Classification-F1: 0.9867213747669157 -> 0.9910627007401202 on epoch=185, global_step=2600
06/17/2022 22:41:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=186
06/17/2022 22:41:58 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=187
06/17/2022 22:42:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
06/17/2022 22:42:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=188
06/17/2022 22:42:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=189
06/17/2022 22:42:13 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.9228413163897036 on epoch=189
06/17/2022 22:42:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=189
06/17/2022 22:42:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=190
06/17/2022 22:42:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
06/17/2022 22:42:23 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
06/17/2022 22:42:26 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=192
06/17/2022 22:42:34 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.9867213747669157 on epoch=192
06/17/2022 22:42:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=193
06/17/2022 22:42:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=194
06/17/2022 22:42:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
06/17/2022 22:42:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=195
06/17/2022 22:42:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=196
06/17/2022 22:42:55 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.9867213747669157 on epoch=196
06/17/2022 22:42:57 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=197
06/17/2022 22:43:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=197
06/17/2022 22:43:02 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
06/17/2022 22:43:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=199
06/17/2022 22:43:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=199
06/17/2022 22:43:15 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.9187894121480459 on epoch=199
06/17/2022 22:43:18 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=200
06/17/2022 22:43:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=201
06/17/2022 22:43:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=202
06/17/2022 22:43:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=202
06/17/2022 22:43:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=203
06/17/2022 22:43:36 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.9867213747669157 on epoch=203
06/17/2022 22:43:38 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=204
06/17/2022 22:43:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=204
06/17/2022 22:43:44 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=205
06/17/2022 22:43:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=206
06/17/2022 22:43:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
06/17/2022 22:43:57 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.9867213747669157 on epoch=207
06/17/2022 22:43:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=207
06/17/2022 22:44:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=208
06/17/2022 22:44:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
06/17/2022 22:44:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
06/17/2022 22:44:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=210
06/17/2022 22:44:17 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.9164955053380103 on epoch=210
06/17/2022 22:44:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=211
06/17/2022 22:44:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
06/17/2022 22:44:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=212
06/17/2022 22:44:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=213
06/17/2022 22:44:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=214
06/17/2022 22:44:32 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 22:44:32 - INFO - __main__ - Printing 3 examples
06/17/2022 22:44:32 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
06/17/2022 22:44:32 - INFO - __main__ - ['Animal']
06/17/2022 22:44:32 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
06/17/2022 22:44:32 - INFO - __main__ - ['Animal']
06/17/2022 22:44:32 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
06/17/2022 22:44:32 - INFO - __main__ - ['Animal']
06/17/2022 22:44:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 22:44:32 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:44:32 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/17/2022 22:44:32 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 22:44:32 - INFO - __main__ - Printing 3 examples
06/17/2022 22:44:32 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
06/17/2022 22:44:32 - INFO - __main__ - ['Animal']
06/17/2022 22:44:32 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
06/17/2022 22:44:32 - INFO - __main__ - ['Animal']
06/17/2022 22:44:32 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
06/17/2022 22:44:32 - INFO - __main__ - ['Animal']
06/17/2022 22:44:32 - INFO - __main__ - Tokenizing Input ...
06/17/2022 22:44:32 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:44:32 - INFO - __main__ - Loaded 224 examples from dev data
06/17/2022 22:44:38 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.9867213747669157 on epoch=214
06/17/2022 22:44:38 - INFO - __main__ - save last model!
06/17/2022 22:44:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 22:44:38 - INFO - __main__ - Start tokenizing ... 3500 instances
06/17/2022 22:44:38 - INFO - __main__ - Printing 3 examples
06/17/2022 22:44:38 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/17/2022 22:44:38 - INFO - __main__ - ['Animal']
06/17/2022 22:44:38 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/17/2022 22:44:38 - INFO - __main__ - ['Animal']
06/17/2022 22:44:38 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/17/2022 22:44:38 - INFO - __main__ - ['Village']
06/17/2022 22:44:38 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 22:44:39 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:44:43 - INFO - __main__ - Loaded 3500 examples from test data
06/17/2022 22:44:49 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 22:44:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 22:44:50 - INFO - __main__ - Starting training!
06/17/2022 22:47:05 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_100_0.4_8_predictions.txt
06/17/2022 22:47:05 - INFO - __main__ - Classification-F1 on test data: 0.5692
06/17/2022 22:47:06 - INFO - __main__ - prefix=dbpedia_14_16_100, lr=0.4, bsz=8, dev_performance=0.9910627007401202, test_performance=0.5692329043286738
06/17/2022 22:47:06 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.3, bsz=8 ...
06/17/2022 22:47:07 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 22:47:07 - INFO - __main__ - Printing 3 examples
06/17/2022 22:47:07 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
06/17/2022 22:47:07 - INFO - __main__ - ['Animal']
06/17/2022 22:47:07 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
06/17/2022 22:47:07 - INFO - __main__ - ['Animal']
06/17/2022 22:47:07 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
06/17/2022 22:47:07 - INFO - __main__ - ['Animal']
06/17/2022 22:47:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 22:47:07 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:47:07 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/17/2022 22:47:07 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 22:47:07 - INFO - __main__ - Printing 3 examples
06/17/2022 22:47:07 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
06/17/2022 22:47:07 - INFO - __main__ - ['Animal']
06/17/2022 22:47:07 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
06/17/2022 22:47:07 - INFO - __main__ - ['Animal']
06/17/2022 22:47:07 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
06/17/2022 22:47:07 - INFO - __main__ - ['Animal']
06/17/2022 22:47:07 - INFO - __main__ - Tokenizing Input ...
06/17/2022 22:47:07 - INFO - __main__ - Tokenizing Output ...
06/17/2022 22:47:07 - INFO - __main__ - Loaded 224 examples from dev data
06/17/2022 22:47:26 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 22:47:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 22:47:27 - INFO - __main__ - Starting training!
06/17/2022 22:47:30 - INFO - __main__ - Step 10 Global step 10 Train loss 6.77 on epoch=0
06/17/2022 22:47:33 - INFO - __main__ - Step 20 Global step 20 Train loss 5.02 on epoch=1
06/17/2022 22:47:35 - INFO - __main__ - Step 30 Global step 30 Train loss 4.31 on epoch=2
06/17/2022 22:47:38 - INFO - __main__ - Step 40 Global step 40 Train loss 3.93 on epoch=2
06/17/2022 22:47:41 - INFO - __main__ - Step 50 Global step 50 Train loss 3.77 on epoch=3
06/17/2022 22:47:46 - INFO - __main__ - Global step 50 Train loss 4.76 Classification-F1 0.04731945823268483 on epoch=3
06/17/2022 22:47:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.04731945823268483 on epoch=3, global_step=50
06/17/2022 22:47:49 - INFO - __main__ - Step 60 Global step 60 Train loss 3.80 on epoch=4
06/17/2022 22:47:52 - INFO - __main__ - Step 70 Global step 70 Train loss 3.10 on epoch=4
06/17/2022 22:47:54 - INFO - __main__ - Step 80 Global step 80 Train loss 3.07 on epoch=5
06/17/2022 22:47:57 - INFO - __main__ - Step 90 Global step 90 Train loss 2.65 on epoch=6
06/17/2022 22:47:59 - INFO - __main__ - Step 100 Global step 100 Train loss 2.54 on epoch=7
06/17/2022 22:48:05 - INFO - __main__ - Global step 100 Train loss 3.03 Classification-F1 0.1071993012765091 on epoch=7
06/17/2022 22:48:05 - INFO - __main__ - Saving model with best Classification-F1: 0.04731945823268483 -> 0.1071993012765091 on epoch=7, global_step=100
06/17/2022 22:48:08 - INFO - __main__ - Step 110 Global step 110 Train loss 2.36 on epoch=7
06/17/2022 22:48:10 - INFO - __main__ - Step 120 Global step 120 Train loss 2.22 on epoch=8
06/17/2022 22:48:13 - INFO - __main__ - Step 130 Global step 130 Train loss 2.29 on epoch=9
06/17/2022 22:48:15 - INFO - __main__ - Step 140 Global step 140 Train loss 1.80 on epoch=9
06/17/2022 22:48:18 - INFO - __main__ - Step 150 Global step 150 Train loss 1.86 on epoch=10
06/17/2022 22:48:23 - INFO - __main__ - Global step 150 Train loss 2.11 Classification-F1 0.1345794404107059 on epoch=10
06/17/2022 22:48:23 - INFO - __main__ - Saving model with best Classification-F1: 0.1071993012765091 -> 0.1345794404107059 on epoch=10, global_step=150
06/17/2022 22:48:26 - INFO - __main__ - Step 160 Global step 160 Train loss 1.70 on epoch=11
06/17/2022 22:48:29 - INFO - __main__ - Step 170 Global step 170 Train loss 1.64 on epoch=12
06/17/2022 22:48:31 - INFO - __main__ - Step 180 Global step 180 Train loss 1.55 on epoch=12
06/17/2022 22:48:34 - INFO - __main__ - Step 190 Global step 190 Train loss 1.52 on epoch=13
06/17/2022 22:48:36 - INFO - __main__ - Step 200 Global step 200 Train loss 1.52 on epoch=14
06/17/2022 22:48:42 - INFO - __main__ - Global step 200 Train loss 1.59 Classification-F1 0.17545108646544172 on epoch=14
06/17/2022 22:48:42 - INFO - __main__ - Saving model with best Classification-F1: 0.1345794404107059 -> 0.17545108646544172 on epoch=14, global_step=200
06/17/2022 22:48:45 - INFO - __main__ - Step 210 Global step 210 Train loss 1.20 on epoch=14
06/17/2022 22:48:47 - INFO - __main__ - Step 220 Global step 220 Train loss 1.20 on epoch=15
06/17/2022 22:48:50 - INFO - __main__ - Step 230 Global step 230 Train loss 1.11 on epoch=16
06/17/2022 22:48:53 - INFO - __main__ - Step 240 Global step 240 Train loss 1.20 on epoch=17
06/17/2022 22:48:55 - INFO - __main__ - Step 250 Global step 250 Train loss 1.02 on epoch=17
06/17/2022 22:49:01 - INFO - __main__ - Global step 250 Train loss 1.15 Classification-F1 0.3111342663984835 on epoch=17
06/17/2022 22:49:01 - INFO - __main__ - Saving model with best Classification-F1: 0.17545108646544172 -> 0.3111342663984835 on epoch=17, global_step=250
06/17/2022 22:49:04 - INFO - __main__ - Step 260 Global step 260 Train loss 1.00 on epoch=18
06/17/2022 22:49:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.97 on epoch=19
06/17/2022 22:49:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.84 on epoch=19
06/17/2022 22:49:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.77 on epoch=20
06/17/2022 22:49:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.75 on epoch=21
06/17/2022 22:49:21 - INFO - __main__ - Global step 300 Train loss 0.87 Classification-F1 0.4600535721642507 on epoch=21
06/17/2022 22:49:21 - INFO - __main__ - Saving model with best Classification-F1: 0.3111342663984835 -> 0.4600535721642507 on epoch=21, global_step=300
06/17/2022 22:49:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.76 on epoch=22
06/17/2022 22:49:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.66 on epoch=22
06/17/2022 22:49:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.67 on epoch=23
06/17/2022 22:49:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.75 on epoch=24
06/17/2022 22:49:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=24
06/17/2022 22:49:41 - INFO - __main__ - Global step 350 Train loss 0.69 Classification-F1 0.5719964201937635 on epoch=24
06/17/2022 22:49:41 - INFO - __main__ - Saving model with best Classification-F1: 0.4600535721642507 -> 0.5719964201937635 on epoch=24, global_step=350
06/17/2022 22:49:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.56 on epoch=25
06/17/2022 22:49:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=26
06/17/2022 22:49:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=27
06/17/2022 22:49:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=27
06/17/2022 22:49:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=28
06/17/2022 22:50:01 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.6349174521398046 on epoch=28
06/17/2022 22:50:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5719964201937635 -> 0.6349174521398046 on epoch=28, global_step=400
06/17/2022 22:50:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=29
06/17/2022 22:50:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.58 on epoch=29
06/17/2022 22:50:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=30
06/17/2022 22:50:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=31
06/17/2022 22:50:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=32
06/17/2022 22:50:21 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.6656978892297298 on epoch=32
06/17/2022 22:50:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6349174521398046 -> 0.6656978892297298 on epoch=32, global_step=450
06/17/2022 22:50:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=32
06/17/2022 22:50:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=33
06/17/2022 22:50:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=34
06/17/2022 22:50:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=34
06/17/2022 22:50:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=35
06/17/2022 22:50:40 - INFO - __main__ - Global step 500 Train loss 0.33 Classification-F1 0.6688625121582717 on epoch=35
06/17/2022 22:50:40 - INFO - __main__ - Saving model with best Classification-F1: 0.6656978892297298 -> 0.6688625121582717 on epoch=35, global_step=500
06/17/2022 22:50:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=36
06/17/2022 22:50:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=37
06/17/2022 22:50:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=37
06/17/2022 22:50:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=38
06/17/2022 22:50:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=39
06/17/2022 22:51:00 - INFO - __main__ - Global step 550 Train loss 0.29 Classification-F1 0.7444768894274978 on epoch=39
06/17/2022 22:51:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6688625121582717 -> 0.7444768894274978 on epoch=39, global_step=550
06/17/2022 22:51:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.34 on epoch=39
06/17/2022 22:51:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=40
06/17/2022 22:51:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=41
06/17/2022 22:51:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=42
06/17/2022 22:51:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=42
06/17/2022 22:51:20 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.679397418276875 on epoch=42
06/17/2022 22:51:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=43
06/17/2022 22:51:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=44
06/17/2022 22:51:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=44
06/17/2022 22:51:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=45
06/17/2022 22:51:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=46
06/17/2022 22:51:39 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.7149342891278375 on epoch=46
06/17/2022 22:51:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=47
06/17/2022 22:51:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=47
06/17/2022 22:51:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=48
06/17/2022 22:51:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=49
06/17/2022 22:51:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=49
06/17/2022 22:51:59 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.8071402216800699 on epoch=49
06/17/2022 22:51:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7444768894274978 -> 0.8071402216800699 on epoch=49, global_step=700
06/17/2022 22:52:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=50
06/17/2022 22:52:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=51
06/17/2022 22:52:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.27 on epoch=52
06/17/2022 22:52:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=52
06/17/2022 22:52:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=53
06/17/2022 22:52:19 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.7535099049107309 on epoch=53
06/17/2022 22:52:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=54
06/17/2022 22:52:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.16 on epoch=54
06/17/2022 22:52:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=55
06/17/2022 22:52:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=56
06/17/2022 22:52:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.16 on epoch=57
06/17/2022 22:52:39 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.8267100388542514 on epoch=57
06/17/2022 22:52:39 - INFO - __main__ - Saving model with best Classification-F1: 0.8071402216800699 -> 0.8267100388542514 on epoch=57, global_step=800
06/17/2022 22:52:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=57
06/17/2022 22:52:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=58
06/17/2022 22:52:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=59
06/17/2022 22:52:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=59
06/17/2022 22:52:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=60
06/17/2022 22:52:58 - INFO - __main__ - Global step 850 Train loss 0.16 Classification-F1 0.8273422418820902 on epoch=60
06/17/2022 22:52:58 - INFO - __main__ - Saving model with best Classification-F1: 0.8267100388542514 -> 0.8273422418820902 on epoch=60, global_step=850
06/17/2022 22:53:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=61
06/17/2022 22:53:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=62
06/17/2022 22:53:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=62
06/17/2022 22:53:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=63
06/17/2022 22:53:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=64
06/17/2022 22:53:18 - INFO - __main__ - Global step 900 Train loss 0.17 Classification-F1 0.7746006941676101 on epoch=64
06/17/2022 22:53:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=64
06/17/2022 22:53:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=65
06/17/2022 22:53:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=66
06/17/2022 22:53:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.14 on epoch=67
06/17/2022 22:53:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.15 on epoch=67
06/17/2022 22:53:37 - INFO - __main__ - Global step 950 Train loss 0.14 Classification-F1 0.8129881748964443 on epoch=67
06/17/2022 22:53:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=68
06/17/2022 22:53:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.14 on epoch=69
06/17/2022 22:53:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=69
06/17/2022 22:53:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=70
06/17/2022 22:53:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=71
06/17/2022 22:53:56 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.7651653410790062 on epoch=71
06/17/2022 22:53:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=72
06/17/2022 22:54:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=72
06/17/2022 22:54:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=73
06/17/2022 22:54:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=74
06/17/2022 22:54:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=74
06/17/2022 22:54:15 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.8273422418820902 on epoch=74
06/17/2022 22:54:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=75
06/17/2022 22:54:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=76
06/17/2022 22:54:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.14 on epoch=77
06/17/2022 22:54:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=77
06/17/2022 22:54:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=78
06/17/2022 22:54:34 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.7663123348356076 on epoch=78
06/17/2022 22:54:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=79
06/17/2022 22:54:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=79
06/17/2022 22:54:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=80
06/17/2022 22:54:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=81
06/17/2022 22:54:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=82
06/17/2022 22:54:54 - INFO - __main__ - Global step 1150 Train loss 0.12 Classification-F1 0.8071402216800699 on epoch=82
06/17/2022 22:54:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=82
06/17/2022 22:54:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=83
06/17/2022 22:55:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=84
06/17/2022 22:55:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.11 on epoch=84
06/17/2022 22:55:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=85
06/17/2022 22:55:13 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.7061415638265733 on epoch=85
06/17/2022 22:55:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=86
06/17/2022 22:55:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=87
06/17/2022 22:55:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=87
06/17/2022 22:55:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=88
06/17/2022 22:55:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=89
06/17/2022 22:55:32 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.7237394273447405 on epoch=89
06/17/2022 22:55:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=89
06/17/2022 22:55:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=90
06/17/2022 22:55:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=91
06/17/2022 22:55:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=92
06/17/2022 22:55:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=92
06/17/2022 22:55:51 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.8427782843342616 on epoch=92
06/17/2022 22:55:52 - INFO - __main__ - Saving model with best Classification-F1: 0.8273422418820902 -> 0.8427782843342616 on epoch=92, global_step=1300
06/17/2022 22:55:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=93
06/17/2022 22:55:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=94
06/17/2022 22:55:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=94
06/17/2022 22:56:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=95
06/17/2022 22:56:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=96
06/17/2022 22:56:11 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.844177258717107 on epoch=96
06/17/2022 22:56:11 - INFO - __main__ - Saving model with best Classification-F1: 0.8427782843342616 -> 0.844177258717107 on epoch=96, global_step=1350
06/17/2022 22:56:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=97
06/17/2022 22:56:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=97
06/17/2022 22:56:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=98
06/17/2022 22:56:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=99
06/17/2022 22:56:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=99
06/17/2022 22:56:30 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.7880317918491823 on epoch=99
06/17/2022 22:56:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=100
06/17/2022 22:56:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=101
06/17/2022 22:56:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=102
06/17/2022 22:56:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=102
06/17/2022 22:56:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.11 on epoch=103
06/17/2022 22:56:49 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.7526323139226365 on epoch=103
06/17/2022 22:56:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=104
06/17/2022 22:56:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=104
06/17/2022 22:56:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=105
06/17/2022 22:57:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=106
06/17/2022 22:57:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=107
06/17/2022 22:57:08 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.8440658505174634 on epoch=107
06/17/2022 22:57:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=107
06/17/2022 22:57:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=108
06/17/2022 22:57:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=109
06/17/2022 22:57:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=109
06/17/2022 22:57:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=110
06/17/2022 22:57:28 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.8312948110520506 on epoch=110
06/17/2022 22:57:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=111
06/17/2022 22:57:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=112
06/17/2022 22:57:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=112
06/17/2022 22:57:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=113
06/17/2022 22:57:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=114
06/17/2022 22:57:47 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.7954135680890899 on epoch=114
06/17/2022 22:57:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=114
06/17/2022 22:57:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=115
06/17/2022 22:57:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=116
06/17/2022 22:57:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=117
06/17/2022 22:58:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=117
06/17/2022 22:58:07 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.842049993017735 on epoch=117
06/17/2022 22:58:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=118
06/17/2022 22:58:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=119
06/17/2022 22:58:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=119
06/17/2022 22:58:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=120
06/17/2022 22:58:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=121
06/17/2022 22:58:26 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.8015743249363073 on epoch=121
06/17/2022 22:58:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=122
06/17/2022 22:58:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=122
06/17/2022 22:58:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=123
06/17/2022 22:58:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=124
06/17/2022 22:58:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=124
06/17/2022 22:58:45 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.8543182228903292 on epoch=124
06/17/2022 22:58:45 - INFO - __main__ - Saving model with best Classification-F1: 0.844177258717107 -> 0.8543182228903292 on epoch=124, global_step=1750
06/17/2022 22:58:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=125
06/17/2022 22:58:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=126
06/17/2022 22:58:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=127
06/17/2022 22:58:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=127
06/17/2022 22:58:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=128
06/17/2022 22:59:05 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.8479759189436609 on epoch=128
06/17/2022 22:59:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=129
06/17/2022 22:59:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=129
06/17/2022 22:59:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=130
06/17/2022 22:59:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=131
06/17/2022 22:59:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=132
06/17/2022 22:59:25 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.8530844757763557 on epoch=132
06/17/2022 22:59:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=132
06/17/2022 22:59:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=133
06/17/2022 22:59:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=134
06/17/2022 22:59:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=134
06/17/2022 22:59:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=135
06/17/2022 22:59:44 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.8029030360248053 on epoch=135
06/17/2022 22:59:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=136
06/17/2022 22:59:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=137
06/17/2022 22:59:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=137
06/17/2022 22:59:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=138
06/17/2022 22:59:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=139
06/17/2022 23:00:04 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.7988929104901871 on epoch=139
06/17/2022 23:00:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=139
06/17/2022 23:00:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=140
06/17/2022 23:00:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=141
06/17/2022 23:00:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=142
06/17/2022 23:00:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=142
06/17/2022 23:00:23 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.8530844757763557 on epoch=142
06/17/2022 23:00:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=143
06/17/2022 23:00:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=144
06/17/2022 23:00:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=144
06/17/2022 23:00:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=145
06/17/2022 23:00:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=146
06/17/2022 23:00:43 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.9867213747669157 on epoch=146
06/17/2022 23:00:43 - INFO - __main__ - Saving model with best Classification-F1: 0.8543182228903292 -> 0.9867213747669157 on epoch=146, global_step=2050
06/17/2022 23:00:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=147
06/17/2022 23:00:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=147
06/17/2022 23:00:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=148
06/17/2022 23:00:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=149
06/17/2022 23:00:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=149
06/17/2022 23:01:03 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.8569156856796718 on epoch=149
06/17/2022 23:01:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=150
06/17/2022 23:01:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=151
06/17/2022 23:01:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=152
06/17/2022 23:01:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=152
06/17/2022 23:01:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=153
06/17/2022 23:01:22 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.8492858155498018 on epoch=153
06/17/2022 23:01:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=154
06/17/2022 23:01:27 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=154
06/17/2022 23:01:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=155
06/17/2022 23:01:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=156
06/17/2022 23:01:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=157
06/17/2022 23:01:42 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.8041907119929221 on epoch=157
06/17/2022 23:01:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=157
06/17/2022 23:01:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=158
06/17/2022 23:01:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=159
06/17/2022 23:01:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=159
06/17/2022 23:01:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=160
06/17/2022 23:02:02 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.8607143459062259 on epoch=160
06/17/2022 23:02:04 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=161
06/17/2022 23:02:07 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=162
06/17/2022 23:02:09 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=162
06/17/2022 23:02:12 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=163
06/17/2022 23:02:15 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=164
06/17/2022 23:02:21 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.8544526314924797 on epoch=164
06/17/2022 23:02:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=164
06/17/2022 23:02:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=165
06/17/2022 23:02:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=166
06/17/2022 23:02:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=167
06/17/2022 23:02:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=167
06/17/2022 23:02:41 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.8607143459062259 on epoch=167
06/17/2022 23:02:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=168
06/17/2022 23:02:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=169
06/17/2022 23:02:49 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=169
06/17/2022 23:02:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=170
06/17/2022 23:02:54 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=171
06/17/2022 23:03:01 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.8630131964809384 on epoch=171
06/17/2022 23:03:03 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=172
06/17/2022 23:03:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=172
06/17/2022 23:03:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=173
06/17/2022 23:03:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=174
06/17/2022 23:03:14 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=174
06/17/2022 23:03:20 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.8569156856796718 on epoch=174
06/17/2022 23:03:23 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=175
06/17/2022 23:03:25 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=176
06/17/2022 23:03:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=177
06/17/2022 23:03:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
06/17/2022 23:03:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=178
06/17/2022 23:03:40 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.91649550533801 on epoch=178
06/17/2022 23:03:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=179
06/17/2022 23:03:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=179
06/17/2022 23:03:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=180
06/17/2022 23:03:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=181
06/17/2022 23:03:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=182
06/17/2022 23:03:59 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.9124088814411396 on epoch=182
06/17/2022 23:04:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=182
06/17/2022 23:04:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=183
06/17/2022 23:04:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=184
06/17/2022 23:04:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=184
06/17/2022 23:04:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=185
06/17/2022 23:04:19 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.9228413163897036 on epoch=185
06/17/2022 23:04:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=186
06/17/2022 23:04:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=187
06/17/2022 23:04:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
06/17/2022 23:04:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=188
06/17/2022 23:04:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=189
06/17/2022 23:04:39 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.9084509015944815 on epoch=189
06/17/2022 23:04:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=189
06/17/2022 23:04:44 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=190
06/17/2022 23:04:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=191
06/17/2022 23:04:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=192
06/17/2022 23:04:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=192
06/17/2022 23:04:58 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.8553833263510684 on epoch=192
06/17/2022 23:05:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=193
06/17/2022 23:05:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
06/17/2022 23:05:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=194
06/17/2022 23:05:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=195
06/17/2022 23:05:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=196
06/17/2022 23:05:17 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.79251764048728 on epoch=196
06/17/2022 23:05:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=197
06/17/2022 23:05:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=197
06/17/2022 23:05:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=198
06/17/2022 23:05:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=199
06/17/2022 23:05:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=199
06/17/2022 23:05:36 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.9140433980583166 on epoch=199
06/17/2022 23:05:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=200
06/17/2022 23:05:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=201
06/17/2022 23:05:43 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=202
06/17/2022 23:05:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
06/17/2022 23:05:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=203
06/17/2022 23:05:55 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.9910627007401202 on epoch=203
06/17/2022 23:05:55 - INFO - __main__ - Saving model with best Classification-F1: 0.9867213747669157 -> 0.9910627007401202 on epoch=203, global_step=2850
06/17/2022 23:05:58 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=204
06/17/2022 23:06:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=204
06/17/2022 23:06:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=205
06/17/2022 23:06:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=206
06/17/2022 23:06:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
06/17/2022 23:06:15 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.9154680445003026 on epoch=207
06/17/2022 23:06:17 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=207
06/17/2022 23:06:20 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=208
06/17/2022 23:06:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
06/17/2022 23:06:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=209
06/17/2022 23:06:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=210
06/17/2022 23:06:34 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.8516727202448265 on epoch=210
06/17/2022 23:06:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=211
06/17/2022 23:06:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
06/17/2022 23:06:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=212
06/17/2022 23:06:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=213
06/17/2022 23:06:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=214
06/17/2022 23:06:49 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 23:06:49 - INFO - __main__ - Printing 3 examples
06/17/2022 23:06:49 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
06/17/2022 23:06:49 - INFO - __main__ - ['Animal']
06/17/2022 23:06:49 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
06/17/2022 23:06:49 - INFO - __main__ - ['Animal']
06/17/2022 23:06:49 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
06/17/2022 23:06:49 - INFO - __main__ - ['Animal']
06/17/2022 23:06:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 23:06:49 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:06:49 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/17/2022 23:06:49 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 23:06:49 - INFO - __main__ - Printing 3 examples
06/17/2022 23:06:49 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
06/17/2022 23:06:49 - INFO - __main__ - ['Animal']
06/17/2022 23:06:49 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
06/17/2022 23:06:49 - INFO - __main__ - ['Animal']
06/17/2022 23:06:49 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
06/17/2022 23:06:49 - INFO - __main__ - ['Animal']
06/17/2022 23:06:49 - INFO - __main__ - Tokenizing Input ...
06/17/2022 23:06:49 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:06:49 - INFO - __main__ - Loaded 224 examples from dev data
06/17/2022 23:06:53 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.8630131964809384 on epoch=214
06/17/2022 23:06:53 - INFO - __main__ - save last model!
06/17/2022 23:06:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 23:06:54 - INFO - __main__ - Start tokenizing ... 3500 instances
06/17/2022 23:06:54 - INFO - __main__ - Printing 3 examples
06/17/2022 23:06:54 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/17/2022 23:06:54 - INFO - __main__ - ['Animal']
06/17/2022 23:06:54 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/17/2022 23:06:54 - INFO - __main__ - ['Animal']
06/17/2022 23:06:54 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/17/2022 23:06:54 - INFO - __main__ - ['Village']
06/17/2022 23:06:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 23:06:55 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:06:59 - INFO - __main__ - Loaded 3500 examples from test data
06/17/2022 23:07:05 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 23:07:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 23:07:05 - INFO - __main__ - Starting training!
06/17/2022 23:09:03 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_100_0.3_8_predictions.txt
06/17/2022 23:09:03 - INFO - __main__ - Classification-F1 on test data: 0.5399
06/17/2022 23:09:03 - INFO - __main__ - prefix=dbpedia_14_16_100, lr=0.3, bsz=8, dev_performance=0.9910627007401202, test_performance=0.5399280866628731
06/17/2022 23:09:03 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.2, bsz=8 ...
06/17/2022 23:09:04 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 23:09:04 - INFO - __main__ - Printing 3 examples
06/17/2022 23:09:04 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
06/17/2022 23:09:04 - INFO - __main__ - ['Animal']
06/17/2022 23:09:04 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
06/17/2022 23:09:04 - INFO - __main__ - ['Animal']
06/17/2022 23:09:04 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
06/17/2022 23:09:04 - INFO - __main__ - ['Animal']
06/17/2022 23:09:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 23:09:04 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:09:04 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/17/2022 23:09:04 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 23:09:04 - INFO - __main__ - Printing 3 examples
06/17/2022 23:09:04 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
06/17/2022 23:09:04 - INFO - __main__ - ['Animal']
06/17/2022 23:09:04 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
06/17/2022 23:09:04 - INFO - __main__ - ['Animal']
06/17/2022 23:09:04 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
06/17/2022 23:09:04 - INFO - __main__ - ['Animal']
06/17/2022 23:09:04 - INFO - __main__ - Tokenizing Input ...
06/17/2022 23:09:05 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:09:05 - INFO - __main__ - Loaded 224 examples from dev data
06/17/2022 23:09:20 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 23:09:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 23:09:21 - INFO - __main__ - Starting training!
06/17/2022 23:09:25 - INFO - __main__ - Step 10 Global step 10 Train loss 7.22 on epoch=0
06/17/2022 23:09:27 - INFO - __main__ - Step 20 Global step 20 Train loss 5.52 on epoch=1
06/17/2022 23:09:30 - INFO - __main__ - Step 30 Global step 30 Train loss 4.79 on epoch=2
06/17/2022 23:09:32 - INFO - __main__ - Step 40 Global step 40 Train loss 4.42 on epoch=2
06/17/2022 23:09:35 - INFO - __main__ - Step 50 Global step 50 Train loss 4.42 on epoch=3
06/17/2022 23:09:42 - INFO - __main__ - Global step 50 Train loss 5.27 Classification-F1 0.020703089439517076 on epoch=3
06/17/2022 23:09:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.020703089439517076 on epoch=3, global_step=50
06/17/2022 23:09:44 - INFO - __main__ - Step 60 Global step 60 Train loss 4.21 on epoch=4
06/17/2022 23:09:47 - INFO - __main__ - Step 70 Global step 70 Train loss 3.61 on epoch=4
06/17/2022 23:09:50 - INFO - __main__ - Step 80 Global step 80 Train loss 3.72 on epoch=5
06/17/2022 23:09:52 - INFO - __main__ - Step 90 Global step 90 Train loss 3.36 on epoch=6
06/17/2022 23:09:55 - INFO - __main__ - Step 100 Global step 100 Train loss 3.29 on epoch=7
06/17/2022 23:10:00 - INFO - __main__ - Global step 100 Train loss 3.64 Classification-F1 0.08391136916820646 on epoch=7
06/17/2022 23:10:00 - INFO - __main__ - Saving model with best Classification-F1: 0.020703089439517076 -> 0.08391136916820646 on epoch=7, global_step=100
06/17/2022 23:10:03 - INFO - __main__ - Step 110 Global step 110 Train loss 3.03 on epoch=7
06/17/2022 23:10:05 - INFO - __main__ - Step 120 Global step 120 Train loss 2.90 on epoch=8
06/17/2022 23:10:08 - INFO - __main__ - Step 130 Global step 130 Train loss 2.93 on epoch=9
06/17/2022 23:10:10 - INFO - __main__ - Step 140 Global step 140 Train loss 2.52 on epoch=9
06/17/2022 23:10:13 - INFO - __main__ - Step 150 Global step 150 Train loss 2.63 on epoch=10
06/17/2022 23:10:18 - INFO - __main__ - Global step 150 Train loss 2.80 Classification-F1 0.09663524552527883 on epoch=10
06/17/2022 23:10:18 - INFO - __main__ - Saving model with best Classification-F1: 0.08391136916820646 -> 0.09663524552527883 on epoch=10, global_step=150
06/17/2022 23:10:21 - INFO - __main__ - Step 160 Global step 160 Train loss 2.40 on epoch=11
06/17/2022 23:10:23 - INFO - __main__ - Step 170 Global step 170 Train loss 2.22 on epoch=12
06/17/2022 23:10:26 - INFO - __main__ - Step 180 Global step 180 Train loss 2.13 on epoch=12
06/17/2022 23:10:29 - INFO - __main__ - Step 190 Global step 190 Train loss 2.11 on epoch=13
06/17/2022 23:10:31 - INFO - __main__ - Step 200 Global step 200 Train loss 2.21 on epoch=14
06/17/2022 23:10:36 - INFO - __main__ - Global step 200 Train loss 2.21 Classification-F1 0.12235946763596535 on epoch=14
06/17/2022 23:10:36 - INFO - __main__ - Saving model with best Classification-F1: 0.09663524552527883 -> 0.12235946763596535 on epoch=14, global_step=200
06/17/2022 23:10:39 - INFO - __main__ - Step 210 Global step 210 Train loss 1.83 on epoch=14
06/17/2022 23:10:41 - INFO - __main__ - Step 220 Global step 220 Train loss 1.90 on epoch=15
06/17/2022 23:10:44 - INFO - __main__ - Step 230 Global step 230 Train loss 1.75 on epoch=16
06/17/2022 23:10:47 - INFO - __main__ - Step 240 Global step 240 Train loss 1.78 on epoch=17
06/17/2022 23:10:49 - INFO - __main__ - Step 250 Global step 250 Train loss 1.65 on epoch=17
06/17/2022 23:10:55 - INFO - __main__ - Global step 250 Train loss 1.78 Classification-F1 0.12223669271355106 on epoch=17
06/17/2022 23:10:57 - INFO - __main__ - Step 260 Global step 260 Train loss 1.60 on epoch=18
06/17/2022 23:11:00 - INFO - __main__ - Step 270 Global step 270 Train loss 1.69 on epoch=19
06/17/2022 23:11:02 - INFO - __main__ - Step 280 Global step 280 Train loss 1.36 on epoch=19
06/17/2022 23:11:05 - INFO - __main__ - Step 290 Global step 290 Train loss 1.46 on epoch=20
06/17/2022 23:11:08 - INFO - __main__ - Step 300 Global step 300 Train loss 1.38 on epoch=21
06/17/2022 23:11:13 - INFO - __main__ - Global step 300 Train loss 1.50 Classification-F1 0.1881363916872976 on epoch=21
06/17/2022 23:11:13 - INFO - __main__ - Saving model with best Classification-F1: 0.12235946763596535 -> 0.1881363916872976 on epoch=21, global_step=300
06/17/2022 23:11:16 - INFO - __main__ - Step 310 Global step 310 Train loss 1.34 on epoch=22
06/17/2022 23:11:19 - INFO - __main__ - Step 320 Global step 320 Train loss 1.17 on epoch=22
06/17/2022 23:11:21 - INFO - __main__ - Step 330 Global step 330 Train loss 1.24 on epoch=23
06/17/2022 23:11:24 - INFO - __main__ - Step 340 Global step 340 Train loss 1.28 on epoch=24
06/17/2022 23:11:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.99 on epoch=24
06/17/2022 23:11:33 - INFO - __main__ - Global step 350 Train loss 1.21 Classification-F1 0.35375775160721395 on epoch=24
06/17/2022 23:11:33 - INFO - __main__ - Saving model with best Classification-F1: 0.1881363916872976 -> 0.35375775160721395 on epoch=24, global_step=350
06/17/2022 23:11:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.98 on epoch=25
06/17/2022 23:11:38 - INFO - __main__ - Step 370 Global step 370 Train loss 1.04 on epoch=26
06/17/2022 23:11:40 - INFO - __main__ - Step 380 Global step 380 Train loss 1.04 on epoch=27
06/17/2022 23:11:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.97 on epoch=27
06/17/2022 23:11:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.99 on epoch=28
06/17/2022 23:11:52 - INFO - __main__ - Global step 400 Train loss 1.00 Classification-F1 0.3512949439578156 on epoch=28
06/17/2022 23:11:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.88 on epoch=29
06/17/2022 23:11:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.87 on epoch=29
06/17/2022 23:12:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.77 on epoch=30
06/17/2022 23:12:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=31
06/17/2022 23:12:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.82 on epoch=32
06/17/2022 23:12:11 - INFO - __main__ - Global step 450 Train loss 0.85 Classification-F1 0.41338003277413077 on epoch=32
06/17/2022 23:12:11 - INFO - __main__ - Saving model with best Classification-F1: 0.35375775160721395 -> 0.41338003277413077 on epoch=32, global_step=450
06/17/2022 23:12:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.67 on epoch=32
06/17/2022 23:12:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.77 on epoch=33
06/17/2022 23:12:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.74 on epoch=34
06/17/2022 23:12:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.67 on epoch=34
06/17/2022 23:12:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.67 on epoch=35
06/17/2022 23:12:31 - INFO - __main__ - Global step 500 Train loss 0.70 Classification-F1 0.5157597568463443 on epoch=35
06/17/2022 23:12:31 - INFO - __main__ - Saving model with best Classification-F1: 0.41338003277413077 -> 0.5157597568463443 on epoch=35, global_step=500
06/17/2022 23:12:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.70 on epoch=36
06/17/2022 23:12:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.61 on epoch=37
06/17/2022 23:12:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.55 on epoch=37
06/17/2022 23:12:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.63 on epoch=38
06/17/2022 23:12:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.49 on epoch=39
06/17/2022 23:12:50 - INFO - __main__ - Global step 550 Train loss 0.60 Classification-F1 0.5365721709347722 on epoch=39
06/17/2022 23:12:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5157597568463443 -> 0.5365721709347722 on epoch=39, global_step=550
06/17/2022 23:12:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=39
06/17/2022 23:12:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=40
06/17/2022 23:12:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=41
06/17/2022 23:13:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=42
06/17/2022 23:13:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=42
06/17/2022 23:13:10 - INFO - __main__ - Global step 600 Train loss 0.50 Classification-F1 0.617591642228739 on epoch=42
06/17/2022 23:13:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5365721709347722 -> 0.617591642228739 on epoch=42, global_step=600
06/17/2022 23:13:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=43
06/17/2022 23:13:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=44
06/17/2022 23:13:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=44
06/17/2022 23:13:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=45
06/17/2022 23:13:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=46
06/17/2022 23:13:30 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.6529817903392257 on epoch=46
06/17/2022 23:13:30 - INFO - __main__ - Saving model with best Classification-F1: 0.617591642228739 -> 0.6529817903392257 on epoch=46, global_step=650
06/17/2022 23:13:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=47
06/17/2022 23:13:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=47
06/17/2022 23:13:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=48
06/17/2022 23:13:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=49
06/17/2022 23:13:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.31 on epoch=49
06/17/2022 23:13:49 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.5661089521426764 on epoch=49
06/17/2022 23:13:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.30 on epoch=50
06/17/2022 23:13:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=51
06/17/2022 23:13:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.35 on epoch=52
06/17/2022 23:14:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=52
06/17/2022 23:14:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=53
06/17/2022 23:14:09 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.6649760003138111 on epoch=53
06/17/2022 23:14:09 - INFO - __main__ - Saving model with best Classification-F1: 0.6529817903392257 -> 0.6649760003138111 on epoch=53, global_step=750
06/17/2022 23:14:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=54
06/17/2022 23:14:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=54
06/17/2022 23:14:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=55
06/17/2022 23:14:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=56
06/17/2022 23:14:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=57
06/17/2022 23:14:29 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.6985836669304413 on epoch=57
06/17/2022 23:14:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6649760003138111 -> 0.6985836669304413 on epoch=57, global_step=800
06/17/2022 23:14:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=57
06/17/2022 23:14:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=58
06/17/2022 23:14:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=59
06/17/2022 23:14:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=59
06/17/2022 23:14:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=60
06/17/2022 23:14:49 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.7167939126626166 on epoch=60
06/17/2022 23:14:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6985836669304413 -> 0.7167939126626166 on epoch=60, global_step=850
06/17/2022 23:14:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=61
06/17/2022 23:14:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=62
06/17/2022 23:14:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=62
06/17/2022 23:14:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.32 on epoch=63
06/17/2022 23:15:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=64
06/17/2022 23:15:09 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.6793271524795544 on epoch=64
06/17/2022 23:15:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=64
06/17/2022 23:15:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.32 on epoch=65
06/17/2022 23:15:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.27 on epoch=66
06/17/2022 23:15:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=67
06/17/2022 23:15:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=67
06/17/2022 23:15:28 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.7140134078503781 on epoch=67
06/17/2022 23:15:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=68
06/17/2022 23:15:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.27 on epoch=69
06/17/2022 23:15:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=69
06/17/2022 23:15:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=70
06/17/2022 23:15:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=71
06/17/2022 23:15:48 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.6795265145210215 on epoch=71
06/17/2022 23:15:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=72
06/17/2022 23:15:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=72
06/17/2022 23:15:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.25 on epoch=73
06/17/2022 23:15:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=74
06/17/2022 23:16:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=74
06/17/2022 23:16:08 - INFO - __main__ - Global step 1050 Train loss 0.25 Classification-F1 0.6709094627201089 on epoch=74
06/17/2022 23:16:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.26 on epoch=75
06/17/2022 23:16:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=76
06/17/2022 23:16:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=77
06/17/2022 23:16:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=77
06/17/2022 23:16:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=78
06/17/2022 23:16:27 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.7633828098847104 on epoch=78
06/17/2022 23:16:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7167939126626166 -> 0.7633828098847104 on epoch=78, global_step=1100
06/17/2022 23:16:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=79
06/17/2022 23:16:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=79
06/17/2022 23:16:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=80
06/17/2022 23:16:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=81
06/17/2022 23:16:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=82
06/17/2022 23:16:46 - INFO - __main__ - Global step 1150 Train loss 0.22 Classification-F1 0.6530859147177932 on epoch=82
06/17/2022 23:16:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=82
06/17/2022 23:16:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.29 on epoch=83
06/17/2022 23:16:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=84
06/17/2022 23:16:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.15 on epoch=84
06/17/2022 23:16:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=85
06/17/2022 23:17:05 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.613520976979909 on epoch=85
06/17/2022 23:17:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=86
06/17/2022 23:17:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=87
06/17/2022 23:17:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=87
06/17/2022 23:17:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.27 on epoch=88
06/17/2022 23:17:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=89
06/17/2022 23:17:24 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.6648312345466045 on epoch=89
06/17/2022 23:17:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.14 on epoch=89
06/17/2022 23:17:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=90
06/17/2022 23:17:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=91
06/17/2022 23:17:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=92
06/17/2022 23:17:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=92
06/17/2022 23:17:44 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.8478740600182726 on epoch=92
06/17/2022 23:17:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7633828098847104 -> 0.8478740600182726 on epoch=92, global_step=1300
06/17/2022 23:17:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=93
06/17/2022 23:17:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=94
06/17/2022 23:17:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=94
06/17/2022 23:17:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=95
06/17/2022 23:17:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=96
06/17/2022 23:18:03 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.723970908826222 on epoch=96
06/17/2022 23:18:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=97
06/17/2022 23:18:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=97
06/17/2022 23:18:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=98
06/17/2022 23:18:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=99
06/17/2022 23:18:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.15 on epoch=99
06/17/2022 23:18:22 - INFO - __main__ - Global step 1400 Train loss 0.14 Classification-F1 0.672598176483126 on epoch=99
06/17/2022 23:18:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=100
06/17/2022 23:18:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=101
06/17/2022 23:18:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=102
06/17/2022 23:18:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=102
06/17/2022 23:18:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=103
06/17/2022 23:18:41 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.7752148435706869 on epoch=103
06/17/2022 23:18:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=104
06/17/2022 23:18:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=104
06/17/2022 23:18:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=105
06/17/2022 23:18:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=106
06/17/2022 23:18:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=107
06/17/2022 23:18:59 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.7695586773608875 on epoch=107
06/17/2022 23:19:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=107
06/17/2022 23:19:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=108
06/17/2022 23:19:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=109
06/17/2022 23:19:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=109
06/17/2022 23:19:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=110
06/17/2022 23:19:18 - INFO - __main__ - Global step 1550 Train loss 0.13 Classification-F1 0.8254483024881507 on epoch=110
06/17/2022 23:19:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=111
06/17/2022 23:19:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=112
06/17/2022 23:19:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=112
06/17/2022 23:19:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.14 on epoch=113
06/17/2022 23:19:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=114
06/17/2022 23:19:37 - INFO - __main__ - Global step 1600 Train loss 0.12 Classification-F1 0.7840226447660313 on epoch=114
06/17/2022 23:19:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=114
06/17/2022 23:19:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.17 on epoch=115
06/17/2022 23:19:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=116
06/17/2022 23:19:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=117
06/17/2022 23:19:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=117
06/17/2022 23:19:56 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.8254483024881507 on epoch=117
06/17/2022 23:19:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=118
06/17/2022 23:20:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=119
06/17/2022 23:20:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=119
06/17/2022 23:20:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=120
06/17/2022 23:20:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.09 on epoch=121
06/17/2022 23:20:15 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.8236657712938549 on epoch=121
06/17/2022 23:20:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=122
06/17/2022 23:20:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=122
06/17/2022 23:20:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=123
06/17/2022 23:20:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=124
06/17/2022 23:20:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=124
06/17/2022 23:20:35 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.7578788539163582 on epoch=124
06/17/2022 23:20:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=125
06/17/2022 23:20:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=126
06/17/2022 23:20:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=127
06/17/2022 23:20:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=127
06/17/2022 23:20:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=128
06/17/2022 23:20:54 - INFO - __main__ - Global step 1800 Train loss 0.09 Classification-F1 0.7633828098847104 on epoch=128
06/17/2022 23:20:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=129
06/17/2022 23:20:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=129
06/17/2022 23:21:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=130
06/17/2022 23:21:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=131
06/17/2022 23:21:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=132
06/17/2022 23:21:13 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.6796208492413951 on epoch=132
06/17/2022 23:21:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=132
06/17/2022 23:21:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=133
06/17/2022 23:21:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=134
06/17/2022 23:21:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=134
06/17/2022 23:21:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=135
06/17/2022 23:21:32 - INFO - __main__ - Global step 1900 Train loss 0.08 Classification-F1 0.6441970258289045 on epoch=135
06/17/2022 23:21:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=136
06/17/2022 23:21:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=137
06/17/2022 23:21:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=137
06/17/2022 23:21:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=138
06/17/2022 23:21:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=139
06/17/2022 23:21:50 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.694695164684179 on epoch=139
06/17/2022 23:21:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=139
06/17/2022 23:21:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=140
06/17/2022 23:21:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=141
06/17/2022 23:22:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=142
06/17/2022 23:22:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=142
06/17/2022 23:22:09 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.7016691213148862 on epoch=142
06/17/2022 23:22:12 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=143
06/17/2022 23:22:14 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=144
06/17/2022 23:22:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=144
06/17/2022 23:22:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=145
06/17/2022 23:22:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=146
06/17/2022 23:22:28 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.8008120911346718 on epoch=146
06/17/2022 23:22:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.11 on epoch=147
06/17/2022 23:22:33 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=147
06/17/2022 23:22:36 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=148
06/17/2022 23:22:38 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=149
06/17/2022 23:22:41 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=149
06/17/2022 23:22:47 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.7926323869397872 on epoch=149
06/17/2022 23:22:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=150
06/17/2022 23:22:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=151
06/17/2022 23:22:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=152
06/17/2022 23:22:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=152
06/17/2022 23:23:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=153
06/17/2022 23:23:06 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.7972368815096798 on epoch=153
06/17/2022 23:23:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=154
06/17/2022 23:23:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=154
06/17/2022 23:23:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.10 on epoch=155
06/17/2022 23:23:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=156
06/17/2022 23:23:19 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=157
06/17/2022 23:23:25 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.7871598996679922 on epoch=157
06/17/2022 23:23:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=157
06/17/2022 23:23:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=158
06/17/2022 23:23:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=159
06/17/2022 23:23:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=159
06/17/2022 23:23:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=160
06/17/2022 23:23:44 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.6894518784272631 on epoch=160
06/17/2022 23:23:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=161
06/17/2022 23:23:49 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=162
06/17/2022 23:23:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.08 on epoch=162
06/17/2022 23:23:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=163
06/17/2022 23:23:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=164
06/17/2022 23:24:02 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.6861591892262546 on epoch=164
06/17/2022 23:24:05 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=164
06/17/2022 23:24:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
06/17/2022 23:24:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.07 on epoch=166
06/17/2022 23:24:13 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=167
06/17/2022 23:24:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=167
06/17/2022 23:24:21 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.743928296522967 on epoch=167
06/17/2022 23:24:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.15 on epoch=168
06/17/2022 23:24:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=169
06/17/2022 23:24:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=169
06/17/2022 23:24:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=170
06/17/2022 23:24:34 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=171
06/17/2022 23:24:40 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.743928296522967 on epoch=171
06/17/2022 23:24:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.09 on epoch=172
06/17/2022 23:24:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=172
06/17/2022 23:24:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=173
06/17/2022 23:24:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=174
06/17/2022 23:24:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=174
06/17/2022 23:24:59 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.7869591776679622 on epoch=174
06/17/2022 23:25:01 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=175
06/17/2022 23:25:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=176
06/17/2022 23:25:06 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=177
06/17/2022 23:25:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=177
06/17/2022 23:25:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=178
06/17/2022 23:25:18 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.7423233795480311 on epoch=178
06/17/2022 23:25:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=179
06/17/2022 23:25:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=179
06/17/2022 23:25:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=180
06/17/2022 23:25:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=181
06/17/2022 23:25:30 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=182
06/17/2022 23:25:37 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.71111797088028 on epoch=182
06/17/2022 23:25:39 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=182
06/17/2022 23:25:42 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=183
06/17/2022 23:25:44 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=184
06/17/2022 23:25:47 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=184
06/17/2022 23:25:50 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=185
06/17/2022 23:25:56 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.7472479379500252 on epoch=185
06/17/2022 23:25:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=186
06/17/2022 23:26:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=187
06/17/2022 23:26:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=187
06/17/2022 23:26:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=188
06/17/2022 23:26:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=189
06/17/2022 23:26:15 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.7563225305160789 on epoch=189
06/17/2022 23:26:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=189
06/17/2022 23:26:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=190
06/17/2022 23:26:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=191
06/17/2022 23:26:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=192
06/17/2022 23:26:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=192
06/17/2022 23:26:34 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.7107261166544099 on epoch=192
06/17/2022 23:26:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=193
06/17/2022 23:26:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=194
06/17/2022 23:26:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=194
06/17/2022 23:26:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=195
06/17/2022 23:26:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=196
06/17/2022 23:26:52 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.7474039129018091 on epoch=196
06/17/2022 23:26:55 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=197
06/17/2022 23:26:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=197
06/17/2022 23:27:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=198
06/17/2022 23:27:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=199
06/17/2022 23:27:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=199
06/17/2022 23:27:11 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.7486264287402808 on epoch=199
06/17/2022 23:27:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=200
06/17/2022 23:27:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=201
06/17/2022 23:27:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=202
06/17/2022 23:27:22 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=202
06/17/2022 23:27:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=203
06/17/2022 23:27:30 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.7529459436480308 on epoch=203
06/17/2022 23:27:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=204
06/17/2022 23:27:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=204
06/17/2022 23:27:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=205
06/17/2022 23:27:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=206
06/17/2022 23:27:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=207
06/17/2022 23:27:49 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.749569356779983 on epoch=207
06/17/2022 23:27:52 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=207
06/17/2022 23:27:54 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=208
06/17/2022 23:27:57 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=209
06/17/2022 23:27:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=209
06/17/2022 23:28:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=210
06/17/2022 23:28:08 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.760538479176472 on epoch=210
06/17/2022 23:28:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=211
06/17/2022 23:28:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=212
06/17/2022 23:28:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=212
06/17/2022 23:28:18 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=213
06/17/2022 23:28:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=214
06/17/2022 23:28:22 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 23:28:22 - INFO - __main__ - Printing 3 examples
06/17/2022 23:28:22 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
06/17/2022 23:28:22 - INFO - __main__ - ['Animal']
06/17/2022 23:28:22 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
06/17/2022 23:28:22 - INFO - __main__ - ['Animal']
06/17/2022 23:28:22 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
06/17/2022 23:28:22 - INFO - __main__ - ['Animal']
06/17/2022 23:28:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 23:28:23 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:28:23 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/17/2022 23:28:23 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 23:28:23 - INFO - __main__ - Printing 3 examples
06/17/2022 23:28:23 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
06/17/2022 23:28:23 - INFO - __main__ - ['Animal']
06/17/2022 23:28:23 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
06/17/2022 23:28:23 - INFO - __main__ - ['Animal']
06/17/2022 23:28:23 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
06/17/2022 23:28:23 - INFO - __main__ - ['Animal']
06/17/2022 23:28:23 - INFO - __main__ - Tokenizing Input ...
06/17/2022 23:28:23 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:28:23 - INFO - __main__ - Loaded 224 examples from dev data
06/17/2022 23:28:27 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7489314396335269 on epoch=214
06/17/2022 23:28:27 - INFO - __main__ - save last model!
06/17/2022 23:28:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 23:28:27 - INFO - __main__ - Start tokenizing ... 3500 instances
06/17/2022 23:28:27 - INFO - __main__ - Printing 3 examples
06/17/2022 23:28:27 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/17/2022 23:28:27 - INFO - __main__ - ['Animal']
06/17/2022 23:28:27 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/17/2022 23:28:27 - INFO - __main__ - ['Animal']
06/17/2022 23:28:27 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/17/2022 23:28:27 - INFO - __main__ - ['Village']
06/17/2022 23:28:27 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 23:28:29 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:28:33 - INFO - __main__ - Loaded 3500 examples from test data
06/17/2022 23:28:42 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 23:28:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 23:28:43 - INFO - __main__ - Starting training!
06/17/2022 23:30:31 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_100_0.2_8_predictions.txt
06/17/2022 23:30:31 - INFO - __main__ - Classification-F1 on test data: 0.4477
06/17/2022 23:30:32 - INFO - __main__ - prefix=dbpedia_14_16_100, lr=0.2, bsz=8, dev_performance=0.8478740600182726, test_performance=0.44766498049286685
06/17/2022 23:30:32 - INFO - __main__ - Running ... prefix=dbpedia_14_16_13, lr=0.5, bsz=8 ...
06/17/2022 23:30:33 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 23:30:33 - INFO - __main__ - Printing 3 examples
06/17/2022 23:30:33 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
06/17/2022 23:30:33 - INFO - __main__ - ['Animal']
06/17/2022 23:30:33 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
06/17/2022 23:30:33 - INFO - __main__ - ['Animal']
06/17/2022 23:30:33 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
06/17/2022 23:30:33 - INFO - __main__ - ['Animal']
06/17/2022 23:30:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 23:30:33 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:30:33 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/17/2022 23:30:33 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 23:30:33 - INFO - __main__ - Printing 3 examples
06/17/2022 23:30:33 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
06/17/2022 23:30:33 - INFO - __main__ - ['Animal']
06/17/2022 23:30:33 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
06/17/2022 23:30:33 - INFO - __main__ - ['Animal']
06/17/2022 23:30:33 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
06/17/2022 23:30:33 - INFO - __main__ - ['Animal']
06/17/2022 23:30:33 - INFO - __main__ - Tokenizing Input ...
06/17/2022 23:30:33 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:30:33 - INFO - __main__ - Loaded 224 examples from dev data
06/17/2022 23:30:48 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 23:30:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 23:30:49 - INFO - __main__ - Starting training!
06/17/2022 23:30:52 - INFO - __main__ - Step 10 Global step 10 Train loss 6.08 on epoch=0
06/17/2022 23:30:55 - INFO - __main__ - Step 20 Global step 20 Train loss 4.76 on epoch=1
06/17/2022 23:30:58 - INFO - __main__ - Step 30 Global step 30 Train loss 3.92 on epoch=2
06/17/2022 23:31:00 - INFO - __main__ - Step 40 Global step 40 Train loss 3.28 on epoch=2
06/17/2022 23:31:03 - INFO - __main__ - Step 50 Global step 50 Train loss 2.99 on epoch=3
06/17/2022 23:31:09 - INFO - __main__ - Global step 50 Train loss 4.21 Classification-F1 0.07866356120454877 on epoch=3
06/17/2022 23:31:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.07866356120454877 on epoch=3, global_step=50
06/17/2022 23:31:11 - INFO - __main__ - Step 60 Global step 60 Train loss 2.55 on epoch=4
06/17/2022 23:31:14 - INFO - __main__ - Step 70 Global step 70 Train loss 2.17 on epoch=4
06/17/2022 23:31:16 - INFO - __main__ - Step 80 Global step 80 Train loss 2.05 on epoch=5
06/17/2022 23:31:19 - INFO - __main__ - Step 90 Global step 90 Train loss 1.83 on epoch=6
06/17/2022 23:31:22 - INFO - __main__ - Step 100 Global step 100 Train loss 1.84 on epoch=7
06/17/2022 23:31:27 - INFO - __main__ - Global step 100 Train loss 2.09 Classification-F1 0.11231891081831913 on epoch=7
06/17/2022 23:31:27 - INFO - __main__ - Saving model with best Classification-F1: 0.07866356120454877 -> 0.11231891081831913 on epoch=7, global_step=100
06/17/2022 23:31:30 - INFO - __main__ - Step 110 Global step 110 Train loss 1.57 on epoch=7
06/17/2022 23:31:32 - INFO - __main__ - Step 120 Global step 120 Train loss 1.50 on epoch=8
06/17/2022 23:31:35 - INFO - __main__ - Step 130 Global step 130 Train loss 1.24 on epoch=9
06/17/2022 23:31:37 - INFO - __main__ - Step 140 Global step 140 Train loss 1.12 on epoch=9
06/17/2022 23:31:40 - INFO - __main__ - Step 150 Global step 150 Train loss 1.12 on epoch=10
06/17/2022 23:31:46 - INFO - __main__ - Global step 150 Train loss 1.31 Classification-F1 0.3149716021008731 on epoch=10
06/17/2022 23:31:46 - INFO - __main__ - Saving model with best Classification-F1: 0.11231891081831913 -> 0.3149716021008731 on epoch=10, global_step=150
06/17/2022 23:31:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.99 on epoch=11
06/17/2022 23:31:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.83 on epoch=12
06/17/2022 23:31:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.71 on epoch=12
06/17/2022 23:31:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.74 on epoch=13
06/17/2022 23:32:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.61 on epoch=14
06/17/2022 23:32:06 - INFO - __main__ - Global step 200 Train loss 0.78 Classification-F1 0.5270462468280661 on epoch=14
06/17/2022 23:32:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3149716021008731 -> 0.5270462468280661 on epoch=14, global_step=200
06/17/2022 23:32:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.56 on epoch=14
06/17/2022 23:32:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=15
06/17/2022 23:32:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.57 on epoch=16
06/17/2022 23:32:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=17
06/17/2022 23:32:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=17
06/17/2022 23:32:26 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.4808251693724932 on epoch=17
06/17/2022 23:32:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=18
06/17/2022 23:32:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=19
06/17/2022 23:32:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=19
06/17/2022 23:32:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=20
06/17/2022 23:32:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.20 on epoch=21
06/17/2022 23:32:45 - INFO - __main__ - Global step 300 Train loss 0.33 Classification-F1 0.6022483577986425 on epoch=21
06/17/2022 23:32:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5270462468280661 -> 0.6022483577986425 on epoch=21, global_step=300
06/17/2022 23:32:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=22
06/17/2022 23:32:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=22
06/17/2022 23:32:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=23
06/17/2022 23:32:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=24
06/17/2022 23:32:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=24
06/17/2022 23:33:05 - INFO - __main__ - Global step 350 Train loss 0.27 Classification-F1 0.635591194200065 on epoch=24
06/17/2022 23:33:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6022483577986425 -> 0.635591194200065 on epoch=24, global_step=350
06/17/2022 23:33:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.19 on epoch=25
06/17/2022 23:33:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.18 on epoch=26
06/17/2022 23:33:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=27
06/17/2022 23:33:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.19 on epoch=27
06/17/2022 23:33:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.21 on epoch=28
06/17/2022 23:33:25 - INFO - __main__ - Global step 400 Train loss 0.20 Classification-F1 0.6818985697381283 on epoch=28
06/17/2022 23:33:25 - INFO - __main__ - Saving model with best Classification-F1: 0.635591194200065 -> 0.6818985697381283 on epoch=28, global_step=400
06/17/2022 23:33:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=29
06/17/2022 23:33:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.17 on epoch=29
06/17/2022 23:33:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.14 on epoch=30
06/17/2022 23:33:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.19 on epoch=31
06/17/2022 23:33:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.18 on epoch=32
06/17/2022 23:33:44 - INFO - __main__ - Global step 450 Train loss 0.18 Classification-F1 0.6868787621546535 on epoch=32
06/17/2022 23:33:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6818985697381283 -> 0.6868787621546535 on epoch=32, global_step=450
06/17/2022 23:33:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=32
06/17/2022 23:33:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.12 on epoch=33
06/17/2022 23:33:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=34
06/17/2022 23:33:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.10 on epoch=34
06/17/2022 23:33:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.16 on epoch=35
06/17/2022 23:34:03 - INFO - __main__ - Global step 500 Train loss 0.16 Classification-F1 0.6292014907135874 on epoch=35
06/17/2022 23:34:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.15 on epoch=36
06/17/2022 23:34:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.12 on epoch=37
06/17/2022 23:34:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=37
06/17/2022 23:34:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=38
06/17/2022 23:34:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.15 on epoch=39
06/17/2022 23:34:22 - INFO - __main__ - Global step 550 Train loss 0.12 Classification-F1 0.6740850839398158 on epoch=39
06/17/2022 23:34:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.09 on epoch=39
06/17/2022 23:34:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.11 on epoch=40
06/17/2022 23:34:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=41
06/17/2022 23:34:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=42
06/17/2022 23:34:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=42
06/17/2022 23:34:41 - INFO - __main__ - Global step 600 Train loss 0.14 Classification-F1 0.7196186837843417 on epoch=42
06/17/2022 23:34:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6868787621546535 -> 0.7196186837843417 on epoch=42, global_step=600
06/17/2022 23:34:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=43
06/17/2022 23:34:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=44
06/17/2022 23:34:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=44
06/17/2022 23:34:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.07 on epoch=45
06/17/2022 23:34:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=46
06/17/2022 23:35:00 - INFO - __main__ - Global step 650 Train loss 0.10 Classification-F1 0.7754971093622652 on epoch=46
06/17/2022 23:35:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7196186837843417 -> 0.7754971093622652 on epoch=46, global_step=650
06/17/2022 23:35:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.10 on epoch=47
06/17/2022 23:35:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.08 on epoch=47
06/17/2022 23:35:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=48
06/17/2022 23:35:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=49
06/17/2022 23:35:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=49
06/17/2022 23:35:19 - INFO - __main__ - Global step 700 Train loss 0.09 Classification-F1 0.7486105998730319 on epoch=49
06/17/2022 23:35:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=50
06/17/2022 23:35:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=51
06/17/2022 23:35:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=52
06/17/2022 23:35:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=52
06/17/2022 23:35:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=53
06/17/2022 23:35:38 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.7855697786284828 on epoch=53
06/17/2022 23:35:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7754971093622652 -> 0.7855697786284828 on epoch=53, global_step=750
06/17/2022 23:35:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=54
06/17/2022 23:35:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=54
06/17/2022 23:35:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=55
06/17/2022 23:35:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=56
06/17/2022 23:35:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=57
06/17/2022 23:35:57 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.7945440507194532 on epoch=57
06/17/2022 23:35:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7855697786284828 -> 0.7945440507194532 on epoch=57, global_step=800
06/17/2022 23:36:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=57
06/17/2022 23:36:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=58
06/17/2022 23:36:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=59
06/17/2022 23:36:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=59
06/17/2022 23:36:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=60
06/17/2022 23:36:17 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.7991906733367834 on epoch=60
06/17/2022 23:36:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7945440507194532 -> 0.7991906733367834 on epoch=60, global_step=850
06/17/2022 23:36:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=61
06/17/2022 23:36:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=62
06/17/2022 23:36:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=62
06/17/2022 23:36:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=63
06/17/2022 23:36:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=64
06/17/2022 23:36:36 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.7529990767893994 on epoch=64
06/17/2022 23:36:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=64
06/17/2022 23:36:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=65
06/17/2022 23:36:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=66
06/17/2022 23:36:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=67
06/17/2022 23:36:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=67
06/17/2022 23:36:55 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.7042924488134078 on epoch=67
06/17/2022 23:36:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=68
06/17/2022 23:37:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=69
06/17/2022 23:37:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=69
06/17/2022 23:37:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=70
06/17/2022 23:37:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=71
06/17/2022 23:37:14 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.7877326458055814 on epoch=71
06/17/2022 23:37:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=72
06/17/2022 23:37:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=72
06/17/2022 23:37:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=73
06/17/2022 23:37:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=74
06/17/2022 23:37:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=74
06/17/2022 23:37:34 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.7968622193662224 on epoch=74
06/17/2022 23:37:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=75
06/17/2022 23:37:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=76
06/17/2022 23:37:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=77
06/17/2022 23:37:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=77
06/17/2022 23:37:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=78
06/17/2022 23:37:53 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.9016525541352145 on epoch=78
06/17/2022 23:37:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7991906733367834 -> 0.9016525541352145 on epoch=78, global_step=1100
06/17/2022 23:37:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=79
06/17/2022 23:37:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=79
06/17/2022 23:38:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=80
06/17/2022 23:38:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=81
06/17/2022 23:38:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=82
06/17/2022 23:38:12 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.901526291508952 on epoch=82
06/17/2022 23:38:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=82
06/17/2022 23:38:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=83
06/17/2022 23:38:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=84
06/17/2022 23:38:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=84
06/17/2022 23:38:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=85
06/17/2022 23:38:31 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.8451662984247699 on epoch=85
06/17/2022 23:38:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=86
06/17/2022 23:38:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=87
06/17/2022 23:38:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=87
06/17/2022 23:38:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=88
06/17/2022 23:38:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=89
06/17/2022 23:38:50 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.8472092462857324 on epoch=89
06/17/2022 23:38:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=89
06/17/2022 23:38:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=90
06/17/2022 23:38:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=91
06/17/2022 23:39:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=92
06/17/2022 23:39:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=92
06/17/2022 23:39:09 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.8975382860866731 on epoch=92
06/17/2022 23:39:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=93
06/17/2022 23:39:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=94
06/17/2022 23:39:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=94
06/17/2022 23:39:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=95
06/17/2022 23:39:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=96
06/17/2022 23:39:28 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.8473697781898684 on epoch=96
06/17/2022 23:39:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=97
06/17/2022 23:39:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=97
06/17/2022 23:39:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=98
06/17/2022 23:39:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=99
06/17/2022 23:39:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=99
06/17/2022 23:39:47 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.793185748777987 on epoch=99
06/17/2022 23:39:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=100
06/17/2022 23:39:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=101
06/17/2022 23:39:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=102
06/17/2022 23:39:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=102
06/17/2022 23:40:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=103
06/17/2022 23:40:06 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.9058404003391897 on epoch=103
06/17/2022 23:40:06 - INFO - __main__ - Saving model with best Classification-F1: 0.9016525541352145 -> 0.9058404003391897 on epoch=103, global_step=1450
06/17/2022 23:40:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=104
06/17/2022 23:40:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=104
06/17/2022 23:40:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=105
06/17/2022 23:40:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=106
06/17/2022 23:40:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=107
06/17/2022 23:40:26 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7894905903627147 on epoch=107
06/17/2022 23:40:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=107
06/17/2022 23:40:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=108
06/17/2022 23:40:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=109
06/17/2022 23:40:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=109
06/17/2022 23:40:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=110
06/17/2022 23:40:45 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.7420226485032969 on epoch=110
06/17/2022 23:40:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=111
06/17/2022 23:40:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=112
06/17/2022 23:40:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=112
06/17/2022 23:40:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=113
06/17/2022 23:40:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=114
06/17/2022 23:41:04 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.7871724217159457 on epoch=114
06/17/2022 23:41:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=114
06/17/2022 23:41:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=115
06/17/2022 23:41:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=116
06/17/2022 23:41:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=117
06/17/2022 23:41:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=117
06/17/2022 23:41:23 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.8392386634411577 on epoch=117
06/17/2022 23:41:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=118
06/17/2022 23:41:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=119
06/17/2022 23:41:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=119
06/17/2022 23:41:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=120
06/17/2022 23:41:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=121
06/17/2022 23:41:43 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.8392386634411576 on epoch=121
06/17/2022 23:41:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=122
06/17/2022 23:41:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=122
06/17/2022 23:41:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=123
06/17/2022 23:41:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=124
06/17/2022 23:41:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=124
06/17/2022 23:42:02 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.8973384453049765 on epoch=124
06/17/2022 23:42:05 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=125
06/17/2022 23:42:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=126
06/17/2022 23:42:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=127
06/17/2022 23:42:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=127
06/17/2022 23:42:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=128
06/17/2022 23:42:22 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7920541658766272 on epoch=128
06/17/2022 23:42:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=129
06/17/2022 23:42:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=129
06/17/2022 23:42:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=130
06/17/2022 23:42:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=131
06/17/2022 23:42:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=132
06/17/2022 23:42:41 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.8432831404695056 on epoch=132
06/17/2022 23:42:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=132
06/17/2022 23:42:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=133
06/17/2022 23:42:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=134
06/17/2022 23:42:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=134
06/17/2022 23:42:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=135
06/17/2022 23:43:01 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7873487518174426 on epoch=135
06/17/2022 23:43:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=136
06/17/2022 23:43:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=137
06/17/2022 23:43:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=137
06/17/2022 23:43:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=138
06/17/2022 23:43:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=139
06/17/2022 23:43:21 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7992021735380369 on epoch=139
06/17/2022 23:43:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=139
06/17/2022 23:43:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=140
06/17/2022 23:43:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=141
06/17/2022 23:43:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=142
06/17/2022 23:43:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=142
06/17/2022 23:43:41 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7412555965871379 on epoch=142
06/17/2022 23:43:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=143
06/17/2022 23:43:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=144
06/17/2022 23:43:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=144
06/17/2022 23:43:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=145
06/17/2022 23:43:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=146
06/17/2022 23:44:01 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.9101938742261323 on epoch=146
06/17/2022 23:44:01 - INFO - __main__ - Saving model with best Classification-F1: 0.9058404003391897 -> 0.9101938742261323 on epoch=146, global_step=2050
06/17/2022 23:44:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=147
06/17/2022 23:44:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=147
06/17/2022 23:44:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=148
06/17/2022 23:44:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=149
06/17/2022 23:44:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=149
06/17/2022 23:44:20 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7920251600726542 on epoch=149
06/17/2022 23:44:23 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=150
06/17/2022 23:44:26 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=151
06/17/2022 23:44:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=152
06/17/2022 23:44:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=152
06/17/2022 23:44:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=153
06/17/2022 23:44:40 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7955004616992563 on epoch=153
06/17/2022 23:44:43 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=154
06/17/2022 23:44:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=154
06/17/2022 23:44:48 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=155
06/17/2022 23:44:50 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=156
06/17/2022 23:44:53 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=157
06/17/2022 23:45:00 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7455009441951632 on epoch=157
06/17/2022 23:45:03 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=157
06/17/2022 23:45:05 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=158
06/17/2022 23:45:08 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=159
06/17/2022 23:45:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=159
06/17/2022 23:45:13 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=160
06/17/2022 23:45:20 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.8511154962857325 on epoch=160
06/17/2022 23:45:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=161
06/17/2022 23:45:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=162
06/17/2022 23:45:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=162
06/17/2022 23:45:30 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=163
06/17/2022 23:45:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=164
06/17/2022 23:45:39 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.9728474335431413 on epoch=164
06/17/2022 23:45:39 - INFO - __main__ - Saving model with best Classification-F1: 0.9101938742261323 -> 0.9728474335431413 on epoch=164, global_step=2300
06/17/2022 23:45:42 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=164
06/17/2022 23:45:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=165
06/17/2022 23:45:47 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=166
06/17/2022 23:45:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=167
06/17/2022 23:45:52 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=167
06/17/2022 23:45:59 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.914360540892799 on epoch=167
06/17/2022 23:46:02 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=168
06/17/2022 23:46:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=169
06/17/2022 23:46:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=169
06/17/2022 23:46:10 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=170
06/17/2022 23:46:12 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=171
06/17/2022 23:46:20 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.9101938742261323 on epoch=171
06/17/2022 23:46:22 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=172
06/17/2022 23:46:25 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=172
06/17/2022 23:46:28 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=173
06/17/2022 23:46:30 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=174
06/17/2022 23:46:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=174
06/17/2022 23:46:40 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.9101938742261323 on epoch=174
06/17/2022 23:46:42 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=175
06/17/2022 23:46:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=176
06/17/2022 23:46:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=177
06/17/2022 23:46:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
06/17/2022 23:46:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=178
06/17/2022 23:47:00 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.9101938742261323 on epoch=178
06/17/2022 23:47:02 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=179
06/17/2022 23:47:05 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
06/17/2022 23:47:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=180
06/17/2022 23:47:10 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=181
06/17/2022 23:47:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=182
06/17/2022 23:47:19 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.9683831478288555 on epoch=182
06/17/2022 23:47:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=182
06/17/2022 23:47:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=183
06/17/2022 23:47:27 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=184
06/17/2022 23:47:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=184
06/17/2022 23:47:32 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=185
06/17/2022 23:47:39 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.9058404003391897 on epoch=185
06/17/2022 23:47:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=186
06/17/2022 23:47:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=187
06/17/2022 23:47:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=187
06/17/2022 23:47:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=188
06/17/2022 23:47:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=189
06/17/2022 23:47:58 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.9058404003391897 on epoch=189
06/17/2022 23:48:01 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
06/17/2022 23:48:03 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=190
06/17/2022 23:48:06 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
06/17/2022 23:48:09 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
06/17/2022 23:48:11 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=192
06/17/2022 23:48:18 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.9730475841362937 on epoch=192
06/17/2022 23:48:18 - INFO - __main__ - Saving model with best Classification-F1: 0.9728474335431413 -> 0.9730475841362937 on epoch=192, global_step=2700
06/17/2022 23:48:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=193
06/17/2022 23:48:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
06/17/2022 23:48:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
06/17/2022 23:48:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=195
06/17/2022 23:48:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=196
06/17/2022 23:48:37 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.9730475841362937 on epoch=196
06/17/2022 23:48:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=197
06/17/2022 23:48:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=197
06/17/2022 23:48:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
06/17/2022 23:48:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=199
06/17/2022 23:48:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=199
06/17/2022 23:48:57 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.9683831478288555 on epoch=199
06/17/2022 23:49:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=200
06/17/2022 23:49:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=201
06/17/2022 23:49:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=202
06/17/2022 23:49:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=202
06/17/2022 23:49:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=203
06/17/2022 23:49:17 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.9730475841362937 on epoch=203
06/17/2022 23:49:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=204
06/17/2022 23:49:22 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=204
06/17/2022 23:49:25 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=205
06/17/2022 23:49:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=206
06/17/2022 23:49:30 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=207
06/17/2022 23:49:37 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.9101938742261323 on epoch=207
06/17/2022 23:49:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=207
06/17/2022 23:49:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=208
06/17/2022 23:49:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
06/17/2022 23:49:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
06/17/2022 23:49:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=210
06/17/2022 23:49:57 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.9058797653958943 on epoch=210
06/17/2022 23:49:59 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=211
06/17/2022 23:50:02 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=212
06/17/2022 23:50:05 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=212
06/17/2022 23:50:07 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=213
06/17/2022 23:50:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=214
06/17/2022 23:50:11 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 23:50:11 - INFO - __main__ - Printing 3 examples
06/17/2022 23:50:11 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
06/17/2022 23:50:11 - INFO - __main__ - ['Animal']
06/17/2022 23:50:11 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
06/17/2022 23:50:11 - INFO - __main__ - ['Animal']
06/17/2022 23:50:11 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
06/17/2022 23:50:11 - INFO - __main__ - ['Animal']
06/17/2022 23:50:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/17/2022 23:50:11 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:50:12 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/17/2022 23:50:12 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 23:50:12 - INFO - __main__ - Printing 3 examples
06/17/2022 23:50:12 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
06/17/2022 23:50:12 - INFO - __main__ - ['Animal']
06/17/2022 23:50:12 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
06/17/2022 23:50:12 - INFO - __main__ - ['Animal']
06/17/2022 23:50:12 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
06/17/2022 23:50:12 - INFO - __main__ - ['Animal']
06/17/2022 23:50:12 - INFO - __main__ - Tokenizing Input ...
06/17/2022 23:50:12 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:50:12 - INFO - __main__ - Loaded 224 examples from dev data
06/17/2022 23:50:17 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.9060402411208862 on epoch=214
06/17/2022 23:50:17 - INFO - __main__ - save last model!
06/17/2022 23:50:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 23:50:17 - INFO - __main__ - Start tokenizing ... 3500 instances
06/17/2022 23:50:17 - INFO - __main__ - Printing 3 examples
06/17/2022 23:50:17 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/17/2022 23:50:17 - INFO - __main__ - ['Animal']
06/17/2022 23:50:17 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/17/2022 23:50:17 - INFO - __main__ - ['Animal']
06/17/2022 23:50:17 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/17/2022 23:50:17 - INFO - __main__ - ['Village']
06/17/2022 23:50:17 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 23:50:19 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:50:22 - INFO - __main__ - Loaded 3500 examples from test data
06/17/2022 23:50:27 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 23:50:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 23:50:28 - INFO - __main__ - Starting training!
06/17/2022 23:52:47 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_13_0.5_8_predictions.txt
06/17/2022 23:52:47 - INFO - __main__ - Classification-F1 on test data: 0.7223
06/17/2022 23:52:47 - INFO - __main__ - prefix=dbpedia_14_16_13, lr=0.5, bsz=8, dev_performance=0.9730475841362937, test_performance=0.7222666295004864
06/17/2022 23:52:47 - INFO - __main__ - Running ... prefix=dbpedia_14_16_13, lr=0.4, bsz=8 ...
06/17/2022 23:52:48 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 23:52:48 - INFO - __main__ - Printing 3 examples
06/17/2022 23:52:48 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
06/17/2022 23:52:48 - INFO - __main__ - ['Animal']
06/17/2022 23:52:48 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
06/17/2022 23:52:48 - INFO - __main__ - ['Animal']
06/17/2022 23:52:48 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
06/17/2022 23:52:48 - INFO - __main__ - ['Animal']
06/17/2022 23:52:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/17/2022 23:52:48 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:52:49 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/17/2022 23:52:49 - INFO - __main__ - Start tokenizing ... 224 instances
06/17/2022 23:52:49 - INFO - __main__ - Printing 3 examples
06/17/2022 23:52:49 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
06/17/2022 23:52:49 - INFO - __main__ - ['Animal']
06/17/2022 23:52:49 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
06/17/2022 23:52:49 - INFO - __main__ - ['Animal']
06/17/2022 23:52:49 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
06/17/2022 23:52:49 - INFO - __main__ - ['Animal']
06/17/2022 23:52:49 - INFO - __main__ - Tokenizing Input ...
06/17/2022 23:52:49 - INFO - __main__ - Tokenizing Output ...
06/17/2022 23:52:49 - INFO - __main__ - Loaded 224 examples from dev data
06/17/2022 23:53:04 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 23:53:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 23:53:05 - INFO - __main__ - Starting training!
06/17/2022 23:53:08 - INFO - __main__ - Step 10 Global step 10 Train loss 6.26 on epoch=0
06/17/2022 23:53:11 - INFO - __main__ - Step 20 Global step 20 Train loss 4.89 on epoch=1
06/17/2022 23:53:14 - INFO - __main__ - Step 30 Global step 30 Train loss 4.29 on epoch=2
06/17/2022 23:53:16 - INFO - __main__ - Step 40 Global step 40 Train loss 3.63 on epoch=2
06/17/2022 23:53:19 - INFO - __main__ - Step 50 Global step 50 Train loss 3.46 on epoch=3
06/17/2022 23:53:24 - INFO - __main__ - Global step 50 Train loss 4.51 Classification-F1 0.07280573767968727 on epoch=3
06/17/2022 23:53:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.07280573767968727 on epoch=3, global_step=50
06/17/2022 23:53:27 - INFO - __main__ - Step 60 Global step 60 Train loss 3.12 on epoch=4
06/17/2022 23:53:30 - INFO - __main__ - Step 70 Global step 70 Train loss 2.61 on epoch=4
06/17/2022 23:53:32 - INFO - __main__ - Step 80 Global step 80 Train loss 2.43 on epoch=5
06/17/2022 23:53:35 - INFO - __main__ - Step 90 Global step 90 Train loss 2.12 on epoch=6
06/17/2022 23:53:37 - INFO - __main__ - Step 100 Global step 100 Train loss 2.13 on epoch=7
06/17/2022 23:53:43 - INFO - __main__ - Global step 100 Train loss 2.48 Classification-F1 0.10469265007640982 on epoch=7
06/17/2022 23:53:43 - INFO - __main__ - Saving model with best Classification-F1: 0.07280573767968727 -> 0.10469265007640982 on epoch=7, global_step=100
06/17/2022 23:53:45 - INFO - __main__ - Step 110 Global step 110 Train loss 1.85 on epoch=7
06/17/2022 23:53:48 - INFO - __main__ - Step 120 Global step 120 Train loss 1.83 on epoch=8
06/17/2022 23:53:50 - INFO - __main__ - Step 130 Global step 130 Train loss 1.63 on epoch=9
06/17/2022 23:53:53 - INFO - __main__ - Step 140 Global step 140 Train loss 1.44 on epoch=9
06/17/2022 23:53:56 - INFO - __main__ - Step 150 Global step 150 Train loss 1.44 on epoch=10
06/17/2022 23:54:02 - INFO - __main__ - Global step 150 Train loss 1.64 Classification-F1 0.18684687078994708 on epoch=10
06/17/2022 23:54:02 - INFO - __main__ - Saving model with best Classification-F1: 0.10469265007640982 -> 0.18684687078994708 on epoch=10, global_step=150
06/17/2022 23:54:04 - INFO - __main__ - Step 160 Global step 160 Train loss 1.32 on epoch=11
06/17/2022 23:54:07 - INFO - __main__ - Step 170 Global step 170 Train loss 1.17 on epoch=12
06/17/2022 23:54:09 - INFO - __main__ - Step 180 Global step 180 Train loss 1.03 on epoch=12
06/17/2022 23:54:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.99 on epoch=13
06/17/2022 23:54:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.87 on epoch=14
06/17/2022 23:54:21 - INFO - __main__ - Global step 200 Train loss 1.07 Classification-F1 0.3857627074781197 on epoch=14
06/17/2022 23:54:21 - INFO - __main__ - Saving model with best Classification-F1: 0.18684687078994708 -> 0.3857627074781197 on epoch=14, global_step=200
06/17/2022 23:54:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.83 on epoch=14
06/17/2022 23:54:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.75 on epoch=15
06/17/2022 23:54:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.68 on epoch=16
06/17/2022 23:54:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.63 on epoch=17
06/17/2022 23:54:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=17
06/17/2022 23:54:41 - INFO - __main__ - Global step 250 Train loss 0.67 Classification-F1 0.5262595862738749 on epoch=17
06/17/2022 23:54:41 - INFO - __main__ - Saving model with best Classification-F1: 0.3857627074781197 -> 0.5262595862738749 on epoch=17, global_step=250
06/17/2022 23:54:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.56 on epoch=18
06/17/2022 23:54:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=19
06/17/2022 23:54:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=19
06/17/2022 23:54:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=20
06/17/2022 23:54:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=21
06/17/2022 23:55:00 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.48024946305112587 on epoch=21
06/17/2022 23:55:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=22
06/17/2022 23:55:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=22
06/17/2022 23:55:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.31 on epoch=23
06/17/2022 23:55:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.31 on epoch=24
06/17/2022 23:55:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.32 on epoch=24
06/17/2022 23:55:19 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.5798843351286426 on epoch=24
06/17/2022 23:55:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5262595862738749 -> 0.5798843351286426 on epoch=24, global_step=350
06/17/2022 23:55:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=25
06/17/2022 23:55:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.29 on epoch=26
06/17/2022 23:55:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=27
06/17/2022 23:55:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=27
06/17/2022 23:55:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=28
06/17/2022 23:55:39 - INFO - __main__ - Global step 400 Train loss 0.28 Classification-F1 0.6160414138605184 on epoch=28
06/17/2022 23:55:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5798843351286426 -> 0.6160414138605184 on epoch=28, global_step=400
06/17/2022 23:55:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=29
06/17/2022 23:55:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=29
06/17/2022 23:55:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.20 on epoch=30
06/17/2022 23:55:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.19 on epoch=31
06/17/2022 23:55:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=32
06/17/2022 23:55:58 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.6955019231993648 on epoch=32
06/17/2022 23:55:58 - INFO - __main__ - Saving model with best Classification-F1: 0.6160414138605184 -> 0.6955019231993648 on epoch=32, global_step=450
06/17/2022 23:56:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.18 on epoch=32
06/17/2022 23:56:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=33
06/17/2022 23:56:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.18 on epoch=34
06/17/2022 23:56:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.18 on epoch=34
06/17/2022 23:56:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.18 on epoch=35
06/17/2022 23:56:17 - INFO - __main__ - Global step 500 Train loss 0.18 Classification-F1 0.6856957275887666 on epoch=35
06/17/2022 23:56:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.13 on epoch=36
06/17/2022 23:56:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=37
06/17/2022 23:56:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=37
06/17/2022 23:56:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.16 on epoch=38
06/17/2022 23:56:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.15 on epoch=39
06/17/2022 23:56:37 - INFO - __main__ - Global step 550 Train loss 0.18 Classification-F1 0.62216582868864 on epoch=39
06/17/2022 23:56:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.13 on epoch=39
06/17/2022 23:56:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=40
06/17/2022 23:56:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=41
06/17/2022 23:56:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.16 on epoch=42
06/17/2022 23:56:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=42
06/17/2022 23:56:56 - INFO - __main__ - Global step 600 Train loss 0.14 Classification-F1 0.7392464972303682 on epoch=42
06/17/2022 23:56:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6955019231993648 -> 0.7392464972303682 on epoch=42, global_step=600
06/17/2022 23:56:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.09 on epoch=43
06/17/2022 23:57:01 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=44
06/17/2022 23:57:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=44
06/17/2022 23:57:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.14 on epoch=45
06/17/2022 23:57:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=46
06/17/2022 23:57:16 - INFO - __main__ - Global step 650 Train loss 0.11 Classification-F1 0.7200664044170719 on epoch=46
06/17/2022 23:57:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=47
06/17/2022 23:57:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=47
06/17/2022 23:57:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=48
06/17/2022 23:57:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=49
06/17/2022 23:57:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=49
06/17/2022 23:57:35 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.7492694730597955 on epoch=49
06/17/2022 23:57:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7392464972303682 -> 0.7492694730597955 on epoch=49, global_step=700
06/17/2022 23:57:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=50
06/17/2022 23:57:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=51
06/17/2022 23:57:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=52
06/17/2022 23:57:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=52
06/17/2022 23:57:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=53
06/17/2022 23:57:54 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.6945835598175599 on epoch=53
06/17/2022 23:57:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.12 on epoch=54
06/17/2022 23:57:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=54
06/17/2022 23:58:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.11 on epoch=55
06/17/2022 23:58:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.16 on epoch=56
06/17/2022 23:58:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=57
06/17/2022 23:58:13 - INFO - __main__ - Global step 800 Train loss 0.12 Classification-F1 0.7795330023118204 on epoch=57
06/17/2022 23:58:13 - INFO - __main__ - Saving model with best Classification-F1: 0.7492694730597955 -> 0.7795330023118204 on epoch=57, global_step=800
06/17/2022 23:58:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=57
06/17/2022 23:58:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=58
06/17/2022 23:58:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=59
06/17/2022 23:58:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=59
06/17/2022 23:58:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=60
06/17/2022 23:58:32 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.7180765993265992 on epoch=60
06/17/2022 23:58:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=61
06/17/2022 23:58:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.11 on epoch=62
06/17/2022 23:58:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=62
06/17/2022 23:58:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=63
06/17/2022 23:58:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=64
06/17/2022 23:58:51 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.7871724217159457 on epoch=64
06/17/2022 23:58:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7795330023118204 -> 0.7871724217159457 on epoch=64, global_step=900
06/17/2022 23:58:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=64
06/17/2022 23:58:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=65
06/17/2022 23:58:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=66
06/17/2022 23:59:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=67
06/17/2022 23:59:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=67
06/17/2022 23:59:10 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.8404841546222594 on epoch=67
06/17/2022 23:59:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7871724217159457 -> 0.8404841546222594 on epoch=67, global_step=950
06/17/2022 23:59:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.14 on epoch=68
06/17/2022 23:59:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=69
06/17/2022 23:59:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=69
06/17/2022 23:59:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=70
06/17/2022 23:59:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=71
06/17/2022 23:59:30 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.7934734870791272 on epoch=71
06/17/2022 23:59:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=72
06/17/2022 23:59:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=72
06/17/2022 23:59:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=73
06/17/2022 23:59:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=74
06/17/2022 23:59:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=74
06/17/2022 23:59:49 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.9058927989573149 on epoch=74
06/17/2022 23:59:49 - INFO - __main__ - Saving model with best Classification-F1: 0.8404841546222594 -> 0.9058927989573149 on epoch=74, global_step=1050
06/17/2022 23:59:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=75
06/17/2022 23:59:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=76
06/17/2022 23:59:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=77
06/17/2022 23:59:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=77
06/18/2022 00:00:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=78
06/18/2022 00:00:08 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.7917807166468213 on epoch=78
06/18/2022 00:00:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=79
06/18/2022 00:00:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=79
06/18/2022 00:00:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=80
06/18/2022 00:00:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=81
06/18/2022 00:00:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=82
06/18/2022 00:00:27 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.8433521199902249 on epoch=82
06/18/2022 00:00:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=82
06/18/2022 00:00:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=83
06/18/2022 00:00:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=84
06/18/2022 00:00:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=84
06/18/2022 00:00:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=85
06/18/2022 00:00:47 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.8472092462857325 on epoch=85
06/18/2022 00:00:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=86
06/18/2022 00:00:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=87
06/18/2022 00:00:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=87
06/18/2022 00:00:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=88
06/18/2022 00:01:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=89
06/18/2022 00:01:06 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.8452460593841642 on epoch=89
06/18/2022 00:01:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=89
06/18/2022 00:01:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=90
06/18/2022 00:01:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=91
06/18/2022 00:01:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=92
06/18/2022 00:01:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=92
06/18/2022 00:01:25 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.7992021735380369 on epoch=92
06/18/2022 00:01:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=93
06/18/2022 00:01:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=94
06/18/2022 00:01:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=94
06/18/2022 00:01:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=95
06/18/2022 00:01:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=96
06/18/2022 00:01:44 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.8410142316229758 on epoch=96
06/18/2022 00:01:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=97
06/18/2022 00:01:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=97
06/18/2022 00:01:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=98
06/18/2022 00:01:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=99
06/18/2022 00:01:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=99
06/18/2022 00:02:03 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.7955257029498016 on epoch=99
06/18/2022 00:02:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=100
06/18/2022 00:02:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=101
06/18/2022 00:02:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=102
06/18/2022 00:02:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=102
06/18/2022 00:02:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=103
06/18/2022 00:02:22 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.8374982929388765 on epoch=103
06/18/2022 00:02:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=104
06/18/2022 00:02:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=104
06/18/2022 00:02:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=105
06/18/2022 00:02:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=106
06/18/2022 00:02:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=107
06/18/2022 00:02:42 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.8392916055718476 on epoch=107
06/18/2022 00:02:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=107
06/18/2022 00:02:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=108
06/18/2022 00:02:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=109
06/18/2022 00:02:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=109
06/18/2022 00:02:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=110
06/18/2022 00:03:02 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.8973384453049765 on epoch=110
06/18/2022 00:03:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=111
06/18/2022 00:03:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=112
06/18/2022 00:03:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=112
06/18/2022 00:03:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=113
06/18/2022 00:03:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=114
06/18/2022 00:03:22 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.8513028470185728 on epoch=114
06/18/2022 00:03:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=114
06/18/2022 00:03:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=115
06/18/2022 00:03:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=116
06/18/2022 00:03:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=117
06/18/2022 00:03:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=117
06/18/2022 00:03:41 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.8513028470185728 on epoch=117
06/18/2022 00:03:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=118
06/18/2022 00:03:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=119
06/18/2022 00:03:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=119
06/18/2022 00:03:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=120
06/18/2022 00:03:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=121
06/18/2022 00:04:01 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.9017959233108418 on epoch=121
06/18/2022 00:04:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=122
06/18/2022 00:04:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=122
06/18/2022 00:04:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=123
06/18/2022 00:04:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=124
06/18/2022 00:04:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=124
06/18/2022 00:04:21 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.9014976847583339 on epoch=124
06/18/2022 00:04:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=125
06/18/2022 00:04:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=126
06/18/2022 00:04:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=127
06/18/2022 00:04:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=127
06/18/2022 00:04:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=128
06/18/2022 00:04:42 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.9100070670058564 on epoch=128
06/18/2022 00:04:42 - INFO - __main__ - Saving model with best Classification-F1: 0.9058927989573149 -> 0.9100070670058564 on epoch=128, global_step=1800
06/18/2022 00:04:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=129
06/18/2022 00:04:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=129
06/18/2022 00:04:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=130
06/18/2022 00:04:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=131
06/18/2022 00:04:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=132
06/18/2022 00:05:02 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.9100070670058564 on epoch=132
06/18/2022 00:05:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=132
06/18/2022 00:05:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=133
06/18/2022 00:05:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=134
06/18/2022 00:05:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=134
06/18/2022 00:05:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=135
06/18/2022 00:05:22 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.9056970311635626 on epoch=135
06/18/2022 00:05:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=136
06/18/2022 00:05:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=137
06/18/2022 00:05:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=137
06/18/2022 00:05:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=138
06/18/2022 00:05:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=139
06/18/2022 00:05:43 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.9101938742261323 on epoch=139
06/18/2022 00:05:43 - INFO - __main__ - Saving model with best Classification-F1: 0.9100070670058564 -> 0.9101938742261323 on epoch=139, global_step=1950
06/18/2022 00:05:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=139
06/18/2022 00:05:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=140
06/18/2022 00:05:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=141
06/18/2022 00:05:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=142
06/18/2022 00:05:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=142
06/18/2022 00:06:03 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.8513028470185728 on epoch=142
06/18/2022 00:06:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=143
06/18/2022 00:06:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=144
06/18/2022 00:06:11 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=144
06/18/2022 00:06:14 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=145
06/18/2022 00:06:16 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=146
06/18/2022 00:06:23 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.8513028470185728 on epoch=146
06/18/2022 00:06:26 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=147
06/18/2022 00:06:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=147
06/18/2022 00:06:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=148
06/18/2022 00:06:34 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=149
06/18/2022 00:06:36 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=149
06/18/2022 00:06:44 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.9103216702125622 on epoch=149
06/18/2022 00:06:44 - INFO - __main__ - Saving model with best Classification-F1: 0.9101938742261323 -> 0.9103216702125622 on epoch=149, global_step=2100
06/18/2022 00:06:46 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=150
06/18/2022 00:06:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=151
06/18/2022 00:06:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=152
06/18/2022 00:06:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=152
06/18/2022 00:06:57 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=153
06/18/2022 00:07:04 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.9100070670058564 on epoch=153
06/18/2022 00:07:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=154
06/18/2022 00:07:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=154
06/18/2022 00:07:12 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=155
06/18/2022 00:07:15 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=156
06/18/2022 00:07:17 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=157
06/18/2022 00:07:24 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.9100070670058564 on epoch=157
06/18/2022 00:07:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=157
06/18/2022 00:07:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=158
06/18/2022 00:07:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=159
06/18/2022 00:07:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=159
06/18/2022 00:07:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=160
06/18/2022 00:07:44 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.9100070670058564 on epoch=160
06/18/2022 00:07:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=161
06/18/2022 00:07:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=162
06/18/2022 00:07:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=162
06/18/2022 00:07:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=163
06/18/2022 00:07:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=164
06/18/2022 00:08:05 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.9100070670058564 on epoch=164
06/18/2022 00:08:07 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=164
06/18/2022 00:08:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
06/18/2022 00:08:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=166
06/18/2022 00:08:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=167
06/18/2022 00:08:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=167
06/18/2022 00:08:25 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.901852394916911 on epoch=167
06/18/2022 00:08:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=168
06/18/2022 00:08:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=169
06/18/2022 00:08:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=169
06/18/2022 00:08:35 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=170
06/18/2022 00:08:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=171
06/18/2022 00:08:45 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.9017261322906482 on epoch=171
06/18/2022 00:08:47 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=172
06/18/2022 00:08:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=172
06/18/2022 00:08:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=173
06/18/2022 00:08:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=174
06/18/2022 00:08:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=174
06/18/2022 00:09:05 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.8511154962857325 on epoch=174
06/18/2022 00:09:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=175
06/18/2022 00:09:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=176
06/18/2022 00:09:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=177
06/18/2022 00:09:15 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=177
06/18/2022 00:09:18 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=178
06/18/2022 00:09:25 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.847246151026393 on epoch=178
06/18/2022 00:09:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=179
06/18/2022 00:09:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
06/18/2022 00:09:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=180
06/18/2022 00:09:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=181
06/18/2022 00:09:38 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=182
06/18/2022 00:09:45 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.8511154962857325 on epoch=182
06/18/2022 00:09:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=182
06/18/2022 00:09:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
06/18/2022 00:09:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=184
06/18/2022 00:09:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=184
06/18/2022 00:09:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=185
06/18/2022 00:10:05 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.8432831404695057 on epoch=185
06/18/2022 00:10:08 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=186
06/18/2022 00:10:10 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=187
06/18/2022 00:10:13 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
06/18/2022 00:10:15 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=188
06/18/2022 00:10:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=189
06/18/2022 00:10:25 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.9101938742261323 on epoch=189
06/18/2022 00:10:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
06/18/2022 00:10:30 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=190
06/18/2022 00:10:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=191
06/18/2022 00:10:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
06/18/2022 00:10:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=192
06/18/2022 00:10:45 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.9102069077875529 on epoch=192
06/18/2022 00:10:48 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=193
06/18/2022 00:10:50 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
06/18/2022 00:10:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
06/18/2022 00:10:56 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=195
06/18/2022 00:10:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=196
06/18/2022 00:11:05 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.89771948530651 on epoch=196
06/18/2022 00:11:07 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=197
06/18/2022 00:11:10 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=197
06/18/2022 00:11:13 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
06/18/2022 00:11:15 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=199
06/18/2022 00:11:18 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=199
06/18/2022 00:11:25 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.9056970311635626 on epoch=199
06/18/2022 00:11:27 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=200
06/18/2022 00:11:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=201
06/18/2022 00:11:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=202
06/18/2022 00:11:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=202
06/18/2022 00:11:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=203
06/18/2022 00:11:45 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.8472092462857325 on epoch=203
06/18/2022 00:11:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=204
06/18/2022 00:11:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=204
06/18/2022 00:11:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=205
06/18/2022 00:11:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=206
06/18/2022 00:11:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=207
06/18/2022 00:12:05 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.851152401026393 on epoch=207
06/18/2022 00:12:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=207
06/18/2022 00:12:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=208
06/18/2022 00:12:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
06/18/2022 00:12:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
06/18/2022 00:12:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=210
06/18/2022 00:12:25 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.8434704912023461 on epoch=210
06/18/2022 00:12:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=211
06/18/2022 00:12:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
06/18/2022 00:12:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=212
06/18/2022 00:12:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=213
06/18/2022 00:12:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=214
06/18/2022 00:12:40 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 00:12:40 - INFO - __main__ - Printing 3 examples
06/18/2022 00:12:40 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
06/18/2022 00:12:40 - INFO - __main__ - ['Animal']
06/18/2022 00:12:40 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
06/18/2022 00:12:40 - INFO - __main__ - ['Animal']
06/18/2022 00:12:40 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
06/18/2022 00:12:40 - INFO - __main__ - ['Animal']
06/18/2022 00:12:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 00:12:40 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:12:40 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 00:12:40 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 00:12:40 - INFO - __main__ - Printing 3 examples
06/18/2022 00:12:40 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
06/18/2022 00:12:40 - INFO - __main__ - ['Animal']
06/18/2022 00:12:40 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
06/18/2022 00:12:40 - INFO - __main__ - ['Animal']
06/18/2022 00:12:40 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
06/18/2022 00:12:40 - INFO - __main__ - ['Animal']
06/18/2022 00:12:40 - INFO - __main__ - Tokenizing Input ...
06/18/2022 00:12:40 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:12:40 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 00:12:45 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7916889483065955 on epoch=214
06/18/2022 00:12:45 - INFO - __main__ - save last model!
06/18/2022 00:12:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 00:12:46 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 00:12:46 - INFO - __main__ - Printing 3 examples
06/18/2022 00:12:46 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 00:12:46 - INFO - __main__ - ['Animal']
06/18/2022 00:12:46 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 00:12:46 - INFO - __main__ - ['Animal']
06/18/2022 00:12:46 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 00:12:46 - INFO - __main__ - ['Village']
06/18/2022 00:12:46 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 00:12:47 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:12:51 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 00:12:56 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 00:12:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 00:12:56 - INFO - __main__ - Starting training!
06/18/2022 00:15:16 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_13_0.4_8_predictions.txt
06/18/2022 00:15:16 - INFO - __main__ - Classification-F1 on test data: 0.6239
06/18/2022 00:15:17 - INFO - __main__ - prefix=dbpedia_14_16_13, lr=0.4, bsz=8, dev_performance=0.9103216702125622, test_performance=0.6239309435130773
06/18/2022 00:15:17 - INFO - __main__ - Running ... prefix=dbpedia_14_16_13, lr=0.3, bsz=8 ...
06/18/2022 00:15:18 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 00:15:18 - INFO - __main__ - Printing 3 examples
06/18/2022 00:15:18 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
06/18/2022 00:15:18 - INFO - __main__ - ['Animal']
06/18/2022 00:15:18 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
06/18/2022 00:15:18 - INFO - __main__ - ['Animal']
06/18/2022 00:15:18 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
06/18/2022 00:15:18 - INFO - __main__ - ['Animal']
06/18/2022 00:15:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 00:15:18 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:15:18 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 00:15:18 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 00:15:18 - INFO - __main__ - Printing 3 examples
06/18/2022 00:15:18 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
06/18/2022 00:15:18 - INFO - __main__ - ['Animal']
06/18/2022 00:15:18 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
06/18/2022 00:15:18 - INFO - __main__ - ['Animal']
06/18/2022 00:15:18 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
06/18/2022 00:15:18 - INFO - __main__ - ['Animal']
06/18/2022 00:15:18 - INFO - __main__ - Tokenizing Input ...
06/18/2022 00:15:18 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:15:18 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 00:15:37 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 00:15:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 00:15:37 - INFO - __main__ - Starting training!
06/18/2022 00:15:41 - INFO - __main__ - Step 10 Global step 10 Train loss 6.56 on epoch=0
06/18/2022 00:15:43 - INFO - __main__ - Step 20 Global step 20 Train loss 5.09 on epoch=1
06/18/2022 00:15:46 - INFO - __main__ - Step 30 Global step 30 Train loss 4.53 on epoch=2
06/18/2022 00:15:49 - INFO - __main__ - Step 40 Global step 40 Train loss 3.93 on epoch=2
06/18/2022 00:15:51 - INFO - __main__ - Step 50 Global step 50 Train loss 3.77 on epoch=3
06/18/2022 00:15:57 - INFO - __main__ - Global step 50 Train loss 4.78 Classification-F1 0.043547577965961294 on epoch=3
06/18/2022 00:15:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.043547577965961294 on epoch=3, global_step=50
06/18/2022 00:16:00 - INFO - __main__ - Step 60 Global step 60 Train loss 3.42 on epoch=4
06/18/2022 00:16:03 - INFO - __main__ - Step 70 Global step 70 Train loss 3.21 on epoch=4
06/18/2022 00:16:05 - INFO - __main__ - Step 80 Global step 80 Train loss 2.99 on epoch=5
06/18/2022 00:16:08 - INFO - __main__ - Step 90 Global step 90 Train loss 2.44 on epoch=6
06/18/2022 00:16:11 - INFO - __main__ - Step 100 Global step 100 Train loss 2.65 on epoch=7
06/18/2022 00:16:16 - INFO - __main__ - Global step 100 Train loss 2.94 Classification-F1 0.09582374769431698 on epoch=7
06/18/2022 00:16:16 - INFO - __main__ - Saving model with best Classification-F1: 0.043547577965961294 -> 0.09582374769431698 on epoch=7, global_step=100
06/18/2022 00:16:19 - INFO - __main__ - Step 110 Global step 110 Train loss 2.33 on epoch=7
06/18/2022 00:16:21 - INFO - __main__ - Step 120 Global step 120 Train loss 2.12 on epoch=8
06/18/2022 00:16:24 - INFO - __main__ - Step 130 Global step 130 Train loss 2.09 on epoch=9
06/18/2022 00:16:27 - INFO - __main__ - Step 140 Global step 140 Train loss 1.92 on epoch=9
06/18/2022 00:16:29 - INFO - __main__ - Step 150 Global step 150 Train loss 1.89 on epoch=10
06/18/2022 00:16:35 - INFO - __main__ - Global step 150 Train loss 2.07 Classification-F1 0.11002376392891822 on epoch=10
06/18/2022 00:16:35 - INFO - __main__ - Saving model with best Classification-F1: 0.09582374769431698 -> 0.11002376392891822 on epoch=10, global_step=150
06/18/2022 00:16:38 - INFO - __main__ - Step 160 Global step 160 Train loss 1.67 on epoch=11
06/18/2022 00:16:40 - INFO - __main__ - Step 170 Global step 170 Train loss 1.62 on epoch=12
06/18/2022 00:16:43 - INFO - __main__ - Step 180 Global step 180 Train loss 1.46 on epoch=12
06/18/2022 00:16:45 - INFO - __main__ - Step 190 Global step 190 Train loss 1.52 on epoch=13
06/18/2022 00:16:48 - INFO - __main__ - Step 200 Global step 200 Train loss 1.28 on epoch=14
06/18/2022 00:16:54 - INFO - __main__ - Global step 200 Train loss 1.51 Classification-F1 0.16569364641731624 on epoch=14
06/18/2022 00:16:54 - INFO - __main__ - Saving model with best Classification-F1: 0.11002376392891822 -> 0.16569364641731624 on epoch=14, global_step=200
06/18/2022 00:16:57 - INFO - __main__ - Step 210 Global step 210 Train loss 1.28 on epoch=14
06/18/2022 00:16:59 - INFO - __main__ - Step 220 Global step 220 Train loss 1.21 on epoch=15
06/18/2022 00:17:02 - INFO - __main__ - Step 230 Global step 230 Train loss 1.06 on epoch=16
06/18/2022 00:17:04 - INFO - __main__ - Step 240 Global step 240 Train loss 1.05 on epoch=17
06/18/2022 00:17:07 - INFO - __main__ - Step 250 Global step 250 Train loss 1.00 on epoch=17
06/18/2022 00:17:14 - INFO - __main__ - Global step 250 Train loss 1.12 Classification-F1 0.3058358746492188 on epoch=17
06/18/2022 00:17:14 - INFO - __main__ - Saving model with best Classification-F1: 0.16569364641731624 -> 0.3058358746492188 on epoch=17, global_step=250
06/18/2022 00:17:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.89 on epoch=18
06/18/2022 00:17:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.88 on epoch=19
06/18/2022 00:17:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.80 on epoch=19
06/18/2022 00:17:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.76 on epoch=20
06/18/2022 00:17:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.71 on epoch=21
06/18/2022 00:17:33 - INFO - __main__ - Global step 300 Train loss 0.81 Classification-F1 0.5136045658836836 on epoch=21
06/18/2022 00:17:33 - INFO - __main__ - Saving model with best Classification-F1: 0.3058358746492188 -> 0.5136045658836836 on epoch=21, global_step=300
06/18/2022 00:17:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.75 on epoch=22
06/18/2022 00:17:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.58 on epoch=22
06/18/2022 00:17:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=23
06/18/2022 00:17:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.62 on epoch=24
06/18/2022 00:17:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=24
06/18/2022 00:17:53 - INFO - __main__ - Global step 350 Train loss 0.60 Classification-F1 0.5494058474197281 on epoch=24
06/18/2022 00:17:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5136045658836836 -> 0.5494058474197281 on epoch=24, global_step=350
06/18/2022 00:17:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=25
06/18/2022 00:17:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.56 on epoch=26
06/18/2022 00:18:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=27
06/18/2022 00:18:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=27
06/18/2022 00:18:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=28
06/18/2022 00:18:12 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.5605345449812786 on epoch=28
06/18/2022 00:18:12 - INFO - __main__ - Saving model with best Classification-F1: 0.5494058474197281 -> 0.5605345449812786 on epoch=28, global_step=400
06/18/2022 00:18:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=29
06/18/2022 00:18:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=29
06/18/2022 00:18:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=30
06/18/2022 00:18:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.32 on epoch=31
06/18/2022 00:18:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=32
06/18/2022 00:18:32 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.609164385687197 on epoch=32
06/18/2022 00:18:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5605345449812786 -> 0.609164385687197 on epoch=32, global_step=450
06/18/2022 00:18:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=32
06/18/2022 00:18:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=33
06/18/2022 00:18:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=34
06/18/2022 00:18:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=34
06/18/2022 00:18:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=35
06/18/2022 00:18:51 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.6788960734557864 on epoch=35
06/18/2022 00:18:51 - INFO - __main__ - Saving model with best Classification-F1: 0.609164385687197 -> 0.6788960734557864 on epoch=35, global_step=500
06/18/2022 00:18:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=36
06/18/2022 00:18:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=37
06/18/2022 00:18:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=37
06/18/2022 00:19:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=38
06/18/2022 00:19:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=39
06/18/2022 00:19:11 - INFO - __main__ - Global step 550 Train loss 0.28 Classification-F1 0.6466136427946699 on epoch=39
06/18/2022 00:19:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=39
06/18/2022 00:19:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=40
06/18/2022 00:19:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=41
06/18/2022 00:19:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=42
06/18/2022 00:19:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=42
06/18/2022 00:19:30 - INFO - __main__ - Global step 600 Train loss 0.25 Classification-F1 0.7256906623842109 on epoch=42
06/18/2022 00:19:30 - INFO - __main__ - Saving model with best Classification-F1: 0.6788960734557864 -> 0.7256906623842109 on epoch=42, global_step=600
06/18/2022 00:19:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=43
06/18/2022 00:19:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.28 on epoch=44
06/18/2022 00:19:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=44
06/18/2022 00:19:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=45
06/18/2022 00:19:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=46
06/18/2022 00:19:49 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.7294191919191919 on epoch=46
06/18/2022 00:19:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7256906623842109 -> 0.7294191919191919 on epoch=46, global_step=650
06/18/2022 00:19:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=47
06/18/2022 00:19:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=47
06/18/2022 00:19:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=48
06/18/2022 00:20:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=49
06/18/2022 00:20:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=49
06/18/2022 00:20:09 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.776299272737273 on epoch=49
06/18/2022 00:20:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7294191919191919 -> 0.776299272737273 on epoch=49, global_step=700
06/18/2022 00:20:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=50
06/18/2022 00:20:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=51
06/18/2022 00:20:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=52
06/18/2022 00:20:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=52
06/18/2022 00:20:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=53
06/18/2022 00:20:28 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.6908887872173102 on epoch=53
06/18/2022 00:20:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=54
06/18/2022 00:20:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.16 on epoch=54
06/18/2022 00:20:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=55
06/18/2022 00:20:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=56
06/18/2022 00:20:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=57
06/18/2022 00:20:47 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.7724212307969953 on epoch=57
06/18/2022 00:20:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=57
06/18/2022 00:20:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=58
06/18/2022 00:20:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=59
06/18/2022 00:20:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=59
06/18/2022 00:21:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.15 on epoch=60
06/18/2022 00:21:06 - INFO - __main__ - Global step 850 Train loss 0.18 Classification-F1 0.7409600047883234 on epoch=60
06/18/2022 00:21:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=61
06/18/2022 00:21:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=62
06/18/2022 00:21:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=62
06/18/2022 00:21:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=63
06/18/2022 00:21:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=64
06/18/2022 00:21:26 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.7009138498739517 on epoch=64
06/18/2022 00:21:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=64
06/18/2022 00:21:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.11 on epoch=65
06/18/2022 00:21:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=66
06/18/2022 00:21:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=67
06/18/2022 00:21:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=67
06/18/2022 00:21:45 - INFO - __main__ - Global step 950 Train loss 0.12 Classification-F1 0.7480839608751795 on epoch=67
06/18/2022 00:21:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=68
06/18/2022 00:21:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.13 on epoch=69
06/18/2022 00:21:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=69
06/18/2022 00:21:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=70
06/18/2022 00:21:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=71
06/18/2022 00:22:04 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.7907374841045375 on epoch=71
06/18/2022 00:22:04 - INFO - __main__ - Saving model with best Classification-F1: 0.776299272737273 -> 0.7907374841045375 on epoch=71, global_step=1000
06/18/2022 00:22:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=72
06/18/2022 00:22:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=72
06/18/2022 00:22:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=73
06/18/2022 00:22:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=74
06/18/2022 00:22:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=74
06/18/2022 00:22:23 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.7789685453750004 on epoch=74
06/18/2022 00:22:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=75
06/18/2022 00:22:28 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=76
06/18/2022 00:22:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.11 on epoch=77
06/18/2022 00:22:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=77
06/18/2022 00:22:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.08 on epoch=78
06/18/2022 00:22:42 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.7826702572137813 on epoch=78
06/18/2022 00:22:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=79
06/18/2022 00:22:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=79
06/18/2022 00:22:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=80
06/18/2022 00:22:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=81
06/18/2022 00:22:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=82
06/18/2022 00:23:02 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.7826702572137813 on epoch=82
06/18/2022 00:23:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=82
06/18/2022 00:23:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=83
06/18/2022 00:23:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=84
06/18/2022 00:23:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=84
06/18/2022 00:23:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=85
06/18/2022 00:23:21 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.8449204816229758 on epoch=85
06/18/2022 00:23:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7907374841045375 -> 0.8449204816229758 on epoch=85, global_step=1200
06/18/2022 00:23:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=86
06/18/2022 00:23:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=87
06/18/2022 00:23:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=87
06/18/2022 00:23:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=88
06/18/2022 00:23:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=89
06/18/2022 00:23:40 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.7883776952883919 on epoch=89
06/18/2022 00:23:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=89
06/18/2022 00:23:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=90
06/18/2022 00:23:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=91
06/18/2022 00:23:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=92
06/18/2022 00:23:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=92
06/18/2022 00:24:00 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.8432831404695056 on epoch=92
06/18/2022 00:24:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=93
06/18/2022 00:24:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=94
06/18/2022 00:24:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=94
06/18/2022 00:24:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=95
06/18/2022 00:24:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=96
06/18/2022 00:24:19 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.8472092462857325 on epoch=96
06/18/2022 00:24:19 - INFO - __main__ - Saving model with best Classification-F1: 0.8449204816229758 -> 0.8472092462857325 on epoch=96, global_step=1350
06/18/2022 00:24:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=97
06/18/2022 00:24:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=97
06/18/2022 00:24:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=98
06/18/2022 00:24:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=99
06/18/2022 00:24:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=99
06/18/2022 00:24:38 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.851290628054741 on epoch=99
06/18/2022 00:24:38 - INFO - __main__ - Saving model with best Classification-F1: 0.8472092462857325 -> 0.851290628054741 on epoch=99, global_step=1400
06/18/2022 00:24:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=100
06/18/2022 00:24:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=101
06/18/2022 00:24:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=102
06/18/2022 00:24:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=102
06/18/2022 00:24:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=103
06/18/2022 00:24:58 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.7897914150997642 on epoch=103
06/18/2022 00:25:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=104
06/18/2022 00:25:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=104
06/18/2022 00:25:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=105
06/18/2022 00:25:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=106
06/18/2022 00:25:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=107
06/18/2022 00:25:17 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.8472583699902249 on epoch=107
06/18/2022 00:25:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=107
06/18/2022 00:25:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=108
06/18/2022 00:25:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=109
06/18/2022 00:25:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=109
06/18/2022 00:25:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=110
06/18/2022 00:25:37 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.8433437194525905 on epoch=110
06/18/2022 00:25:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=111
06/18/2022 00:25:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=112
06/18/2022 00:25:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=112
06/18/2022 00:25:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=113
06/18/2022 00:25:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=114
06/18/2022 00:25:56 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.7916693084471776 on epoch=114
06/18/2022 00:25:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=114
06/18/2022 00:26:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=115
06/18/2022 00:26:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=116
06/18/2022 00:26:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=117
06/18/2022 00:26:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=117
06/18/2022 00:26:16 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.787885742234769 on epoch=117
06/18/2022 00:26:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=118
06/18/2022 00:26:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=119
06/18/2022 00:26:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=119
06/18/2022 00:26:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=120
06/18/2022 00:26:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=121
06/18/2022 00:26:35 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.7438753634845698 on epoch=121
06/18/2022 00:26:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=122
06/18/2022 00:26:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=122
06/18/2022 00:26:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=123
06/18/2022 00:26:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=124
06/18/2022 00:26:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=124
06/18/2022 00:26:55 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.8393076429618769 on epoch=124
06/18/2022 00:26:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=125
06/18/2022 00:27:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=126
06/18/2022 00:27:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=127
06/18/2022 00:27:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=127
06/18/2022 00:27:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=128
06/18/2022 00:27:15 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.8933601192187529 on epoch=128
06/18/2022 00:27:15 - INFO - __main__ - Saving model with best Classification-F1: 0.851290628054741 -> 0.8933601192187529 on epoch=128, global_step=1800
06/18/2022 00:27:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=129
06/18/2022 00:27:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=129
06/18/2022 00:27:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=130
06/18/2022 00:27:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=131
06/18/2022 00:27:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=132
06/18/2022 00:27:35 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.9015876507005537 on epoch=132
06/18/2022 00:27:35 - INFO - __main__ - Saving model with best Classification-F1: 0.8933601192187529 -> 0.9015876507005537 on epoch=132, global_step=1850
06/18/2022 00:27:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=132
06/18/2022 00:27:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=133
06/18/2022 00:27:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=134
06/18/2022 00:27:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=134
06/18/2022 00:27:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=135
06/18/2022 00:27:55 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.847246151026393 on epoch=135
06/18/2022 00:27:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=136
06/18/2022 00:28:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=137
06/18/2022 00:28:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=137
06/18/2022 00:28:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=138
06/18/2022 00:28:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=139
06/18/2022 00:28:14 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.9101938742261323 on epoch=139
06/18/2022 00:28:14 - INFO - __main__ - Saving model with best Classification-F1: 0.9015876507005537 -> 0.9101938742261323 on epoch=139, global_step=1950
06/18/2022 00:28:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=139
06/18/2022 00:28:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=140
06/18/2022 00:28:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=141
06/18/2022 00:28:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=142
06/18/2022 00:28:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=142
06/18/2022 00:28:34 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.9017049527533397 on epoch=142
06/18/2022 00:28:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=143
06/18/2022 00:28:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=144
06/18/2022 00:28:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=144
06/18/2022 00:28:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=145
06/18/2022 00:28:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=146
06/18/2022 00:28:54 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.9058797653958943 on epoch=146
06/18/2022 00:28:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=147
06/18/2022 00:28:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=147
06/18/2022 00:29:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=148
06/18/2022 00:29:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=149
06/18/2022 00:29:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=149
06/18/2022 00:29:13 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.9015393250703726 on epoch=149
06/18/2022 00:29:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=150
06/18/2022 00:29:18 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=151
06/18/2022 00:29:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=152
06/18/2022 00:29:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=152
06/18/2022 00:29:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=153
06/18/2022 00:29:33 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.8431685876835818 on epoch=153
06/18/2022 00:29:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=154
06/18/2022 00:29:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=154
06/18/2022 00:29:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=155
06/18/2022 00:29:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=156
06/18/2022 00:29:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=157
06/18/2022 00:29:53 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.8432261119257087 on epoch=157
06/18/2022 00:29:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=157
06/18/2022 00:29:58 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=158
06/18/2022 00:30:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=159
06/18/2022 00:30:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=159
06/18/2022 00:30:06 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=160
06/18/2022 00:30:12 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.9058797653958943 on epoch=160
06/18/2022 00:30:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=161
06/18/2022 00:30:18 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=162
06/18/2022 00:30:20 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=162
06/18/2022 00:30:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=163
06/18/2022 00:30:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=164
06/18/2022 00:30:32 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.847246151026393 on epoch=164
06/18/2022 00:30:35 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=164
06/18/2022 00:30:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=165
06/18/2022 00:30:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=166
06/18/2022 00:30:42 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=167
06/18/2022 00:30:45 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=167
06/18/2022 00:30:52 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.901526291508952 on epoch=167
06/18/2022 00:30:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=168
06/18/2022 00:30:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=169
06/18/2022 00:31:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=169
06/18/2022 00:31:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=170
06/18/2022 00:31:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=171
06/18/2022 00:31:11 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.847246151026393 on epoch=171
06/18/2022 00:31:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=172
06/18/2022 00:31:17 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=172
06/18/2022 00:31:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=173
06/18/2022 00:31:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=174
06/18/2022 00:31:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=174
06/18/2022 00:31:31 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.8472583699902249 on epoch=174
06/18/2022 00:31:33 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=175
06/18/2022 00:31:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=176
06/18/2022 00:31:38 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=177
06/18/2022 00:31:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
06/18/2022 00:31:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=178
06/18/2022 00:31:50 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.8433200452101662 on epoch=178
06/18/2022 00:31:52 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=179
06/18/2022 00:31:55 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=179
06/18/2022 00:31:57 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=180
06/18/2022 00:32:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=181
06/18/2022 00:32:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=182
06/18/2022 00:32:09 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.8472193321976884 on epoch=182
06/18/2022 00:32:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
06/18/2022 00:32:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
06/18/2022 00:32:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=184
06/18/2022 00:32:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
06/18/2022 00:32:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=185
06/18/2022 00:32:28 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7880382225817466 on epoch=185
06/18/2022 00:32:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=186
06/18/2022 00:32:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=187
06/18/2022 00:32:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
06/18/2022 00:32:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=188
06/18/2022 00:32:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=189
06/18/2022 00:32:47 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.8512638092260365 on epoch=189
06/18/2022 00:32:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=189
06/18/2022 00:32:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=190
06/18/2022 00:32:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
06/18/2022 00:32:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
06/18/2022 00:33:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=192
06/18/2022 00:33:07 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.8393869763814618 on epoch=192
06/18/2022 00:33:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=193
06/18/2022 00:33:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
06/18/2022 00:33:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=194
06/18/2022 00:33:17 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=195
06/18/2022 00:33:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=196
06/18/2022 00:33:26 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.8393335593540683 on epoch=196
06/18/2022 00:33:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=197
06/18/2022 00:33:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=197
06/18/2022 00:33:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=198
06/18/2022 00:33:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=199
06/18/2022 00:33:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=199
06/18/2022 00:33:46 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.8393869763814618 on epoch=199
06/18/2022 00:33:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=200
06/18/2022 00:33:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=201
06/18/2022 00:33:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=202
06/18/2022 00:33:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
06/18/2022 00:33:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=203
06/18/2022 00:34:06 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.8473645222385142 on epoch=203
06/18/2022 00:34:08 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=204
06/18/2022 00:34:11 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=204
06/18/2022 00:34:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=205
06/18/2022 00:34:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=206
06/18/2022 00:34:19 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
06/18/2022 00:34:25 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.8434314534098097 on epoch=207
06/18/2022 00:34:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=207
06/18/2022 00:34:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=208
06/18/2022 00:34:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
06/18/2022 00:34:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
06/18/2022 00:34:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=210
06/18/2022 00:34:44 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7991906733367834 on epoch=210
06/18/2022 00:34:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=211
06/18/2022 00:34:50 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
06/18/2022 00:34:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=212
06/18/2022 00:34:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=213
06/18/2022 00:34:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=214
06/18/2022 00:34:59 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 00:34:59 - INFO - __main__ - Printing 3 examples
06/18/2022 00:34:59 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
06/18/2022 00:34:59 - INFO - __main__ - ['Animal']
06/18/2022 00:34:59 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
06/18/2022 00:34:59 - INFO - __main__ - ['Animal']
06/18/2022 00:34:59 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
06/18/2022 00:34:59 - INFO - __main__ - ['Animal']
06/18/2022 00:34:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 00:34:59 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:34:59 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 00:34:59 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 00:34:59 - INFO - __main__ - Printing 3 examples
06/18/2022 00:34:59 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
06/18/2022 00:34:59 - INFO - __main__ - ['Animal']
06/18/2022 00:34:59 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
06/18/2022 00:34:59 - INFO - __main__ - ['Animal']
06/18/2022 00:34:59 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
06/18/2022 00:34:59 - INFO - __main__ - ['Animal']
06/18/2022 00:34:59 - INFO - __main__ - Tokenizing Input ...
06/18/2022 00:34:59 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:35:00 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 00:35:04 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.7953493728483048 on epoch=214
06/18/2022 00:35:04 - INFO - __main__ - save last model!
06/18/2022 00:35:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 00:35:04 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 00:35:04 - INFO - __main__ - Printing 3 examples
06/18/2022 00:35:04 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 00:35:04 - INFO - __main__ - ['Animal']
06/18/2022 00:35:04 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 00:35:04 - INFO - __main__ - ['Animal']
06/18/2022 00:35:04 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 00:35:04 - INFO - __main__ - ['Village']
06/18/2022 00:35:04 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 00:35:06 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:35:09 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 00:35:18 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 00:35:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 00:35:19 - INFO - __main__ - Starting training!
06/18/2022 00:37:28 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_13_0.3_8_predictions.txt
06/18/2022 00:37:28 - INFO - __main__ - Classification-F1 on test data: 0.5946
06/18/2022 00:37:28 - INFO - __main__ - prefix=dbpedia_14_16_13, lr=0.3, bsz=8, dev_performance=0.9101938742261323, test_performance=0.5945963422921524
06/18/2022 00:37:28 - INFO - __main__ - Running ... prefix=dbpedia_14_16_13, lr=0.2, bsz=8 ...
06/18/2022 00:37:29 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 00:37:29 - INFO - __main__ - Printing 3 examples
06/18/2022 00:37:29 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
06/18/2022 00:37:29 - INFO - __main__ - ['Animal']
06/18/2022 00:37:29 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
06/18/2022 00:37:29 - INFO - __main__ - ['Animal']
06/18/2022 00:37:29 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
06/18/2022 00:37:29 - INFO - __main__ - ['Animal']
06/18/2022 00:37:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 00:37:29 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:37:29 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 00:37:29 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 00:37:29 - INFO - __main__ - Printing 3 examples
06/18/2022 00:37:29 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
06/18/2022 00:37:29 - INFO - __main__ - ['Animal']
06/18/2022 00:37:29 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
06/18/2022 00:37:29 - INFO - __main__ - ['Animal']
06/18/2022 00:37:29 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
06/18/2022 00:37:29 - INFO - __main__ - ['Animal']
06/18/2022 00:37:29 - INFO - __main__ - Tokenizing Input ...
06/18/2022 00:37:29 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:37:30 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 00:37:45 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 00:37:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 00:37:46 - INFO - __main__ - Starting training!
06/18/2022 00:37:49 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=0
06/18/2022 00:37:52 - INFO - __main__ - Step 20 Global step 20 Train loss 5.50 on epoch=1
06/18/2022 00:37:54 - INFO - __main__ - Step 30 Global step 30 Train loss 4.96 on epoch=2
06/18/2022 00:37:57 - INFO - __main__ - Step 40 Global step 40 Train loss 4.32 on epoch=2
06/18/2022 00:38:00 - INFO - __main__ - Step 50 Global step 50 Train loss 4.23 on epoch=3
06/18/2022 00:38:07 - INFO - __main__ - Global step 50 Train loss 5.18 Classification-F1 0.01925277471343448 on epoch=3
06/18/2022 00:38:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.01925277471343448 on epoch=3, global_step=50
06/18/2022 00:38:10 - INFO - __main__ - Step 60 Global step 60 Train loss 4.12 on epoch=4
06/18/2022 00:38:12 - INFO - __main__ - Step 70 Global step 70 Train loss 3.74 on epoch=4
06/18/2022 00:38:15 - INFO - __main__ - Step 80 Global step 80 Train loss 3.46 on epoch=5
06/18/2022 00:38:17 - INFO - __main__ - Step 90 Global step 90 Train loss 3.25 on epoch=6
06/18/2022 00:38:20 - INFO - __main__ - Step 100 Global step 100 Train loss 3.29 on epoch=7
06/18/2022 00:38:25 - INFO - __main__ - Global step 100 Train loss 3.57 Classification-F1 0.08264550264550265 on epoch=7
06/18/2022 00:38:25 - INFO - __main__ - Saving model with best Classification-F1: 0.01925277471343448 -> 0.08264550264550265 on epoch=7, global_step=100
06/18/2022 00:38:27 - INFO - __main__ - Step 110 Global step 110 Train loss 3.00 on epoch=7
06/18/2022 00:38:30 - INFO - __main__ - Step 120 Global step 120 Train loss 2.82 on epoch=8
06/18/2022 00:38:33 - INFO - __main__ - Step 130 Global step 130 Train loss 2.74 on epoch=9
06/18/2022 00:38:35 - INFO - __main__ - Step 140 Global step 140 Train loss 2.48 on epoch=9
06/18/2022 00:38:38 - INFO - __main__ - Step 150 Global step 150 Train loss 2.40 on epoch=10
06/18/2022 00:38:43 - INFO - __main__ - Global step 150 Train loss 2.69 Classification-F1 0.09987331892366472 on epoch=10
06/18/2022 00:38:43 - INFO - __main__ - Saving model with best Classification-F1: 0.08264550264550265 -> 0.09987331892366472 on epoch=10, global_step=150
06/18/2022 00:38:46 - INFO - __main__ - Step 160 Global step 160 Train loss 2.28 on epoch=11
06/18/2022 00:38:48 - INFO - __main__ - Step 170 Global step 170 Train loss 2.30 on epoch=12
06/18/2022 00:38:51 - INFO - __main__ - Step 180 Global step 180 Train loss 2.08 on epoch=12
06/18/2022 00:38:53 - INFO - __main__ - Step 190 Global step 190 Train loss 2.01 on epoch=13
06/18/2022 00:38:56 - INFO - __main__ - Step 200 Global step 200 Train loss 1.96 on epoch=14
06/18/2022 00:39:01 - INFO - __main__ - Global step 200 Train loss 2.13 Classification-F1 0.10846438507728831 on epoch=14
06/18/2022 00:39:01 - INFO - __main__ - Saving model with best Classification-F1: 0.09987331892366472 -> 0.10846438507728831 on epoch=14, global_step=200
06/18/2022 00:39:04 - INFO - __main__ - Step 210 Global step 210 Train loss 1.94 on epoch=14
06/18/2022 00:39:06 - INFO - __main__ - Step 220 Global step 220 Train loss 1.79 on epoch=15
06/18/2022 00:39:09 - INFO - __main__ - Step 230 Global step 230 Train loss 1.59 on epoch=16
06/18/2022 00:39:12 - INFO - __main__ - Step 240 Global step 240 Train loss 1.76 on epoch=17
06/18/2022 00:39:14 - INFO - __main__ - Step 250 Global step 250 Train loss 1.60 on epoch=17
06/18/2022 00:39:20 - INFO - __main__ - Global step 250 Train loss 1.74 Classification-F1 0.12109067411083539 on epoch=17
06/18/2022 00:39:20 - INFO - __main__ - Saving model with best Classification-F1: 0.10846438507728831 -> 0.12109067411083539 on epoch=17, global_step=250
06/18/2022 00:39:23 - INFO - __main__ - Step 260 Global step 260 Train loss 1.51 on epoch=18
06/18/2022 00:39:25 - INFO - __main__ - Step 270 Global step 270 Train loss 1.49 on epoch=19
06/18/2022 00:39:28 - INFO - __main__ - Step 280 Global step 280 Train loss 1.29 on epoch=19
06/18/2022 00:39:30 - INFO - __main__ - Step 290 Global step 290 Train loss 1.33 on epoch=20
06/18/2022 00:39:33 - INFO - __main__ - Step 300 Global step 300 Train loss 1.22 on epoch=21
06/18/2022 00:39:39 - INFO - __main__ - Global step 300 Train loss 1.37 Classification-F1 0.1951379010850813 on epoch=21
06/18/2022 00:39:39 - INFO - __main__ - Saving model with best Classification-F1: 0.12109067411083539 -> 0.1951379010850813 on epoch=21, global_step=300
06/18/2022 00:39:41 - INFO - __main__ - Step 310 Global step 310 Train loss 1.29 on epoch=22
06/18/2022 00:39:44 - INFO - __main__ - Step 320 Global step 320 Train loss 1.23 on epoch=22
06/18/2022 00:39:46 - INFO - __main__ - Step 330 Global step 330 Train loss 1.20 on epoch=23
06/18/2022 00:39:49 - INFO - __main__ - Step 340 Global step 340 Train loss 1.05 on epoch=24
06/18/2022 00:39:52 - INFO - __main__ - Step 350 Global step 350 Train loss 1.00 on epoch=24
06/18/2022 00:39:58 - INFO - __main__ - Global step 350 Train loss 1.15 Classification-F1 0.3141213358018795 on epoch=24
06/18/2022 00:39:58 - INFO - __main__ - Saving model with best Classification-F1: 0.1951379010850813 -> 0.3141213358018795 on epoch=24, global_step=350
06/18/2022 00:40:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.99 on epoch=25
06/18/2022 00:40:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.94 on epoch=26
06/18/2022 00:40:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.86 on epoch=27
06/18/2022 00:40:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.90 on epoch=27
06/18/2022 00:40:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.83 on epoch=28
06/18/2022 00:40:17 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.3719990152508375 on epoch=28
06/18/2022 00:40:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3141213358018795 -> 0.3719990152508375 on epoch=28, global_step=400
06/18/2022 00:40:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.75 on epoch=29
06/18/2022 00:40:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.79 on epoch=29
06/18/2022 00:40:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.73 on epoch=30
06/18/2022 00:40:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.65 on epoch=31
06/18/2022 00:40:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.72 on epoch=32
06/18/2022 00:40:37 - INFO - __main__ - Global step 450 Train loss 0.73 Classification-F1 0.4606336356584217 on epoch=32
06/18/2022 00:40:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3719990152508375 -> 0.4606336356584217 on epoch=32, global_step=450
06/18/2022 00:40:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=32
06/18/2022 00:40:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.59 on epoch=33
06/18/2022 00:40:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.58 on epoch=34
06/18/2022 00:40:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.62 on epoch=34
06/18/2022 00:40:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.59 on epoch=35
06/18/2022 00:40:57 - INFO - __main__ - Global step 500 Train loss 0.61 Classification-F1 0.4286579828722488 on epoch=35
06/18/2022 00:40:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.55 on epoch=36
06/18/2022 00:41:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.51 on epoch=37
06/18/2022 00:41:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.53 on epoch=37
06/18/2022 00:41:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=38
06/18/2022 00:41:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.48 on epoch=39
06/18/2022 00:41:16 - INFO - __main__ - Global step 550 Train loss 0.51 Classification-F1 0.4721622693483343 on epoch=39
06/18/2022 00:41:16 - INFO - __main__ - Saving model with best Classification-F1: 0.4606336356584217 -> 0.4721622693483343 on epoch=39, global_step=550
06/18/2022 00:41:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=39
06/18/2022 00:41:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=40
06/18/2022 00:41:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=41
06/18/2022 00:41:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=42
06/18/2022 00:41:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=42
06/18/2022 00:41:36 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.5554332853518148 on epoch=42
06/18/2022 00:41:36 - INFO - __main__ - Saving model with best Classification-F1: 0.4721622693483343 -> 0.5554332853518148 on epoch=42, global_step=600
06/18/2022 00:41:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=43
06/18/2022 00:41:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=44
06/18/2022 00:41:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=44
06/18/2022 00:41:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=45
06/18/2022 00:41:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=46
06/18/2022 00:41:55 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.5102248934900343 on epoch=46
06/18/2022 00:41:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=47
06/18/2022 00:42:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=47
06/18/2022 00:42:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=48
06/18/2022 00:42:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=49
06/18/2022 00:42:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=49
06/18/2022 00:42:15 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.5558820117104575 on epoch=49
06/18/2022 00:42:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5554332853518148 -> 0.5558820117104575 on epoch=49, global_step=700
06/18/2022 00:42:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=50
06/18/2022 00:42:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=51
06/18/2022 00:42:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=52
06/18/2022 00:42:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=52
06/18/2022 00:42:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=53
06/18/2022 00:42:34 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.6248982431845335 on epoch=53
06/18/2022 00:42:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5558820117104575 -> 0.6248982431845335 on epoch=53, global_step=750
06/18/2022 00:42:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=54
06/18/2022 00:42:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.31 on epoch=54
06/18/2022 00:42:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=55
06/18/2022 00:42:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=56
06/18/2022 00:42:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=57
06/18/2022 00:42:53 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.5720102417133208 on epoch=57
06/18/2022 00:42:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=57
06/18/2022 00:42:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=58
06/18/2022 00:43:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.28 on epoch=59
06/18/2022 00:43:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=59
06/18/2022 00:43:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.16 on epoch=60
06/18/2022 00:43:13 - INFO - __main__ - Global step 850 Train loss 0.23 Classification-F1 0.6308241691104595 on epoch=60
06/18/2022 00:43:13 - INFO - __main__ - Saving model with best Classification-F1: 0.6248982431845335 -> 0.6308241691104595 on epoch=60, global_step=850
06/18/2022 00:43:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=61
06/18/2022 00:43:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=62
06/18/2022 00:43:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=62
06/18/2022 00:43:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=63
06/18/2022 00:43:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=64
06/18/2022 00:43:32 - INFO - __main__ - Global step 900 Train loss 0.24 Classification-F1 0.6273833088954056 on epoch=64
06/18/2022 00:43:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=64
06/18/2022 00:43:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=65
06/18/2022 00:43:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=66
06/18/2022 00:43:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=67
06/18/2022 00:43:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=67
06/18/2022 00:43:52 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.7571017688887988 on epoch=67
06/18/2022 00:43:52 - INFO - __main__ - Saving model with best Classification-F1: 0.6308241691104595 -> 0.7571017688887988 on epoch=67, global_step=950
06/18/2022 00:43:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=68
06/18/2022 00:43:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=69
06/18/2022 00:43:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.16 on epoch=69
06/18/2022 00:44:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=70
06/18/2022 00:44:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.17 on epoch=71
06/18/2022 00:44:11 - INFO - __main__ - Global step 1000 Train loss 0.18 Classification-F1 0.6717709206349405 on epoch=71
06/18/2022 00:44:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=72
06/18/2022 00:44:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.17 on epoch=72
06/18/2022 00:44:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=73
06/18/2022 00:44:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=74
06/18/2022 00:44:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=74
06/18/2022 00:44:31 - INFO - __main__ - Global step 1050 Train loss 0.17 Classification-F1 0.6723905065873438 on epoch=74
06/18/2022 00:44:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=75
06/18/2022 00:44:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.15 on epoch=76
06/18/2022 00:44:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=77
06/18/2022 00:44:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.15 on epoch=77
06/18/2022 00:44:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.15 on epoch=78
06/18/2022 00:44:50 - INFO - __main__ - Global step 1100 Train loss 0.16 Classification-F1 0.7294916065604904 on epoch=78
06/18/2022 00:44:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=79
06/18/2022 00:44:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=79
06/18/2022 00:44:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=80
06/18/2022 00:45:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.10 on epoch=81
06/18/2022 00:45:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=82
06/18/2022 00:45:09 - INFO - __main__ - Global step 1150 Train loss 0.12 Classification-F1 0.7620641966077206 on epoch=82
06/18/2022 00:45:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7571017688887988 -> 0.7620641966077206 on epoch=82, global_step=1150
06/18/2022 00:45:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=82
06/18/2022 00:45:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=83
06/18/2022 00:45:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=84
06/18/2022 00:45:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=84
06/18/2022 00:45:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=85
06/18/2022 00:45:29 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.7331195014662756 on epoch=85
06/18/2022 00:45:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=86
06/18/2022 00:45:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=87
06/18/2022 00:45:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=87
06/18/2022 00:45:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=88
06/18/2022 00:45:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=89
06/18/2022 00:45:48 - INFO - __main__ - Global step 1250 Train loss 0.14 Classification-F1 0.7762441780231154 on epoch=89
06/18/2022 00:45:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7620641966077206 -> 0.7762441780231154 on epoch=89, global_step=1250
06/18/2022 00:45:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=89
06/18/2022 00:45:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=90
06/18/2022 00:45:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=91
06/18/2022 00:45:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=92
06/18/2022 00:46:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=92
06/18/2022 00:46:07 - INFO - __main__ - Global step 1300 Train loss 0.12 Classification-F1 0.7721698210075819 on epoch=92
06/18/2022 00:46:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=93
06/18/2022 00:46:12 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=94
06/18/2022 00:46:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=94
06/18/2022 00:46:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=95
06/18/2022 00:46:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.17 on epoch=96
06/18/2022 00:46:26 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.8170775293255133 on epoch=96
06/18/2022 00:46:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7762441780231154 -> 0.8170775293255133 on epoch=96, global_step=1350
06/18/2022 00:46:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=97
06/18/2022 00:46:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=97
06/18/2022 00:46:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=98
06/18/2022 00:46:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=99
06/18/2022 00:46:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=99
06/18/2022 00:46:45 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.7903181243171755 on epoch=99
06/18/2022 00:46:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=100
06/18/2022 00:46:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=101
06/18/2022 00:46:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=102
06/18/2022 00:46:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=102
06/18/2022 00:46:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=103
06/18/2022 00:47:05 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.8287120083195206 on epoch=103
06/18/2022 00:47:05 - INFO - __main__ - Saving model with best Classification-F1: 0.8170775293255133 -> 0.8287120083195206 on epoch=103, global_step=1450
06/18/2022 00:47:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=104
06/18/2022 00:47:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=104
06/18/2022 00:47:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=105
06/18/2022 00:47:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=106
06/18/2022 00:47:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=107
06/18/2022 00:47:24 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.8378386519767569 on epoch=107
06/18/2022 00:47:24 - INFO - __main__ - Saving model with best Classification-F1: 0.8287120083195206 -> 0.8378386519767569 on epoch=107, global_step=1500
06/18/2022 00:47:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=107
06/18/2022 00:47:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.10 on epoch=108
06/18/2022 00:47:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=109
06/18/2022 00:47:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=109
06/18/2022 00:47:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=110
06/18/2022 00:47:43 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.8339125461605301 on epoch=110
06/18/2022 00:47:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=111
06/18/2022 00:47:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=112
06/18/2022 00:47:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=112
06/18/2022 00:47:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=113
06/18/2022 00:47:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=114
06/18/2022 00:48:02 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.8402968038894191 on epoch=114
06/18/2022 00:48:02 - INFO - __main__ - Saving model with best Classification-F1: 0.8378386519767569 -> 0.8402968038894191 on epoch=114, global_step=1600
06/18/2022 00:48:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=114
06/18/2022 00:48:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=115
06/18/2022 00:48:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=116
06/18/2022 00:48:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=117
06/18/2022 00:48:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=117
06/18/2022 00:48:21 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.851290628054741 on epoch=117
06/18/2022 00:48:21 - INFO - __main__ - Saving model with best Classification-F1: 0.8402968038894191 -> 0.851290628054741 on epoch=117, global_step=1650
06/18/2022 00:48:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=118
06/18/2022 00:48:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=119
06/18/2022 00:48:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=119
06/18/2022 00:48:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=120
06/18/2022 00:48:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=121
06/18/2022 00:48:40 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.8427598580766111 on epoch=121
06/18/2022 00:48:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=122
06/18/2022 00:48:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=122
06/18/2022 00:48:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=123
06/18/2022 00:48:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=124
06/18/2022 00:48:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=124
06/18/2022 00:48:59 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.8375130742155684 on epoch=124
06/18/2022 00:49:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=125
06/18/2022 00:49:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=126
06/18/2022 00:49:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=127
06/18/2022 00:49:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=127
06/18/2022 00:49:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=128
06/18/2022 00:49:18 - INFO - __main__ - Global step 1800 Train loss 0.09 Classification-F1 0.8402968038894191 on epoch=128
06/18/2022 00:49:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=129
06/18/2022 00:49:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=129
06/18/2022 00:49:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=130
06/18/2022 00:49:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=131
06/18/2022 00:49:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=132
06/18/2022 00:49:37 - INFO - __main__ - Global step 1850 Train loss 0.07 Classification-F1 0.8511154962857325 on epoch=132
06/18/2022 00:49:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=132
06/18/2022 00:49:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=133
06/18/2022 00:49:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=134
06/18/2022 00:49:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=134
06/18/2022 00:49:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=135
06/18/2022 00:49:56 - INFO - __main__ - Global step 1900 Train loss 0.10 Classification-F1 0.8470710192573845 on epoch=135
06/18/2022 00:49:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=136
06/18/2022 00:50:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=137
06/18/2022 00:50:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=137
06/18/2022 00:50:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=138
06/18/2022 00:50:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=139
06/18/2022 00:50:15 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.8442030538894191 on epoch=139
06/18/2022 00:50:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=139
06/18/2022 00:50:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=140
06/18/2022 00:50:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=141
06/18/2022 00:50:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=142
06/18/2022 00:50:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=142
06/18/2022 00:50:34 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.8427598580766111 on epoch=142
06/18/2022 00:50:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=143
06/18/2022 00:50:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=144
06/18/2022 00:50:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=144
06/18/2022 00:50:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=145
06/18/2022 00:50:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=146
06/18/2022 00:50:54 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.8427598580766111 on epoch=146
06/18/2022 00:50:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=147
06/18/2022 00:50:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=147
06/18/2022 00:51:02 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=148
06/18/2022 00:51:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=149
06/18/2022 00:51:07 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=149
06/18/2022 00:51:13 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.8402968038894191 on epoch=149
06/18/2022 00:51:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=150
06/18/2022 00:51:18 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.09 on epoch=151
06/18/2022 00:51:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=152
06/18/2022 00:51:23 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=152
06/18/2022 00:51:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=153
06/18/2022 00:51:32 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.7908675801312179 on epoch=153
06/18/2022 00:51:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=154
06/18/2022 00:51:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=154
06/18/2022 00:51:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=155
06/18/2022 00:51:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=156
06/18/2022 00:51:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=157
06/18/2022 00:51:51 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.7893791821630713 on epoch=157
06/18/2022 00:51:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=157
06/18/2022 00:51:56 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=158
06/18/2022 00:51:59 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=159
06/18/2022 00:52:01 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=159
06/18/2022 00:52:04 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=160
06/18/2022 00:52:10 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.8450587086513238 on epoch=160
06/18/2022 00:52:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=161
06/18/2022 00:52:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=162
06/18/2022 00:52:18 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=162
06/18/2022 00:52:20 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=163
06/18/2022 00:52:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=164
06/18/2022 00:52:29 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7953493728483049 on epoch=164
06/18/2022 00:52:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=164
06/18/2022 00:52:34 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=165
06/18/2022 00:52:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=166
06/18/2022 00:52:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=167
06/18/2022 00:52:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=167
06/18/2022 00:52:48 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.8427598580766111 on epoch=167
06/18/2022 00:52:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=168
06/18/2022 00:52:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=169
06/18/2022 00:52:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=169
06/18/2022 00:52:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=170
06/18/2022 00:53:01 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=171
06/18/2022 00:53:07 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.8511154962857325 on epoch=171
06/18/2022 00:53:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=172
06/18/2022 00:53:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=172
06/18/2022 00:53:15 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=173
06/18/2022 00:53:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=174
06/18/2022 00:53:20 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=174
06/18/2022 00:53:26 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.9058404003391897 on epoch=174
06/18/2022 00:53:26 - INFO - __main__ - Saving model with best Classification-F1: 0.851290628054741 -> 0.9058404003391897 on epoch=174, global_step=2450
06/18/2022 00:53:29 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=175
06/18/2022 00:53:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=176
06/18/2022 00:53:34 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=177
06/18/2022 00:53:37 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=177
06/18/2022 00:53:39 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=178
06/18/2022 00:53:46 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.8470710192573845 on epoch=178
06/18/2022 00:53:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=179
06/18/2022 00:53:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=179
06/18/2022 00:53:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=180
06/18/2022 00:53:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=181
06/18/2022 00:53:59 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=182
06/18/2022 00:54:05 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.8427598580766111 on epoch=182
06/18/2022 00:54:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=182
06/18/2022 00:54:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=183
06/18/2022 00:54:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=184
06/18/2022 00:54:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=184
06/18/2022 00:54:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=185
06/18/2022 00:54:23 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.7954758750620935 on epoch=185
06/18/2022 00:54:26 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=186
06/18/2022 00:54:29 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=187
06/18/2022 00:54:31 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=187
06/18/2022 00:54:34 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=188
06/18/2022 00:54:36 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=189
06/18/2022 00:54:42 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.7954758750620935 on epoch=189
06/18/2022 00:54:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=189
06/18/2022 00:54:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=190
06/18/2022 00:54:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=191
06/18/2022 00:54:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=192
06/18/2022 00:54:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=192
06/18/2022 00:55:01 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.7934518396414736 on epoch=192
06/18/2022 00:55:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=193
06/18/2022 00:55:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
06/18/2022 00:55:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=194
06/18/2022 00:55:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=195
06/18/2022 00:55:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=196
06/18/2022 00:55:20 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.9100070670058564 on epoch=196
06/18/2022 00:55:20 - INFO - __main__ - Saving model with best Classification-F1: 0.9058404003391897 -> 0.9100070670058564 on epoch=196, global_step=2750
06/18/2022 00:55:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=197
06/18/2022 00:55:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=197
06/18/2022 00:55:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=198
06/18/2022 00:55:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=199
06/18/2022 00:55:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=199
06/18/2022 00:55:39 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.9100070670058564 on epoch=199
06/18/2022 00:55:42 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=200
06/18/2022 00:55:44 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=201
06/18/2022 00:55:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=202
06/18/2022 00:55:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
06/18/2022 00:55:52 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=203
06/18/2022 00:55:58 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.9100070670058567 on epoch=203
06/18/2022 00:55:58 - INFO - __main__ - Saving model with best Classification-F1: 0.9100070670058564 -> 0.9100070670058567 on epoch=203, global_step=2850
06/18/2022 00:56:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=204
06/18/2022 00:56:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=204
06/18/2022 00:56:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=205
06/18/2022 00:56:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=206
06/18/2022 00:56:11 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=207
06/18/2022 00:56:17 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.8471893904695056 on epoch=207
06/18/2022 00:56:20 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=207
06/18/2022 00:56:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=208
06/18/2022 00:56:25 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=209
06/18/2022 00:56:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=209
06/18/2022 00:56:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=210
06/18/2022 00:56:36 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.8471893904695056 on epoch=210
06/18/2022 00:56:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=211
06/18/2022 00:56:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=212
06/18/2022 00:56:44 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=212
06/18/2022 00:56:47 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=213
06/18/2022 00:56:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=214
06/18/2022 00:56:51 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 00:56:51 - INFO - __main__ - Printing 3 examples
06/18/2022 00:56:51 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
06/18/2022 00:56:51 - INFO - __main__ - ['Plant']
06/18/2022 00:56:51 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
06/18/2022 00:56:51 - INFO - __main__ - ['Plant']
06/18/2022 00:56:51 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
06/18/2022 00:56:51 - INFO - __main__ - ['Plant']
06/18/2022 00:56:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 00:56:51 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:56:51 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 00:56:51 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 00:56:51 - INFO - __main__ - Printing 3 examples
06/18/2022 00:56:51 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
06/18/2022 00:56:51 - INFO - __main__ - ['Plant']
06/18/2022 00:56:51 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
06/18/2022 00:56:51 - INFO - __main__ - ['Plant']
06/18/2022 00:56:51 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
06/18/2022 00:56:51 - INFO - __main__ - ['Plant']
06/18/2022 00:56:51 - INFO - __main__ - Tokenizing Input ...
06/18/2022 00:56:51 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:56:52 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 00:56:56 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.8432831404695057 on epoch=214
06/18/2022 00:56:56 - INFO - __main__ - save last model!
06/18/2022 00:56:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 00:56:56 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 00:56:56 - INFO - __main__ - Printing 3 examples
06/18/2022 00:56:56 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 00:56:56 - INFO - __main__ - ['Animal']
06/18/2022 00:56:56 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 00:56:56 - INFO - __main__ - ['Animal']
06/18/2022 00:56:56 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 00:56:56 - INFO - __main__ - ['Village']
06/18/2022 00:56:56 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 00:56:57 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:57:01 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 00:57:07 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 00:57:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 00:57:08 - INFO - __main__ - Starting training!
06/18/2022 00:59:04 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_13_0.2_8_predictions.txt
06/18/2022 00:59:04 - INFO - __main__ - Classification-F1 on test data: 0.6538
06/18/2022 00:59:05 - INFO - __main__ - prefix=dbpedia_14_16_13, lr=0.2, bsz=8, dev_performance=0.9100070670058567, test_performance=0.6538348790807644
06/18/2022 00:59:05 - INFO - __main__ - Running ... prefix=dbpedia_14_16_21, lr=0.5, bsz=8 ...
06/18/2022 00:59:06 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 00:59:06 - INFO - __main__ - Printing 3 examples
06/18/2022 00:59:06 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
06/18/2022 00:59:06 - INFO - __main__ - ['Plant']
06/18/2022 00:59:06 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
06/18/2022 00:59:06 - INFO - __main__ - ['Plant']
06/18/2022 00:59:06 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
06/18/2022 00:59:06 - INFO - __main__ - ['Plant']
06/18/2022 00:59:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 00:59:06 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:59:06 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 00:59:06 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 00:59:06 - INFO - __main__ - Printing 3 examples
06/18/2022 00:59:06 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
06/18/2022 00:59:06 - INFO - __main__ - ['Plant']
06/18/2022 00:59:06 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
06/18/2022 00:59:06 - INFO - __main__ - ['Plant']
06/18/2022 00:59:06 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
06/18/2022 00:59:06 - INFO - __main__ - ['Plant']
06/18/2022 00:59:06 - INFO - __main__ - Tokenizing Input ...
06/18/2022 00:59:06 - INFO - __main__ - Tokenizing Output ...
06/18/2022 00:59:06 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 00:59:21 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 00:59:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 00:59:22 - INFO - __main__ - Starting training!
06/18/2022 00:59:26 - INFO - __main__ - Step 10 Global step 10 Train loss 5.94 on epoch=0
06/18/2022 00:59:28 - INFO - __main__ - Step 20 Global step 20 Train loss 4.36 on epoch=1
06/18/2022 00:59:31 - INFO - __main__ - Step 30 Global step 30 Train loss 3.84 on epoch=2
06/18/2022 00:59:33 - INFO - __main__ - Step 40 Global step 40 Train loss 3.31 on epoch=2
06/18/2022 00:59:36 - INFO - __main__ - Step 50 Global step 50 Train loss 3.05 on epoch=3
06/18/2022 00:59:42 - INFO - __main__ - Global step 50 Train loss 4.10 Classification-F1 0.08273929772308103 on epoch=3
06/18/2022 00:59:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.08273929772308103 on epoch=3, global_step=50
06/18/2022 00:59:44 - INFO - __main__ - Step 60 Global step 60 Train loss 2.61 on epoch=4
06/18/2022 00:59:47 - INFO - __main__ - Step 70 Global step 70 Train loss 2.33 on epoch=4
06/18/2022 00:59:50 - INFO - __main__ - Step 80 Global step 80 Train loss 1.96 on epoch=5
06/18/2022 00:59:52 - INFO - __main__ - Step 90 Global step 90 Train loss 1.83 on epoch=6
06/18/2022 00:59:55 - INFO - __main__ - Step 100 Global step 100 Train loss 1.70 on epoch=7
06/18/2022 01:00:00 - INFO - __main__ - Global step 100 Train loss 2.09 Classification-F1 0.12160545901312478 on epoch=7
06/18/2022 01:00:00 - INFO - __main__ - Saving model with best Classification-F1: 0.08273929772308103 -> 0.12160545901312478 on epoch=7, global_step=100
06/18/2022 01:00:03 - INFO - __main__ - Step 110 Global step 110 Train loss 1.34 on epoch=7
06/18/2022 01:00:05 - INFO - __main__ - Step 120 Global step 120 Train loss 1.50 on epoch=8
06/18/2022 01:00:08 - INFO - __main__ - Step 130 Global step 130 Train loss 1.26 on epoch=9
06/18/2022 01:00:11 - INFO - __main__ - Step 140 Global step 140 Train loss 1.20 on epoch=9
06/18/2022 01:00:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.99 on epoch=10
06/18/2022 01:00:19 - INFO - __main__ - Global step 150 Train loss 1.26 Classification-F1 0.2578836611731348 on epoch=10
06/18/2022 01:00:19 - INFO - __main__ - Saving model with best Classification-F1: 0.12160545901312478 -> 0.2578836611731348 on epoch=10, global_step=150
06/18/2022 01:00:22 - INFO - __main__ - Step 160 Global step 160 Train loss 1.00 on epoch=11
06/18/2022 01:00:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.77 on epoch=12
06/18/2022 01:00:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.68 on epoch=12
06/18/2022 01:00:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.74 on epoch=13
06/18/2022 01:00:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.68 on epoch=14
06/18/2022 01:00:39 - INFO - __main__ - Global step 200 Train loss 0.77 Classification-F1 0.46370690289990124 on epoch=14
06/18/2022 01:00:39 - INFO - __main__ - Saving model with best Classification-F1: 0.2578836611731348 -> 0.46370690289990124 on epoch=14, global_step=200
06/18/2022 01:00:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=14
06/18/2022 01:00:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.58 on epoch=15
06/18/2022 01:00:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=16
06/18/2022 01:00:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=17
06/18/2022 01:00:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=17
06/18/2022 01:00:58 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.4373717572780958 on epoch=17
06/18/2022 01:01:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=18
06/18/2022 01:01:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.34 on epoch=19
06/18/2022 01:01:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.34 on epoch=19
06/18/2022 01:01:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=20
06/18/2022 01:01:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=21
06/18/2022 01:01:18 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.5171056568125616 on epoch=21
06/18/2022 01:01:18 - INFO - __main__ - Saving model with best Classification-F1: 0.46370690289990124 -> 0.5171056568125616 on epoch=21, global_step=300
06/18/2022 01:01:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=22
06/18/2022 01:01:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=22
06/18/2022 01:01:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=23
06/18/2022 01:01:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=24
06/18/2022 01:01:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=24
06/18/2022 01:01:38 - INFO - __main__ - Global step 350 Train loss 0.28 Classification-F1 0.5534638257031567 on epoch=24
06/18/2022 01:01:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5171056568125616 -> 0.5534638257031567 on epoch=24, global_step=350
06/18/2022 01:01:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=25
06/18/2022 01:01:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=26
06/18/2022 01:01:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.18 on epoch=27
06/18/2022 01:01:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=27
06/18/2022 01:01:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=28
06/18/2022 01:01:58 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.523745983397741 on epoch=28
06/18/2022 01:02:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=29
06/18/2022 01:02:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.17 on epoch=29
06/18/2022 01:02:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.19 on epoch=30
06/18/2022 01:02:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.16 on epoch=31
06/18/2022 01:02:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=32
06/18/2022 01:02:17 - INFO - __main__ - Global step 450 Train loss 0.18 Classification-F1 0.5764873158007539 on epoch=32
06/18/2022 01:02:17 - INFO - __main__ - Saving model with best Classification-F1: 0.5534638257031567 -> 0.5764873158007539 on epoch=32, global_step=450
06/18/2022 01:02:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.19 on epoch=32
06/18/2022 01:02:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=33
06/18/2022 01:02:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=34
06/18/2022 01:02:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.16 on epoch=34
06/18/2022 01:02:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=35
06/18/2022 01:02:37 - INFO - __main__ - Global step 500 Train loss 0.17 Classification-F1 0.6051851127188103 on epoch=35
06/18/2022 01:02:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5764873158007539 -> 0.6051851127188103 on epoch=35, global_step=500
06/18/2022 01:02:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=36
06/18/2022 01:02:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=37
06/18/2022 01:02:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=37
06/18/2022 01:02:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.15 on epoch=38
06/18/2022 01:02:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=39
06/18/2022 01:02:56 - INFO - __main__ - Global step 550 Train loss 0.15 Classification-F1 0.5447750023125405 on epoch=39
06/18/2022 01:02:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.09 on epoch=39
06/18/2022 01:03:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.15 on epoch=40
06/18/2022 01:03:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.14 on epoch=41
06/18/2022 01:03:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.09 on epoch=42
06/18/2022 01:03:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=42
06/18/2022 01:03:16 - INFO - __main__ - Global step 600 Train loss 0.12 Classification-F1 0.5510152990177327 on epoch=42
06/18/2022 01:03:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=43
06/18/2022 01:03:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=44
06/18/2022 01:03:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=44
06/18/2022 01:03:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.09 on epoch=45
06/18/2022 01:03:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=46
06/18/2022 01:03:35 - INFO - __main__ - Global step 650 Train loss 0.12 Classification-F1 0.608098332197004 on epoch=46
06/18/2022 01:03:35 - INFO - __main__ - Saving model with best Classification-F1: 0.6051851127188103 -> 0.608098332197004 on epoch=46, global_step=650
06/18/2022 01:03:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=47
06/18/2022 01:03:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.14 on epoch=47
06/18/2022 01:03:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=48
06/18/2022 01:03:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=49
06/18/2022 01:03:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=49
06/18/2022 01:03:54 - INFO - __main__ - Global step 700 Train loss 0.11 Classification-F1 0.5280742074444637 on epoch=49
06/18/2022 01:03:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=50
06/18/2022 01:03:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=51
06/18/2022 01:04:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=52
06/18/2022 01:04:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=52
06/18/2022 01:04:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=53
06/18/2022 01:04:14 - INFO - __main__ - Global step 750 Train loss 0.10 Classification-F1 0.666285092006654 on epoch=53
06/18/2022 01:04:14 - INFO - __main__ - Saving model with best Classification-F1: 0.608098332197004 -> 0.666285092006654 on epoch=53, global_step=750
06/18/2022 01:04:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.12 on epoch=54
06/18/2022 01:04:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=54
06/18/2022 01:04:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=55
06/18/2022 01:04:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=56
06/18/2022 01:04:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=57
06/18/2022 01:04:33 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.6235601572007102 on epoch=57
06/18/2022 01:04:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=57
06/18/2022 01:04:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=58
06/18/2022 01:04:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=59
06/18/2022 01:04:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=59
06/18/2022 01:04:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=60
06/18/2022 01:04:52 - INFO - __main__ - Global step 850 Train loss 0.09 Classification-F1 0.6438562898534934 on epoch=60
06/18/2022 01:04:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=61
06/18/2022 01:04:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=62
06/18/2022 01:05:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=62
06/18/2022 01:05:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=63
06/18/2022 01:05:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=64
06/18/2022 01:05:11 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.7253624160645035 on epoch=64
06/18/2022 01:05:11 - INFO - __main__ - Saving model with best Classification-F1: 0.666285092006654 -> 0.7253624160645035 on epoch=64, global_step=900
06/18/2022 01:05:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=64
06/18/2022 01:05:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=65
06/18/2022 01:05:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=66
06/18/2022 01:05:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=67
06/18/2022 01:05:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=67
06/18/2022 01:05:31 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.7447182353083155 on epoch=67
06/18/2022 01:05:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7253624160645035 -> 0.7447182353083155 on epoch=67, global_step=950
06/18/2022 01:05:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=68
06/18/2022 01:05:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.14 on epoch=69
06/18/2022 01:05:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=69
06/18/2022 01:05:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=70
06/18/2022 01:05:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.12 on epoch=71
06/18/2022 01:05:50 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.7334986521986357 on epoch=71
06/18/2022 01:05:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=72
06/18/2022 01:05:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=72
06/18/2022 01:05:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=73
06/18/2022 01:06:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=74
06/18/2022 01:06:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=74
06/18/2022 01:06:09 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.6901591567326113 on epoch=74
06/18/2022 01:06:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=75
06/18/2022 01:06:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=76
06/18/2022 01:06:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=77
06/18/2022 01:06:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=77
06/18/2022 01:06:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=78
06/18/2022 01:06:28 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.6338719260920399 on epoch=78
06/18/2022 01:06:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=79
06/18/2022 01:06:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=79
06/18/2022 01:06:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=80
06/18/2022 01:06:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=81
06/18/2022 01:06:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=82
06/18/2022 01:06:47 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.6901591567326113 on epoch=82
06/18/2022 01:06:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=82
06/18/2022 01:06:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=83
06/18/2022 01:06:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=84
06/18/2022 01:06:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=84
06/18/2022 01:07:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=85
06/18/2022 01:07:06 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.6767580773383757 on epoch=85
06/18/2022 01:07:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=86
06/18/2022 01:07:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=87
06/18/2022 01:07:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=87
06/18/2022 01:07:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=88
06/18/2022 01:07:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=89
06/18/2022 01:07:25 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.7706405576383302 on epoch=89
06/18/2022 01:07:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7447182353083155 -> 0.7706405576383302 on epoch=89, global_step=1250
06/18/2022 01:07:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=89
06/18/2022 01:07:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=90
06/18/2022 01:07:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=91
06/18/2022 01:07:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=92
06/18/2022 01:07:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=92
06/18/2022 01:07:45 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.6845022682633789 on epoch=92
06/18/2022 01:07:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=93
06/18/2022 01:07:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=94
06/18/2022 01:07:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=94
06/18/2022 01:07:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=95
06/18/2022 01:07:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=96
06/18/2022 01:08:04 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.6820942875272247 on epoch=96
06/18/2022 01:08:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=97
06/18/2022 01:08:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=97
06/18/2022 01:08:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=98
06/18/2022 01:08:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=99
06/18/2022 01:08:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=99
06/18/2022 01:08:23 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.7560245833713855 on epoch=99
06/18/2022 01:08:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=100
06/18/2022 01:08:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=101
06/18/2022 01:08:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=102
06/18/2022 01:08:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=102
06/18/2022 01:08:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=103
06/18/2022 01:08:42 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.7095033022727021 on epoch=103
06/18/2022 01:08:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=104
06/18/2022 01:08:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=104
06/18/2022 01:08:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=105
06/18/2022 01:08:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=106
06/18/2022 01:08:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=107
06/18/2022 01:09:02 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7500030170281067 on epoch=107
06/18/2022 01:09:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=107
06/18/2022 01:09:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=108
06/18/2022 01:09:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=109
06/18/2022 01:09:12 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=109
06/18/2022 01:09:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=110
06/18/2022 01:09:21 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.7394711903986096 on epoch=110
06/18/2022 01:09:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=111
06/18/2022 01:09:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=112
06/18/2022 01:09:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=112
06/18/2022 01:09:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=113
06/18/2022 01:09:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=114
06/18/2022 01:09:40 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.7449660886040815 on epoch=114
06/18/2022 01:09:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=114
06/18/2022 01:09:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=115
06/18/2022 01:09:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=116
06/18/2022 01:09:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=117
06/18/2022 01:09:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=117
06/18/2022 01:09:59 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.7949503848460205 on epoch=117
06/18/2022 01:09:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7706405576383302 -> 0.7949503848460205 on epoch=117, global_step=1650
06/18/2022 01:10:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=118
06/18/2022 01:10:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=119
06/18/2022 01:10:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=119
06/18/2022 01:10:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=120
06/18/2022 01:10:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=121
06/18/2022 01:10:18 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.7865416943439044 on epoch=121
06/18/2022 01:10:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=122
06/18/2022 01:10:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=122
06/18/2022 01:10:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=123
06/18/2022 01:10:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=124
06/18/2022 01:10:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=124
06/18/2022 01:10:38 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.8407851557045105 on epoch=124
06/18/2022 01:10:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7949503848460205 -> 0.8407851557045105 on epoch=124, global_step=1750
06/18/2022 01:10:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=125
06/18/2022 01:10:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=126
06/18/2022 01:10:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=127
06/18/2022 01:10:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=127
06/18/2022 01:10:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=128
06/18/2022 01:10:57 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.7750560634811109 on epoch=128
06/18/2022 01:10:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=129
06/18/2022 01:11:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=129
06/18/2022 01:11:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=130
06/18/2022 01:11:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=131
06/18/2022 01:11:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=132
06/18/2022 01:11:16 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.7731334762644062 on epoch=132
06/18/2022 01:11:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=132
06/18/2022 01:11:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=133
06/18/2022 01:11:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=134
06/18/2022 01:11:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=134
06/18/2022 01:11:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=135
06/18/2022 01:11:35 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7944628879861607 on epoch=135
06/18/2022 01:11:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=136
06/18/2022 01:11:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=137
06/18/2022 01:11:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=137
06/18/2022 01:11:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=138
06/18/2022 01:11:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=139
06/18/2022 01:11:54 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.8467623211307984 on epoch=139
06/18/2022 01:11:54 - INFO - __main__ - Saving model with best Classification-F1: 0.8407851557045105 -> 0.8467623211307984 on epoch=139, global_step=1950
06/18/2022 01:11:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=139
06/18/2022 01:12:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=140
06/18/2022 01:12:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=141
06/18/2022 01:12:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=142
06/18/2022 01:12:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=142
06/18/2022 01:12:13 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7149119048075404 on epoch=142
06/18/2022 01:12:16 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=143
06/18/2022 01:12:19 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=144
06/18/2022 01:12:21 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=144
06/18/2022 01:12:24 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=145
06/18/2022 01:12:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=146
06/18/2022 01:12:32 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7343447121435737 on epoch=146
06/18/2022 01:12:35 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=147
06/18/2022 01:12:38 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=147
06/18/2022 01:12:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=148
06/18/2022 01:12:43 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=149
06/18/2022 01:12:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=149
06/18/2022 01:12:51 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7074396298280249 on epoch=149
06/18/2022 01:12:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=150
06/18/2022 01:12:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=151
06/18/2022 01:12:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=152
06/18/2022 01:13:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=152
06/18/2022 01:13:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=153
06/18/2022 01:13:11 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.7766354166776012 on epoch=153
06/18/2022 01:13:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=154
06/18/2022 01:13:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=154
06/18/2022 01:13:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=155
06/18/2022 01:13:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=156
06/18/2022 01:13:24 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=157
06/18/2022 01:13:30 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7905413193587097 on epoch=157
06/18/2022 01:13:33 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=157
06/18/2022 01:13:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=158
06/18/2022 01:13:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=159
06/18/2022 01:13:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=159
06/18/2022 01:13:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=160
06/18/2022 01:13:49 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6976913377290819 on epoch=160
06/18/2022 01:13:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=161
06/18/2022 01:13:55 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=162
06/18/2022 01:13:58 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=162
06/18/2022 01:14:00 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=163
06/18/2022 01:14:03 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=164
06/18/2022 01:14:09 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.6548363274928739 on epoch=164
06/18/2022 01:14:11 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=164
06/18/2022 01:14:14 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
06/18/2022 01:14:17 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=166
06/18/2022 01:14:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=167
06/18/2022 01:14:22 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=167
06/18/2022 01:14:28 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.73291621492265 on epoch=167
06/18/2022 01:14:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=168
06/18/2022 01:14:33 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=169
06/18/2022 01:14:36 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=169
06/18/2022 01:14:38 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=170
06/18/2022 01:14:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=171
06/18/2022 01:14:47 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.6935650321249044 on epoch=171
06/18/2022 01:14:50 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=172
06/18/2022 01:14:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=172
06/18/2022 01:14:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=173
06/18/2022 01:14:58 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=174
06/18/2022 01:15:01 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=174
06/18/2022 01:15:07 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7336982286078896 on epoch=174
06/18/2022 01:15:10 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=175
06/18/2022 01:15:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=176
06/18/2022 01:15:15 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=177
06/18/2022 01:15:17 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
06/18/2022 01:15:20 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=178
06/18/2022 01:15:26 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7223321130342004 on epoch=178
06/18/2022 01:15:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=179
06/18/2022 01:15:31 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=179
06/18/2022 01:15:34 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=180
06/18/2022 01:15:36 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=181
06/18/2022 01:15:39 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=182
06/18/2022 01:15:46 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6215049069481166 on epoch=182
06/18/2022 01:15:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=182
06/18/2022 01:15:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
06/18/2022 01:15:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=184
06/18/2022 01:15:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
06/18/2022 01:15:59 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=185
06/18/2022 01:16:05 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7342234044651337 on epoch=185
06/18/2022 01:16:07 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=186
06/18/2022 01:16:10 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=187
06/18/2022 01:16:13 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
06/18/2022 01:16:15 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=188
06/18/2022 01:16:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=189
06/18/2022 01:16:24 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.708864902962576 on epoch=189
06/18/2022 01:16:27 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
06/18/2022 01:16:29 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=190
06/18/2022 01:16:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
06/18/2022 01:16:34 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
06/18/2022 01:16:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=192
06/18/2022 01:16:43 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6618223865792652 on epoch=192
06/18/2022 01:16:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=193
06/18/2022 01:16:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
06/18/2022 01:16:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
06/18/2022 01:16:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=195
06/18/2022 01:16:56 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=196
06/18/2022 01:17:03 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7431576020944566 on epoch=196
06/18/2022 01:17:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=197
06/18/2022 01:17:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=197
06/18/2022 01:17:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
06/18/2022 01:17:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=199
06/18/2022 01:17:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=199
06/18/2022 01:17:23 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7008576956017784 on epoch=199
06/18/2022 01:17:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=200
06/18/2022 01:17:28 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=201
06/18/2022 01:17:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=202
06/18/2022 01:17:33 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=202
06/18/2022 01:17:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=203
06/18/2022 01:17:43 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6916136295543067 on epoch=203
06/18/2022 01:17:45 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=204
06/18/2022 01:17:48 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=204
06/18/2022 01:17:50 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=205
06/18/2022 01:17:53 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=206
06/18/2022 01:17:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
06/18/2022 01:18:02 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6935377948061461 on epoch=207
06/18/2022 01:18:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=207
06/18/2022 01:18:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=208
06/18/2022 01:18:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=209
06/18/2022 01:18:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=209
06/18/2022 01:18:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=210
06/18/2022 01:18:22 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.5872034705051784 on epoch=210
06/18/2022 01:18:24 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=211
06/18/2022 01:18:27 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
06/18/2022 01:18:30 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=212
06/18/2022 01:18:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=213
06/18/2022 01:18:35 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=214
06/18/2022 01:18:36 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 01:18:36 - INFO - __main__ - Printing 3 examples
06/18/2022 01:18:36 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
06/18/2022 01:18:36 - INFO - __main__ - ['Plant']
06/18/2022 01:18:36 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
06/18/2022 01:18:36 - INFO - __main__ - ['Plant']
06/18/2022 01:18:36 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
06/18/2022 01:18:36 - INFO - __main__ - ['Plant']
06/18/2022 01:18:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 01:18:36 - INFO - __main__ - Tokenizing Output ...
06/18/2022 01:18:37 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 01:18:37 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 01:18:37 - INFO - __main__ - Printing 3 examples
06/18/2022 01:18:37 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
06/18/2022 01:18:37 - INFO - __main__ - ['Plant']
06/18/2022 01:18:37 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
06/18/2022 01:18:37 - INFO - __main__ - ['Plant']
06/18/2022 01:18:37 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
06/18/2022 01:18:37 - INFO - __main__ - ['Plant']
06/18/2022 01:18:37 - INFO - __main__ - Tokenizing Input ...
06/18/2022 01:18:37 - INFO - __main__ - Tokenizing Output ...
06/18/2022 01:18:37 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 01:18:42 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.658438386320265 on epoch=214
06/18/2022 01:18:42 - INFO - __main__ - save last model!
06/18/2022 01:18:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 01:18:42 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 01:18:42 - INFO - __main__ - Printing 3 examples
06/18/2022 01:18:42 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 01:18:42 - INFO - __main__ - ['Animal']
06/18/2022 01:18:42 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 01:18:42 - INFO - __main__ - ['Animal']
06/18/2022 01:18:42 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 01:18:42 - INFO - __main__ - ['Village']
06/18/2022 01:18:42 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 01:18:44 - INFO - __main__ - Tokenizing Output ...
06/18/2022 01:18:47 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 01:18:52 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 01:18:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 01:18:53 - INFO - __main__ - Starting training!
06/18/2022 01:20:50 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_21_0.5_8_predictions.txt
06/18/2022 01:20:50 - INFO - __main__ - Classification-F1 on test data: 0.4042
06/18/2022 01:20:50 - INFO - __main__ - prefix=dbpedia_14_16_21, lr=0.5, bsz=8, dev_performance=0.8467623211307984, test_performance=0.40420556704913846
06/18/2022 01:20:50 - INFO - __main__ - Running ... prefix=dbpedia_14_16_21, lr=0.4, bsz=8 ...
06/18/2022 01:20:51 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 01:20:51 - INFO - __main__ - Printing 3 examples
06/18/2022 01:20:51 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
06/18/2022 01:20:51 - INFO - __main__ - ['Plant']
06/18/2022 01:20:51 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
06/18/2022 01:20:51 - INFO - __main__ - ['Plant']
06/18/2022 01:20:51 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
06/18/2022 01:20:51 - INFO - __main__ - ['Plant']
06/18/2022 01:20:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 01:20:51 - INFO - __main__ - Tokenizing Output ...
06/18/2022 01:20:52 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 01:20:52 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 01:20:52 - INFO - __main__ - Printing 3 examples
06/18/2022 01:20:52 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
06/18/2022 01:20:52 - INFO - __main__ - ['Plant']
06/18/2022 01:20:52 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
06/18/2022 01:20:52 - INFO - __main__ - ['Plant']
06/18/2022 01:20:52 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
06/18/2022 01:20:52 - INFO - __main__ - ['Plant']
06/18/2022 01:20:52 - INFO - __main__ - Tokenizing Input ...
06/18/2022 01:20:52 - INFO - __main__ - Tokenizing Output ...
06/18/2022 01:20:52 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 01:21:07 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 01:21:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 01:21:08 - INFO - __main__ - Starting training!
06/18/2022 01:21:11 - INFO - __main__ - Step 10 Global step 10 Train loss 6.25 on epoch=0
06/18/2022 01:21:14 - INFO - __main__ - Step 20 Global step 20 Train loss 4.62 on epoch=1
06/18/2022 01:21:17 - INFO - __main__ - Step 30 Global step 30 Train loss 4.12 on epoch=2
06/18/2022 01:21:19 - INFO - __main__ - Step 40 Global step 40 Train loss 3.52 on epoch=2
06/18/2022 01:21:22 - INFO - __main__ - Step 50 Global step 50 Train loss 3.34 on epoch=3
06/18/2022 01:21:27 - INFO - __main__ - Global step 50 Train loss 4.37 Classification-F1 0.063732264974501 on epoch=3
06/18/2022 01:21:27 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.063732264974501 on epoch=3, global_step=50
06/18/2022 01:21:30 - INFO - __main__ - Step 60 Global step 60 Train loss 2.80 on epoch=4
06/18/2022 01:21:33 - INFO - __main__ - Step 70 Global step 70 Train loss 2.63 on epoch=4
06/18/2022 01:21:35 - INFO - __main__ - Step 80 Global step 80 Train loss 2.25 on epoch=5
06/18/2022 01:21:38 - INFO - __main__ - Step 90 Global step 90 Train loss 2.08 on epoch=6
06/18/2022 01:21:41 - INFO - __main__ - Step 100 Global step 100 Train loss 1.99 on epoch=7
06/18/2022 01:21:46 - INFO - __main__ - Global step 100 Train loss 2.35 Classification-F1 0.12043522881416437 on epoch=7
06/18/2022 01:21:46 - INFO - __main__ - Saving model with best Classification-F1: 0.063732264974501 -> 0.12043522881416437 on epoch=7, global_step=100
06/18/2022 01:21:49 - INFO - __main__ - Step 110 Global step 110 Train loss 1.79 on epoch=7
06/18/2022 01:21:52 - INFO - __main__ - Step 120 Global step 120 Train loss 1.70 on epoch=8
06/18/2022 01:21:54 - INFO - __main__ - Step 130 Global step 130 Train loss 1.56 on epoch=9
06/18/2022 01:21:57 - INFO - __main__ - Step 140 Global step 140 Train loss 1.38 on epoch=9
06/18/2022 01:21:59 - INFO - __main__ - Step 150 Global step 150 Train loss 1.19 on epoch=10
06/18/2022 01:22:05 - INFO - __main__ - Global step 150 Train loss 1.52 Classification-F1 0.16916957857910805 on epoch=10
06/18/2022 01:22:05 - INFO - __main__ - Saving model with best Classification-F1: 0.12043522881416437 -> 0.16916957857910805 on epoch=10, global_step=150
06/18/2022 01:22:08 - INFO - __main__ - Step 160 Global step 160 Train loss 1.27 on epoch=11
06/18/2022 01:22:10 - INFO - __main__ - Step 170 Global step 170 Train loss 1.05 on epoch=12
06/18/2022 01:22:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.97 on epoch=12
06/18/2022 01:22:15 - INFO - __main__ - Step 190 Global step 190 Train loss 1.08 on epoch=13
06/18/2022 01:22:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.78 on epoch=14
06/18/2022 01:22:25 - INFO - __main__ - Global step 200 Train loss 1.03 Classification-F1 0.2915047257091171 on epoch=14
06/18/2022 01:22:25 - INFO - __main__ - Saving model with best Classification-F1: 0.16916957857910805 -> 0.2915047257091171 on epoch=14, global_step=200
06/18/2022 01:22:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.84 on epoch=14
06/18/2022 01:22:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.77 on epoch=15
06/18/2022 01:22:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.83 on epoch=16
06/18/2022 01:22:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.65 on epoch=17
06/18/2022 01:22:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.59 on epoch=17
06/18/2022 01:22:44 - INFO - __main__ - Global step 250 Train loss 0.74 Classification-F1 0.43650126395224437 on epoch=17
06/18/2022 01:22:44 - INFO - __main__ - Saving model with best Classification-F1: 0.2915047257091171 -> 0.43650126395224437 on epoch=17, global_step=250
06/18/2022 01:22:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.56 on epoch=18
06/18/2022 01:22:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=19
06/18/2022 01:22:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=19
06/18/2022 01:22:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.58 on epoch=20
06/18/2022 01:22:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=21
06/18/2022 01:23:04 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.5035497760761077 on epoch=21
06/18/2022 01:23:04 - INFO - __main__ - Saving model with best Classification-F1: 0.43650126395224437 -> 0.5035497760761077 on epoch=21, global_step=300
06/18/2022 01:23:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=22
06/18/2022 01:23:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=22
06/18/2022 01:23:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=23
06/18/2022 01:23:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=24
06/18/2022 01:23:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=24
06/18/2022 01:23:24 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.6251385367206459 on epoch=24
06/18/2022 01:23:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5035497760761077 -> 0.6251385367206459 on epoch=24, global_step=350
06/18/2022 01:23:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=25
06/18/2022 01:23:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=26
06/18/2022 01:23:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.27 on epoch=27
06/18/2022 01:23:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=27
06/18/2022 01:23:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=28
06/18/2022 01:23:44 - INFO - __main__ - Global step 400 Train loss 0.33 Classification-F1 0.577057181169259 on epoch=28
06/18/2022 01:23:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=29
06/18/2022 01:23:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=29
06/18/2022 01:23:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=30
06/18/2022 01:23:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=31
06/18/2022 01:23:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=32
06/18/2022 01:24:03 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.6332712241961252 on epoch=32
06/18/2022 01:24:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6251385367206459 -> 0.6332712241961252 on epoch=32, global_step=450
06/18/2022 01:24:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=32
06/18/2022 01:24:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.18 on epoch=33
06/18/2022 01:24:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=34
06/18/2022 01:24:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.16 on epoch=34
06/18/2022 01:24:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.18 on epoch=35
06/18/2022 01:24:23 - INFO - __main__ - Global step 500 Train loss 0.19 Classification-F1 0.5740950767872763 on epoch=35
06/18/2022 01:24:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.18 on epoch=36
06/18/2022 01:24:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=37
06/18/2022 01:24:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=37
06/18/2022 01:24:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.15 on epoch=38
06/18/2022 01:24:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=39
06/18/2022 01:24:42 - INFO - __main__ - Global step 550 Train loss 0.19 Classification-F1 0.6702456238273038 on epoch=39
06/18/2022 01:24:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6332712241961252 -> 0.6702456238273038 on epoch=39, global_step=550
06/18/2022 01:24:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.14 on epoch=39
06/18/2022 01:24:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=40
06/18/2022 01:24:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=41
06/18/2022 01:24:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=42
06/18/2022 01:24:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=42
06/18/2022 01:25:01 - INFO - __main__ - Global step 600 Train loss 0.15 Classification-F1 0.633542223088042 on epoch=42
06/18/2022 01:25:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=43
06/18/2022 01:25:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=44
06/18/2022 01:25:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=44
06/18/2022 01:25:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=45
06/18/2022 01:25:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=46
06/18/2022 01:25:21 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.5750352955987336 on epoch=46
06/18/2022 01:25:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.09 on epoch=47
06/18/2022 01:25:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=47
06/18/2022 01:25:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=48
06/18/2022 01:25:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=49
06/18/2022 01:25:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=49
06/18/2022 01:25:40 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.6233383499015948 on epoch=49
06/18/2022 01:25:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=50
06/18/2022 01:25:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=51
06/18/2022 01:25:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=52
06/18/2022 01:25:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=52
06/18/2022 01:25:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=53
06/18/2022 01:25:59 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.5848381874667393 on epoch=53
06/18/2022 01:26:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=54
06/18/2022 01:26:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=54
06/18/2022 01:26:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.14 on epoch=55
06/18/2022 01:26:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=56
06/18/2022 01:26:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=57
06/18/2022 01:26:19 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.687258335696659 on epoch=57
06/18/2022 01:26:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6702456238273038 -> 0.687258335696659 on epoch=57, global_step=800
06/18/2022 01:26:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=57
06/18/2022 01:26:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=58
06/18/2022 01:26:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=59
06/18/2022 01:26:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=59
06/18/2022 01:26:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=60
06/18/2022 01:26:39 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.5958200875658181 on epoch=60
06/18/2022 01:26:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=61
06/18/2022 01:26:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=62
06/18/2022 01:26:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=62
06/18/2022 01:26:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=63
06/18/2022 01:26:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=64
06/18/2022 01:26:58 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.5591462719018636 on epoch=64
06/18/2022 01:27:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=64
06/18/2022 01:27:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=65
06/18/2022 01:27:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=66
06/18/2022 01:27:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=67
06/18/2022 01:27:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=67
06/18/2022 01:27:18 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.5813912057313817 on epoch=67
06/18/2022 01:27:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=68
06/18/2022 01:27:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=69
06/18/2022 01:27:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=69
06/18/2022 01:27:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=70
06/18/2022 01:27:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=71
06/18/2022 01:27:37 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.6715814769083019 on epoch=71
06/18/2022 01:27:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=72
06/18/2022 01:27:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=72
06/18/2022 01:27:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=73
06/18/2022 01:27:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=74
06/18/2022 01:27:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=74
06/18/2022 01:27:57 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.6411458603191011 on epoch=74
06/18/2022 01:28:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=75
06/18/2022 01:28:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=76
06/18/2022 01:28:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=77
06/18/2022 01:28:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=77
06/18/2022 01:28:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=78
06/18/2022 01:28:17 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.7485463503252877 on epoch=78
06/18/2022 01:28:17 - INFO - __main__ - Saving model with best Classification-F1: 0.687258335696659 -> 0.7485463503252877 on epoch=78, global_step=1100
06/18/2022 01:28:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=79
06/18/2022 01:28:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=79
06/18/2022 01:28:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=80
06/18/2022 01:28:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=81
06/18/2022 01:28:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=82
06/18/2022 01:28:36 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.7066622581842379 on epoch=82
06/18/2022 01:28:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=82
06/18/2022 01:28:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=83
06/18/2022 01:28:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=84
06/18/2022 01:28:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=84
06/18/2022 01:28:49 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=85
06/18/2022 01:28:56 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.6194822797154051 on epoch=85
06/18/2022 01:28:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=86
06/18/2022 01:29:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=87
06/18/2022 01:29:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=87
06/18/2022 01:29:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=88
06/18/2022 01:29:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=89
06/18/2022 01:29:15 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.5693329800735628 on epoch=89
06/18/2022 01:29:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=89
06/18/2022 01:29:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=90
06/18/2022 01:29:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=91
06/18/2022 01:29:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=92
06/18/2022 01:29:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=92
06/18/2022 01:29:35 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.5954765814984835 on epoch=92
06/18/2022 01:29:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=93
06/18/2022 01:29:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=94
06/18/2022 01:29:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=94
06/18/2022 01:29:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=95
06/18/2022 01:29:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=96
06/18/2022 01:29:54 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.601226474344754 on epoch=96
06/18/2022 01:29:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=97
06/18/2022 01:29:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=97
06/18/2022 01:30:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=98
06/18/2022 01:30:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=99
06/18/2022 01:30:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=99
06/18/2022 01:30:13 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.7021168919798202 on epoch=99
06/18/2022 01:30:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=100
06/18/2022 01:30:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=101
06/18/2022 01:30:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=102
06/18/2022 01:30:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=102
06/18/2022 01:30:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.11 on epoch=103
06/18/2022 01:30:33 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.7041446743152342 on epoch=103
06/18/2022 01:30:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=104
06/18/2022 01:30:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=104
06/18/2022 01:30:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=105
06/18/2022 01:30:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=106
06/18/2022 01:30:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=107
06/18/2022 01:30:52 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.656703019260475 on epoch=107
06/18/2022 01:30:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=107
06/18/2022 01:30:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=108
06/18/2022 01:31:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=109
06/18/2022 01:31:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=109
06/18/2022 01:31:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=110
06/18/2022 01:31:11 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.6962443250802868 on epoch=110
06/18/2022 01:31:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=111
06/18/2022 01:31:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=112
06/18/2022 01:31:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=112
06/18/2022 01:31:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=113
06/18/2022 01:31:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=114
06/18/2022 01:31:31 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.6567420636269066 on epoch=114
06/18/2022 01:31:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=114
06/18/2022 01:31:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=115
06/18/2022 01:31:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=116
06/18/2022 01:31:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=117
06/18/2022 01:31:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=117
06/18/2022 01:31:50 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.6853794978326112 on epoch=117
06/18/2022 01:31:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=118
06/18/2022 01:31:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=119
06/18/2022 01:31:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=119
06/18/2022 01:32:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=120
06/18/2022 01:32:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=121
06/18/2022 01:32:09 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.6814183427086653 on epoch=121
06/18/2022 01:32:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=122
06/18/2022 01:32:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=122
06/18/2022 01:32:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=123
06/18/2022 01:32:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=124
06/18/2022 01:32:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=124
06/18/2022 01:32:28 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.7329373978330336 on epoch=124
06/18/2022 01:32:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=125
06/18/2022 01:32:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=126
06/18/2022 01:32:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=127
06/18/2022 01:32:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=127
06/18/2022 01:32:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=128
06/18/2022 01:32:47 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.7010658450411772 on epoch=128
06/18/2022 01:32:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=129
06/18/2022 01:32:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=129
06/18/2022 01:32:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=130
06/18/2022 01:32:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=131
06/18/2022 01:33:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=132
06/18/2022 01:33:06 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6432772547203787 on epoch=132
06/18/2022 01:33:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=132
06/18/2022 01:33:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=133
06/18/2022 01:33:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=134
06/18/2022 01:33:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=134
06/18/2022 01:33:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=135
06/18/2022 01:33:26 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.7074437883512102 on epoch=135
06/18/2022 01:33:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=136
06/18/2022 01:33:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=137
06/18/2022 01:33:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=137
06/18/2022 01:33:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=138
06/18/2022 01:33:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=139
06/18/2022 01:33:45 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.7399719936275356 on epoch=139
06/18/2022 01:33:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=139
06/18/2022 01:33:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=140
06/18/2022 01:33:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=141
06/18/2022 01:33:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=142
06/18/2022 01:33:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=142
06/18/2022 01:34:04 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.7240135536506505 on epoch=142
06/18/2022 01:34:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=143
06/18/2022 01:34:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=144
06/18/2022 01:34:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=144
06/18/2022 01:34:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=145
06/18/2022 01:34:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=146
06/18/2022 01:34:23 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.7558391745940921 on epoch=146
06/18/2022 01:34:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7485463503252877 -> 0.7558391745940921 on epoch=146, global_step=2050
06/18/2022 01:34:26 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=147
06/18/2022 01:34:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=147
06/18/2022 01:34:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=148
06/18/2022 01:34:34 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=149
06/18/2022 01:34:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=149
06/18/2022 01:34:43 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7365297849168818 on epoch=149
06/18/2022 01:34:46 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=150
06/18/2022 01:34:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=151
06/18/2022 01:34:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=152
06/18/2022 01:34:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=152
06/18/2022 01:34:57 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=153
06/18/2022 01:35:03 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.7171617457670588 on epoch=153
06/18/2022 01:35:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=154
06/18/2022 01:35:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=154
06/18/2022 01:35:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=155
06/18/2022 01:35:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=156
06/18/2022 01:35:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=157
06/18/2022 01:35:23 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.6089472975720853 on epoch=157
06/18/2022 01:35:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=157
06/18/2022 01:35:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=158
06/18/2022 01:35:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=159
06/18/2022 01:35:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=159
06/18/2022 01:35:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=160
06/18/2022 01:35:42 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7189857931793416 on epoch=160
06/18/2022 01:35:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=161
06/18/2022 01:35:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=162
06/18/2022 01:35:50 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=162
06/18/2022 01:35:53 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=163
06/18/2022 01:35:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=164
06/18/2022 01:36:02 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6990435010244258 on epoch=164
06/18/2022 01:36:05 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=164
06/18/2022 01:36:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=165
06/18/2022 01:36:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=166
06/18/2022 01:36:13 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=167
06/18/2022 01:36:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=167
06/18/2022 01:36:22 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7027319030714616 on epoch=167
06/18/2022 01:36:25 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=168
06/18/2022 01:36:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=169
06/18/2022 01:36:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=169
06/18/2022 01:36:33 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=170
06/18/2022 01:36:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=171
06/18/2022 01:36:42 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.9052292715717765 on epoch=171
06/18/2022 01:36:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7558391745940921 -> 0.9052292715717765 on epoch=171, global_step=2400
06/18/2022 01:36:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=172
06/18/2022 01:36:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=172
06/18/2022 01:36:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=173
06/18/2022 01:36:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=174
06/18/2022 01:36:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=174
06/18/2022 01:37:02 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.9060034883943803 on epoch=174
06/18/2022 01:37:02 - INFO - __main__ - Saving model with best Classification-F1: 0.9052292715717765 -> 0.9060034883943803 on epoch=174, global_step=2450
06/18/2022 01:37:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=175
06/18/2022 01:37:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=176
06/18/2022 01:37:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=177
06/18/2022 01:37:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=177
06/18/2022 01:37:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=178
06/18/2022 01:37:22 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7841906733367834 on epoch=178
06/18/2022 01:37:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=179
06/18/2022 01:37:27 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=179
06/18/2022 01:37:30 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=180
06/18/2022 01:37:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=181
06/18/2022 01:37:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=182
06/18/2022 01:37:41 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.8345276195578615 on epoch=182
06/18/2022 01:37:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
06/18/2022 01:37:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=183
06/18/2022 01:37:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=184
06/18/2022 01:37:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
06/18/2022 01:37:54 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=185
06/18/2022 01:38:01 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.7780761290011765 on epoch=185
06/18/2022 01:38:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=186
06/18/2022 01:38:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=187
06/18/2022 01:38:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=187
06/18/2022 01:38:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=188
06/18/2022 01:38:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=189
06/18/2022 01:38:20 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7830774538554425 on epoch=189
06/18/2022 01:38:23 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
06/18/2022 01:38:25 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=190
06/18/2022 01:38:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
06/18/2022 01:38:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
06/18/2022 01:38:33 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=192
06/18/2022 01:38:39 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7500030170281067 on epoch=192
06/18/2022 01:38:42 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=193
06/18/2022 01:38:45 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=194
06/18/2022 01:38:47 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=194
06/18/2022 01:38:50 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=195
06/18/2022 01:38:52 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=196
06/18/2022 01:38:59 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7989288950487667 on epoch=196
06/18/2022 01:39:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=197
06/18/2022 01:39:04 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=197
06/18/2022 01:39:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=198
06/18/2022 01:39:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=199
06/18/2022 01:39:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=199
06/18/2022 01:39:18 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7967076747897711 on epoch=199
06/18/2022 01:39:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=200
06/18/2022 01:39:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=201
06/18/2022 01:39:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=202
06/18/2022 01:39:28 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
06/18/2022 01:39:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=203
06/18/2022 01:39:37 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.8512944464809384 on epoch=203
06/18/2022 01:39:40 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=204
06/18/2022 01:39:42 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=204
06/18/2022 01:39:45 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=205
06/18/2022 01:39:47 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=206
06/18/2022 01:39:50 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
06/18/2022 01:39:56 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.855196878054741 on epoch=207
06/18/2022 01:39:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=207
06/18/2022 01:40:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=208
06/18/2022 01:40:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
06/18/2022 01:40:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
06/18/2022 01:40:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=210
06/18/2022 01:40:16 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.8511608015640274 on epoch=210
06/18/2022 01:40:19 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=211
06/18/2022 01:40:21 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=212
06/18/2022 01:40:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=212
06/18/2022 01:40:26 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=213
06/18/2022 01:40:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=214
06/18/2022 01:40:30 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 01:40:30 - INFO - __main__ - Printing 3 examples
06/18/2022 01:40:30 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
06/18/2022 01:40:30 - INFO - __main__ - ['Plant']
06/18/2022 01:40:30 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
06/18/2022 01:40:30 - INFO - __main__ - ['Plant']
06/18/2022 01:40:30 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
06/18/2022 01:40:30 - INFO - __main__ - ['Plant']
06/18/2022 01:40:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 01:40:31 - INFO - __main__ - Tokenizing Output ...
06/18/2022 01:40:31 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 01:40:31 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 01:40:31 - INFO - __main__ - Printing 3 examples
06/18/2022 01:40:31 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
06/18/2022 01:40:31 - INFO - __main__ - ['Plant']
06/18/2022 01:40:31 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
06/18/2022 01:40:31 - INFO - __main__ - ['Plant']
06/18/2022 01:40:31 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
06/18/2022 01:40:31 - INFO - __main__ - ['Plant']
06/18/2022 01:40:31 - INFO - __main__ - Tokenizing Input ...
06/18/2022 01:40:31 - INFO - __main__ - Tokenizing Output ...
06/18/2022 01:40:31 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 01:40:35 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.9060361681329423 on epoch=214
06/18/2022 01:40:36 - INFO - __main__ - Saving model with best Classification-F1: 0.9060034883943803 -> 0.9060361681329423 on epoch=214, global_step=3000
06/18/2022 01:40:36 - INFO - __main__ - save last model!
06/18/2022 01:40:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 01:40:36 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 01:40:36 - INFO - __main__ - Printing 3 examples
06/18/2022 01:40:36 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 01:40:36 - INFO - __main__ - ['Animal']
06/18/2022 01:40:36 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 01:40:36 - INFO - __main__ - ['Animal']
06/18/2022 01:40:36 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 01:40:36 - INFO - __main__ - ['Village']
06/18/2022 01:40:36 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 01:40:38 - INFO - __main__ - Tokenizing Output ...
06/18/2022 01:40:41 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 01:40:46 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 01:40:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 01:40:47 - INFO - __main__ - Starting training!
06/18/2022 01:42:48 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_21_0.4_8_predictions.txt
06/18/2022 01:42:48 - INFO - __main__ - Classification-F1 on test data: 0.5464
06/18/2022 01:42:49 - INFO - __main__ - prefix=dbpedia_14_16_21, lr=0.4, bsz=8, dev_performance=0.9060361681329423, test_performance=0.5463578915556568
06/18/2022 01:42:49 - INFO - __main__ - Running ... prefix=dbpedia_14_16_21, lr=0.3, bsz=8 ...
06/18/2022 01:42:50 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 01:42:50 - INFO - __main__ - Printing 3 examples
06/18/2022 01:42:50 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
06/18/2022 01:42:50 - INFO - __main__ - ['Plant']
06/18/2022 01:42:50 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
06/18/2022 01:42:50 - INFO - __main__ - ['Plant']
06/18/2022 01:42:50 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
06/18/2022 01:42:50 - INFO - __main__ - ['Plant']
06/18/2022 01:42:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 01:42:50 - INFO - __main__ - Tokenizing Output ...
06/18/2022 01:42:50 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 01:42:50 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 01:42:50 - INFO - __main__ - Printing 3 examples
06/18/2022 01:42:50 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
06/18/2022 01:42:50 - INFO - __main__ - ['Plant']
06/18/2022 01:42:50 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
06/18/2022 01:42:50 - INFO - __main__ - ['Plant']
06/18/2022 01:42:50 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
06/18/2022 01:42:50 - INFO - __main__ - ['Plant']
06/18/2022 01:42:50 - INFO - __main__ - Tokenizing Input ...
06/18/2022 01:42:50 - INFO - __main__ - Tokenizing Output ...
06/18/2022 01:42:50 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 01:43:06 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 01:43:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 01:43:06 - INFO - __main__ - Starting training!
06/18/2022 01:43:10 - INFO - __main__ - Step 10 Global step 10 Train loss 6.43 on epoch=0
06/18/2022 01:43:13 - INFO - __main__ - Step 20 Global step 20 Train loss 4.98 on epoch=1
06/18/2022 01:43:15 - INFO - __main__ - Step 30 Global step 30 Train loss 4.38 on epoch=2
06/18/2022 01:43:18 - INFO - __main__ - Step 40 Global step 40 Train loss 3.99 on epoch=2
06/18/2022 01:43:20 - INFO - __main__ - Step 50 Global step 50 Train loss 3.84 on epoch=3
06/18/2022 01:43:26 - INFO - __main__ - Global step 50 Train loss 4.72 Classification-F1 0.051399807298518285 on epoch=3
06/18/2022 01:43:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.051399807298518285 on epoch=3, global_step=50
06/18/2022 01:43:29 - INFO - __main__ - Step 60 Global step 60 Train loss 3.21 on epoch=4
06/18/2022 01:43:31 - INFO - __main__ - Step 70 Global step 70 Train loss 3.20 on epoch=4
06/18/2022 01:43:34 - INFO - __main__ - Step 80 Global step 80 Train loss 2.69 on epoch=5
06/18/2022 01:43:36 - INFO - __main__ - Step 90 Global step 90 Train loss 2.78 on epoch=6
06/18/2022 01:43:39 - INFO - __main__ - Step 100 Global step 100 Train loss 2.53 on epoch=7
06/18/2022 01:43:44 - INFO - __main__ - Global step 100 Train loss 2.88 Classification-F1 0.10748874882281115 on epoch=7
06/18/2022 01:43:44 - INFO - __main__ - Saving model with best Classification-F1: 0.051399807298518285 -> 0.10748874882281115 on epoch=7, global_step=100
06/18/2022 01:43:47 - INFO - __main__ - Step 110 Global step 110 Train loss 2.16 on epoch=7
06/18/2022 01:43:49 - INFO - __main__ - Step 120 Global step 120 Train loss 2.16 on epoch=8
06/18/2022 01:43:52 - INFO - __main__ - Step 130 Global step 130 Train loss 1.85 on epoch=9
06/18/2022 01:43:54 - INFO - __main__ - Step 140 Global step 140 Train loss 1.82 on epoch=9
06/18/2022 01:43:57 - INFO - __main__ - Step 150 Global step 150 Train loss 1.65 on epoch=10
06/18/2022 01:44:02 - INFO - __main__ - Global step 150 Train loss 1.93 Classification-F1 0.12391960936751359 on epoch=10
06/18/2022 01:44:02 - INFO - __main__ - Saving model with best Classification-F1: 0.10748874882281115 -> 0.12391960936751359 on epoch=10, global_step=150
06/18/2022 01:44:04 - INFO - __main__ - Step 160 Global step 160 Train loss 1.66 on epoch=11
06/18/2022 01:44:07 - INFO - __main__ - Step 170 Global step 170 Train loss 1.60 on epoch=12
06/18/2022 01:44:10 - INFO - __main__ - Step 180 Global step 180 Train loss 1.37 on epoch=12
06/18/2022 01:44:12 - INFO - __main__ - Step 190 Global step 190 Train loss 1.38 on epoch=13
06/18/2022 01:44:15 - INFO - __main__ - Step 200 Global step 200 Train loss 1.21 on epoch=14
06/18/2022 01:44:20 - INFO - __main__ - Global step 200 Train loss 1.45 Classification-F1 0.13733143295663774 on epoch=14
06/18/2022 01:44:20 - INFO - __main__ - Saving model with best Classification-F1: 0.12391960936751359 -> 0.13733143295663774 on epoch=14, global_step=200
06/18/2022 01:44:23 - INFO - __main__ - Step 210 Global step 210 Train loss 1.26 on epoch=14
06/18/2022 01:44:26 - INFO - __main__ - Step 220 Global step 220 Train loss 1.05 on epoch=15
06/18/2022 01:44:28 - INFO - __main__ - Step 230 Global step 230 Train loss 1.05 on epoch=16
06/18/2022 01:44:31 - INFO - __main__ - Step 240 Global step 240 Train loss 1.00 on epoch=17
06/18/2022 01:44:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.96 on epoch=17
06/18/2022 01:44:39 - INFO - __main__ - Global step 250 Train loss 1.06 Classification-F1 0.3274618697909638 on epoch=17
06/18/2022 01:44:39 - INFO - __main__ - Saving model with best Classification-F1: 0.13733143295663774 -> 0.3274618697909638 on epoch=17, global_step=250
06/18/2022 01:44:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.87 on epoch=18
06/18/2022 01:44:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.77 on epoch=19
06/18/2022 01:44:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.72 on epoch=19
06/18/2022 01:44:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.82 on epoch=20
06/18/2022 01:44:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.67 on epoch=21
06/18/2022 01:44:58 - INFO - __main__ - Global step 300 Train loss 0.77 Classification-F1 0.37824021584194156 on epoch=21
06/18/2022 01:44:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3274618697909638 -> 0.37824021584194156 on epoch=21, global_step=300
06/18/2022 01:45:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.67 on epoch=22
06/18/2022 01:45:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.59 on epoch=22
06/18/2022 01:45:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.69 on epoch=23
06/18/2022 01:45:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.56 on epoch=24
06/18/2022 01:45:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.57 on epoch=24
06/18/2022 01:45:18 - INFO - __main__ - Global step 350 Train loss 0.62 Classification-F1 0.5422880106751073 on epoch=24
06/18/2022 01:45:18 - INFO - __main__ - Saving model with best Classification-F1: 0.37824021584194156 -> 0.5422880106751073 on epoch=24, global_step=350
06/18/2022 01:45:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=25
06/18/2022 01:45:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=26
06/18/2022 01:45:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=27
06/18/2022 01:45:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=27
06/18/2022 01:45:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=28
06/18/2022 01:45:38 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.496671055249214 on epoch=28
06/18/2022 01:45:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=29
06/18/2022 01:45:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=29
06/18/2022 01:45:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=30
06/18/2022 01:45:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=31
06/18/2022 01:45:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=32
06/18/2022 01:45:57 - INFO - __main__ - Global step 450 Train loss 0.33 Classification-F1 0.65358674896276 on epoch=32
06/18/2022 01:45:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5422880106751073 -> 0.65358674896276 on epoch=32, global_step=450
06/18/2022 01:46:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=32
06/18/2022 01:46:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=33
06/18/2022 01:46:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=34
06/18/2022 01:46:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=34
06/18/2022 01:46:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=35
06/18/2022 01:46:17 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.6318217228155967 on epoch=35
06/18/2022 01:46:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.28 on epoch=36
06/18/2022 01:46:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.19 on epoch=37
06/18/2022 01:46:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=37
06/18/2022 01:46:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=38
06/18/2022 01:46:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=39
06/18/2022 01:46:37 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.6701073471010053 on epoch=39
06/18/2022 01:46:37 - INFO - __main__ - Saving model with best Classification-F1: 0.65358674896276 -> 0.6701073471010053 on epoch=39, global_step=550
06/18/2022 01:46:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=39
06/18/2022 01:46:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=40
06/18/2022 01:46:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=41
06/18/2022 01:46:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.29 on epoch=42
06/18/2022 01:46:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=42
06/18/2022 01:46:57 - INFO - __main__ - Global step 600 Train loss 0.25 Classification-F1 0.6715011687832978 on epoch=42
06/18/2022 01:46:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6701073471010053 -> 0.6715011687832978 on epoch=42, global_step=600
06/18/2022 01:46:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.29 on epoch=43
06/18/2022 01:47:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=44
06/18/2022 01:47:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=44
06/18/2022 01:47:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=45
06/18/2022 01:47:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=46
06/18/2022 01:47:16 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.6750319392832151 on epoch=46
06/18/2022 01:47:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6715011687832978 -> 0.6750319392832151 on epoch=46, global_step=650
06/18/2022 01:47:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=47
06/18/2022 01:47:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=47
06/18/2022 01:47:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=48
06/18/2022 01:47:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=49
06/18/2022 01:47:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=49
06/18/2022 01:47:36 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.6803852742081502 on epoch=49
06/18/2022 01:47:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6750319392832151 -> 0.6803852742081502 on epoch=49, global_step=700
06/18/2022 01:47:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=50
06/18/2022 01:47:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=51
06/18/2022 01:47:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=52
06/18/2022 01:47:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=52
06/18/2022 01:47:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=53
06/18/2022 01:47:55 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.6666775420271307 on epoch=53
06/18/2022 01:47:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=54
06/18/2022 01:48:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=54
06/18/2022 01:48:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=55
06/18/2022 01:48:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=56
06/18/2022 01:48:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=57
06/18/2022 01:48:15 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.725601692008811 on epoch=57
06/18/2022 01:48:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6803852742081502 -> 0.725601692008811 on epoch=57, global_step=800
06/18/2022 01:48:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=57
06/18/2022 01:48:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=58
06/18/2022 01:48:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=59
06/18/2022 01:48:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=59
06/18/2022 01:48:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.16 on epoch=60
06/18/2022 01:48:34 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.7095249145507876 on epoch=60
06/18/2022 01:48:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=61
06/18/2022 01:48:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=62
06/18/2022 01:48:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=62
06/18/2022 01:48:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=63
06/18/2022 01:48:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=64
06/18/2022 01:48:54 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.6924729095644836 on epoch=64
06/18/2022 01:48:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=64
06/18/2022 01:48:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=65
06/18/2022 01:49:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=66
06/18/2022 01:49:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=67
06/18/2022 01:49:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=67
06/18/2022 01:49:13 - INFO - __main__ - Global step 950 Train loss 0.12 Classification-F1 0.6612374966561916 on epoch=67
06/18/2022 01:49:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=68
06/18/2022 01:49:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=69
06/18/2022 01:49:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=69
06/18/2022 01:49:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=70
06/18/2022 01:49:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=71
06/18/2022 01:49:32 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.625031610826178 on epoch=71
06/18/2022 01:49:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=72
06/18/2022 01:49:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=72
06/18/2022 01:49:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=73
06/18/2022 01:49:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=74
06/18/2022 01:49:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=74
06/18/2022 01:49:52 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.680498460399435 on epoch=74
06/18/2022 01:49:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=75
06/18/2022 01:49:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=76
06/18/2022 01:50:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=77
06/18/2022 01:50:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=77
06/18/2022 01:50:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=78
06/18/2022 01:50:11 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.619699108281193 on epoch=78
06/18/2022 01:50:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=79
06/18/2022 01:50:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=79
06/18/2022 01:50:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=80
06/18/2022 01:50:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=81
06/18/2022 01:50:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=82
06/18/2022 01:50:30 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.7384324781239228 on epoch=82
06/18/2022 01:50:30 - INFO - __main__ - Saving model with best Classification-F1: 0.725601692008811 -> 0.7384324781239228 on epoch=82, global_step=1150
06/18/2022 01:50:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=82
06/18/2022 01:50:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.15 on epoch=83
06/18/2022 01:50:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=84
06/18/2022 01:50:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=84
06/18/2022 01:50:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=85
06/18/2022 01:50:50 - INFO - __main__ - Global step 1200 Train loss 0.10 Classification-F1 0.6489636335877258 on epoch=85
06/18/2022 01:50:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.11 on epoch=86
06/18/2022 01:50:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=87
06/18/2022 01:50:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=87
06/18/2022 01:51:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=88
06/18/2022 01:51:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=89
06/18/2022 01:51:09 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.6493981441550227 on epoch=89
06/18/2022 01:51:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=89
06/18/2022 01:51:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.10 on epoch=90
06/18/2022 01:51:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=91
06/18/2022 01:51:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=92
06/18/2022 01:51:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=92
06/18/2022 01:51:28 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.66143022971652 on epoch=92
06/18/2022 01:51:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=93
06/18/2022 01:51:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=94
06/18/2022 01:51:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=94
06/18/2022 01:51:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=95
06/18/2022 01:51:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=96
06/18/2022 01:51:47 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.6563326679645465 on epoch=96
06/18/2022 01:51:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=97
06/18/2022 01:51:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=97
06/18/2022 01:51:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=98
06/18/2022 01:51:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=99
06/18/2022 01:52:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=99
06/18/2022 01:52:07 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.7401387143322629 on epoch=99
06/18/2022 01:52:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7384324781239228 -> 0.7401387143322629 on epoch=99, global_step=1400
06/18/2022 01:52:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=100
06/18/2022 01:52:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=101
06/18/2022 01:52:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=102
06/18/2022 01:52:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=102
06/18/2022 01:52:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=103
06/18/2022 01:52:27 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.6948537551004887 on epoch=103
06/18/2022 01:52:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=104
06/18/2022 01:52:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=104
06/18/2022 01:52:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=105
06/18/2022 01:52:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=106
06/18/2022 01:52:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=107
06/18/2022 01:52:46 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.6876634836435124 on epoch=107
06/18/2022 01:52:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=107
06/18/2022 01:52:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=108
06/18/2022 01:52:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=109
06/18/2022 01:52:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=109
06/18/2022 01:52:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=110
06/18/2022 01:53:06 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.7193344569567981 on epoch=110
06/18/2022 01:53:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=111
06/18/2022 01:53:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=112
06/18/2022 01:53:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=112
06/18/2022 01:53:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=113
06/18/2022 01:53:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=114
06/18/2022 01:53:25 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.7228032850224003 on epoch=114
06/18/2022 01:53:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=114
06/18/2022 01:53:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=115
06/18/2022 01:53:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=116
06/18/2022 01:53:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=117
06/18/2022 01:53:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=117
06/18/2022 01:53:44 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.7804220103263572 on epoch=117
06/18/2022 01:53:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7401387143322629 -> 0.7804220103263572 on epoch=117, global_step=1650
06/18/2022 01:53:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=118
06/18/2022 01:53:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=119
06/18/2022 01:53:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=119
06/18/2022 01:53:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=120
06/18/2022 01:53:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=121
06/18/2022 01:54:04 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.7297038333091463 on epoch=121
06/18/2022 01:54:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=122
06/18/2022 01:54:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=122
06/18/2022 01:54:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=123
06/18/2022 01:54:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=124
06/18/2022 01:54:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=124
06/18/2022 01:54:23 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.7173266336254451 on epoch=124
06/18/2022 01:54:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=125
06/18/2022 01:54:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=126
06/18/2022 01:54:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=127
06/18/2022 01:54:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=127
06/18/2022 01:54:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=128
06/18/2022 01:54:42 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.7193072238413295 on epoch=128
06/18/2022 01:54:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=129
06/18/2022 01:54:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=129
06/18/2022 01:54:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=130
06/18/2022 01:54:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=131
06/18/2022 01:54:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=132
06/18/2022 01:55:02 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.74324880685412 on epoch=132
06/18/2022 01:55:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=132
06/18/2022 01:55:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=133
06/18/2022 01:55:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=134
06/18/2022 01:55:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=134
06/18/2022 01:55:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=135
06/18/2022 01:55:21 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7448270896824027 on epoch=135
06/18/2022 01:55:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=136
06/18/2022 01:55:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=137
06/18/2022 01:55:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=137
06/18/2022 01:55:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=138
06/18/2022 01:55:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=139
06/18/2022 01:55:40 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.7780004230431177 on epoch=139
06/18/2022 01:55:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=139
06/18/2022 01:55:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=140
06/18/2022 01:55:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=141
06/18/2022 01:55:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=142
06/18/2022 01:55:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=142
06/18/2022 01:56:00 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.6927935837091435 on epoch=142
06/18/2022 01:56:02 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=143
06/18/2022 01:56:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=144
06/18/2022 01:56:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=144
06/18/2022 01:56:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=145
06/18/2022 01:56:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=146
06/18/2022 01:56:20 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.7448270896824027 on epoch=146
06/18/2022 01:56:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=147
06/18/2022 01:56:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=147
06/18/2022 01:56:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=148
06/18/2022 01:56:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=149
06/18/2022 01:56:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=149
06/18/2022 01:56:39 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.7421395759842464 on epoch=149
06/18/2022 01:56:42 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=150
06/18/2022 01:56:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=151
06/18/2022 01:56:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=152
06/18/2022 01:56:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=152
06/18/2022 01:56:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=153
06/18/2022 01:56:58 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.7421395759842464 on epoch=153
06/18/2022 01:57:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=154
06/18/2022 01:57:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=154
06/18/2022 01:57:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=155
06/18/2022 01:57:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=156
06/18/2022 01:57:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=157
06/18/2022 01:57:18 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.8844500507925557 on epoch=157
06/18/2022 01:57:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7804220103263572 -> 0.8844500507925557 on epoch=157, global_step=2200
06/18/2022 01:57:20 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=157
06/18/2022 01:57:23 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=158
06/18/2022 01:57:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=159
06/18/2022 01:57:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=159
06/18/2022 01:57:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=160
06/18/2022 01:57:37 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.7923421597377953 on epoch=160
06/18/2022 01:57:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=161
06/18/2022 01:57:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=162
06/18/2022 01:57:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=162
06/18/2022 01:57:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=163
06/18/2022 01:57:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=164
06/18/2022 01:57:57 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.790671036743143 on epoch=164
06/18/2022 01:58:00 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=164
06/18/2022 01:58:02 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=165
06/18/2022 01:58:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=166
06/18/2022 01:58:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=167
06/18/2022 01:58:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=167
06/18/2022 01:58:16 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.8291788856304985 on epoch=167
06/18/2022 01:58:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=168
06/18/2022 01:58:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=169
06/18/2022 01:58:24 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=169
06/18/2022 01:58:27 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=170
06/18/2022 01:58:30 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=171
06/18/2022 01:58:36 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.7448509286412511 on epoch=171
06/18/2022 01:58:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=172
06/18/2022 01:58:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=172
06/18/2022 01:58:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=173
06/18/2022 01:58:46 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=174
06/18/2022 01:58:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=174
06/18/2022 01:58:55 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.7448543227978711 on epoch=174
06/18/2022 01:58:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=175
06/18/2022 01:59:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=176
06/18/2022 01:59:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=177
06/18/2022 01:59:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=177
06/18/2022 01:59:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=178
06/18/2022 01:59:15 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.8775844466167049 on epoch=178
06/18/2022 01:59:17 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=179
06/18/2022 01:59:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=179
06/18/2022 01:59:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=180
06/18/2022 01:59:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=181
06/18/2022 01:59:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=182
06/18/2022 01:59:34 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.8719174809497391 on epoch=182
06/18/2022 01:59:37 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=182
06/18/2022 01:59:39 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=183
06/18/2022 01:59:42 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=184
06/18/2022 01:59:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=184
06/18/2022 01:59:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=185
06/18/2022 01:59:54 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.7126151122988568 on epoch=185
06/18/2022 01:59:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=186
06/18/2022 01:59:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=187
06/18/2022 02:00:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=187
06/18/2022 02:00:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=188
06/18/2022 02:00:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=189
06/18/2022 02:00:13 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.722704255511606 on epoch=189
06/18/2022 02:00:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
06/18/2022 02:00:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=190
06/18/2022 02:00:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=191
06/18/2022 02:00:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=192
06/18/2022 02:00:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=192
06/18/2022 02:00:33 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.8097955342038035 on epoch=192
06/18/2022 02:00:35 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=193
06/18/2022 02:00:38 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=194
06/18/2022 02:00:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=194
06/18/2022 02:00:43 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=195
06/18/2022 02:00:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=196
06/18/2022 02:00:52 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.7192382226338084 on epoch=196
06/18/2022 02:00:55 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=197
06/18/2022 02:00:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=197
06/18/2022 02:01:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
06/18/2022 02:01:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=199
06/18/2022 02:01:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=199
06/18/2022 02:01:11 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7282183906472331 on epoch=199
06/18/2022 02:01:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=200
06/18/2022 02:01:17 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=201
06/18/2022 02:01:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=202
06/18/2022 02:01:22 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=202
06/18/2022 02:01:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=203
06/18/2022 02:01:31 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.731714451828304 on epoch=203
06/18/2022 02:01:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=204
06/18/2022 02:01:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=204
06/18/2022 02:01:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=205
06/18/2022 02:01:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=206
06/18/2022 02:01:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
06/18/2022 02:01:50 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.7242825383398888 on epoch=207
06/18/2022 02:01:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=207
06/18/2022 02:01:56 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=208
06/18/2022 02:01:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=209
06/18/2022 02:02:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
06/18/2022 02:02:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=210
06/18/2022 02:02:09 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.8331244917879813 on epoch=210
06/18/2022 02:02:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=211
06/18/2022 02:02:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
06/18/2022 02:02:17 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=212
06/18/2022 02:02:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=213
06/18/2022 02:02:22 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=214
06/18/2022 02:02:24 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 02:02:24 - INFO - __main__ - Printing 3 examples
06/18/2022 02:02:24 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
06/18/2022 02:02:24 - INFO - __main__ - ['Plant']
06/18/2022 02:02:24 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
06/18/2022 02:02:24 - INFO - __main__ - ['Plant']
06/18/2022 02:02:24 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
06/18/2022 02:02:24 - INFO - __main__ - ['Plant']
06/18/2022 02:02:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 02:02:24 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:02:24 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 02:02:24 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 02:02:24 - INFO - __main__ - Printing 3 examples
06/18/2022 02:02:24 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
06/18/2022 02:02:24 - INFO - __main__ - ['Plant']
06/18/2022 02:02:24 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
06/18/2022 02:02:24 - INFO - __main__ - ['Plant']
06/18/2022 02:02:24 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
06/18/2022 02:02:24 - INFO - __main__ - ['Plant']
06/18/2022 02:02:24 - INFO - __main__ - Tokenizing Input ...
06/18/2022 02:02:24 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:02:24 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 02:02:28 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.774700910207105 on epoch=214
06/18/2022 02:02:28 - INFO - __main__ - save last model!
06/18/2022 02:02:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 02:02:28 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 02:02:29 - INFO - __main__ - Printing 3 examples
06/18/2022 02:02:29 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 02:02:29 - INFO - __main__ - ['Animal']
06/18/2022 02:02:29 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 02:02:29 - INFO - __main__ - ['Animal']
06/18/2022 02:02:29 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 02:02:29 - INFO - __main__ - ['Village']
06/18/2022 02:02:29 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 02:02:30 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:02:34 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 02:02:40 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 02:02:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 02:02:40 - INFO - __main__ - Starting training!
06/18/2022 02:04:33 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_21_0.3_8_predictions.txt
06/18/2022 02:04:33 - INFO - __main__ - Classification-F1 on test data: 0.4908
06/18/2022 02:04:34 - INFO - __main__ - prefix=dbpedia_14_16_21, lr=0.3, bsz=8, dev_performance=0.8844500507925557, test_performance=0.490833918833348
06/18/2022 02:04:34 - INFO - __main__ - Running ... prefix=dbpedia_14_16_21, lr=0.2, bsz=8 ...
06/18/2022 02:04:35 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 02:04:35 - INFO - __main__ - Printing 3 examples
06/18/2022 02:04:35 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
06/18/2022 02:04:35 - INFO - __main__ - ['Plant']
06/18/2022 02:04:35 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
06/18/2022 02:04:35 - INFO - __main__ - ['Plant']
06/18/2022 02:04:35 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
06/18/2022 02:04:35 - INFO - __main__ - ['Plant']
06/18/2022 02:04:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 02:04:35 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:04:35 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 02:04:35 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 02:04:35 - INFO - __main__ - Printing 3 examples
06/18/2022 02:04:35 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
06/18/2022 02:04:35 - INFO - __main__ - ['Plant']
06/18/2022 02:04:35 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
06/18/2022 02:04:35 - INFO - __main__ - ['Plant']
06/18/2022 02:04:35 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
06/18/2022 02:04:35 - INFO - __main__ - ['Plant']
06/18/2022 02:04:35 - INFO - __main__ - Tokenizing Input ...
06/18/2022 02:04:35 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:04:35 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 02:04:54 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 02:04:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 02:04:55 - INFO - __main__ - Starting training!
06/18/2022 02:04:58 - INFO - __main__ - Step 10 Global step 10 Train loss 7.16 on epoch=0
06/18/2022 02:05:01 - INFO - __main__ - Step 20 Global step 20 Train loss 5.34 on epoch=1
06/18/2022 02:05:03 - INFO - __main__ - Step 30 Global step 30 Train loss 4.98 on epoch=2
06/18/2022 02:05:06 - INFO - __main__ - Step 40 Global step 40 Train loss 4.60 on epoch=2
06/18/2022 02:05:09 - INFO - __main__ - Step 50 Global step 50 Train loss 4.21 on epoch=3
06/18/2022 02:05:16 - INFO - __main__ - Global step 50 Train loss 5.26 Classification-F1 0.026289139060258124 on epoch=3
06/18/2022 02:05:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.026289139060258124 on epoch=3, global_step=50
06/18/2022 02:05:19 - INFO - __main__ - Step 60 Global step 60 Train loss 3.92 on epoch=4
06/18/2022 02:05:21 - INFO - __main__ - Step 70 Global step 70 Train loss 3.91 on epoch=4
06/18/2022 02:05:24 - INFO - __main__ - Step 80 Global step 80 Train loss 3.44 on epoch=5
06/18/2022 02:05:27 - INFO - __main__ - Step 90 Global step 90 Train loss 3.36 on epoch=6
06/18/2022 02:05:29 - INFO - __main__ - Step 100 Global step 100 Train loss 3.26 on epoch=7
06/18/2022 02:05:35 - INFO - __main__ - Global step 100 Train loss 3.58 Classification-F1 0.06365112060151865 on epoch=7
06/18/2022 02:05:35 - INFO - __main__ - Saving model with best Classification-F1: 0.026289139060258124 -> 0.06365112060151865 on epoch=7, global_step=100
06/18/2022 02:05:37 - INFO - __main__ - Step 110 Global step 110 Train loss 2.67 on epoch=7
06/18/2022 02:05:40 - INFO - __main__ - Step 120 Global step 120 Train loss 2.90 on epoch=8
06/18/2022 02:05:43 - INFO - __main__ - Step 130 Global step 130 Train loss 2.61 on epoch=9
06/18/2022 02:05:45 - INFO - __main__ - Step 140 Global step 140 Train loss 2.51 on epoch=9
06/18/2022 02:05:48 - INFO - __main__ - Step 150 Global step 150 Train loss 2.16 on epoch=10
06/18/2022 02:05:53 - INFO - __main__ - Global step 150 Train loss 2.57 Classification-F1 0.0997158225165953 on epoch=10
06/18/2022 02:05:53 - INFO - __main__ - Saving model with best Classification-F1: 0.06365112060151865 -> 0.0997158225165953 on epoch=10, global_step=150
06/18/2022 02:05:56 - INFO - __main__ - Step 160 Global step 160 Train loss 2.22 on epoch=11
06/18/2022 02:05:58 - INFO - __main__ - Step 170 Global step 170 Train loss 2.18 on epoch=12
06/18/2022 02:06:01 - INFO - __main__ - Step 180 Global step 180 Train loss 1.99 on epoch=12
06/18/2022 02:06:04 - INFO - __main__ - Step 190 Global step 190 Train loss 1.98 on epoch=13
06/18/2022 02:06:06 - INFO - __main__ - Step 200 Global step 200 Train loss 1.84 on epoch=14
06/18/2022 02:06:11 - INFO - __main__ - Global step 200 Train loss 2.04 Classification-F1 0.1174718956828449 on epoch=14
06/18/2022 02:06:11 - INFO - __main__ - Saving model with best Classification-F1: 0.0997158225165953 -> 0.1174718956828449 on epoch=14, global_step=200
06/18/2022 02:06:14 - INFO - __main__ - Step 210 Global step 210 Train loss 1.81 on epoch=14
06/18/2022 02:06:17 - INFO - __main__ - Step 220 Global step 220 Train loss 1.73 on epoch=15
06/18/2022 02:06:19 - INFO - __main__ - Step 230 Global step 230 Train loss 1.65 on epoch=16
06/18/2022 02:06:22 - INFO - __main__ - Step 240 Global step 240 Train loss 1.62 on epoch=17
06/18/2022 02:06:24 - INFO - __main__ - Step 250 Global step 250 Train loss 1.57 on epoch=17
06/18/2022 02:06:30 - INFO - __main__ - Global step 250 Train loss 1.68 Classification-F1 0.12851920009814746 on epoch=17
06/18/2022 02:06:30 - INFO - __main__ - Saving model with best Classification-F1: 0.1174718956828449 -> 0.12851920009814746 on epoch=17, global_step=250
06/18/2022 02:06:33 - INFO - __main__ - Step 260 Global step 260 Train loss 1.54 on epoch=18
06/18/2022 02:06:35 - INFO - __main__ - Step 270 Global step 270 Train loss 1.33 on epoch=19
06/18/2022 02:06:38 - INFO - __main__ - Step 280 Global step 280 Train loss 1.30 on epoch=19
06/18/2022 02:06:40 - INFO - __main__ - Step 290 Global step 290 Train loss 1.28 on epoch=20
06/18/2022 02:06:43 - INFO - __main__ - Step 300 Global step 300 Train loss 1.19 on epoch=21
06/18/2022 02:06:49 - INFO - __main__ - Global step 300 Train loss 1.33 Classification-F1 0.1422633987466372 on epoch=21
06/18/2022 02:06:49 - INFO - __main__ - Saving model with best Classification-F1: 0.12851920009814746 -> 0.1422633987466372 on epoch=21, global_step=300
06/18/2022 02:06:52 - INFO - __main__ - Step 310 Global step 310 Train loss 1.38 on epoch=22
06/18/2022 02:06:54 - INFO - __main__ - Step 320 Global step 320 Train loss 1.06 on epoch=22
06/18/2022 02:06:57 - INFO - __main__ - Step 330 Global step 330 Train loss 1.10 on epoch=23
06/18/2022 02:07:00 - INFO - __main__ - Step 340 Global step 340 Train loss 1.01 on epoch=24
06/18/2022 02:07:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.99 on epoch=24
06/18/2022 02:07:08 - INFO - __main__ - Global step 350 Train loss 1.11 Classification-F1 0.2279055258588073 on epoch=24
06/18/2022 02:07:08 - INFO - __main__ - Saving model with best Classification-F1: 0.1422633987466372 -> 0.2279055258588073 on epoch=24, global_step=350
06/18/2022 02:07:11 - INFO - __main__ - Step 360 Global step 360 Train loss 1.02 on epoch=25
06/18/2022 02:07:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=26
06/18/2022 02:07:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.94 on epoch=27
06/18/2022 02:07:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.80 on epoch=27
06/18/2022 02:07:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.88 on epoch=28
06/18/2022 02:07:28 - INFO - __main__ - Global step 400 Train loss 0.91 Classification-F1 0.3750574837537396 on epoch=28
06/18/2022 02:07:28 - INFO - __main__ - Saving model with best Classification-F1: 0.2279055258588073 -> 0.3750574837537396 on epoch=28, global_step=400
06/18/2022 02:07:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.84 on epoch=29
06/18/2022 02:07:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.70 on epoch=29
06/18/2022 02:07:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.78 on epoch=30
06/18/2022 02:07:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.68 on epoch=31
06/18/2022 02:07:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.68 on epoch=32
06/18/2022 02:07:47 - INFO - __main__ - Global step 450 Train loss 0.74 Classification-F1 0.44782820665173606 on epoch=32
06/18/2022 02:07:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3750574837537396 -> 0.44782820665173606 on epoch=32, global_step=450
06/18/2022 02:07:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.55 on epoch=32
06/18/2022 02:07:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.58 on epoch=33
06/18/2022 02:07:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.61 on epoch=34
06/18/2022 02:07:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.60 on epoch=34
06/18/2022 02:08:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=35
06/18/2022 02:08:07 - INFO - __main__ - Global step 500 Train loss 0.57 Classification-F1 0.4592916288897936 on epoch=35
06/18/2022 02:08:07 - INFO - __main__ - Saving model with best Classification-F1: 0.44782820665173606 -> 0.4592916288897936 on epoch=35, global_step=500
06/18/2022 02:08:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=36
06/18/2022 02:08:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=37
06/18/2022 02:08:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.65 on epoch=37
06/18/2022 02:08:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.52 on epoch=38
06/18/2022 02:08:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=39
06/18/2022 02:08:27 - INFO - __main__ - Global step 550 Train loss 0.51 Classification-F1 0.5388472820099749 on epoch=39
06/18/2022 02:08:28 - INFO - __main__ - Saving model with best Classification-F1: 0.4592916288897936 -> 0.5388472820099749 on epoch=39, global_step=550
06/18/2022 02:08:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=39
06/18/2022 02:08:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.54 on epoch=40
06/18/2022 02:08:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=41
06/18/2022 02:08:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=42
06/18/2022 02:08:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=42
06/18/2022 02:08:47 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.5906852886771049 on epoch=42
06/18/2022 02:08:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5388472820099749 -> 0.5906852886771049 on epoch=42, global_step=600
06/18/2022 02:08:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=43
06/18/2022 02:08:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=44
06/18/2022 02:08:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.30 on epoch=44
06/18/2022 02:08:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=45
06/18/2022 02:09:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=46
06/18/2022 02:09:08 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.5683469766573468 on epoch=46
06/18/2022 02:09:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=47
06/18/2022 02:09:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=47
06/18/2022 02:09:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=48
06/18/2022 02:09:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=49
06/18/2022 02:09:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=49
06/18/2022 02:09:28 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.6230238136310243 on epoch=49
06/18/2022 02:09:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5906852886771049 -> 0.6230238136310243 on epoch=49, global_step=700
06/18/2022 02:09:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=50
06/18/2022 02:09:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=51
06/18/2022 02:09:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=52
06/18/2022 02:09:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=52
06/18/2022 02:09:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=53
06/18/2022 02:09:48 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.5486610139753799 on epoch=53
06/18/2022 02:09:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=54
06/18/2022 02:09:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=54
06/18/2022 02:09:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=55
06/18/2022 02:09:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.29 on epoch=56
06/18/2022 02:10:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=57
06/18/2022 02:10:08 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.5699965863444819 on epoch=57
06/18/2022 02:10:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.32 on epoch=57
06/18/2022 02:10:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=58
06/18/2022 02:10:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=59
06/18/2022 02:10:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=59
06/18/2022 02:10:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=60
06/18/2022 02:10:28 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.5737923031207904 on epoch=60
06/18/2022 02:10:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=61
06/18/2022 02:10:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=62
06/18/2022 02:10:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=62
06/18/2022 02:10:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=63
06/18/2022 02:10:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=64
06/18/2022 02:10:47 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.6014435313681208 on epoch=64
06/18/2022 02:10:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=64
06/18/2022 02:10:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=65
06/18/2022 02:10:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=66
06/18/2022 02:10:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=67
06/18/2022 02:11:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=67
06/18/2022 02:11:07 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.675461199315035 on epoch=67
06/18/2022 02:11:07 - INFO - __main__ - Saving model with best Classification-F1: 0.6230238136310243 -> 0.675461199315035 on epoch=67, global_step=950
06/18/2022 02:11:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=68
06/18/2022 02:11:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.16 on epoch=69
06/18/2022 02:11:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=69
06/18/2022 02:11:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=70
06/18/2022 02:11:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=71
06/18/2022 02:11:27 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.584575161058554 on epoch=71
06/18/2022 02:11:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=72
06/18/2022 02:11:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=72
06/18/2022 02:11:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=73
06/18/2022 02:11:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=74
06/18/2022 02:11:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.15 on epoch=74
06/18/2022 02:11:47 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.5533735116226604 on epoch=74
06/18/2022 02:11:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=75
06/18/2022 02:11:52 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=76
06/18/2022 02:11:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=77
06/18/2022 02:11:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=77
06/18/2022 02:12:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=78
06/18/2022 02:12:07 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.5305708891572269 on epoch=78
06/18/2022 02:12:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=79
06/18/2022 02:12:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=79
06/18/2022 02:12:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=80
06/18/2022 02:12:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=81
06/18/2022 02:12:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=82
06/18/2022 02:12:27 - INFO - __main__ - Global step 1150 Train loss 0.17 Classification-F1 0.5581859349205278 on epoch=82
06/18/2022 02:12:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=82
06/18/2022 02:12:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=83
06/18/2022 02:12:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=84
06/18/2022 02:12:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=84
06/18/2022 02:12:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=85
06/18/2022 02:12:46 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.5305708891572269 on epoch=85
06/18/2022 02:12:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=86
06/18/2022 02:12:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=87
06/18/2022 02:12:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=87
06/18/2022 02:12:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=88
06/18/2022 02:12:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.13 on epoch=89
06/18/2022 02:13:06 - INFO - __main__ - Global step 1250 Train loss 0.16 Classification-F1 0.615802083554006 on epoch=89
06/18/2022 02:13:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=89
06/18/2022 02:13:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=90
06/18/2022 02:13:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=91
06/18/2022 02:13:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=92
06/18/2022 02:13:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=92
06/18/2022 02:13:25 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.6580530980720735 on epoch=92
06/18/2022 02:13:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=93
06/18/2022 02:13:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=94
06/18/2022 02:13:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=94
06/18/2022 02:13:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=95
06/18/2022 02:13:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=96
06/18/2022 02:13:44 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.6483126178392332 on epoch=96
06/18/2022 02:13:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=97
06/18/2022 02:13:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=97
06/18/2022 02:13:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=98
06/18/2022 02:13:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.12 on epoch=99
06/18/2022 02:13:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=99
06/18/2022 02:14:04 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.6525231441550228 on epoch=99
06/18/2022 02:14:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.10 on epoch=100
06/18/2022 02:14:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=101
06/18/2022 02:14:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=102
06/18/2022 02:14:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=102
06/18/2022 02:14:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=103
06/18/2022 02:14:23 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.684132127436884 on epoch=103
06/18/2022 02:14:23 - INFO - __main__ - Saving model with best Classification-F1: 0.675461199315035 -> 0.684132127436884 on epoch=103, global_step=1450
06/18/2022 02:14:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=104
06/18/2022 02:14:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.13 on epoch=104
06/18/2022 02:14:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=105
06/18/2022 02:14:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=106
06/18/2022 02:14:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=107
06/18/2022 02:14:42 - INFO - __main__ - Global step 1500 Train loss 0.13 Classification-F1 0.6842453136281685 on epoch=107
06/18/2022 02:14:42 - INFO - __main__ - Saving model with best Classification-F1: 0.684132127436884 -> 0.6842453136281685 on epoch=107, global_step=1500
06/18/2022 02:14:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.09 on epoch=107
06/18/2022 02:14:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=108
06/18/2022 02:14:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=109
06/18/2022 02:14:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.11 on epoch=109
06/18/2022 02:14:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=110
06/18/2022 02:15:01 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.6219733105980984 on epoch=110
06/18/2022 02:15:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=111
06/18/2022 02:15:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=112
06/18/2022 02:15:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=112
06/18/2022 02:15:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=113
06/18/2022 02:15:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=114
06/18/2022 02:15:20 - INFO - __main__ - Global step 1600 Train loss 0.14 Classification-F1 0.6556695992179863 on epoch=114
06/18/2022 02:15:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=114
06/18/2022 02:15:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=115
06/18/2022 02:15:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=116
06/18/2022 02:15:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=117
06/18/2022 02:15:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=117
06/18/2022 02:15:39 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.6959995296158284 on epoch=117
06/18/2022 02:15:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6842453136281685 -> 0.6959995296158284 on epoch=117, global_step=1650
06/18/2022 02:15:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=118
06/18/2022 02:15:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=119
06/18/2022 02:15:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=119
06/18/2022 02:15:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=120
06/18/2022 02:15:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=121
06/18/2022 02:15:58 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.6858525045829577 on epoch=121
06/18/2022 02:16:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=122
06/18/2022 02:16:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=122
06/18/2022 02:16:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=123
06/18/2022 02:16:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=124
06/18/2022 02:16:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=124
06/18/2022 02:16:17 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.7223709493315256 on epoch=124
06/18/2022 02:16:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6959995296158284 -> 0.7223709493315256 on epoch=124, global_step=1750
06/18/2022 02:16:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=125
06/18/2022 02:16:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=126
06/18/2022 02:16:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=127
06/18/2022 02:16:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=127
06/18/2022 02:16:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=128
06/18/2022 02:16:36 - INFO - __main__ - Global step 1800 Train loss 0.09 Classification-F1 0.7170607615262897 on epoch=128
06/18/2022 02:16:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=129
06/18/2022 02:16:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=129
06/18/2022 02:16:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=130
06/18/2022 02:16:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=131
06/18/2022 02:16:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=132
06/18/2022 02:16:56 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.776077648585741 on epoch=132
06/18/2022 02:16:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7223709493315256 -> 0.776077648585741 on epoch=132, global_step=1850
06/18/2022 02:16:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=132
06/18/2022 02:17:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=133
06/18/2022 02:17:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=134
06/18/2022 02:17:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=134
06/18/2022 02:17:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=135
06/18/2022 02:17:15 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.6531205796101431 on epoch=135
06/18/2022 02:17:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=136
06/18/2022 02:17:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=137
06/18/2022 02:17:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.14 on epoch=137
06/18/2022 02:17:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=138
06/18/2022 02:17:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=139
06/18/2022 02:17:34 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.6910957908398735 on epoch=139
06/18/2022 02:17:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=139
06/18/2022 02:17:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=140
06/18/2022 02:17:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=141
06/18/2022 02:17:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=142
06/18/2022 02:17:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=142
06/18/2022 02:17:53 - INFO - __main__ - Global step 2000 Train loss 0.10 Classification-F1 0.6947412681695614 on epoch=142
06/18/2022 02:17:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=143
06/18/2022 02:17:58 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=144
06/18/2022 02:18:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=144
06/18/2022 02:18:03 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=145
06/18/2022 02:18:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=146
06/18/2022 02:18:12 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.6980697474705808 on epoch=146
06/18/2022 02:18:15 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=147
06/18/2022 02:18:17 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.11 on epoch=147
06/18/2022 02:18:20 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.13 on epoch=148
06/18/2022 02:18:23 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=149
06/18/2022 02:18:25 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=149
06/18/2022 02:18:31 - INFO - __main__ - Global step 2100 Train loss 0.08 Classification-F1 0.6548205711903531 on epoch=149
06/18/2022 02:18:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=150
06/18/2022 02:18:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.09 on epoch=151
06/18/2022 02:18:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=152
06/18/2022 02:18:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=152
06/18/2022 02:18:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=153
06/18/2022 02:18:51 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.6565410012978798 on epoch=153
06/18/2022 02:18:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=154
06/18/2022 02:18:56 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=154
06/18/2022 02:18:59 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.11 on epoch=155
06/18/2022 02:19:01 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.10 on epoch=156
06/18/2022 02:19:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=157
06/18/2022 02:19:10 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.6548205711903531 on epoch=157
06/18/2022 02:19:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.09 on epoch=157
06/18/2022 02:19:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=158
06/18/2022 02:19:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=159
06/18/2022 02:19:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=159
06/18/2022 02:19:23 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=160
06/18/2022 02:19:29 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.7027125343114557 on epoch=160
06/18/2022 02:19:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.11 on epoch=161
06/18/2022 02:19:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=162
06/18/2022 02:19:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=162
06/18/2022 02:19:40 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=163
06/18/2022 02:19:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.12 on epoch=164
06/18/2022 02:19:49 - INFO - __main__ - Global step 2300 Train loss 0.08 Classification-F1 0.6510110473808292 on epoch=164
06/18/2022 02:19:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=164
06/18/2022 02:19:54 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=165
06/18/2022 02:19:57 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=166
06/18/2022 02:19:59 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=167
06/18/2022 02:20:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=167
06/18/2022 02:20:08 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.740347461288906 on epoch=167
06/18/2022 02:20:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=168
06/18/2022 02:20:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=169
06/18/2022 02:20:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=169
06/18/2022 02:20:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=170
06/18/2022 02:20:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.11 on epoch=171
06/18/2022 02:20:27 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.7333380052900925 on epoch=171
06/18/2022 02:20:30 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=172
06/18/2022 02:20:33 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=172
06/18/2022 02:20:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=173
06/18/2022 02:20:38 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=174
06/18/2022 02:20:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=174
06/18/2022 02:20:47 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.7430349749870623 on epoch=174
06/18/2022 02:20:50 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=175
06/18/2022 02:20:52 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=176
06/18/2022 02:20:55 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=177
06/18/2022 02:20:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=177
06/18/2022 02:21:00 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=178
06/18/2022 02:21:06 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.7333380052900925 on epoch=178
06/18/2022 02:21:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=179
06/18/2022 02:21:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=179
06/18/2022 02:21:14 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=180
06/18/2022 02:21:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=181
06/18/2022 02:21:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=182
06/18/2022 02:21:26 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.6929302891090069 on epoch=182
06/18/2022 02:21:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=182
06/18/2022 02:21:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=183
06/18/2022 02:21:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=184
06/18/2022 02:21:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=184
06/18/2022 02:21:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=185
06/18/2022 02:21:45 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.7236789143810017 on epoch=185
06/18/2022 02:21:48 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=186
06/18/2022 02:21:51 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=187
06/18/2022 02:21:53 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=187
06/18/2022 02:21:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=188
06/18/2022 02:21:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=189
06/18/2022 02:22:04 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.7333380052900925 on epoch=189
06/18/2022 02:22:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=189
06/18/2022 02:22:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=190
06/18/2022 02:22:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.09 on epoch=191
06/18/2022 02:22:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=192
06/18/2022 02:22:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=192
06/18/2022 02:22:23 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.6879043032980845 on epoch=192
06/18/2022 02:22:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.10 on epoch=193
06/18/2022 02:22:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=194
06/18/2022 02:22:32 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=194
06/18/2022 02:22:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=195
06/18/2022 02:22:37 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.07 on epoch=196
06/18/2022 02:22:43 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.6753601976123226 on epoch=196
06/18/2022 02:22:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=197
06/18/2022 02:22:48 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=197
06/18/2022 02:22:51 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.08 on epoch=198
06/18/2022 02:22:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=199
06/18/2022 02:22:56 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=199
06/18/2022 02:23:02 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.7039278710403748 on epoch=199
06/18/2022 02:23:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=200
06/18/2022 02:23:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.09 on epoch=201
06/18/2022 02:23:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=202
06/18/2022 02:23:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=202
06/18/2022 02:23:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=203
06/18/2022 02:23:21 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.7009015552509013 on epoch=203
06/18/2022 02:23:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=204
06/18/2022 02:23:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=204
06/18/2022 02:23:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=205
06/18/2022 02:23:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=206
06/18/2022 02:23:35 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=207
06/18/2022 02:23:40 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.7011625071859812 on epoch=207
06/18/2022 02:23:43 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=207
06/18/2022 02:23:46 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=208
06/18/2022 02:23:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=209
06/18/2022 02:23:51 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=209
06/18/2022 02:23:54 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=210
06/18/2022 02:24:00 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.7279015228111838 on epoch=210
06/18/2022 02:24:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.07 on epoch=211
06/18/2022 02:24:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=212
06/18/2022 02:24:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=212
06/18/2022 02:24:10 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=213
06/18/2022 02:24:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=214
06/18/2022 02:24:14 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 02:24:14 - INFO - __main__ - Printing 3 examples
06/18/2022 02:24:14 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
06/18/2022 02:24:14 - INFO - __main__ - ['Company']
06/18/2022 02:24:14 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
06/18/2022 02:24:14 - INFO - __main__ - ['Company']
06/18/2022 02:24:14 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
06/18/2022 02:24:14 - INFO - __main__ - ['Company']
06/18/2022 02:24:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 02:24:14 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:24:15 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 02:24:15 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 02:24:15 - INFO - __main__ - Printing 3 examples
06/18/2022 02:24:15 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
06/18/2022 02:24:15 - INFO - __main__ - ['Company']
06/18/2022 02:24:15 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
06/18/2022 02:24:15 - INFO - __main__ - ['Company']
06/18/2022 02:24:15 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
06/18/2022 02:24:15 - INFO - __main__ - ['Company']
06/18/2022 02:24:15 - INFO - __main__ - Tokenizing Input ...
06/18/2022 02:24:15 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:24:15 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 02:24:19 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.7356077263098135 on epoch=214
06/18/2022 02:24:19 - INFO - __main__ - save last model!
06/18/2022 02:24:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 02:24:19 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 02:24:19 - INFO - __main__ - Printing 3 examples
06/18/2022 02:24:19 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 02:24:19 - INFO - __main__ - ['Animal']
06/18/2022 02:24:19 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 02:24:19 - INFO - __main__ - ['Animal']
06/18/2022 02:24:19 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 02:24:19 - INFO - __main__ - ['Village']
06/18/2022 02:24:19 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 02:24:21 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:24:24 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 02:24:30 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 02:24:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 02:24:31 - INFO - __main__ - Starting training!
06/18/2022 02:26:18 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_21_0.2_8_predictions.txt
06/18/2022 02:26:18 - INFO - __main__ - Classification-F1 on test data: 0.4669
06/18/2022 02:26:18 - INFO - __main__ - prefix=dbpedia_14_16_21, lr=0.2, bsz=8, dev_performance=0.776077648585741, test_performance=0.4669094101208661
06/18/2022 02:26:18 - INFO - __main__ - Running ... prefix=dbpedia_14_16_42, lr=0.5, bsz=8 ...
06/18/2022 02:26:19 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 02:26:19 - INFO - __main__ - Printing 3 examples
06/18/2022 02:26:19 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
06/18/2022 02:26:19 - INFO - __main__ - ['Company']
06/18/2022 02:26:19 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
06/18/2022 02:26:19 - INFO - __main__ - ['Company']
06/18/2022 02:26:19 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
06/18/2022 02:26:19 - INFO - __main__ - ['Company']
06/18/2022 02:26:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 02:26:19 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:26:19 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 02:26:19 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 02:26:19 - INFO - __main__ - Printing 3 examples
06/18/2022 02:26:19 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
06/18/2022 02:26:19 - INFO - __main__ - ['Company']
06/18/2022 02:26:19 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
06/18/2022 02:26:19 - INFO - __main__ - ['Company']
06/18/2022 02:26:19 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
06/18/2022 02:26:19 - INFO - __main__ - ['Company']
06/18/2022 02:26:19 - INFO - __main__ - Tokenizing Input ...
06/18/2022 02:26:20 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:26:20 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 02:26:35 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 02:26:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 02:26:36 - INFO - __main__ - Starting training!
06/18/2022 02:26:39 - INFO - __main__ - Step 10 Global step 10 Train loss 6.25 on epoch=0
06/18/2022 02:26:42 - INFO - __main__ - Step 20 Global step 20 Train loss 4.86 on epoch=1
06/18/2022 02:26:44 - INFO - __main__ - Step 30 Global step 30 Train loss 4.12 on epoch=2
06/18/2022 02:26:47 - INFO - __main__ - Step 40 Global step 40 Train loss 3.73 on epoch=2
06/18/2022 02:26:50 - INFO - __main__ - Step 50 Global step 50 Train loss 3.40 on epoch=3
06/18/2022 02:26:54 - INFO - __main__ - Global step 50 Train loss 4.47 Classification-F1 0.10069255299546831 on epoch=3
06/18/2022 02:26:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10069255299546831 on epoch=3, global_step=50
06/18/2022 02:26:57 - INFO - __main__ - Step 60 Global step 60 Train loss 2.84 on epoch=4
06/18/2022 02:27:00 - INFO - __main__ - Step 70 Global step 70 Train loss 2.45 on epoch=4
06/18/2022 02:27:02 - INFO - __main__ - Step 80 Global step 80 Train loss 2.17 on epoch=5
06/18/2022 02:27:05 - INFO - __main__ - Step 90 Global step 90 Train loss 1.97 on epoch=6
06/18/2022 02:27:07 - INFO - __main__ - Step 100 Global step 100 Train loss 1.82 on epoch=7
06/18/2022 02:27:12 - INFO - __main__ - Global step 100 Train loss 2.25 Classification-F1 0.11814507328444412 on epoch=7
06/18/2022 02:27:12 - INFO - __main__ - Saving model with best Classification-F1: 0.10069255299546831 -> 0.11814507328444412 on epoch=7, global_step=100
06/18/2022 02:27:15 - INFO - __main__ - Step 110 Global step 110 Train loss 1.75 on epoch=7
06/18/2022 02:27:18 - INFO - __main__ - Step 120 Global step 120 Train loss 1.46 on epoch=8
06/18/2022 02:27:20 - INFO - __main__ - Step 130 Global step 130 Train loss 1.26 on epoch=9
06/18/2022 02:27:23 - INFO - __main__ - Step 140 Global step 140 Train loss 1.27 on epoch=9
06/18/2022 02:27:25 - INFO - __main__ - Step 150 Global step 150 Train loss 1.10 on epoch=10
06/18/2022 02:27:32 - INFO - __main__ - Global step 150 Train loss 1.37 Classification-F1 0.25905025257966435 on epoch=10
06/18/2022 02:27:32 - INFO - __main__ - Saving model with best Classification-F1: 0.11814507328444412 -> 0.25905025257966435 on epoch=10, global_step=150
06/18/2022 02:27:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.99 on epoch=11
06/18/2022 02:27:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.82 on epoch=12
06/18/2022 02:27:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.81 on epoch=12
06/18/2022 02:27:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.82 on epoch=13
06/18/2022 02:27:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.72 on epoch=14
06/18/2022 02:27:52 - INFO - __main__ - Global step 200 Train loss 0.83 Classification-F1 0.5183390486079749 on epoch=14
06/18/2022 02:27:52 - INFO - __main__ - Saving model with best Classification-F1: 0.25905025257966435 -> 0.5183390486079749 on epoch=14, global_step=200
06/18/2022 02:27:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.75 on epoch=14
06/18/2022 02:27:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.60 on epoch=15
06/18/2022 02:27:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=16
06/18/2022 02:28:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=17
06/18/2022 02:28:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=17
06/18/2022 02:28:11 - INFO - __main__ - Global step 250 Train loss 0.57 Classification-F1 0.5107145061014178 on epoch=17
06/18/2022 02:28:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=18
06/18/2022 02:28:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=19
06/18/2022 02:28:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=19
06/18/2022 02:28:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=20
06/18/2022 02:28:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=21
06/18/2022 02:28:31 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.6083611939399416 on epoch=21
06/18/2022 02:28:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5183390486079749 -> 0.6083611939399416 on epoch=21, global_step=300
06/18/2022 02:28:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=22
06/18/2022 02:28:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=22
06/18/2022 02:28:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=23
06/18/2022 02:28:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=24
06/18/2022 02:28:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=24
06/18/2022 02:28:50 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.6438330170777988 on epoch=24
06/18/2022 02:28:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6083611939399416 -> 0.6438330170777988 on epoch=24, global_step=350
06/18/2022 02:28:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=25
06/18/2022 02:28:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=26
06/18/2022 02:28:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=27
06/18/2022 02:29:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=27
06/18/2022 02:29:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=28
06/18/2022 02:29:10 - INFO - __main__ - Global step 400 Train loss 0.28 Classification-F1 0.6692641199520187 on epoch=28
06/18/2022 02:29:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6438330170777988 -> 0.6692641199520187 on epoch=28, global_step=400
06/18/2022 02:29:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=29
06/18/2022 02:29:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=29
06/18/2022 02:29:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=30
06/18/2022 02:29:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=31
06/18/2022 02:29:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=32
06/18/2022 02:29:30 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.6372641021217872 on epoch=32
06/18/2022 02:29:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=32
06/18/2022 02:29:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=33
06/18/2022 02:29:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.17 on epoch=34
06/18/2022 02:29:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=34
06/18/2022 02:29:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=35
06/18/2022 02:29:49 - INFO - __main__ - Global step 500 Train loss 0.20 Classification-F1 0.6526565464895636 on epoch=35
06/18/2022 02:29:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=36
06/18/2022 02:29:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.18 on epoch=37
06/18/2022 02:29:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=37
06/18/2022 02:29:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=38
06/18/2022 02:30:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=39
06/18/2022 02:30:08 - INFO - __main__ - Global step 550 Train loss 0.19 Classification-F1 0.7204301075268817 on epoch=39
06/18/2022 02:30:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6692641199520187 -> 0.7204301075268817 on epoch=39, global_step=550
06/18/2022 02:30:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=39
06/18/2022 02:30:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=40
06/18/2022 02:30:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=41
06/18/2022 02:30:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=42
06/18/2022 02:30:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.16 on epoch=42
06/18/2022 02:30:27 - INFO - __main__ - Global step 600 Train loss 0.16 Classification-F1 0.71874660584338 on epoch=42
06/18/2022 02:30:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=43
06/18/2022 02:30:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=44
06/18/2022 02:30:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=44
06/18/2022 02:30:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=45
06/18/2022 02:30:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=46
06/18/2022 02:30:46 - INFO - __main__ - Global step 650 Train loss 0.15 Classification-F1 0.6794167582143215 on epoch=46
06/18/2022 02:30:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=47
06/18/2022 02:30:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=47
06/18/2022 02:30:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=48
06/18/2022 02:30:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.08 on epoch=49
06/18/2022 02:30:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=49
06/18/2022 02:31:05 - INFO - __main__ - Global step 700 Train loss 0.13 Classification-F1 0.7135874877810361 on epoch=49
06/18/2022 02:31:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=50
06/18/2022 02:31:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.15 on epoch=51
06/18/2022 02:31:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=52
06/18/2022 02:31:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=52
06/18/2022 02:31:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.13 on epoch=53
06/18/2022 02:31:24 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.7310922848557256 on epoch=53
06/18/2022 02:31:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7204301075268817 -> 0.7310922848557256 on epoch=53, global_step=750
06/18/2022 02:31:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=54
06/18/2022 02:31:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=54
06/18/2022 02:31:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=55
06/18/2022 02:31:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=56
06/18/2022 02:31:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.11 on epoch=57
06/18/2022 02:31:43 - INFO - __main__ - Global step 800 Train loss 0.12 Classification-F1 0.8311507936507936 on epoch=57
06/18/2022 02:31:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7310922848557256 -> 0.8311507936507936 on epoch=57, global_step=800
06/18/2022 02:31:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=57
06/18/2022 02:31:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=58
06/18/2022 02:31:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=59
06/18/2022 02:31:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=59
06/18/2022 02:31:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=60
06/18/2022 02:32:01 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.8894117647058823 on epoch=60
06/18/2022 02:32:01 - INFO - __main__ - Saving model with best Classification-F1: 0.8311507936507936 -> 0.8894117647058823 on epoch=60, global_step=850
06/18/2022 02:32:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=61
06/18/2022 02:32:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=62
06/18/2022 02:32:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=62
06/18/2022 02:32:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=63
06/18/2022 02:32:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=64
06/18/2022 02:32:20 - INFO - __main__ - Global step 900 Train loss 0.11 Classification-F1 0.7310227344183201 on epoch=64
06/18/2022 02:32:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=64
06/18/2022 02:32:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=65
06/18/2022 02:32:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=66
06/18/2022 02:32:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=67
06/18/2022 02:32:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=67
06/18/2022 02:32:39 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.6756938246419313 on epoch=67
06/18/2022 02:32:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=68
06/18/2022 02:32:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=69
06/18/2022 02:32:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=69
06/18/2022 02:32:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=70
06/18/2022 02:32:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=71
06/18/2022 02:32:58 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.7569544911480395 on epoch=71
06/18/2022 02:33:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=72
06/18/2022 02:33:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=72
06/18/2022 02:33:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=73
06/18/2022 02:33:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=74
06/18/2022 02:33:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.15 on epoch=74
06/18/2022 02:33:16 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.8177103099304238 on epoch=74
06/18/2022 02:33:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=75
06/18/2022 02:33:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=76
06/18/2022 02:33:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=77
06/18/2022 02:33:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=77
06/18/2022 02:33:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=78
06/18/2022 02:33:35 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.8709677419354839 on epoch=78
06/18/2022 02:33:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=79
06/18/2022 02:33:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=79
06/18/2022 02:33:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=80
06/18/2022 02:33:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=81
06/18/2022 02:33:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=82
06/18/2022 02:33:54 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.7507709162688125 on epoch=82
06/18/2022 02:33:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=82
06/18/2022 02:33:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=83
06/18/2022 02:34:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=84
06/18/2022 02:34:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=84
06/18/2022 02:34:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=85
06/18/2022 02:34:13 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.7632873038879713 on epoch=85
06/18/2022 02:34:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=86
06/18/2022 02:34:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=87
06/18/2022 02:34:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=87
06/18/2022 02:34:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=88
06/18/2022 02:34:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=89
06/18/2022 02:34:31 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.8631514235092864 on epoch=89
06/18/2022 02:34:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=89
06/18/2022 02:34:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=90
06/18/2022 02:34:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=91
06/18/2022 02:34:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=92
06/18/2022 02:34:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=92
06/18/2022 02:34:50 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.7613757148796082 on epoch=92
06/18/2022 02:34:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=93
06/18/2022 02:34:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=94
06/18/2022 02:34:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=94
06/18/2022 02:35:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=95
06/18/2022 02:35:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=96
06/18/2022 02:35:09 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.7576257001660226 on epoch=96
06/18/2022 02:35:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=97
06/18/2022 02:35:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=97
06/18/2022 02:35:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=98
06/18/2022 02:35:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=99
06/18/2022 02:35:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=99
06/18/2022 02:35:27 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.721303308833313 on epoch=99
06/18/2022 02:35:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=100
06/18/2022 02:35:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=101
06/18/2022 02:35:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=102
06/18/2022 02:35:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=102
06/18/2022 02:35:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=103
06/18/2022 02:35:46 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.7079671838336118 on epoch=103
06/18/2022 02:35:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=104
06/18/2022 02:35:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=104
06/18/2022 02:35:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=105
06/18/2022 02:35:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=106
06/18/2022 02:35:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=107
06/18/2022 02:36:05 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.8058719822239747 on epoch=107
06/18/2022 02:36:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=107
06/18/2022 02:36:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=108
06/18/2022 02:36:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=109
06/18/2022 02:36:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=109
06/18/2022 02:36:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=110
06/18/2022 02:36:23 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.757903492657386 on epoch=110
06/18/2022 02:36:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=111
06/18/2022 02:36:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=112
06/18/2022 02:36:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=112
06/18/2022 02:36:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=113
06/18/2022 02:36:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=114
06/18/2022 02:36:42 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.8081865570578519 on epoch=114
06/18/2022 02:36:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=114
06/18/2022 02:36:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=115
06/18/2022 02:36:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=116
06/18/2022 02:36:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=117
06/18/2022 02:36:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=117
06/18/2022 02:37:01 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7980042583319099 on epoch=117
06/18/2022 02:37:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=118
06/18/2022 02:37:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=119
06/18/2022 02:37:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=119
06/18/2022 02:37:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=120
06/18/2022 02:37:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=121
06/18/2022 02:37:20 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.9228413163897036 on epoch=121
06/18/2022 02:37:20 - INFO - __main__ - Saving model with best Classification-F1: 0.8894117647058823 -> 0.9228413163897036 on epoch=121, global_step=1700
06/18/2022 02:37:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=122
06/18/2022 02:37:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=122
06/18/2022 02:37:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=123
06/18/2022 02:37:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=124
06/18/2022 02:37:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=124
06/18/2022 02:37:39 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.9228413163897036 on epoch=124
06/18/2022 02:37:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=125
06/18/2022 02:37:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=126
06/18/2022 02:37:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=127
06/18/2022 02:37:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=127
06/18/2022 02:37:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=128
06/18/2022 02:37:58 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.9910627007401202 on epoch=128
06/18/2022 02:37:58 - INFO - __main__ - Saving model with best Classification-F1: 0.9228413163897036 -> 0.9910627007401202 on epoch=128, global_step=1800
06/18/2022 02:38:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=129
06/18/2022 02:38:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=129
06/18/2022 02:38:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=130
06/18/2022 02:38:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=131
06/18/2022 02:38:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=132
06/18/2022 02:38:17 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.99553135037006 on epoch=132
06/18/2022 02:38:17 - INFO - __main__ - Saving model with best Classification-F1: 0.9910627007401202 -> 0.99553135037006 on epoch=132, global_step=1850
06/18/2022 02:38:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=132
06/18/2022 02:38:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=133
06/18/2022 02:38:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=134
06/18/2022 02:38:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=134
06/18/2022 02:38:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=135
06/18/2022 02:38:36 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.9821297653958945 on epoch=135
06/18/2022 02:38:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=136
06/18/2022 02:38:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=137
06/18/2022 02:38:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=137
06/18/2022 02:38:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=138
06/18/2022 02:38:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=139
06/18/2022 02:38:55 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.9910627007401202 on epoch=139
06/18/2022 02:38:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=139
06/18/2022 02:39:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=140
06/18/2022 02:39:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=141
06/18/2022 02:39:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=142
06/18/2022 02:39:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=142
06/18/2022 02:39:14 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.9910627007401202 on epoch=142
06/18/2022 02:39:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=143
06/18/2022 02:39:19 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=144
06/18/2022 02:39:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=144
06/18/2022 02:39:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=145
06/18/2022 02:39:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=146
06/18/2022 02:39:33 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.9270120560443141 on epoch=146
06/18/2022 02:39:36 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=147
06/18/2022 02:39:38 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=147
06/18/2022 02:39:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=148
06/18/2022 02:39:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=149
06/18/2022 02:39:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=149
06/18/2022 02:39:52 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.99553135037006 on epoch=149
06/18/2022 02:39:55 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=150
06/18/2022 02:39:58 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=151
06/18/2022 02:40:00 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=152
06/18/2022 02:40:03 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=152
06/18/2022 02:40:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=153
06/18/2022 02:40:12 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.9228413163897033 on epoch=153
06/18/2022 02:40:14 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=154
06/18/2022 02:40:17 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=154
06/18/2022 02:40:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=155
06/18/2022 02:40:22 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=156
06/18/2022 02:40:25 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=157
06/18/2022 02:40:31 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.99553135037006 on epoch=157
06/18/2022 02:40:34 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=157
06/18/2022 02:40:36 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=158
06/18/2022 02:40:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=159
06/18/2022 02:40:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=159
06/18/2022 02:40:44 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=160
06/18/2022 02:40:50 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.9910627007401202 on epoch=160
06/18/2022 02:40:53 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=161
06/18/2022 02:40:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=162
06/18/2022 02:40:58 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=162
06/18/2022 02:41:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=163
06/18/2022 02:41:03 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=164
06/18/2022 02:41:10 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.9910627007401202 on epoch=164
06/18/2022 02:41:12 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=164
06/18/2022 02:41:15 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
06/18/2022 02:41:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=166
06/18/2022 02:41:20 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=167
06/18/2022 02:41:23 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=167
06/18/2022 02:41:29 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.99553135037006 on epoch=167
06/18/2022 02:41:32 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=168
06/18/2022 02:41:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=169
06/18/2022 02:41:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=169
06/18/2022 02:41:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=170
06/18/2022 02:41:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=171
06/18/2022 02:41:49 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.99553135037006 on epoch=171
06/18/2022 02:41:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=172
06/18/2022 02:41:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=172
06/18/2022 02:41:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=173
06/18/2022 02:41:59 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=174
06/18/2022 02:42:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.09 on epoch=174
06/18/2022 02:42:09 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.9270120560443141 on epoch=174
06/18/2022 02:42:11 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=175
06/18/2022 02:42:14 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=176
06/18/2022 02:42:16 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=177
06/18/2022 02:42:19 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
06/18/2022 02:42:21 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=178
06/18/2022 02:42:28 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 1.0 on epoch=178
06/18/2022 02:42:28 - INFO - __main__ - Saving model with best Classification-F1: 0.99553135037006 -> 1.0 on epoch=178, global_step=2500
06/18/2022 02:42:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=179
06/18/2022 02:42:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
06/18/2022 02:42:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=180
06/18/2022 02:42:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=181
06/18/2022 02:42:41 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=182
06/18/2022 02:42:47 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.99553135037006 on epoch=182
06/18/2022 02:42:50 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=182
06/18/2022 02:42:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=183
06/18/2022 02:42:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=184
06/18/2022 02:42:58 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
06/18/2022 02:43:00 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=185
06/18/2022 02:43:07 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.99553135037006 on epoch=185
06/18/2022 02:43:10 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=186
06/18/2022 02:43:12 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=187
06/18/2022 02:43:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
06/18/2022 02:43:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=188
06/18/2022 02:43:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=189
06/18/2022 02:43:27 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.9154680445003026 on epoch=189
06/18/2022 02:43:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=189
06/18/2022 02:43:32 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=190
06/18/2022 02:43:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
06/18/2022 02:43:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=192
06/18/2022 02:43:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=192
06/18/2022 02:43:46 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.9085942707701089 on epoch=192
06/18/2022 02:43:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=193
06/18/2022 02:43:51 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=194
06/18/2022 02:43:54 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
06/18/2022 02:43:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=195
06/18/2022 02:43:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=196
06/18/2022 02:44:06 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.8649071358748779 on epoch=196
06/18/2022 02:44:08 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=197
06/18/2022 02:44:11 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=197
06/18/2022 02:44:13 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=198
06/18/2022 02:44:16 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=199
06/18/2022 02:44:19 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=199
06/18/2022 02:44:25 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.8570908174486804 on epoch=199
06/18/2022 02:44:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=200
06/18/2022 02:44:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=201
06/18/2022 02:44:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=202
06/18/2022 02:44:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
06/18/2022 02:44:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=203
06/18/2022 02:44:45 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.9205474095796676 on epoch=203
06/18/2022 02:44:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=204
06/18/2022 02:44:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=204
06/18/2022 02:44:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=205
06/18/2022 02:44:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=206
06/18/2022 02:44:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
06/18/2022 02:45:04 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.9228413163897036 on epoch=207
06/18/2022 02:45:07 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=207
06/18/2022 02:45:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=208
06/18/2022 02:45:12 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
06/18/2022 02:45:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
06/18/2022 02:45:17 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=210
06/18/2022 02:45:24 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=210
06/18/2022 02:45:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=211
06/18/2022 02:45:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=212
06/18/2022 02:45:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=212
06/18/2022 02:45:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=213
06/18/2022 02:45:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=214
06/18/2022 02:45:38 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 02:45:38 - INFO - __main__ - Printing 3 examples
06/18/2022 02:45:38 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
06/18/2022 02:45:38 - INFO - __main__ - ['Company']
06/18/2022 02:45:38 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
06/18/2022 02:45:38 - INFO - __main__ - ['Company']
06/18/2022 02:45:38 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
06/18/2022 02:45:38 - INFO - __main__ - ['Company']
06/18/2022 02:45:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 02:45:38 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:45:39 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 02:45:39 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 02:45:39 - INFO - __main__ - Printing 3 examples
06/18/2022 02:45:39 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
06/18/2022 02:45:39 - INFO - __main__ - ['Company']
06/18/2022 02:45:39 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
06/18/2022 02:45:39 - INFO - __main__ - ['Company']
06/18/2022 02:45:39 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
06/18/2022 02:45:39 - INFO - __main__ - ['Company']
06/18/2022 02:45:39 - INFO - __main__ - Tokenizing Input ...
06/18/2022 02:45:39 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:45:39 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 02:45:43 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.9186705767350929 on epoch=214
06/18/2022 02:45:43 - INFO - __main__ - save last model!
06/18/2022 02:45:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 02:45:43 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 02:45:43 - INFO - __main__ - Printing 3 examples
06/18/2022 02:45:43 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 02:45:43 - INFO - __main__ - ['Animal']
06/18/2022 02:45:43 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 02:45:43 - INFO - __main__ - ['Animal']
06/18/2022 02:45:43 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 02:45:43 - INFO - __main__ - ['Village']
06/18/2022 02:45:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 02:45:45 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:45:49 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 02:45:58 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 02:45:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 02:45:59 - INFO - __main__ - Starting training!
06/18/2022 02:47:57 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_42_0.5_8_predictions.txt
06/18/2022 02:47:57 - INFO - __main__ - Classification-F1 on test data: 0.5451
06/18/2022 02:47:58 - INFO - __main__ - prefix=dbpedia_14_16_42, lr=0.5, bsz=8, dev_performance=1.0, test_performance=0.5451495855541265
06/18/2022 02:47:58 - INFO - __main__ - Running ... prefix=dbpedia_14_16_42, lr=0.4, bsz=8 ...
06/18/2022 02:47:59 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 02:47:59 - INFO - __main__ - Printing 3 examples
06/18/2022 02:47:59 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
06/18/2022 02:47:59 - INFO - __main__ - ['Company']
06/18/2022 02:47:59 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
06/18/2022 02:47:59 - INFO - __main__ - ['Company']
06/18/2022 02:47:59 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
06/18/2022 02:47:59 - INFO - __main__ - ['Company']
06/18/2022 02:47:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 02:47:59 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:47:59 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 02:47:59 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 02:47:59 - INFO - __main__ - Printing 3 examples
06/18/2022 02:47:59 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
06/18/2022 02:47:59 - INFO - __main__ - ['Company']
06/18/2022 02:47:59 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
06/18/2022 02:47:59 - INFO - __main__ - ['Company']
06/18/2022 02:47:59 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
06/18/2022 02:47:59 - INFO - __main__ - ['Company']
06/18/2022 02:47:59 - INFO - __main__ - Tokenizing Input ...
06/18/2022 02:47:59 - INFO - __main__ - Tokenizing Output ...
06/18/2022 02:47:59 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 02:48:14 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 02:48:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 02:48:15 - INFO - __main__ - Starting training!
06/18/2022 02:48:18 - INFO - __main__ - Step 10 Global step 10 Train loss 6.59 on epoch=0
06/18/2022 02:48:21 - INFO - __main__ - Step 20 Global step 20 Train loss 4.85 on epoch=1
06/18/2022 02:48:24 - INFO - __main__ - Step 30 Global step 30 Train loss 4.62 on epoch=2
06/18/2022 02:48:26 - INFO - __main__ - Step 40 Global step 40 Train loss 4.09 on epoch=2
06/18/2022 02:48:29 - INFO - __main__ - Step 50 Global step 50 Train loss 3.60 on epoch=3
06/18/2022 02:48:34 - INFO - __main__ - Global step 50 Train loss 4.75 Classification-F1 0.06470518887976193 on epoch=3
06/18/2022 02:48:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.06470518887976193 on epoch=3, global_step=50
06/18/2022 02:48:36 - INFO - __main__ - Step 60 Global step 60 Train loss 3.25 on epoch=4
06/18/2022 02:48:39 - INFO - __main__ - Step 70 Global step 70 Train loss 2.76 on epoch=4
06/18/2022 02:48:42 - INFO - __main__ - Step 80 Global step 80 Train loss 2.63 on epoch=5
06/18/2022 02:48:44 - INFO - __main__ - Step 90 Global step 90 Train loss 2.30 on epoch=6
06/18/2022 02:48:47 - INFO - __main__ - Step 100 Global step 100 Train loss 2.26 on epoch=7
06/18/2022 02:48:52 - INFO - __main__ - Global step 100 Train loss 2.64 Classification-F1 0.10012121774349166 on epoch=7
06/18/2022 02:48:52 - INFO - __main__ - Saving model with best Classification-F1: 0.06470518887976193 -> 0.10012121774349166 on epoch=7, global_step=100
06/18/2022 02:48:54 - INFO - __main__ - Step 110 Global step 110 Train loss 1.99 on epoch=7
06/18/2022 02:48:57 - INFO - __main__ - Step 120 Global step 120 Train loss 1.92 on epoch=8
06/18/2022 02:48:59 - INFO - __main__ - Step 130 Global step 130 Train loss 1.79 on epoch=9
06/18/2022 02:49:02 - INFO - __main__ - Step 140 Global step 140 Train loss 1.58 on epoch=9
06/18/2022 02:49:05 - INFO - __main__ - Step 150 Global step 150 Train loss 1.54 on epoch=10
06/18/2022 02:49:10 - INFO - __main__ - Global step 150 Train loss 1.76 Classification-F1 0.13968205795642238 on epoch=10
06/18/2022 02:49:10 - INFO - __main__ - Saving model with best Classification-F1: 0.10012121774349166 -> 0.13968205795642238 on epoch=10, global_step=150
06/18/2022 02:49:13 - INFO - __main__ - Step 160 Global step 160 Train loss 1.24 on epoch=11
06/18/2022 02:49:15 - INFO - __main__ - Step 170 Global step 170 Train loss 1.44 on epoch=12
06/18/2022 02:49:18 - INFO - __main__ - Step 180 Global step 180 Train loss 1.17 on epoch=12
06/18/2022 02:49:21 - INFO - __main__ - Step 190 Global step 190 Train loss 1.07 on epoch=13
06/18/2022 02:49:23 - INFO - __main__ - Step 200 Global step 200 Train loss 1.06 on epoch=14
06/18/2022 02:49:29 - INFO - __main__ - Global step 200 Train loss 1.20 Classification-F1 0.31124668337211886 on epoch=14
06/18/2022 02:49:29 - INFO - __main__ - Saving model with best Classification-F1: 0.13968205795642238 -> 0.31124668337211886 on epoch=14, global_step=200
06/18/2022 02:49:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.97 on epoch=14
06/18/2022 02:49:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=15
06/18/2022 02:49:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.77 on epoch=16
06/18/2022 02:49:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.70 on epoch=17
06/18/2022 02:49:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.68 on epoch=17
06/18/2022 02:49:49 - INFO - __main__ - Global step 250 Train loss 0.80 Classification-F1 0.5087260990872672 on epoch=17
06/18/2022 02:49:49 - INFO - __main__ - Saving model with best Classification-F1: 0.31124668337211886 -> 0.5087260990872672 on epoch=17, global_step=250
06/18/2022 02:49:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.58 on epoch=18
06/18/2022 02:49:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.70 on epoch=19
06/18/2022 02:49:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=19
06/18/2022 02:49:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=20
06/18/2022 02:50:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.57 on epoch=21
06/18/2022 02:50:08 - INFO - __main__ - Global step 300 Train loss 0.58 Classification-F1 0.49852722127900395 on epoch=21
06/18/2022 02:50:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=22
06/18/2022 02:50:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=22
06/18/2022 02:50:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=23
06/18/2022 02:50:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.53 on epoch=24
06/18/2022 02:50:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=24
06/18/2022 02:50:27 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.5552824023427725 on epoch=24
06/18/2022 02:50:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5087260990872672 -> 0.5552824023427725 on epoch=24, global_step=350
06/18/2022 02:50:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=25
06/18/2022 02:50:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=26
06/18/2022 02:50:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=27
06/18/2022 02:50:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=27
06/18/2022 02:50:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=28
06/18/2022 02:50:46 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.6578853046594982 on epoch=28
06/18/2022 02:50:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5552824023427725 -> 0.6578853046594982 on epoch=28, global_step=400
06/18/2022 02:50:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=29
06/18/2022 02:50:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=29
06/18/2022 02:50:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=30
06/18/2022 02:50:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=31
06/18/2022 02:50:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=32
06/18/2022 02:51:06 - INFO - __main__ - Global step 450 Train loss 0.31 Classification-F1 0.7309836718438869 on epoch=32
06/18/2022 02:51:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6578853046594982 -> 0.7309836718438869 on epoch=32, global_step=450
06/18/2022 02:51:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=32
06/18/2022 02:51:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=33
06/18/2022 02:51:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=34
06/18/2022 02:51:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=34
06/18/2022 02:51:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=35
06/18/2022 02:51:25 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.6892181578089898 on epoch=35
06/18/2022 02:51:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.33 on epoch=36
06/18/2022 02:51:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=37
06/18/2022 02:51:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=37
06/18/2022 02:51:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=38
06/18/2022 02:51:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=39
06/18/2022 02:51:44 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.7967914438502673 on epoch=39
06/18/2022 02:51:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7309836718438869 -> 0.7967914438502673 on epoch=39, global_step=550
06/18/2022 02:51:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=39
06/18/2022 02:51:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=40
06/18/2022 02:51:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=41
06/18/2022 02:51:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=42
06/18/2022 02:51:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=42
06/18/2022 02:52:03 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.7734280202120286 on epoch=42
06/18/2022 02:52:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=43
06/18/2022 02:52:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=44
06/18/2022 02:52:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=44
06/18/2022 02:52:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=45
06/18/2022 02:52:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=46
06/18/2022 02:52:23 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.7967914438502673 on epoch=46
06/18/2022 02:52:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=47
06/18/2022 02:52:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.14 on epoch=47
06/18/2022 02:52:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=48
06/18/2022 02:52:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=49
06/18/2022 02:52:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=49
06/18/2022 02:52:42 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.7853569580324798 on epoch=49
06/18/2022 02:52:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=50
06/18/2022 02:52:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=51
06/18/2022 02:52:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.18 on epoch=52
06/18/2022 02:52:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.16 on epoch=52
06/18/2022 02:52:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=53
06/18/2022 02:53:01 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.7740977133766507 on epoch=53
06/18/2022 02:53:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=54
06/18/2022 02:53:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=54
06/18/2022 02:53:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=55
06/18/2022 02:53:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=56
06/18/2022 02:53:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=57
06/18/2022 02:53:20 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.7258245537815431 on epoch=57
06/18/2022 02:53:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=57
06/18/2022 02:53:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=58
06/18/2022 02:53:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=59
06/18/2022 02:53:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=59
06/18/2022 02:53:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=60
06/18/2022 02:53:39 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.8305355179095099 on epoch=60
06/18/2022 02:53:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7967914438502673 -> 0.8305355179095099 on epoch=60, global_step=850
06/18/2022 02:53:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=61
06/18/2022 02:53:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=62
06/18/2022 02:53:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=62
06/18/2022 02:53:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=63
06/18/2022 02:53:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=64
06/18/2022 02:53:58 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.8363357073034492 on epoch=64
06/18/2022 02:53:58 - INFO - __main__ - Saving model with best Classification-F1: 0.8305355179095099 -> 0.8363357073034492 on epoch=64, global_step=900
06/18/2022 02:54:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=64
06/18/2022 02:54:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=65
06/18/2022 02:54:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=66
06/18/2022 02:54:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=67
06/18/2022 02:54:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=67
06/18/2022 02:54:17 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.6958405428604696 on epoch=67
06/18/2022 02:54:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=68
06/18/2022 02:54:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=69
06/18/2022 02:54:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=69
06/18/2022 02:54:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=70
06/18/2022 02:54:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=71
06/18/2022 02:54:36 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.7538720538720539 on epoch=71
06/18/2022 02:54:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.16 on epoch=72
06/18/2022 02:54:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.16 on epoch=72
06/18/2022 02:54:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=73
06/18/2022 02:54:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=74
06/18/2022 02:54:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=74
06/18/2022 02:54:55 - INFO - __main__ - Global step 1050 Train loss 0.11 Classification-F1 0.7631651502619244 on epoch=74
06/18/2022 02:54:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=75
06/18/2022 02:55:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=76
06/18/2022 02:55:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=77
06/18/2022 02:55:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=77
06/18/2022 02:55:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=78
06/18/2022 02:55:14 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.7229985634060337 on epoch=78
06/18/2022 02:55:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=79
06/18/2022 02:55:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=79
06/18/2022 02:55:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=80
06/18/2022 02:55:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=81
06/18/2022 02:55:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=82
06/18/2022 02:55:33 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.9865800865800866 on epoch=82
06/18/2022 02:55:33 - INFO - __main__ - Saving model with best Classification-F1: 0.8363357073034492 -> 0.9865800865800866 on epoch=82, global_step=1150
06/18/2022 02:55:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=82
06/18/2022 02:55:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=83
06/18/2022 02:55:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=84
06/18/2022 02:55:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=84
06/18/2022 02:55:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=85
06/18/2022 02:55:52 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.8591107649071359 on epoch=85
06/18/2022 02:55:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=86
06/18/2022 02:55:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=87
06/18/2022 02:56:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=87
06/18/2022 02:56:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=88
06/18/2022 02:56:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=89
06/18/2022 02:56:11 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.8043771535232636 on epoch=89
06/18/2022 02:56:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=89
06/18/2022 02:56:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=90
06/18/2022 02:56:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=91
06/18/2022 02:56:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=92
06/18/2022 02:56:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=92
06/18/2022 02:56:29 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.7507709162688125 on epoch=92
06/18/2022 02:56:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=93
06/18/2022 02:56:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=94
06/18/2022 02:56:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=94
06/18/2022 02:56:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=95
06/18/2022 02:56:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=96
06/18/2022 02:56:48 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.8527606046507257 on epoch=96
06/18/2022 02:56:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=97
06/18/2022 02:56:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=97
06/18/2022 02:56:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=98
06/18/2022 02:56:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=99
06/18/2022 02:57:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=99
06/18/2022 02:57:07 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.8535934784674704 on epoch=99
06/18/2022 02:57:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=100
06/18/2022 02:57:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=101
06/18/2022 02:57:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=102
06/18/2022 02:57:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=102
06/18/2022 02:57:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=103
06/18/2022 02:57:26 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.80954845281221 on epoch=103
06/18/2022 02:57:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=104
06/18/2022 02:57:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=104
06/18/2022 02:57:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=105
06/18/2022 02:57:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=106
06/18/2022 02:57:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=107
06/18/2022 02:57:45 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.8607181643324232 on epoch=107
06/18/2022 02:57:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=107
06/18/2022 02:57:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=108
06/18/2022 02:57:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=109
06/18/2022 02:57:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=109
06/18/2022 02:57:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=110
06/18/2022 02:58:05 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.8070585679693839 on epoch=110
06/18/2022 02:58:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=111
06/18/2022 02:58:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=112
06/18/2022 02:58:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=112
06/18/2022 02:58:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=113
06/18/2022 02:58:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=114
06/18/2022 02:58:24 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.916384815900945 on epoch=114
06/18/2022 02:58:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=114
06/18/2022 02:58:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=115
06/18/2022 02:58:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=116
06/18/2022 02:58:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=117
06/18/2022 02:58:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=117
06/18/2022 02:58:44 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.8607181643324232 on epoch=117
06/18/2022 02:58:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=118
06/18/2022 02:58:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=119
06/18/2022 02:58:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=119
06/18/2022 02:58:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=120
06/18/2022 02:58:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=121
06/18/2022 02:59:03 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.6581311027681995 on epoch=121
06/18/2022 02:59:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=122
06/18/2022 02:59:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=122
06/18/2022 02:59:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=123
06/18/2022 02:59:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=124
06/18/2022 02:59:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=124
06/18/2022 02:59:22 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.6328711610969675 on epoch=124
06/18/2022 02:59:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=125
06/18/2022 02:59:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=126
06/18/2022 02:59:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=127
06/18/2022 02:59:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=127
06/18/2022 02:59:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=128
06/18/2022 02:59:42 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.7076798688640794 on epoch=128
06/18/2022 02:59:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=129
06/18/2022 02:59:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=129
06/18/2022 02:59:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=130
06/18/2022 02:59:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=131
06/18/2022 02:59:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=132
06/18/2022 03:00:01 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.8606913455037187 on epoch=132
06/18/2022 03:00:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=132
06/18/2022 03:00:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=133
06/18/2022 03:00:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=134
06/18/2022 03:00:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=134
06/18/2022 03:00:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=135
06/18/2022 03:00:20 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.8031576266870384 on epoch=135
06/18/2022 03:00:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=136
06/18/2022 03:00:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=137
06/18/2022 03:00:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=137
06/18/2022 03:00:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=138
06/18/2022 03:00:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=139
06/18/2022 03:00:39 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.703962703962704 on epoch=139
06/18/2022 03:00:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=139
06/18/2022 03:00:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=140
06/18/2022 03:00:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=141
06/18/2022 03:00:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=142
06/18/2022 03:00:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=142
06/18/2022 03:00:58 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.6745621276871276 on epoch=142
06/18/2022 03:01:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=143
06/18/2022 03:01:03 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=144
06/18/2022 03:01:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=144
06/18/2022 03:01:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=145
06/18/2022 03:01:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=146
06/18/2022 03:01:17 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.7122899737500756 on epoch=146
06/18/2022 03:01:20 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=147
06/18/2022 03:01:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=147
06/18/2022 03:01:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=148
06/18/2022 03:01:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=149
06/18/2022 03:01:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=149
06/18/2022 03:01:37 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7475517075517075 on epoch=149
06/18/2022 03:01:40 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=150
06/18/2022 03:01:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=151
06/18/2022 03:01:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=152
06/18/2022 03:01:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=152
06/18/2022 03:01:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=153
06/18/2022 03:01:56 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.8137641546658102 on epoch=153
06/18/2022 03:01:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=154
06/18/2022 03:02:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=154
06/18/2022 03:02:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=155
06/18/2022 03:02:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=156
06/18/2022 03:02:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=157
06/18/2022 03:02:16 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.8583734813573524 on epoch=157
06/18/2022 03:02:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=157
06/18/2022 03:02:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=158
06/18/2022 03:02:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=159
06/18/2022 03:02:27 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=159
06/18/2022 03:02:29 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=160
06/18/2022 03:02:36 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.7548999657515291 on epoch=160
06/18/2022 03:02:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=161
06/18/2022 03:02:42 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=162
06/18/2022 03:02:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=162
06/18/2022 03:02:47 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=163
06/18/2022 03:02:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=164
06/18/2022 03:02:56 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.7386597456041899 on epoch=164
06/18/2022 03:02:59 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=164
06/18/2022 03:03:01 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
06/18/2022 03:03:04 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=166
06/18/2022 03:03:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=167
06/18/2022 03:03:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=167
06/18/2022 03:03:16 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7016161808781947 on epoch=167
06/18/2022 03:03:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=168
06/18/2022 03:03:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=169
06/18/2022 03:03:24 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=169
06/18/2022 03:03:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=170
06/18/2022 03:03:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=171
06/18/2022 03:03:36 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.8587020353001652 on epoch=171
06/18/2022 03:03:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=172
06/18/2022 03:03:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=172
06/18/2022 03:03:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=173
06/18/2022 03:03:46 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=174
06/18/2022 03:03:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=174
06/18/2022 03:03:56 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.9141778066604672 on epoch=174
06/18/2022 03:03:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=175
06/18/2022 03:04:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=176
06/18/2022 03:04:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=177
06/18/2022 03:04:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=177
06/18/2022 03:04:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=178
06/18/2022 03:04:15 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.9228413163897036 on epoch=178
06/18/2022 03:04:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=179
06/18/2022 03:04:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
06/18/2022 03:04:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=180
06/18/2022 03:04:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=181
06/18/2022 03:04:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=182
06/18/2022 03:04:35 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.8569602272727272 on epoch=182
06/18/2022 03:04:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=182
06/18/2022 03:04:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=183
06/18/2022 03:04:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=184
06/18/2022 03:04:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
06/18/2022 03:04:48 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=185
06/18/2022 03:04:55 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.9866027789414886 on epoch=185
06/18/2022 03:04:55 - INFO - __main__ - Saving model with best Classification-F1: 0.9865800865800866 -> 0.9866027789414886 on epoch=185, global_step=2600
06/18/2022 03:04:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=186
06/18/2022 03:05:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=187
06/18/2022 03:05:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=187
06/18/2022 03:05:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=188
06/18/2022 03:05:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=189
06/18/2022 03:05:15 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.9270120560443141 on epoch=189
06/18/2022 03:05:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=189
06/18/2022 03:05:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=190
06/18/2022 03:05:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=191
06/18/2022 03:05:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
06/18/2022 03:05:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=192
06/18/2022 03:05:35 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.9247181492342783 on epoch=192
06/18/2022 03:05:38 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=193
06/18/2022 03:05:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
06/18/2022 03:05:43 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=194
06/18/2022 03:05:46 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=195
06/18/2022 03:05:48 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=196
06/18/2022 03:05:55 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.9270120560443141 on epoch=196
06/18/2022 03:05:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=197
06/18/2022 03:06:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=197
06/18/2022 03:06:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=198
06/18/2022 03:06:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=199
06/18/2022 03:06:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=199
06/18/2022 03:06:15 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.9311827956989248 on epoch=199
06/18/2022 03:06:17 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=200
06/18/2022 03:06:20 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=201
06/18/2022 03:06:22 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=202
06/18/2022 03:06:25 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
06/18/2022 03:06:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=203
06/18/2022 03:06:34 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 1.0 on epoch=203
06/18/2022 03:06:34 - INFO - __main__ - Saving model with best Classification-F1: 0.9866027789414886 -> 1.0 on epoch=203, global_step=2850
06/18/2022 03:06:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=204
06/18/2022 03:06:40 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=204
06/18/2022 03:06:42 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=205
06/18/2022 03:06:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=206
06/18/2022 03:06:47 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
06/18/2022 03:06:54 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=207
06/18/2022 03:06:57 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=207
06/18/2022 03:06:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=208
06/18/2022 03:07:02 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=209
06/18/2022 03:07:04 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
06/18/2022 03:07:07 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=210
06/18/2022 03:07:14 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.9226979472140762 on epoch=210
06/18/2022 03:07:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=211
06/18/2022 03:07:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=212
06/18/2022 03:07:22 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=212
06/18/2022 03:07:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=213
06/18/2022 03:07:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=214
06/18/2022 03:07:29 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 03:07:29 - INFO - __main__ - Printing 3 examples
06/18/2022 03:07:29 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
06/18/2022 03:07:29 - INFO - __main__ - ['Company']
06/18/2022 03:07:29 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
06/18/2022 03:07:29 - INFO - __main__ - ['Company']
06/18/2022 03:07:29 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
06/18/2022 03:07:29 - INFO - __main__ - ['Company']
06/18/2022 03:07:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 03:07:29 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:07:29 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 03:07:29 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 03:07:29 - INFO - __main__ - Printing 3 examples
06/18/2022 03:07:29 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
06/18/2022 03:07:29 - INFO - __main__ - ['Company']
06/18/2022 03:07:29 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
06/18/2022 03:07:29 - INFO - __main__ - ['Company']
06/18/2022 03:07:29 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
06/18/2022 03:07:29 - INFO - __main__ - ['Company']
06/18/2022 03:07:29 - INFO - __main__ - Tokenizing Input ...
06/18/2022 03:07:29 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:07:29 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 03:07:34 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.9185272075594656 on epoch=214
06/18/2022 03:07:34 - INFO - __main__ - save last model!
06/18/2022 03:07:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 03:07:34 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 03:07:34 - INFO - __main__ - Printing 3 examples
06/18/2022 03:07:34 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 03:07:34 - INFO - __main__ - ['Animal']
06/18/2022 03:07:34 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 03:07:34 - INFO - __main__ - ['Animal']
06/18/2022 03:07:34 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 03:07:34 - INFO - __main__ - ['Village']
06/18/2022 03:07:34 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 03:07:36 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:07:39 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 03:07:48 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 03:07:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 03:07:49 - INFO - __main__ - Starting training!
06/18/2022 03:10:10 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_42_0.4_8_predictions.txt
06/18/2022 03:10:10 - INFO - __main__ - Classification-F1 on test data: 0.4825
06/18/2022 03:10:10 - INFO - __main__ - prefix=dbpedia_14_16_42, lr=0.4, bsz=8, dev_performance=1.0, test_performance=0.4824738129606215
06/18/2022 03:10:10 - INFO - __main__ - Running ... prefix=dbpedia_14_16_42, lr=0.3, bsz=8 ...
06/18/2022 03:10:11 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 03:10:11 - INFO - __main__ - Printing 3 examples
06/18/2022 03:10:11 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
06/18/2022 03:10:11 - INFO - __main__ - ['Company']
06/18/2022 03:10:11 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
06/18/2022 03:10:11 - INFO - __main__ - ['Company']
06/18/2022 03:10:11 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
06/18/2022 03:10:11 - INFO - __main__ - ['Company']
06/18/2022 03:10:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 03:10:11 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:10:11 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 03:10:11 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 03:10:11 - INFO - __main__ - Printing 3 examples
06/18/2022 03:10:11 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
06/18/2022 03:10:11 - INFO - __main__ - ['Company']
06/18/2022 03:10:11 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
06/18/2022 03:10:11 - INFO - __main__ - ['Company']
06/18/2022 03:10:11 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
06/18/2022 03:10:11 - INFO - __main__ - ['Company']
06/18/2022 03:10:11 - INFO - __main__ - Tokenizing Input ...
06/18/2022 03:10:11 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:10:12 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 03:10:27 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 03:10:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 03:10:28 - INFO - __main__ - Starting training!
06/18/2022 03:10:31 - INFO - __main__ - Step 10 Global step 10 Train loss 6.52 on epoch=0
06/18/2022 03:10:34 - INFO - __main__ - Step 20 Global step 20 Train loss 5.23 on epoch=1
06/18/2022 03:10:36 - INFO - __main__ - Step 30 Global step 30 Train loss 4.81 on epoch=2
06/18/2022 03:10:39 - INFO - __main__ - Step 40 Global step 40 Train loss 4.34 on epoch=2
06/18/2022 03:10:41 - INFO - __main__ - Step 50 Global step 50 Train loss 4.07 on epoch=3
06/18/2022 03:10:47 - INFO - __main__ - Global step 50 Train loss 4.99 Classification-F1 0.048105358529931586 on epoch=3
06/18/2022 03:10:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.048105358529931586 on epoch=3, global_step=50
06/18/2022 03:10:49 - INFO - __main__ - Step 60 Global step 60 Train loss 3.55 on epoch=4
06/18/2022 03:10:52 - INFO - __main__ - Step 70 Global step 70 Train loss 3.23 on epoch=4
06/18/2022 03:10:54 - INFO - __main__ - Step 80 Global step 80 Train loss 3.09 on epoch=5
06/18/2022 03:10:57 - INFO - __main__ - Step 90 Global step 90 Train loss 2.66 on epoch=6
06/18/2022 03:10:59 - INFO - __main__ - Step 100 Global step 100 Train loss 2.56 on epoch=7
06/18/2022 03:11:05 - INFO - __main__ - Global step 100 Train loss 3.02 Classification-F1 0.09228751780933944 on epoch=7
06/18/2022 03:11:05 - INFO - __main__ - Saving model with best Classification-F1: 0.048105358529931586 -> 0.09228751780933944 on epoch=7, global_step=100
06/18/2022 03:11:08 - INFO - __main__ - Step 110 Global step 110 Train loss 2.48 on epoch=7
06/18/2022 03:11:10 - INFO - __main__ - Step 120 Global step 120 Train loss 2.20 on epoch=8
06/18/2022 03:11:13 - INFO - __main__ - Step 130 Global step 130 Train loss 2.09 on epoch=9
06/18/2022 03:11:15 - INFO - __main__ - Step 140 Global step 140 Train loss 1.97 on epoch=9
06/18/2022 03:11:18 - INFO - __main__ - Step 150 Global step 150 Train loss 1.89 on epoch=10
06/18/2022 03:11:23 - INFO - __main__ - Global step 150 Train loss 2.13 Classification-F1 0.1022598222405637 on epoch=10
06/18/2022 03:11:23 - INFO - __main__ - Saving model with best Classification-F1: 0.09228751780933944 -> 0.1022598222405637 on epoch=10, global_step=150
06/18/2022 03:11:26 - INFO - __main__ - Step 160 Global step 160 Train loss 1.69 on epoch=11
06/18/2022 03:11:28 - INFO - __main__ - Step 170 Global step 170 Train loss 1.72 on epoch=12
06/18/2022 03:11:31 - INFO - __main__ - Step 180 Global step 180 Train loss 1.60 on epoch=12
06/18/2022 03:11:33 - INFO - __main__ - Step 190 Global step 190 Train loss 1.45 on epoch=13
06/18/2022 03:11:36 - INFO - __main__ - Step 200 Global step 200 Train loss 1.33 on epoch=14
06/18/2022 03:11:42 - INFO - __main__ - Global step 200 Train loss 1.56 Classification-F1 0.16140396151669495 on epoch=14
06/18/2022 03:11:42 - INFO - __main__ - Saving model with best Classification-F1: 0.1022598222405637 -> 0.16140396151669495 on epoch=14, global_step=200
06/18/2022 03:11:44 - INFO - __main__ - Step 210 Global step 210 Train loss 1.24 on epoch=14
06/18/2022 03:11:47 - INFO - __main__ - Step 220 Global step 220 Train loss 1.20 on epoch=15
06/18/2022 03:11:49 - INFO - __main__ - Step 230 Global step 230 Train loss 1.11 on epoch=16
06/18/2022 03:11:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.97 on epoch=17
06/18/2022 03:11:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.95 on epoch=17
06/18/2022 03:12:01 - INFO - __main__ - Global step 250 Train loss 1.09 Classification-F1 0.29950238486199077 on epoch=17
06/18/2022 03:12:01 - INFO - __main__ - Saving model with best Classification-F1: 0.16140396151669495 -> 0.29950238486199077 on epoch=17, global_step=250
06/18/2022 03:12:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.86 on epoch=18
06/18/2022 03:12:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.95 on epoch=19
06/18/2022 03:12:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.75 on epoch=19
06/18/2022 03:12:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=20
06/18/2022 03:12:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.72 on epoch=21
06/18/2022 03:12:20 - INFO - __main__ - Global step 300 Train loss 0.83 Classification-F1 0.5032910975770549 on epoch=21
06/18/2022 03:12:20 - INFO - __main__ - Saving model with best Classification-F1: 0.29950238486199077 -> 0.5032910975770549 on epoch=21, global_step=300
06/18/2022 03:12:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.64 on epoch=22
06/18/2022 03:12:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.66 on epoch=22
06/18/2022 03:12:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.62 on epoch=23
06/18/2022 03:12:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.57 on epoch=24
06/18/2022 03:12:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.57 on epoch=24
06/18/2022 03:12:39 - INFO - __main__ - Global step 350 Train loss 0.61 Classification-F1 0.514877814683154 on epoch=24
06/18/2022 03:12:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5032910975770549 -> 0.514877814683154 on epoch=24, global_step=350
06/18/2022 03:12:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=25
06/18/2022 03:12:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.54 on epoch=26
06/18/2022 03:12:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.57 on epoch=27
06/18/2022 03:12:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.52 on epoch=27
06/18/2022 03:12:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=28
06/18/2022 03:12:59 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.5070029325513197 on epoch=28
06/18/2022 03:13:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=29
06/18/2022 03:13:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=29
06/18/2022 03:13:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=30
06/18/2022 03:13:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=31
06/18/2022 03:13:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=32
06/18/2022 03:13:18 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.5485566949802573 on epoch=32
06/18/2022 03:13:18 - INFO - __main__ - Saving model with best Classification-F1: 0.514877814683154 -> 0.5485566949802573 on epoch=32, global_step=450
06/18/2022 03:13:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=32
06/18/2022 03:13:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.35 on epoch=33
06/18/2022 03:13:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=34
06/18/2022 03:13:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=34
06/18/2022 03:13:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=35
06/18/2022 03:13:37 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.6159754224270353 on epoch=35
06/18/2022 03:13:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5485566949802573 -> 0.6159754224270353 on epoch=35, global_step=500
06/18/2022 03:13:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.30 on epoch=36
06/18/2022 03:13:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=37
06/18/2022 03:13:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.29 on epoch=37
06/18/2022 03:13:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.31 on epoch=38
06/18/2022 03:13:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=39
06/18/2022 03:13:56 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.6404996837444655 on epoch=39
06/18/2022 03:13:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6159754224270353 -> 0.6404996837444655 on epoch=39, global_step=550
06/18/2022 03:13:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=39
06/18/2022 03:14:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=40
06/18/2022 03:14:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=41
06/18/2022 03:14:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=42
06/18/2022 03:14:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=42
06/18/2022 03:14:15 - INFO - __main__ - Global step 600 Train loss 0.28 Classification-F1 0.6424884792626728 on epoch=42
06/18/2022 03:14:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6404996837444655 -> 0.6424884792626728 on epoch=42, global_step=600
06/18/2022 03:14:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=43
06/18/2022 03:14:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=44
06/18/2022 03:14:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=44
06/18/2022 03:14:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=45
06/18/2022 03:14:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.15 on epoch=46
06/18/2022 03:14:35 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.6808149405772496 on epoch=46
06/18/2022 03:14:35 - INFO - __main__ - Saving model with best Classification-F1: 0.6424884792626728 -> 0.6808149405772496 on epoch=46, global_step=650
06/18/2022 03:14:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=47
06/18/2022 03:14:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=47
06/18/2022 03:14:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=48
06/18/2022 03:14:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=49
06/18/2022 03:14:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=49
06/18/2022 03:14:54 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.7275266732372495 on epoch=49
06/18/2022 03:14:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6808149405772496 -> 0.7275266732372495 on epoch=49, global_step=700
06/18/2022 03:14:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.15 on epoch=50
06/18/2022 03:14:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=51
06/18/2022 03:15:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.26 on epoch=52
06/18/2022 03:15:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=52
06/18/2022 03:15:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=53
06/18/2022 03:15:13 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.6892181578089898 on epoch=53
06/18/2022 03:15:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=54
06/18/2022 03:15:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=54
06/18/2022 03:15:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=55
06/18/2022 03:15:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=56
06/18/2022 03:15:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=57
06/18/2022 03:15:32 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.6892357956984468 on epoch=57
06/18/2022 03:15:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=57
06/18/2022 03:15:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.17 on epoch=58
06/18/2022 03:15:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=59
06/18/2022 03:15:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=59
06/18/2022 03:15:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=60
06/18/2022 03:15:51 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.7759216049438759 on epoch=60
06/18/2022 03:15:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7275266732372495 -> 0.7759216049438759 on epoch=60, global_step=850
06/18/2022 03:15:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=61
06/18/2022 03:15:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=62
06/18/2022 03:15:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.15 on epoch=62
06/18/2022 03:16:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=63
06/18/2022 03:16:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=64
06/18/2022 03:16:10 - INFO - __main__ - Global step 900 Train loss 0.18 Classification-F1 0.8185687520364939 on epoch=64
06/18/2022 03:16:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7759216049438759 -> 0.8185687520364939 on epoch=64, global_step=900
06/18/2022 03:16:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=64
06/18/2022 03:16:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=65
06/18/2022 03:16:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.16 on epoch=66
06/18/2022 03:16:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=67
06/18/2022 03:16:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=67
06/18/2022 03:16:29 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.8375 on epoch=67
06/18/2022 03:16:29 - INFO - __main__ - Saving model with best Classification-F1: 0.8185687520364939 -> 0.8375 on epoch=67, global_step=950
06/18/2022 03:16:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=68
06/18/2022 03:16:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=69
06/18/2022 03:16:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=69
06/18/2022 03:16:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=70
06/18/2022 03:16:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=71
06/18/2022 03:16:48 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.903030303030303 on epoch=71
06/18/2022 03:16:48 - INFO - __main__ - Saving model with best Classification-F1: 0.8375 -> 0.903030303030303 on epoch=71, global_step=1000
06/18/2022 03:16:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.13 on epoch=72
06/18/2022 03:16:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.14 on epoch=72
06/18/2022 03:16:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=73
06/18/2022 03:16:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=74
06/18/2022 03:17:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=74
06/18/2022 03:17:07 - INFO - __main__ - Global step 1050 Train loss 0.11 Classification-F1 0.8849918540241121 on epoch=74
06/18/2022 03:17:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=75
06/18/2022 03:17:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=76
06/18/2022 03:17:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=77
06/18/2022 03:17:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=77
06/18/2022 03:17:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=78
06/18/2022 03:17:26 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.7095588646485523 on epoch=78
06/18/2022 03:17:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=79
06/18/2022 03:17:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.13 on epoch=79
06/18/2022 03:17:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=80
06/18/2022 03:17:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=81
06/18/2022 03:17:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=82
06/18/2022 03:17:45 - INFO - __main__ - Global step 1150 Train loss 0.12 Classification-F1 0.8178368121442126 on epoch=82
06/18/2022 03:17:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=82
06/18/2022 03:17:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=83
06/18/2022 03:17:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=84
06/18/2022 03:17:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=84
06/18/2022 03:17:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=85
06/18/2022 03:18:04 - INFO - __main__ - Global step 1200 Train loss 0.11 Classification-F1 0.8178368121442126 on epoch=85
06/18/2022 03:18:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=86
06/18/2022 03:18:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=87
06/18/2022 03:18:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=87
06/18/2022 03:18:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=88
06/18/2022 03:18:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=89
06/18/2022 03:18:23 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.830220713073005 on epoch=89
06/18/2022 03:18:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=89
06/18/2022 03:18:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=90
06/18/2022 03:18:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.09 on epoch=91
06/18/2022 03:18:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=92
06/18/2022 03:18:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=92
06/18/2022 03:18:42 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.7677047289504036 on epoch=92
06/18/2022 03:18:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=93
06/18/2022 03:18:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=94
06/18/2022 03:18:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=94
06/18/2022 03:18:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=95
06/18/2022 03:18:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=96
06/18/2022 03:19:00 - INFO - __main__ - Global step 1350 Train loss 0.10 Classification-F1 0.7360623781676413 on epoch=96
06/18/2022 03:19:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=97
06/18/2022 03:19:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=97
06/18/2022 03:19:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=98
06/18/2022 03:19:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=99
06/18/2022 03:19:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=99
06/18/2022 03:19:19 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.6787658802177858 on epoch=99
06/18/2022 03:19:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=100
06/18/2022 03:19:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=101
06/18/2022 03:19:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.13 on epoch=102
06/18/2022 03:19:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=102
06/18/2022 03:19:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=103
06/18/2022 03:19:38 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.7811692985415422 on epoch=103
06/18/2022 03:19:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=104
06/18/2022 03:19:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=104
06/18/2022 03:19:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=105
06/18/2022 03:19:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=106
06/18/2022 03:19:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=107
06/18/2022 03:19:57 - INFO - __main__ - Global step 1500 Train loss 0.09 Classification-F1 0.7825311942959002 on epoch=107
06/18/2022 03:19:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=107
06/18/2022 03:20:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=108
06/18/2022 03:20:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=109
06/18/2022 03:20:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=109
06/18/2022 03:20:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=110
06/18/2022 03:20:16 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.7887955182072829 on epoch=110
06/18/2022 03:20:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=111
06/18/2022 03:20:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=112
06/18/2022 03:20:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=112
06/18/2022 03:20:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=113
06/18/2022 03:20:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=114
06/18/2022 03:20:35 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.7937356760886173 on epoch=114
06/18/2022 03:20:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=114
06/18/2022 03:20:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=115
06/18/2022 03:20:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=116
06/18/2022 03:20:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=117
06/18/2022 03:20:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=117
06/18/2022 03:20:54 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.7948939106434362 on epoch=117
06/18/2022 03:20:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=118
06/18/2022 03:20:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=119
06/18/2022 03:21:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=119
06/18/2022 03:21:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=120
06/18/2022 03:21:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=121
06/18/2022 03:21:12 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.9030756371569837 on epoch=121
06/18/2022 03:21:12 - INFO - __main__ - Saving model with best Classification-F1: 0.903030303030303 -> 0.9030756371569837 on epoch=121, global_step=1700
06/18/2022 03:21:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=122
06/18/2022 03:21:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=122
06/18/2022 03:21:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=123
06/18/2022 03:21:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=124
06/18/2022 03:21:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=124
06/18/2022 03:21:31 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.856100754084625 on epoch=124
06/18/2022 03:21:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=125
06/18/2022 03:21:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=126
06/18/2022 03:21:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=127
06/18/2022 03:21:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=127
06/18/2022 03:21:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=128
06/18/2022 03:21:50 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.7943098568098568 on epoch=128
06/18/2022 03:21:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=129
06/18/2022 03:21:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=129
06/18/2022 03:21:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=130
06/18/2022 03:22:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=131
06/18/2022 03:22:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=132
06/18/2022 03:22:09 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.7527139784084229 on epoch=132
06/18/2022 03:22:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=132
06/18/2022 03:22:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=133
06/18/2022 03:22:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=134
06/18/2022 03:22:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=134
06/18/2022 03:22:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=135
06/18/2022 03:22:28 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.8117401192451903 on epoch=135
06/18/2022 03:22:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=136
06/18/2022 03:22:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=137
06/18/2022 03:22:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=137
06/18/2022 03:22:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=138
06/18/2022 03:22:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=139
06/18/2022 03:22:47 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.6798035298035299 on epoch=139
06/18/2022 03:22:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=139
06/18/2022 03:22:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=140
06/18/2022 03:22:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=141
06/18/2022 03:22:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=142
06/18/2022 03:23:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=142
06/18/2022 03:23:06 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.7128630457440935 on epoch=142
06/18/2022 03:23:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=143
06/18/2022 03:23:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=144
06/18/2022 03:23:14 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=144
06/18/2022 03:23:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.09 on epoch=145
06/18/2022 03:23:19 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=146
06/18/2022 03:23:25 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.6817739731532835 on epoch=146
06/18/2022 03:23:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=147
06/18/2022 03:23:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=147
06/18/2022 03:23:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=148
06/18/2022 03:23:36 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=149
06/18/2022 03:23:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=149
06/18/2022 03:23:45 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.635028003832558 on epoch=149
06/18/2022 03:23:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=150
06/18/2022 03:23:50 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=151
06/18/2022 03:23:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=152
06/18/2022 03:23:55 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=152
06/18/2022 03:23:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=153
06/18/2022 03:24:05 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.6330382162640227 on epoch=153
06/18/2022 03:24:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=154
06/18/2022 03:24:10 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=154
06/18/2022 03:24:12 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=155
06/18/2022 03:24:15 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=156
06/18/2022 03:24:18 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=157
06/18/2022 03:24:24 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.6997676354073038 on epoch=157
06/18/2022 03:24:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=157
06/18/2022 03:24:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=158
06/18/2022 03:24:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=159
06/18/2022 03:24:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=159
06/18/2022 03:24:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=160
06/18/2022 03:24:44 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.7535670429788077 on epoch=160
06/18/2022 03:24:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=161
06/18/2022 03:24:49 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=162
06/18/2022 03:24:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=162
06/18/2022 03:24:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=163
06/18/2022 03:24:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=164
06/18/2022 03:25:03 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.7387522281639929 on epoch=164
06/18/2022 03:25:06 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=164
06/18/2022 03:25:09 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=165
06/18/2022 03:25:11 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=166
06/18/2022 03:25:14 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=167
06/18/2022 03:25:16 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=167
06/18/2022 03:25:23 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7130942377122343 on epoch=167
06/18/2022 03:25:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=168
06/18/2022 03:25:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=169
06/18/2022 03:25:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=169
06/18/2022 03:25:33 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=170
06/18/2022 03:25:36 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=171
06/18/2022 03:25:42 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.7091627279520949 on epoch=171
06/18/2022 03:25:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=172
06/18/2022 03:25:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=172
06/18/2022 03:25:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=173
06/18/2022 03:25:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=174
06/18/2022 03:25:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=174
06/18/2022 03:26:02 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7238028273681923 on epoch=174
06/18/2022 03:26:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=175
06/18/2022 03:26:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=176
06/18/2022 03:26:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=177
06/18/2022 03:26:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=177
06/18/2022 03:26:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=178
06/18/2022 03:26:22 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.8090711031887503 on epoch=178
06/18/2022 03:26:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=179
06/18/2022 03:26:27 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=179
06/18/2022 03:26:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=180
06/18/2022 03:26:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=181
06/18/2022 03:26:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=182
06/18/2022 03:26:41 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.806494543518765 on epoch=182
06/18/2022 03:26:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
06/18/2022 03:26:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=183
06/18/2022 03:26:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=184
06/18/2022 03:26:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
06/18/2022 03:26:54 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=185
06/18/2022 03:27:01 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.8059869842365098 on epoch=185
06/18/2022 03:27:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=186
06/18/2022 03:27:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=187
06/18/2022 03:27:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=187
06/18/2022 03:27:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=188
06/18/2022 03:27:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=189
06/18/2022 03:27:20 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.8666405433646813 on epoch=189
06/18/2022 03:27:23 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=189
06/18/2022 03:27:25 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=190
06/18/2022 03:27:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=191
06/18/2022 03:27:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=192
06/18/2022 03:27:33 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=192
06/18/2022 03:27:40 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.8133435192258721 on epoch=192
06/18/2022 03:27:42 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=193
06/18/2022 03:27:45 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=194
06/18/2022 03:27:47 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=194
06/18/2022 03:27:50 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=195
06/18/2022 03:27:53 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=196
06/18/2022 03:27:59 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.8063896887426298 on epoch=196
06/18/2022 03:28:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=197
06/18/2022 03:28:04 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=197
06/18/2022 03:28:07 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=198
06/18/2022 03:28:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=199
06/18/2022 03:28:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=199
06/18/2022 03:28:18 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.9223963775687913 on epoch=199
06/18/2022 03:28:18 - INFO - __main__ - Saving model with best Classification-F1: 0.9030756371569837 -> 0.9223963775687913 on epoch=199, global_step=2800
06/18/2022 03:28:21 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=200
06/18/2022 03:28:24 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=201
06/18/2022 03:28:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=202
06/18/2022 03:28:29 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=202
06/18/2022 03:28:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=203
06/18/2022 03:28:38 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.8044921555357988 on epoch=203
06/18/2022 03:28:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=204
06/18/2022 03:28:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=204
06/18/2022 03:28:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=205
06/18/2022 03:28:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=206
06/18/2022 03:28:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=207
06/18/2022 03:28:58 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.8034970993936839 on epoch=207
06/18/2022 03:29:00 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=207
06/18/2022 03:29:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=208
06/18/2022 03:29:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=209
06/18/2022 03:29:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=209
06/18/2022 03:29:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=210
06/18/2022 03:29:17 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.8071771637948109 on epoch=210
06/18/2022 03:29:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=211
06/18/2022 03:29:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=212
06/18/2022 03:29:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=212
06/18/2022 03:29:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=213
06/18/2022 03:29:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=214
06/18/2022 03:29:32 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 03:29:32 - INFO - __main__ - Printing 3 examples
06/18/2022 03:29:32 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
06/18/2022 03:29:32 - INFO - __main__ - ['Company']
06/18/2022 03:29:32 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
06/18/2022 03:29:32 - INFO - __main__ - ['Company']
06/18/2022 03:29:32 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
06/18/2022 03:29:32 - INFO - __main__ - ['Company']
06/18/2022 03:29:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 03:29:32 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:29:32 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 03:29:32 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 03:29:32 - INFO - __main__ - Printing 3 examples
06/18/2022 03:29:32 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
06/18/2022 03:29:32 - INFO - __main__ - ['Company']
06/18/2022 03:29:32 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
06/18/2022 03:29:32 - INFO - __main__ - ['Company']
06/18/2022 03:29:32 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
06/18/2022 03:29:32 - INFO - __main__ - ['Company']
06/18/2022 03:29:32 - INFO - __main__ - Tokenizing Input ...
06/18/2022 03:29:32 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:29:32 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 03:29:37 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.8641774891774892 on epoch=214
06/18/2022 03:29:37 - INFO - __main__ - save last model!
06/18/2022 03:29:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 03:29:37 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 03:29:37 - INFO - __main__ - Printing 3 examples
06/18/2022 03:29:37 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 03:29:37 - INFO - __main__ - ['Animal']
06/18/2022 03:29:37 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 03:29:37 - INFO - __main__ - ['Animal']
06/18/2022 03:29:37 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 03:29:37 - INFO - __main__ - ['Village']
06/18/2022 03:29:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 03:29:39 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:29:42 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 03:29:51 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 03:29:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 03:29:52 - INFO - __main__ - Starting training!
06/18/2022 03:31:50 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_42_0.3_8_predictions.txt
06/18/2022 03:31:50 - INFO - __main__ - Classification-F1 on test data: 0.5199
06/18/2022 03:31:50 - INFO - __main__ - prefix=dbpedia_14_16_42, lr=0.3, bsz=8, dev_performance=0.9223963775687913, test_performance=0.5199229989713151
06/18/2022 03:31:50 - INFO - __main__ - Running ... prefix=dbpedia_14_16_42, lr=0.2, bsz=8 ...
06/18/2022 03:31:51 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 03:31:51 - INFO - __main__ - Printing 3 examples
06/18/2022 03:31:51 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
06/18/2022 03:31:51 - INFO - __main__ - ['Company']
06/18/2022 03:31:51 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
06/18/2022 03:31:51 - INFO - __main__ - ['Company']
06/18/2022 03:31:51 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
06/18/2022 03:31:51 - INFO - __main__ - ['Company']
06/18/2022 03:31:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 03:31:51 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:31:51 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 03:31:51 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 03:31:51 - INFO - __main__ - Printing 3 examples
06/18/2022 03:31:51 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
06/18/2022 03:31:51 - INFO - __main__ - ['Company']
06/18/2022 03:31:51 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
06/18/2022 03:31:51 - INFO - __main__ - ['Company']
06/18/2022 03:31:51 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
06/18/2022 03:31:51 - INFO - __main__ - ['Company']
06/18/2022 03:31:51 - INFO - __main__ - Tokenizing Input ...
06/18/2022 03:31:51 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:31:52 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 03:32:10 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 03:32:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 03:32:11 - INFO - __main__ - Starting training!
06/18/2022 03:32:14 - INFO - __main__ - Step 10 Global step 10 Train loss 7.10 on epoch=0
06/18/2022 03:32:17 - INFO - __main__ - Step 20 Global step 20 Train loss 5.53 on epoch=1
06/18/2022 03:32:19 - INFO - __main__ - Step 30 Global step 30 Train loss 5.09 on epoch=2
06/18/2022 03:32:22 - INFO - __main__ - Step 40 Global step 40 Train loss 4.74 on epoch=2
06/18/2022 03:32:25 - INFO - __main__ - Step 50 Global step 50 Train loss 4.52 on epoch=3
06/18/2022 03:32:32 - INFO - __main__ - Global step 50 Train loss 5.39 Classification-F1 0.021315904139433548 on epoch=3
06/18/2022 03:32:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.021315904139433548 on epoch=3, global_step=50
06/18/2022 03:32:35 - INFO - __main__ - Step 60 Global step 60 Train loss 4.11 on epoch=4
06/18/2022 03:32:37 - INFO - __main__ - Step 70 Global step 70 Train loss 4.22 on epoch=4
06/18/2022 03:32:40 - INFO - __main__ - Step 80 Global step 80 Train loss 3.83 on epoch=5
06/18/2022 03:32:42 - INFO - __main__ - Step 90 Global step 90 Train loss 3.48 on epoch=6
06/18/2022 03:32:45 - INFO - __main__ - Step 100 Global step 100 Train loss 3.54 on epoch=7
06/18/2022 03:32:50 - INFO - __main__ - Global step 100 Train loss 3.84 Classification-F1 0.05878059840612136 on epoch=7
06/18/2022 03:32:50 - INFO - __main__ - Saving model with best Classification-F1: 0.021315904139433548 -> 0.05878059840612136 on epoch=7, global_step=100
06/18/2022 03:32:53 - INFO - __main__ - Step 110 Global step 110 Train loss 3.34 on epoch=7
06/18/2022 03:32:56 - INFO - __main__ - Step 120 Global step 120 Train loss 3.20 on epoch=8
06/18/2022 03:32:58 - INFO - __main__ - Step 130 Global step 130 Train loss 2.96 on epoch=9
06/18/2022 03:33:01 - INFO - __main__ - Step 140 Global step 140 Train loss 2.76 on epoch=9
06/18/2022 03:33:03 - INFO - __main__ - Step 150 Global step 150 Train loss 2.51 on epoch=10
06/18/2022 03:33:09 - INFO - __main__ - Global step 150 Train loss 2.95 Classification-F1 0.08421485795406741 on epoch=10
06/18/2022 03:33:09 - INFO - __main__ - Saving model with best Classification-F1: 0.05878059840612136 -> 0.08421485795406741 on epoch=10, global_step=150
06/18/2022 03:33:11 - INFO - __main__ - Step 160 Global step 160 Train loss 2.31 on epoch=11
06/18/2022 03:33:14 - INFO - __main__ - Step 170 Global step 170 Train loss 2.33 on epoch=12
06/18/2022 03:33:17 - INFO - __main__ - Step 180 Global step 180 Train loss 2.34 on epoch=12
06/18/2022 03:33:19 - INFO - __main__ - Step 190 Global step 190 Train loss 2.01 on epoch=13
06/18/2022 03:33:22 - INFO - __main__ - Step 200 Global step 200 Train loss 2.02 on epoch=14
06/18/2022 03:33:27 - INFO - __main__ - Global step 200 Train loss 2.20 Classification-F1 0.10155534837181113 on epoch=14
06/18/2022 03:33:28 - INFO - __main__ - Saving model with best Classification-F1: 0.08421485795406741 -> 0.10155534837181113 on epoch=14, global_step=200
06/18/2022 03:33:30 - INFO - __main__ - Step 210 Global step 210 Train loss 1.91 on epoch=14
06/18/2022 03:33:33 - INFO - __main__ - Step 220 Global step 220 Train loss 2.02 on epoch=15
06/18/2022 03:33:35 - INFO - __main__ - Step 230 Global step 230 Train loss 1.81 on epoch=16
06/18/2022 03:33:38 - INFO - __main__ - Step 240 Global step 240 Train loss 1.80 on epoch=17
06/18/2022 03:33:41 - INFO - __main__ - Step 250 Global step 250 Train loss 1.73 on epoch=17
06/18/2022 03:33:46 - INFO - __main__ - Global step 250 Train loss 1.85 Classification-F1 0.10797936003266487 on epoch=17
06/18/2022 03:33:46 - INFO - __main__ - Saving model with best Classification-F1: 0.10155534837181113 -> 0.10797936003266487 on epoch=17, global_step=250
06/18/2022 03:33:49 - INFO - __main__ - Step 260 Global step 260 Train loss 1.61 on epoch=18
06/18/2022 03:33:52 - INFO - __main__ - Step 270 Global step 270 Train loss 1.52 on epoch=19
06/18/2022 03:33:54 - INFO - __main__ - Step 280 Global step 280 Train loss 1.52 on epoch=19
06/18/2022 03:33:57 - INFO - __main__ - Step 290 Global step 290 Train loss 1.54 on epoch=20
06/18/2022 03:33:59 - INFO - __main__ - Step 300 Global step 300 Train loss 1.32 on epoch=21
06/18/2022 03:34:05 - INFO - __main__ - Global step 300 Train loss 1.50 Classification-F1 0.14671805714933345 on epoch=21
06/18/2022 03:34:05 - INFO - __main__ - Saving model with best Classification-F1: 0.10797936003266487 -> 0.14671805714933345 on epoch=21, global_step=300
06/18/2022 03:34:08 - INFO - __main__ - Step 310 Global step 310 Train loss 1.35 on epoch=22
06/18/2022 03:34:11 - INFO - __main__ - Step 320 Global step 320 Train loss 1.32 on epoch=22
06/18/2022 03:34:13 - INFO - __main__ - Step 330 Global step 330 Train loss 1.19 on epoch=23
06/18/2022 03:34:16 - INFO - __main__ - Step 340 Global step 340 Train loss 1.26 on epoch=24
06/18/2022 03:34:18 - INFO - __main__ - Step 350 Global step 350 Train loss 1.20 on epoch=24
06/18/2022 03:34:24 - INFO - __main__ - Global step 350 Train loss 1.26 Classification-F1 0.22721709267163812 on epoch=24
06/18/2022 03:34:24 - INFO - __main__ - Saving model with best Classification-F1: 0.14671805714933345 -> 0.22721709267163812 on epoch=24, global_step=350
06/18/2022 03:34:27 - INFO - __main__ - Step 360 Global step 360 Train loss 1.10 on epoch=25
06/18/2022 03:34:30 - INFO - __main__ - Step 370 Global step 370 Train loss 1.01 on epoch=26
06/18/2022 03:34:32 - INFO - __main__ - Step 380 Global step 380 Train loss 1.06 on epoch=27
06/18/2022 03:34:35 - INFO - __main__ - Step 390 Global step 390 Train loss 1.08 on epoch=27
06/18/2022 03:34:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.90 on epoch=28
06/18/2022 03:34:44 - INFO - __main__ - Global step 400 Train loss 1.03 Classification-F1 0.32482740626160267 on epoch=28
06/18/2022 03:34:44 - INFO - __main__ - Saving model with best Classification-F1: 0.22721709267163812 -> 0.32482740626160267 on epoch=28, global_step=400
06/18/2022 03:34:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.83 on epoch=29
06/18/2022 03:34:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.92 on epoch=29
06/18/2022 03:34:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.85 on epoch=30
06/18/2022 03:34:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.72 on epoch=31
06/18/2022 03:34:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.87 on epoch=32
06/18/2022 03:35:04 - INFO - __main__ - Global step 450 Train loss 0.84 Classification-F1 0.37469559483571413 on epoch=32
06/18/2022 03:35:04 - INFO - __main__ - Saving model with best Classification-F1: 0.32482740626160267 -> 0.37469559483571413 on epoch=32, global_step=450
06/18/2022 03:35:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.84 on epoch=32
06/18/2022 03:35:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.61 on epoch=33
06/18/2022 03:35:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.73 on epoch=34
06/18/2022 03:35:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.63 on epoch=34
06/18/2022 03:35:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.62 on epoch=35
06/18/2022 03:35:23 - INFO - __main__ - Global step 500 Train loss 0.68 Classification-F1 0.5535030251918297 on epoch=35
06/18/2022 03:35:23 - INFO - __main__ - Saving model with best Classification-F1: 0.37469559483571413 -> 0.5535030251918297 on epoch=35, global_step=500
06/18/2022 03:35:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=36
06/18/2022 03:35:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.61 on epoch=37
06/18/2022 03:35:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=37
06/18/2022 03:35:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=38
06/18/2022 03:35:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=39
06/18/2022 03:35:43 - INFO - __main__ - Global step 550 Train loss 0.57 Classification-F1 0.5783433144945938 on epoch=39
06/18/2022 03:35:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5535030251918297 -> 0.5783433144945938 on epoch=39, global_step=550
06/18/2022 03:35:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.61 on epoch=39
06/18/2022 03:35:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=40
06/18/2022 03:35:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=41
06/18/2022 03:35:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=42
06/18/2022 03:35:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=42
06/18/2022 03:36:03 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.5720438759715912 on epoch=42
06/18/2022 03:36:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=43
06/18/2022 03:36:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=44
06/18/2022 03:36:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=44
06/18/2022 03:36:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=45
06/18/2022 03:36:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=46
06/18/2022 03:36:23 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.614336917562724 on epoch=46
06/18/2022 03:36:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5783433144945938 -> 0.614336917562724 on epoch=46, global_step=650
06/18/2022 03:36:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=47
06/18/2022 03:36:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=47
06/18/2022 03:36:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=48
06/18/2022 03:36:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=49
06/18/2022 03:36:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=49
06/18/2022 03:36:43 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.6113578178094308 on epoch=49
06/18/2022 03:36:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=50
06/18/2022 03:36:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.29 on epoch=51
06/18/2022 03:36:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=52
06/18/2022 03:36:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=52
06/18/2022 03:36:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=53
06/18/2022 03:37:03 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.6113578178094308 on epoch=53
06/18/2022 03:37:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=54
06/18/2022 03:37:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=54
06/18/2022 03:37:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.27 on epoch=55
06/18/2022 03:37:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.29 on epoch=56
06/18/2022 03:37:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=57
06/18/2022 03:37:23 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.614336917562724 on epoch=57
06/18/2022 03:37:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=57
06/18/2022 03:37:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=58
06/18/2022 03:37:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=59
06/18/2022 03:37:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=59
06/18/2022 03:37:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=60
06/18/2022 03:37:43 - INFO - __main__ - Global step 850 Train loss 0.35 Classification-F1 0.614336917562724 on epoch=60
06/18/2022 03:37:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.32 on epoch=61
06/18/2022 03:37:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.32 on epoch=62
06/18/2022 03:37:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.31 on epoch=62
06/18/2022 03:37:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=63
06/18/2022 03:37:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.34 on epoch=64
06/18/2022 03:38:02 - INFO - __main__ - Global step 900 Train loss 0.31 Classification-F1 0.646774193548387 on epoch=64
06/18/2022 03:38:02 - INFO - __main__ - Saving model with best Classification-F1: 0.614336917562724 -> 0.646774193548387 on epoch=64, global_step=900
06/18/2022 03:38:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.28 on epoch=64
06/18/2022 03:38:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=65
06/18/2022 03:38:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=66
06/18/2022 03:38:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=67
06/18/2022 03:38:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=67
06/18/2022 03:38:22 - INFO - __main__ - Global step 950 Train loss 0.26 Classification-F1 0.6159754224270353 on epoch=67
06/18/2022 03:38:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=68
06/18/2022 03:38:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=69
06/18/2022 03:38:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=69
06/18/2022 03:38:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=70
06/18/2022 03:38:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=71
06/18/2022 03:38:42 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.6159754224270353 on epoch=71
06/18/2022 03:38:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=72
06/18/2022 03:38:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.31 on epoch=72
06/18/2022 03:38:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=73
06/18/2022 03:38:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=74
06/18/2022 03:38:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=74
06/18/2022 03:39:01 - INFO - __main__ - Global step 1050 Train loss 0.25 Classification-F1 0.6115533212307407 on epoch=74
06/18/2022 03:39:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=75
06/18/2022 03:39:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=76
06/18/2022 03:39:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=77
06/18/2022 03:39:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=77
06/18/2022 03:39:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=78
06/18/2022 03:39:21 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.6099996988042529 on epoch=78
06/18/2022 03:39:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=79
06/18/2022 03:39:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.17 on epoch=79
06/18/2022 03:39:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=80
06/18/2022 03:39:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=81
06/18/2022 03:39:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=82
06/18/2022 03:39:41 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.613174301978856 on epoch=82
06/18/2022 03:39:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=82
06/18/2022 03:39:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.27 on epoch=83
06/18/2022 03:39:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=84
06/18/2022 03:39:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=84
06/18/2022 03:39:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=85
06/18/2022 03:40:00 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.613174301978856 on epoch=85
06/18/2022 03:40:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=86
06/18/2022 03:40:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=87
06/18/2022 03:40:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=87
06/18/2022 03:40:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=88
06/18/2022 03:40:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=89
06/18/2022 03:40:20 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.6825127334465195 on epoch=89
06/18/2022 03:40:20 - INFO - __main__ - Saving model with best Classification-F1: 0.646774193548387 -> 0.6825127334465195 on epoch=89, global_step=1250
06/18/2022 03:40:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.22 on epoch=89
06/18/2022 03:40:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.18 on epoch=90
06/18/2022 03:40:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=91
06/18/2022 03:40:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=92
06/18/2022 03:40:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=92
06/18/2022 03:40:39 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.7205387205387205 on epoch=92
06/18/2022 03:40:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6825127334465195 -> 0.7205387205387205 on epoch=92, global_step=1300
06/18/2022 03:40:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=93
06/18/2022 03:40:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=94
06/18/2022 03:40:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=94
06/18/2022 03:40:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=95
06/18/2022 03:40:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=96
06/18/2022 03:40:59 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.7287581699346405 on epoch=96
06/18/2022 03:40:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7205387205387205 -> 0.7287581699346405 on epoch=96, global_step=1350
06/18/2022 03:41:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=97
06/18/2022 03:41:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=97
06/18/2022 03:41:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=98
06/18/2022 03:41:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=99
06/18/2022 03:41:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=99
06/18/2022 03:41:18 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.71874660584338 on epoch=99
06/18/2022 03:41:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=100
06/18/2022 03:41:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=101
06/18/2022 03:41:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=102
06/18/2022 03:41:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=102
06/18/2022 03:41:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=103
06/18/2022 03:41:38 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.6512391466850669 on epoch=103
06/18/2022 03:41:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=104
06/18/2022 03:41:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=104
06/18/2022 03:41:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=105
06/18/2022 03:41:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=106
06/18/2022 03:41:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=107
06/18/2022 03:41:57 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.7345679012345678 on epoch=107
06/18/2022 03:41:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7287581699346405 -> 0.7345679012345678 on epoch=107, global_step=1500
06/18/2022 03:42:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=107
06/18/2022 03:42:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=108
06/18/2022 03:42:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=109
06/18/2022 03:42:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=109
06/18/2022 03:42:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=110
06/18/2022 03:42:16 - INFO - __main__ - Global step 1550 Train loss 0.14 Classification-F1 0.7287581699346405 on epoch=110
06/18/2022 03:42:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=111
06/18/2022 03:42:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=112
06/18/2022 03:42:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=112
06/18/2022 03:42:27 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=113
06/18/2022 03:42:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.11 on epoch=114
06/18/2022 03:42:36 - INFO - __main__ - Global step 1600 Train loss 0.12 Classification-F1 0.7777777777777777 on epoch=114
06/18/2022 03:42:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7345679012345678 -> 0.7777777777777777 on epoch=114, global_step=1600
06/18/2022 03:42:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=114
06/18/2022 03:42:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=115
06/18/2022 03:42:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=116
06/18/2022 03:42:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=117
06/18/2022 03:42:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=117
06/18/2022 03:42:55 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.7778191381507071 on epoch=117
06/18/2022 03:42:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7777777777777777 -> 0.7778191381507071 on epoch=117, global_step=1650
06/18/2022 03:42:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=118
06/18/2022 03:43:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=119
06/18/2022 03:43:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=119
06/18/2022 03:43:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=120
06/18/2022 03:43:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=121
06/18/2022 03:43:14 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.9010554351367815 on epoch=121
06/18/2022 03:43:14 - INFO - __main__ - Saving model with best Classification-F1: 0.7778191381507071 -> 0.9010554351367815 on epoch=121, global_step=1700
06/18/2022 03:43:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=122
06/18/2022 03:43:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=122
06/18/2022 03:43:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=123
06/18/2022 03:43:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=124
06/18/2022 03:43:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=124
06/18/2022 03:43:34 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.8043771535232636 on epoch=124
06/18/2022 03:43:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=125
06/18/2022 03:43:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.12 on epoch=126
06/18/2022 03:43:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=127
06/18/2022 03:43:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=127
06/18/2022 03:43:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=128
06/18/2022 03:43:53 - INFO - __main__ - Global step 1800 Train loss 0.10 Classification-F1 0.7574671445639187 on epoch=128
06/18/2022 03:43:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=129
06/18/2022 03:43:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=129
06/18/2022 03:44:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=130
06/18/2022 03:44:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=131
06/18/2022 03:44:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=132
06/18/2022 03:44:13 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.7656975972388158 on epoch=132
06/18/2022 03:44:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=132
06/18/2022 03:44:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=133
06/18/2022 03:44:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=134
06/18/2022 03:44:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=134
06/18/2022 03:44:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=135
06/18/2022 03:44:32 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.7229985634060337 on epoch=135
06/18/2022 03:44:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=136
06/18/2022 03:44:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=137
06/18/2022 03:44:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=137
06/18/2022 03:44:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=138
06/18/2022 03:44:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=139
06/18/2022 03:44:51 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.813228517213337 on epoch=139
06/18/2022 03:44:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=139
06/18/2022 03:44:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=140
06/18/2022 03:44:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=141
06/18/2022 03:45:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=142
06/18/2022 03:45:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=142
06/18/2022 03:45:10 - INFO - __main__ - Global step 2000 Train loss 0.10 Classification-F1 0.6820001503872473 on epoch=142
06/18/2022 03:45:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=143
06/18/2022 03:45:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=144
06/18/2022 03:45:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.10 on epoch=144
06/18/2022 03:45:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=145
06/18/2022 03:45:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.10 on epoch=146
06/18/2022 03:45:29 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.8156862745098039 on epoch=146
06/18/2022 03:45:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.08 on epoch=147
06/18/2022 03:45:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=147
06/18/2022 03:45:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.08 on epoch=148
06/18/2022 03:45:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=149
06/18/2022 03:45:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=149
06/18/2022 03:45:48 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.8624738766980147 on epoch=149
06/18/2022 03:45:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=150
06/18/2022 03:45:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=151
06/18/2022 03:45:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=152
06/18/2022 03:45:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=152
06/18/2022 03:46:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=153
06/18/2022 03:46:07 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.8107386323705109 on epoch=153
06/18/2022 03:46:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=154
06/18/2022 03:46:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=154
06/18/2022 03:46:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=155
06/18/2022 03:46:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=156
06/18/2022 03:46:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=157
06/18/2022 03:46:26 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.8177103099304238 on epoch=157
06/18/2022 03:46:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=157
06/18/2022 03:46:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=158
06/18/2022 03:46:34 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=159
06/18/2022 03:46:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.08 on epoch=159
06/18/2022 03:46:39 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=160
06/18/2022 03:46:45 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.858560794044665 on epoch=160
06/18/2022 03:46:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=161
06/18/2022 03:46:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.14 on epoch=162
06/18/2022 03:46:53 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=162
06/18/2022 03:46:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=163
06/18/2022 03:46:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=164
06/18/2022 03:47:04 - INFO - __main__ - Global step 2300 Train loss 0.08 Classification-F1 0.9180707685373 on epoch=164
06/18/2022 03:47:04 - INFO - __main__ - Saving model with best Classification-F1: 0.9010554351367815 -> 0.9180707685373 on epoch=164, global_step=2300
06/18/2022 03:47:07 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=164
06/18/2022 03:47:09 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.07 on epoch=165
06/18/2022 03:47:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.07 on epoch=166
06/18/2022 03:47:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=167
06/18/2022 03:47:17 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=167
06/18/2022 03:47:23 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.813228517213337 on epoch=167
06/18/2022 03:47:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.12 on epoch=168
06/18/2022 03:47:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=169
06/18/2022 03:47:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.09 on epoch=169
06/18/2022 03:47:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=170
06/18/2022 03:47:36 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=171
06/18/2022 03:47:42 - INFO - __main__ - Global step 2400 Train loss 0.09 Classification-F1 0.8585638082718172 on epoch=171
06/18/2022 03:47:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=172
06/18/2022 03:47:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=172
06/18/2022 03:47:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.09 on epoch=173
06/18/2022 03:47:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.13 on epoch=174
06/18/2022 03:47:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=174
06/18/2022 03:48:01 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.8080600548440633 on epoch=174
06/18/2022 03:48:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=175
06/18/2022 03:48:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=176
06/18/2022 03:48:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=177
06/18/2022 03:48:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=177
06/18/2022 03:48:14 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=178
06/18/2022 03:48:20 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.8069320657555953 on epoch=178
06/18/2022 03:48:23 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=179
06/18/2022 03:48:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=179
06/18/2022 03:48:28 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=180
06/18/2022 03:48:31 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=181
06/18/2022 03:48:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=182
06/18/2022 03:48:39 - INFO - __main__ - Global step 2550 Train loss 0.08 Classification-F1 0.8080600548440633 on epoch=182
06/18/2022 03:48:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=182
06/18/2022 03:48:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=183
06/18/2022 03:48:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=184
06/18/2022 03:48:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=184
06/18/2022 03:48:52 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=185
06/18/2022 03:48:59 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.7195926880137407 on epoch=185
06/18/2022 03:49:01 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=186
06/18/2022 03:49:04 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=187
06/18/2022 03:49:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=187
06/18/2022 03:49:09 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=188
06/18/2022 03:49:12 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=189
06/18/2022 03:49:18 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.7170015948963318 on epoch=189
06/18/2022 03:49:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=189
06/18/2022 03:49:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.13 on epoch=190
06/18/2022 03:49:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=191
06/18/2022 03:49:28 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=192
06/18/2022 03:49:31 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=192
06/18/2022 03:49:37 - INFO - __main__ - Global step 2700 Train loss 0.08 Classification-F1 0.7621025065469511 on epoch=192
06/18/2022 03:49:40 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=193
06/18/2022 03:49:42 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=194
06/18/2022 03:49:45 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=194
06/18/2022 03:49:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=195
06/18/2022 03:49:50 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=196
06/18/2022 03:49:56 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.7209269508081053 on epoch=196
06/18/2022 03:49:59 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=197
06/18/2022 03:50:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=197
06/18/2022 03:50:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=198
06/18/2022 03:50:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=199
06/18/2022 03:50:09 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=199
06/18/2022 03:50:15 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.7666434459537909 on epoch=199
06/18/2022 03:50:18 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=200
06/18/2022 03:50:20 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=201
06/18/2022 03:50:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=202
06/18/2022 03:50:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=202
06/18/2022 03:50:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=203
06/18/2022 03:50:34 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.813903743315508 on epoch=203
06/18/2022 03:50:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=204
06/18/2022 03:50:40 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=204
06/18/2022 03:50:42 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=205
06/18/2022 03:50:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=206
06/18/2022 03:50:47 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=207
06/18/2022 03:50:54 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.813903743315508 on epoch=207
06/18/2022 03:50:56 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=207
06/18/2022 03:50:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=208
06/18/2022 03:51:02 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=209
06/18/2022 03:51:04 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=209
06/18/2022 03:51:07 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=210
06/18/2022 03:51:13 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.8069320657555953 on epoch=210
06/18/2022 03:51:16 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=211
06/18/2022 03:51:18 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=212
06/18/2022 03:51:21 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=212
06/18/2022 03:51:23 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=213
06/18/2022 03:51:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=214
06/18/2022 03:51:27 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 03:51:27 - INFO - __main__ - Printing 3 examples
06/18/2022 03:51:27 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
06/18/2022 03:51:27 - INFO - __main__ - ['Film']
06/18/2022 03:51:27 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
06/18/2022 03:51:27 - INFO - __main__ - ['Film']
06/18/2022 03:51:27 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
06/18/2022 03:51:27 - INFO - __main__ - ['Film']
06/18/2022 03:51:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 03:51:27 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:51:28 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 03:51:28 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 03:51:28 - INFO - __main__ - Printing 3 examples
06/18/2022 03:51:28 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
06/18/2022 03:51:28 - INFO - __main__ - ['Film']
06/18/2022 03:51:28 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
06/18/2022 03:51:28 - INFO - __main__ - ['Film']
06/18/2022 03:51:28 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
06/18/2022 03:51:28 - INFO - __main__ - ['Film']
06/18/2022 03:51:28 - INFO - __main__ - Tokenizing Input ...
06/18/2022 03:51:28 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:51:28 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 03:51:32 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.8624738766980147 on epoch=214
06/18/2022 03:51:32 - INFO - __main__ - save last model!
06/18/2022 03:51:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 03:51:32 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 03:51:32 - INFO - __main__ - Printing 3 examples
06/18/2022 03:51:32 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 03:51:32 - INFO - __main__ - ['Animal']
06/18/2022 03:51:32 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 03:51:32 - INFO - __main__ - ['Animal']
06/18/2022 03:51:32 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 03:51:32 - INFO - __main__ - ['Village']
06/18/2022 03:51:32 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 03:51:34 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:51:38 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 03:51:47 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 03:51:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 03:51:48 - INFO - __main__ - Starting training!
06/18/2022 03:53:48 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_42_0.2_8_predictions.txt
06/18/2022 03:53:48 - INFO - __main__ - Classification-F1 on test data: 0.5215
06/18/2022 03:53:48 - INFO - __main__ - prefix=dbpedia_14_16_42, lr=0.2, bsz=8, dev_performance=0.9180707685373, test_performance=0.5215368294006367
06/18/2022 03:53:48 - INFO - __main__ - Running ... prefix=dbpedia_14_16_87, lr=0.5, bsz=8 ...
06/18/2022 03:53:49 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 03:53:49 - INFO - __main__ - Printing 3 examples
06/18/2022 03:53:49 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
06/18/2022 03:53:49 - INFO - __main__ - ['Film']
06/18/2022 03:53:49 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
06/18/2022 03:53:49 - INFO - __main__ - ['Film']
06/18/2022 03:53:49 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
06/18/2022 03:53:49 - INFO - __main__ - ['Film']
06/18/2022 03:53:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 03:53:49 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:53:49 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 03:53:49 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 03:53:49 - INFO - __main__ - Printing 3 examples
06/18/2022 03:53:49 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
06/18/2022 03:53:49 - INFO - __main__ - ['Film']
06/18/2022 03:53:49 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
06/18/2022 03:53:49 - INFO - __main__ - ['Film']
06/18/2022 03:53:49 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
06/18/2022 03:53:49 - INFO - __main__ - ['Film']
06/18/2022 03:53:49 - INFO - __main__ - Tokenizing Input ...
06/18/2022 03:53:49 - INFO - __main__ - Tokenizing Output ...
06/18/2022 03:53:50 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 03:54:05 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 03:54:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 03:54:06 - INFO - __main__ - Starting training!
06/18/2022 03:54:09 - INFO - __main__ - Step 10 Global step 10 Train loss 6.59 on epoch=0
06/18/2022 03:54:12 - INFO - __main__ - Step 20 Global step 20 Train loss 4.41 on epoch=1
06/18/2022 03:54:15 - INFO - __main__ - Step 30 Global step 30 Train loss 4.17 on epoch=2
06/18/2022 03:54:17 - INFO - __main__ - Step 40 Global step 40 Train loss 3.34 on epoch=2
06/18/2022 03:54:20 - INFO - __main__ - Step 50 Global step 50 Train loss 3.25 on epoch=3
06/18/2022 03:54:26 - INFO - __main__ - Global step 50 Train loss 4.35 Classification-F1 0.09305804930513539 on epoch=3
06/18/2022 03:54:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.09305804930513539 on epoch=3, global_step=50
06/18/2022 03:54:28 - INFO - __main__ - Step 60 Global step 60 Train loss 2.84 on epoch=4
06/18/2022 03:54:31 - INFO - __main__ - Step 70 Global step 70 Train loss 2.26 on epoch=4
06/18/2022 03:54:34 - INFO - __main__ - Step 80 Global step 80 Train loss 2.29 on epoch=5
06/18/2022 03:54:36 - INFO - __main__ - Step 90 Global step 90 Train loss 1.90 on epoch=6
06/18/2022 03:54:39 - INFO - __main__ - Step 100 Global step 100 Train loss 1.77 on epoch=7
06/18/2022 03:54:44 - INFO - __main__ - Global step 100 Train loss 2.21 Classification-F1 0.12863950949204406 on epoch=7
06/18/2022 03:54:44 - INFO - __main__ - Saving model with best Classification-F1: 0.09305804930513539 -> 0.12863950949204406 on epoch=7, global_step=100
06/18/2022 03:54:47 - INFO - __main__ - Step 110 Global step 110 Train loss 1.56 on epoch=7
06/18/2022 03:54:50 - INFO - __main__ - Step 120 Global step 120 Train loss 1.70 on epoch=8
06/18/2022 03:54:52 - INFO - __main__ - Step 130 Global step 130 Train loss 1.44 on epoch=9
06/18/2022 03:54:55 - INFO - __main__ - Step 140 Global step 140 Train loss 1.19 on epoch=9
06/18/2022 03:54:58 - INFO - __main__ - Step 150 Global step 150 Train loss 1.32 on epoch=10
06/18/2022 03:55:04 - INFO - __main__ - Global step 150 Train loss 1.44 Classification-F1 0.30376575938587563 on epoch=10
06/18/2022 03:55:04 - INFO - __main__ - Saving model with best Classification-F1: 0.12863950949204406 -> 0.30376575938587563 on epoch=10, global_step=150
06/18/2022 03:55:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.99 on epoch=11
06/18/2022 03:55:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=12
06/18/2022 03:55:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.85 on epoch=12
06/18/2022 03:55:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.77 on epoch=13
06/18/2022 03:55:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.78 on epoch=14
06/18/2022 03:55:24 - INFO - __main__ - Global step 200 Train loss 0.86 Classification-F1 0.39122881451936964 on epoch=14
06/18/2022 03:55:24 - INFO - __main__ - Saving model with best Classification-F1: 0.30376575938587563 -> 0.39122881451936964 on epoch=14, global_step=200
06/18/2022 03:55:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.58 on epoch=14
06/18/2022 03:55:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.66 on epoch=15
06/18/2022 03:55:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=16
06/18/2022 03:55:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.53 on epoch=17
06/18/2022 03:55:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=17
06/18/2022 03:55:44 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.6163229409470331 on epoch=17
06/18/2022 03:55:44 - INFO - __main__ - Saving model with best Classification-F1: 0.39122881451936964 -> 0.6163229409470331 on epoch=17, global_step=250
06/18/2022 03:55:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=18
06/18/2022 03:55:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=19
06/18/2022 03:55:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=19
06/18/2022 03:55:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=20
06/18/2022 03:55:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.35 on epoch=21
06/18/2022 03:56:04 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.7098411700852595 on epoch=21
06/18/2022 03:56:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6163229409470331 -> 0.7098411700852595 on epoch=21, global_step=300
06/18/2022 03:56:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=22
06/18/2022 03:56:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=22
06/18/2022 03:56:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=23
06/18/2022 03:56:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=24
06/18/2022 03:56:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.32 on epoch=24
06/18/2022 03:56:24 - INFO - __main__ - Global step 350 Train loss 0.30 Classification-F1 0.7186049623405127 on epoch=24
06/18/2022 03:56:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7098411700852595 -> 0.7186049623405127 on epoch=24, global_step=350
06/18/2022 03:56:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=25
06/18/2022 03:56:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=26
06/18/2022 03:56:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.18 on epoch=27
06/18/2022 03:56:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=27
06/18/2022 03:56:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=28
06/18/2022 03:56:44 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.7963204065122864 on epoch=28
06/18/2022 03:56:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7186049623405127 -> 0.7963204065122864 on epoch=28, global_step=400
06/18/2022 03:56:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=29
06/18/2022 03:56:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.30 on epoch=29
06/18/2022 03:56:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.17 on epoch=30
06/18/2022 03:56:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=31
06/18/2022 03:56:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=32
06/18/2022 03:57:03 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.7730074414233284 on epoch=32
06/18/2022 03:57:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.16 on epoch=32
06/18/2022 03:57:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=33
06/18/2022 03:57:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.15 on epoch=34
06/18/2022 03:57:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=34
06/18/2022 03:57:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=35
06/18/2022 03:57:23 - INFO - __main__ - Global step 500 Train loss 0.16 Classification-F1 0.7343788764841397 on epoch=35
06/18/2022 03:57:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.17 on epoch=36
06/18/2022 03:57:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.15 on epoch=37
06/18/2022 03:57:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=37
06/18/2022 03:57:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=38
06/18/2022 03:57:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.12 on epoch=39
06/18/2022 03:57:42 - INFO - __main__ - Global step 550 Train loss 0.14 Classification-F1 0.7188528762216302 on epoch=39
06/18/2022 03:57:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.13 on epoch=39
06/18/2022 03:57:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.09 on epoch=40
06/18/2022 03:57:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.14 on epoch=41
06/18/2022 03:57:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.12 on epoch=42
06/18/2022 03:57:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=42
06/18/2022 03:58:02 - INFO - __main__ - Global step 600 Train loss 0.12 Classification-F1 0.8222661676184597 on epoch=42
06/18/2022 03:58:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7963204065122864 -> 0.8222661676184597 on epoch=42, global_step=600
06/18/2022 03:58:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.08 on epoch=43
06/18/2022 03:58:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=44
06/18/2022 03:58:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.12 on epoch=44
06/18/2022 03:58:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=45
06/18/2022 03:58:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=46
06/18/2022 03:58:21 - INFO - __main__ - Global step 650 Train loss 0.12 Classification-F1 0.8322912302751013 on epoch=46
06/18/2022 03:58:21 - INFO - __main__ - Saving model with best Classification-F1: 0.8222661676184597 -> 0.8322912302751013 on epoch=46, global_step=650
06/18/2022 03:58:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.09 on epoch=47
06/18/2022 03:58:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=47
06/18/2022 03:58:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=48
06/18/2022 03:58:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=49
06/18/2022 03:58:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=49
06/18/2022 03:58:40 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.8859004794488666 on epoch=49
06/18/2022 03:58:40 - INFO - __main__ - Saving model with best Classification-F1: 0.8322912302751013 -> 0.8859004794488666 on epoch=49, global_step=700
06/18/2022 03:58:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=50
06/18/2022 03:58:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=51
06/18/2022 03:58:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=52
06/18/2022 03:58:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=52
06/18/2022 03:58:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=53
06/18/2022 03:58:59 - INFO - __main__ - Global step 750 Train loss 0.09 Classification-F1 0.764818403929912 on epoch=53
06/18/2022 03:59:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=54
06/18/2022 03:59:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=54
06/18/2022 03:59:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=55
06/18/2022 03:59:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=56
06/18/2022 03:59:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=57
06/18/2022 03:59:19 - INFO - __main__ - Global step 800 Train loss 0.08 Classification-F1 0.752563030964153 on epoch=57
06/18/2022 03:59:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=57
06/18/2022 03:59:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=58
06/18/2022 03:59:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=59
06/18/2022 03:59:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=59
06/18/2022 03:59:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=60
06/18/2022 03:59:38 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.9126461750117663 on epoch=60
06/18/2022 03:59:38 - INFO - __main__ - Saving model with best Classification-F1: 0.8859004794488666 -> 0.9126461750117663 on epoch=60, global_step=850
06/18/2022 03:59:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=61
06/18/2022 03:59:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=62
06/18/2022 03:59:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=62
06/18/2022 03:59:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=63
06/18/2022 03:59:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=64
06/18/2022 03:59:57 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.8437536656891496 on epoch=64
06/18/2022 04:00:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=64
06/18/2022 04:00:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=65
06/18/2022 04:00:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=66
06/18/2022 04:00:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=67
06/18/2022 04:00:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=67
06/18/2022 04:00:16 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.9205474095796676 on epoch=67
06/18/2022 04:00:16 - INFO - __main__ - Saving model with best Classification-F1: 0.9126461750117663 -> 0.9205474095796676 on epoch=67, global_step=950
06/18/2022 04:00:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=68
06/18/2022 04:00:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=69
06/18/2022 04:00:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=69
06/18/2022 04:00:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=70
06/18/2022 04:00:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=71
06/18/2022 04:00:35 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.9186705767350929 on epoch=71
06/18/2022 04:00:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=72
06/18/2022 04:00:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.10 on epoch=72
06/18/2022 04:00:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=73
06/18/2022 04:00:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=74
06/18/2022 04:00:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=74
06/18/2022 04:00:54 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.7417260159195643 on epoch=74
06/18/2022 04:00:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=75
06/18/2022 04:01:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=76
06/18/2022 04:01:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=77
06/18/2022 04:01:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=77
06/18/2022 04:01:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=78
06/18/2022 04:01:14 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.9186460429724188 on epoch=78
06/18/2022 04:01:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=79
06/18/2022 04:01:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=79
06/18/2022 04:01:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=80
06/18/2022 04:01:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=81
06/18/2022 04:01:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=82
06/18/2022 04:01:33 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.9144012186058905 on epoch=82
06/18/2022 04:01:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=82
06/18/2022 04:01:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=83
06/18/2022 04:01:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=84
06/18/2022 04:01:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=84
06/18/2022 04:01:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=85
06/18/2022 04:01:52 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.9820991153059465 on epoch=85
06/18/2022 04:01:52 - INFO - __main__ - Saving model with best Classification-F1: 0.9205474095796676 -> 0.9820991153059465 on epoch=85, global_step=1200
06/18/2022 04:01:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=86
06/18/2022 04:01:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=87
06/18/2022 04:02:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=87
06/18/2022 04:02:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=88
06/18/2022 04:02:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=89
06/18/2022 04:02:12 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.9865677649358864 on epoch=89
06/18/2022 04:02:12 - INFO - __main__ - Saving model with best Classification-F1: 0.9820991153059465 -> 0.9865677649358864 on epoch=89, global_step=1250
06/18/2022 04:02:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=89
06/18/2022 04:02:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=90
06/18/2022 04:02:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=91
06/18/2022 04:02:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=92
06/18/2022 04:02:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=92
06/18/2022 04:02:32 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.9123247656833996 on epoch=92
06/18/2022 04:02:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=93
06/18/2022 04:02:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=94
06/18/2022 04:02:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=94
06/18/2022 04:02:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=95
06/18/2022 04:02:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=96
06/18/2022 04:02:52 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.859103128054741 on epoch=96
06/18/2022 04:02:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=97
06/18/2022 04:02:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=97
06/18/2022 04:03:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=98
06/18/2022 04:03:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=99
06/18/2022 04:03:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=99
06/18/2022 04:03:12 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.9120231960381148 on epoch=99
06/18/2022 04:03:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=100
06/18/2022 04:03:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=101
06/18/2022 04:03:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=102
06/18/2022 04:03:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=102
06/18/2022 04:03:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=103
06/18/2022 04:03:32 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.914360540892799 on epoch=103
06/18/2022 04:03:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=104
06/18/2022 04:03:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=104
06/18/2022 04:03:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=105
06/18/2022 04:03:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=106
06/18/2022 04:03:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=107
06/18/2022 04:03:51 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.8964315629638212 on epoch=107
06/18/2022 04:03:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=107
06/18/2022 04:03:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=108
06/18/2022 04:03:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=109
06/18/2022 04:04:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=109
06/18/2022 04:04:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=110
06/18/2022 04:04:11 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.9182256379141808 on epoch=110
06/18/2022 04:04:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=111
06/18/2022 04:04:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=112
06/18/2022 04:04:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=112
06/18/2022 04:04:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=113
06/18/2022 04:04:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=114
06/18/2022 04:04:31 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.8377953130627154 on epoch=114
06/18/2022 04:04:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=114
06/18/2022 04:04:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=115
06/18/2022 04:04:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=116
06/18/2022 04:04:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=117
06/18/2022 04:04:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=117
06/18/2022 04:04:51 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.8964274899758773 on epoch=117
06/18/2022 04:04:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=118
06/18/2022 04:04:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=119
06/18/2022 04:04:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=119
06/18/2022 04:05:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=120
06/18/2022 04:05:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=121
06/18/2022 04:05:11 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.8977060842383423 on epoch=121
06/18/2022 04:05:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=122
06/18/2022 04:05:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=122
06/18/2022 04:05:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=123
06/18/2022 04:05:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=124
06/18/2022 04:05:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=124
06/18/2022 04:05:31 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.8434933933673853 on epoch=124
06/18/2022 04:05:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=125
06/18/2022 04:05:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=126
06/18/2022 04:05:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=127
06/18/2022 04:05:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=127
06/18/2022 04:05:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=128
06/18/2022 04:05:52 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.8421721826560535 on epoch=128
06/18/2022 04:05:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=129
06/18/2022 04:05:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=129
06/18/2022 04:06:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=130
06/18/2022 04:06:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=131
06/18/2022 04:06:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=132
06/18/2022 04:06:12 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.8318944048782758 on epoch=132
06/18/2022 04:06:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=132
06/18/2022 04:06:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=133
06/18/2022 04:06:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=134
06/18/2022 04:06:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=134
06/18/2022 04:06:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=135
06/18/2022 04:06:32 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.9513801610575803 on epoch=135
06/18/2022 04:06:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=136
06/18/2022 04:06:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=137
06/18/2022 04:06:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=137
06/18/2022 04:06:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=138
06/18/2022 04:06:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=139
06/18/2022 04:06:52 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.946948085196933 on epoch=139
06/18/2022 04:06:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=139
06/18/2022 04:06:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=140
06/18/2022 04:07:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=141
06/18/2022 04:07:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=142
06/18/2022 04:07:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=142
06/18/2022 04:07:11 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.9284586381360574 on epoch=142
06/18/2022 04:07:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=143
06/18/2022 04:07:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=144
06/18/2022 04:07:19 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=144
06/18/2022 04:07:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=145
06/18/2022 04:07:24 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=146
06/18/2022 04:07:31 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.9362869378850888 on epoch=146
06/18/2022 04:07:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=147
06/18/2022 04:07:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=147
06/18/2022 04:07:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=148
06/18/2022 04:07:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=149
06/18/2022 04:07:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=149
06/18/2022 04:07:51 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.934873394550814 on epoch=149
06/18/2022 04:07:53 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=150
06/18/2022 04:07:56 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=151
06/18/2022 04:07:58 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=152
06/18/2022 04:08:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=152
06/18/2022 04:08:04 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=153
06/18/2022 04:08:11 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.894781175813434 on epoch=153
06/18/2022 04:08:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=154
06/18/2022 04:08:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=154
06/18/2022 04:08:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=155
06/18/2022 04:08:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=156
06/18/2022 04:08:24 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=157
06/18/2022 04:08:31 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.9633009887678818 on epoch=157
06/18/2022 04:08:34 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=157
06/18/2022 04:08:36 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=158
06/18/2022 04:08:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=159
06/18/2022 04:08:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=159
06/18/2022 04:08:44 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=160
06/18/2022 04:08:51 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.894781175813434 on epoch=160
06/18/2022 04:08:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=161
06/18/2022 04:08:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=162
06/18/2022 04:08:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=162
06/18/2022 04:09:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=163
06/18/2022 04:09:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=164
06/18/2022 04:09:11 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.8379459921798632 on epoch=164
06/18/2022 04:09:14 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=164
06/18/2022 04:09:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=165
06/18/2022 04:09:19 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=166
06/18/2022 04:09:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=167
06/18/2022 04:09:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=167
06/18/2022 04:09:32 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.8367565119889597 on epoch=167
06/18/2022 04:09:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=168
06/18/2022 04:09:37 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=169
06/18/2022 04:09:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=169
06/18/2022 04:09:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=170
06/18/2022 04:09:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=171
06/18/2022 04:09:52 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.8502486462163881 on epoch=171
06/18/2022 04:09:55 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=172
06/18/2022 04:09:57 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=172
06/18/2022 04:10:00 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=173
06/18/2022 04:10:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=174
06/18/2022 04:10:05 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=174
06/18/2022 04:10:13 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.8916414051897924 on epoch=174
06/18/2022 04:10:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=175
06/18/2022 04:10:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=176
06/18/2022 04:10:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=177
06/18/2022 04:10:23 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
06/18/2022 04:10:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=178
06/18/2022 04:10:33 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.8958121448444027 on epoch=178
06/18/2022 04:10:36 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=179
06/18/2022 04:10:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=179
06/18/2022 04:10:41 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=180
06/18/2022 04:10:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=181
06/18/2022 04:10:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=182
06/18/2022 04:10:53 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.9425641541391067 on epoch=182
06/18/2022 04:10:56 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
06/18/2022 04:10:58 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
06/18/2022 04:11:01 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=184
06/18/2022 04:11:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
06/18/2022 04:11:06 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=185
06/18/2022 04:11:13 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.9632655277816566 on epoch=185
06/18/2022 04:11:16 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=186
06/18/2022 04:11:19 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=187
06/18/2022 04:11:21 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=187
06/18/2022 04:11:24 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=188
06/18/2022 04:11:26 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=189
06/18/2022 04:11:33 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.972843069627487 on epoch=189
06/18/2022 04:11:36 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=189
06/18/2022 04:11:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=190
06/18/2022 04:11:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=191
06/18/2022 04:11:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
06/18/2022 04:11:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=192
06/18/2022 04:11:54 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.9776567518503005 on epoch=192
06/18/2022 04:11:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=193
06/18/2022 04:11:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=194
06/18/2022 04:12:02 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
06/18/2022 04:12:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=195
06/18/2022 04:12:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=196
06/18/2022 04:12:14 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.9730388563049855 on epoch=196
06/18/2022 04:12:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=197
06/18/2022 04:12:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=197
06/18/2022 04:12:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
06/18/2022 04:12:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=199
06/18/2022 04:12:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=199
06/18/2022 04:12:34 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.9008597830356211 on epoch=199
06/18/2022 04:12:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=200
06/18/2022 04:12:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=201
06/18/2022 04:12:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=202
06/18/2022 04:12:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
06/18/2022 04:12:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=203
06/18/2022 04:12:54 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.9140548982595701 on epoch=203
06/18/2022 04:12:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=204
06/18/2022 04:13:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=204
06/18/2022 04:13:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=205
06/18/2022 04:13:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=206
06/18/2022 04:13:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=207
06/18/2022 04:13:15 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.8588204065122864 on epoch=207
06/18/2022 04:13:17 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=207
06/18/2022 04:13:20 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=208
06/18/2022 04:13:22 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
06/18/2022 04:13:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
06/18/2022 04:13:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=210
06/18/2022 04:13:36 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.9865940511101802 on epoch=210
06/18/2022 04:13:36 - INFO - __main__ - Saving model with best Classification-F1: 0.9865677649358864 -> 0.9865940511101802 on epoch=210, global_step=2950
06/18/2022 04:13:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=211
06/18/2022 04:13:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=212
06/18/2022 04:13:44 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=212
06/18/2022 04:13:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=213
06/18/2022 04:13:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=214
06/18/2022 04:13:50 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 04:13:50 - INFO - __main__ - Printing 3 examples
06/18/2022 04:13:50 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
06/18/2022 04:13:50 - INFO - __main__ - ['Film']
06/18/2022 04:13:50 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
06/18/2022 04:13:50 - INFO - __main__ - ['Film']
06/18/2022 04:13:50 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
06/18/2022 04:13:50 - INFO - __main__ - ['Film']
06/18/2022 04:13:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 04:13:50 - INFO - __main__ - Tokenizing Output ...
06/18/2022 04:13:51 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 04:13:51 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 04:13:51 - INFO - __main__ - Printing 3 examples
06/18/2022 04:13:51 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
06/18/2022 04:13:51 - INFO - __main__ - ['Film']
06/18/2022 04:13:51 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
06/18/2022 04:13:51 - INFO - __main__ - ['Film']
06/18/2022 04:13:51 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
06/18/2022 04:13:51 - INFO - __main__ - ['Film']
06/18/2022 04:13:51 - INFO - __main__ - Tokenizing Input ...
06/18/2022 04:13:51 - INFO - __main__ - Tokenizing Output ...
06/18/2022 04:13:51 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 04:13:55 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.9185272075594656 on epoch=214
06/18/2022 04:13:55 - INFO - __main__ - save last model!
06/18/2022 04:13:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 04:13:55 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 04:13:55 - INFO - __main__ - Printing 3 examples
06/18/2022 04:13:55 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 04:13:55 - INFO - __main__ - ['Animal']
06/18/2022 04:13:55 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 04:13:55 - INFO - __main__ - ['Animal']
06/18/2022 04:13:55 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 04:13:55 - INFO - __main__ - ['Village']
06/18/2022 04:13:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 04:13:57 - INFO - __main__ - Tokenizing Output ...
06/18/2022 04:14:01 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 04:14:07 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 04:14:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 04:14:07 - INFO - __main__ - Starting training!
06/18/2022 04:16:16 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_87_0.5_8_predictions.txt
06/18/2022 04:16:16 - INFO - __main__ - Classification-F1 on test data: 0.5255
06/18/2022 04:16:17 - INFO - __main__ - prefix=dbpedia_14_16_87, lr=0.5, bsz=8, dev_performance=0.9865940511101802, test_performance=0.5254862719657365
06/18/2022 04:16:17 - INFO - __main__ - Running ... prefix=dbpedia_14_16_87, lr=0.4, bsz=8 ...
06/18/2022 04:16:18 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 04:16:18 - INFO - __main__ - Printing 3 examples
06/18/2022 04:16:18 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
06/18/2022 04:16:18 - INFO - __main__ - ['Film']
06/18/2022 04:16:18 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
06/18/2022 04:16:18 - INFO - __main__ - ['Film']
06/18/2022 04:16:18 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
06/18/2022 04:16:18 - INFO - __main__ - ['Film']
06/18/2022 04:16:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 04:16:18 - INFO - __main__ - Tokenizing Output ...
06/18/2022 04:16:18 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 04:16:18 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 04:16:18 - INFO - __main__ - Printing 3 examples
06/18/2022 04:16:18 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
06/18/2022 04:16:18 - INFO - __main__ - ['Film']
06/18/2022 04:16:18 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
06/18/2022 04:16:18 - INFO - __main__ - ['Film']
06/18/2022 04:16:18 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
06/18/2022 04:16:18 - INFO - __main__ - ['Film']
06/18/2022 04:16:18 - INFO - __main__ - Tokenizing Input ...
06/18/2022 04:16:18 - INFO - __main__ - Tokenizing Output ...
06/18/2022 04:16:18 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 04:16:33 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 04:16:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 04:16:34 - INFO - __main__ - Starting training!
06/18/2022 04:16:38 - INFO - __main__ - Step 10 Global step 10 Train loss 6.74 on epoch=0
06/18/2022 04:16:41 - INFO - __main__ - Step 20 Global step 20 Train loss 4.86 on epoch=1
06/18/2022 04:16:44 - INFO - __main__ - Step 30 Global step 30 Train loss 4.21 on epoch=2
06/18/2022 04:16:46 - INFO - __main__ - Step 40 Global step 40 Train loss 3.51 on epoch=2
06/18/2022 04:16:49 - INFO - __main__ - Step 50 Global step 50 Train loss 3.61 on epoch=3
06/18/2022 04:16:54 - INFO - __main__ - Global step 50 Train loss 4.59 Classification-F1 0.08110053918103455 on epoch=3
06/18/2022 04:16:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.08110053918103455 on epoch=3, global_step=50
06/18/2022 04:16:57 - INFO - __main__ - Step 60 Global step 60 Train loss 3.11 on epoch=4
06/18/2022 04:17:00 - INFO - __main__ - Step 70 Global step 70 Train loss 2.65 on epoch=4
06/18/2022 04:17:02 - INFO - __main__ - Step 80 Global step 80 Train loss 2.68 on epoch=5
06/18/2022 04:17:05 - INFO - __main__ - Step 90 Global step 90 Train loss 2.24 on epoch=6
06/18/2022 04:17:08 - INFO - __main__ - Step 100 Global step 100 Train loss 2.24 on epoch=7
06/18/2022 04:17:13 - INFO - __main__ - Global step 100 Train loss 2.58 Classification-F1 0.10910918061601792 on epoch=7
06/18/2022 04:17:13 - INFO - __main__ - Saving model with best Classification-F1: 0.08110053918103455 -> 0.10910918061601792 on epoch=7, global_step=100
06/18/2022 04:17:16 - INFO - __main__ - Step 110 Global step 110 Train loss 1.82 on epoch=7
06/18/2022 04:17:18 - INFO - __main__ - Step 120 Global step 120 Train loss 1.90 on epoch=8
06/18/2022 04:17:21 - INFO - __main__ - Step 130 Global step 130 Train loss 1.77 on epoch=9
06/18/2022 04:17:24 - INFO - __main__ - Step 140 Global step 140 Train loss 1.49 on epoch=9
06/18/2022 04:17:26 - INFO - __main__ - Step 150 Global step 150 Train loss 1.61 on epoch=10
06/18/2022 04:17:32 - INFO - __main__ - Global step 150 Train loss 1.72 Classification-F1 0.15622440461673598 on epoch=10
06/18/2022 04:17:32 - INFO - __main__ - Saving model with best Classification-F1: 0.10910918061601792 -> 0.15622440461673598 on epoch=10, global_step=150
06/18/2022 04:17:34 - INFO - __main__ - Step 160 Global step 160 Train loss 1.31 on epoch=11
06/18/2022 04:17:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.25 on epoch=12
06/18/2022 04:17:40 - INFO - __main__ - Step 180 Global step 180 Train loss 1.17 on epoch=12
06/18/2022 04:17:42 - INFO - __main__ - Step 190 Global step 190 Train loss 1.11 on epoch=13
06/18/2022 04:17:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.98 on epoch=14
06/18/2022 04:17:51 - INFO - __main__ - Global step 200 Train loss 1.16 Classification-F1 0.3479951173218964 on epoch=14
06/18/2022 04:17:51 - INFO - __main__ - Saving model with best Classification-F1: 0.15622440461673598 -> 0.3479951173218964 on epoch=14, global_step=200
06/18/2022 04:17:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.84 on epoch=14
06/18/2022 04:17:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.84 on epoch=15
06/18/2022 04:17:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.85 on epoch=16
06/18/2022 04:18:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.70 on epoch=17
06/18/2022 04:18:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.70 on epoch=17
06/18/2022 04:18:11 - INFO - __main__ - Global step 250 Train loss 0.78 Classification-F1 0.4278107426663294 on epoch=17
06/18/2022 04:18:11 - INFO - __main__ - Saving model with best Classification-F1: 0.3479951173218964 -> 0.4278107426663294 on epoch=17, global_step=250
06/18/2022 04:18:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.66 on epoch=18
06/18/2022 04:18:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.62 on epoch=19
06/18/2022 04:18:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=19
06/18/2022 04:18:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.58 on epoch=20
06/18/2022 04:18:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=21
06/18/2022 04:18:31 - INFO - __main__ - Global step 300 Train loss 0.55 Classification-F1 0.5685974000738475 on epoch=21
06/18/2022 04:18:31 - INFO - __main__ - Saving model with best Classification-F1: 0.4278107426663294 -> 0.5685974000738475 on epoch=21, global_step=300
06/18/2022 04:18:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=22
06/18/2022 04:18:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=22
06/18/2022 04:18:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=23
06/18/2022 04:18:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=24
06/18/2022 04:18:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=24
06/18/2022 04:18:50 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.6301298701298701 on epoch=24
06/18/2022 04:18:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5685974000738475 -> 0.6301298701298701 on epoch=24, global_step=350
06/18/2022 04:18:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=25
06/18/2022 04:18:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=26
06/18/2022 04:18:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=27
06/18/2022 04:19:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.34 on epoch=27
06/18/2022 04:19:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=28
06/18/2022 04:19:10 - INFO - __main__ - Global step 400 Train loss 0.34 Classification-F1 0.7064190289996741 on epoch=28
06/18/2022 04:19:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6301298701298701 -> 0.7064190289996741 on epoch=28, global_step=400
06/18/2022 04:19:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=29
06/18/2022 04:19:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=29
06/18/2022 04:19:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.30 on epoch=30
06/18/2022 04:19:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=31
06/18/2022 04:19:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.29 on epoch=32
06/18/2022 04:19:30 - INFO - __main__ - Global step 450 Train loss 0.28 Classification-F1 0.7094276094276094 on epoch=32
06/18/2022 04:19:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7064190289996741 -> 0.7094276094276094 on epoch=32, global_step=450
06/18/2022 04:19:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.29 on epoch=32
06/18/2022 04:19:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=33
06/18/2022 04:19:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=34
06/18/2022 04:19:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=34
06/18/2022 04:19:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=35
06/18/2022 04:19:50 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.8137923351158645 on epoch=35
06/18/2022 04:19:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7094276094276094 -> 0.8137923351158645 on epoch=35, global_step=500
06/18/2022 04:19:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=36
06/18/2022 04:19:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=37
06/18/2022 04:19:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=37
06/18/2022 04:20:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=38
06/18/2022 04:20:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=39
06/18/2022 04:20:10 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.8710363396271714 on epoch=39
06/18/2022 04:20:10 - INFO - __main__ - Saving model with best Classification-F1: 0.8137923351158645 -> 0.8710363396271714 on epoch=39, global_step=550
06/18/2022 04:20:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.13 on epoch=39
06/18/2022 04:20:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=40
06/18/2022 04:20:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=41
06/18/2022 04:20:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=42
06/18/2022 04:20:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=42
06/18/2022 04:20:30 - INFO - __main__ - Global step 600 Train loss 0.17 Classification-F1 0.8947341578477624 on epoch=42
06/18/2022 04:20:30 - INFO - __main__ - Saving model with best Classification-F1: 0.8710363396271714 -> 0.8947341578477624 on epoch=42, global_step=600
06/18/2022 04:20:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=43
06/18/2022 04:20:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=44
06/18/2022 04:20:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=44
06/18/2022 04:20:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=45
06/18/2022 04:20:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=46
06/18/2022 04:20:50 - INFO - __main__ - Global step 650 Train loss 0.14 Classification-F1 0.8859004794488666 on epoch=46
06/18/2022 04:20:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=47
06/18/2022 04:20:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=47
06/18/2022 04:20:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=48
06/18/2022 04:21:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=49
06/18/2022 04:21:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=49
06/18/2022 04:21:09 - INFO - __main__ - Global step 700 Train loss 0.13 Classification-F1 0.8857571102732393 on epoch=49
06/18/2022 04:21:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=50
06/18/2022 04:21:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=51
06/18/2022 04:21:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=52
06/18/2022 04:21:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=52
06/18/2022 04:21:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=53
06/18/2022 04:21:29 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.8198205968604451 on epoch=53
06/18/2022 04:21:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=54
06/18/2022 04:21:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=54
06/18/2022 04:21:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=55
06/18/2022 04:21:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=56
06/18/2022 04:21:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=57
06/18/2022 04:21:49 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.8859004794488666 on epoch=57
06/18/2022 04:21:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=57
06/18/2022 04:21:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=58
06/18/2022 04:21:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=59
06/18/2022 04:21:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=59
06/18/2022 04:22:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=60
06/18/2022 04:22:09 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.8402859237536657 on epoch=60
06/18/2022 04:22:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=61
06/18/2022 04:22:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=62
06/18/2022 04:22:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=62
06/18/2022 04:22:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=63
06/18/2022 04:22:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=64
06/18/2022 04:22:28 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.8562351626867756 on epoch=64
06/18/2022 04:22:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=64
06/18/2022 04:22:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=65
06/18/2022 04:22:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=66
06/18/2022 04:22:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=67
06/18/2022 04:22:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=67
06/18/2022 04:22:48 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.8535896600412729 on epoch=67
06/18/2022 04:22:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=68
06/18/2022 04:22:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=69
06/18/2022 04:22:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=69
06/18/2022 04:22:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=70
06/18/2022 04:23:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=71
06/18/2022 04:23:08 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.8424402798142718 on epoch=71
06/18/2022 04:23:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=72
06/18/2022 04:23:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=72
06/18/2022 04:23:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=73
06/18/2022 04:23:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=74
06/18/2022 04:23:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=74
06/18/2022 04:23:27 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.7763454389863588 on epoch=74
06/18/2022 04:23:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=75
06/18/2022 04:23:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=76
06/18/2022 04:23:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=77
06/18/2022 04:23:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=77
06/18/2022 04:23:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=78
06/18/2022 04:23:47 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.7750778612481928 on epoch=78
06/18/2022 04:23:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=79
06/18/2022 04:23:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=79
06/18/2022 04:23:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=80
06/18/2022 04:23:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=81
06/18/2022 04:24:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=82
06/18/2022 04:24:07 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.8005705869083478 on epoch=82
06/18/2022 04:24:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=82
06/18/2022 04:24:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=83
06/18/2022 04:24:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=84
06/18/2022 04:24:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=84
06/18/2022 04:24:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=85
06/18/2022 04:24:27 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.849545183012925 on epoch=85
06/18/2022 04:24:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=86
06/18/2022 04:24:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=87
06/18/2022 04:24:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=87
06/18/2022 04:24:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=88
06/18/2022 04:24:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=89
06/18/2022 04:24:47 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.8523717430141234 on epoch=89
06/18/2022 04:24:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=89
06/18/2022 04:24:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=90
06/18/2022 04:24:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=91
06/18/2022 04:24:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=92
06/18/2022 04:25:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=92
06/18/2022 04:25:07 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.7473323437481143 on epoch=92
06/18/2022 04:25:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=93
06/18/2022 04:25:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=94
06/18/2022 04:25:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=94
06/18/2022 04:25:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=95
06/18/2022 04:25:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=96
06/18/2022 04:25:27 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.7137719665733249 on epoch=96
06/18/2022 04:25:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=97
06/18/2022 04:25:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=97
06/18/2022 04:25:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=98
06/18/2022 04:25:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=99
06/18/2022 04:25:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=99
06/18/2022 04:25:48 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6469640166340204 on epoch=99
06/18/2022 04:25:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=100
06/18/2022 04:25:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=101
06/18/2022 04:25:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=102
06/18/2022 04:25:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=102
06/18/2022 04:26:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=103
06/18/2022 04:26:08 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.9228413163897036 on epoch=103
06/18/2022 04:26:08 - INFO - __main__ - Saving model with best Classification-F1: 0.8947341578477624 -> 0.9228413163897036 on epoch=103, global_step=1450
06/18/2022 04:26:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=104
06/18/2022 04:26:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=104
06/18/2022 04:26:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=105
06/18/2022 04:26:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=106
06/18/2022 04:26:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=107
06/18/2022 04:26:29 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.8591069464809384 on epoch=107
06/18/2022 04:26:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=107
06/18/2022 04:26:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=108
06/18/2022 04:26:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=109
06/18/2022 04:26:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=109
06/18/2022 04:26:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=110
06/18/2022 04:26:50 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.7983364867615341 on epoch=110
06/18/2022 04:26:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=111
06/18/2022 04:26:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=112
06/18/2022 04:26:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=112
06/18/2022 04:27:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=113
06/18/2022 04:27:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=114
06/18/2022 04:27:10 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.7998170349925569 on epoch=114
06/18/2022 04:27:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=114
06/18/2022 04:27:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=115
06/18/2022 04:27:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=116
06/18/2022 04:27:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=117
06/18/2022 04:27:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=117
06/18/2022 04:27:31 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.8424285093639932 on epoch=117
06/18/2022 04:27:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=118
06/18/2022 04:27:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=119
06/18/2022 04:27:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=119
06/18/2022 04:27:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=120
06/18/2022 04:27:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=121
06/18/2022 04:27:52 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.8588204065122864 on epoch=121
06/18/2022 04:27:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=122
06/18/2022 04:27:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=122
06/18/2022 04:28:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=123
06/18/2022 04:28:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=124
06/18/2022 04:28:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=124
06/18/2022 04:28:13 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.8463385777901907 on epoch=124
06/18/2022 04:28:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=125
06/18/2022 04:28:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=126
06/18/2022 04:28:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=127
06/18/2022 04:28:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=127
06/18/2022 04:28:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=128
06/18/2022 04:28:33 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.7040378417148617 on epoch=128
06/18/2022 04:28:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=129
06/18/2022 04:28:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=129
06/18/2022 04:28:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=130
06/18/2022 04:28:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=131
06/18/2022 04:28:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=132
06/18/2022 04:28:54 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.9140548982595701 on epoch=132
06/18/2022 04:28:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=132
06/18/2022 04:28:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=133
06/18/2022 04:29:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=134
06/18/2022 04:29:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=134
06/18/2022 04:29:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=135
06/18/2022 04:29:14 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.9055741227626656 on epoch=135
06/18/2022 04:29:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=136
06/18/2022 04:29:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=137
06/18/2022 04:29:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=137
06/18/2022 04:29:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=138
06/18/2022 04:29:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=139
06/18/2022 04:29:35 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.9007450206106119 on epoch=139
06/18/2022 04:29:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=139
06/18/2022 04:29:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=140
06/18/2022 04:29:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=141
06/18/2022 04:29:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=142
06/18/2022 04:29:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=142
06/18/2022 04:29:56 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.9095262738526496 on epoch=142
06/18/2022 04:29:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=143
06/18/2022 04:30:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=144
06/18/2022 04:30:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=144
06/18/2022 04:30:07 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=145
06/18/2022 04:30:10 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=146
06/18/2022 04:30:17 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7907860387556783 on epoch=146
06/18/2022 04:30:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=147
06/18/2022 04:30:22 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=147
06/18/2022 04:30:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=148
06/18/2022 04:30:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=149
06/18/2022 04:30:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=149
06/18/2022 04:30:38 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.8505571629311549 on epoch=149
06/18/2022 04:30:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=150
06/18/2022 04:30:43 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=151
06/18/2022 04:30:46 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=152
06/18/2022 04:30:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=152
06/18/2022 04:30:51 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=153
06/18/2022 04:30:59 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.9775075059349254 on epoch=153
06/18/2022 04:30:59 - INFO - __main__ - Saving model with best Classification-F1: 0.9228413163897036 -> 0.9775075059349254 on epoch=153, global_step=2150
06/18/2022 04:31:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=154
06/18/2022 04:31:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=154
06/18/2022 04:31:07 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=155
06/18/2022 04:31:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=156
06/18/2022 04:31:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=157
06/18/2022 04:31:20 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.9098882315929034 on epoch=157
06/18/2022 04:31:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=157
06/18/2022 04:31:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=158
06/18/2022 04:31:28 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=159
06/18/2022 04:31:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=159
06/18/2022 04:31:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=160
06/18/2022 04:31:41 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.851004088086089 on epoch=160
06/18/2022 04:31:44 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=161
06/18/2022 04:31:46 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=162
06/18/2022 04:31:49 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=162
06/18/2022 04:31:52 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=163
06/18/2022 04:31:54 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=164
06/18/2022 04:32:02 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.854910338086089 on epoch=164
06/18/2022 04:32:04 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=164
06/18/2022 04:32:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=165
06/18/2022 04:32:09 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=166
06/18/2022 04:32:12 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=167
06/18/2022 04:32:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=167
06/18/2022 04:32:22 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7981459119979045 on epoch=167
06/18/2022 04:32:25 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=168
06/18/2022 04:32:27 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=169
06/18/2022 04:32:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=169
06/18/2022 04:32:33 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=170
06/18/2022 04:32:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=171
06/18/2022 04:32:43 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7961336323385659 on epoch=171
06/18/2022 04:32:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=172
06/18/2022 04:32:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=172
06/18/2022 04:32:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=173
06/18/2022 04:32:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=174
06/18/2022 04:32:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=174
06/18/2022 04:33:04 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7924571617503307 on epoch=174
06/18/2022 04:33:06 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=175
06/18/2022 04:33:09 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=176
06/18/2022 04:33:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=177
06/18/2022 04:33:14 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
06/18/2022 04:33:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=178
06/18/2022 04:33:25 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7961336323385659 on epoch=178
06/18/2022 04:33:27 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=179
06/18/2022 04:33:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
06/18/2022 04:33:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=180
06/18/2022 04:33:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=181
06/18/2022 04:33:38 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=182
06/18/2022 04:33:46 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.9036654679918438 on epoch=182
06/18/2022 04:33:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
06/18/2022 04:33:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
06/18/2022 04:33:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=184
06/18/2022 04:33:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=184
06/18/2022 04:33:59 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=185
06/18/2022 04:34:08 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7876422304302091 on epoch=185
06/18/2022 04:34:10 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=186
06/18/2022 04:34:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=187
06/18/2022 04:34:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=187
06/18/2022 04:34:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=188
06/18/2022 04:34:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=189
06/18/2022 04:34:30 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.8450626574366493 on epoch=189
06/18/2022 04:34:32 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=189
06/18/2022 04:34:35 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=190
06/18/2022 04:34:38 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
06/18/2022 04:34:40 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
06/18/2022 04:34:43 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=192
06/18/2022 04:34:51 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.9014001679324262 on epoch=192
06/18/2022 04:34:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=193
06/18/2022 04:34:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
06/18/2022 04:34:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
06/18/2022 04:35:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=195
06/18/2022 04:35:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=196
06/18/2022 04:35:12 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.8469565968305888 on epoch=196
06/18/2022 04:35:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=197
06/18/2022 04:35:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=197
06/18/2022 04:35:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
06/18/2022 04:35:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=199
06/18/2022 04:35:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=199
06/18/2022 04:35:34 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.9014001679324262 on epoch=199
06/18/2022 04:35:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=200
06/18/2022 04:35:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=201
06/18/2022 04:35:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=202
06/18/2022 04:35:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
06/18/2022 04:35:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=203
06/18/2022 04:35:56 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.9044391043046958 on epoch=203
06/18/2022 04:35:58 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=204
06/18/2022 04:36:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=204
06/18/2022 04:36:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=205
06/18/2022 04:36:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=206
06/18/2022 04:36:09 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=207
06/18/2022 04:36:16 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.9064593063248978 on epoch=207
06/18/2022 04:36:19 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=207
06/18/2022 04:36:22 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=208
06/18/2022 04:36:24 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=209
06/18/2022 04:36:27 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=209
06/18/2022 04:36:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=210
06/18/2022 04:36:37 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.907260973793232 on epoch=210
06/18/2022 04:36:40 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=211
06/18/2022 04:36:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=212
06/18/2022 04:36:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=212
06/18/2022 04:36:48 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=213
06/18/2022 04:36:50 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=214
06/18/2022 04:36:52 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 04:36:52 - INFO - __main__ - Printing 3 examples
06/18/2022 04:36:52 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
06/18/2022 04:36:52 - INFO - __main__ - ['Film']
06/18/2022 04:36:52 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
06/18/2022 04:36:52 - INFO - __main__ - ['Film']
06/18/2022 04:36:52 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
06/18/2022 04:36:52 - INFO - __main__ - ['Film']
06/18/2022 04:36:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 04:36:52 - INFO - __main__ - Tokenizing Output ...
06/18/2022 04:36:52 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 04:36:52 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 04:36:52 - INFO - __main__ - Printing 3 examples
06/18/2022 04:36:52 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
06/18/2022 04:36:52 - INFO - __main__ - ['Film']
06/18/2022 04:36:52 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
06/18/2022 04:36:52 - INFO - __main__ - ['Film']
06/18/2022 04:36:52 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
06/18/2022 04:36:52 - INFO - __main__ - ['Film']
06/18/2022 04:36:52 - INFO - __main__ - Tokenizing Input ...
06/18/2022 04:36:52 - INFO - __main__ - Tokenizing Output ...
06/18/2022 04:36:53 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 04:36:58 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.854910338086089 on epoch=214
06/18/2022 04:36:58 - INFO - __main__ - save last model!
06/18/2022 04:36:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 04:36:58 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 04:36:58 - INFO - __main__ - Printing 3 examples
06/18/2022 04:36:58 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 04:36:58 - INFO - __main__ - ['Animal']
06/18/2022 04:36:58 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 04:36:58 - INFO - __main__ - ['Animal']
06/18/2022 04:36:58 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 04:36:58 - INFO - __main__ - ['Village']
06/18/2022 04:36:58 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 04:37:00 - INFO - __main__ - Tokenizing Output ...
06/18/2022 04:37:04 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 04:37:08 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 04:37:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 04:37:08 - INFO - __main__ - Starting training!
06/18/2022 04:40:01 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_87_0.4_8_predictions.txt
06/18/2022 04:40:01 - INFO - __main__ - Classification-F1 on test data: 0.5473
06/18/2022 04:40:01 - INFO - __main__ - prefix=dbpedia_14_16_87, lr=0.4, bsz=8, dev_performance=0.9775075059349254, test_performance=0.547299650906635
06/18/2022 04:40:01 - INFO - __main__ - Running ... prefix=dbpedia_14_16_87, lr=0.3, bsz=8 ...
06/18/2022 04:40:02 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 04:40:02 - INFO - __main__ - Printing 3 examples
06/18/2022 04:40:02 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
06/18/2022 04:40:02 - INFO - __main__ - ['Film']
06/18/2022 04:40:02 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
06/18/2022 04:40:02 - INFO - __main__ - ['Film']
06/18/2022 04:40:02 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
06/18/2022 04:40:02 - INFO - __main__ - ['Film']
06/18/2022 04:40:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 04:40:02 - INFO - __main__ - Tokenizing Output ...
06/18/2022 04:40:03 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 04:40:03 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 04:40:03 - INFO - __main__ - Printing 3 examples
06/18/2022 04:40:03 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
06/18/2022 04:40:03 - INFO - __main__ - ['Film']
06/18/2022 04:40:03 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
06/18/2022 04:40:03 - INFO - __main__ - ['Film']
06/18/2022 04:40:03 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
06/18/2022 04:40:03 - INFO - __main__ - ['Film']
06/18/2022 04:40:03 - INFO - __main__ - Tokenizing Input ...
06/18/2022 04:40:03 - INFO - __main__ - Tokenizing Output ...
06/18/2022 04:40:03 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 04:40:18 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 04:40:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 04:40:19 - INFO - __main__ - Starting training!
06/18/2022 04:40:22 - INFO - __main__ - Step 10 Global step 10 Train loss 7.19 on epoch=0
06/18/2022 04:40:25 - INFO - __main__ - Step 20 Global step 20 Train loss 5.17 on epoch=1
06/18/2022 04:40:28 - INFO - __main__ - Step 30 Global step 30 Train loss 4.64 on epoch=2
06/18/2022 04:40:31 - INFO - __main__ - Step 40 Global step 40 Train loss 4.10 on epoch=2
06/18/2022 04:40:33 - INFO - __main__ - Step 50 Global step 50 Train loss 4.25 on epoch=3
06/18/2022 04:40:40 - INFO - __main__ - Global step 50 Train loss 5.07 Classification-F1 0.0505208943783774 on epoch=3
06/18/2022 04:40:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0505208943783774 on epoch=3, global_step=50
06/18/2022 04:40:42 - INFO - __main__ - Step 60 Global step 60 Train loss 3.66 on epoch=4
06/18/2022 04:40:45 - INFO - __main__ - Step 70 Global step 70 Train loss 2.98 on epoch=4
06/18/2022 04:40:48 - INFO - __main__ - Step 80 Global step 80 Train loss 3.39 on epoch=5
06/18/2022 04:40:50 - INFO - __main__ - Step 90 Global step 90 Train loss 2.66 on epoch=6
06/18/2022 04:40:53 - INFO - __main__ - Step 100 Global step 100 Train loss 2.64 on epoch=7
06/18/2022 04:40:58 - INFO - __main__ - Global step 100 Train loss 3.07 Classification-F1 0.11005321483432082 on epoch=7
06/18/2022 04:40:58 - INFO - __main__ - Saving model with best Classification-F1: 0.0505208943783774 -> 0.11005321483432082 on epoch=7, global_step=100
06/18/2022 04:41:01 - INFO - __main__ - Step 110 Global step 110 Train loss 2.44 on epoch=7
06/18/2022 04:41:03 - INFO - __main__ - Step 120 Global step 120 Train loss 2.39 on epoch=8
06/18/2022 04:41:06 - INFO - __main__ - Step 130 Global step 130 Train loss 2.25 on epoch=9
06/18/2022 04:41:08 - INFO - __main__ - Step 140 Global step 140 Train loss 1.86 on epoch=9
06/18/2022 04:41:11 - INFO - __main__ - Step 150 Global step 150 Train loss 2.10 on epoch=10
06/18/2022 04:41:16 - INFO - __main__ - Global step 150 Train loss 2.21 Classification-F1 0.1258738678999007 on epoch=10
06/18/2022 04:41:16 - INFO - __main__ - Saving model with best Classification-F1: 0.11005321483432082 -> 0.1258738678999007 on epoch=10, global_step=150
06/18/2022 04:41:19 - INFO - __main__ - Step 160 Global step 160 Train loss 1.81 on epoch=11
06/18/2022 04:41:22 - INFO - __main__ - Step 170 Global step 170 Train loss 1.81 on epoch=12
06/18/2022 04:41:24 - INFO - __main__ - Step 180 Global step 180 Train loss 1.57 on epoch=12
06/18/2022 04:41:27 - INFO - __main__ - Step 190 Global step 190 Train loss 1.65 on epoch=13
06/18/2022 04:41:29 - INFO - __main__ - Step 200 Global step 200 Train loss 1.42 on epoch=14
06/18/2022 04:41:35 - INFO - __main__ - Global step 200 Train loss 1.65 Classification-F1 0.15988980390350097 on epoch=14
06/18/2022 04:41:35 - INFO - __main__ - Saving model with best Classification-F1: 0.1258738678999007 -> 0.15988980390350097 on epoch=14, global_step=200
06/18/2022 04:41:38 - INFO - __main__ - Step 210 Global step 210 Train loss 1.30 on epoch=14
06/18/2022 04:41:40 - INFO - __main__ - Step 220 Global step 220 Train loss 1.41 on epoch=15
06/18/2022 04:41:43 - INFO - __main__ - Step 230 Global step 230 Train loss 1.21 on epoch=16
06/18/2022 04:41:46 - INFO - __main__ - Step 240 Global step 240 Train loss 1.12 on epoch=17
06/18/2022 04:41:48 - INFO - __main__ - Step 250 Global step 250 Train loss 1.07 on epoch=17
06/18/2022 04:41:55 - INFO - __main__ - Global step 250 Train loss 1.22 Classification-F1 0.2846629257384868 on epoch=17
06/18/2022 04:41:55 - INFO - __main__ - Saving model with best Classification-F1: 0.15988980390350097 -> 0.2846629257384868 on epoch=17, global_step=250
06/18/2022 04:41:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.97 on epoch=18
06/18/2022 04:42:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.88 on epoch=19
06/18/2022 04:42:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.79 on epoch=19
06/18/2022 04:42:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.88 on epoch=20
06/18/2022 04:42:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.79 on epoch=21
06/18/2022 04:42:14 - INFO - __main__ - Global step 300 Train loss 0.87 Classification-F1 0.37000562718842295 on epoch=21
06/18/2022 04:42:14 - INFO - __main__ - Saving model with best Classification-F1: 0.2846629257384868 -> 0.37000562718842295 on epoch=21, global_step=300
06/18/2022 04:42:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.78 on epoch=22
06/18/2022 04:42:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.71 on epoch=22
06/18/2022 04:42:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.64 on epoch=23
06/18/2022 04:42:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.60 on epoch=24
06/18/2022 04:42:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.59 on epoch=24
06/18/2022 04:42:34 - INFO - __main__ - Global step 350 Train loss 0.66 Classification-F1 0.5355348718602575 on epoch=24
06/18/2022 04:42:34 - INFO - __main__ - Saving model with best Classification-F1: 0.37000562718842295 -> 0.5355348718602575 on epoch=24, global_step=350
06/18/2022 04:42:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.56 on epoch=25
06/18/2022 04:42:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=26
06/18/2022 04:42:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.57 on epoch=27
06/18/2022 04:42:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=27
06/18/2022 04:42:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=28
06/18/2022 04:42:54 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.5834194336171323 on epoch=28
06/18/2022 04:42:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5355348718602575 -> 0.5834194336171323 on epoch=28, global_step=400
06/18/2022 04:42:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=29
06/18/2022 04:42:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=29
06/18/2022 04:43:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=30
06/18/2022 04:43:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.32 on epoch=31
06/18/2022 04:43:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=32
06/18/2022 04:43:13 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.6347507127333978 on epoch=32
06/18/2022 04:43:13 - INFO - __main__ - Saving model with best Classification-F1: 0.5834194336171323 -> 0.6347507127333978 on epoch=32, global_step=450
06/18/2022 04:43:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=32
06/18/2022 04:43:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=33
06/18/2022 04:43:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=34
06/18/2022 04:43:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=34
06/18/2022 04:43:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=35
06/18/2022 04:43:33 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.7433402881485294 on epoch=35
06/18/2022 04:43:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6347507127333978 -> 0.7433402881485294 on epoch=35, global_step=500
06/18/2022 04:43:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=36
06/18/2022 04:43:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=37
06/18/2022 04:43:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=37
06/18/2022 04:43:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=38
06/18/2022 04:43:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=39
06/18/2022 04:43:53 - INFO - __main__ - Global step 550 Train loss 0.30 Classification-F1 0.752721319681918 on epoch=39
06/18/2022 04:43:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7433402881485294 -> 0.752721319681918 on epoch=39, global_step=550
06/18/2022 04:43:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=39
06/18/2022 04:43:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=40
06/18/2022 04:44:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=41
06/18/2022 04:44:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=42
06/18/2022 04:44:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=42
06/18/2022 04:44:13 - INFO - __main__ - Global step 600 Train loss 0.29 Classification-F1 0.7831407049623368 on epoch=42
06/18/2022 04:44:13 - INFO - __main__ - Saving model with best Classification-F1: 0.752721319681918 -> 0.7831407049623368 on epoch=42, global_step=600
06/18/2022 04:44:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=43
06/18/2022 04:44:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.28 on epoch=44
06/18/2022 04:44:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.29 on epoch=44
06/18/2022 04:44:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.31 on epoch=45
06/18/2022 04:44:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=46
06/18/2022 04:44:32 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.7481160772530269 on epoch=46
06/18/2022 04:44:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=47
06/18/2022 04:44:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=47
06/18/2022 04:44:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=48
06/18/2022 04:44:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=49
06/18/2022 04:44:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=49
06/18/2022 04:44:51 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.7182118464850914 on epoch=49
06/18/2022 04:44:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=50
06/18/2022 04:44:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=51
06/18/2022 04:44:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=52
06/18/2022 04:45:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=52
06/18/2022 04:45:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=53
06/18/2022 04:45:11 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.6939724678264576 on epoch=53
06/18/2022 04:45:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=54
06/18/2022 04:45:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=54
06/18/2022 04:45:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=55
06/18/2022 04:45:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=56
06/18/2022 04:45:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=57
06/18/2022 04:45:30 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.7238434143357743 on epoch=57
06/18/2022 04:45:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=57
06/18/2022 04:45:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.15 on epoch=58
06/18/2022 04:45:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=59
06/18/2022 04:45:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=59
06/18/2022 04:45:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=60
06/18/2022 04:45:50 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.7988929104901871 on epoch=60
06/18/2022 04:45:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7831407049623368 -> 0.7988929104901871 on epoch=60, global_step=850
06/18/2022 04:45:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=61
06/18/2022 04:45:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=62
06/18/2022 04:45:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=62
06/18/2022 04:46:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=63
06/18/2022 04:46:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=64
06/18/2022 04:46:09 - INFO - __main__ - Global step 900 Train loss 0.16 Classification-F1 0.795892128112242 on epoch=64
06/18/2022 04:46:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=64
06/18/2022 04:46:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=65
06/18/2022 04:46:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=66
06/18/2022 04:46:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=67
06/18/2022 04:46:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=67
06/18/2022 04:46:28 - INFO - __main__ - Global step 950 Train loss 0.13 Classification-F1 0.7788511298947731 on epoch=67
06/18/2022 04:46:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=68
06/18/2022 04:46:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=69
06/18/2022 04:46:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=69
06/18/2022 04:46:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.13 on epoch=70
06/18/2022 04:46:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=71
06/18/2022 04:46:47 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.8244167052528683 on epoch=71
06/18/2022 04:46:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7988929104901871 -> 0.8244167052528683 on epoch=71, global_step=1000
06/18/2022 04:46:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=72
06/18/2022 04:46:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.10 on epoch=72
06/18/2022 04:46:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=73
06/18/2022 04:46:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=74
06/18/2022 04:47:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=74
06/18/2022 04:47:07 - INFO - __main__ - Global step 1050 Train loss 0.13 Classification-F1 0.849679863147605 on epoch=74
06/18/2022 04:47:07 - INFO - __main__ - Saving model with best Classification-F1: 0.8244167052528683 -> 0.849679863147605 on epoch=74, global_step=1050
06/18/2022 04:47:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=75
06/18/2022 04:47:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=76
06/18/2022 04:47:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=77
06/18/2022 04:47:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.15 on epoch=77
06/18/2022 04:47:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=78
06/18/2022 04:47:26 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.8527567862245282 on epoch=78
06/18/2022 04:47:26 - INFO - __main__ - Saving model with best Classification-F1: 0.849679863147605 -> 0.8527567862245282 on epoch=78, global_step=1100
06/18/2022 04:47:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=79
06/18/2022 04:47:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=79
06/18/2022 04:47:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=80
06/18/2022 04:47:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=81
06/18/2022 04:47:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=82
06/18/2022 04:47:45 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.8488237173958237 on epoch=82
06/18/2022 04:47:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=82
06/18/2022 04:47:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=83
06/18/2022 04:47:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=84
06/18/2022 04:47:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=84
06/18/2022 04:47:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=85
06/18/2022 04:48:04 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.7856674457788344 on epoch=85
06/18/2022 04:48:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=86
06/18/2022 04:48:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=87
06/18/2022 04:48:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=87
06/18/2022 04:48:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=88
06/18/2022 04:48:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=89
06/18/2022 04:48:23 - INFO - __main__ - Global step 1250 Train loss 0.09 Classification-F1 0.9078672876775344 on epoch=89
06/18/2022 04:48:23 - INFO - __main__ - Saving model with best Classification-F1: 0.8527567862245282 -> 0.9078672876775344 on epoch=89, global_step=1250
06/18/2022 04:48:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=89
06/18/2022 04:48:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=90
06/18/2022 04:48:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=91
06/18/2022 04:48:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=92
06/18/2022 04:48:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=92
06/18/2022 04:48:42 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.8454638012439164 on epoch=92
06/18/2022 04:48:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=93
06/18/2022 04:48:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=94
06/18/2022 04:48:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=94
06/18/2022 04:48:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=95
06/18/2022 04:48:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=96
06/18/2022 04:49:01 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.9186705767350929 on epoch=96
06/18/2022 04:49:01 - INFO - __main__ - Saving model with best Classification-F1: 0.9078672876775344 -> 0.9186705767350929 on epoch=96, global_step=1350
06/18/2022 04:49:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=97
06/18/2022 04:49:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=97
06/18/2022 04:49:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=98
06/18/2022 04:49:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=99
06/18/2022 04:49:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=99
06/18/2022 04:49:20 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.7946453116826966 on epoch=99
06/18/2022 04:49:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=100
06/18/2022 04:49:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=101
06/18/2022 04:49:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=102
06/18/2022 04:49:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=102
06/18/2022 04:49:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=103
06/18/2022 04:49:40 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.7969634803294657 on epoch=103
06/18/2022 04:49:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=104
06/18/2022 04:49:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=104
06/18/2022 04:49:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=105
06/18/2022 04:49:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=106
06/18/2022 04:49:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=107
06/18/2022 04:49:59 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.7526877314222732 on epoch=107
06/18/2022 04:50:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=107
06/18/2022 04:50:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=108
06/18/2022 04:50:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=109
06/18/2022 04:50:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=109
06/18/2022 04:50:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=110
06/18/2022 04:50:18 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.855058651026393 on epoch=110
06/18/2022 04:50:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=111
06/18/2022 04:50:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=112
06/18/2022 04:50:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=112
06/18/2022 04:50:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=113
06/18/2022 04:50:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=114
06/18/2022 04:50:36 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.7800452357337103 on epoch=114
06/18/2022 04:50:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=114
06/18/2022 04:50:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=115
06/18/2022 04:50:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=116
06/18/2022 04:50:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=117
06/18/2022 04:50:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=117
06/18/2022 04:50:56 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.8389367584766066 on epoch=117
06/18/2022 04:50:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=118
06/18/2022 04:51:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=119
06/18/2022 04:51:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=119
06/18/2022 04:51:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=120
06/18/2022 04:51:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=121
06/18/2022 04:51:15 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.7945299201466185 on epoch=121
06/18/2022 04:51:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=122
06/18/2022 04:51:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=122
06/18/2022 04:51:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=123
06/18/2022 04:51:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=124
06/18/2022 04:51:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=124
06/18/2022 04:51:34 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.8550255647119298 on epoch=124
06/18/2022 04:51:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=125
06/18/2022 04:51:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=126
06/18/2022 04:51:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=127
06/18/2022 04:51:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=127
06/18/2022 04:51:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=128
06/18/2022 04:51:53 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.9141737336725231 on epoch=128
06/18/2022 04:51:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=129
06/18/2022 04:51:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=129
06/18/2022 04:52:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=130
06/18/2022 04:52:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=131
06/18/2022 04:52:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=132
06/18/2022 04:52:12 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.8999713842977601 on epoch=132
06/18/2022 04:52:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=132
06/18/2022 04:52:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=133
06/18/2022 04:52:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=134
06/18/2022 04:52:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=134
06/18/2022 04:52:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=135
06/18/2022 04:52:31 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.9102345519392238 on epoch=135
06/18/2022 04:52:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=136
06/18/2022 04:52:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=137
06/18/2022 04:52:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=137
06/18/2022 04:52:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=138
06/18/2022 04:52:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=139
06/18/2022 04:52:51 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.9103086366511415 on epoch=139
06/18/2022 04:52:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=139
06/18/2022 04:52:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=140
06/18/2022 04:52:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=141
06/18/2022 04:53:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=142
06/18/2022 04:53:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=142
06/18/2022 04:53:10 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.9821254014802404 on epoch=142
06/18/2022 04:53:10 - INFO - __main__ - Saving model with best Classification-F1: 0.9186705767350929 -> 0.9821254014802404 on epoch=142, global_step=2000
06/18/2022 04:53:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=143
06/18/2022 04:53:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=144
06/18/2022 04:53:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=144
06/18/2022 04:53:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=145
06/18/2022 04:53:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=146
06/18/2022 04:53:30 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.9143564679048553 on epoch=146
06/18/2022 04:53:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=147
06/18/2022 04:53:35 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=147
06/18/2022 04:53:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=148
06/18/2022 04:53:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=149
06/18/2022 04:53:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=149
06/18/2022 04:53:49 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.9185272075594656 on epoch=149
06/18/2022 04:53:52 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=150
06/18/2022 04:53:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=151
06/18/2022 04:53:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=152
06/18/2022 04:53:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=152
06/18/2022 04:54:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=153
06/18/2022 04:54:09 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.8550217462857325 on epoch=153
06/18/2022 04:54:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=154
06/18/2022 04:54:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=154
06/18/2022 04:54:17 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=155
06/18/2022 04:54:19 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=156
06/18/2022 04:54:22 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=157
06/18/2022 04:54:28 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.9101857282502446 on epoch=157
06/18/2022 04:54:31 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=157
06/18/2022 04:54:34 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=158
06/18/2022 04:54:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=159
06/18/2022 04:54:39 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=159
06/18/2022 04:54:41 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=160
06/18/2022 04:54:48 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.8103501811281698 on epoch=160
06/18/2022 04:54:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=161
06/18/2022 04:54:53 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=162
06/18/2022 04:54:56 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=162
06/18/2022 04:54:58 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=163
06/18/2022 04:55:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=164
06/18/2022 04:55:08 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.857086999022483 on epoch=164
06/18/2022 04:55:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=164
06/18/2022 04:55:13 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
06/18/2022 04:55:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=166
06/18/2022 04:55:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=167
06/18/2022 04:55:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=167
06/18/2022 04:55:27 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.8569156856796718 on epoch=167
06/18/2022 04:55:30 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=168
06/18/2022 04:55:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=169
06/18/2022 04:55:35 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=169
06/18/2022 04:55:38 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=170
06/18/2022 04:55:40 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=171
06/18/2022 04:55:47 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.8609970674486804 on epoch=171
06/18/2022 04:55:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=172
06/18/2022 04:55:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=172
06/18/2022 04:55:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=173
06/18/2022 04:55:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=174
06/18/2022 04:56:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=174
06/18/2022 04:56:06 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.8591069464809384 on epoch=174
06/18/2022 04:56:09 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=175
06/18/2022 04:56:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=176
06/18/2022 04:56:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=177
06/18/2022 04:56:17 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=177
06/18/2022 04:56:20 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=178
06/18/2022 04:56:27 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.8631514235092864 on epoch=178
06/18/2022 04:56:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=179
06/18/2022 04:56:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
06/18/2022 04:56:34 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=180
06/18/2022 04:56:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=181
06/18/2022 04:56:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=182
06/18/2022 04:56:46 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.905238767604359 on epoch=182
06/18/2022 04:56:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
06/18/2022 04:56:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=183
06/18/2022 04:56:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=184
06/18/2022 04:56:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=184
06/18/2022 04:57:00 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=185
06/18/2022 04:57:06 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.9100029940179126 on epoch=185
06/18/2022 04:57:09 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=186
06/18/2022 04:57:12 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=187
06/18/2022 04:57:14 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
06/18/2022 04:57:17 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=188
06/18/2022 04:57:19 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=189
06/18/2022 04:57:26 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.9100029940179126 on epoch=189
06/18/2022 04:57:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
06/18/2022 04:57:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=190
06/18/2022 04:57:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
06/18/2022 04:57:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
06/18/2022 04:57:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=192
06/18/2022 04:57:46 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.9100029940179126 on epoch=192
06/18/2022 04:57:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=193
06/18/2022 04:57:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
06/18/2022 04:57:54 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
06/18/2022 04:57:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=195
06/18/2022 04:57:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=196
06/18/2022 04:58:06 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.9186705767350929 on epoch=196
06/18/2022 04:58:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=197
06/18/2022 04:58:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=197
06/18/2022 04:58:14 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=198
06/18/2022 04:58:17 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=199
06/18/2022 04:58:19 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=199
06/18/2022 04:58:26 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.9144998370804825 on epoch=199
06/18/2022 04:58:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.07 on epoch=200
06/18/2022 04:58:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=201
06/18/2022 04:58:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=202
06/18/2022 04:58:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
06/18/2022 04:58:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=203
06/18/2022 04:58:47 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.9865940511101802 on epoch=203
06/18/2022 04:58:47 - INFO - __main__ - Saving model with best Classification-F1: 0.9821254014802404 -> 0.9865940511101802 on epoch=203, global_step=2850
06/18/2022 04:58:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=204
06/18/2022 04:58:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=204
06/18/2022 04:58:54 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=205
06/18/2022 04:58:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=206
06/18/2022 04:59:00 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=207
06/18/2022 04:59:07 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.9098841586049595 on epoch=207
06/18/2022 04:59:10 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=207
06/18/2022 04:59:12 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=208
06/18/2022 04:59:15 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
06/18/2022 04:59:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=209
06/18/2022 04:59:20 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=210
06/18/2022 04:59:28 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.9098882315929034 on epoch=210
06/18/2022 04:59:31 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=211
06/18/2022 04:59:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
06/18/2022 04:59:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=212
06/18/2022 04:59:39 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=213
06/18/2022 04:59:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=214
06/18/2022 04:59:43 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 04:59:43 - INFO - __main__ - Printing 3 examples
06/18/2022 04:59:43 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
06/18/2022 04:59:43 - INFO - __main__ - ['Film']
06/18/2022 04:59:43 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
06/18/2022 04:59:43 - INFO - __main__ - ['Film']
06/18/2022 04:59:43 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
06/18/2022 04:59:43 - INFO - __main__ - ['Film']
06/18/2022 04:59:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 04:59:43 - INFO - __main__ - Tokenizing Output ...
06/18/2022 04:59:43 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 04:59:43 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 04:59:43 - INFO - __main__ - Printing 3 examples
06/18/2022 04:59:43 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
06/18/2022 04:59:43 - INFO - __main__ - ['Film']
06/18/2022 04:59:43 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
06/18/2022 04:59:43 - INFO - __main__ - ['Film']
06/18/2022 04:59:43 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
06/18/2022 04:59:43 - INFO - __main__ - ['Film']
06/18/2022 04:59:43 - INFO - __main__ - Tokenizing Input ...
06/18/2022 04:59:43 - INFO - __main__ - Tokenizing Output ...
06/18/2022 04:59:43 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 04:59:49 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.8370536005213425 on epoch=214
06/18/2022 04:59:49 - INFO - __main__ - save last model!
06/18/2022 04:59:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 04:59:49 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 04:59:49 - INFO - __main__ - Printing 3 examples
06/18/2022 04:59:49 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 04:59:49 - INFO - __main__ - ['Animal']
06/18/2022 04:59:49 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 04:59:49 - INFO - __main__ - ['Animal']
06/18/2022 04:59:49 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 04:59:49 - INFO - __main__ - ['Village']
06/18/2022 04:59:49 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 04:59:51 - INFO - __main__ - Tokenizing Output ...
06/18/2022 04:59:54 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 04:59:59 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 04:59:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 04:59:59 - INFO - __main__ - Starting training!
06/18/2022 05:02:08 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_87_0.3_8_predictions.txt
06/18/2022 05:02:08 - INFO - __main__ - Classification-F1 on test data: 0.5191
06/18/2022 05:02:08 - INFO - __main__ - prefix=dbpedia_14_16_87, lr=0.3, bsz=8, dev_performance=0.9865940511101802, test_performance=0.5191102088585087
06/18/2022 05:02:08 - INFO - __main__ - Running ... prefix=dbpedia_14_16_87, lr=0.2, bsz=8 ...
06/18/2022 05:02:09 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 05:02:09 - INFO - __main__ - Printing 3 examples
06/18/2022 05:02:09 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
06/18/2022 05:02:09 - INFO - __main__ - ['Film']
06/18/2022 05:02:09 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
06/18/2022 05:02:09 - INFO - __main__ - ['Film']
06/18/2022 05:02:09 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
06/18/2022 05:02:09 - INFO - __main__ - ['Film']
06/18/2022 05:02:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 05:02:09 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:02:10 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
06/18/2022 05:02:10 - INFO - __main__ - Start tokenizing ... 224 instances
06/18/2022 05:02:10 - INFO - __main__ - Printing 3 examples
06/18/2022 05:02:10 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
06/18/2022 05:02:10 - INFO - __main__ - ['Film']
06/18/2022 05:02:10 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
06/18/2022 05:02:10 - INFO - __main__ - ['Film']
06/18/2022 05:02:10 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
06/18/2022 05:02:10 - INFO - __main__ - ['Film']
06/18/2022 05:02:10 - INFO - __main__ - Tokenizing Input ...
06/18/2022 05:02:10 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:02:10 - INFO - __main__ - Loaded 224 examples from dev data
06/18/2022 05:02:25 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 05:02:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 05:02:26 - INFO - __main__ - Starting training!
06/18/2022 05:02:30 - INFO - __main__ - Step 10 Global step 10 Train loss 7.45 on epoch=0
06/18/2022 05:02:32 - INFO - __main__ - Step 20 Global step 20 Train loss 5.48 on epoch=1
06/18/2022 05:02:35 - INFO - __main__ - Step 30 Global step 30 Train loss 5.12 on epoch=2
06/18/2022 05:02:37 - INFO - __main__ - Step 40 Global step 40 Train loss 4.46 on epoch=2
06/18/2022 05:02:40 - INFO - __main__ - Step 50 Global step 50 Train loss 4.63 on epoch=3
06/18/2022 05:02:47 - INFO - __main__ - Global step 50 Train loss 5.43 Classification-F1 0.026548271130314476 on epoch=3
06/18/2022 05:02:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.026548271130314476 on epoch=3, global_step=50
06/18/2022 05:02:49 - INFO - __main__ - Step 60 Global step 60 Train loss 4.37 on epoch=4
06/18/2022 05:02:52 - INFO - __main__ - Step 70 Global step 70 Train loss 3.60 on epoch=4
06/18/2022 05:02:54 - INFO - __main__ - Step 80 Global step 80 Train loss 4.06 on epoch=5
06/18/2022 05:02:57 - INFO - __main__ - Step 90 Global step 90 Train loss 3.57 on epoch=6
06/18/2022 05:03:00 - INFO - __main__ - Step 100 Global step 100 Train loss 3.38 on epoch=7
06/18/2022 05:03:05 - INFO - __main__ - Global step 100 Train loss 3.80 Classification-F1 0.08999842382531843 on epoch=7
06/18/2022 05:03:05 - INFO - __main__ - Saving model with best Classification-F1: 0.026548271130314476 -> 0.08999842382531843 on epoch=7, global_step=100
06/18/2022 05:03:07 - INFO - __main__ - Step 110 Global step 110 Train loss 3.10 on epoch=7
06/18/2022 05:03:10 - INFO - __main__ - Step 120 Global step 120 Train loss 3.13 on epoch=8
06/18/2022 05:03:13 - INFO - __main__ - Step 130 Global step 130 Train loss 2.85 on epoch=9
06/18/2022 05:03:15 - INFO - __main__ - Step 140 Global step 140 Train loss 2.59 on epoch=9
06/18/2022 05:03:18 - INFO - __main__ - Step 150 Global step 150 Train loss 2.74 on epoch=10
06/18/2022 05:03:23 - INFO - __main__ - Global step 150 Train loss 2.88 Classification-F1 0.11528598840426797 on epoch=10
06/18/2022 05:03:23 - INFO - __main__ - Saving model with best Classification-F1: 0.08999842382531843 -> 0.11528598840426797 on epoch=10, global_step=150
06/18/2022 05:03:26 - INFO - __main__ - Step 160 Global step 160 Train loss 2.37 on epoch=11
06/18/2022 05:03:28 - INFO - __main__ - Step 170 Global step 170 Train loss 2.34 on epoch=12
06/18/2022 05:03:31 - INFO - __main__ - Step 180 Global step 180 Train loss 1.98 on epoch=12
06/18/2022 05:03:33 - INFO - __main__ - Step 190 Global step 190 Train loss 2.22 on epoch=13
06/18/2022 05:03:36 - INFO - __main__ - Step 200 Global step 200 Train loss 2.04 on epoch=14
06/18/2022 05:03:41 - INFO - __main__ - Global step 200 Train loss 2.19 Classification-F1 0.12308497286457616 on epoch=14
06/18/2022 05:03:41 - INFO - __main__ - Saving model with best Classification-F1: 0.11528598840426797 -> 0.12308497286457616 on epoch=14, global_step=200
06/18/2022 05:03:44 - INFO - __main__ - Step 210 Global step 210 Train loss 1.88 on epoch=14
06/18/2022 05:03:46 - INFO - __main__ - Step 220 Global step 220 Train loss 2.27 on epoch=15
06/18/2022 05:03:49 - INFO - __main__ - Step 230 Global step 230 Train loss 1.80 on epoch=16
06/18/2022 05:03:52 - INFO - __main__ - Step 240 Global step 240 Train loss 1.72 on epoch=17
06/18/2022 05:03:54 - INFO - __main__ - Step 250 Global step 250 Train loss 1.62 on epoch=17
06/18/2022 05:03:59 - INFO - __main__ - Global step 250 Train loss 1.86 Classification-F1 0.13690792875211621 on epoch=17
06/18/2022 05:03:59 - INFO - __main__ - Saving model with best Classification-F1: 0.12308497286457616 -> 0.13690792875211621 on epoch=17, global_step=250
06/18/2022 05:04:02 - INFO - __main__ - Step 260 Global step 260 Train loss 1.86 on epoch=18
06/18/2022 05:04:04 - INFO - __main__ - Step 270 Global step 270 Train loss 1.58 on epoch=19
06/18/2022 05:04:07 - INFO - __main__ - Step 280 Global step 280 Train loss 1.41 on epoch=19
06/18/2022 05:04:10 - INFO - __main__ - Step 290 Global step 290 Train loss 1.62 on epoch=20
06/18/2022 05:04:12 - INFO - __main__ - Step 300 Global step 300 Train loss 1.45 on epoch=21
06/18/2022 05:04:18 - INFO - __main__ - Global step 300 Train loss 1.58 Classification-F1 0.17133788667533928 on epoch=21
06/18/2022 05:04:18 - INFO - __main__ - Saving model with best Classification-F1: 0.13690792875211621 -> 0.17133788667533928 on epoch=21, global_step=300
06/18/2022 05:04:20 - INFO - __main__ - Step 310 Global step 310 Train loss 1.36 on epoch=22
06/18/2022 05:04:23 - INFO - __main__ - Step 320 Global step 320 Train loss 1.25 on epoch=22
06/18/2022 05:04:26 - INFO - __main__ - Step 330 Global step 330 Train loss 1.20 on epoch=23
06/18/2022 05:04:28 - INFO - __main__ - Step 340 Global step 340 Train loss 1.20 on epoch=24
06/18/2022 05:04:31 - INFO - __main__ - Step 350 Global step 350 Train loss 1.02 on epoch=24
06/18/2022 05:04:37 - INFO - __main__ - Global step 350 Train loss 1.21 Classification-F1 0.24990313358476052 on epoch=24
06/18/2022 05:04:37 - INFO - __main__ - Saving model with best Classification-F1: 0.17133788667533928 -> 0.24990313358476052 on epoch=24, global_step=350
06/18/2022 05:04:39 - INFO - __main__ - Step 360 Global step 360 Train loss 1.23 on epoch=25
06/18/2022 05:04:42 - INFO - __main__ - Step 370 Global step 370 Train loss 1.10 on epoch=26
06/18/2022 05:04:44 - INFO - __main__ - Step 380 Global step 380 Train loss 1.04 on epoch=27
06/18/2022 05:04:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.92 on epoch=27
06/18/2022 05:04:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.96 on epoch=28
06/18/2022 05:04:56 - INFO - __main__ - Global step 400 Train loss 1.05 Classification-F1 0.34342235331156185 on epoch=28
06/18/2022 05:04:56 - INFO - __main__ - Saving model with best Classification-F1: 0.24990313358476052 -> 0.34342235331156185 on epoch=28, global_step=400
06/18/2022 05:04:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.98 on epoch=29
06/18/2022 05:05:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.87 on epoch=29
06/18/2022 05:05:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.83 on epoch=30
06/18/2022 05:05:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.84 on epoch=31
06/18/2022 05:05:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.77 on epoch=32
06/18/2022 05:05:16 - INFO - __main__ - Global step 450 Train loss 0.86 Classification-F1 0.48032743080940427 on epoch=32
06/18/2022 05:05:16 - INFO - __main__ - Saving model with best Classification-F1: 0.34342235331156185 -> 0.48032743080940427 on epoch=32, global_step=450
06/18/2022 05:05:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.65 on epoch=32
06/18/2022 05:05:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.66 on epoch=33
06/18/2022 05:05:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.70 on epoch=34
06/18/2022 05:05:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.75 on epoch=34
06/18/2022 05:05:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.62 on epoch=35
06/18/2022 05:05:36 - INFO - __main__ - Global step 500 Train loss 0.68 Classification-F1 0.4606033644605053 on epoch=35
06/18/2022 05:05:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=36
06/18/2022 05:05:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=37
06/18/2022 05:05:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.58 on epoch=37
06/18/2022 05:05:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=38
06/18/2022 05:05:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.58 on epoch=39
06/18/2022 05:05:56 - INFO - __main__ - Global step 550 Train loss 0.56 Classification-F1 0.5344033343156765 on epoch=39
06/18/2022 05:05:56 - INFO - __main__ - Saving model with best Classification-F1: 0.48032743080940427 -> 0.5344033343156765 on epoch=39, global_step=550
06/18/2022 05:05:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=39
06/18/2022 05:06:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.53 on epoch=40
06/18/2022 05:06:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=41
06/18/2022 05:06:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=42
06/18/2022 05:06:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=42
06/18/2022 05:06:16 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.5674244691667496 on epoch=42
06/18/2022 05:06:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5344033343156765 -> 0.5674244691667496 on epoch=42, global_step=600
06/18/2022 05:06:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.50 on epoch=43
06/18/2022 05:06:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=44
06/18/2022 05:06:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=44
06/18/2022 05:06:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=45
06/18/2022 05:06:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=46
06/18/2022 05:06:36 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.6672735475230164 on epoch=46
06/18/2022 05:06:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5674244691667496 -> 0.6672735475230164 on epoch=46, global_step=650
06/18/2022 05:06:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=47
06/18/2022 05:06:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=47
06/18/2022 05:06:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=48
06/18/2022 05:06:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=49
06/18/2022 05:06:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=49
06/18/2022 05:06:56 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.6687746264234761 on epoch=49
06/18/2022 05:06:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6672735475230164 -> 0.6687746264234761 on epoch=49, global_step=700
06/18/2022 05:06:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.32 on epoch=50
06/18/2022 05:07:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=51
06/18/2022 05:07:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=52
06/18/2022 05:07:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=52
06/18/2022 05:07:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=53
06/18/2022 05:07:16 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.7143763090566728 on epoch=53
06/18/2022 05:07:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6687746264234761 -> 0.7143763090566728 on epoch=53, global_step=750
06/18/2022 05:07:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=54
06/18/2022 05:07:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.30 on epoch=54
06/18/2022 05:07:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=55
06/18/2022 05:07:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=56
06/18/2022 05:07:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=57
06/18/2022 05:07:36 - INFO - __main__ - Global step 800 Train loss 0.30 Classification-F1 0.7564230315206987 on epoch=57
06/18/2022 05:07:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7143763090566728 -> 0.7564230315206987 on epoch=57, global_step=800
06/18/2022 05:07:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=57
06/18/2022 05:07:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=58
06/18/2022 05:07:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=59
06/18/2022 05:07:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=59
06/18/2022 05:07:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=60
06/18/2022 05:07:56 - INFO - __main__ - Global step 850 Train loss 0.27 Classification-F1 0.7516531539301937 on epoch=60
06/18/2022 05:07:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=61
06/18/2022 05:08:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=62
06/18/2022 05:08:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=62
06/18/2022 05:08:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=63
06/18/2022 05:08:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=64
06/18/2022 05:08:16 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.7702290053233827 on epoch=64
06/18/2022 05:08:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7564230315206987 -> 0.7702290053233827 on epoch=64, global_step=900
06/18/2022 05:08:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.28 on epoch=64
06/18/2022 05:08:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=65
06/18/2022 05:08:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=66
06/18/2022 05:08:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=67
06/18/2022 05:08:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=67
06/18/2022 05:08:35 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.7662299894845861 on epoch=67
06/18/2022 05:08:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=68
06/18/2022 05:08:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=69
06/18/2022 05:08:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=69
06/18/2022 05:08:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=70
06/18/2022 05:08:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=71
06/18/2022 05:08:55 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.7662299894845861 on epoch=71
06/18/2022 05:08:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=72
06/18/2022 05:09:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=72
06/18/2022 05:09:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=73
06/18/2022 05:09:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=74
06/18/2022 05:09:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=74
06/18/2022 05:09:15 - INFO - __main__ - Global step 1050 Train loss 0.23 Classification-F1 0.7622421333549808 on epoch=74
06/18/2022 05:09:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=75
06/18/2022 05:09:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=76
06/18/2022 05:09:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.16 on epoch=77
06/18/2022 05:09:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=77
06/18/2022 05:09:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=78
06/18/2022 05:09:34 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.7685201157686926 on epoch=78
06/18/2022 05:09:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=79
06/18/2022 05:09:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=79
06/18/2022 05:09:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=80
06/18/2022 05:09:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=81
06/18/2022 05:09:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=82
06/18/2022 05:09:54 - INFO - __main__ - Global step 1150 Train loss 0.17 Classification-F1 0.8463465298142718 on epoch=82
06/18/2022 05:09:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7702290053233827 -> 0.8463465298142718 on epoch=82, global_step=1150
06/18/2022 05:09:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=82
06/18/2022 05:09:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=83
06/18/2022 05:10:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.17 on epoch=84
06/18/2022 05:10:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.11 on epoch=84
06/18/2022 05:10:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=85
06/18/2022 05:10:14 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.8275293255131965 on epoch=85
06/18/2022 05:10:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=86
06/18/2022 05:10:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=87
06/18/2022 05:10:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=87
06/18/2022 05:10:24 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=88
06/18/2022 05:10:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.13 on epoch=89
06/18/2022 05:10:33 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.8424364613880744 on epoch=89
06/18/2022 05:10:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=89
06/18/2022 05:10:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=90
06/18/2022 05:10:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=91
06/18/2022 05:10:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=92
06/18/2022 05:10:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=92
06/18/2022 05:10:53 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.7996986947271577 on epoch=92
06/18/2022 05:10:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=93
06/18/2022 05:10:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=94
06/18/2022 05:11:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=94
06/18/2022 05:11:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=95
06/18/2022 05:11:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.10 on epoch=96
06/18/2022 05:11:13 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.7437002091981054 on epoch=96
06/18/2022 05:11:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=97
06/18/2022 05:11:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=97
06/18/2022 05:11:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=98
06/18/2022 05:11:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.14 on epoch=99
06/18/2022 05:11:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.15 on epoch=99
06/18/2022 05:11:33 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.8516957206473336 on epoch=99
06/18/2022 05:11:33 - INFO - __main__ - Saving model with best Classification-F1: 0.8463465298142718 -> 0.8516957206473336 on epoch=99, global_step=1400
06/18/2022 05:11:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=100
06/18/2022 05:11:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=101
06/18/2022 05:11:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=102
06/18/2022 05:11:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=102
06/18/2022 05:11:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=103
06/18/2022 05:11:52 - INFO - __main__ - Global step 1450 Train loss 0.11 Classification-F1 0.8568042774800284 on epoch=103
06/18/2022 05:11:52 - INFO - __main__ - Saving model with best Classification-F1: 0.8516957206473336 -> 0.8568042774800284 on epoch=103, global_step=1450
06/18/2022 05:11:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=104
06/18/2022 05:11:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=104
06/18/2022 05:12:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=105
06/18/2022 05:12:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=106
06/18/2022 05:12:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=107
06/18/2022 05:12:12 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.801595972373961 on epoch=107
06/18/2022 05:12:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=107
06/18/2022 05:12:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=108
06/18/2022 05:12:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=109
06/18/2022 05:12:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=109
06/18/2022 05:12:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=110
06/18/2022 05:12:31 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.849545183012925 on epoch=110
06/18/2022 05:12:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=111
06/18/2022 05:12:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=112
06/18/2022 05:12:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=112
06/18/2022 05:12:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=113
06/18/2022 05:12:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=114
06/18/2022 05:12:50 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.849545183012925 on epoch=114
06/18/2022 05:12:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.11 on epoch=114
06/18/2022 05:12:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.16 on epoch=115
06/18/2022 05:12:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=116
06/18/2022 05:13:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=117
06/18/2022 05:13:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=117
06/18/2022 05:13:10 - INFO - __main__ - Global step 1650 Train loss 0.14 Classification-F1 0.7551512737892667 on epoch=117
06/18/2022 05:13:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=118
06/18/2022 05:13:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=119
06/18/2022 05:13:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=119
06/18/2022 05:13:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=120
06/18/2022 05:13:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=121
06/18/2022 05:13:30 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.8568042774800284 on epoch=121
06/18/2022 05:13:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=122
06/18/2022 05:13:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=122
06/18/2022 05:13:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=123
06/18/2022 05:13:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=124
06/18/2022 05:13:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=124
06/18/2022 05:13:50 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.8100840902646831 on epoch=124
06/18/2022 05:13:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=125
06/18/2022 05:13:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=126
06/18/2022 05:13:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=127
06/18/2022 05:14:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=127
06/18/2022 05:14:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=128
06/18/2022 05:14:09 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.810223678914381 on epoch=128
06/18/2022 05:14:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=129
06/18/2022 05:14:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=129
06/18/2022 05:14:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=130
06/18/2022 05:14:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=131
06/18/2022 05:14:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=132
06/18/2022 05:14:29 - INFO - __main__ - Global step 1850 Train loss 0.07 Classification-F1 0.8569525904203323 on epoch=132
06/18/2022 05:14:29 - INFO - __main__ - Saving model with best Classification-F1: 0.8568042774800284 -> 0.8569525904203323 on epoch=132, global_step=1850
06/18/2022 05:14:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=132
06/18/2022 05:14:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=133
06/18/2022 05:14:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=134
06/18/2022 05:14:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=134
06/18/2022 05:14:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=135
06/18/2022 05:14:49 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.8043799904429363 on epoch=135
06/18/2022 05:14:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=136
06/18/2022 05:14:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=137
06/18/2022 05:14:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=137
06/18/2022 05:14:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=138
06/18/2022 05:15:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=139
06/18/2022 05:15:09 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.859103128054741 on epoch=139
06/18/2022 05:15:09 - INFO - __main__ - Saving model with best Classification-F1: 0.8569525904203323 -> 0.859103128054741 on epoch=139, global_step=1950
06/18/2022 05:15:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=139
06/18/2022 05:15:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=140
06/18/2022 05:15:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=141
06/18/2022 05:15:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=142
06/18/2022 05:15:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=142
06/18/2022 05:15:28 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.9186705767350929 on epoch=142
06/18/2022 05:15:28 - INFO - __main__ - Saving model with best Classification-F1: 0.859103128054741 -> 0.9186705767350929 on epoch=142, global_step=2000
06/18/2022 05:15:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=143
06/18/2022 05:15:34 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=144
06/18/2022 05:15:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=144
06/18/2022 05:15:39 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=145
06/18/2022 05:15:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=146
06/18/2022 05:15:48 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.810223678914381 on epoch=146
06/18/2022 05:15:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=147
06/18/2022 05:15:53 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=147
06/18/2022 05:15:56 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=148
06/18/2022 05:15:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=149
06/18/2022 05:16:01 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=149
06/18/2022 05:16:08 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.8063823784259023 on epoch=149
06/18/2022 05:16:10 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=150
06/18/2022 05:16:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=151
06/18/2022 05:16:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=152
06/18/2022 05:16:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=152
06/18/2022 05:16:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=153
06/18/2022 05:16:27 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.8063823784259023 on epoch=153
06/18/2022 05:16:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=154
06/18/2022 05:16:32 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=154
06/18/2022 05:16:35 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=155
06/18/2022 05:16:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=156
06/18/2022 05:16:40 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=157
06/18/2022 05:16:46 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.8006999260418093 on epoch=157
06/18/2022 05:16:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=157
06/18/2022 05:16:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=158
06/18/2022 05:16:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=159
06/18/2022 05:16:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=159
06/18/2022 05:17:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=160
06/18/2022 05:17:06 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.7577806241877432 on epoch=160
06/18/2022 05:17:08 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=161
06/18/2022 05:17:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=162
06/18/2022 05:17:14 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=162
06/18/2022 05:17:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=163
06/18/2022 05:17:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=164
06/18/2022 05:17:25 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.8046460813064229 on epoch=164
06/18/2022 05:17:28 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=164
06/18/2022 05:17:31 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=165
06/18/2022 05:17:33 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=166
06/18/2022 05:17:36 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=167
06/18/2022 05:17:38 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=167
06/18/2022 05:17:45 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7540686808762449 on epoch=167
06/18/2022 05:17:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=168
06/18/2022 05:17:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=169
06/18/2022 05:17:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=169
06/18/2022 05:17:55 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=170
06/18/2022 05:17:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=171
06/18/2022 05:18:05 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.8005632765916202 on epoch=171
06/18/2022 05:18:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=172
06/18/2022 05:18:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=172
06/18/2022 05:18:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=173
06/18/2022 05:18:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=174
06/18/2022 05:18:18 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=174
06/18/2022 05:18:24 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.8005632765916202 on epoch=174
06/18/2022 05:18:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=175
06/18/2022 05:18:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=176
06/18/2022 05:18:32 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=177
06/18/2022 05:18:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=177
06/18/2022 05:18:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=178
06/18/2022 05:18:43 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.8550255647119299 on epoch=178
06/18/2022 05:18:46 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=179
06/18/2022 05:18:49 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=179
06/18/2022 05:18:51 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=180
06/18/2022 05:18:54 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=181
06/18/2022 05:18:56 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=182
06/18/2022 05:19:03 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.9056888851876747 on epoch=182
06/18/2022 05:19:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=182
06/18/2022 05:19:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=183
06/18/2022 05:19:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=184
06/18/2022 05:19:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=184
06/18/2022 05:19:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=185
06/18/2022 05:19:22 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.855196878054741 on epoch=185
06/18/2022 05:19:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=186
06/18/2022 05:19:27 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=187
06/18/2022 05:19:30 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=187
06/18/2022 05:19:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=188
06/18/2022 05:19:35 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=189
06/18/2022 05:19:42 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.9056888851876747 on epoch=189
06/18/2022 05:19:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=189
06/18/2022 05:19:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=190
06/18/2022 05:19:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=191
06/18/2022 05:19:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=192
06/18/2022 05:19:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=192
06/18/2022 05:20:01 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.9143564679048553 on epoch=192
06/18/2022 05:20:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=193
06/18/2022 05:20:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=194
06/18/2022 05:20:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=194
06/18/2022 05:20:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=195
06/18/2022 05:20:15 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=196
06/18/2022 05:20:21 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.9100029940179126 on epoch=196
06/18/2022 05:20:24 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=197
06/18/2022 05:20:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=197
06/18/2022 05:20:29 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=198
06/18/2022 05:20:32 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=199
06/18/2022 05:20:34 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=199
06/18/2022 05:20:41 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.9186705767350929 on epoch=199
06/18/2022 05:20:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=200
06/18/2022 05:20:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=201
06/18/2022 05:20:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=202
06/18/2022 05:20:51 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=202
06/18/2022 05:20:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=203
06/18/2022 05:21:01 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.9055830191314063 on epoch=203
06/18/2022 05:21:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=204
06/18/2022 05:21:06 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=204
06/18/2022 05:21:09 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=205
06/18/2022 05:21:11 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=206
06/18/2022 05:21:14 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
06/18/2022 05:21:20 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.9143564679048553 on epoch=207
06/18/2022 05:21:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=207
06/18/2022 05:21:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=208
06/18/2022 05:21:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=209
06/18/2022 05:21:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=209
06/18/2022 05:21:33 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=210
06/18/2022 05:21:40 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.9143564679048553 on epoch=210
06/18/2022 05:21:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=211
06/18/2022 05:21:45 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=212
06/18/2022 05:21:48 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=212
06/18/2022 05:21:50 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=213
06/18/2022 05:21:53 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=214
06/18/2022 05:21:59 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.8064076196764479 on epoch=214
06/18/2022 05:21:59 - INFO - __main__ - save last model!
06/18/2022 05:21:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 05:21:59 - INFO - __main__ - Start tokenizing ... 3500 instances
06/18/2022 05:21:59 - INFO - __main__ - Printing 3 examples
06/18/2022 05:21:59 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
06/18/2022 05:21:59 - INFO - __main__ - ['Animal']
06/18/2022 05:21:59 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
06/18/2022 05:21:59 - INFO - __main__ - ['Animal']
06/18/2022 05:21:59 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
06/18/2022 05:21:59 - INFO - __main__ - ['Village']
06/18/2022 05:21:59 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 05:22:01 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:22:05 - INFO - __main__ - Loaded 3500 examples from test data
06/18/2022 05:24:14 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-dbpedia_14/dbpedia_14_16_87_0.2_8_predictions.txt
06/18/2022 05:24:14 - INFO - __main__ - Classification-F1 on test data: 0.4678
06/18/2022 05:24:15 - INFO - __main__ - prefix=dbpedia_14_16_87, lr=0.2, bsz=8, dev_performance=0.9186705767350929, test_performance=0.46782794641742337
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (85251): No such process
Task: wiki_qa, Checkpoint: models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt, Identifier: T5-large-multitask-cls2cls-5e-1-4-10-up64shot
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/singletask_from_multi_cls2cls.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/singletask_from_multi_cls2cls.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-wiki_qa/log.txt'
06/18/2022 05:24:20 - INFO - __main__ - Namespace(task_dir='data/wiki_qa/', task_name='wiki_qa', identifier='T5-large-multitask-cls2cls-5e-1-4-10-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/18/2022 05:24:20 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-wiki_qa
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_multi_cls2cls.py', '--local_rank=1', '--task_dir', 'data/wiki_qa/', '--task_name', 'wiki_qa', '--identifier', 'T5-large-multitask-cls2cls-5e-1-4-10-up64shot', '--checkpoint', 'models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-wiki_qa', '--cuda', '4,5', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 85257
Killing subprocess 85258
++++++++++++++++++++++++++++++
kill: (85264): No such process
Task: emo, Checkpoint: models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt, Identifier: T5-large-multitask-cls2cls-5e-1-4-10-up64shot
06/18/2022 05:24:24 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-10-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/18/2022 05:24:24 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo
06/18/2022 05:24:24 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-10-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/18/2022 05:24:24 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo
06/18/2022 05:24:25 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/18/2022 05:24:25 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/18/2022 05:24:25 - INFO - __main__ - args.device: cuda:0
06/18/2022 05:24:25 - INFO - __main__ - Using 2 gpus
06/18/2022 05:24:25 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
06/18/2022 05:24:25 - INFO - __main__ - args.device: cuda:1
06/18/2022 05:24:25 - INFO - __main__ - Using 2 gpus
06/18/2022 05:24:25 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
06/18/2022 05:24:29 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
06/18/2022 05:24:30 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 05:24:30 - INFO - __main__ - Printing 3 examples
06/18/2022 05:24:30 - INFO - __main__ -  [emo] how cause yes am listening
06/18/2022 05:24:30 - INFO - __main__ - ['others']
06/18/2022 05:24:30 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/18/2022 05:24:30 - INFO - __main__ - ['others']
06/18/2022 05:24:30 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/18/2022 05:24:30 - INFO - __main__ - ['others']
06/18/2022 05:24:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 05:24:30 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:24:30 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 05:24:30 - INFO - __main__ - Printing 3 examples
06/18/2022 05:24:30 - INFO - __main__ -  [emo] how cause yes am listening
06/18/2022 05:24:30 - INFO - __main__ - ['others']
06/18/2022 05:24:30 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/18/2022 05:24:30 - INFO - __main__ - ['others']
06/18/2022 05:24:30 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/18/2022 05:24:30 - INFO - __main__ - ['others']
06/18/2022 05:24:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 05:24:30 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:24:30 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 05:24:30 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 05:24:30 - INFO - __main__ - Printing 3 examples
06/18/2022 05:24:30 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/18/2022 05:24:30 - INFO - __main__ - ['others']
06/18/2022 05:24:30 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/18/2022 05:24:30 - INFO - __main__ - ['others']
06/18/2022 05:24:30 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/18/2022 05:24:30 - INFO - __main__ - ['others']
06/18/2022 05:24:30 - INFO - __main__ - Tokenizing Input ...
06/18/2022 05:24:30 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 05:24:30 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 05:24:30 - INFO - __main__ - Printing 3 examples
06/18/2022 05:24:30 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/18/2022 05:24:30 - INFO - __main__ - ['others']
06/18/2022 05:24:30 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/18/2022 05:24:30 - INFO - __main__ - ['others']
06/18/2022 05:24:30 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/18/2022 05:24:30 - INFO - __main__ - ['others']
06/18/2022 05:24:30 - INFO - __main__ - Tokenizing Input ...
06/18/2022 05:24:30 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:24:30 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:24:30 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 05:24:30 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 05:24:48 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 05:24:48 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 05:24:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 05:24:49 - INFO - __main__ - Starting training!
06/18/2022 05:24:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 05:24:54 - INFO - __main__ - Starting training!
06/18/2022 05:24:57 - INFO - __main__ - Step 10 Global step 10 Train loss 4.25 on epoch=2
06/18/2022 05:24:59 - INFO - __main__ - Step 20 Global step 20 Train loss 2.80 on epoch=4
06/18/2022 05:25:02 - INFO - __main__ - Step 30 Global step 30 Train loss 1.98 on epoch=7
06/18/2022 05:25:04 - INFO - __main__ - Step 40 Global step 40 Train loss 1.51 on epoch=9
06/18/2022 05:25:07 - INFO - __main__ - Step 50 Global step 50 Train loss 1.25 on epoch=12
06/18/2022 05:25:08 - INFO - __main__ - Global step 50 Train loss 2.36 Classification-F1 0.2701449810693508 on epoch=12
06/18/2022 05:25:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2701449810693508 on epoch=12, global_step=50
06/18/2022 05:25:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.85 on epoch=14
06/18/2022 05:25:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.93 on epoch=17
06/18/2022 05:25:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.79 on epoch=19
06/18/2022 05:25:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.77 on epoch=22
06/18/2022 05:25:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.68 on epoch=24
06/18/2022 05:25:21 - INFO - __main__ - Global step 100 Train loss 0.80 Classification-F1 0.5469294425087108 on epoch=24
06/18/2022 05:25:21 - INFO - __main__ - Saving model with best Classification-F1: 0.2701449810693508 -> 0.5469294425087108 on epoch=24, global_step=100
06/18/2022 05:25:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.79 on epoch=27
06/18/2022 05:25:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.64 on epoch=29
06/18/2022 05:25:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.63 on epoch=32
06/18/2022 05:25:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.63 on epoch=34
06/18/2022 05:25:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=37
06/18/2022 05:25:34 - INFO - __main__ - Global step 150 Train loss 0.65 Classification-F1 0.5711805555555556 on epoch=37
06/18/2022 05:25:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5469294425087108 -> 0.5711805555555556 on epoch=37, global_step=150
06/18/2022 05:25:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=39
06/18/2022 05:25:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=42
06/18/2022 05:25:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=44
06/18/2022 05:25:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=47
06/18/2022 05:25:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=49
06/18/2022 05:25:47 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.6199682674428005 on epoch=49
06/18/2022 05:25:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5711805555555556 -> 0.6199682674428005 on epoch=49, global_step=200
06/18/2022 05:25:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=52
06/18/2022 05:25:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=54
06/18/2022 05:25:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=57
06/18/2022 05:25:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=59
06/18/2022 05:25:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=62
06/18/2022 05:26:00 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.5882976672450356 on epoch=62
06/18/2022 05:26:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=64
06/18/2022 05:26:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=67
06/18/2022 05:26:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=69
06/18/2022 05:26:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=72
06/18/2022 05:26:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.34 on epoch=74
06/18/2022 05:26:13 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.635554818744474 on epoch=74
06/18/2022 05:26:13 - INFO - __main__ - Saving model with best Classification-F1: 0.6199682674428005 -> 0.635554818744474 on epoch=74, global_step=300
06/18/2022 05:26:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=77
06/18/2022 05:26:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=79
06/18/2022 05:26:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=82
06/18/2022 05:26:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=84
06/18/2022 05:26:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=87
06/18/2022 05:26:26 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.6232909860859044 on epoch=87
06/18/2022 05:26:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=89
06/18/2022 05:26:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=92
06/18/2022 05:26:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=94
06/18/2022 05:26:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=97
06/18/2022 05:26:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=99
06/18/2022 05:26:39 - INFO - __main__ - Global step 400 Train loss 0.28 Classification-F1 0.6934697855750487 on epoch=99
06/18/2022 05:26:39 - INFO - __main__ - Saving model with best Classification-F1: 0.635554818744474 -> 0.6934697855750487 on epoch=99, global_step=400
06/18/2022 05:26:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=102
06/18/2022 05:26:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=104
06/18/2022 05:26:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.30 on epoch=107
06/18/2022 05:26:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=109
06/18/2022 05:26:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.28 on epoch=112
06/18/2022 05:26:52 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.6781258702311335 on epoch=112
06/18/2022 05:26:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=114
06/18/2022 05:26:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=117
06/18/2022 05:26:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=119
06/18/2022 05:27:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=122
06/18/2022 05:27:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=124
06/18/2022 05:27:05 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.6436122357174989 on epoch=124
06/18/2022 05:27:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.18 on epoch=127
06/18/2022 05:27:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=129
06/18/2022 05:27:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=132
06/18/2022 05:27:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=134
06/18/2022 05:27:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.13 on epoch=137
06/18/2022 05:27:18 - INFO - __main__ - Global step 550 Train loss 0.18 Classification-F1 0.6203708133971292 on epoch=137
06/18/2022 05:27:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.13 on epoch=139
06/18/2022 05:27:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=142
06/18/2022 05:27:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=144
06/18/2022 05:27:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=147
06/18/2022 05:27:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=149
06/18/2022 05:27:31 - INFO - __main__ - Global step 600 Train loss 0.15 Classification-F1 0.6899554727140933 on epoch=149
06/18/2022 05:27:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=152
06/18/2022 05:27:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=154
06/18/2022 05:27:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=157
06/18/2022 05:27:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=159
06/18/2022 05:27:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=162
06/18/2022 05:27:44 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.6899554727140933 on epoch=162
06/18/2022 05:27:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=164
06/18/2022 05:27:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=167
06/18/2022 05:27:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=169
06/18/2022 05:27:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=172
06/18/2022 05:27:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=174
06/18/2022 05:27:57 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.6303258145363408 on epoch=174
06/18/2022 05:27:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=177
06/18/2022 05:28:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=179
06/18/2022 05:28:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=182
06/18/2022 05:28:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=184
06/18/2022 05:28:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=187
06/18/2022 05:28:10 - INFO - __main__ - Global step 750 Train loss 0.10 Classification-F1 0.6746031746031746 on epoch=187
06/18/2022 05:28:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=189
06/18/2022 05:28:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=192
06/18/2022 05:28:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=194
06/18/2022 05:28:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=197
06/18/2022 05:28:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=199
06/18/2022 05:28:23 - INFO - __main__ - Global step 800 Train loss 0.09 Classification-F1 0.682093278737641 on epoch=199
06/18/2022 05:28:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=202
06/18/2022 05:28:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=204
06/18/2022 05:28:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=207
06/18/2022 05:28:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=209
06/18/2022 05:28:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=212
06/18/2022 05:28:36 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.6288882741971987 on epoch=212
06/18/2022 05:28:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=214
06/18/2022 05:28:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=217
06/18/2022 05:28:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=219
06/18/2022 05:28:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=222
06/18/2022 05:28:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=224
06/18/2022 05:28:49 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.6410716069110157 on epoch=224
06/18/2022 05:28:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=227
06/18/2022 05:28:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=229
06/18/2022 05:28:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
06/18/2022 05:28:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=234
06/18/2022 05:29:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=237
06/18/2022 05:29:02 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.6700142994260642 on epoch=237
06/18/2022 05:29:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=239
06/18/2022 05:29:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
06/18/2022 05:29:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=244
06/18/2022 05:29:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=247
06/18/2022 05:29:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=249
06/18/2022 05:29:15 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.6607142857142858 on epoch=249
06/18/2022 05:29:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
06/18/2022 05:29:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
06/18/2022 05:29:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=257
06/18/2022 05:29:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=259
06/18/2022 05:29:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=262
06/18/2022 05:29:28 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.7037050787050787 on epoch=262
06/18/2022 05:29:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6934697855750487 -> 0.7037050787050787 on epoch=262, global_step=1050
06/18/2022 05:29:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
06/18/2022 05:29:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=267
06/18/2022 05:29:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=269
06/18/2022 05:29:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
06/18/2022 05:29:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
06/18/2022 05:29:41 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.6450327467845025 on epoch=274
06/18/2022 05:29:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
06/18/2022 05:29:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=279
06/18/2022 05:29:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
06/18/2022 05:29:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=284
06/18/2022 05:29:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
06/18/2022 05:29:54 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.6868843231072334 on epoch=287
06/18/2022 05:29:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
06/18/2022 05:29:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
06/18/2022 05:30:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
06/18/2022 05:30:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=297
06/18/2022 05:30:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=299
06/18/2022 05:30:07 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6820921195921196 on epoch=299
06/18/2022 05:30:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
06/18/2022 05:30:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
06/18/2022 05:30:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
06/18/2022 05:30:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
06/18/2022 05:30:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
06/18/2022 05:30:20 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.6954477889260497 on epoch=312
06/18/2022 05:30:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=314
06/18/2022 05:30:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
06/18/2022 05:30:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
06/18/2022 05:30:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=322
06/18/2022 05:30:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=324
06/18/2022 05:30:33 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.6792623344347483 on epoch=324
06/18/2022 05:30:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
06/18/2022 05:30:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
06/18/2022 05:30:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
06/18/2022 05:30:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=334
06/18/2022 05:30:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
06/18/2022 05:30:46 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.669924924924925 on epoch=337
06/18/2022 05:30:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
06/18/2022 05:30:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
06/18/2022 05:30:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
06/18/2022 05:30:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
06/18/2022 05:30:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=349
06/18/2022 05:30:59 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6880672268907563 on epoch=349
06/18/2022 05:31:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=352
06/18/2022 05:31:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
06/18/2022 05:31:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=357
06/18/2022 05:31:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
06/18/2022 05:31:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
06/18/2022 05:31:12 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.685502061605627 on epoch=362
06/18/2022 05:31:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
06/18/2022 05:31:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=367
06/18/2022 05:31:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=369
06/18/2022 05:31:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
06/18/2022 05:31:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
06/18/2022 05:31:25 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.685502061605627 on epoch=374
06/18/2022 05:31:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
06/18/2022 05:31:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
06/18/2022 05:31:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
06/18/2022 05:31:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
06/18/2022 05:31:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
06/18/2022 05:31:38 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7096453859321507 on epoch=387
06/18/2022 05:31:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7037050787050787 -> 0.7096453859321507 on epoch=387, global_step=1550
06/18/2022 05:31:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
06/18/2022 05:31:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
06/18/2022 05:31:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
06/18/2022 05:31:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=397
06/18/2022 05:31:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/18/2022 05:31:51 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.6683982683982684 on epoch=399
06/18/2022 05:31:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/18/2022 05:31:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
06/18/2022 05:31:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
06/18/2022 05:32:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/18/2022 05:32:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
06/18/2022 05:32:05 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.653758382442593 on epoch=412
06/18/2022 05:32:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
06/18/2022 05:32:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
06/18/2022 05:32:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=419
06/18/2022 05:32:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
06/18/2022 05:32:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
06/18/2022 05:32:18 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6649305555555556 on epoch=424
06/18/2022 05:32:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
06/18/2022 05:32:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/18/2022 05:32:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/18/2022 05:32:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
06/18/2022 05:32:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
06/18/2022 05:32:31 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.6939588189588191 on epoch=437
06/18/2022 05:32:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
06/18/2022 05:32:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
06/18/2022 05:32:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
06/18/2022 05:32:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
06/18/2022 05:32:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=449
06/18/2022 05:32:44 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.6891891891891893 on epoch=449
06/18/2022 05:32:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
06/18/2022 05:32:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
06/18/2022 05:32:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
06/18/2022 05:32:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
06/18/2022 05:32:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
06/18/2022 05:32:57 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.674689352108707 on epoch=462
06/18/2022 05:32:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/18/2022 05:33:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
06/18/2022 05:33:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
06/18/2022 05:33:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
06/18/2022 05:33:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
06/18/2022 05:33:10 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.6757278669043375 on epoch=474
06/18/2022 05:33:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
06/18/2022 05:33:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
06/18/2022 05:33:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/18/2022 05:33:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/18/2022 05:33:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
06/18/2022 05:33:23 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7080302050890286 on epoch=487
06/18/2022 05:33:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
06/18/2022 05:33:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
06/18/2022 05:33:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=494
06/18/2022 05:33:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
06/18/2022 05:33:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
06/18/2022 05:33:36 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7279130042821726 on epoch=499
06/18/2022 05:33:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7096453859321507 -> 0.7279130042821726 on epoch=499, global_step=2000
06/18/2022 05:33:39 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
06/18/2022 05:33:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
06/18/2022 05:33:44 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
06/18/2022 05:33:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
06/18/2022 05:33:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
06/18/2022 05:33:50 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6798649006845424 on epoch=512
06/18/2022 05:33:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
06/18/2022 05:33:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
06/18/2022 05:33:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
06/18/2022 05:33:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
06/18/2022 05:34:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
06/18/2022 05:34:03 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7001220575414123 on epoch=524
06/18/2022 05:34:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
06/18/2022 05:34:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/18/2022 05:34:10 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/18/2022 05:34:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
06/18/2022 05:34:15 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/18/2022 05:34:16 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7025906120023767 on epoch=537
06/18/2022 05:34:19 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/18/2022 05:34:21 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/18/2022 05:34:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
06/18/2022 05:34:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/18/2022 05:34:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/18/2022 05:34:30 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.689655172413793 on epoch=549
06/18/2022 05:34:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
06/18/2022 05:34:34 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
06/18/2022 05:34:37 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/18/2022 05:34:39 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
06/18/2022 05:34:42 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/18/2022 05:34:43 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7162933684672814 on epoch=562
06/18/2022 05:34:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
06/18/2022 05:34:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
06/18/2022 05:34:50 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/18/2022 05:34:53 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/18/2022 05:34:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/18/2022 05:34:56 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7250213346987541 on epoch=574
06/18/2022 05:34:59 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/18/2022 05:35:01 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/18/2022 05:35:03 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
06/18/2022 05:35:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
06/18/2022 05:35:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
06/18/2022 05:35:09 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7096453859321507 on epoch=587
06/18/2022 05:35:12 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/18/2022 05:35:14 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/18/2022 05:35:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/18/2022 05:35:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
06/18/2022 05:35:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/18/2022 05:35:23 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.7161764705882353 on epoch=599
06/18/2022 05:35:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
06/18/2022 05:35:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/18/2022 05:35:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/18/2022 05:35:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
06/18/2022 05:35:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=612
06/18/2022 05:35:36 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6929492291334396 on epoch=612
06/18/2022 05:35:38 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/18/2022 05:35:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/18/2022 05:35:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=619
06/18/2022 05:35:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
06/18/2022 05:35:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/18/2022 05:35:49 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6899381868131869 on epoch=624
06/18/2022 05:35:52 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/18/2022 05:35:54 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
06/18/2022 05:35:57 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/18/2022 05:35:59 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/18/2022 05:36:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=637
06/18/2022 05:36:03 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7443564745141661 on epoch=637
06/18/2022 05:36:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7279130042821726 -> 0.7443564745141661 on epoch=637, global_step=2550
06/18/2022 05:36:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/18/2022 05:36:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/18/2022 05:36:10 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/18/2022 05:36:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/18/2022 05:36:15 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/18/2022 05:36:16 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7314593301435407 on epoch=649
06/18/2022 05:36:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/18/2022 05:36:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/18/2022 05:36:23 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/18/2022 05:36:26 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 05:36:28 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
06/18/2022 05:36:29 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.736969696969697 on epoch=662
06/18/2022 05:36:32 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/18/2022 05:36:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/18/2022 05:36:37 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
06/18/2022 05:36:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/18/2022 05:36:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=674
06/18/2022 05:36:43 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7123200862911653 on epoch=674
06/18/2022 05:36:45 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
06/18/2022 05:36:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/18/2022 05:36:50 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
06/18/2022 05:36:53 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/18/2022 05:36:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
06/18/2022 05:36:56 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7073757818539602 on epoch=687
06/18/2022 05:36:59 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 05:37:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=692
06/18/2022 05:37:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/18/2022 05:37:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
06/18/2022 05:37:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/18/2022 05:37:09 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6885055304172951 on epoch=699
06/18/2022 05:37:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/18/2022 05:37:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/18/2022 05:37:17 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=707
06/18/2022 05:37:19 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 05:37:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/18/2022 05:37:23 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6885055304172951 on epoch=712
06/18/2022 05:37:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/18/2022 05:37:28 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
06/18/2022 05:37:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/18/2022 05:37:33 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
06/18/2022 05:37:35 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
06/18/2022 05:37:36 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7023529411764705 on epoch=724
06/18/2022 05:37:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/18/2022 05:37:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/18/2022 05:37:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
06/18/2022 05:37:46 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 05:37:48 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/18/2022 05:37:49 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7294311480416957 on epoch=737
06/18/2022 05:37:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 05:37:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
06/18/2022 05:37:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
06/18/2022 05:37:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/18/2022 05:38:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/18/2022 05:38:03 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7136886102403344 on epoch=749
06/18/2022 05:38:03 - INFO - __main__ - save last model!
06/18/2022 05:38:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 05:38:03 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 05:38:03 - INFO - __main__ - Printing 3 examples
06/18/2022 05:38:03 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 05:38:03 - INFO - __main__ - ['others']
06/18/2022 05:38:03 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 05:38:03 - INFO - __main__ - ['others']
06/18/2022 05:38:03 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 05:38:03 - INFO - __main__ - ['others']
06/18/2022 05:38:03 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 05:38:03 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 05:38:03 - INFO - __main__ - Printing 3 examples
06/18/2022 05:38:03 - INFO - __main__ -  [emo] how cause yes am listening
06/18/2022 05:38:03 - INFO - __main__ - ['others']
06/18/2022 05:38:03 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/18/2022 05:38:03 - INFO - __main__ - ['others']
06/18/2022 05:38:03 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/18/2022 05:38:03 - INFO - __main__ - ['others']
06/18/2022 05:38:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 05:38:03 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:38:03 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 05:38:03 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 05:38:03 - INFO - __main__ - Printing 3 examples
06/18/2022 05:38:03 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/18/2022 05:38:03 - INFO - __main__ - ['others']
06/18/2022 05:38:03 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/18/2022 05:38:03 - INFO - __main__ - ['others']
06/18/2022 05:38:03 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/18/2022 05:38:03 - INFO - __main__ - ['others']
06/18/2022 05:38:03 - INFO - __main__ - Tokenizing Input ...
06/18/2022 05:38:03 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:38:03 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 05:38:05 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:38:10 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 05:38:22 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 05:38:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 05:38:22 - INFO - __main__ - Starting training!
06/18/2022 05:39:45 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_100_0.5_8_predictions.txt
06/18/2022 05:39:45 - INFO - __main__ - Classification-F1 on test data: 0.2422
06/18/2022 05:39:45 - INFO - __main__ - prefix=emo_16_100, lr=0.5, bsz=8, dev_performance=0.7443564745141661, test_performance=0.24220414491744766
06/18/2022 05:39:45 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.4, bsz=8 ...
06/18/2022 05:39:46 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 05:39:46 - INFO - __main__ - Printing 3 examples
06/18/2022 05:39:46 - INFO - __main__ -  [emo] how cause yes am listening
06/18/2022 05:39:46 - INFO - __main__ - ['others']
06/18/2022 05:39:46 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/18/2022 05:39:46 - INFO - __main__ - ['others']
06/18/2022 05:39:46 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/18/2022 05:39:46 - INFO - __main__ - ['others']
06/18/2022 05:39:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 05:39:46 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:39:46 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 05:39:46 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 05:39:46 - INFO - __main__ - Printing 3 examples
06/18/2022 05:39:46 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/18/2022 05:39:46 - INFO - __main__ - ['others']
06/18/2022 05:39:46 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/18/2022 05:39:46 - INFO - __main__ - ['others']
06/18/2022 05:39:46 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/18/2022 05:39:46 - INFO - __main__ - ['others']
06/18/2022 05:39:46 - INFO - __main__ - Tokenizing Input ...
06/18/2022 05:39:46 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:39:46 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 05:40:05 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 05:40:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 05:40:06 - INFO - __main__ - Starting training!
06/18/2022 05:40:09 - INFO - __main__ - Step 10 Global step 10 Train loss 4.41 on epoch=2
06/18/2022 05:40:12 - INFO - __main__ - Step 20 Global step 20 Train loss 3.04 on epoch=4
06/18/2022 05:40:14 - INFO - __main__ - Step 30 Global step 30 Train loss 2.33 on epoch=7
06/18/2022 05:40:17 - INFO - __main__ - Step 40 Global step 40 Train loss 1.64 on epoch=9
06/18/2022 05:40:19 - INFO - __main__ - Step 50 Global step 50 Train loss 1.46 on epoch=12
06/18/2022 05:40:20 - INFO - __main__ - Global step 50 Train loss 2.57 Classification-F1 0.23263764347202298 on epoch=12
06/18/2022 05:40:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.23263764347202298 on epoch=12, global_step=50
06/18/2022 05:40:23 - INFO - __main__ - Step 60 Global step 60 Train loss 1.09 on epoch=14
06/18/2022 05:40:25 - INFO - __main__ - Step 70 Global step 70 Train loss 1.02 on epoch=17
06/18/2022 05:40:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=19
06/18/2022 05:40:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=22
06/18/2022 05:40:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.75 on epoch=24
06/18/2022 05:40:34 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.5086587436332768 on epoch=24
06/18/2022 05:40:34 - INFO - __main__ - Saving model with best Classification-F1: 0.23263764347202298 -> 0.5086587436332768 on epoch=24, global_step=100
06/18/2022 05:40:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=27
06/18/2022 05:40:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.76 on epoch=29
06/18/2022 05:40:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.64 on epoch=32
06/18/2022 05:40:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.68 on epoch=34
06/18/2022 05:40:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.78 on epoch=37
06/18/2022 05:40:46 - INFO - __main__ - Global step 150 Train loss 0.74 Classification-F1 0.5710763039532866 on epoch=37
06/18/2022 05:40:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5086587436332768 -> 0.5710763039532866 on epoch=37, global_step=150
06/18/2022 05:40:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.68 on epoch=39
06/18/2022 05:40:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.59 on epoch=42
06/18/2022 05:40:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.68 on epoch=44
06/18/2022 05:40:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=47
06/18/2022 05:40:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=49
06/18/2022 05:40:59 - INFO - __main__ - Global step 200 Train loss 0.58 Classification-F1 0.5471230158730159 on epoch=49
06/18/2022 05:41:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.56 on epoch=52
06/18/2022 05:41:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.55 on epoch=54
06/18/2022 05:41:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=57
06/18/2022 05:41:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.58 on epoch=59
06/18/2022 05:41:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.64 on epoch=62
06/18/2022 05:41:12 - INFO - __main__ - Global step 250 Train loss 0.57 Classification-F1 0.6216659298555851 on epoch=62
06/18/2022 05:41:12 - INFO - __main__ - Saving model with best Classification-F1: 0.5710763039532866 -> 0.6216659298555851 on epoch=62, global_step=250
06/18/2022 05:41:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.56 on epoch=64
06/18/2022 05:41:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=67
06/18/2022 05:41:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=69
06/18/2022 05:41:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=72
06/18/2022 05:41:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=74
06/18/2022 05:41:24 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.6209795321637427 on epoch=74
06/18/2022 05:41:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=77
06/18/2022 05:41:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=79
06/18/2022 05:41:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=82
06/18/2022 05:41:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=84
06/18/2022 05:41:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=87
06/18/2022 05:41:37 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.6348684210526315 on epoch=87
06/18/2022 05:41:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6216659298555851 -> 0.6348684210526315 on epoch=87, global_step=350
06/18/2022 05:41:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=89
06/18/2022 05:41:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=92
06/18/2022 05:41:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=94
06/18/2022 05:41:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=97
06/18/2022 05:41:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=99
06/18/2022 05:41:50 - INFO - __main__ - Global step 400 Train loss 0.34 Classification-F1 0.6851290388132494 on epoch=99
06/18/2022 05:41:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6348684210526315 -> 0.6851290388132494 on epoch=99, global_step=400
06/18/2022 05:41:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=102
06/18/2022 05:41:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=104
06/18/2022 05:41:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=107
06/18/2022 05:42:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=109
06/18/2022 05:42:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=112
06/18/2022 05:42:03 - INFO - __main__ - Global step 450 Train loss 0.33 Classification-F1 0.6866866028708134 on epoch=112
06/18/2022 05:42:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6851290388132494 -> 0.6866866028708134 on epoch=112, global_step=450
06/18/2022 05:42:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=114
06/18/2022 05:42:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=117
06/18/2022 05:42:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=119
06/18/2022 05:42:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=122
06/18/2022 05:42:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=124
06/18/2022 05:42:16 - INFO - __main__ - Global step 500 Train loss 0.28 Classification-F1 0.6598197696023783 on epoch=124
06/18/2022 05:42:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=127
06/18/2022 05:42:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.29 on epoch=129
06/18/2022 05:42:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=132
06/18/2022 05:42:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.20 on epoch=134
06/18/2022 05:42:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=137
06/18/2022 05:42:28 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.7020932787376412 on epoch=137
06/18/2022 05:42:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6866866028708134 -> 0.7020932787376412 on epoch=137, global_step=550
06/18/2022 05:42:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=139
06/18/2022 05:42:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=142
06/18/2022 05:42:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=144
06/18/2022 05:42:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=147
06/18/2022 05:42:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=149
06/18/2022 05:42:41 - INFO - __main__ - Global step 600 Train loss 0.19 Classification-F1 0.6656021864998022 on epoch=149
06/18/2022 05:42:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=152
06/18/2022 05:42:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=154
06/18/2022 05:42:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=157
06/18/2022 05:42:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=159
06/18/2022 05:42:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=162
06/18/2022 05:42:54 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.6868174962292609 on epoch=162
06/18/2022 05:42:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=164
06/18/2022 05:42:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=167
06/18/2022 05:43:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=169
06/18/2022 05:43:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=172
06/18/2022 05:43:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.16 on epoch=174
06/18/2022 05:43:07 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.6933286227403874 on epoch=174
06/18/2022 05:43:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=177
06/18/2022 05:43:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=179
06/18/2022 05:43:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=182
06/18/2022 05:43:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=184
06/18/2022 05:43:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=187
06/18/2022 05:43:19 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.6797619047619048 on epoch=187
06/18/2022 05:43:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=189
06/18/2022 05:43:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=192
06/18/2022 05:43:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=194
06/18/2022 05:43:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=197
06/18/2022 05:43:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=199
06/18/2022 05:43:32 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.6658783783783784 on epoch=199
06/18/2022 05:43:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=202
06/18/2022 05:43:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=204
06/18/2022 05:43:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=207
06/18/2022 05:43:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.13 on epoch=209
06/18/2022 05:43:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.11 on epoch=212
06/18/2022 05:43:45 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.6371798749747933 on epoch=212
06/18/2022 05:43:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=214
06/18/2022 05:43:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=217
06/18/2022 05:43:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=219
06/18/2022 05:43:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=222
06/18/2022 05:43:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
06/18/2022 05:43:58 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.6534534534534534 on epoch=224
06/18/2022 05:44:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=227
06/18/2022 05:44:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=229
06/18/2022 05:44:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=232
06/18/2022 05:44:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=234
06/18/2022 05:44:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=237
06/18/2022 05:44:11 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.6751599079185285 on epoch=237
06/18/2022 05:44:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=239
06/18/2022 05:44:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=242
06/18/2022 05:44:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=244
06/18/2022 05:44:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=247
06/18/2022 05:44:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=249
06/18/2022 05:44:23 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.7106209150326797 on epoch=249
06/18/2022 05:44:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7020932787376412 -> 0.7106209150326797 on epoch=249, global_step=1000
06/18/2022 05:44:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=252
06/18/2022 05:44:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
06/18/2022 05:44:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=257
06/18/2022 05:44:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
06/18/2022 05:44:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=262
06/18/2022 05:44:36 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.653078078078078 on epoch=262
06/18/2022 05:44:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
06/18/2022 05:44:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
06/18/2022 05:44:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=269
06/18/2022 05:44:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=272
06/18/2022 05:44:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
06/18/2022 05:44:49 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.6726110147162778 on epoch=274
06/18/2022 05:44:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
06/18/2022 05:44:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
06/18/2022 05:44:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=282
06/18/2022 05:44:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=284
06/18/2022 05:45:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
06/18/2022 05:45:02 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.7109469074986317 on epoch=287
06/18/2022 05:45:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7106209150326797 -> 0.7109469074986317 on epoch=287, global_step=1150
06/18/2022 05:45:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
06/18/2022 05:45:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
06/18/2022 05:45:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
06/18/2022 05:45:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=297
06/18/2022 05:45:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
06/18/2022 05:45:15 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6729627003820552 on epoch=299
06/18/2022 05:45:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
06/18/2022 05:45:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
06/18/2022 05:45:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
06/18/2022 05:45:24 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=309
06/18/2022 05:45:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
06/18/2022 05:45:27 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.664835474419653 on epoch=312
06/18/2022 05:45:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
06/18/2022 05:45:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
06/18/2022 05:45:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
06/18/2022 05:45:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
06/18/2022 05:45:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=324
06/18/2022 05:45:40 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6585249042145593 on epoch=324
06/18/2022 05:45:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=327
06/18/2022 05:45:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
06/18/2022 05:45:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=332
06/18/2022 05:45:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
06/18/2022 05:45:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
06/18/2022 05:45:53 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.6585249042145593 on epoch=337
06/18/2022 05:45:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
06/18/2022 05:45:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
06/18/2022 05:46:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
06/18/2022 05:46:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
06/18/2022 05:46:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
06/18/2022 05:46:06 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.6737179487179488 on epoch=349
06/18/2022 05:46:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
06/18/2022 05:46:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
06/18/2022 05:46:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=357
06/18/2022 05:46:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
06/18/2022 05:46:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
06/18/2022 05:46:19 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.6579365079365079 on epoch=362
06/18/2022 05:46:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=364
06/18/2022 05:46:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
06/18/2022 05:46:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=369
06/18/2022 05:46:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
06/18/2022 05:46:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
06/18/2022 05:46:32 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.6970588235294117 on epoch=374
06/18/2022 05:46:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
06/18/2022 05:46:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
06/18/2022 05:46:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/18/2022 05:46:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
06/18/2022 05:46:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
06/18/2022 05:46:45 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.694047619047619 on epoch=387
06/18/2022 05:46:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
06/18/2022 05:46:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
06/18/2022 05:46:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
06/18/2022 05:46:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=397
06/18/2022 05:46:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
06/18/2022 05:46:58 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.6698744198744199 on epoch=399
06/18/2022 05:47:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
06/18/2022 05:47:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
06/18/2022 05:47:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
06/18/2022 05:47:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
06/18/2022 05:47:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
06/18/2022 05:47:10 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.6935958254269449 on epoch=412
06/18/2022 05:47:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
06/18/2022 05:47:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
06/18/2022 05:47:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/18/2022 05:47:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
06/18/2022 05:47:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
06/18/2022 05:47:23 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.709469696969697 on epoch=424
06/18/2022 05:47:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
06/18/2022 05:47:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
06/18/2022 05:47:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=432
06/18/2022 05:47:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
06/18/2022 05:47:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=437
06/18/2022 05:47:36 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.7090996168582375 on epoch=437
06/18/2022 05:47:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
06/18/2022 05:47:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
06/18/2022 05:47:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
06/18/2022 05:47:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
06/18/2022 05:47:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=449
06/18/2022 05:47:49 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.7081263922118638 on epoch=449
06/18/2022 05:47:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=452
06/18/2022 05:47:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
06/18/2022 05:47:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
06/18/2022 05:47:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
06/18/2022 05:48:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=462
06/18/2022 05:48:02 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.7219875047461255 on epoch=462
06/18/2022 05:48:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7109469074986317 -> 0.7219875047461255 on epoch=462, global_step=1850
06/18/2022 05:48:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
06/18/2022 05:48:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
06/18/2022 05:48:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/18/2022 05:48:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
06/18/2022 05:48:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
06/18/2022 05:48:15 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.7221153846153846 on epoch=474
06/18/2022 05:48:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7219875047461255 -> 0.7221153846153846 on epoch=474, global_step=1900
06/18/2022 05:48:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=477
06/18/2022 05:48:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
06/18/2022 05:48:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/18/2022 05:48:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/18/2022 05:48:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
06/18/2022 05:48:28 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7226082161566032 on epoch=487
06/18/2022 05:48:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7221153846153846 -> 0.7226082161566032 on epoch=487, global_step=1950
06/18/2022 05:48:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
06/18/2022 05:48:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
06/18/2022 05:48:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
06/18/2022 05:48:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
06/18/2022 05:48:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
06/18/2022 05:48:41 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6888249005896065 on epoch=499
06/18/2022 05:48:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/18/2022 05:48:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
06/18/2022 05:48:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
06/18/2022 05:48:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/18/2022 05:48:53 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/18/2022 05:48:54 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7026839826839827 on epoch=512
06/18/2022 05:48:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/18/2022 05:48:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
06/18/2022 05:49:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=519
06/18/2022 05:49:03 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
06/18/2022 05:49:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/18/2022 05:49:07 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7026839826839827 on epoch=524
06/18/2022 05:49:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
06/18/2022 05:49:12 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/18/2022 05:49:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=532
06/18/2022 05:49:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
06/18/2022 05:49:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/18/2022 05:49:20 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7226082161566032 on epoch=537
06/18/2022 05:49:22 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/18/2022 05:49:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
06/18/2022 05:49:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
06/18/2022 05:49:29 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
06/18/2022 05:49:32 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
06/18/2022 05:49:33 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7219875047461255 on epoch=549
06/18/2022 05:49:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/18/2022 05:49:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
06/18/2022 05:49:40 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/18/2022 05:49:42 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/18/2022 05:49:45 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=562
06/18/2022 05:49:46 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.707843137254902 on epoch=562
06/18/2022 05:49:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
06/18/2022 05:49:51 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
06/18/2022 05:49:53 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
06/18/2022 05:49:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
06/18/2022 05:49:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
06/18/2022 05:49:59 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.680461951187594 on epoch=574
06/18/2022 05:50:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
06/18/2022 05:50:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/18/2022 05:50:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
06/18/2022 05:50:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
06/18/2022 05:50:11 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/18/2022 05:50:12 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6959657426188866 on epoch=587
06/18/2022 05:50:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=589
06/18/2022 05:50:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=592
06/18/2022 05:50:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
06/18/2022 05:50:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=597
06/18/2022 05:50:24 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/18/2022 05:50:25 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.7219628647214855 on epoch=599
06/18/2022 05:50:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.09 on epoch=602
06/18/2022 05:50:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/18/2022 05:50:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/18/2022 05:50:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/18/2022 05:50:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/18/2022 05:50:38 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7055694792536898 on epoch=612
06/18/2022 05:50:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/18/2022 05:50:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
06/18/2022 05:50:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/18/2022 05:50:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/18/2022 05:50:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
06/18/2022 05:50:51 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7620689655172412 on epoch=624
06/18/2022 05:50:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7226082161566032 -> 0.7620689655172412 on epoch=624, global_step=2500
06/18/2022 05:50:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
06/18/2022 05:50:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=629
06/18/2022 05:50:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=632
06/18/2022 05:51:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
06/18/2022 05:51:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
06/18/2022 05:51:04 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.7364971050454922 on epoch=637
06/18/2022 05:51:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/18/2022 05:51:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/18/2022 05:51:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.14 on epoch=644
06/18/2022 05:51:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/18/2022 05:51:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
06/18/2022 05:51:17 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.707843137254902 on epoch=649
06/18/2022 05:51:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/18/2022 05:51:22 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/18/2022 05:51:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/18/2022 05:51:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 05:51:29 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/18/2022 05:51:30 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6787243633085418 on epoch=662
06/18/2022 05:51:33 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
06/18/2022 05:51:35 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=667
06/18/2022 05:51:37 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=669
06/18/2022 05:51:40 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/18/2022 05:51:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/18/2022 05:51:43 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.7226878206479725 on epoch=674
06/18/2022 05:51:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=677
06/18/2022 05:51:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/18/2022 05:51:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/18/2022 05:51:53 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/18/2022 05:51:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
06/18/2022 05:51:56 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7238514173998045 on epoch=687
06/18/2022 05:51:59 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 05:52:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=692
06/18/2022 05:52:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=694
06/18/2022 05:52:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
06/18/2022 05:52:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=699
06/18/2022 05:52:09 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.7226878206479725 on epoch=699
06/18/2022 05:52:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
06/18/2022 05:52:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/18/2022 05:52:17 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
06/18/2022 05:52:19 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 05:52:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/18/2022 05:52:23 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7270960108181204 on epoch=712
06/18/2022 05:52:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/18/2022 05:52:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=717
06/18/2022 05:52:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/18/2022 05:52:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
06/18/2022 05:52:35 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
06/18/2022 05:52:36 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.706842136253901 on epoch=724
06/18/2022 05:52:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/18/2022 05:52:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/18/2022 05:52:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
06/18/2022 05:52:45 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 05:52:48 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/18/2022 05:52:49 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6874390968508615 on epoch=737
06/18/2022 05:52:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 05:52:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/18/2022 05:52:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
06/18/2022 05:52:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/18/2022 05:53:01 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
06/18/2022 05:53:02 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7216251885369532 on epoch=749
06/18/2022 05:53:02 - INFO - __main__ - save last model!
06/18/2022 05:53:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 05:53:02 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 05:53:02 - INFO - __main__ - Printing 3 examples
06/18/2022 05:53:02 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 05:53:02 - INFO - __main__ - ['others']
06/18/2022 05:53:02 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 05:53:02 - INFO - __main__ - ['others']
06/18/2022 05:53:02 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 05:53:02 - INFO - __main__ - ['others']
06/18/2022 05:53:02 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 05:53:02 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 05:53:02 - INFO - __main__ - Printing 3 examples
06/18/2022 05:53:02 - INFO - __main__ -  [emo] how cause yes am listening
06/18/2022 05:53:02 - INFO - __main__ - ['others']
06/18/2022 05:53:02 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/18/2022 05:53:02 - INFO - __main__ - ['others']
06/18/2022 05:53:02 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/18/2022 05:53:02 - INFO - __main__ - ['others']
06/18/2022 05:53:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 05:53:02 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:53:02 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 05:53:02 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 05:53:02 - INFO - __main__ - Printing 3 examples
06/18/2022 05:53:02 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/18/2022 05:53:02 - INFO - __main__ - ['others']
06/18/2022 05:53:02 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/18/2022 05:53:02 - INFO - __main__ - ['others']
06/18/2022 05:53:02 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/18/2022 05:53:02 - INFO - __main__ - ['others']
06/18/2022 05:53:02 - INFO - __main__ - Tokenizing Input ...
06/18/2022 05:53:02 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:53:02 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 05:53:04 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:53:10 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 05:53:21 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 05:53:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 05:53:22 - INFO - __main__ - Starting training!
06/18/2022 05:54:38 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_100_0.4_8_predictions.txt
06/18/2022 05:54:38 - INFO - __main__ - Classification-F1 on test data: 0.1977
06/18/2022 05:54:38 - INFO - __main__ - prefix=emo_16_100, lr=0.4, bsz=8, dev_performance=0.7620689655172412, test_performance=0.19774780740908857
06/18/2022 05:54:38 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.3, bsz=8 ...
06/18/2022 05:54:39 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 05:54:39 - INFO - __main__ - Printing 3 examples
06/18/2022 05:54:39 - INFO - __main__ -  [emo] how cause yes am listening
06/18/2022 05:54:39 - INFO - __main__ - ['others']
06/18/2022 05:54:39 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/18/2022 05:54:39 - INFO - __main__ - ['others']
06/18/2022 05:54:39 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/18/2022 05:54:39 - INFO - __main__ - ['others']
06/18/2022 05:54:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 05:54:39 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:54:39 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 05:54:39 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 05:54:39 - INFO - __main__ - Printing 3 examples
06/18/2022 05:54:39 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/18/2022 05:54:39 - INFO - __main__ - ['others']
06/18/2022 05:54:39 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/18/2022 05:54:39 - INFO - __main__ - ['others']
06/18/2022 05:54:39 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/18/2022 05:54:39 - INFO - __main__ - ['others']
06/18/2022 05:54:39 - INFO - __main__ - Tokenizing Input ...
06/18/2022 05:54:39 - INFO - __main__ - Tokenizing Output ...
06/18/2022 05:54:39 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 05:54:58 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 05:54:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 05:54:58 - INFO - __main__ - Starting training!
06/18/2022 05:55:01 - INFO - __main__ - Step 10 Global step 10 Train loss 4.43 on epoch=2
06/18/2022 05:55:04 - INFO - __main__ - Step 20 Global step 20 Train loss 3.20 on epoch=4
06/18/2022 05:55:06 - INFO - __main__ - Step 30 Global step 30 Train loss 2.67 on epoch=7
06/18/2022 05:55:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.92 on epoch=9
06/18/2022 05:55:11 - INFO - __main__ - Step 50 Global step 50 Train loss 1.61 on epoch=12
06/18/2022 05:55:13 - INFO - __main__ - Global step 50 Train loss 2.77 Classification-F1 0.0993655303030303 on epoch=12
06/18/2022 05:55:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0993655303030303 on epoch=12, global_step=50
06/18/2022 05:55:15 - INFO - __main__ - Step 60 Global step 60 Train loss 1.36 on epoch=14
06/18/2022 05:55:18 - INFO - __main__ - Step 70 Global step 70 Train loss 1.24 on epoch=17
06/18/2022 05:55:20 - INFO - __main__ - Step 80 Global step 80 Train loss 1.05 on epoch=19
06/18/2022 05:55:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.87 on epoch=22
06/18/2022 05:55:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.87 on epoch=24
06/18/2022 05:55:26 - INFO - __main__ - Global step 100 Train loss 1.08 Classification-F1 0.3341430499325237 on epoch=24
06/18/2022 05:55:26 - INFO - __main__ - Saving model with best Classification-F1: 0.0993655303030303 -> 0.3341430499325237 on epoch=24, global_step=100
06/18/2022 05:55:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.88 on epoch=27
06/18/2022 05:55:31 - INFO - __main__ - Step 120 Global step 120 Train loss 0.87 on epoch=29
06/18/2022 05:55:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.77 on epoch=32
06/18/2022 05:55:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.76 on epoch=34
06/18/2022 05:55:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.78 on epoch=37
06/18/2022 05:55:39 - INFO - __main__ - Global step 150 Train loss 0.81 Classification-F1 0.532354862676778 on epoch=37
06/18/2022 05:55:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3341430499325237 -> 0.532354862676778 on epoch=37, global_step=150
06/18/2022 05:55:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.76 on epoch=39
06/18/2022 05:55:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.63 on epoch=42
06/18/2022 05:55:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.60 on epoch=44
06/18/2022 05:55:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.67 on epoch=47
06/18/2022 05:55:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.62 on epoch=49
06/18/2022 05:55:53 - INFO - __main__ - Global step 200 Train loss 0.65 Classification-F1 0.5222275493685451 on epoch=49
06/18/2022 05:55:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.63 on epoch=52
06/18/2022 05:55:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.59 on epoch=54
06/18/2022 05:56:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.59 on epoch=57
06/18/2022 05:56:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.70 on epoch=59
06/18/2022 05:56:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=62
06/18/2022 05:56:06 - INFO - __main__ - Global step 250 Train loss 0.61 Classification-F1 0.5731177606177607 on epoch=62
06/18/2022 05:56:06 - INFO - __main__ - Saving model with best Classification-F1: 0.532354862676778 -> 0.5731177606177607 on epoch=62, global_step=250
06/18/2022 05:56:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=64
06/18/2022 05:56:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.56 on epoch=67
06/18/2022 05:56:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.56 on epoch=69
06/18/2022 05:56:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.59 on epoch=72
06/18/2022 05:56:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=74
06/18/2022 05:56:20 - INFO - __main__ - Global step 300 Train loss 0.54 Classification-F1 0.6410785027083838 on epoch=74
06/18/2022 05:56:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5731177606177607 -> 0.6410785027083838 on epoch=74, global_step=300
06/18/2022 05:56:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.54 on epoch=77
06/18/2022 05:56:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=79
06/18/2022 05:56:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=82
06/18/2022 05:56:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=84
06/18/2022 05:56:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=87
06/18/2022 05:56:33 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.6246294273677641 on epoch=87
06/18/2022 05:56:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=89
06/18/2022 05:56:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=92
06/18/2022 05:56:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=94
06/18/2022 05:56:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=97
06/18/2022 05:56:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=99
06/18/2022 05:56:46 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.5876012145748988 on epoch=99
06/18/2022 05:56:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=102
06/18/2022 05:56:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=104
06/18/2022 05:56:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=107
06/18/2022 05:56:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=109
06/18/2022 05:56:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=112
06/18/2022 05:57:00 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.6575457875457875 on epoch=112
06/18/2022 05:57:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6410785027083838 -> 0.6575457875457875 on epoch=112, global_step=450
06/18/2022 05:57:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=114
06/18/2022 05:57:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=117
06/18/2022 05:57:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.40 on epoch=119
06/18/2022 05:57:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=122
06/18/2022 05:57:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=124
06/18/2022 05:57:13 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.6506274046319813 on epoch=124
06/18/2022 05:57:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=127
06/18/2022 05:57:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=129
06/18/2022 05:57:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=132
06/18/2022 05:57:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=134
06/18/2022 05:57:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=137
06/18/2022 05:57:26 - INFO - __main__ - Global step 550 Train loss 0.30 Classification-F1 0.640739064856712 on epoch=137
06/18/2022 05:57:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=139
06/18/2022 05:57:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.31 on epoch=142
06/18/2022 05:57:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=144
06/18/2022 05:57:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.29 on epoch=147
06/18/2022 05:57:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=149
06/18/2022 05:57:39 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.6273579717318375 on epoch=149
06/18/2022 05:57:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=152
06/18/2022 05:57:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=154
06/18/2022 05:57:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=157
06/18/2022 05:57:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=159
06/18/2022 05:57:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.31 on epoch=162
06/18/2022 05:57:53 - INFO - __main__ - Global step 650 Train loss 0.26 Classification-F1 0.6277913674741578 on epoch=162
06/18/2022 05:57:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=164
06/18/2022 05:57:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=167
06/18/2022 05:58:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=169
06/18/2022 05:58:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=172
06/18/2022 05:58:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=174
06/18/2022 05:58:06 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.6445766418592505 on epoch=174
06/18/2022 05:58:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=177
06/18/2022 05:58:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=179
06/18/2022 05:58:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=182
06/18/2022 05:58:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=184
06/18/2022 05:58:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=187
06/18/2022 05:58:19 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.6607118560207805 on epoch=187
06/18/2022 05:58:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6575457875457875 -> 0.6607118560207805 on epoch=187, global_step=750
06/18/2022 05:58:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=189
06/18/2022 05:58:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.16 on epoch=192
06/18/2022 05:58:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=194
06/18/2022 05:58:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.16 on epoch=197
06/18/2022 05:58:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=199
06/18/2022 05:58:33 - INFO - __main__ - Global step 800 Train loss 0.17 Classification-F1 0.634009009009009 on epoch=199
06/18/2022 05:58:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=202
06/18/2022 05:58:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.13 on epoch=204
06/18/2022 05:58:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=207
06/18/2022 05:58:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=209
06/18/2022 05:58:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=212
06/18/2022 05:58:46 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.6371798749747933 on epoch=212
06/18/2022 05:58:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=214
06/18/2022 05:58:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=217
06/18/2022 05:58:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=219
06/18/2022 05:58:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=222
06/18/2022 05:58:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=224
06/18/2022 05:58:59 - INFO - __main__ - Global step 900 Train loss 0.18 Classification-F1 0.6347520566259033 on epoch=224
06/18/2022 05:59:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.14 on epoch=227
06/18/2022 05:59:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=229
06/18/2022 05:59:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=232
06/18/2022 05:59:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=234
06/18/2022 05:59:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=237
06/18/2022 05:59:13 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.6364114114114113 on epoch=237
06/18/2022 05:59:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.14 on epoch=239
06/18/2022 05:59:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=242
06/18/2022 05:59:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=244
06/18/2022 05:59:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.11 on epoch=247
06/18/2022 05:59:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=249
06/18/2022 05:59:26 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.6303325901151988 on epoch=249
06/18/2022 05:59:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=252
06/18/2022 05:59:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=254
06/18/2022 05:59:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=257
06/18/2022 05:59:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=259
06/18/2022 05:59:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=262
06/18/2022 05:59:39 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.7035307829425477 on epoch=262
06/18/2022 05:59:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6607118560207805 -> 0.7035307829425477 on epoch=262, global_step=1050
06/18/2022 05:59:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=264
06/18/2022 05:59:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=267
06/18/2022 05:59:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=269
06/18/2022 05:59:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=272
06/18/2022 05:59:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=274
06/18/2022 05:59:53 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.6567821067821068 on epoch=274
06/18/2022 05:59:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=277
06/18/2022 05:59:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=279
06/18/2022 06:00:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.12 on epoch=282
06/18/2022 06:00:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=284
06/18/2022 06:00:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=287
06/18/2022 06:00:06 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.6844272844272844 on epoch=287
06/18/2022 06:00:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
06/18/2022 06:00:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=292
06/18/2022 06:00:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=294
06/18/2022 06:00:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=297
06/18/2022 06:00:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=299
06/18/2022 06:00:19 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.6844272844272844 on epoch=299
06/18/2022 06:00:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=302
06/18/2022 06:00:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=304
06/18/2022 06:00:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=307
06/18/2022 06:00:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=309
06/18/2022 06:00:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=312
06/18/2022 06:00:33 - INFO - __main__ - Global step 1250 Train loss 0.09 Classification-F1 0.6943165672065927 on epoch=312
06/18/2022 06:00:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
06/18/2022 06:00:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=317
06/18/2022 06:00:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
06/18/2022 06:00:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
06/18/2022 06:00:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=324
06/18/2022 06:00:46 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.7099943693693693 on epoch=324
06/18/2022 06:00:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7035307829425477 -> 0.7099943693693693 on epoch=324, global_step=1300
06/18/2022 06:00:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=327
06/18/2022 06:00:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
06/18/2022 06:00:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=332
06/18/2022 06:00:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=334
06/18/2022 06:00:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
06/18/2022 06:00:59 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.6836538461538462 on epoch=337
06/18/2022 06:01:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
06/18/2022 06:01:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
06/18/2022 06:01:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=344
06/18/2022 06:01:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=347
06/18/2022 06:01:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
06/18/2022 06:01:12 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6808990100309211 on epoch=349
06/18/2022 06:01:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
06/18/2022 06:01:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=354
06/18/2022 06:01:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
06/18/2022 06:01:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
06/18/2022 06:01:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=362
06/18/2022 06:01:25 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.6719272844272843 on epoch=362
06/18/2022 06:01:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=364
06/18/2022 06:01:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
06/18/2022 06:01:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
06/18/2022 06:01:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
06/18/2022 06:01:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=374
06/18/2022 06:01:39 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.695580808080808 on epoch=374
06/18/2022 06:01:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
06/18/2022 06:01:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
06/18/2022 06:01:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
06/18/2022 06:01:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=384
06/18/2022 06:01:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
06/18/2022 06:01:52 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.695580808080808 on epoch=387
06/18/2022 06:01:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
06/18/2022 06:01:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=392
06/18/2022 06:01:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
06/18/2022 06:02:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=397
06/18/2022 06:02:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
06/18/2022 06:02:05 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.6816919191919192 on epoch=399
06/18/2022 06:02:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=402
06/18/2022 06:02:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
06/18/2022 06:02:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
06/18/2022 06:02:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=409
06/18/2022 06:02:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=412
06/18/2022 06:02:19 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.6980546123372948 on epoch=412
06/18/2022 06:02:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=414
06/18/2022 06:02:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=417
06/18/2022 06:02:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=419
06/18/2022 06:02:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
06/18/2022 06:02:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
06/18/2022 06:02:32 - INFO - __main__ - Global step 1700 Train loss 0.07 Classification-F1 0.6802952690321505 on epoch=424
06/18/2022 06:02:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
06/18/2022 06:02:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
06/18/2022 06:02:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
06/18/2022 06:02:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=434
06/18/2022 06:02:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
06/18/2022 06:02:45 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.6975694444444444 on epoch=437
06/18/2022 06:02:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
06/18/2022 06:02:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
06/18/2022 06:02:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=444
06/18/2022 06:02:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=447
06/18/2022 06:02:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
06/18/2022 06:02:58 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.6816919191919192 on epoch=449
06/18/2022 06:03:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
06/18/2022 06:03:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
06/18/2022 06:03:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=457
06/18/2022 06:03:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
06/18/2022 06:03:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
06/18/2022 06:03:12 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6836538461538462 on epoch=462
06/18/2022 06:03:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/18/2022 06:03:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/18/2022 06:03:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/18/2022 06:03:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
06/18/2022 06:03:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
06/18/2022 06:03:25 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6836538461538462 on epoch=474
06/18/2022 06:03:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
06/18/2022 06:03:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
06/18/2022 06:03:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=482
06/18/2022 06:03:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/18/2022 06:03:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
06/18/2022 06:03:38 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.6880555555555555 on epoch=487
06/18/2022 06:03:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
06/18/2022 06:03:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
06/18/2022 06:03:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
06/18/2022 06:03:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
06/18/2022 06:03:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
06/18/2022 06:03:51 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6598026598026597 on epoch=499
06/18/2022 06:03:54 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=502
06/18/2022 06:03:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
06/18/2022 06:03:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
06/18/2022 06:04:01 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/18/2022 06:04:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
06/18/2022 06:04:05 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.682627276105537 on epoch=512
06/18/2022 06:04:07 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
06/18/2022 06:04:10 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/18/2022 06:04:12 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
06/18/2022 06:04:15 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
06/18/2022 06:04:17 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
06/18/2022 06:04:18 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7037684537684538 on epoch=524
06/18/2022 06:04:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
06/18/2022 06:04:23 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=529
06/18/2022 06:04:25 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=532
06/18/2022 06:04:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
06/18/2022 06:04:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
06/18/2022 06:04:31 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.7026839826839828 on epoch=537
06/18/2022 06:04:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
06/18/2022 06:04:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=542
06/18/2022 06:04:39 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=544
06/18/2022 06:04:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=547
06/18/2022 06:04:44 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
06/18/2022 06:04:45 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.6654173088955697 on epoch=549
06/18/2022 06:04:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
06/18/2022 06:04:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
06/18/2022 06:04:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=557
06/18/2022 06:04:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=559
06/18/2022 06:04:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
06/18/2022 06:04:58 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.6722477972477973 on epoch=562
06/18/2022 06:05:00 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
06/18/2022 06:05:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
06/18/2022 06:05:05 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=569
06/18/2022 06:05:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
06/18/2022 06:05:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
06/18/2022 06:05:11 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7024372759856631 on epoch=574
06/18/2022 06:05:14 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/18/2022 06:05:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/18/2022 06:05:19 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
06/18/2022 06:05:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
06/18/2022 06:05:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
06/18/2022 06:05:25 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7033783783783782 on epoch=587
06/18/2022 06:05:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
06/18/2022 06:05:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
06/18/2022 06:05:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/18/2022 06:05:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
06/18/2022 06:05:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
06/18/2022 06:05:38 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.68066534914361 on epoch=599
06/18/2022 06:05:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
06/18/2022 06:05:43 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/18/2022 06:05:45 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/18/2022 06:05:48 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/18/2022 06:05:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
06/18/2022 06:05:51 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6970280422889856 on epoch=612
06/18/2022 06:05:53 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/18/2022 06:05:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
06/18/2022 06:05:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
06/18/2022 06:06:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
06/18/2022 06:06:03 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=624
06/18/2022 06:06:04 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.718866608544028 on epoch=624
06/18/2022 06:06:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7099943693693693 -> 0.718866608544028 on epoch=624, global_step=2500
06/18/2022 06:06:07 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
06/18/2022 06:06:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=629
06/18/2022 06:06:12 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/18/2022 06:06:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
06/18/2022 06:06:17 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/18/2022 06:06:18 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6808990100309211 on epoch=637
06/18/2022 06:06:20 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/18/2022 06:06:23 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/18/2022 06:06:25 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
06/18/2022 06:06:27 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=647
06/18/2022 06:06:30 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
06/18/2022 06:06:31 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6808990100309211 on epoch=649
06/18/2022 06:06:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=652
06/18/2022 06:06:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
06/18/2022 06:06:38 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=657
06/18/2022 06:06:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 06:06:43 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=662
06/18/2022 06:06:44 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.7175694444444444 on epoch=662
06/18/2022 06:06:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=664
06/18/2022 06:06:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/18/2022 06:06:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
06/18/2022 06:06:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
06/18/2022 06:06:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
06/18/2022 06:06:58 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.703377446925834 on epoch=674
06/18/2022 06:07:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
06/18/2022 06:07:02 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/18/2022 06:07:05 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/18/2022 06:07:07 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/18/2022 06:07:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/18/2022 06:07:11 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.6964808558558558 on epoch=687
06/18/2022 06:07:13 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 06:07:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/18/2022 06:07:18 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
06/18/2022 06:07:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=697
06/18/2022 06:07:23 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/18/2022 06:07:24 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.718866608544028 on epoch=699
06/18/2022 06:07:27 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
06/18/2022 06:07:29 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
06/18/2022 06:07:32 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/18/2022 06:07:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/18/2022 06:07:37 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
06/18/2022 06:07:38 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6980546123372948 on epoch=712
06/18/2022 06:07:40 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/18/2022 06:07:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=717
06/18/2022 06:07:45 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/18/2022 06:07:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=722
06/18/2022 06:07:50 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/18/2022 06:07:51 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.716969696969697 on epoch=724
06/18/2022 06:07:54 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/18/2022 06:07:56 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=729
06/18/2022 06:07:59 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
06/18/2022 06:08:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/18/2022 06:08:04 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=737
06/18/2022 06:08:05 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7164141414141414 on epoch=737
06/18/2022 06:08:07 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 06:08:10 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
06/18/2022 06:08:12 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/18/2022 06:08:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
06/18/2022 06:08:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
06/18/2022 06:08:18 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7055555555555555 on epoch=749
06/18/2022 06:08:18 - INFO - __main__ - save last model!
06/18/2022 06:08:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 06:08:18 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 06:08:18 - INFO - __main__ - Printing 3 examples
06/18/2022 06:08:18 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 06:08:18 - INFO - __main__ - ['others']
06/18/2022 06:08:18 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 06:08:18 - INFO - __main__ - ['others']
06/18/2022 06:08:18 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 06:08:18 - INFO - __main__ - ['others']
06/18/2022 06:08:18 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 06:08:18 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:08:18 - INFO - __main__ - Printing 3 examples
06/18/2022 06:08:18 - INFO - __main__ -  [emo] how cause yes am listening
06/18/2022 06:08:18 - INFO - __main__ - ['others']
06/18/2022 06:08:18 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/18/2022 06:08:18 - INFO - __main__ - ['others']
06/18/2022 06:08:18 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/18/2022 06:08:18 - INFO - __main__ - ['others']
06/18/2022 06:08:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 06:08:18 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:08:18 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 06:08:18 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:08:18 - INFO - __main__ - Printing 3 examples
06/18/2022 06:08:18 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/18/2022 06:08:18 - INFO - __main__ - ['others']
06/18/2022 06:08:18 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/18/2022 06:08:18 - INFO - __main__ - ['others']
06/18/2022 06:08:18 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/18/2022 06:08:18 - INFO - __main__ - ['others']
06/18/2022 06:08:18 - INFO - __main__ - Tokenizing Input ...
06/18/2022 06:08:18 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:08:19 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 06:08:20 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:08:26 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 06:08:34 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 06:08:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 06:08:35 - INFO - __main__ - Starting training!
06/18/2022 06:09:51 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_100_0.3_8_predictions.txt
06/18/2022 06:09:51 - INFO - __main__ - Classification-F1 on test data: 0.3218
06/18/2022 06:09:51 - INFO - __main__ - prefix=emo_16_100, lr=0.3, bsz=8, dev_performance=0.718866608544028, test_performance=0.32182164142374137
06/18/2022 06:09:51 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.2, bsz=8 ...
06/18/2022 06:09:52 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:09:52 - INFO - __main__ - Printing 3 examples
06/18/2022 06:09:52 - INFO - __main__ -  [emo] how cause yes am listening
06/18/2022 06:09:52 - INFO - __main__ - ['others']
06/18/2022 06:09:52 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/18/2022 06:09:52 - INFO - __main__ - ['others']
06/18/2022 06:09:52 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/18/2022 06:09:52 - INFO - __main__ - ['others']
06/18/2022 06:09:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 06:09:52 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:09:52 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 06:09:52 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:09:52 - INFO - __main__ - Printing 3 examples
06/18/2022 06:09:52 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/18/2022 06:09:52 - INFO - __main__ - ['others']
06/18/2022 06:09:52 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/18/2022 06:09:52 - INFO - __main__ - ['others']
06/18/2022 06:09:52 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/18/2022 06:09:52 - INFO - __main__ - ['others']
06/18/2022 06:09:52 - INFO - __main__ - Tokenizing Input ...
06/18/2022 06:09:52 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:09:52 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 06:10:08 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 06:10:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 06:10:09 - INFO - __main__ - Starting training!
06/18/2022 06:10:12 - INFO - __main__ - Step 10 Global step 10 Train loss 4.52 on epoch=2
06/18/2022 06:10:14 - INFO - __main__ - Step 20 Global step 20 Train loss 3.53 on epoch=4
06/18/2022 06:10:17 - INFO - __main__ - Step 30 Global step 30 Train loss 3.18 on epoch=7
06/18/2022 06:10:19 - INFO - __main__ - Step 40 Global step 40 Train loss 2.61 on epoch=9
06/18/2022 06:10:21 - INFO - __main__ - Step 50 Global step 50 Train loss 2.28 on epoch=12
06/18/2022 06:10:23 - INFO - __main__ - Global step 50 Train loss 3.22 Classification-F1 0.028956356736242885 on epoch=12
06/18/2022 06:10:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.028956356736242885 on epoch=12, global_step=50
06/18/2022 06:10:25 - INFO - __main__ - Step 60 Global step 60 Train loss 2.01 on epoch=14
06/18/2022 06:10:28 - INFO - __main__ - Step 70 Global step 70 Train loss 1.80 on epoch=17
06/18/2022 06:10:30 - INFO - __main__ - Step 80 Global step 80 Train loss 1.51 on epoch=19
06/18/2022 06:10:33 - INFO - __main__ - Step 90 Global step 90 Train loss 1.29 on epoch=22
06/18/2022 06:10:35 - INFO - __main__ - Step 100 Global step 100 Train loss 1.09 on epoch=24
06/18/2022 06:10:36 - INFO - __main__ - Global step 100 Train loss 1.54 Classification-F1 0.2073794382617912 on epoch=24
06/18/2022 06:10:36 - INFO - __main__ - Saving model with best Classification-F1: 0.028956356736242885 -> 0.2073794382617912 on epoch=24, global_step=100
06/18/2022 06:10:39 - INFO - __main__ - Step 110 Global step 110 Train loss 1.14 on epoch=27
06/18/2022 06:10:41 - INFO - __main__ - Step 120 Global step 120 Train loss 1.06 on epoch=29
06/18/2022 06:10:43 - INFO - __main__ - Step 130 Global step 130 Train loss 1.02 on epoch=32
06/18/2022 06:10:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.85 on epoch=34
06/18/2022 06:10:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.96 on epoch=37
06/18/2022 06:10:49 - INFO - __main__ - Global step 150 Train loss 1.00 Classification-F1 0.3623475472224273 on epoch=37
06/18/2022 06:10:49 - INFO - __main__ - Saving model with best Classification-F1: 0.2073794382617912 -> 0.3623475472224273 on epoch=37, global_step=150
06/18/2022 06:10:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.90 on epoch=39
06/18/2022 06:10:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.82 on epoch=42
06/18/2022 06:10:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.79 on epoch=44
06/18/2022 06:10:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.84 on epoch=47
06/18/2022 06:11:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.71 on epoch=49
06/18/2022 06:11:02 - INFO - __main__ - Global step 200 Train loss 0.81 Classification-F1 0.5723837031603398 on epoch=49
06/18/2022 06:11:02 - INFO - __main__ - Saving model with best Classification-F1: 0.3623475472224273 -> 0.5723837031603398 on epoch=49, global_step=200
06/18/2022 06:11:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.80 on epoch=52
06/18/2022 06:11:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.63 on epoch=54
06/18/2022 06:11:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.69 on epoch=57
06/18/2022 06:11:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.64 on epoch=59
06/18/2022 06:11:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.61 on epoch=62
06/18/2022 06:11:15 - INFO - __main__ - Global step 250 Train loss 0.67 Classification-F1 0.5972950525276106 on epoch=62
06/18/2022 06:11:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5723837031603398 -> 0.5972950525276106 on epoch=62, global_step=250
06/18/2022 06:11:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.59 on epoch=64
06/18/2022 06:11:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.60 on epoch=67
06/18/2022 06:11:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.63 on epoch=69
06/18/2022 06:11:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.74 on epoch=72
06/18/2022 06:11:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.52 on epoch=74
06/18/2022 06:11:28 - INFO - __main__ - Global step 300 Train loss 0.61 Classification-F1 0.6229716877914121 on epoch=74
06/18/2022 06:11:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5972950525276106 -> 0.6229716877914121 on epoch=74, global_step=300
06/18/2022 06:11:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.64 on epoch=77
06/18/2022 06:11:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.68 on epoch=79
06/18/2022 06:11:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.61 on epoch=82
06/18/2022 06:11:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.57 on epoch=84
06/18/2022 06:11:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.59 on epoch=87
06/18/2022 06:11:42 - INFO - __main__ - Global step 350 Train loss 0.62 Classification-F1 0.6438492063492064 on epoch=87
06/18/2022 06:11:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6229716877914121 -> 0.6438492063492064 on epoch=87, global_step=350
06/18/2022 06:11:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.66 on epoch=89
06/18/2022 06:11:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.56 on epoch=92
06/18/2022 06:11:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=94
06/18/2022 06:11:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.67 on epoch=97
06/18/2022 06:11:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=99
06/18/2022 06:11:55 - INFO - __main__ - Global step 400 Train loss 0.60 Classification-F1 0.6357123189019741 on epoch=99
06/18/2022 06:11:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=102
06/18/2022 06:11:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.55 on epoch=104
06/18/2022 06:12:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=107
06/18/2022 06:12:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=109
06/18/2022 06:12:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.53 on epoch=112
06/18/2022 06:12:08 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.6092836257309941 on epoch=112
06/18/2022 06:12:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=114
06/18/2022 06:12:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=117
06/18/2022 06:12:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.51 on epoch=119
06/18/2022 06:12:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=122
06/18/2022 06:12:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=124
06/18/2022 06:12:21 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.6376025290498974 on epoch=124
06/18/2022 06:12:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=127
06/18/2022 06:12:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=129
06/18/2022 06:12:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=132
06/18/2022 06:12:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=134
06/18/2022 06:12:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=137
06/18/2022 06:12:34 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.6357123189019741 on epoch=137
06/18/2022 06:12:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=139
06/18/2022 06:12:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=142
06/18/2022 06:12:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=144
06/18/2022 06:12:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=147
06/18/2022 06:12:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=149
06/18/2022 06:12:47 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.6217547637858825 on epoch=149
06/18/2022 06:12:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=152
06/18/2022 06:12:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=154
06/18/2022 06:12:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=157
06/18/2022 06:12:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=159
06/18/2022 06:12:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=162
06/18/2022 06:13:00 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.6201321735849675 on epoch=162
06/18/2022 06:13:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=164
06/18/2022 06:13:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=167
06/18/2022 06:13:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.33 on epoch=169
06/18/2022 06:13:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=172
06/18/2022 06:13:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.31 on epoch=174
06/18/2022 06:13:13 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.6199682674428005 on epoch=174
06/18/2022 06:13:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=177
06/18/2022 06:13:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=179
06/18/2022 06:13:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=182
06/18/2022 06:13:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.34 on epoch=184
06/18/2022 06:13:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=187
06/18/2022 06:13:26 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.6348684210526315 on epoch=187
06/18/2022 06:13:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=189
06/18/2022 06:13:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.29 on epoch=192
06/18/2022 06:13:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=194
06/18/2022 06:13:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=197
06/18/2022 06:13:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=199
06/18/2022 06:13:39 - INFO - __main__ - Global step 800 Train loss 0.30 Classification-F1 0.6225225225225224 on epoch=199
06/18/2022 06:13:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=202
06/18/2022 06:13:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=204
06/18/2022 06:13:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=207
06/18/2022 06:13:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=209
06/18/2022 06:13:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.31 on epoch=212
06/18/2022 06:13:52 - INFO - __main__ - Global step 850 Train loss 0.30 Classification-F1 0.6348684210526315 on epoch=212
06/18/2022 06:13:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=214
06/18/2022 06:13:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.27 on epoch=217
06/18/2022 06:13:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=219
06/18/2022 06:14:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=222
06/18/2022 06:14:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=224
06/18/2022 06:14:05 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.6217872284048755 on epoch=224
06/18/2022 06:14:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=227
06/18/2022 06:14:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=229
06/18/2022 06:14:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=232
06/18/2022 06:14:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=234
06/18/2022 06:14:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=237
06/18/2022 06:14:18 - INFO - __main__ - Global step 950 Train loss 0.26 Classification-F1 0.6159889472049183 on epoch=237
06/18/2022 06:14:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=239
06/18/2022 06:14:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=242
06/18/2022 06:14:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=244
06/18/2022 06:14:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=247
06/18/2022 06:14:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.26 on epoch=249
06/18/2022 06:14:31 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.6415701415701416 on epoch=249
06/18/2022 06:14:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=252
06/18/2022 06:14:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=254
06/18/2022 06:14:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=257
06/18/2022 06:14:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=259
06/18/2022 06:14:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=262
06/18/2022 06:14:45 - INFO - __main__ - Global step 1050 Train loss 0.25 Classification-F1 0.6864160401002507 on epoch=262
06/18/2022 06:14:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6438492063492064 -> 0.6864160401002507 on epoch=262, global_step=1050
06/18/2022 06:14:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=264
06/18/2022 06:14:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=267
06/18/2022 06:14:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.16 on epoch=269
06/18/2022 06:14:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=272
06/18/2022 06:14:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=274
06/18/2022 06:14:58 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.6596910209442179 on epoch=274
06/18/2022 06:15:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=277
06/18/2022 06:15:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.17 on epoch=279
06/18/2022 06:15:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=282
06/18/2022 06:15:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=284
06/18/2022 06:15:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.14 on epoch=287
06/18/2022 06:15:11 - INFO - __main__ - Global step 1150 Train loss 0.16 Classification-F1 0.6453697737007694 on epoch=287
06/18/2022 06:15:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=289
06/18/2022 06:15:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=292
06/18/2022 06:15:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=294
06/18/2022 06:15:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.26 on epoch=297
06/18/2022 06:15:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=299
06/18/2022 06:15:24 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.6554972804972805 on epoch=299
06/18/2022 06:15:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=302
06/18/2022 06:15:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=304
06/18/2022 06:15:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=307
06/18/2022 06:15:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
06/18/2022 06:15:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=312
06/18/2022 06:15:38 - INFO - __main__ - Global step 1250 Train loss 0.16 Classification-F1 0.688673421432042 on epoch=312
06/18/2022 06:15:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6864160401002507 -> 0.688673421432042 on epoch=312, global_step=1250
06/18/2022 06:15:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.13 on epoch=314
06/18/2022 06:15:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=317
06/18/2022 06:15:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=319
06/18/2022 06:15:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=322
06/18/2022 06:15:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=324
06/18/2022 06:15:51 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.6452709907072497 on epoch=324
06/18/2022 06:15:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=327
06/18/2022 06:15:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=329
06/18/2022 06:15:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
06/18/2022 06:16:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=334
06/18/2022 06:16:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
06/18/2022 06:16:04 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.6325208181744684 on epoch=337
06/18/2022 06:16:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=339
06/18/2022 06:16:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=342
06/18/2022 06:16:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=344
06/18/2022 06:16:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=347
06/18/2022 06:16:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
06/18/2022 06:16:17 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.6032732732732733 on epoch=349
06/18/2022 06:16:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=352
06/18/2022 06:16:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=354
06/18/2022 06:16:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=357
06/18/2022 06:16:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=359
06/18/2022 06:16:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=362
06/18/2022 06:16:30 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.6452709907072497 on epoch=362
06/18/2022 06:16:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=364
06/18/2022 06:16:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=367
06/18/2022 06:16:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=369
06/18/2022 06:16:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=372
06/18/2022 06:16:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=374
06/18/2022 06:16:44 - INFO - __main__ - Global step 1500 Train loss 0.09 Classification-F1 0.6154970760233918 on epoch=374
06/18/2022 06:16:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.11 on epoch=377
06/18/2022 06:16:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=379
06/18/2022 06:16:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=382
06/18/2022 06:16:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=384
06/18/2022 06:16:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
06/18/2022 06:16:57 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.6146334727373155 on epoch=387
06/18/2022 06:16:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=389
06/18/2022 06:17:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=392
06/18/2022 06:17:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=394
06/18/2022 06:17:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=397
06/18/2022 06:17:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
06/18/2022 06:17:10 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.6138650091739337 on epoch=399
06/18/2022 06:17:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=402
06/18/2022 06:17:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=404
06/18/2022 06:17:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=407
06/18/2022 06:17:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
06/18/2022 06:17:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=412
06/18/2022 06:17:23 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.6514144639144639 on epoch=412
06/18/2022 06:17:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=414
06/18/2022 06:17:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=417
06/18/2022 06:17:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=419
06/18/2022 06:17:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=422
06/18/2022 06:17:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=424
06/18/2022 06:17:36 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.6138650091739337 on epoch=424
06/18/2022 06:17:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=427
06/18/2022 06:17:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=429
06/18/2022 06:17:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=432
06/18/2022 06:17:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.15 on epoch=434
06/18/2022 06:17:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=437
06/18/2022 06:17:49 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.6588209088209087 on epoch=437
06/18/2022 06:17:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=439
06/18/2022 06:17:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=442
06/18/2022 06:17:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=444
06/18/2022 06:17:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
06/18/2022 06:18:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=449
06/18/2022 06:18:02 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.6275120732017284 on epoch=449
06/18/2022 06:18:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=452
06/18/2022 06:18:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=454
06/18/2022 06:18:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=457
06/18/2022 06:18:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=459
06/18/2022 06:18:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=462
06/18/2022 06:18:15 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.6280466835189473 on epoch=462
06/18/2022 06:18:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/18/2022 06:18:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=467
06/18/2022 06:18:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
06/18/2022 06:18:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=472
06/18/2022 06:18:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=474
06/18/2022 06:18:29 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.6521825396825397 on epoch=474
06/18/2022 06:18:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=477
06/18/2022 06:18:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=479
06/18/2022 06:18:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
06/18/2022 06:18:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
06/18/2022 06:18:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.12 on epoch=487
06/18/2022 06:18:42 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.6677534630120837 on epoch=487
06/18/2022 06:18:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
06/18/2022 06:18:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
06/18/2022 06:18:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
06/18/2022 06:18:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
06/18/2022 06:18:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
06/18/2022 06:18:55 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.6128337147215865 on epoch=499
06/18/2022 06:18:58 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/18/2022 06:19:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=504
06/18/2022 06:19:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
06/18/2022 06:19:05 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
06/18/2022 06:19:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=512
06/18/2022 06:19:08 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.6598026598026597 on epoch=512
06/18/2022 06:19:11 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=514
06/18/2022 06:19:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.13 on epoch=517
06/18/2022 06:19:16 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
06/18/2022 06:19:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
06/18/2022 06:19:21 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=524
06/18/2022 06:19:21 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.6436895705774424 on epoch=524
06/18/2022 06:19:24 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=527
06/18/2022 06:19:26 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
06/18/2022 06:19:29 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=532
06/18/2022 06:19:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
06/18/2022 06:19:34 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=537
06/18/2022 06:19:35 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.6292407414427049 on epoch=537
06/18/2022 06:19:37 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.10 on epoch=539
06/18/2022 06:19:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
06/18/2022 06:19:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
06/18/2022 06:19:45 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
06/18/2022 06:19:47 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
06/18/2022 06:19:48 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.6179108691268401 on epoch=549
06/18/2022 06:19:50 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
06/18/2022 06:19:53 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
06/18/2022 06:19:55 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=557
06/18/2022 06:19:58 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
06/18/2022 06:20:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
06/18/2022 06:20:01 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.6179108691268401 on epoch=562
06/18/2022 06:20:04 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
06/18/2022 06:20:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=567
06/18/2022 06:20:09 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=569
06/18/2022 06:20:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=572
06/18/2022 06:20:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
06/18/2022 06:20:14 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.6585249042145593 on epoch=574
06/18/2022 06:20:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
06/18/2022 06:20:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/18/2022 06:20:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.07 on epoch=582
06/18/2022 06:20:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
06/18/2022 06:20:27 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
06/18/2022 06:20:28 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6024014336917562 on epoch=587
06/18/2022 06:20:30 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
06/18/2022 06:20:33 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
06/18/2022 06:20:35 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/18/2022 06:20:38 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
06/18/2022 06:20:40 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=599
06/18/2022 06:20:41 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.6568616501465867 on epoch=599
06/18/2022 06:20:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=602
06/18/2022 06:20:46 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
06/18/2022 06:20:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=607
06/18/2022 06:20:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=609
06/18/2022 06:20:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
06/18/2022 06:20:54 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.6032732732732733 on epoch=612
06/18/2022 06:20:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
06/18/2022 06:20:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
06/18/2022 06:21:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=619
06/18/2022 06:21:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
06/18/2022 06:21:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=624
06/18/2022 06:21:07 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.660824838244193 on epoch=624
06/18/2022 06:21:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=627
06/18/2022 06:21:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
06/18/2022 06:21:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/18/2022 06:21:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
06/18/2022 06:21:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=637
06/18/2022 06:21:21 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.6177480158730159 on epoch=637
06/18/2022 06:21:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/18/2022 06:21:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=642
06/18/2022 06:21:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
06/18/2022 06:21:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/18/2022 06:21:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
06/18/2022 06:21:34 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6585249042145593 on epoch=649
06/18/2022 06:21:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=652
06/18/2022 06:21:39 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/18/2022 06:21:41 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/18/2022 06:21:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
06/18/2022 06:21:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
06/18/2022 06:21:47 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6585249042145593 on epoch=662
06/18/2022 06:21:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=664
06/18/2022 06:21:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
06/18/2022 06:21:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=669
06/18/2022 06:21:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
06/18/2022 06:22:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
06/18/2022 06:22:00 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.6510687638636822 on epoch=674
06/18/2022 06:22:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=677
06/18/2022 06:22:05 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
06/18/2022 06:22:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=682
06/18/2022 06:22:10 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=684
06/18/2022 06:22:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
06/18/2022 06:22:14 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.6813486873970744 on epoch=687
06/18/2022 06:22:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
06/18/2022 06:22:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
06/18/2022 06:22:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
06/18/2022 06:22:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
06/18/2022 06:22:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/18/2022 06:22:27 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6277913674741576 on epoch=699
06/18/2022 06:22:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
06/18/2022 06:22:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/18/2022 06:22:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=707
06/18/2022 06:22:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
06/18/2022 06:22:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.10 on epoch=712
06/18/2022 06:22:40 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.6355231026283658 on epoch=712
06/18/2022 06:22:43 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
06/18/2022 06:22:45 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/18/2022 06:22:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
06/18/2022 06:22:50 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
06/18/2022 06:22:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
06/18/2022 06:22:53 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6675324675324675 on epoch=724
06/18/2022 06:22:56 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=727
06/18/2022 06:22:58 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
06/18/2022 06:23:01 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=732
06/18/2022 06:23:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=734
06/18/2022 06:23:06 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.14 on epoch=737
06/18/2022 06:23:07 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.6304974329167877 on epoch=737
06/18/2022 06:23:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=739
06/18/2022 06:23:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=742
06/18/2022 06:23:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
06/18/2022 06:23:16 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
06/18/2022 06:23:19 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=749
06/18/2022 06:23:20 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.6604617604617604 on epoch=749
06/18/2022 06:23:20 - INFO - __main__ - save last model!
06/18/2022 06:23:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 06:23:20 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 06:23:20 - INFO - __main__ - Printing 3 examples
06/18/2022 06:23:20 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 06:23:20 - INFO - __main__ - ['others']
06/18/2022 06:23:20 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 06:23:20 - INFO - __main__ - ['others']
06/18/2022 06:23:20 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 06:23:20 - INFO - __main__ - ['others']
06/18/2022 06:23:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 06:23:20 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:23:20 - INFO - __main__ - Printing 3 examples
06/18/2022 06:23:20 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/18/2022 06:23:20 - INFO - __main__ - ['others']
06/18/2022 06:23:20 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/18/2022 06:23:20 - INFO - __main__ - ['others']
06/18/2022 06:23:20 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/18/2022 06:23:20 - INFO - __main__ - ['others']
06/18/2022 06:23:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 06:23:20 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:23:20 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 06:23:20 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:23:20 - INFO - __main__ - Printing 3 examples
06/18/2022 06:23:20 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/18/2022 06:23:20 - INFO - __main__ - ['others']
06/18/2022 06:23:20 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/18/2022 06:23:20 - INFO - __main__ - ['others']
06/18/2022 06:23:20 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/18/2022 06:23:20 - INFO - __main__ - ['others']
06/18/2022 06:23:20 - INFO - __main__ - Tokenizing Input ...
06/18/2022 06:23:20 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:23:20 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 06:23:22 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:23:27 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 06:23:39 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 06:23:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 06:23:40 - INFO - __main__ - Starting training!
06/18/2022 06:24:41 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_100_0.2_8_predictions.txt
06/18/2022 06:24:42 - INFO - __main__ - Classification-F1 on test data: 0.1642
06/18/2022 06:24:42 - INFO - __main__ - prefix=emo_16_100, lr=0.2, bsz=8, dev_performance=0.688673421432042, test_performance=0.16420468744698097
06/18/2022 06:24:42 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.5, bsz=8 ...
06/18/2022 06:24:43 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:24:43 - INFO - __main__ - Printing 3 examples
06/18/2022 06:24:43 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/18/2022 06:24:43 - INFO - __main__ - ['others']
06/18/2022 06:24:43 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/18/2022 06:24:43 - INFO - __main__ - ['others']
06/18/2022 06:24:43 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/18/2022 06:24:43 - INFO - __main__ - ['others']
06/18/2022 06:24:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 06:24:43 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:24:43 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 06:24:43 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:24:43 - INFO - __main__ - Printing 3 examples
06/18/2022 06:24:43 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/18/2022 06:24:43 - INFO - __main__ - ['others']
06/18/2022 06:24:43 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/18/2022 06:24:43 - INFO - __main__ - ['others']
06/18/2022 06:24:43 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/18/2022 06:24:43 - INFO - __main__ - ['others']
06/18/2022 06:24:43 - INFO - __main__ - Tokenizing Input ...
06/18/2022 06:24:43 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:24:43 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 06:24:58 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 06:24:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 06:24:59 - INFO - __main__ - Starting training!
06/18/2022 06:25:02 - INFO - __main__ - Step 10 Global step 10 Train loss 3.90 on epoch=2
06/18/2022 06:25:04 - INFO - __main__ - Step 20 Global step 20 Train loss 2.62 on epoch=4
06/18/2022 06:25:07 - INFO - __main__ - Step 30 Global step 30 Train loss 1.79 on epoch=7
06/18/2022 06:25:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.32 on epoch=9
06/18/2022 06:25:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.91 on epoch=12
06/18/2022 06:25:13 - INFO - __main__ - Global step 50 Train loss 2.11 Classification-F1 0.25686685899117057 on epoch=12
06/18/2022 06:25:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.25686685899117057 on epoch=12, global_step=50
06/18/2022 06:25:15 - INFO - __main__ - Step 60 Global step 60 Train loss 0.98 on epoch=14
06/18/2022 06:25:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.74 on epoch=17
06/18/2022 06:25:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.81 on epoch=19
06/18/2022 06:25:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=22
06/18/2022 06:25:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=24
06/18/2022 06:25:26 - INFO - __main__ - Global step 100 Train loss 0.74 Classification-F1 0.5455867121384363 on epoch=24
06/18/2022 06:25:26 - INFO - __main__ - Saving model with best Classification-F1: 0.25686685899117057 -> 0.5455867121384363 on epoch=24, global_step=100
06/18/2022 06:25:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=27
06/18/2022 06:25:31 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=29
06/18/2022 06:25:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=32
06/18/2022 06:25:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=34
06/18/2022 06:25:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=37
06/18/2022 06:25:39 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.6845081453634085 on epoch=37
06/18/2022 06:25:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5455867121384363 -> 0.6845081453634085 on epoch=37, global_step=150
06/18/2022 06:25:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=39
06/18/2022 06:25:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=42
06/18/2022 06:25:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=44
06/18/2022 06:25:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=47
06/18/2022 06:25:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=49
06/18/2022 06:25:52 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.5382702615300239 on epoch=49
06/18/2022 06:25:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.32 on epoch=52
06/18/2022 06:25:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=54
06/18/2022 06:26:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.32 on epoch=57
06/18/2022 06:26:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.31 on epoch=59
06/18/2022 06:26:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=62
06/18/2022 06:26:05 - INFO - __main__ - Global step 250 Train loss 0.32 Classification-F1 0.7029411764705883 on epoch=62
06/18/2022 06:26:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6845081453634085 -> 0.7029411764705883 on epoch=62, global_step=250
06/18/2022 06:26:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=64
06/18/2022 06:26:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=67
06/18/2022 06:26:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=69
06/18/2022 06:26:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=72
06/18/2022 06:26:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.20 on epoch=74
06/18/2022 06:26:18 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.6951032059727711 on epoch=74
06/18/2022 06:26:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.30 on epoch=77
06/18/2022 06:26:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.20 on epoch=79
06/18/2022 06:26:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=82
06/18/2022 06:26:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.17 on epoch=84
06/18/2022 06:26:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.16 on epoch=87
06/18/2022 06:26:32 - INFO - __main__ - Global step 350 Train loss 0.22 Classification-F1 0.7163515406162465 on epoch=87
06/18/2022 06:26:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7029411764705883 -> 0.7163515406162465 on epoch=87, global_step=350
06/18/2022 06:26:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.17 on epoch=89
06/18/2022 06:26:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=92
06/18/2022 06:26:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.17 on epoch=94
06/18/2022 06:26:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=97
06/18/2022 06:26:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.15 on epoch=99
06/18/2022 06:26:45 - INFO - __main__ - Global step 400 Train loss 0.19 Classification-F1 0.6947232947232947 on epoch=99
06/18/2022 06:26:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.15 on epoch=102
06/18/2022 06:26:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.13 on epoch=104
06/18/2022 06:26:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.13 on epoch=107
06/18/2022 06:26:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.09 on epoch=109
06/18/2022 06:26:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=112
06/18/2022 06:26:58 - INFO - __main__ - Global step 450 Train loss 0.13 Classification-F1 0.6988400113400113 on epoch=112
06/18/2022 06:27:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=114
06/18/2022 06:27:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.11 on epoch=117
06/18/2022 06:27:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.12 on epoch=119
06/18/2022 06:27:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=122
06/18/2022 06:27:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.04 on epoch=124
06/18/2022 06:27:11 - INFO - __main__ - Global step 500 Train loss 0.11 Classification-F1 0.695109126984127 on epoch=124
06/18/2022 06:27:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.11 on epoch=127
06/18/2022 06:27:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=129
06/18/2022 06:27:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=132
06/18/2022 06:27:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=134
06/18/2022 06:27:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.10 on epoch=137
06/18/2022 06:27:24 - INFO - __main__ - Global step 550 Train loss 0.09 Classification-F1 0.6764705882352942 on epoch=137
06/18/2022 06:27:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.11 on epoch=139
06/18/2022 06:27:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.09 on epoch=142
06/18/2022 06:27:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=144
06/18/2022 06:27:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=147
06/18/2022 06:27:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.06 on epoch=149
06/18/2022 06:27:37 - INFO - __main__ - Global step 600 Train loss 0.08 Classification-F1 0.7311688311688312 on epoch=149
06/18/2022 06:27:37 - INFO - __main__ - Saving model with best Classification-F1: 0.7163515406162465 -> 0.7311688311688312 on epoch=149, global_step=600
06/18/2022 06:27:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.09 on epoch=152
06/18/2022 06:27:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=154
06/18/2022 06:27:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=157
06/18/2022 06:27:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=159
06/18/2022 06:27:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=162
06/18/2022 06:27:50 - INFO - __main__ - Global step 650 Train loss 0.06 Classification-F1 0.7109827806602 on epoch=162
06/18/2022 06:27:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=164
06/18/2022 06:27:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=167
06/18/2022 06:27:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=169
06/18/2022 06:28:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.07 on epoch=172
06/18/2022 06:28:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=174
06/18/2022 06:28:03 - INFO - __main__ - Global step 700 Train loss 0.08 Classification-F1 0.7034845946387709 on epoch=174
06/18/2022 06:28:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=177
06/18/2022 06:28:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=179
06/18/2022 06:28:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=182
06/18/2022 06:28:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=184
06/18/2022 06:28:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=187
06/18/2022 06:28:17 - INFO - __main__ - Global step 750 Train loss 0.06 Classification-F1 0.6815274455077087 on epoch=187
06/18/2022 06:28:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=189
06/18/2022 06:28:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=192
06/18/2022 06:28:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=194
06/18/2022 06:28:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=197
06/18/2022 06:28:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=199
06/18/2022 06:28:30 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.6987240829346092 on epoch=199
06/18/2022 06:28:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=202
06/18/2022 06:28:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=204
06/18/2022 06:28:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=207
06/18/2022 06:28:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=209
06/18/2022 06:28:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=212
06/18/2022 06:28:43 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.704602231384914 on epoch=212
06/18/2022 06:28:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=214
06/18/2022 06:28:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=217
06/18/2022 06:28:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=219
06/18/2022 06:28:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=222
06/18/2022 06:28:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=224
06/18/2022 06:28:56 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.667227485775873 on epoch=224
06/18/2022 06:28:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=227
06/18/2022 06:29:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=229
06/18/2022 06:29:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
06/18/2022 06:29:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=234
06/18/2022 06:29:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=237
06/18/2022 06:29:09 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.7154168694915725 on epoch=237
06/18/2022 06:29:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=239
06/18/2022 06:29:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=242
06/18/2022 06:29:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=244
06/18/2022 06:29:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=247
06/18/2022 06:29:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=249
06/18/2022 06:29:22 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.6889705882352941 on epoch=249
06/18/2022 06:29:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=252
06/18/2022 06:29:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=254
06/18/2022 06:29:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=257
06/18/2022 06:29:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
06/18/2022 06:29:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=262
06/18/2022 06:29:35 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.7654875366568914 on epoch=262
06/18/2022 06:29:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7311688311688312 -> 0.7654875366568914 on epoch=262, global_step=1050
06/18/2022 06:29:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
06/18/2022 06:29:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
06/18/2022 06:29:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=269
06/18/2022 06:29:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=272
06/18/2022 06:29:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=274
06/18/2022 06:29:49 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.6804233870967743 on epoch=274
06/18/2022 06:29:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=277
06/18/2022 06:29:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
06/18/2022 06:29:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=282
06/18/2022 06:29:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
06/18/2022 06:30:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
06/18/2022 06:30:02 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.709977146263911 on epoch=287
06/18/2022 06:30:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=289
06/18/2022 06:30:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
06/18/2022 06:30:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
06/18/2022 06:30:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
06/18/2022 06:30:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
06/18/2022 06:30:15 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.7597902097902098 on epoch=299
06/18/2022 06:30:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=302
06/18/2022 06:30:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
06/18/2022 06:30:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
06/18/2022 06:30:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
06/18/2022 06:30:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
06/18/2022 06:30:29 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.7636796123372949 on epoch=312
06/18/2022 06:30:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
06/18/2022 06:30:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=317
06/18/2022 06:30:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
06/18/2022 06:30:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=322
06/18/2022 06:30:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=324
06/18/2022 06:30:42 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.7250644162588635 on epoch=324
06/18/2022 06:30:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=327
06/18/2022 06:30:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=329
06/18/2022 06:30:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
06/18/2022 06:30:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
06/18/2022 06:30:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
06/18/2022 06:30:55 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.7740350877192983 on epoch=337
06/18/2022 06:30:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7654875366568914 -> 0.7740350877192983 on epoch=337, global_step=1350
06/18/2022 06:30:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
06/18/2022 06:31:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=342
06/18/2022 06:31:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
06/18/2022 06:31:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
06/18/2022 06:31:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
06/18/2022 06:31:09 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.7295360938187764 on epoch=349
06/18/2022 06:31:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=352
06/18/2022 06:31:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=354
06/18/2022 06:31:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
06/18/2022 06:31:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
06/18/2022 06:31:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
06/18/2022 06:31:22 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.733853586027499 on epoch=362
06/18/2022 06:31:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
06/18/2022 06:31:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
06/18/2022 06:31:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
06/18/2022 06:31:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
06/18/2022 06:31:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
06/18/2022 06:31:36 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.7181341799855124 on epoch=374
06/18/2022 06:31:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
06/18/2022 06:31:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
06/18/2022 06:31:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/18/2022 06:31:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
06/18/2022 06:31:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
06/18/2022 06:31:49 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.733853586027499 on epoch=387
06/18/2022 06:31:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
06/18/2022 06:31:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=392
06/18/2022 06:31:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
06/18/2022 06:31:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
06/18/2022 06:32:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/18/2022 06:32:02 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.6874842690699468 on epoch=399
06/18/2022 06:32:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=402
06/18/2022 06:32:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
06/18/2022 06:32:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
06/18/2022 06:32:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
06/18/2022 06:32:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
06/18/2022 06:32:16 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6803508053508054 on epoch=412
06/18/2022 06:32:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
06/18/2022 06:32:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
06/18/2022 06:32:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
06/18/2022 06:32:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
06/18/2022 06:32:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
06/18/2022 06:32:29 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.7036799027288156 on epoch=424
06/18/2022 06:32:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
06/18/2022 06:32:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
06/18/2022 06:32:37 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/18/2022 06:32:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
06/18/2022 06:32:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
06/18/2022 06:32:43 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.7426854395604396 on epoch=437
06/18/2022 06:32:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
06/18/2022 06:32:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=442
06/18/2022 06:32:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
06/18/2022 06:32:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
06/18/2022 06:32:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
06/18/2022 06:32:56 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7278861865258924 on epoch=449
06/18/2022 06:32:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
06/18/2022 06:33:01 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
06/18/2022 06:33:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
06/18/2022 06:33:06 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
06/18/2022 06:33:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
06/18/2022 06:33:09 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7187671727237683 on epoch=462
06/18/2022 06:33:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/18/2022 06:33:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/18/2022 06:33:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
06/18/2022 06:33:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
06/18/2022 06:33:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
06/18/2022 06:33:23 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7434464825356666 on epoch=474
06/18/2022 06:33:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
06/18/2022 06:33:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
06/18/2022 06:33:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/18/2022 06:33:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/18/2022 06:33:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
06/18/2022 06:33:36 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.7187886725517696 on epoch=487
06/18/2022 06:33:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
06/18/2022 06:33:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
06/18/2022 06:33:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
06/18/2022 06:33:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/18/2022 06:33:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
06/18/2022 06:33:50 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7732905982905983 on epoch=499
06/18/2022 06:33:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
06/18/2022 06:33:55 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
06/18/2022 06:33:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
06/18/2022 06:34:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=509
06/18/2022 06:34:02 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/18/2022 06:34:03 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.7327654424428618 on epoch=512
06/18/2022 06:34:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/18/2022 06:34:08 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
06/18/2022 06:34:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
06/18/2022 06:34:13 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
06/18/2022 06:34:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/18/2022 06:34:17 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.7239583333333334 on epoch=524
06/18/2022 06:34:19 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
06/18/2022 06:34:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
06/18/2022 06:34:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/18/2022 06:34:27 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
06/18/2022 06:34:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/18/2022 06:34:30 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7187671727237683 on epoch=537
06/18/2022 06:34:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
06/18/2022 06:34:35 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/18/2022 06:34:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/18/2022 06:34:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/18/2022 06:34:43 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/18/2022 06:34:44 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7771987868762062 on epoch=549
06/18/2022 06:34:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7740350877192983 -> 0.7771987868762062 on epoch=549, global_step=2200
06/18/2022 06:34:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
06/18/2022 06:34:49 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/18/2022 06:34:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
06/18/2022 06:34:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
06/18/2022 06:34:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
06/18/2022 06:34:57 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.733853586027499 on epoch=562
06/18/2022 06:35:00 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
06/18/2022 06:35:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
06/18/2022 06:35:05 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/18/2022 06:35:07 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
06/18/2022 06:35:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/18/2022 06:35:11 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6884633994608418 on epoch=574
06/18/2022 06:35:13 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/18/2022 06:35:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/18/2022 06:35:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
06/18/2022 06:35:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
06/18/2022 06:35:23 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/18/2022 06:35:24 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7231686900804548 on epoch=587
06/18/2022 06:35:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/18/2022 06:35:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/18/2022 06:35:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/18/2022 06:35:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/18/2022 06:35:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/18/2022 06:35:38 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.6884633994608418 on epoch=599
06/18/2022 06:35:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
06/18/2022 06:35:43 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/18/2022 06:35:45 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/18/2022 06:35:48 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/18/2022 06:35:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/18/2022 06:35:51 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.7276470588235294 on epoch=612
06/18/2022 06:35:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/18/2022 06:35:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.08 on epoch=617
06/18/2022 06:35:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/18/2022 06:36:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/18/2022 06:36:03 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=624
06/18/2022 06:36:05 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7170439455046231 on epoch=624
06/18/2022 06:36:07 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/18/2022 06:36:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/18/2022 06:36:12 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/18/2022 06:36:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/18/2022 06:36:17 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.08 on epoch=637
06/18/2022 06:36:18 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7399804496578691 on epoch=637
06/18/2022 06:36:20 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/18/2022 06:36:23 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/18/2022 06:36:25 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/18/2022 06:36:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/18/2022 06:36:30 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/18/2022 06:36:32 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6941526610644257 on epoch=649
06/18/2022 06:36:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/18/2022 06:36:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=654
06/18/2022 06:36:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/18/2022 06:36:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 06:36:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/18/2022 06:36:45 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7136292016806723 on epoch=662
06/18/2022 06:36:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/18/2022 06:36:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/18/2022 06:36:53 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/18/2022 06:36:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
06/18/2022 06:36:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
06/18/2022 06:36:59 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7391443863218057 on epoch=674
06/18/2022 06:37:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/18/2022 06:37:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/18/2022 06:37:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/18/2022 06:37:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/18/2022 06:37:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/18/2022 06:37:12 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7312920661995205 on epoch=687
06/18/2022 06:37:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 06:37:17 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/18/2022 06:37:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/18/2022 06:37:22 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
06/18/2022 06:37:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/18/2022 06:37:26 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7089980158730158 on epoch=699
06/18/2022 06:37:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/18/2022 06:37:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/18/2022 06:37:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/18/2022 06:37:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 06:37:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/18/2022 06:37:39 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6941526610644257 on epoch=712
06/18/2022 06:37:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/18/2022 06:37:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/18/2022 06:37:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/18/2022 06:37:49 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/18/2022 06:37:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/18/2022 06:37:53 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7375032249742002 on epoch=724
06/18/2022 06:37:55 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/18/2022 06:37:58 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/18/2022 06:38:00 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/18/2022 06:38:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 06:38:05 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/18/2022 06:38:06 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6941526610644257 on epoch=737
06/18/2022 06:38:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=739
06/18/2022 06:38:11 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/18/2022 06:38:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.18 on epoch=744
06/18/2022 06:38:16 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/18/2022 06:38:19 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/18/2022 06:38:20 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.7825440742466605 on epoch=749
06/18/2022 06:38:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7771987868762062 -> 0.7825440742466605 on epoch=749, global_step=3000
06/18/2022 06:38:20 - INFO - __main__ - save last model!
06/18/2022 06:38:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 06:38:20 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 06:38:20 - INFO - __main__ - Printing 3 examples
06/18/2022 06:38:20 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 06:38:20 - INFO - __main__ - ['others']
06/18/2022 06:38:20 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 06:38:20 - INFO - __main__ - ['others']
06/18/2022 06:38:20 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 06:38:20 - INFO - __main__ - ['others']
06/18/2022 06:38:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 06:38:20 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:38:20 - INFO - __main__ - Printing 3 examples
06/18/2022 06:38:20 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/18/2022 06:38:20 - INFO - __main__ - ['others']
06/18/2022 06:38:20 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/18/2022 06:38:20 - INFO - __main__ - ['others']
06/18/2022 06:38:20 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/18/2022 06:38:20 - INFO - __main__ - ['others']
06/18/2022 06:38:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 06:38:20 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:38:20 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 06:38:20 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:38:20 - INFO - __main__ - Printing 3 examples
06/18/2022 06:38:20 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/18/2022 06:38:20 - INFO - __main__ - ['others']
06/18/2022 06:38:20 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/18/2022 06:38:20 - INFO - __main__ - ['others']
06/18/2022 06:38:20 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/18/2022 06:38:20 - INFO - __main__ - ['others']
06/18/2022 06:38:20 - INFO - __main__ - Tokenizing Input ...
06/18/2022 06:38:20 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:38:20 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 06:38:22 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:38:27 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 06:38:35 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 06:38:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 06:38:36 - INFO - __main__ - Starting training!
06/18/2022 06:40:01 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_13_0.5_8_predictions.txt
06/18/2022 06:40:01 - INFO - __main__ - Classification-F1 on test data: 0.3344
06/18/2022 06:40:01 - INFO - __main__ - prefix=emo_16_13, lr=0.5, bsz=8, dev_performance=0.7825440742466605, test_performance=0.3343839129365355
06/18/2022 06:40:01 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.4, bsz=8 ...
06/18/2022 06:40:02 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:40:02 - INFO - __main__ - Printing 3 examples
06/18/2022 06:40:02 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/18/2022 06:40:02 - INFO - __main__ - ['others']
06/18/2022 06:40:02 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/18/2022 06:40:02 - INFO - __main__ - ['others']
06/18/2022 06:40:02 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/18/2022 06:40:02 - INFO - __main__ - ['others']
06/18/2022 06:40:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 06:40:02 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:40:02 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 06:40:02 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:40:02 - INFO - __main__ - Printing 3 examples
06/18/2022 06:40:02 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/18/2022 06:40:02 - INFO - __main__ - ['others']
06/18/2022 06:40:02 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/18/2022 06:40:02 - INFO - __main__ - ['others']
06/18/2022 06:40:02 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/18/2022 06:40:02 - INFO - __main__ - ['others']
06/18/2022 06:40:02 - INFO - __main__ - Tokenizing Input ...
06/18/2022 06:40:02 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:40:02 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 06:40:20 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 06:40:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 06:40:21 - INFO - __main__ - Starting training!
06/18/2022 06:40:24 - INFO - __main__ - Step 10 Global step 10 Train loss 3.97 on epoch=2
06/18/2022 06:40:27 - INFO - __main__ - Step 20 Global step 20 Train loss 3.06 on epoch=4
06/18/2022 06:40:29 - INFO - __main__ - Step 30 Global step 30 Train loss 2.29 on epoch=7
06/18/2022 06:40:31 - INFO - __main__ - Step 40 Global step 40 Train loss 1.66 on epoch=9
06/18/2022 06:40:34 - INFO - __main__ - Step 50 Global step 50 Train loss 1.26 on epoch=12
06/18/2022 06:40:35 - INFO - __main__ - Global step 50 Train loss 2.45 Classification-F1 0.22037037037037038 on epoch=12
06/18/2022 06:40:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.22037037037037038 on epoch=12, global_step=50
06/18/2022 06:40:37 - INFO - __main__ - Step 60 Global step 60 Train loss 1.03 on epoch=14
06/18/2022 06:40:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.95 on epoch=17
06/18/2022 06:40:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.78 on epoch=19
06/18/2022 06:40:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.63 on epoch=22
06/18/2022 06:40:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.63 on epoch=24
06/18/2022 06:40:48 - INFO - __main__ - Global step 100 Train loss 0.80 Classification-F1 0.5461666666666667 on epoch=24
06/18/2022 06:40:48 - INFO - __main__ - Saving model with best Classification-F1: 0.22037037037037038 -> 0.5461666666666667 on epoch=24, global_step=100
06/18/2022 06:40:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.61 on epoch=27
06/18/2022 06:40:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=29
06/18/2022 06:40:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.63 on epoch=32
06/18/2022 06:40:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.59 on epoch=34
06/18/2022 06:41:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=37
06/18/2022 06:41:01 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.6730550284629981 on epoch=37
06/18/2022 06:41:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5461666666666667 -> 0.6730550284629981 on epoch=37, global_step=150
06/18/2022 06:41:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.60 on epoch=39
06/18/2022 06:41:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=42
06/18/2022 06:41:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=44
06/18/2022 06:41:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=47
06/18/2022 06:41:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=49
06/18/2022 06:41:14 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.6805323653962492 on epoch=49
06/18/2022 06:41:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6730550284629981 -> 0.6805323653962492 on epoch=49, global_step=200
06/18/2022 06:41:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=52
06/18/2022 06:41:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.33 on epoch=54
06/18/2022 06:41:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=57
06/18/2022 06:41:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=59
06/18/2022 06:41:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.33 on epoch=62
06/18/2022 06:41:28 - INFO - __main__ - Global step 250 Train loss 0.34 Classification-F1 0.6851532567049808 on epoch=62
06/18/2022 06:41:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6805323653962492 -> 0.6851532567049808 on epoch=62, global_step=250
06/18/2022 06:41:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=64
06/18/2022 06:41:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=67
06/18/2022 06:41:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=69
06/18/2022 06:41:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=72
06/18/2022 06:41:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=74
06/18/2022 06:41:41 - INFO - __main__ - Global step 300 Train loss 0.32 Classification-F1 0.6597222222222222 on epoch=74
06/18/2022 06:41:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=77
06/18/2022 06:41:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=79
06/18/2022 06:41:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=82
06/18/2022 06:41:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=84
06/18/2022 06:41:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=87
06/18/2022 06:41:54 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.6831271397447869 on epoch=87
06/18/2022 06:41:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=89
06/18/2022 06:41:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.17 on epoch=92
06/18/2022 06:42:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=94
06/18/2022 06:42:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=97
06/18/2022 06:42:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=99
06/18/2022 06:42:07 - INFO - __main__ - Global step 400 Train loss 0.25 Classification-F1 0.6800606460532931 on epoch=99
06/18/2022 06:42:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=102
06/18/2022 06:42:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=104
06/18/2022 06:42:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=107
06/18/2022 06:42:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.18 on epoch=109
06/18/2022 06:42:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.17 on epoch=112
06/18/2022 06:42:20 - INFO - __main__ - Global step 450 Train loss 0.19 Classification-F1 0.6955723345525977 on epoch=112
06/18/2022 06:42:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6851532567049808 -> 0.6955723345525977 on epoch=112, global_step=450
06/18/2022 06:42:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.17 on epoch=114
06/18/2022 06:42:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.12 on epoch=117
06/18/2022 06:42:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.15 on epoch=119
06/18/2022 06:42:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.17 on epoch=122
06/18/2022 06:42:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.18 on epoch=124
06/18/2022 06:42:33 - INFO - __main__ - Global step 500 Train loss 0.16 Classification-F1 0.6741159608806668 on epoch=124
06/18/2022 06:42:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.16 on epoch=127
06/18/2022 06:42:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.07 on epoch=129
06/18/2022 06:42:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=132
06/18/2022 06:42:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=134
06/18/2022 06:42:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.12 on epoch=137
06/18/2022 06:42:46 - INFO - __main__ - Global step 550 Train loss 0.13 Classification-F1 0.704602231384914 on epoch=137
06/18/2022 06:42:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6955723345525977 -> 0.704602231384914 on epoch=137, global_step=550
06/18/2022 06:42:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=139
06/18/2022 06:42:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.10 on epoch=142
06/18/2022 06:42:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=144
06/18/2022 06:42:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.10 on epoch=147
06/18/2022 06:42:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.09 on epoch=149
06/18/2022 06:42:58 - INFO - __main__ - Global step 600 Train loss 0.10 Classification-F1 0.704602231384914 on epoch=149
06/18/2022 06:43:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.11 on epoch=152
06/18/2022 06:43:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=154
06/18/2022 06:43:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=157
06/18/2022 06:43:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=159
06/18/2022 06:43:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=162
06/18/2022 06:43:11 - INFO - __main__ - Global step 650 Train loss 0.11 Classification-F1 0.7111449502341343 on epoch=162
06/18/2022 06:43:11 - INFO - __main__ - Saving model with best Classification-F1: 0.704602231384914 -> 0.7111449502341343 on epoch=162, global_step=650
06/18/2022 06:43:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=164
06/18/2022 06:43:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=167
06/18/2022 06:43:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=169
06/18/2022 06:43:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=172
06/18/2022 06:43:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=174
06/18/2022 06:43:24 - INFO - __main__ - Global step 700 Train loss 0.09 Classification-F1 0.6741159608806668 on epoch=174
06/18/2022 06:43:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=177
06/18/2022 06:43:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=179
06/18/2022 06:43:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=182
06/18/2022 06:43:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=184
06/18/2022 06:43:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=187
06/18/2022 06:43:37 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.6944871794871795 on epoch=187
06/18/2022 06:43:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.06 on epoch=189
06/18/2022 06:43:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=192
06/18/2022 06:43:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=194
06/18/2022 06:43:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=197
06/18/2022 06:43:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=199
06/18/2022 06:43:49 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.6751091269841271 on epoch=199
06/18/2022 06:43:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=202
06/18/2022 06:43:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=204
06/18/2022 06:43:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=207
06/18/2022 06:43:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=209
06/18/2022 06:44:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=212
06/18/2022 06:44:02 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.6739958407605467 on epoch=212
06/18/2022 06:44:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=214
06/18/2022 06:44:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=217
06/18/2022 06:44:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=219
06/18/2022 06:44:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=222
06/18/2022 06:44:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=224
06/18/2022 06:44:15 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.7184905803419127 on epoch=224
06/18/2022 06:44:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7111449502341343 -> 0.7184905803419127 on epoch=224, global_step=900
06/18/2022 06:44:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=227
06/18/2022 06:44:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=229
06/18/2022 06:44:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
06/18/2022 06:44:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=234
06/18/2022 06:44:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=237
06/18/2022 06:44:28 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.7238618082368082 on epoch=237
06/18/2022 06:44:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7184905803419127 -> 0.7238618082368082 on epoch=237, global_step=950
06/18/2022 06:44:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=239
06/18/2022 06:44:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=242
06/18/2022 06:44:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=244
06/18/2022 06:44:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
06/18/2022 06:44:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=249
06/18/2022 06:44:40 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.6878810944667721 on epoch=249
06/18/2022 06:44:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=252
06/18/2022 06:44:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=254
06/18/2022 06:44:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=257
06/18/2022 06:44:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
06/18/2022 06:44:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=262
06/18/2022 06:44:53 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.6878810944667721 on epoch=262
06/18/2022 06:44:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
06/18/2022 06:44:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=267
06/18/2022 06:45:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=269
06/18/2022 06:45:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
06/18/2022 06:45:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=274
06/18/2022 06:45:06 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.6976133887898593 on epoch=274
06/18/2022 06:45:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
06/18/2022 06:45:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
06/18/2022 06:45:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
06/18/2022 06:45:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
06/18/2022 06:45:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
06/18/2022 06:45:19 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.7126503126503128 on epoch=287
06/18/2022 06:45:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=289
06/18/2022 06:45:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
06/18/2022 06:45:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
06/18/2022 06:45:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
06/18/2022 06:45:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
06/18/2022 06:45:32 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.7231686900804548 on epoch=299
06/18/2022 06:45:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
06/18/2022 06:45:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
06/18/2022 06:45:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
06/18/2022 06:45:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
06/18/2022 06:45:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
06/18/2022 06:45:45 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.7136292016806723 on epoch=312
06/18/2022 06:45:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
06/18/2022 06:45:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=317
06/18/2022 06:45:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=319
06/18/2022 06:45:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=322
06/18/2022 06:45:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=324
06/18/2022 06:45:58 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.7086554172951232 on epoch=324
06/18/2022 06:46:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
06/18/2022 06:46:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
06/18/2022 06:46:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
06/18/2022 06:46:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
06/18/2022 06:46:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
06/18/2022 06:46:12 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.7238618082368082 on epoch=337
06/18/2022 06:46:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
06/18/2022 06:46:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
06/18/2022 06:46:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
06/18/2022 06:46:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
06/18/2022 06:46:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
06/18/2022 06:46:25 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.7086554172951232 on epoch=349
06/18/2022 06:46:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
06/18/2022 06:46:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
06/18/2022 06:46:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
06/18/2022 06:46:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
06/18/2022 06:46:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
06/18/2022 06:46:39 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.6933243227360875 on epoch=362
06/18/2022 06:46:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
06/18/2022 06:46:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
06/18/2022 06:46:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
06/18/2022 06:46:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
06/18/2022 06:46:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
06/18/2022 06:46:52 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.7087847774244833 on epoch=374
06/18/2022 06:46:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
06/18/2022 06:46:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
06/18/2022 06:47:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/18/2022 06:47:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
06/18/2022 06:47:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
06/18/2022 06:47:05 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.7241004962779156 on epoch=387
06/18/2022 06:47:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7238618082368082 -> 0.7241004962779156 on epoch=387, global_step=1550
06/18/2022 06:47:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
06/18/2022 06:47:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
06/18/2022 06:47:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
06/18/2022 06:47:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
06/18/2022 06:47:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
06/18/2022 06:47:19 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.6937699555346615 on epoch=399
06/18/2022 06:47:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
06/18/2022 06:47:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
06/18/2022 06:47:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
06/18/2022 06:47:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/18/2022 06:47:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
06/18/2022 06:47:33 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.6741125541125542 on epoch=412
06/18/2022 06:47:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
06/18/2022 06:47:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
06/18/2022 06:47:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
06/18/2022 06:47:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
06/18/2022 06:47:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
06/18/2022 06:47:46 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.7270784770784771 on epoch=424
06/18/2022 06:47:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7241004962779156 -> 0.7270784770784771 on epoch=424, global_step=1700
06/18/2022 06:47:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
06/18/2022 06:47:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/18/2022 06:47:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=432
06/18/2022 06:47:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=434
06/18/2022 06:47:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
06/18/2022 06:48:00 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.6941526610644259 on epoch=437
06/18/2022 06:48:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
06/18/2022 06:48:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
06/18/2022 06:48:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
06/18/2022 06:48:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
06/18/2022 06:48:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
06/18/2022 06:48:13 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7024044795783926 on epoch=449
06/18/2022 06:48:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
06/18/2022 06:48:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
06/18/2022 06:48:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/18/2022 06:48:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
06/18/2022 06:48:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
06/18/2022 06:48:27 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7239583333333334 on epoch=462
06/18/2022 06:48:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
06/18/2022 06:48:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
06/18/2022 06:48:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=469
06/18/2022 06:48:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
06/18/2022 06:48:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
06/18/2022 06:48:41 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7078847296494356 on epoch=474
06/18/2022 06:48:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
06/18/2022 06:48:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
06/18/2022 06:48:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/18/2022 06:48:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
06/18/2022 06:48:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
06/18/2022 06:48:54 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7100052635536507 on epoch=487
06/18/2022 06:48:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
06/18/2022 06:48:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
06/18/2022 06:49:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
06/18/2022 06:49:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
06/18/2022 06:49:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
06/18/2022 06:49:08 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7094509109311741 on epoch=499
06/18/2022 06:49:11 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
06/18/2022 06:49:13 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
06/18/2022 06:49:16 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
06/18/2022 06:49:18 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/18/2022 06:49:21 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
06/18/2022 06:49:22 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7254152097902098 on epoch=512
06/18/2022 06:49:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
06/18/2022 06:49:27 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/18/2022 06:49:29 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
06/18/2022 06:49:32 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
06/18/2022 06:49:34 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/18/2022 06:49:35 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7135146103896104 on epoch=524
06/18/2022 06:49:38 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
06/18/2022 06:49:40 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
06/18/2022 06:49:43 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/18/2022 06:49:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=534
06/18/2022 06:49:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
06/18/2022 06:49:49 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6961416630534277 on epoch=537
06/18/2022 06:49:51 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/18/2022 06:49:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/18/2022 06:49:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/18/2022 06:49:59 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
06/18/2022 06:50:02 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/18/2022 06:50:03 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.695128367003367 on epoch=549
06/18/2022 06:50:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/18/2022 06:50:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=554
06/18/2022 06:50:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
06/18/2022 06:50:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
06/18/2022 06:50:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
06/18/2022 06:50:16 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7096551765669413 on epoch=562
06/18/2022 06:50:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
06/18/2022 06:50:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
06/18/2022 06:50:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=569
06/18/2022 06:50:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/18/2022 06:50:29 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
06/18/2022 06:50:30 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6981156098803158 on epoch=574
06/18/2022 06:50:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/18/2022 06:50:35 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/18/2022 06:50:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/18/2022 06:50:40 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
06/18/2022 06:50:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
06/18/2022 06:50:44 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7136292016806723 on epoch=587
06/18/2022 06:50:46 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
06/18/2022 06:50:49 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
06/18/2022 06:50:51 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/18/2022 06:50:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/18/2022 06:50:56 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/18/2022 06:50:57 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7087477556227556 on epoch=599
06/18/2022 06:51:00 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
06/18/2022 06:51:02 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/18/2022 06:51:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/18/2022 06:51:07 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
06/18/2022 06:51:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/18/2022 06:51:11 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.7312920661995205 on epoch=612
06/18/2022 06:51:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7270784770784771 -> 0.7312920661995205 on epoch=612, global_step=2450
06/18/2022 06:51:13 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
06/18/2022 06:51:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/18/2022 06:51:18 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/18/2022 06:51:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
06/18/2022 06:51:23 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/18/2022 06:51:25 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7277066071183719 on epoch=624
06/18/2022 06:51:27 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/18/2022 06:51:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/18/2022 06:51:32 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/18/2022 06:51:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/18/2022 06:51:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/18/2022 06:51:38 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.7275357744107743 on epoch=637
06/18/2022 06:51:41 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=639
06/18/2022 06:51:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/18/2022 06:51:46 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/18/2022 06:51:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/18/2022 06:51:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/18/2022 06:51:52 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7136292016806723 on epoch=649
06/18/2022 06:51:55 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/18/2022 06:51:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/18/2022 06:52:00 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/18/2022 06:52:02 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 06:52:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/18/2022 06:52:06 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.7277066071183719 on epoch=662
06/18/2022 06:52:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/18/2022 06:52:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/18/2022 06:52:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
06/18/2022 06:52:16 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
06/18/2022 06:52:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/18/2022 06:52:19 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.7277437363834423 on epoch=674
06/18/2022 06:52:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
06/18/2022 06:52:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/18/2022 06:52:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/18/2022 06:52:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/18/2022 06:52:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
06/18/2022 06:52:33 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.651917081328846 on epoch=687
06/18/2022 06:52:35 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 06:52:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/18/2022 06:52:40 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
06/18/2022 06:52:43 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/18/2022 06:52:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=699
06/18/2022 06:52:47 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6882395382395383 on epoch=699
06/18/2022 06:52:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/18/2022 06:52:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/18/2022 06:52:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/18/2022 06:52:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 06:52:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/18/2022 06:53:00 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7277437363834423 on epoch=712
06/18/2022 06:53:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/18/2022 06:53:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/18/2022 06:53:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/18/2022 06:53:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/18/2022 06:53:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
06/18/2022 06:53:14 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7034845946387709 on epoch=724
06/18/2022 06:53:17 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/18/2022 06:53:19 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
06/18/2022 06:53:22 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/18/2022 06:53:24 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 06:53:27 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
06/18/2022 06:53:28 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6989441452856088 on epoch=737
06/18/2022 06:53:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 06:53:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/18/2022 06:53:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/18/2022 06:53:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
06/18/2022 06:53:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/18/2022 06:53:42 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7295360938187764 on epoch=749
06/18/2022 06:53:42 - INFO - __main__ - save last model!
06/18/2022 06:53:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 06:53:42 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 06:53:42 - INFO - __main__ - Printing 3 examples
06/18/2022 06:53:42 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 06:53:42 - INFO - __main__ - ['others']
06/18/2022 06:53:42 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 06:53:42 - INFO - __main__ - ['others']
06/18/2022 06:53:42 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 06:53:42 - INFO - __main__ - ['others']
06/18/2022 06:53:42 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 06:53:42 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:53:42 - INFO - __main__ - Printing 3 examples
06/18/2022 06:53:42 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/18/2022 06:53:42 - INFO - __main__ - ['others']
06/18/2022 06:53:42 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/18/2022 06:53:42 - INFO - __main__ - ['others']
06/18/2022 06:53:42 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/18/2022 06:53:42 - INFO - __main__ - ['others']
06/18/2022 06:53:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 06:53:42 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:53:42 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 06:53:42 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:53:42 - INFO - __main__ - Printing 3 examples
06/18/2022 06:53:42 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/18/2022 06:53:42 - INFO - __main__ - ['others']
06/18/2022 06:53:42 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/18/2022 06:53:42 - INFO - __main__ - ['others']
06/18/2022 06:53:42 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/18/2022 06:53:42 - INFO - __main__ - ['others']
06/18/2022 06:53:42 - INFO - __main__ - Tokenizing Input ...
06/18/2022 06:53:42 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:53:42 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 06:53:44 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:53:50 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 06:53:57 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 06:53:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 06:53:58 - INFO - __main__ - Starting training!
06/18/2022 06:55:23 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_13_0.4_8_predictions.txt
06/18/2022 06:55:23 - INFO - __main__ - Classification-F1 on test data: 0.1801
06/18/2022 06:55:23 - INFO - __main__ - prefix=emo_16_13, lr=0.4, bsz=8, dev_performance=0.7312920661995205, test_performance=0.18010580204004317
06/18/2022 06:55:23 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.3, bsz=8 ...
06/18/2022 06:55:24 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:55:24 - INFO - __main__ - Printing 3 examples
06/18/2022 06:55:24 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/18/2022 06:55:24 - INFO - __main__ - ['others']
06/18/2022 06:55:24 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/18/2022 06:55:24 - INFO - __main__ - ['others']
06/18/2022 06:55:24 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/18/2022 06:55:24 - INFO - __main__ - ['others']
06/18/2022 06:55:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 06:55:24 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:55:24 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 06:55:24 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 06:55:24 - INFO - __main__ - Printing 3 examples
06/18/2022 06:55:24 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/18/2022 06:55:24 - INFO - __main__ - ['others']
06/18/2022 06:55:24 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/18/2022 06:55:24 - INFO - __main__ - ['others']
06/18/2022 06:55:24 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/18/2022 06:55:24 - INFO - __main__ - ['others']
06/18/2022 06:55:24 - INFO - __main__ - Tokenizing Input ...
06/18/2022 06:55:24 - INFO - __main__ - Tokenizing Output ...
06/18/2022 06:55:24 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 06:55:39 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 06:55:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 06:55:40 - INFO - __main__ - Starting training!
06/18/2022 06:55:43 - INFO - __main__ - Step 10 Global step 10 Train loss 4.12 on epoch=2
06/18/2022 06:55:46 - INFO - __main__ - Step 20 Global step 20 Train loss 3.19 on epoch=4
06/18/2022 06:55:48 - INFO - __main__ - Step 30 Global step 30 Train loss 2.54 on epoch=7
06/18/2022 06:55:51 - INFO - __main__ - Step 40 Global step 40 Train loss 2.05 on epoch=9
06/18/2022 06:55:53 - INFO - __main__ - Step 50 Global step 50 Train loss 1.67 on epoch=12
06/18/2022 06:55:54 - INFO - __main__ - Global step 50 Train loss 2.71 Classification-F1 0.1528799019607843 on epoch=12
06/18/2022 06:55:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1528799019607843 on epoch=12, global_step=50
06/18/2022 06:55:57 - INFO - __main__ - Step 60 Global step 60 Train loss 1.17 on epoch=14
06/18/2022 06:55:59 - INFO - __main__ - Step 70 Global step 70 Train loss 1.04 on epoch=17
06/18/2022 06:56:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.87 on epoch=19
06/18/2022 06:56:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.82 on epoch=22
06/18/2022 06:56:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.78 on epoch=24
06/18/2022 06:56:08 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.4063178201109236 on epoch=24
06/18/2022 06:56:08 - INFO - __main__ - Saving model with best Classification-F1: 0.1528799019607843 -> 0.4063178201109236 on epoch=24, global_step=100
06/18/2022 06:56:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.62 on epoch=27
06/18/2022 06:56:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.74 on epoch=29
06/18/2022 06:56:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.69 on epoch=32
06/18/2022 06:56:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.56 on epoch=34
06/18/2022 06:56:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.57 on epoch=37
06/18/2022 06:56:21 - INFO - __main__ - Global step 150 Train loss 0.64 Classification-F1 0.5419172113289761 on epoch=37
06/18/2022 06:56:21 - INFO - __main__ - Saving model with best Classification-F1: 0.4063178201109236 -> 0.5419172113289761 on epoch=37, global_step=150
06/18/2022 06:56:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=39
06/18/2022 06:56:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=42
06/18/2022 06:56:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=44
06/18/2022 06:56:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=47
06/18/2022 06:56:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.60 on epoch=49
06/18/2022 06:56:34 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.47504690431519697 on epoch=49
06/18/2022 06:56:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=52
06/18/2022 06:56:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=54
06/18/2022 06:56:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=57
06/18/2022 06:56:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.36 on epoch=59
06/18/2022 06:56:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=62
06/18/2022 06:56:48 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.5408776493444686 on epoch=62
06/18/2022 06:56:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=64
06/18/2022 06:56:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=67
06/18/2022 06:56:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=69
06/18/2022 06:56:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.34 on epoch=72
06/18/2022 06:57:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.31 on epoch=74
06/18/2022 06:57:01 - INFO - __main__ - Global step 300 Train loss 0.34 Classification-F1 0.6997354497354497 on epoch=74
06/18/2022 06:57:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5419172113289761 -> 0.6997354497354497 on epoch=74, global_step=300
06/18/2022 06:57:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=77
06/18/2022 06:57:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=79
06/18/2022 06:57:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=82
06/18/2022 06:57:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=84
06/18/2022 06:57:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=87
06/18/2022 06:57:14 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.6514550264550265 on epoch=87
06/18/2022 06:57:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.31 on epoch=89
06/18/2022 06:57:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.28 on epoch=92
06/18/2022 06:57:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=94
06/18/2022 06:57:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=97
06/18/2022 06:57:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.31 on epoch=99
06/18/2022 06:57:27 - INFO - __main__ - Global step 400 Train loss 0.29 Classification-F1 0.63816261914088 on epoch=99
06/18/2022 06:57:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=102
06/18/2022 06:57:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=104
06/18/2022 06:57:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=107
06/18/2022 06:57:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=109
06/18/2022 06:57:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=112
06/18/2022 06:57:40 - INFO - __main__ - Global step 450 Train loss 0.26 Classification-F1 0.6838845215505558 on epoch=112
06/18/2022 06:57:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.30 on epoch=114
06/18/2022 06:57:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=117
06/18/2022 06:57:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=119
06/18/2022 06:57:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=122
06/18/2022 06:57:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=124
06/18/2022 06:57:53 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.6459920634920635 on epoch=124
06/18/2022 06:57:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=127
06/18/2022 06:57:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=129
06/18/2022 06:58:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=132
06/18/2022 06:58:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=134
06/18/2022 06:58:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=137
06/18/2022 06:58:07 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.6468817204301076 on epoch=137
06/18/2022 06:58:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=139
06/18/2022 06:58:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=142
06/18/2022 06:58:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=144
06/18/2022 06:58:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=147
06/18/2022 06:58:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=149
06/18/2022 06:58:20 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.6317533891547049 on epoch=149
06/18/2022 06:58:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=152
06/18/2022 06:58:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=154
06/18/2022 06:58:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=157
06/18/2022 06:58:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=159
06/18/2022 06:58:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=162
06/18/2022 06:58:33 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.6460283773951007 on epoch=162
06/18/2022 06:58:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=164
06/18/2022 06:58:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=167
06/18/2022 06:58:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.15 on epoch=169
06/18/2022 06:58:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=172
06/18/2022 06:58:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=174
06/18/2022 06:58:46 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.6743506493506494 on epoch=174
06/18/2022 06:58:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=177
06/18/2022 06:58:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=179
06/18/2022 06:58:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=182
06/18/2022 06:58:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=184
06/18/2022 06:58:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=187
06/18/2022 06:58:59 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.6593137254901961 on epoch=187
06/18/2022 06:59:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.12 on epoch=189
06/18/2022 06:59:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=192
06/18/2022 06:59:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=194
06/18/2022 06:59:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=197
06/18/2022 06:59:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=199
06/18/2022 06:59:12 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.6888061145510835 on epoch=199
06/18/2022 06:59:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=202
06/18/2022 06:59:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=204
06/18/2022 06:59:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=207
06/18/2022 06:59:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=209
06/18/2022 06:59:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=212
06/18/2022 06:59:25 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.6888061145510835 on epoch=212
06/18/2022 06:59:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=214
06/18/2022 06:59:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=217
06/18/2022 06:59:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=219
06/18/2022 06:59:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=222
06/18/2022 06:59:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=224
06/18/2022 06:59:39 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.6739958407605467 on epoch=224
06/18/2022 06:59:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=227
06/18/2022 06:59:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=229
06/18/2022 06:59:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=232
06/18/2022 06:59:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=234
06/18/2022 06:59:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=237
06/18/2022 06:59:52 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.6741159608806668 on epoch=237
06/18/2022 06:59:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=239
06/18/2022 06:59:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=242
06/18/2022 06:59:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=244
06/18/2022 07:00:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=247
06/18/2022 07:00:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=249
06/18/2022 07:00:05 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.688794440968354 on epoch=249
06/18/2022 07:00:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=252
06/18/2022 07:00:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=254
06/18/2022 07:00:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=257
06/18/2022 07:00:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=259
06/18/2022 07:00:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=262
06/18/2022 07:00:18 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.7087477556227556 on epoch=262
06/18/2022 07:00:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6997354497354497 -> 0.7087477556227556 on epoch=262, global_step=1050
06/18/2022 07:00:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
06/18/2022 07:00:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=267
06/18/2022 07:00:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=269
06/18/2022 07:00:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=272
06/18/2022 07:00:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
06/18/2022 07:00:32 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.7034845946387709 on epoch=274
06/18/2022 07:00:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
06/18/2022 07:00:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=279
06/18/2022 07:00:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
06/18/2022 07:00:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
06/18/2022 07:00:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
06/18/2022 07:00:45 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.7091537081339713 on epoch=287
06/18/2022 07:00:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7087477556227556 -> 0.7091537081339713 on epoch=287, global_step=1150
06/18/2022 07:00:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
06/18/2022 07:00:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
06/18/2022 07:00:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
06/18/2022 07:00:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
06/18/2022 07:00:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
06/18/2022 07:00:58 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.693958818958819 on epoch=299
06/18/2022 07:01:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
06/18/2022 07:01:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
06/18/2022 07:01:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
06/18/2022 07:01:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
06/18/2022 07:01:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
06/18/2022 07:01:11 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.6367301993261073 on epoch=312
06/18/2022 07:01:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=314
06/18/2022 07:01:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=317
06/18/2022 07:01:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
06/18/2022 07:01:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
06/18/2022 07:01:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
06/18/2022 07:01:25 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.7234906597774244 on epoch=324
06/18/2022 07:01:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7091537081339713 -> 0.7234906597774244 on epoch=324, global_step=1300
06/18/2022 07:01:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=327
06/18/2022 07:01:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=329
06/18/2022 07:01:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=332
06/18/2022 07:01:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=334
06/18/2022 07:01:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
06/18/2022 07:01:38 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.7087477556227556 on epoch=337
06/18/2022 07:01:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=339
06/18/2022 07:01:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
06/18/2022 07:01:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
06/18/2022 07:01:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
06/18/2022 07:01:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
06/18/2022 07:01:51 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6786554621848739 on epoch=349
06/18/2022 07:01:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
06/18/2022 07:01:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
06/18/2022 07:01:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
06/18/2022 07:02:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=359
06/18/2022 07:02:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
06/18/2022 07:02:05 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.6971677559912854 on epoch=362
06/18/2022 07:02:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=364
06/18/2022 07:02:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
06/18/2022 07:02:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
06/18/2022 07:02:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
06/18/2022 07:02:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
06/18/2022 07:02:18 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7382223109957056 on epoch=374
06/18/2022 07:02:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7234906597774244 -> 0.7382223109957056 on epoch=374, global_step=1500
06/18/2022 07:02:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
06/18/2022 07:02:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
06/18/2022 07:02:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/18/2022 07:02:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=384
06/18/2022 07:02:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
06/18/2022 07:02:31 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.7096551765669413 on epoch=387
06/18/2022 07:02:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
06/18/2022 07:02:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
06/18/2022 07:02:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
06/18/2022 07:02:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=397
06/18/2022 07:02:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
06/18/2022 07:02:44 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.6882395382395383 on epoch=399
06/18/2022 07:02:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/18/2022 07:02:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
06/18/2022 07:02:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
06/18/2022 07:02:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/18/2022 07:02:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
06/18/2022 07:02:58 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7234906597774244 on epoch=412
06/18/2022 07:03:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
06/18/2022 07:03:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
06/18/2022 07:03:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/18/2022 07:03:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
06/18/2022 07:03:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
06/18/2022 07:03:11 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.7044221640995835 on epoch=424
06/18/2022 07:03:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=427
06/18/2022 07:03:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/18/2022 07:03:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/18/2022 07:03:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
06/18/2022 07:03:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
06/18/2022 07:03:24 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7426854395604396 on epoch=437
06/18/2022 07:03:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7382223109957056 -> 0.7426854395604396 on epoch=437, global_step=1750
06/18/2022 07:03:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
06/18/2022 07:03:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
06/18/2022 07:03:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
06/18/2022 07:03:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
06/18/2022 07:03:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
06/18/2022 07:03:37 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7277066071183719 on epoch=449
06/18/2022 07:03:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
06/18/2022 07:03:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
06/18/2022 07:03:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=457
06/18/2022 07:03:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
06/18/2022 07:03:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
06/18/2022 07:03:50 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.7119269619269619 on epoch=462
06/18/2022 07:03:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/18/2022 07:03:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/18/2022 07:03:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=469
06/18/2022 07:04:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
06/18/2022 07:04:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
06/18/2022 07:04:04 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7436823033597227 on epoch=474
06/18/2022 07:04:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7426854395604396 -> 0.7436823033597227 on epoch=474, global_step=1900
06/18/2022 07:04:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
06/18/2022 07:04:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
06/18/2022 07:04:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
06/18/2022 07:04:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/18/2022 07:04:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
06/18/2022 07:04:17 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.688794440968354 on epoch=487
06/18/2022 07:04:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
06/18/2022 07:04:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/18/2022 07:04:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
06/18/2022 07:04:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/18/2022 07:04:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
06/18/2022 07:04:30 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6879084967320261 on epoch=499
06/18/2022 07:04:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/18/2022 07:04:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
06/18/2022 07:04:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
06/18/2022 07:04:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/18/2022 07:04:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/18/2022 07:04:44 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7425920688788336 on epoch=512
06/18/2022 07:04:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
06/18/2022 07:04:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
06/18/2022 07:04:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
06/18/2022 07:04:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
06/18/2022 07:04:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
06/18/2022 07:04:57 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7130007247654306 on epoch=524
06/18/2022 07:04:59 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
06/18/2022 07:05:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/18/2022 07:05:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
06/18/2022 07:05:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
06/18/2022 07:05:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/18/2022 07:05:10 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7609681372549019 on epoch=537
06/18/2022 07:05:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7436823033597227 -> 0.7609681372549019 on epoch=537, global_step=2150
06/18/2022 07:05:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
06/18/2022 07:05:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
06/18/2022 07:05:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
06/18/2022 07:05:20 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
06/18/2022 07:05:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=549
06/18/2022 07:05:24 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7426854395604396 on epoch=549
06/18/2022 07:05:26 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/18/2022 07:05:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
06/18/2022 07:05:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/18/2022 07:05:34 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/18/2022 07:05:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/18/2022 07:05:37 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7246774193548388 on epoch=562
06/18/2022 07:05:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
06/18/2022 07:05:42 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
06/18/2022 07:05:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
06/18/2022 07:05:47 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=572
06/18/2022 07:05:50 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/18/2022 07:05:51 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.6879084967320261 on epoch=574
06/18/2022 07:05:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/18/2022 07:05:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/18/2022 07:05:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
06/18/2022 07:06:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
06/18/2022 07:06:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
06/18/2022 07:06:04 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7234906597774244 on epoch=587
06/18/2022 07:06:07 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
06/18/2022 07:06:09 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/18/2022 07:06:12 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/18/2022 07:06:14 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/18/2022 07:06:17 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/18/2022 07:06:18 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6892361111111112 on epoch=599
06/18/2022 07:06:20 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
06/18/2022 07:06:23 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/18/2022 07:06:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/18/2022 07:06:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
06/18/2022 07:06:30 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/18/2022 07:06:31 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.7044221640995835 on epoch=612
06/18/2022 07:06:33 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/18/2022 07:06:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
06/18/2022 07:06:38 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=619
06/18/2022 07:06:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=622
06/18/2022 07:06:44 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
06/18/2022 07:06:45 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.744973544973545 on epoch=624
06/18/2022 07:06:47 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/18/2022 07:06:49 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/18/2022 07:06:52 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/18/2022 07:06:54 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
06/18/2022 07:06:57 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/18/2022 07:06:58 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7436823033597227 on epoch=637
06/18/2022 07:07:01 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/18/2022 07:07:03 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
06/18/2022 07:07:06 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/18/2022 07:07:08 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/18/2022 07:07:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/18/2022 07:07:12 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7426854395604396 on epoch=649
06/18/2022 07:07:14 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=652
06/18/2022 07:07:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=654
06/18/2022 07:07:19 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=657
06/18/2022 07:07:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 07:07:24 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
06/18/2022 07:07:25 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.703125 on epoch=662
06/18/2022 07:07:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/18/2022 07:07:30 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/18/2022 07:07:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/18/2022 07:07:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/18/2022 07:07:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/18/2022 07:07:39 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7234076054664291 on epoch=674
06/18/2022 07:07:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
06/18/2022 07:07:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=679
06/18/2022 07:07:46 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/18/2022 07:07:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
06/18/2022 07:07:51 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/18/2022 07:07:52 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7025252525252526 on epoch=687
06/18/2022 07:07:55 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=689
06/18/2022 07:07:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
06/18/2022 07:08:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/18/2022 07:08:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
06/18/2022 07:08:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=699
06/18/2022 07:08:06 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.7087017231134879 on epoch=699
06/18/2022 07:08:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/18/2022 07:08:11 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/18/2022 07:08:13 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/18/2022 07:08:15 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 07:08:18 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/18/2022 07:08:19 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7277066071183719 on epoch=712
06/18/2022 07:08:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
06/18/2022 07:08:24 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/18/2022 07:08:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/18/2022 07:08:29 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
06/18/2022 07:08:32 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
06/18/2022 07:08:33 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.6972248576850095 on epoch=724
06/18/2022 07:08:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/18/2022 07:08:38 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=729
06/18/2022 07:08:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/18/2022 07:08:43 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 07:08:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=737
06/18/2022 07:08:46 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7425920688788336 on epoch=737
06/18/2022 07:08:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 07:08:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/18/2022 07:08:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/18/2022 07:08:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/18/2022 07:08:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
06/18/2022 07:09:00 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6882395382395383 on epoch=749
06/18/2022 07:09:00 - INFO - __main__ - save last model!
06/18/2022 07:09:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 07:09:00 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 07:09:00 - INFO - __main__ - Printing 3 examples
06/18/2022 07:09:00 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 07:09:00 - INFO - __main__ - ['others']
06/18/2022 07:09:00 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 07:09:00 - INFO - __main__ - ['others']
06/18/2022 07:09:00 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 07:09:00 - INFO - __main__ - ['others']
06/18/2022 07:09:00 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 07:09:00 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:09:00 - INFO - __main__ - Printing 3 examples
06/18/2022 07:09:00 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/18/2022 07:09:00 - INFO - __main__ - ['others']
06/18/2022 07:09:00 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/18/2022 07:09:00 - INFO - __main__ - ['others']
06/18/2022 07:09:00 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/18/2022 07:09:00 - INFO - __main__ - ['others']
06/18/2022 07:09:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 07:09:00 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:09:00 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 07:09:00 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:09:00 - INFO - __main__ - Printing 3 examples
06/18/2022 07:09:00 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/18/2022 07:09:00 - INFO - __main__ - ['others']
06/18/2022 07:09:00 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/18/2022 07:09:00 - INFO - __main__ - ['others']
06/18/2022 07:09:00 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/18/2022 07:09:00 - INFO - __main__ - ['others']
06/18/2022 07:09:00 - INFO - __main__ - Tokenizing Input ...
06/18/2022 07:09:00 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:09:00 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 07:09:02 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:09:07 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 07:09:15 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 07:09:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 07:09:16 - INFO - __main__ - Starting training!
06/18/2022 07:10:35 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_13_0.3_8_predictions.txt
06/18/2022 07:10:35 - INFO - __main__ - Classification-F1 on test data: 0.1989
06/18/2022 07:10:35 - INFO - __main__ - prefix=emo_16_13, lr=0.3, bsz=8, dev_performance=0.7609681372549019, test_performance=0.19890556375037938
06/18/2022 07:10:35 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.2, bsz=8 ...
06/18/2022 07:10:36 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:10:36 - INFO - __main__ - Printing 3 examples
06/18/2022 07:10:36 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/18/2022 07:10:36 - INFO - __main__ - ['others']
06/18/2022 07:10:36 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/18/2022 07:10:36 - INFO - __main__ - ['others']
06/18/2022 07:10:36 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/18/2022 07:10:36 - INFO - __main__ - ['others']
06/18/2022 07:10:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 07:10:36 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:10:36 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 07:10:36 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:10:36 - INFO - __main__ - Printing 3 examples
06/18/2022 07:10:36 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/18/2022 07:10:36 - INFO - __main__ - ['others']
06/18/2022 07:10:36 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/18/2022 07:10:36 - INFO - __main__ - ['others']
06/18/2022 07:10:36 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/18/2022 07:10:36 - INFO - __main__ - ['others']
06/18/2022 07:10:36 - INFO - __main__ - Tokenizing Input ...
06/18/2022 07:10:36 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:10:36 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 07:10:51 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 07:10:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 07:10:52 - INFO - __main__ - Starting training!
06/18/2022 07:10:55 - INFO - __main__ - Step 10 Global step 10 Train loss 4.17 on epoch=2
06/18/2022 07:10:58 - INFO - __main__ - Step 20 Global step 20 Train loss 3.58 on epoch=4
06/18/2022 07:11:00 - INFO - __main__ - Step 30 Global step 30 Train loss 3.06 on epoch=7
06/18/2022 07:11:02 - INFO - __main__ - Step 40 Global step 40 Train loss 2.53 on epoch=9
06/18/2022 07:11:05 - INFO - __main__ - Step 50 Global step 50 Train loss 2.19 on epoch=12
06/18/2022 07:11:06 - INFO - __main__ - Global step 50 Train loss 3.11 Classification-F1 0.0385667463466325 on epoch=12
06/18/2022 07:11:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0385667463466325 on epoch=12, global_step=50
06/18/2022 07:11:09 - INFO - __main__ - Step 60 Global step 60 Train loss 1.95 on epoch=14
06/18/2022 07:11:11 - INFO - __main__ - Step 70 Global step 70 Train loss 1.64 on epoch=17
06/18/2022 07:11:14 - INFO - __main__ - Step 80 Global step 80 Train loss 1.41 on epoch=19
06/18/2022 07:11:16 - INFO - __main__ - Step 90 Global step 90 Train loss 1.33 on epoch=22
06/18/2022 07:11:19 - INFO - __main__ - Step 100 Global step 100 Train loss 1.08 on epoch=24
06/18/2022 07:11:20 - INFO - __main__ - Global step 100 Train loss 1.48 Classification-F1 0.23031417112299468 on epoch=24
06/18/2022 07:11:20 - INFO - __main__ - Saving model with best Classification-F1: 0.0385667463466325 -> 0.23031417112299468 on epoch=24, global_step=100
06/18/2022 07:11:22 - INFO - __main__ - Step 110 Global step 110 Train loss 1.04 on epoch=27
06/18/2022 07:11:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.84 on epoch=29
06/18/2022 07:11:27 - INFO - __main__ - Step 130 Global step 130 Train loss 0.76 on epoch=32
06/18/2022 07:11:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.77 on epoch=34
06/18/2022 07:11:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.71 on epoch=37
06/18/2022 07:11:33 - INFO - __main__ - Global step 150 Train loss 0.82 Classification-F1 0.3883717965064389 on epoch=37
06/18/2022 07:11:33 - INFO - __main__ - Saving model with best Classification-F1: 0.23031417112299468 -> 0.3883717965064389 on epoch=37, global_step=150
06/18/2022 07:11:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.62 on epoch=39
06/18/2022 07:11:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.58 on epoch=42
06/18/2022 07:11:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=44
06/18/2022 07:11:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.73 on epoch=47
06/18/2022 07:11:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=49
06/18/2022 07:11:47 - INFO - __main__ - Global step 200 Train loss 0.61 Classification-F1 0.49658902691511386 on epoch=49
06/18/2022 07:11:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3883717965064389 -> 0.49658902691511386 on epoch=49, global_step=200
06/18/2022 07:11:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.67 on epoch=52
06/18/2022 07:11:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=54
06/18/2022 07:11:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=57
06/18/2022 07:11:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=59
06/18/2022 07:11:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=62
06/18/2022 07:12:00 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.5427741935483871 on epoch=62
06/18/2022 07:12:00 - INFO - __main__ - Saving model with best Classification-F1: 0.49658902691511386 -> 0.5427741935483871 on epoch=62, global_step=250
06/18/2022 07:12:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=64
06/18/2022 07:12:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=67
06/18/2022 07:12:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=69
06/18/2022 07:12:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=72
06/18/2022 07:12:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=74
06/18/2022 07:12:13 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.4946520146520147 on epoch=74
06/18/2022 07:12:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=77
06/18/2022 07:12:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=79
06/18/2022 07:12:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=82
06/18/2022 07:12:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=84
06/18/2022 07:12:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=87
06/18/2022 07:12:26 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.4838512404029645 on epoch=87
06/18/2022 07:12:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=89
06/18/2022 07:12:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=92
06/18/2022 07:12:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=94
06/18/2022 07:12:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.34 on epoch=97
06/18/2022 07:12:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=99
06/18/2022 07:12:39 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.6745413892153023 on epoch=99
06/18/2022 07:12:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5427741935483871 -> 0.6745413892153023 on epoch=99, global_step=400
06/18/2022 07:12:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=102
06/18/2022 07:12:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=104
06/18/2022 07:12:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=107
06/18/2022 07:12:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.30 on epoch=109
06/18/2022 07:12:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=112
06/18/2022 07:12:53 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.693951093951094 on epoch=112
06/18/2022 07:12:53 - INFO - __main__ - Saving model with best Classification-F1: 0.6745413892153023 -> 0.693951093951094 on epoch=112, global_step=450
06/18/2022 07:12:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=114
06/18/2022 07:12:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=117
06/18/2022 07:13:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=119
06/18/2022 07:13:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=122
06/18/2022 07:13:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=124
06/18/2022 07:13:06 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.6381396445063677 on epoch=124
06/18/2022 07:13:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.30 on epoch=127
06/18/2022 07:13:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.28 on epoch=129
06/18/2022 07:13:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=132
06/18/2022 07:13:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.31 on epoch=134
06/18/2022 07:13:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=137
06/18/2022 07:13:19 - INFO - __main__ - Global step 550 Train loss 0.28 Classification-F1 0.6449538388700387 on epoch=137
06/18/2022 07:13:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=139
06/18/2022 07:13:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=142
06/18/2022 07:13:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=144
06/18/2022 07:13:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=147
06/18/2022 07:13:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=149
06/18/2022 07:13:32 - INFO - __main__ - Global step 600 Train loss 0.26 Classification-F1 0.6745413892153023 on epoch=149
06/18/2022 07:13:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.28 on epoch=152
06/18/2022 07:13:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=154
06/18/2022 07:13:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=157
06/18/2022 07:13:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=159
06/18/2022 07:13:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=162
06/18/2022 07:13:46 - INFO - __main__ - Global step 650 Train loss 0.28 Classification-F1 0.6753345210568211 on epoch=162
06/18/2022 07:13:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=164
06/18/2022 07:13:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=167
06/18/2022 07:13:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=169
06/18/2022 07:13:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=172
06/18/2022 07:13:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=174
06/18/2022 07:13:59 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.6944444444444444 on epoch=174
06/18/2022 07:13:59 - INFO - __main__ - Saving model with best Classification-F1: 0.693951093951094 -> 0.6944444444444444 on epoch=174, global_step=700
06/18/2022 07:14:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.15 on epoch=177
06/18/2022 07:14:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.15 on epoch=179
06/18/2022 07:14:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=182
06/18/2022 07:14:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=184
06/18/2022 07:14:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.18 on epoch=187
06/18/2022 07:14:13 - INFO - __main__ - Global step 750 Train loss 0.17 Classification-F1 0.6753345210568211 on epoch=187
06/18/2022 07:14:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=189
06/18/2022 07:14:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.17 on epoch=192
06/18/2022 07:14:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=194
06/18/2022 07:14:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=197
06/18/2022 07:14:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=199
06/18/2022 07:14:26 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.6460283773951007 on epoch=199
06/18/2022 07:14:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=202
06/18/2022 07:14:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=204
06/18/2022 07:14:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=207
06/18/2022 07:14:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=209
06/18/2022 07:14:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=212
06/18/2022 07:14:39 - INFO - __main__ - Global step 850 Train loss 0.17 Classification-F1 0.6603764478764479 on epoch=212
06/18/2022 07:14:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=214
06/18/2022 07:14:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=217
06/18/2022 07:14:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.15 on epoch=219
06/18/2022 07:14:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=222
06/18/2022 07:14:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=224
06/18/2022 07:14:52 - INFO - __main__ - Global step 900 Train loss 0.18 Classification-F1 0.6798809523809524 on epoch=224
06/18/2022 07:14:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=227
06/18/2022 07:14:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=229
06/18/2022 07:14:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=232
06/18/2022 07:15:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=234
06/18/2022 07:15:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.15 on epoch=237
06/18/2022 07:15:05 - INFO - __main__ - Global step 950 Train loss 0.17 Classification-F1 0.6798809523809524 on epoch=237
06/18/2022 07:15:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.14 on epoch=239
06/18/2022 07:15:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.13 on epoch=242
06/18/2022 07:15:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=244
06/18/2022 07:15:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=247
06/18/2022 07:15:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=249
06/18/2022 07:15:18 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.6783424908424908 on epoch=249
06/18/2022 07:15:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=252
06/18/2022 07:15:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
06/18/2022 07:15:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=257
06/18/2022 07:15:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
06/18/2022 07:15:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=262
06/18/2022 07:15:32 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.6798809523809524 on epoch=262
06/18/2022 07:15:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=264
06/18/2022 07:15:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=267
06/18/2022 07:15:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=269
06/18/2022 07:15:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=272
06/18/2022 07:15:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=274
06/18/2022 07:15:45 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.652487714987715 on epoch=274
06/18/2022 07:15:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=277
06/18/2022 07:15:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.13 on epoch=279
06/18/2022 07:15:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=282
06/18/2022 07:15:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=284
06/18/2022 07:15:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=287
06/18/2022 07:15:58 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.6745413892153023 on epoch=287
06/18/2022 07:16:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.15 on epoch=289
06/18/2022 07:16:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=292
06/18/2022 07:16:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.11 on epoch=294
06/18/2022 07:16:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=297
06/18/2022 07:16:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=299
06/18/2022 07:16:11 - INFO - __main__ - Global step 1200 Train loss 0.11 Classification-F1 0.6603764478764479 on epoch=299
06/18/2022 07:16:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.11 on epoch=302
06/18/2022 07:16:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=304
06/18/2022 07:16:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=307
06/18/2022 07:16:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
06/18/2022 07:16:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=312
06/18/2022 07:16:24 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.6941125541125542 on epoch=312
06/18/2022 07:16:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=314
06/18/2022 07:16:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=317
06/18/2022 07:16:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=319
06/18/2022 07:16:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
06/18/2022 07:16:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=324
06/18/2022 07:16:38 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.6885155906895036 on epoch=324
06/18/2022 07:16:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=327
06/18/2022 07:16:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=329
06/18/2022 07:16:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=332
06/18/2022 07:16:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=334
06/18/2022 07:16:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
06/18/2022 07:16:51 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.7086760461760462 on epoch=337
06/18/2022 07:16:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6944444444444444 -> 0.7086760461760462 on epoch=337, global_step=1350
06/18/2022 07:16:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=339
06/18/2022 07:16:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
06/18/2022 07:16:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=344
06/18/2022 07:17:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
06/18/2022 07:17:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=349
06/18/2022 07:17:04 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.6745413892153023 on epoch=349
06/18/2022 07:17:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
06/18/2022 07:17:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=354
06/18/2022 07:17:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=357
06/18/2022 07:17:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=359
06/18/2022 07:17:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=362
06/18/2022 07:17:17 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.7283982683982685 on epoch=362
06/18/2022 07:17:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7086760461760462 -> 0.7283982683982685 on epoch=362, global_step=1450
06/18/2022 07:17:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=364
06/18/2022 07:17:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
06/18/2022 07:17:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=369
06/18/2022 07:17:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
06/18/2022 07:17:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
06/18/2022 07:17:30 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.7086760461760462 on epoch=374
06/18/2022 07:17:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
06/18/2022 07:17:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=379
06/18/2022 07:17:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=382
06/18/2022 07:17:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
06/18/2022 07:17:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
06/18/2022 07:17:43 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.6745413892153023 on epoch=387
06/18/2022 07:17:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
06/18/2022 07:17:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=392
06/18/2022 07:17:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
06/18/2022 07:17:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=397
06/18/2022 07:17:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
06/18/2022 07:17:57 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.6856643356643357 on epoch=399
06/18/2022 07:17:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=402
06/18/2022 07:18:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
06/18/2022 07:18:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
06/18/2022 07:18:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
06/18/2022 07:18:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
06/18/2022 07:18:10 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.7086760461760462 on epoch=412
06/18/2022 07:18:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=414
06/18/2022 07:18:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=417
06/18/2022 07:18:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
06/18/2022 07:18:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=422
06/18/2022 07:18:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=424
06/18/2022 07:18:23 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.6745413892153023 on epoch=424
06/18/2022 07:18:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
06/18/2022 07:18:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
06/18/2022 07:18:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/18/2022 07:18:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
06/18/2022 07:18:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
06/18/2022 07:18:36 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6601933187339549 on epoch=437
06/18/2022 07:18:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
06/18/2022 07:18:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
06/18/2022 07:18:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=444
06/18/2022 07:18:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
06/18/2022 07:18:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
06/18/2022 07:18:50 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7134208928326575 on epoch=449
06/18/2022 07:18:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
06/18/2022 07:18:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
06/18/2022 07:18:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
06/18/2022 07:18:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
06/18/2022 07:19:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=462
06/18/2022 07:19:03 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6994085893466699 on epoch=462
06/18/2022 07:19:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/18/2022 07:19:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/18/2022 07:19:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
06/18/2022 07:19:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=472
06/18/2022 07:19:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
06/18/2022 07:19:16 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7086760461760462 on epoch=474
06/18/2022 07:19:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=477
06/18/2022 07:19:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
06/18/2022 07:19:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/18/2022 07:19:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/18/2022 07:19:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
06/18/2022 07:19:29 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.7281639044506691 on epoch=487
06/18/2022 07:19:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
06/18/2022 07:19:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/18/2022 07:19:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
06/18/2022 07:19:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/18/2022 07:19:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
06/18/2022 07:19:43 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7232837301587302 on epoch=499
06/18/2022 07:19:45 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/18/2022 07:19:47 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
06/18/2022 07:19:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
06/18/2022 07:19:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.09 on epoch=509
06/18/2022 07:19:55 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/18/2022 07:19:56 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.6888480345703346 on epoch=512
06/18/2022 07:19:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/18/2022 07:20:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
06/18/2022 07:20:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=519
06/18/2022 07:20:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=522
06/18/2022 07:20:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
06/18/2022 07:20:10 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7426854395604396 on epoch=524
06/18/2022 07:20:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7283982683982685 -> 0.7426854395604396 on epoch=524, global_step=2100
06/18/2022 07:20:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=527
06/18/2022 07:20:15 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
06/18/2022 07:20:17 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
06/18/2022 07:20:20 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
06/18/2022 07:20:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/18/2022 07:20:23 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.7426854395604396 on epoch=537
06/18/2022 07:20:26 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
06/18/2022 07:20:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=542
06/18/2022 07:20:31 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
06/18/2022 07:20:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
06/18/2022 07:20:36 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
06/18/2022 07:20:37 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7232837301587302 on epoch=549
06/18/2022 07:20:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
06/18/2022 07:20:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
06/18/2022 07:20:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/18/2022 07:20:47 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/18/2022 07:20:49 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
06/18/2022 07:20:50 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7239583333333334 on epoch=562
06/18/2022 07:20:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
06/18/2022 07:20:55 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
06/18/2022 07:20:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
06/18/2022 07:21:00 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
06/18/2022 07:21:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
06/18/2022 07:21:03 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7033045273534404 on epoch=574
06/18/2022 07:21:06 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=577
06/18/2022 07:21:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.12 on epoch=579
06/18/2022 07:21:11 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
06/18/2022 07:21:13 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
06/18/2022 07:21:16 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/18/2022 07:21:17 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7426854395604396 on epoch=587
06/18/2022 07:21:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=589
06/18/2022 07:21:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
06/18/2022 07:21:24 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
06/18/2022 07:21:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=597
06/18/2022 07:21:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
06/18/2022 07:21:30 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.7033045273534404 on epoch=599
06/18/2022 07:21:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
06/18/2022 07:21:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
06/18/2022 07:21:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=607
06/18/2022 07:21:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
06/18/2022 07:21:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
06/18/2022 07:21:43 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.6894802457288617 on epoch=612
06/18/2022 07:21:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
06/18/2022 07:21:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
06/18/2022 07:21:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=619
06/18/2022 07:21:53 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=622
06/18/2022 07:21:56 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=624
06/18/2022 07:21:56 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7036799027288156 on epoch=624
06/18/2022 07:21:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
06/18/2022 07:22:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=629
06/18/2022 07:22:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/18/2022 07:22:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
06/18/2022 07:22:09 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/18/2022 07:22:10 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7043664065403195 on epoch=637
06/18/2022 07:22:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=639
06/18/2022 07:22:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=642
06/18/2022 07:22:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
06/18/2022 07:22:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/18/2022 07:22:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
06/18/2022 07:22:23 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.7391443863218057 on epoch=649
06/18/2022 07:22:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/18/2022 07:22:28 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/18/2022 07:22:30 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/18/2022 07:22:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
06/18/2022 07:22:35 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
06/18/2022 07:22:36 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7239583333333334 on epoch=662
06/18/2022 07:22:39 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
06/18/2022 07:22:41 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
06/18/2022 07:22:44 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
06/18/2022 07:22:46 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
06/18/2022 07:22:49 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/18/2022 07:22:50 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7442105263157894 on epoch=674
06/18/2022 07:22:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7426854395604396 -> 0.7442105263157894 on epoch=674, global_step=2700
06/18/2022 07:22:52 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
06/18/2022 07:22:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
06/18/2022 07:22:57 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
06/18/2022 07:23:00 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/18/2022 07:23:02 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
06/18/2022 07:23:03 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7442105263157894 on epoch=687
06/18/2022 07:23:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 07:23:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=692
06/18/2022 07:23:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
06/18/2022 07:23:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.08 on epoch=697
06/18/2022 07:23:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
06/18/2022 07:23:17 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.7588666085440279 on epoch=699
06/18/2022 07:23:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7442105263157894 -> 0.7588666085440279 on epoch=699, global_step=2800
06/18/2022 07:23:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=702
06/18/2022 07:23:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/18/2022 07:23:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
06/18/2022 07:23:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/18/2022 07:23:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
06/18/2022 07:23:30 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7618058464832659 on epoch=712
06/18/2022 07:23:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7588666085440279 -> 0.7618058464832659 on epoch=712, global_step=2850
06/18/2022 07:23:32 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/18/2022 07:23:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/18/2022 07:23:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=719
06/18/2022 07:23:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
06/18/2022 07:23:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
06/18/2022 07:23:43 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7618058464832659 on epoch=724
06/18/2022 07:23:46 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/18/2022 07:23:48 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=729
06/18/2022 07:23:51 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
06/18/2022 07:23:53 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 07:23:56 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.12 on epoch=737
06/18/2022 07:23:57 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.757968017645437 on epoch=737
06/18/2022 07:23:59 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 07:24:02 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
06/18/2022 07:24:04 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
06/18/2022 07:24:07 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/18/2022 07:24:09 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=749
06/18/2022 07:24:10 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7618058464832659 on epoch=749
06/18/2022 07:24:10 - INFO - __main__ - save last model!
06/18/2022 07:24:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 07:24:10 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 07:24:10 - INFO - __main__ - Printing 3 examples
06/18/2022 07:24:10 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 07:24:10 - INFO - __main__ - ['others']
06/18/2022 07:24:10 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 07:24:10 - INFO - __main__ - ['others']
06/18/2022 07:24:10 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 07:24:10 - INFO - __main__ - ['others']
06/18/2022 07:24:10 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 07:24:11 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:24:11 - INFO - __main__ - Printing 3 examples
06/18/2022 07:24:11 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/18/2022 07:24:11 - INFO - __main__ - ['sad']
06/18/2022 07:24:11 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/18/2022 07:24:11 - INFO - __main__ - ['sad']
06/18/2022 07:24:11 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/18/2022 07:24:11 - INFO - __main__ - ['sad']
06/18/2022 07:24:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 07:24:11 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:24:11 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 07:24:11 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:24:11 - INFO - __main__ - Printing 3 examples
06/18/2022 07:24:11 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/18/2022 07:24:11 - INFO - __main__ - ['sad']
06/18/2022 07:24:11 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/18/2022 07:24:11 - INFO - __main__ - ['sad']
06/18/2022 07:24:11 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/18/2022 07:24:11 - INFO - __main__ - ['sad']
06/18/2022 07:24:11 - INFO - __main__ - Tokenizing Input ...
06/18/2022 07:24:11 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:24:11 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 07:24:12 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:24:18 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 07:24:30 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 07:24:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 07:24:30 - INFO - __main__ - Starting training!
06/18/2022 07:25:39 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_13_0.2_8_predictions.txt
06/18/2022 07:25:39 - INFO - __main__ - Classification-F1 on test data: 0.1415
06/18/2022 07:25:39 - INFO - __main__ - prefix=emo_16_13, lr=0.2, bsz=8, dev_performance=0.7618058464832659, test_performance=0.14149451258735582
06/18/2022 07:25:39 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.5, bsz=8 ...
06/18/2022 07:25:40 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:25:40 - INFO - __main__ - Printing 3 examples
06/18/2022 07:25:40 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/18/2022 07:25:40 - INFO - __main__ - ['sad']
06/18/2022 07:25:40 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/18/2022 07:25:40 - INFO - __main__ - ['sad']
06/18/2022 07:25:40 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/18/2022 07:25:40 - INFO - __main__ - ['sad']
06/18/2022 07:25:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 07:25:40 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:25:41 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 07:25:41 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:25:41 - INFO - __main__ - Printing 3 examples
06/18/2022 07:25:41 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/18/2022 07:25:41 - INFO - __main__ - ['sad']
06/18/2022 07:25:41 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/18/2022 07:25:41 - INFO - __main__ - ['sad']
06/18/2022 07:25:41 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/18/2022 07:25:41 - INFO - __main__ - ['sad']
06/18/2022 07:25:41 - INFO - __main__ - Tokenizing Input ...
06/18/2022 07:25:41 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:25:41 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 07:25:56 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 07:25:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 07:25:56 - INFO - __main__ - Starting training!
06/18/2022 07:25:59 - INFO - __main__ - Step 10 Global step 10 Train loss 4.12 on epoch=2
06/18/2022 07:26:02 - INFO - __main__ - Step 20 Global step 20 Train loss 3.05 on epoch=4
06/18/2022 07:26:04 - INFO - __main__ - Step 30 Global step 30 Train loss 2.09 on epoch=7
06/18/2022 07:26:06 - INFO - __main__ - Step 40 Global step 40 Train loss 1.49 on epoch=9
06/18/2022 07:26:09 - INFO - __main__ - Step 50 Global step 50 Train loss 1.11 on epoch=12
06/18/2022 07:26:10 - INFO - __main__ - Global step 50 Train loss 2.37 Classification-F1 0.3832894736842105 on epoch=12
06/18/2022 07:26:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3832894736842105 on epoch=12, global_step=50
06/18/2022 07:26:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.85 on epoch=14
06/18/2022 07:26:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.70 on epoch=17
06/18/2022 07:26:17 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=19
06/18/2022 07:26:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=22
06/18/2022 07:26:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=24
06/18/2022 07:26:22 - INFO - __main__ - Global step 100 Train loss 0.65 Classification-F1 0.5640350877192982 on epoch=24
06/18/2022 07:26:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3832894736842105 -> 0.5640350877192982 on epoch=24, global_step=100
06/18/2022 07:26:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=27
06/18/2022 07:26:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.64 on epoch=29
06/18/2022 07:26:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=32
06/18/2022 07:26:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=34
06/18/2022 07:26:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=37
06/18/2022 07:26:35 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.5773310023310022 on epoch=37
06/18/2022 07:26:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5640350877192982 -> 0.5773310023310022 on epoch=37, global_step=150
06/18/2022 07:26:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=39
06/18/2022 07:26:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=42
06/18/2022 07:26:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=44
06/18/2022 07:26:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=47
06/18/2022 07:26:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=49
06/18/2022 07:26:48 - INFO - __main__ - Global step 200 Train loss 0.37 Classification-F1 0.6374458874458874 on epoch=49
06/18/2022 07:26:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5773310023310022 -> 0.6374458874458874 on epoch=49, global_step=200
06/18/2022 07:26:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=52
06/18/2022 07:26:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.30 on epoch=54
06/18/2022 07:26:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=57
06/18/2022 07:26:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.28 on epoch=59
06/18/2022 07:27:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.30 on epoch=62
06/18/2022 07:27:00 - INFO - __main__ - Global step 250 Train loss 0.34 Classification-F1 0.6530872636135794 on epoch=62
06/18/2022 07:27:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6374458874458874 -> 0.6530872636135794 on epoch=62, global_step=250
06/18/2022 07:27:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=64
06/18/2022 07:27:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.24 on epoch=67
06/18/2022 07:27:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=69
06/18/2022 07:27:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=72
06/18/2022 07:27:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=74
06/18/2022 07:27:13 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.6791125541125541 on epoch=74
06/18/2022 07:27:13 - INFO - __main__ - Saving model with best Classification-F1: 0.6530872636135794 -> 0.6791125541125541 on epoch=74, global_step=300
06/18/2022 07:27:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=77
06/18/2022 07:27:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.16 on epoch=79
06/18/2022 07:27:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.19 on epoch=82
06/18/2022 07:27:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.17 on epoch=84
06/18/2022 07:27:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.17 on epoch=87
06/18/2022 07:27:26 - INFO - __main__ - Global step 350 Train loss 0.18 Classification-F1 0.6544806618819776 on epoch=87
06/18/2022 07:27:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=89
06/18/2022 07:27:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.15 on epoch=92
06/18/2022 07:27:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.16 on epoch=94
06/18/2022 07:27:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.14 on epoch=97
06/18/2022 07:27:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.11 on epoch=99
06/18/2022 07:27:38 - INFO - __main__ - Global step 400 Train loss 0.14 Classification-F1 0.6680999180999181 on epoch=99
06/18/2022 07:27:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=102
06/18/2022 07:27:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=104
06/18/2022 07:27:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.16 on epoch=107
06/18/2022 07:27:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.08 on epoch=109
06/18/2022 07:27:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=112
06/18/2022 07:27:51 - INFO - __main__ - Global step 450 Train loss 0.13 Classification-F1 0.6680999180999181 on epoch=112
06/18/2022 07:27:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=114
06/18/2022 07:27:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.05 on epoch=117
06/18/2022 07:27:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.08 on epoch=119
06/18/2022 07:28:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.15 on epoch=122
06/18/2022 07:28:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=124
06/18/2022 07:28:04 - INFO - __main__ - Global step 500 Train loss 0.10 Classification-F1 0.6680999180999181 on epoch=124
06/18/2022 07:28:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.11 on epoch=127
06/18/2022 07:28:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=129
06/18/2022 07:28:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=132
06/18/2022 07:28:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=134
06/18/2022 07:28:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=137
06/18/2022 07:28:16 - INFO - __main__ - Global step 550 Train loss 0.08 Classification-F1 0.6544806618819776 on epoch=137
06/18/2022 07:28:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=139
06/18/2022 07:28:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=142
06/18/2022 07:28:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.10 on epoch=144
06/18/2022 07:28:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=147
06/18/2022 07:28:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=149
06/18/2022 07:28:29 - INFO - __main__ - Global step 600 Train loss 0.06 Classification-F1 0.6741159608806668 on epoch=149
06/18/2022 07:28:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=152
06/18/2022 07:28:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=154
06/18/2022 07:28:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=157
06/18/2022 07:28:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=159
06/18/2022 07:28:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=162
06/18/2022 07:28:42 - INFO - __main__ - Global step 650 Train loss 0.09 Classification-F1 0.6543018551814319 on epoch=162
06/18/2022 07:28:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=164
06/18/2022 07:28:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=167
06/18/2022 07:28:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=169
06/18/2022 07:28:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=172
06/18/2022 07:28:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=174
06/18/2022 07:28:55 - INFO - __main__ - Global step 700 Train loss 0.05 Classification-F1 0.6543018551814319 on epoch=174
06/18/2022 07:28:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=177
06/18/2022 07:28:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=179
06/18/2022 07:29:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=182
06/18/2022 07:29:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=184
06/18/2022 07:29:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=187
06/18/2022 07:29:07 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.6252574002574003 on epoch=187
06/18/2022 07:29:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=189
06/18/2022 07:29:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=192
06/18/2022 07:29:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=194
06/18/2022 07:29:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=197
06/18/2022 07:29:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.08 on epoch=199
06/18/2022 07:29:20 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.6526383526383527 on epoch=199
06/18/2022 07:29:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=202
06/18/2022 07:29:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=204
06/18/2022 07:29:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=207
06/18/2022 07:29:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=209
06/18/2022 07:29:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=212
06/18/2022 07:29:33 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.6392560827343435 on epoch=212
06/18/2022 07:29:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=214
06/18/2022 07:29:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=217
06/18/2022 07:29:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=219
06/18/2022 07:29:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=222
06/18/2022 07:29:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=224
06/18/2022 07:29:46 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.6736652236652236 on epoch=224
06/18/2022 07:29:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=227
06/18/2022 07:29:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=229
06/18/2022 07:29:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=232
06/18/2022 07:29:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=234
06/18/2022 07:29:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=237
06/18/2022 07:29:58 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.6744123736451102 on epoch=237
06/18/2022 07:30:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=239
06/18/2022 07:30:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=242
06/18/2022 07:30:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=244
06/18/2022 07:30:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=247
06/18/2022 07:30:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=249
06/18/2022 07:30:11 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.6753472222222222 on epoch=249
06/18/2022 07:30:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
06/18/2022 07:30:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=254
06/18/2022 07:30:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=257
06/18/2022 07:30:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
06/18/2022 07:30:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=262
06/18/2022 07:30:24 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.6682387787650945 on epoch=262
06/18/2022 07:30:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=264
06/18/2022 07:30:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=267
06/18/2022 07:30:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.16 on epoch=269
06/18/2022 07:30:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=272
06/18/2022 07:30:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
06/18/2022 07:30:37 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.6896602418341549 on epoch=274
06/18/2022 07:30:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6791125541125541 -> 0.6896602418341549 on epoch=274, global_step=1100
06/18/2022 07:30:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
06/18/2022 07:30:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
06/18/2022 07:30:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
06/18/2022 07:30:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
06/18/2022 07:30:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
06/18/2022 07:30:50 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.668112714987715 on epoch=287
06/18/2022 07:30:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
06/18/2022 07:30:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
06/18/2022 07:30:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
06/18/2022 07:30:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
06/18/2022 07:31:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=299
06/18/2022 07:31:02 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.6667409387997624 on epoch=299
06/18/2022 07:31:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=302
06/18/2022 07:31:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=304
06/18/2022 07:31:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
06/18/2022 07:31:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
06/18/2022 07:31:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=312
06/18/2022 07:31:15 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.6670553105335715 on epoch=312
06/18/2022 07:31:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
06/18/2022 07:31:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=317
06/18/2022 07:31:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
06/18/2022 07:31:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
06/18/2022 07:31:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
06/18/2022 07:31:28 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.6809086779675015 on epoch=324
06/18/2022 07:31:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=327
06/18/2022 07:31:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
06/18/2022 07:31:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
06/18/2022 07:31:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=334
06/18/2022 07:31:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
06/18/2022 07:31:41 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.6535953906220386 on epoch=337
06/18/2022 07:31:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
06/18/2022 07:31:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=342
06/18/2022 07:31:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=344
06/18/2022 07:31:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=347
06/18/2022 07:31:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
06/18/2022 07:31:54 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6593137254901961 on epoch=349
06/18/2022 07:31:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
06/18/2022 07:31:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=354
06/18/2022 07:32:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
06/18/2022 07:32:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
06/18/2022 07:32:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
06/18/2022 07:32:07 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.6379673608573864 on epoch=362
06/18/2022 07:32:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
06/18/2022 07:32:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
06/18/2022 07:32:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=369
06/18/2022 07:32:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
06/18/2022 07:32:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
06/18/2022 07:32:20 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.6517316017316017 on epoch=374
06/18/2022 07:32:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
06/18/2022 07:32:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
06/18/2022 07:32:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=382
06/18/2022 07:32:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
06/18/2022 07:32:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
06/18/2022 07:32:32 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.7035119969040248 on epoch=387
06/18/2022 07:32:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6896602418341549 -> 0.7035119969040248 on epoch=387, global_step=1550
06/18/2022 07:32:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
06/18/2022 07:32:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
06/18/2022 07:32:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
06/18/2022 07:32:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
06/18/2022 07:32:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
06/18/2022 07:32:45 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.681988806988807 on epoch=399
06/18/2022 07:32:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/18/2022 07:32:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
06/18/2022 07:32:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
06/18/2022 07:32:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
06/18/2022 07:32:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
06/18/2022 07:32:58 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.6895149613899614 on epoch=412
06/18/2022 07:33:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
06/18/2022 07:33:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=417
06/18/2022 07:33:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
06/18/2022 07:33:08 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
06/18/2022 07:33:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
06/18/2022 07:33:11 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.7039459561198691 on epoch=424
06/18/2022 07:33:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7035119969040248 -> 0.7039459561198691 on epoch=424, global_step=1700
06/18/2022 07:33:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
06/18/2022 07:33:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
06/18/2022 07:33:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/18/2022 07:33:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
06/18/2022 07:33:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
06/18/2022 07:33:24 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.681988806988807 on epoch=437
06/18/2022 07:33:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
06/18/2022 07:33:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
06/18/2022 07:33:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
06/18/2022 07:33:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
06/18/2022 07:33:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
06/18/2022 07:33:37 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.688463399460842 on epoch=449
06/18/2022 07:33:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
06/18/2022 07:33:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
06/18/2022 07:33:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
06/18/2022 07:33:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=459
06/18/2022 07:33:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
06/18/2022 07:33:50 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.681988806988807 on epoch=462
06/18/2022 07:33:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
06/18/2022 07:33:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
06/18/2022 07:33:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
06/18/2022 07:33:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
06/18/2022 07:34:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=474
06/18/2022 07:34:03 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.681988806988807 on epoch=474
06/18/2022 07:34:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
06/18/2022 07:34:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
06/18/2022 07:34:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
06/18/2022 07:34:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
06/18/2022 07:34:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
06/18/2022 07:34:15 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7035119969040248 on epoch=487
06/18/2022 07:34:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
06/18/2022 07:34:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
06/18/2022 07:34:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
06/18/2022 07:34:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
06/18/2022 07:34:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
06/18/2022 07:34:28 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6956823877876509 on epoch=499
06/18/2022 07:34:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/18/2022 07:34:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
06/18/2022 07:34:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
06/18/2022 07:34:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
06/18/2022 07:34:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
06/18/2022 07:34:41 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.6815274455077087 on epoch=512
06/18/2022 07:34:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/18/2022 07:34:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
06/18/2022 07:34:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
06/18/2022 07:34:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
06/18/2022 07:34:53 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/18/2022 07:34:54 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.688463399460842 on epoch=524
06/18/2022 07:34:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
06/18/2022 07:34:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
06/18/2022 07:35:01 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/18/2022 07:35:03 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
06/18/2022 07:35:06 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/18/2022 07:35:07 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7036671058410189 on epoch=537
06/18/2022 07:35:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/18/2022 07:35:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/18/2022 07:35:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/18/2022 07:35:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/18/2022 07:35:19 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
06/18/2022 07:35:20 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6956823877876509 on epoch=549
06/18/2022 07:35:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
06/18/2022 07:35:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/18/2022 07:35:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
06/18/2022 07:35:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/18/2022 07:35:32 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/18/2022 07:35:33 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7179429916339325 on epoch=562
06/18/2022 07:35:33 - INFO - __main__ - Saving model with best Classification-F1: 0.7039459561198691 -> 0.7179429916339325 on epoch=562, global_step=2250
06/18/2022 07:35:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
06/18/2022 07:35:38 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
06/18/2022 07:35:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/18/2022 07:35:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/18/2022 07:35:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/18/2022 07:35:46 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7023458436838433 on epoch=574
06/18/2022 07:35:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/18/2022 07:35:50 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/18/2022 07:35:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/18/2022 07:35:55 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
06/18/2022 07:35:57 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/18/2022 07:35:58 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.6815274455077087 on epoch=587
06/18/2022 07:36:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=589
06/18/2022 07:36:03 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
06/18/2022 07:36:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/18/2022 07:36:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/18/2022 07:36:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/18/2022 07:36:11 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6956823877876509 on epoch=599
06/18/2022 07:36:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
06/18/2022 07:36:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.07 on epoch=604
06/18/2022 07:36:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/18/2022 07:36:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/18/2022 07:36:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/18/2022 07:36:24 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7023458436838433 on epoch=612
06/18/2022 07:36:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/18/2022 07:36:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/18/2022 07:36:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/18/2022 07:36:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
06/18/2022 07:36:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/18/2022 07:36:37 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7031692818137831 on epoch=624
06/18/2022 07:36:39 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/18/2022 07:36:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/18/2022 07:36:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/18/2022 07:36:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/18/2022 07:36:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/18/2022 07:36:50 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.7035119969040248 on epoch=637
06/18/2022 07:36:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/18/2022 07:36:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=642
06/18/2022 07:36:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/18/2022 07:37:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/18/2022 07:37:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/18/2022 07:37:03 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6809086779675015 on epoch=649
06/18/2022 07:37:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/18/2022 07:37:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/18/2022 07:37:10 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/18/2022 07:37:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
06/18/2022 07:37:15 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
06/18/2022 07:37:16 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7035119969040248 on epoch=662
06/18/2022 07:37:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/18/2022 07:37:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/18/2022 07:37:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/18/2022 07:37:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/18/2022 07:37:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/18/2022 07:37:29 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6956823877876509 on epoch=674
06/18/2022 07:37:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/18/2022 07:37:34 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/18/2022 07:37:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/18/2022 07:37:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/18/2022 07:37:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/18/2022 07:37:42 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7179429916339325 on epoch=687
06/18/2022 07:37:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 07:37:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/18/2022 07:37:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/18/2022 07:37:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/18/2022 07:37:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/18/2022 07:37:55 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.688821843233608 on epoch=699
06/18/2022 07:37:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/18/2022 07:38:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/18/2022 07:38:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/18/2022 07:38:04 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 07:38:07 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/18/2022 07:38:08 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6956823877876509 on epoch=712
06/18/2022 07:38:10 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/18/2022 07:38:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
06/18/2022 07:38:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/18/2022 07:38:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/18/2022 07:38:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/18/2022 07:38:21 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7035119969040248 on epoch=724
06/18/2022 07:38:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=727
06/18/2022 07:38:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/18/2022 07:38:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/18/2022 07:38:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 07:38:33 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/18/2022 07:38:34 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.731833384007297 on epoch=737
06/18/2022 07:38:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7179429916339325 -> 0.731833384007297 on epoch=737, global_step=2950
06/18/2022 07:38:36 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 07:38:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/18/2022 07:38:41 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/18/2022 07:38:43 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/18/2022 07:38:46 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/18/2022 07:38:47 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.731833384007297 on epoch=749
06/18/2022 07:38:47 - INFO - __main__ - save last model!
06/18/2022 07:38:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 07:38:47 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 07:38:47 - INFO - __main__ - Printing 3 examples
06/18/2022 07:38:47 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 07:38:47 - INFO - __main__ - ['others']
06/18/2022 07:38:47 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 07:38:47 - INFO - __main__ - ['others']
06/18/2022 07:38:47 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 07:38:47 - INFO - __main__ - ['others']
06/18/2022 07:38:47 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 07:38:47 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:38:47 - INFO - __main__ - Printing 3 examples
06/18/2022 07:38:47 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/18/2022 07:38:47 - INFO - __main__ - ['sad']
06/18/2022 07:38:47 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/18/2022 07:38:47 - INFO - __main__ - ['sad']
06/18/2022 07:38:47 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/18/2022 07:38:47 - INFO - __main__ - ['sad']
06/18/2022 07:38:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 07:38:47 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:38:47 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 07:38:47 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:38:47 - INFO - __main__ - Printing 3 examples
06/18/2022 07:38:47 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/18/2022 07:38:47 - INFO - __main__ - ['sad']
06/18/2022 07:38:47 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/18/2022 07:38:47 - INFO - __main__ - ['sad']
06/18/2022 07:38:47 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/18/2022 07:38:47 - INFO - __main__ - ['sad']
06/18/2022 07:38:47 - INFO - __main__ - Tokenizing Input ...
06/18/2022 07:38:47 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:38:47 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 07:38:49 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:38:54 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 07:39:06 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 07:39:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 07:39:06 - INFO - __main__ - Starting training!
06/18/2022 07:40:29 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_21_0.5_8_predictions.txt
06/18/2022 07:40:29 - INFO - __main__ - Classification-F1 on test data: 0.2193
06/18/2022 07:40:29 - INFO - __main__ - prefix=emo_16_21, lr=0.5, bsz=8, dev_performance=0.731833384007297, test_performance=0.21930686845568279
06/18/2022 07:40:29 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.4, bsz=8 ...
06/18/2022 07:40:30 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:40:30 - INFO - __main__ - Printing 3 examples
06/18/2022 07:40:30 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/18/2022 07:40:30 - INFO - __main__ - ['sad']
06/18/2022 07:40:30 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/18/2022 07:40:30 - INFO - __main__ - ['sad']
06/18/2022 07:40:30 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/18/2022 07:40:30 - INFO - __main__ - ['sad']
06/18/2022 07:40:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 07:40:30 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:40:30 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 07:40:30 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:40:30 - INFO - __main__ - Printing 3 examples
06/18/2022 07:40:30 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/18/2022 07:40:30 - INFO - __main__ - ['sad']
06/18/2022 07:40:30 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/18/2022 07:40:30 - INFO - __main__ - ['sad']
06/18/2022 07:40:30 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/18/2022 07:40:30 - INFO - __main__ - ['sad']
06/18/2022 07:40:30 - INFO - __main__ - Tokenizing Input ...
06/18/2022 07:40:30 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:40:30 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 07:40:49 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 07:40:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 07:40:49 - INFO - __main__ - Starting training!
06/18/2022 07:40:52 - INFO - __main__ - Step 10 Global step 10 Train loss 4.37 on epoch=2
06/18/2022 07:40:55 - INFO - __main__ - Step 20 Global step 20 Train loss 3.28 on epoch=4
06/18/2022 07:40:57 - INFO - __main__ - Step 30 Global step 30 Train loss 2.42 on epoch=7
06/18/2022 07:41:00 - INFO - __main__ - Step 40 Global step 40 Train loss 1.79 on epoch=9
06/18/2022 07:41:02 - INFO - __main__ - Step 50 Global step 50 Train loss 1.38 on epoch=12
06/18/2022 07:41:04 - INFO - __main__ - Global step 50 Train loss 2.65 Classification-F1 0.28949938949938947 on epoch=12
06/18/2022 07:41:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.28949938949938947 on epoch=12, global_step=50
06/18/2022 07:41:06 - INFO - __main__ - Step 60 Global step 60 Train loss 1.15 on epoch=14
06/18/2022 07:41:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=17
06/18/2022 07:41:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.82 on epoch=19
06/18/2022 07:41:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.68 on epoch=22
06/18/2022 07:41:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.74 on epoch=24
06/18/2022 07:41:17 - INFO - __main__ - Global step 100 Train loss 0.87 Classification-F1 0.5752543122512164 on epoch=24
06/18/2022 07:41:17 - INFO - __main__ - Saving model with best Classification-F1: 0.28949938949938947 -> 0.5752543122512164 on epoch=24, global_step=100
06/18/2022 07:41:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.64 on epoch=27
06/18/2022 07:41:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.67 on epoch=29
06/18/2022 07:41:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.65 on epoch=32
06/18/2022 07:41:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=34
06/18/2022 07:41:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=37
06/18/2022 07:41:31 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.6042471042471041 on epoch=37
06/18/2022 07:41:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5752543122512164 -> 0.6042471042471041 on epoch=37, global_step=150
06/18/2022 07:41:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=39
06/18/2022 07:41:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=42
06/18/2022 07:41:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.37 on epoch=44
06/18/2022 07:41:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=47
06/18/2022 07:41:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=49
06/18/2022 07:41:44 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.6515527950310559 on epoch=49
06/18/2022 07:41:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6042471042471041 -> 0.6515527950310559 on epoch=49, global_step=200
06/18/2022 07:41:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=52
06/18/2022 07:41:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=54
06/18/2022 07:41:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=57
06/18/2022 07:41:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=59
06/18/2022 07:41:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=62
06/18/2022 07:41:57 - INFO - __main__ - Global step 250 Train loss 0.36 Classification-F1 0.6382002801120448 on epoch=62
06/18/2022 07:42:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=64
06/18/2022 07:42:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=67
06/18/2022 07:42:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.30 on epoch=69
06/18/2022 07:42:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=72
06/18/2022 07:42:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=74
06/18/2022 07:42:11 - INFO - __main__ - Global step 300 Train loss 0.30 Classification-F1 0.6032509157509157 on epoch=74
06/18/2022 07:42:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=77
06/18/2022 07:42:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=79
06/18/2022 07:42:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.21 on epoch=82
06/18/2022 07:42:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=84
06/18/2022 07:42:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=87
06/18/2022 07:42:24 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.6392316017316018 on epoch=87
06/18/2022 07:42:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=89
06/18/2022 07:42:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=92
06/18/2022 07:42:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.19 on epoch=94
06/18/2022 07:42:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=97
06/18/2022 07:42:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.31 on epoch=99
06/18/2022 07:42:38 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.6665708388618605 on epoch=99
06/18/2022 07:42:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6515527950310559 -> 0.6665708388618605 on epoch=99, global_step=400
06/18/2022 07:42:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.17 on epoch=102
06/18/2022 07:42:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=104
06/18/2022 07:42:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=107
06/18/2022 07:42:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.13 on epoch=109
06/18/2022 07:42:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.20 on epoch=112
06/18/2022 07:42:51 - INFO - __main__ - Global step 450 Train loss 0.18 Classification-F1 0.6421919243342856 on epoch=112
06/18/2022 07:42:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.15 on epoch=114
06/18/2022 07:42:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=117
06/18/2022 07:42:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=119
06/18/2022 07:43:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.17 on epoch=122
06/18/2022 07:43:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.18 on epoch=124
06/18/2022 07:43:04 - INFO - __main__ - Global step 500 Train loss 0.17 Classification-F1 0.6548566017316018 on epoch=124
06/18/2022 07:43:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.11 on epoch=127
06/18/2022 07:43:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.15 on epoch=129
06/18/2022 07:43:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=132
06/18/2022 07:43:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=134
06/18/2022 07:43:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=137
06/18/2022 07:43:18 - INFO - __main__ - Global step 550 Train loss 0.10 Classification-F1 0.6548566017316018 on epoch=137
06/18/2022 07:43:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=139
06/18/2022 07:43:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=142
06/18/2022 07:43:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=144
06/18/2022 07:43:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.11 on epoch=147
06/18/2022 07:43:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=149
06/18/2022 07:43:31 - INFO - __main__ - Global step 600 Train loss 0.09 Classification-F1 0.6674111749963143 on epoch=149
06/18/2022 07:43:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6665708388618605 -> 0.6674111749963143 on epoch=149, global_step=600
06/18/2022 07:43:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.16 on epoch=152
06/18/2022 07:43:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=154
06/18/2022 07:43:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.07 on epoch=157
06/18/2022 07:43:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=159
06/18/2022 07:43:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=162
06/18/2022 07:43:45 - INFO - __main__ - Global step 650 Train loss 0.09 Classification-F1 0.6441421003580714 on epoch=162
06/18/2022 07:43:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=164
06/18/2022 07:43:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=167
06/18/2022 07:43:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=169
06/18/2022 07:43:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=172
06/18/2022 07:43:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=174
06/18/2022 07:43:58 - INFO - __main__ - Global step 700 Train loss 0.06 Classification-F1 0.668112714987715 on epoch=174
06/18/2022 07:43:58 - INFO - __main__ - Saving model with best Classification-F1: 0.6674111749963143 -> 0.668112714987715 on epoch=174, global_step=700
06/18/2022 07:44:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.08 on epoch=177
06/18/2022 07:44:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=179
06/18/2022 07:44:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=182
06/18/2022 07:44:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=184
06/18/2022 07:44:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=187
06/18/2022 07:44:12 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.6609952131691261 on epoch=187
06/18/2022 07:44:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=189
06/18/2022 07:44:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=192
06/18/2022 07:44:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=194
06/18/2022 07:44:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=197
06/18/2022 07:44:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=199
06/18/2022 07:44:25 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.660277124951038 on epoch=199
06/18/2022 07:44:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=202
06/18/2022 07:44:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=204
06/18/2022 07:44:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=207
06/18/2022 07:44:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=209
06/18/2022 07:44:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=212
06/18/2022 07:44:39 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.6525664319781966 on epoch=212
06/18/2022 07:44:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=214
06/18/2022 07:44:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=217
06/18/2022 07:44:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=219
06/18/2022 07:44:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=222
06/18/2022 07:44:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=224
06/18/2022 07:44:52 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.6738053613053613 on epoch=224
06/18/2022 07:44:52 - INFO - __main__ - Saving model with best Classification-F1: 0.668112714987715 -> 0.6738053613053613 on epoch=224, global_step=900
06/18/2022 07:44:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=227
06/18/2022 07:44:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=229
06/18/2022 07:45:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=232
06/18/2022 07:45:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=234
06/18/2022 07:45:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=237
06/18/2022 07:45:05 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.6741159608806668 on epoch=237
06/18/2022 07:45:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6738053613053613 -> 0.6741159608806668 on epoch=237, global_step=950
06/18/2022 07:45:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=239
06/18/2022 07:45:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
06/18/2022 07:45:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=244
06/18/2022 07:45:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=247
06/18/2022 07:45:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=249
06/18/2022 07:45:19 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.6582752106945655 on epoch=249
06/18/2022 07:45:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=252
06/18/2022 07:45:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=254
06/18/2022 07:45:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=257
06/18/2022 07:45:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=259
06/18/2022 07:45:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=262
06/18/2022 07:45:32 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.6255189604445897 on epoch=262
06/18/2022 07:45:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
06/18/2022 07:45:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=267
06/18/2022 07:45:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=269
06/18/2022 07:45:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=272
06/18/2022 07:45:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
06/18/2022 07:45:45 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.6245652994102835 on epoch=274
06/18/2022 07:45:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
06/18/2022 07:45:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
06/18/2022 07:45:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
06/18/2022 07:45:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
06/18/2022 07:45:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=287
06/18/2022 07:45:59 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.6245652994102835 on epoch=287
06/18/2022 07:46:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
06/18/2022 07:46:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=292
06/18/2022 07:46:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
06/18/2022 07:46:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
06/18/2022 07:46:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
06/18/2022 07:46:12 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.6520515080297689 on epoch=299
06/18/2022 07:46:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
06/18/2022 07:46:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
06/18/2022 07:46:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
06/18/2022 07:46:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
06/18/2022 07:46:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
06/18/2022 07:46:25 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.637466358363974 on epoch=312
06/18/2022 07:46:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
06/18/2022 07:46:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
06/18/2022 07:46:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
06/18/2022 07:46:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
06/18/2022 07:46:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
06/18/2022 07:46:39 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.637466358363974 on epoch=324
06/18/2022 07:46:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
06/18/2022 07:46:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=329
06/18/2022 07:46:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
06/18/2022 07:46:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
06/18/2022 07:46:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
06/18/2022 07:46:52 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.6876294743941803 on epoch=337
06/18/2022 07:46:52 - INFO - __main__ - Saving model with best Classification-F1: 0.6741159608806668 -> 0.6876294743941803 on epoch=337, global_step=1350
06/18/2022 07:46:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=339
06/18/2022 07:46:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
06/18/2022 07:47:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
06/18/2022 07:47:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
06/18/2022 07:47:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
06/18/2022 07:47:06 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6664619164619164 on epoch=349
06/18/2022 07:47:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=352
06/18/2022 07:47:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
06/18/2022 07:47:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
06/18/2022 07:47:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
06/18/2022 07:47:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
06/18/2022 07:47:19 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.6520515080297689 on epoch=362
06/18/2022 07:47:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
06/18/2022 07:47:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
06/18/2022 07:47:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
06/18/2022 07:47:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.12 on epoch=372
06/18/2022 07:47:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
06/18/2022 07:47:33 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.6662831097613706 on epoch=374
06/18/2022 07:47:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
06/18/2022 07:47:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
06/18/2022 07:47:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
06/18/2022 07:47:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
06/18/2022 07:47:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
06/18/2022 07:47:46 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.651834749918274 on epoch=387
06/18/2022 07:47:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=389
06/18/2022 07:47:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
06/18/2022 07:47:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
06/18/2022 07:47:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
06/18/2022 07:47:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
06/18/2022 07:48:00 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6522708062753829 on epoch=399
06/18/2022 07:48:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/18/2022 07:48:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
06/18/2022 07:48:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
06/18/2022 07:48:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
06/18/2022 07:48:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
06/18/2022 07:48:13 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.6670101211420323 on epoch=412
06/18/2022 07:48:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
06/18/2022 07:48:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
06/18/2022 07:48:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/18/2022 07:48:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
06/18/2022 07:48:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
06/18/2022 07:48:26 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.6674455044721525 on epoch=424
06/18/2022 07:48:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
06/18/2022 07:48:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
06/18/2022 07:48:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
06/18/2022 07:48:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
06/18/2022 07:48:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
06/18/2022 07:48:40 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.664992644655116 on epoch=437
06/18/2022 07:48:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
06/18/2022 07:48:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
06/18/2022 07:48:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
06/18/2022 07:48:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
06/18/2022 07:48:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
06/18/2022 07:48:53 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6649739327168994 on epoch=449
06/18/2022 07:48:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
06/18/2022 07:48:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
06/18/2022 07:49:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
06/18/2022 07:49:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
06/18/2022 07:49:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
06/18/2022 07:49:07 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6655138339920948 on epoch=462
06/18/2022 07:49:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
06/18/2022 07:49:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
06/18/2022 07:49:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
06/18/2022 07:49:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
06/18/2022 07:49:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
06/18/2022 07:49:20 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.6801680158788743 on epoch=474
06/18/2022 07:49:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
06/18/2022 07:49:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
06/18/2022 07:49:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/18/2022 07:49:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
06/18/2022 07:49:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
06/18/2022 07:49:34 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6802660172926653 on epoch=487
06/18/2022 07:49:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
06/18/2022 07:49:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
06/18/2022 07:49:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
06/18/2022 07:49:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/18/2022 07:49:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
06/18/2022 07:49:48 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.651834749918274 on epoch=499
06/18/2022 07:49:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
06/18/2022 07:49:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
06/18/2022 07:49:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
06/18/2022 07:49:57 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
06/18/2022 07:50:00 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
06/18/2022 07:50:01 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.651834749918274 on epoch=512
06/18/2022 07:50:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/18/2022 07:50:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
06/18/2022 07:50:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
06/18/2022 07:50:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
06/18/2022 07:50:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/18/2022 07:50:15 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.651834749918274 on epoch=524
06/18/2022 07:50:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
06/18/2022 07:50:20 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
06/18/2022 07:50:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/18/2022 07:50:25 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
06/18/2022 07:50:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/18/2022 07:50:29 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.651834749918274 on epoch=537
06/18/2022 07:50:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/18/2022 07:50:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/18/2022 07:50:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
06/18/2022 07:50:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
06/18/2022 07:50:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
06/18/2022 07:50:42 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6802660172926653 on epoch=549
06/18/2022 07:50:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/18/2022 07:50:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/18/2022 07:50:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=557
06/18/2022 07:50:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/18/2022 07:50:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
06/18/2022 07:50:56 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6649739327168994 on epoch=562
06/18/2022 07:50:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
06/18/2022 07:51:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
06/18/2022 07:51:03 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/18/2022 07:51:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/18/2022 07:51:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/18/2022 07:51:09 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6800479985963858 on epoch=574
06/18/2022 07:51:12 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/18/2022 07:51:14 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/18/2022 07:51:17 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
06/18/2022 07:51:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
06/18/2022 07:51:22 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
06/18/2022 07:51:23 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6793843411490471 on epoch=587
06/18/2022 07:51:25 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/18/2022 07:51:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/18/2022 07:51:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/18/2022 07:51:33 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/18/2022 07:51:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/18/2022 07:51:37 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.6649739327168994 on epoch=599
06/18/2022 07:51:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/18/2022 07:51:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/18/2022 07:51:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/18/2022 07:51:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/18/2022 07:51:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
06/18/2022 07:51:50 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6538522264051903 on epoch=612
06/18/2022 07:51:53 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/18/2022 07:51:55 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
06/18/2022 07:51:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/18/2022 07:52:00 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=622
06/18/2022 07:52:03 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/18/2022 07:52:04 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.6670101211420323 on epoch=624
06/18/2022 07:52:06 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.08 on epoch=627
06/18/2022 07:52:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/18/2022 07:52:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=632
06/18/2022 07:52:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/18/2022 07:52:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/18/2022 07:52:17 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.6387572927618694 on epoch=637
06/18/2022 07:52:20 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/18/2022 07:52:22 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
06/18/2022 07:52:25 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/18/2022 07:52:27 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=647
06/18/2022 07:52:30 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/18/2022 07:52:31 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6790023701788408 on epoch=649
06/18/2022 07:52:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/18/2022 07:52:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/18/2022 07:52:38 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/18/2022 07:52:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 07:52:43 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/18/2022 07:52:44 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.680783397888661 on epoch=662
06/18/2022 07:52:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/18/2022 07:52:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/18/2022 07:52:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
06/18/2022 07:52:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/18/2022 07:52:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/18/2022 07:52:58 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6793843411490471 on epoch=674
06/18/2022 07:53:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/18/2022 07:53:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/18/2022 07:53:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=682
06/18/2022 07:53:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/18/2022 07:53:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/18/2022 07:53:12 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6790023701788408 on epoch=687
06/18/2022 07:53:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 07:53:17 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/18/2022 07:53:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
06/18/2022 07:53:22 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
06/18/2022 07:53:24 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/18/2022 07:53:25 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6802660172926653 on epoch=699
06/18/2022 07:53:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/18/2022 07:53:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.09 on epoch=704
06/18/2022 07:53:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=707
06/18/2022 07:53:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 07:53:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/18/2022 07:53:39 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6802660172926653 on epoch=712
06/18/2022 07:53:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/18/2022 07:53:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/18/2022 07:53:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/18/2022 07:53:49 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/18/2022 07:53:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/18/2022 07:53:53 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6802660172926653 on epoch=724
06/18/2022 07:53:55 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/18/2022 07:53:58 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/18/2022 07:54:00 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/18/2022 07:54:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 07:54:05 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/18/2022 07:54:06 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6802660172926653 on epoch=737
06/18/2022 07:54:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 07:54:11 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/18/2022 07:54:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/18/2022 07:54:16 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/18/2022 07:54:19 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
06/18/2022 07:54:20 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6802660172926653 on epoch=749
06/18/2022 07:54:20 - INFO - __main__ - save last model!
06/18/2022 07:54:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 07:54:20 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:54:20 - INFO - __main__ - Printing 3 examples
06/18/2022 07:54:20 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/18/2022 07:54:20 - INFO - __main__ - ['sad']
06/18/2022 07:54:20 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/18/2022 07:54:20 - INFO - __main__ - ['sad']
06/18/2022 07:54:20 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/18/2022 07:54:20 - INFO - __main__ - ['sad']
06/18/2022 07:54:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 07:54:20 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 07:54:20 - INFO - __main__ - Printing 3 examples
06/18/2022 07:54:20 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 07:54:20 - INFO - __main__ - ['others']
06/18/2022 07:54:20 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 07:54:20 - INFO - __main__ - ['others']
06/18/2022 07:54:20 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 07:54:20 - INFO - __main__ - ['others']
06/18/2022 07:54:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 07:54:20 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:54:20 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 07:54:20 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:54:20 - INFO - __main__ - Printing 3 examples
06/18/2022 07:54:20 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/18/2022 07:54:20 - INFO - __main__ - ['sad']
06/18/2022 07:54:20 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/18/2022 07:54:20 - INFO - __main__ - ['sad']
06/18/2022 07:54:20 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/18/2022 07:54:20 - INFO - __main__ - ['sad']
06/18/2022 07:54:20 - INFO - __main__ - Tokenizing Input ...
06/18/2022 07:54:20 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:54:20 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 07:54:22 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:54:28 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 07:54:39 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 07:54:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 07:54:40 - INFO - __main__ - Starting training!
06/18/2022 07:56:05 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_21_0.4_8_predictions.txt
06/18/2022 07:56:05 - INFO - __main__ - Classification-F1 on test data: 0.1696
06/18/2022 07:56:05 - INFO - __main__ - prefix=emo_16_21, lr=0.4, bsz=8, dev_performance=0.6876294743941803, test_performance=0.16955701723527034
06/18/2022 07:56:05 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.3, bsz=8 ...
06/18/2022 07:56:06 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:56:06 - INFO - __main__ - Printing 3 examples
06/18/2022 07:56:06 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/18/2022 07:56:06 - INFO - __main__ - ['sad']
06/18/2022 07:56:06 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/18/2022 07:56:06 - INFO - __main__ - ['sad']
06/18/2022 07:56:06 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/18/2022 07:56:06 - INFO - __main__ - ['sad']
06/18/2022 07:56:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 07:56:06 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:56:06 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 07:56:06 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 07:56:06 - INFO - __main__ - Printing 3 examples
06/18/2022 07:56:06 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/18/2022 07:56:06 - INFO - __main__ - ['sad']
06/18/2022 07:56:06 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/18/2022 07:56:06 - INFO - __main__ - ['sad']
06/18/2022 07:56:06 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/18/2022 07:56:06 - INFO - __main__ - ['sad']
06/18/2022 07:56:06 - INFO - __main__ - Tokenizing Input ...
06/18/2022 07:56:06 - INFO - __main__ - Tokenizing Output ...
06/18/2022 07:56:06 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 07:56:25 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 07:56:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 07:56:25 - INFO - __main__ - Starting training!
06/18/2022 07:56:28 - INFO - __main__ - Step 10 Global step 10 Train loss 4.19 on epoch=2
06/18/2022 07:56:31 - INFO - __main__ - Step 20 Global step 20 Train loss 3.42 on epoch=4
06/18/2022 07:56:33 - INFO - __main__ - Step 30 Global step 30 Train loss 2.62 on epoch=7
06/18/2022 07:56:36 - INFO - __main__ - Step 40 Global step 40 Train loss 2.20 on epoch=9
06/18/2022 07:56:38 - INFO - __main__ - Step 50 Global step 50 Train loss 1.65 on epoch=12
06/18/2022 07:56:40 - INFO - __main__ - Global step 50 Train loss 2.82 Classification-F1 0.1276128762541806 on epoch=12
06/18/2022 07:56:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1276128762541806 on epoch=12, global_step=50
06/18/2022 07:56:43 - INFO - __main__ - Step 60 Global step 60 Train loss 1.42 on epoch=14
06/18/2022 07:56:45 - INFO - __main__ - Step 70 Global step 70 Train loss 1.33 on epoch=17
06/18/2022 07:56:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.91 on epoch=19
06/18/2022 07:56:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.94 on epoch=22
06/18/2022 07:56:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.79 on epoch=24
06/18/2022 07:56:53 - INFO - __main__ - Global step 100 Train loss 1.08 Classification-F1 0.4652218782249742 on epoch=24
06/18/2022 07:56:53 - INFO - __main__ - Saving model with best Classification-F1: 0.1276128762541806 -> 0.4652218782249742 on epoch=24, global_step=100
06/18/2022 07:56:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.84 on epoch=27
06/18/2022 07:56:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.69 on epoch=29
06/18/2022 07:57:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.66 on epoch=32
06/18/2022 07:57:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.68 on epoch=34
06/18/2022 07:57:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.61 on epoch=37
06/18/2022 07:57:07 - INFO - __main__ - Global step 150 Train loss 0.70 Classification-F1 0.5666125541125542 on epoch=37
06/18/2022 07:57:07 - INFO - __main__ - Saving model with best Classification-F1: 0.4652218782249742 -> 0.5666125541125542 on epoch=37, global_step=150
06/18/2022 07:57:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.64 on epoch=39
06/18/2022 07:57:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.65 on epoch=42
06/18/2022 07:57:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=44
06/18/2022 07:57:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.61 on epoch=47
06/18/2022 07:57:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=49
06/18/2022 07:57:20 - INFO - __main__ - Global step 200 Train loss 0.56 Classification-F1 0.6067669172932331 on epoch=49
06/18/2022 07:57:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5666125541125542 -> 0.6067669172932331 on epoch=49, global_step=200
06/18/2022 07:57:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=52
06/18/2022 07:57:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=54
06/18/2022 07:57:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=57
06/18/2022 07:57:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=59
06/18/2022 07:57:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=62
06/18/2022 07:57:34 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.5633971291866029 on epoch=62
06/18/2022 07:57:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=64
06/18/2022 07:57:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.56 on epoch=67
06/18/2022 07:57:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=69
06/18/2022 07:57:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=72
06/18/2022 07:57:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=74
06/18/2022 07:57:47 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.6800075102504771 on epoch=74
06/18/2022 07:57:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6067669172932331 -> 0.6800075102504771 on epoch=74, global_step=300
06/18/2022 07:57:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=77
06/18/2022 07:57:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=79
06/18/2022 07:57:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=82
06/18/2022 07:57:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=84
06/18/2022 07:57:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=87
06/18/2022 07:58:00 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.6394348894348895 on epoch=87
06/18/2022 07:58:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.30 on epoch=89
06/18/2022 07:58:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=92
06/18/2022 07:58:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.33 on epoch=94
06/18/2022 07:58:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=97
06/18/2022 07:58:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=99
06/18/2022 07:58:14 - INFO - __main__ - Global step 400 Train loss 0.31 Classification-F1 0.6527052926433732 on epoch=99
06/18/2022 07:58:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.31 on epoch=102
06/18/2022 07:58:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=104
06/18/2022 07:58:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
06/18/2022 07:58:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=109
06/18/2022 07:58:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=112
06/18/2022 07:58:27 - INFO - __main__ - Global step 450 Train loss 0.25 Classification-F1 0.6666229636817872 on epoch=112
06/18/2022 07:58:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=114
06/18/2022 07:58:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=117
06/18/2022 07:58:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=119
06/18/2022 07:58:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.18 on epoch=122
06/18/2022 07:58:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=124
06/18/2022 07:58:40 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.6666229636817872 on epoch=124
06/18/2022 07:58:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=127
06/18/2022 07:58:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=129
06/18/2022 07:58:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.16 on epoch=132
06/18/2022 07:58:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=134
06/18/2022 07:58:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=137
06/18/2022 07:58:54 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.6548566017316018 on epoch=137
06/18/2022 07:58:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=139
06/18/2022 07:58:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.15 on epoch=142
06/18/2022 07:59:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=144
06/18/2022 07:59:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=147
06/18/2022 07:59:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=149
06/18/2022 07:59:07 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.6548566017316018 on epoch=149
06/18/2022 07:59:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=152
06/18/2022 07:59:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=154
06/18/2022 07:59:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=157
06/18/2022 07:59:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=159
06/18/2022 07:59:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=162
06/18/2022 07:59:21 - INFO - __main__ - Global step 650 Train loss 0.15 Classification-F1 0.6411133221617092 on epoch=162
06/18/2022 07:59:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=164
06/18/2022 07:59:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=167
06/18/2022 07:59:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=169
06/18/2022 07:59:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=172
06/18/2022 07:59:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=174
06/18/2022 07:59:34 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.6752292471042471 on epoch=174
06/18/2022 07:59:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=177
06/18/2022 07:59:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=179
06/18/2022 07:59:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=182
06/18/2022 07:59:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=184
06/18/2022 07:59:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=187
06/18/2022 07:59:47 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.6610295901042931 on epoch=187
06/18/2022 07:59:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=189
06/18/2022 07:59:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=192
06/18/2022 07:59:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=194
06/18/2022 07:59:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=197
06/18/2022 08:00:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=199
06/18/2022 08:00:01 - INFO - __main__ - Global step 800 Train loss 0.12 Classification-F1 0.6403392773659253 on epoch=199
06/18/2022 08:00:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=202
06/18/2022 08:00:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.15 on epoch=204
06/18/2022 08:00:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=207
06/18/2022 08:00:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=209
06/18/2022 08:00:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=212
06/18/2022 08:00:14 - INFO - __main__ - Global step 850 Train loss 0.09 Classification-F1 0.6249842899036447 on epoch=212
06/18/2022 08:00:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=214
06/18/2022 08:00:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=217
06/18/2022 08:00:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=219
06/18/2022 08:00:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=222
06/18/2022 08:00:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
06/18/2022 08:00:27 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.652948402948403 on epoch=224
06/18/2022 08:00:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=227
06/18/2022 08:00:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=229
06/18/2022 08:00:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
06/18/2022 08:00:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=234
06/18/2022 08:00:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=237
06/18/2022 08:00:41 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.6243890518084065 on epoch=237
06/18/2022 08:00:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=239
06/18/2022 08:00:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=242
06/18/2022 08:00:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
06/18/2022 08:00:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=247
06/18/2022 08:00:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
06/18/2022 08:00:54 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.6619588744588745 on epoch=249
06/18/2022 08:00:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=252
06/18/2022 08:00:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=254
06/18/2022 08:01:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=257
06/18/2022 08:01:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
06/18/2022 08:01:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=262
06/18/2022 08:01:07 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.6537366142629302 on epoch=262
06/18/2022 08:01:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
06/18/2022 08:01:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
06/18/2022 08:01:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=269
06/18/2022 08:01:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=272
06/18/2022 08:01:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
06/18/2022 08:01:21 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.6243890518084065 on epoch=274
06/18/2022 08:01:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=277
06/18/2022 08:01:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
06/18/2022 08:01:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
06/18/2022 08:01:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=284
06/18/2022 08:01:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
06/18/2022 08:01:34 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.6386363636363637 on epoch=287
06/18/2022 08:01:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
06/18/2022 08:01:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=292
06/18/2022 08:01:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
06/18/2022 08:01:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=297
06/18/2022 08:01:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=299
06/18/2022 08:01:47 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.6243890518084065 on epoch=299
06/18/2022 08:01:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=302
06/18/2022 08:01:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
06/18/2022 08:01:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=307
06/18/2022 08:01:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
06/18/2022 08:02:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
06/18/2022 08:02:01 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.6398809523809523 on epoch=312
06/18/2022 08:02:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=314
06/18/2022 08:02:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
06/18/2022 08:02:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
06/18/2022 08:02:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=322
06/18/2022 08:02:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=324
06/18/2022 08:02:14 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.6093208874458875 on epoch=324
06/18/2022 08:02:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
06/18/2022 08:02:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
06/18/2022 08:02:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
06/18/2022 08:02:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
06/18/2022 08:02:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
06/18/2022 08:02:28 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.6377968877968877 on epoch=337
06/18/2022 08:02:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
06/18/2022 08:02:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=342
06/18/2022 08:02:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
06/18/2022 08:02:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
06/18/2022 08:02:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
06/18/2022 08:02:41 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6596846846846847 on epoch=349
06/18/2022 08:02:44 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.10 on epoch=352
06/18/2022 08:02:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=354
06/18/2022 08:02:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
06/18/2022 08:02:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
06/18/2022 08:02:54 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
06/18/2022 08:02:55 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.6525664319781966 on epoch=362
06/18/2022 08:02:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
06/18/2022 08:03:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
06/18/2022 08:03:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
06/18/2022 08:03:05 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
06/18/2022 08:03:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
06/18/2022 08:03:08 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.644652124951038 on epoch=374
06/18/2022 08:03:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
06/18/2022 08:03:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
06/18/2022 08:03:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=382
06/18/2022 08:03:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
06/18/2022 08:03:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
06/18/2022 08:03:22 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.6234402852049911 on epoch=387
06/18/2022 08:03:24 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
06/18/2022 08:03:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
06/18/2022 08:03:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
06/18/2022 08:03:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
06/18/2022 08:03:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/18/2022 08:03:36 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6233492014742015 on epoch=399
06/18/2022 08:03:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
06/18/2022 08:03:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
06/18/2022 08:03:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
06/18/2022 08:03:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/18/2022 08:03:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=412
06/18/2022 08:03:49 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.6532759263022421 on epoch=412
06/18/2022 08:03:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
06/18/2022 08:03:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
06/18/2022 08:03:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/18/2022 08:03:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
06/18/2022 08:04:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
06/18/2022 08:04:03 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.6254272043745728 on epoch=424
06/18/2022 08:04:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
06/18/2022 08:04:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/18/2022 08:04:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/18/2022 08:04:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
06/18/2022 08:04:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
06/18/2022 08:04:16 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.6394348894348895 on epoch=437
06/18/2022 08:04:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
06/18/2022 08:04:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
06/18/2022 08:04:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
06/18/2022 08:04:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
06/18/2022 08:04:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
06/18/2022 08:04:29 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6525664319781966 on epoch=449
06/18/2022 08:04:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
06/18/2022 08:04:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
06/18/2022 08:04:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
06/18/2022 08:04:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
06/18/2022 08:04:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
06/18/2022 08:04:43 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6532759263022421 on epoch=462
06/18/2022 08:04:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
06/18/2022 08:04:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/18/2022 08:04:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/18/2022 08:04:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
06/18/2022 08:04:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
06/18/2022 08:04:56 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.6745087266826397 on epoch=474
06/18/2022 08:04:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
06/18/2022 08:05:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
06/18/2022 08:05:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/18/2022 08:05:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
06/18/2022 08:05:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
06/18/2022 08:05:10 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.652948402948403 on epoch=487
06/18/2022 08:05:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
06/18/2022 08:05:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/18/2022 08:05:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
06/18/2022 08:05:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
06/18/2022 08:05:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
06/18/2022 08:05:23 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6731811145510836 on epoch=499
06/18/2022 08:05:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/18/2022 08:05:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
06/18/2022 08:05:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
06/18/2022 08:05:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/18/2022 08:05:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
06/18/2022 08:05:36 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6661518661518662 on epoch=512
06/18/2022 08:05:39 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/18/2022 08:05:41 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
06/18/2022 08:05:44 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
06/18/2022 08:05:47 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
06/18/2022 08:05:49 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
06/18/2022 08:05:50 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6250244810027418 on epoch=524
06/18/2022 08:05:53 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
06/18/2022 08:05:55 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/18/2022 08:05:58 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
06/18/2022 08:06:00 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
06/18/2022 08:06:03 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/18/2022 08:06:04 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6525664319781966 on epoch=537
06/18/2022 08:06:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/18/2022 08:06:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/18/2022 08:06:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/18/2022 08:06:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=547
06/18/2022 08:06:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
06/18/2022 08:06:17 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6515873015873015 on epoch=549
06/18/2022 08:06:20 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/18/2022 08:06:22 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/18/2022 08:06:25 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=557
06/18/2022 08:06:27 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/18/2022 08:06:29 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/18/2022 08:06:31 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.652948402948403 on epoch=562
06/18/2022 08:06:33 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
06/18/2022 08:06:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
06/18/2022 08:06:38 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/18/2022 08:06:40 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/18/2022 08:06:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/18/2022 08:06:43 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.652948402948403 on epoch=574
06/18/2022 08:06:46 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/18/2022 08:06:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/18/2022 08:06:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/18/2022 08:06:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
06/18/2022 08:06:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
06/18/2022 08:06:56 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.6535953906220386 on epoch=587
06/18/2022 08:06:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
06/18/2022 08:07:01 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=592
06/18/2022 08:07:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/18/2022 08:07:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/18/2022 08:07:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/18/2022 08:07:10 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.652948402948403 on epoch=599
06/18/2022 08:07:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/18/2022 08:07:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
06/18/2022 08:07:17 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/18/2022 08:07:19 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/18/2022 08:07:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/18/2022 08:07:23 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6385379945162553 on epoch=612
06/18/2022 08:07:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/18/2022 08:07:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/18/2022 08:07:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/18/2022 08:07:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/18/2022 08:07:35 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/18/2022 08:07:36 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6517316017316017 on epoch=624
06/18/2022 08:07:38 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/18/2022 08:07:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/18/2022 08:07:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/18/2022 08:07:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/18/2022 08:07:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/18/2022 08:07:49 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.6517316017316017 on epoch=637
06/18/2022 08:07:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/18/2022 08:07:54 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
06/18/2022 08:07:56 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/18/2022 08:07:59 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/18/2022 08:08:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/18/2022 08:08:02 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6535953906220386 on epoch=649
06/18/2022 08:08:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/18/2022 08:08:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
06/18/2022 08:08:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/18/2022 08:08:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 08:08:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/18/2022 08:08:15 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6596846846846847 on epoch=662
06/18/2022 08:08:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
06/18/2022 08:08:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/18/2022 08:08:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=669
06/18/2022 08:08:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/18/2022 08:08:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/18/2022 08:08:28 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6735720375106563 on epoch=674
06/18/2022 08:08:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=677
06/18/2022 08:08:33 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/18/2022 08:08:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/18/2022 08:08:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/18/2022 08:08:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/18/2022 08:08:42 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6596042471042471 on epoch=687
06/18/2022 08:08:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 08:08:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
06/18/2022 08:08:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=694
06/18/2022 08:08:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/18/2022 08:08:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/18/2022 08:08:55 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.6389133698916307 on epoch=699
06/18/2022 08:08:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/18/2022 08:09:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/18/2022 08:09:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/18/2022 08:09:04 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 08:09:07 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/18/2022 08:09:08 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6389133698916307 on epoch=712
06/18/2022 08:09:10 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/18/2022 08:09:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/18/2022 08:09:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/18/2022 08:09:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/18/2022 08:09:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
06/18/2022 08:09:21 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6538522264051903 on epoch=724
06/18/2022 08:09:24 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=727
06/18/2022 08:09:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/18/2022 08:09:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/18/2022 08:09:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
06/18/2022 08:09:33 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
06/18/2022 08:09:34 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.6683695507708667 on epoch=737
06/18/2022 08:09:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=739
06/18/2022 08:09:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
06/18/2022 08:09:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/18/2022 08:09:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
06/18/2022 08:09:46 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/18/2022 08:09:48 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.6527695962478571 on epoch=749
06/18/2022 08:09:48 - INFO - __main__ - save last model!
06/18/2022 08:09:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 08:09:48 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 08:09:48 - INFO - __main__ - Printing 3 examples
06/18/2022 08:09:48 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 08:09:48 - INFO - __main__ - ['others']
06/18/2022 08:09:48 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 08:09:48 - INFO - __main__ - ['others']
06/18/2022 08:09:48 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 08:09:48 - INFO - __main__ - ['others']
06/18/2022 08:09:48 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 08:09:48 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:09:48 - INFO - __main__ - Printing 3 examples
06/18/2022 08:09:48 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/18/2022 08:09:48 - INFO - __main__ - ['sad']
06/18/2022 08:09:48 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/18/2022 08:09:48 - INFO - __main__ - ['sad']
06/18/2022 08:09:48 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/18/2022 08:09:48 - INFO - __main__ - ['sad']
06/18/2022 08:09:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 08:09:48 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:09:48 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 08:09:48 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:09:48 - INFO - __main__ - Printing 3 examples
06/18/2022 08:09:48 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/18/2022 08:09:48 - INFO - __main__ - ['sad']
06/18/2022 08:09:48 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/18/2022 08:09:48 - INFO - __main__ - ['sad']
06/18/2022 08:09:48 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/18/2022 08:09:48 - INFO - __main__ - ['sad']
06/18/2022 08:09:48 - INFO - __main__ - Tokenizing Input ...
06/18/2022 08:09:48 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:09:48 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 08:09:50 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:09:55 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 08:10:05 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 08:10:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 08:10:06 - INFO - __main__ - Starting training!
06/18/2022 08:11:34 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_21_0.3_8_predictions.txt
06/18/2022 08:11:34 - INFO - __main__ - Classification-F1 on test data: 0.1234
06/18/2022 08:11:34 - INFO - __main__ - prefix=emo_16_21, lr=0.3, bsz=8, dev_performance=0.6800075102504771, test_performance=0.12342620742718995
06/18/2022 08:11:34 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.2, bsz=8 ...
06/18/2022 08:11:35 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:11:35 - INFO - __main__ - Printing 3 examples
06/18/2022 08:11:35 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/18/2022 08:11:35 - INFO - __main__ - ['sad']
06/18/2022 08:11:35 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/18/2022 08:11:35 - INFO - __main__ - ['sad']
06/18/2022 08:11:35 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/18/2022 08:11:35 - INFO - __main__ - ['sad']
06/18/2022 08:11:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 08:11:35 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:11:35 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 08:11:35 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:11:35 - INFO - __main__ - Printing 3 examples
06/18/2022 08:11:35 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/18/2022 08:11:35 - INFO - __main__ - ['sad']
06/18/2022 08:11:35 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/18/2022 08:11:35 - INFO - __main__ - ['sad']
06/18/2022 08:11:35 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/18/2022 08:11:35 - INFO - __main__ - ['sad']
06/18/2022 08:11:35 - INFO - __main__ - Tokenizing Input ...
06/18/2022 08:11:35 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:11:35 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 08:11:54 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 08:11:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 08:11:54 - INFO - __main__ - Starting training!
06/18/2022 08:11:57 - INFO - __main__ - Step 10 Global step 10 Train loss 4.78 on epoch=2
06/18/2022 08:12:00 - INFO - __main__ - Step 20 Global step 20 Train loss 3.77 on epoch=4
06/18/2022 08:12:02 - INFO - __main__ - Step 30 Global step 30 Train loss 3.26 on epoch=7
06/18/2022 08:12:05 - INFO - __main__ - Step 40 Global step 40 Train loss 2.75 on epoch=9
06/18/2022 08:12:07 - INFO - __main__ - Step 50 Global step 50 Train loss 2.46 on epoch=12
06/18/2022 08:12:09 - INFO - __main__ - Global step 50 Train loss 3.40 Classification-F1 0.034782608695652174 on epoch=12
06/18/2022 08:12:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.034782608695652174 on epoch=12, global_step=50
06/18/2022 08:12:11 - INFO - __main__ - Step 60 Global step 60 Train loss 2.03 on epoch=14
06/18/2022 08:12:14 - INFO - __main__ - Step 70 Global step 70 Train loss 1.90 on epoch=17
06/18/2022 08:12:16 - INFO - __main__ - Step 80 Global step 80 Train loss 1.63 on epoch=19
06/18/2022 08:12:19 - INFO - __main__ - Step 90 Global step 90 Train loss 1.34 on epoch=22
06/18/2022 08:12:21 - INFO - __main__ - Step 100 Global step 100 Train loss 1.15 on epoch=24
06/18/2022 08:12:23 - INFO - __main__ - Global step 100 Train loss 1.61 Classification-F1 0.28142325510746563 on epoch=24
06/18/2022 08:12:23 - INFO - __main__ - Saving model with best Classification-F1: 0.034782608695652174 -> 0.28142325510746563 on epoch=24, global_step=100
06/18/2022 08:12:25 - INFO - __main__ - Step 110 Global step 110 Train loss 1.12 on epoch=27
06/18/2022 08:12:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.95 on epoch=29
06/18/2022 08:12:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.88 on epoch=32
06/18/2022 08:12:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.95 on epoch=34
06/18/2022 08:12:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.80 on epoch=37
06/18/2022 08:12:36 - INFO - __main__ - Global step 150 Train loss 0.94 Classification-F1 0.5249644381223328 on epoch=37
06/18/2022 08:12:36 - INFO - __main__ - Saving model with best Classification-F1: 0.28142325510746563 -> 0.5249644381223328 on epoch=37, global_step=150
06/18/2022 08:12:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.76 on epoch=39
06/18/2022 08:12:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.69 on epoch=42
06/18/2022 08:12:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.66 on epoch=44
06/18/2022 08:12:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.72 on epoch=47
06/18/2022 08:12:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.65 on epoch=49
06/18/2022 08:12:49 - INFO - __main__ - Global step 200 Train loss 0.70 Classification-F1 0.5649859943977591 on epoch=49
06/18/2022 08:12:49 - INFO - __main__ - Saving model with best Classification-F1: 0.5249644381223328 -> 0.5649859943977591 on epoch=49, global_step=200
06/18/2022 08:12:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.67 on epoch=52
06/18/2022 08:12:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.59 on epoch=54
06/18/2022 08:12:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=57
06/18/2022 08:12:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=59
06/18/2022 08:13:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=62
06/18/2022 08:13:03 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.5923423423423422 on epoch=62
06/18/2022 08:13:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5649859943977591 -> 0.5923423423423422 on epoch=62, global_step=250
06/18/2022 08:13:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.58 on epoch=64
06/18/2022 08:13:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=67
06/18/2022 08:13:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=69
06/18/2022 08:13:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=72
06/18/2022 08:13:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=74
06/18/2022 08:13:16 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.5856054006968641 on epoch=74
06/18/2022 08:13:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=77
06/18/2022 08:13:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=79
06/18/2022 08:13:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=82
06/18/2022 08:13:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=84
06/18/2022 08:13:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=87
06/18/2022 08:13:30 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.629004329004329 on epoch=87
06/18/2022 08:13:30 - INFO - __main__ - Saving model with best Classification-F1: 0.5923423423423422 -> 0.629004329004329 on epoch=87, global_step=350
06/18/2022 08:13:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=89
06/18/2022 08:13:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.32 on epoch=92
06/18/2022 08:13:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=94
06/18/2022 08:13:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=97
06/18/2022 08:13:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=99
06/18/2022 08:13:43 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.6423423423423423 on epoch=99
06/18/2022 08:13:43 - INFO - __main__ - Saving model with best Classification-F1: 0.629004329004329 -> 0.6423423423423423 on epoch=99, global_step=400
06/18/2022 08:13:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=102
06/18/2022 08:13:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=104
06/18/2022 08:13:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=107
06/18/2022 08:13:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=109
06/18/2022 08:13:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=112
06/18/2022 08:13:57 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.6530872636135794 on epoch=112
06/18/2022 08:13:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6423423423423423 -> 0.6530872636135794 on epoch=112, global_step=450
06/18/2022 08:13:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=114
06/18/2022 08:14:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=117
06/18/2022 08:14:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=119
06/18/2022 08:14:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=122
06/18/2022 08:14:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=124
06/18/2022 08:14:10 - INFO - __main__ - Global step 500 Train loss 0.33 Classification-F1 0.6405056759545924 on epoch=124
06/18/2022 08:14:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.33 on epoch=127
06/18/2022 08:14:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=129
06/18/2022 08:14:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.30 on epoch=132
06/18/2022 08:14:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=134
06/18/2022 08:14:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=137
06/18/2022 08:14:24 - INFO - __main__ - Global step 550 Train loss 0.30 Classification-F1 0.588095238095238 on epoch=137
06/18/2022 08:14:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=139
06/18/2022 08:14:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=142
06/18/2022 08:14:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=144
06/18/2022 08:14:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.29 on epoch=147
06/18/2022 08:14:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.28 on epoch=149
06/18/2022 08:14:37 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.588095238095238 on epoch=149
06/18/2022 08:14:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=152
06/18/2022 08:14:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.26 on epoch=154
06/18/2022 08:14:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=157
06/18/2022 08:14:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=159
06/18/2022 08:14:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=162
06/18/2022 08:14:51 - INFO - __main__ - Global step 650 Train loss 0.26 Classification-F1 0.6139097744360902 on epoch=162
06/18/2022 08:14:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=164
06/18/2022 08:14:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=167
06/18/2022 08:14:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=169
06/18/2022 08:15:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=172
06/18/2022 08:15:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=174
06/18/2022 08:15:04 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.651917081328846 on epoch=174
06/18/2022 08:15:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=177
06/18/2022 08:15:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=179
06/18/2022 08:15:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=182
06/18/2022 08:15:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.16 on epoch=184
06/18/2022 08:15:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=187
06/18/2022 08:15:17 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.6379357484620642 on epoch=187
06/18/2022 08:15:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=189
06/18/2022 08:15:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=192
06/18/2022 08:15:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=194
06/18/2022 08:15:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=197
06/18/2022 08:15:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.17 on epoch=199
06/18/2022 08:15:31 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.6518661518661519 on epoch=199
06/18/2022 08:15:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=202
06/18/2022 08:15:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.15 on epoch=204
06/18/2022 08:15:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=207
06/18/2022 08:15:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=209
06/18/2022 08:15:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=212
06/18/2022 08:15:44 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.6656204906204907 on epoch=212
06/18/2022 08:15:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6530872636135794 -> 0.6656204906204907 on epoch=212, global_step=850
06/18/2022 08:15:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=214
06/18/2022 08:15:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=217
06/18/2022 08:15:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=219
06/18/2022 08:15:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=222
06/18/2022 08:15:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=224
06/18/2022 08:15:58 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.6654761904761904 on epoch=224
06/18/2022 08:16:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=227
06/18/2022 08:16:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.11 on epoch=229
06/18/2022 08:16:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=232
06/18/2022 08:16:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=234
06/18/2022 08:16:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=237
06/18/2022 08:16:11 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.6735953801810579 on epoch=237
06/18/2022 08:16:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6656204906204907 -> 0.6735953801810579 on epoch=237, global_step=950
06/18/2022 08:16:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=239
06/18/2022 08:16:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.16 on epoch=242
06/18/2022 08:16:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=244
06/18/2022 08:16:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=247
06/18/2022 08:16:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.13 on epoch=249
06/18/2022 08:16:24 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.6548566017316018 on epoch=249
06/18/2022 08:16:27 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=252
06/18/2022 08:16:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.17 on epoch=254
06/18/2022 08:16:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=257
06/18/2022 08:16:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=259
06/18/2022 08:16:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=262
06/18/2022 08:16:38 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.6596042471042471 on epoch=262
06/18/2022 08:16:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=264
06/18/2022 08:16:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=267
06/18/2022 08:16:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=269
06/18/2022 08:16:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=272
06/18/2022 08:16:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=274
06/18/2022 08:16:51 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.6548566017316018 on epoch=274
06/18/2022 08:16:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.13 on epoch=277
06/18/2022 08:16:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=279
06/18/2022 08:16:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=282
06/18/2022 08:17:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=284
06/18/2022 08:17:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=287
06/18/2022 08:17:05 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.6403392773659253 on epoch=287
06/18/2022 08:17:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
06/18/2022 08:17:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=292
06/18/2022 08:17:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=294
06/18/2022 08:17:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=297
06/18/2022 08:17:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=299
06/18/2022 08:17:18 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.6548566017316018 on epoch=299
06/18/2022 08:17:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=302
06/18/2022 08:17:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=304
06/18/2022 08:17:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.10 on epoch=307
06/18/2022 08:17:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.16 on epoch=309
06/18/2022 08:17:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=312
06/18/2022 08:17:32 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.6760409856162144 on epoch=312
06/18/2022 08:17:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6735953801810579 -> 0.6760409856162144 on epoch=312, global_step=1250
06/18/2022 08:17:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
06/18/2022 08:17:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=317
06/18/2022 08:17:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
06/18/2022 08:17:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.09 on epoch=322
06/18/2022 08:17:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
06/18/2022 08:17:45 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.6537366142629302 on epoch=324
06/18/2022 08:17:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=327
06/18/2022 08:17:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
06/18/2022 08:17:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=332
06/18/2022 08:17:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=334
06/18/2022 08:17:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=337
06/18/2022 08:17:58 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.6231601731601731 on epoch=337
06/18/2022 08:18:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=339
06/18/2022 08:18:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
06/18/2022 08:18:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=344
06/18/2022 08:18:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=347
06/18/2022 08:18:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
06/18/2022 08:18:12 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6593096658953437 on epoch=349
06/18/2022 08:18:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.10 on epoch=352
06/18/2022 08:18:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=354
06/18/2022 08:18:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=357
06/18/2022 08:18:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=359
06/18/2022 08:18:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=362
06/18/2022 08:18:25 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.675902124951038 on epoch=362
06/18/2022 08:18:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
06/18/2022 08:18:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
06/18/2022 08:18:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
06/18/2022 08:18:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.12 on epoch=372
06/18/2022 08:18:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
06/18/2022 08:18:39 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.6608907294391165 on epoch=374
06/18/2022 08:18:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
06/18/2022 08:18:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=379
06/18/2022 08:18:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=382
06/18/2022 08:18:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
06/18/2022 08:18:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=387
06/18/2022 08:18:52 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.6596846846846847 on epoch=387
06/18/2022 08:18:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=389
06/18/2022 08:18:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
06/18/2022 08:19:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
06/18/2022 08:19:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
06/18/2022 08:19:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
06/18/2022 08:19:06 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.6399633375163013 on epoch=399
06/18/2022 08:19:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.11 on epoch=402
06/18/2022 08:19:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
06/18/2022 08:19:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
06/18/2022 08:19:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
06/18/2022 08:19:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
06/18/2022 08:19:19 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.6591991341991341 on epoch=412
06/18/2022 08:19:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
06/18/2022 08:19:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
06/18/2022 08:19:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
06/18/2022 08:19:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=422
06/18/2022 08:19:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
06/18/2022 08:19:33 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6238343052582367 on epoch=424
06/18/2022 08:19:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
06/18/2022 08:19:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
06/18/2022 08:19:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
06/18/2022 08:19:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
06/18/2022 08:19:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=437
06/18/2022 08:19:47 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.6243911408957176 on epoch=437
06/18/2022 08:19:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
06/18/2022 08:19:52 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=442
06/18/2022 08:19:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=444
06/18/2022 08:19:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
06/18/2022 08:19:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
06/18/2022 08:20:00 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.6597222222222222 on epoch=449
06/18/2022 08:20:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=452
06/18/2022 08:20:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=454
06/18/2022 08:20:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
06/18/2022 08:20:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
06/18/2022 08:20:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
06/18/2022 08:20:14 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6385379945162553 on epoch=462
06/18/2022 08:20:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
06/18/2022 08:20:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
06/18/2022 08:20:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=469
06/18/2022 08:20:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
06/18/2022 08:20:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=474
06/18/2022 08:20:27 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.623847167325428 on epoch=474
06/18/2022 08:20:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
06/18/2022 08:20:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
06/18/2022 08:20:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/18/2022 08:20:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=484
06/18/2022 08:20:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
06/18/2022 08:20:41 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.6449134199134199 on epoch=487
06/18/2022 08:20:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
06/18/2022 08:20:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/18/2022 08:20:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
06/18/2022 08:20:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
06/18/2022 08:20:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
06/18/2022 08:20:54 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6449134199134199 on epoch=499
06/18/2022 08:20:57 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
06/18/2022 08:20:59 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
06/18/2022 08:21:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
06/18/2022 08:21:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/18/2022 08:21:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/18/2022 08:21:08 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.623847167325428 on epoch=512
06/18/2022 08:21:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/18/2022 08:21:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
06/18/2022 08:21:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
06/18/2022 08:21:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
06/18/2022 08:21:21 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/18/2022 08:21:22 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6597222222222222 on epoch=524
06/18/2022 08:21:24 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
06/18/2022 08:21:27 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/18/2022 08:21:29 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=532
06/18/2022 08:21:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=534
06/18/2022 08:21:34 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/18/2022 08:21:35 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6603764478764479 on epoch=537
06/18/2022 08:21:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
06/18/2022 08:21:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=542
06/18/2022 08:21:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=544
06/18/2022 08:21:45 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
06/18/2022 08:21:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
06/18/2022 08:21:49 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6446078431372549 on epoch=549
06/18/2022 08:21:51 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
06/18/2022 08:21:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
06/18/2022 08:21:56 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
06/18/2022 08:21:59 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.16 on epoch=559
06/18/2022 08:22:01 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
06/18/2022 08:22:02 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.6597222222222222 on epoch=562
06/18/2022 08:22:05 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
06/18/2022 08:22:07 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
06/18/2022 08:22:10 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
06/18/2022 08:22:12 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/18/2022 08:22:15 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
06/18/2022 08:22:16 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6747623291740938 on epoch=574
06/18/2022 08:22:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=577
06/18/2022 08:22:21 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/18/2022 08:22:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
06/18/2022 08:22:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
06/18/2022 08:22:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/18/2022 08:22:29 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.6597222222222222 on epoch=587
06/18/2022 08:22:32 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=589
06/18/2022 08:22:34 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/18/2022 08:22:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.10 on epoch=594
06/18/2022 08:22:39 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
06/18/2022 08:22:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/18/2022 08:22:43 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.6446078431372549 on epoch=599
06/18/2022 08:22:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/18/2022 08:22:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
06/18/2022 08:22:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/18/2022 08:22:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
06/18/2022 08:22:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
06/18/2022 08:22:56 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6597222222222222 on epoch=612
06/18/2022 08:22:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/18/2022 08:23:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=617
06/18/2022 08:23:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
06/18/2022 08:23:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
06/18/2022 08:23:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
06/18/2022 08:23:10 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.6449134199134199 on epoch=624
06/18/2022 08:23:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
06/18/2022 08:23:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
06/18/2022 08:23:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
06/18/2022 08:23:20 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
06/18/2022 08:23:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
06/18/2022 08:23:23 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6385379945162553 on epoch=637
06/18/2022 08:23:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/18/2022 08:23:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
06/18/2022 08:23:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=644
06/18/2022 08:23:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/18/2022 08:23:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/18/2022 08:23:37 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6385379945162553 on epoch=649
06/18/2022 08:23:39 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
06/18/2022 08:23:41 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/18/2022 08:23:44 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/18/2022 08:23:46 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 08:23:49 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
06/18/2022 08:23:50 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6597222222222222 on epoch=662
06/18/2022 08:23:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
06/18/2022 08:23:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/18/2022 08:23:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
06/18/2022 08:24:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
06/18/2022 08:24:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/18/2022 08:24:04 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6597222222222222 on epoch=674
06/18/2022 08:24:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/18/2022 08:24:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/18/2022 08:24:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
06/18/2022 08:24:14 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/18/2022 08:24:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
06/18/2022 08:24:17 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.623847167325428 on epoch=687
06/18/2022 08:24:20 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
06/18/2022 08:24:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
06/18/2022 08:24:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/18/2022 08:24:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
06/18/2022 08:24:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/18/2022 08:24:31 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.623847167325428 on epoch=699
06/18/2022 08:24:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
06/18/2022 08:24:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/18/2022 08:24:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=707
06/18/2022 08:24:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 08:24:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
06/18/2022 08:24:44 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6449134199134199 on epoch=712
06/18/2022 08:24:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
06/18/2022 08:24:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=717
06/18/2022 08:24:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/18/2022 08:24:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/18/2022 08:24:57 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/18/2022 08:24:58 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.6449134199134199 on epoch=724
06/18/2022 08:25:00 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/18/2022 08:25:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
06/18/2022 08:25:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
06/18/2022 08:25:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=734
06/18/2022 08:25:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
06/18/2022 08:25:11 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.6601731601731602 on epoch=737
06/18/2022 08:25:14 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 08:25:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
06/18/2022 08:25:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=744
06/18/2022 08:25:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
06/18/2022 08:25:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=749
06/18/2022 08:25:25 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.659706491292169 on epoch=749
06/18/2022 08:25:25 - INFO - __main__ - save last model!
06/18/2022 08:25:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 08:25:25 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 08:25:25 - INFO - __main__ - Printing 3 examples
06/18/2022 08:25:25 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 08:25:25 - INFO - __main__ - ['others']
06/18/2022 08:25:25 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 08:25:25 - INFO - __main__ - ['others']
06/18/2022 08:25:25 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 08:25:25 - INFO - __main__ - ['others']
06/18/2022 08:25:25 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 08:25:25 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:25:25 - INFO - __main__ - Printing 3 examples
06/18/2022 08:25:25 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/18/2022 08:25:25 - INFO - __main__ - ['happy']
06/18/2022 08:25:25 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/18/2022 08:25:25 - INFO - __main__ - ['happy']
06/18/2022 08:25:25 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/18/2022 08:25:25 - INFO - __main__ - ['happy']
06/18/2022 08:25:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 08:25:25 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:25:25 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 08:25:25 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:25:25 - INFO - __main__ - Printing 3 examples
06/18/2022 08:25:25 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/18/2022 08:25:25 - INFO - __main__ - ['happy']
06/18/2022 08:25:25 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/18/2022 08:25:25 - INFO - __main__ - ['happy']
06/18/2022 08:25:25 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/18/2022 08:25:25 - INFO - __main__ - ['happy']
06/18/2022 08:25:25 - INFO - __main__ - Tokenizing Input ...
06/18/2022 08:25:25 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:25:25 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 08:25:27 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:25:32 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 08:25:44 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 08:25:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 08:25:45 - INFO - __main__ - Starting training!
06/18/2022 08:27:08 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_21_0.2_8_predictions.txt
06/18/2022 08:27:09 - INFO - __main__ - Classification-F1 on test data: 0.1701
06/18/2022 08:27:09 - INFO - __main__ - prefix=emo_16_21, lr=0.2, bsz=8, dev_performance=0.6760409856162144, test_performance=0.1701023865515359
06/18/2022 08:27:09 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.5, bsz=8 ...
06/18/2022 08:27:10 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:27:10 - INFO - __main__ - Printing 3 examples
06/18/2022 08:27:10 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/18/2022 08:27:10 - INFO - __main__ - ['happy']
06/18/2022 08:27:10 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/18/2022 08:27:10 - INFO - __main__ - ['happy']
06/18/2022 08:27:10 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/18/2022 08:27:10 - INFO - __main__ - ['happy']
06/18/2022 08:27:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 08:27:10 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:27:10 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 08:27:10 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:27:10 - INFO - __main__ - Printing 3 examples
06/18/2022 08:27:10 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/18/2022 08:27:10 - INFO - __main__ - ['happy']
06/18/2022 08:27:10 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/18/2022 08:27:10 - INFO - __main__ - ['happy']
06/18/2022 08:27:10 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/18/2022 08:27:10 - INFO - __main__ - ['happy']
06/18/2022 08:27:10 - INFO - __main__ - Tokenizing Input ...
06/18/2022 08:27:10 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:27:10 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 08:27:29 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 08:27:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 08:27:29 - INFO - __main__ - Starting training!
06/18/2022 08:27:32 - INFO - __main__ - Step 10 Global step 10 Train loss 3.87 on epoch=2
06/18/2022 08:27:35 - INFO - __main__ - Step 20 Global step 20 Train loss 2.65 on epoch=4
06/18/2022 08:27:37 - INFO - __main__ - Step 30 Global step 30 Train loss 1.86 on epoch=7
06/18/2022 08:27:40 - INFO - __main__ - Step 40 Global step 40 Train loss 1.43 on epoch=9
06/18/2022 08:27:42 - INFO - __main__ - Step 50 Global step 50 Train loss 0.97 on epoch=12
06/18/2022 08:27:43 - INFO - __main__ - Global step 50 Train loss 2.16 Classification-F1 0.34847689075630256 on epoch=12
06/18/2022 08:27:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.34847689075630256 on epoch=12, global_step=50
06/18/2022 08:27:46 - INFO - __main__ - Step 60 Global step 60 Train loss 0.78 on epoch=14
06/18/2022 08:27:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.63 on epoch=17
06/18/2022 08:27:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.74 on epoch=19
06/18/2022 08:27:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.64 on epoch=22
06/18/2022 08:27:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=24
06/18/2022 08:27:57 - INFO - __main__ - Global step 100 Train loss 0.67 Classification-F1 0.7115015360983103 on epoch=24
06/18/2022 08:27:57 - INFO - __main__ - Saving model with best Classification-F1: 0.34847689075630256 -> 0.7115015360983103 on epoch=24, global_step=100
06/18/2022 08:28:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.57 on epoch=27
06/18/2022 08:28:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=29
06/18/2022 08:28:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.39 on epoch=32
06/18/2022 08:28:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=34
06/18/2022 08:28:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=37
06/18/2022 08:28:11 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.7191484985602632 on epoch=37
06/18/2022 08:28:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7115015360983103 -> 0.7191484985602632 on epoch=37, global_step=150
06/18/2022 08:28:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=39
06/18/2022 08:28:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=42
06/18/2022 08:28:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.34 on epoch=44
06/18/2022 08:28:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=47
06/18/2022 08:28:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=49
06/18/2022 08:28:24 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.7902462121212122 on epoch=49
06/18/2022 08:28:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7191484985602632 -> 0.7902462121212122 on epoch=49, global_step=200
06/18/2022 08:28:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.35 on epoch=52
06/18/2022 08:28:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.30 on epoch=54
06/18/2022 08:28:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.30 on epoch=57
06/18/2022 08:28:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=59
06/18/2022 08:28:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=62
06/18/2022 08:28:38 - INFO - __main__ - Global step 250 Train loss 0.31 Classification-F1 0.7857349327937564 on epoch=62
06/18/2022 08:28:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.21 on epoch=64
06/18/2022 08:28:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=67
06/18/2022 08:28:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=69
06/18/2022 08:28:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=72
06/18/2022 08:28:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=74
06/18/2022 08:28:51 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.7519213877909531 on epoch=74
06/18/2022 08:28:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=77
06/18/2022 08:28:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.21 on epoch=79
06/18/2022 08:28:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=82
06/18/2022 08:29:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.15 on epoch=84
06/18/2022 08:29:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.17 on epoch=87
06/18/2022 08:29:05 - INFO - __main__ - Global step 350 Train loss 0.21 Classification-F1 0.7620429139825692 on epoch=87
06/18/2022 08:29:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.17 on epoch=89
06/18/2022 08:29:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=92
06/18/2022 08:29:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.17 on epoch=94
06/18/2022 08:29:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.13 on epoch=97
06/18/2022 08:29:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.14 on epoch=99
06/18/2022 08:29:19 - INFO - __main__ - Global step 400 Train loss 0.15 Classification-F1 0.777917358165837 on epoch=99
06/18/2022 08:29:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=102
06/18/2022 08:29:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.09 on epoch=104
06/18/2022 08:29:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=107
06/18/2022 08:29:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=109
06/18/2022 08:29:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.12 on epoch=112
06/18/2022 08:29:32 - INFO - __main__ - Global step 450 Train loss 0.11 Classification-F1 0.7567873303167421 on epoch=112
06/18/2022 08:29:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=114
06/18/2022 08:29:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.11 on epoch=117
06/18/2022 08:29:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.10 on epoch=119
06/18/2022 08:29:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=122
06/18/2022 08:29:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.04 on epoch=124
06/18/2022 08:29:46 - INFO - __main__ - Global step 500 Train loss 0.07 Classification-F1 0.7620895865677649 on epoch=124
06/18/2022 08:29:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=127
06/18/2022 08:29:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=129
06/18/2022 08:29:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.05 on epoch=132
06/18/2022 08:29:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=134
06/18/2022 08:29:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=137
06/18/2022 08:29:59 - INFO - __main__ - Global step 550 Train loss 0.09 Classification-F1 0.7726670520788168 on epoch=137
06/18/2022 08:30:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=139
06/18/2022 08:30:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=142
06/18/2022 08:30:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.02 on epoch=144
06/18/2022 08:30:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=147
06/18/2022 08:30:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.06 on epoch=149
06/18/2022 08:30:13 - INFO - __main__ - Global step 600 Train loss 0.04 Classification-F1 0.7758467023172906 on epoch=149
06/18/2022 08:30:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.08 on epoch=152
06/18/2022 08:30:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=154
06/18/2022 08:30:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.06 on epoch=157
06/18/2022 08:30:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=159
06/18/2022 08:30:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=162
06/18/2022 08:30:26 - INFO - __main__ - Global step 650 Train loss 0.06 Classification-F1 0.7307908809456797 on epoch=162
06/18/2022 08:30:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=164
06/18/2022 08:30:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=167
06/18/2022 08:30:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=169
06/18/2022 08:30:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=172
06/18/2022 08:30:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=174
06/18/2022 08:30:39 - INFO - __main__ - Global step 700 Train loss 0.04 Classification-F1 0.7757575757575758 on epoch=174
06/18/2022 08:30:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=177
06/18/2022 08:30:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=179
06/18/2022 08:30:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=182
06/18/2022 08:30:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=184
06/18/2022 08:30:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=187
06/18/2022 08:30:53 - INFO - __main__ - Global step 750 Train loss 0.04 Classification-F1 0.7567873303167421 on epoch=187
06/18/2022 08:30:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=189
06/18/2022 08:30:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=192
06/18/2022 08:31:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=194
06/18/2022 08:31:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=197
06/18/2022 08:31:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=199
06/18/2022 08:31:06 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.7942911255411256 on epoch=199
06/18/2022 08:31:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7902462121212122 -> 0.7942911255411256 on epoch=199, global_step=800
06/18/2022 08:31:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=202
06/18/2022 08:31:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=204
06/18/2022 08:31:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=207
06/18/2022 08:31:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=209
06/18/2022 08:31:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=212
06/18/2022 08:31:20 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.7758467023172906 on epoch=212
06/18/2022 08:31:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=214
06/18/2022 08:31:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=217
06/18/2022 08:31:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=219
06/18/2022 08:31:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=222
06/18/2022 08:31:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=224
06/18/2022 08:31:34 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.7768308080808081 on epoch=224
06/18/2022 08:31:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=227
06/18/2022 08:31:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=229
06/18/2022 08:31:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
06/18/2022 08:31:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=234
06/18/2022 08:31:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=237
06/18/2022 08:31:47 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.7470196489593041 on epoch=237
06/18/2022 08:31:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=239
06/18/2022 08:31:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
06/18/2022 08:31:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
06/18/2022 08:31:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=247
06/18/2022 08:32:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
06/18/2022 08:32:01 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.7786637931034482 on epoch=249
06/18/2022 08:32:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=252
06/18/2022 08:32:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=254
06/18/2022 08:32:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=257
06/18/2022 08:32:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
06/18/2022 08:32:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=262
06/18/2022 08:32:14 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.7571438365556012 on epoch=262
06/18/2022 08:32:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=264
06/18/2022 08:32:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=267
06/18/2022 08:32:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=269
06/18/2022 08:32:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
06/18/2022 08:32:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
06/18/2022 08:32:28 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.7724480095068331 on epoch=274
06/18/2022 08:32:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
06/18/2022 08:32:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
06/18/2022 08:32:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
06/18/2022 08:32:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=284
06/18/2022 08:32:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
06/18/2022 08:32:41 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.7573815073815074 on epoch=287
06/18/2022 08:32:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
06/18/2022 08:32:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
06/18/2022 08:32:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=294
06/18/2022 08:32:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
06/18/2022 08:32:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=299
06/18/2022 08:32:55 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.737296494355318 on epoch=299
06/18/2022 08:32:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
06/18/2022 08:33:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
06/18/2022 08:33:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
06/18/2022 08:33:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
06/18/2022 08:33:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
06/18/2022 08:33:09 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.7573815073815074 on epoch=312
06/18/2022 08:33:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
06/18/2022 08:33:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=317
06/18/2022 08:33:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=319
06/18/2022 08:33:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
06/18/2022 08:33:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=324
06/18/2022 08:33:22 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.7768308080808081 on epoch=324
06/18/2022 08:33:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
06/18/2022 08:33:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
06/18/2022 08:33:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=332
06/18/2022 08:33:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
06/18/2022 08:33:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
06/18/2022 08:33:36 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.7942911255411256 on epoch=337
06/18/2022 08:33:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=339
06/18/2022 08:33:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=342
06/18/2022 08:33:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=344
06/18/2022 08:33:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
06/18/2022 08:33:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=349
06/18/2022 08:33:50 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.7768308080808081 on epoch=349
06/18/2022 08:33:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
06/18/2022 08:33:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
06/18/2022 08:33:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
06/18/2022 08:34:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=359
06/18/2022 08:34:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
06/18/2022 08:34:03 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.7942911255411256 on epoch=362
06/18/2022 08:34:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
06/18/2022 08:34:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
06/18/2022 08:34:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
06/18/2022 08:34:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
06/18/2022 08:34:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=374
06/18/2022 08:34:16 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.7582170688788336 on epoch=374
06/18/2022 08:34:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
06/18/2022 08:34:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
06/18/2022 08:34:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=382
06/18/2022 08:34:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
06/18/2022 08:34:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
06/18/2022 08:34:30 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.7768308080808081 on epoch=387
06/18/2022 08:34:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
06/18/2022 08:34:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=392
06/18/2022 08:34:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
06/18/2022 08:34:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=397
06/18/2022 08:34:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/18/2022 08:34:43 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.7768308080808081 on epoch=399
06/18/2022 08:34:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
06/18/2022 08:34:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
06/18/2022 08:34:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
06/18/2022 08:34:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
06/18/2022 08:34:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
06/18/2022 08:34:57 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7614087301587301 on epoch=412
06/18/2022 08:35:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
06/18/2022 08:35:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
06/18/2022 08:35:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
06/18/2022 08:35:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
06/18/2022 08:35:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
06/18/2022 08:35:11 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.75875504000504 on epoch=424
06/18/2022 08:35:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
06/18/2022 08:35:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/18/2022 08:35:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
06/18/2022 08:35:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
06/18/2022 08:35:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
06/18/2022 08:35:24 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.7614087301587301 on epoch=437
06/18/2022 08:35:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
06/18/2022 08:35:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
06/18/2022 08:35:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
06/18/2022 08:35:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
06/18/2022 08:35:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
06/18/2022 08:35:38 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.7614087301587301 on epoch=449
06/18/2022 08:35:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
06/18/2022 08:35:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
06/18/2022 08:35:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
06/18/2022 08:35:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
06/18/2022 08:35:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
06/18/2022 08:35:51 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.7614087301587301 on epoch=462
06/18/2022 08:35:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
06/18/2022 08:35:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
06/18/2022 08:35:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/18/2022 08:36:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
06/18/2022 08:36:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
06/18/2022 08:36:05 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7765931372549019 on epoch=474
06/18/2022 08:36:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
06/18/2022 08:36:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
06/18/2022 08:36:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
06/18/2022 08:36:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/18/2022 08:36:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
06/18/2022 08:36:19 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7768308080808081 on epoch=487
06/18/2022 08:36:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
06/18/2022 08:36:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
06/18/2022 08:36:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
06/18/2022 08:36:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/18/2022 08:36:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
06/18/2022 08:36:33 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7460728297421845 on epoch=499
06/18/2022 08:36:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
06/18/2022 08:36:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
06/18/2022 08:36:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
06/18/2022 08:36:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/18/2022 08:36:46 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/18/2022 08:36:47 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7954656862745099 on epoch=512
06/18/2022 08:36:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7942911255411256 -> 0.7954656862745099 on epoch=512, global_step=2050
06/18/2022 08:36:49 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=514
06/18/2022 08:36:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/18/2022 08:36:54 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
06/18/2022 08:36:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
06/18/2022 08:36:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
06/18/2022 08:37:01 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7460728297421845 on epoch=524
06/18/2022 08:37:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=527
06/18/2022 08:37:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
06/18/2022 08:37:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/18/2022 08:37:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
06/18/2022 08:37:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/18/2022 08:37:14 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7573815073815074 on epoch=537
06/18/2022 08:37:17 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/18/2022 08:37:19 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/18/2022 08:37:22 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/18/2022 08:37:24 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/18/2022 08:37:27 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/18/2022 08:37:28 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.7954656862745099 on epoch=549
06/18/2022 08:37:31 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/18/2022 08:37:33 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/18/2022 08:37:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
06/18/2022 08:37:38 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
06/18/2022 08:37:41 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
06/18/2022 08:37:42 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.8112572223246666 on epoch=562
06/18/2022 08:37:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7954656862745099 -> 0.8112572223246666 on epoch=562, global_step=2250
06/18/2022 08:37:44 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
06/18/2022 08:37:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
06/18/2022 08:37:49 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=569
06/18/2022 08:37:52 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/18/2022 08:37:54 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/18/2022 08:37:55 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.8112572223246666 on epoch=574
06/18/2022 08:37:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/18/2022 08:38:01 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/18/2022 08:38:03 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/18/2022 08:38:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
06/18/2022 08:38:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
06/18/2022 08:38:09 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.8112572223246666 on epoch=587
06/18/2022 08:38:12 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/18/2022 08:38:14 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=592
06/18/2022 08:38:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/18/2022 08:38:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
06/18/2022 08:38:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/18/2022 08:38:23 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7942911255411256 on epoch=599
06/18/2022 08:38:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/18/2022 08:38:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/18/2022 08:38:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/18/2022 08:38:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/18/2022 08:38:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/18/2022 08:38:36 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.7768308080808081 on epoch=612
06/18/2022 08:38:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/18/2022 08:38:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/18/2022 08:38:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/18/2022 08:38:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/18/2022 08:38:49 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/18/2022 08:38:50 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.7768308080808081 on epoch=624
06/18/2022 08:38:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/18/2022 08:38:55 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/18/2022 08:38:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/18/2022 08:39:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
06/18/2022 08:39:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/18/2022 08:39:04 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.7957905669599218 on epoch=637
06/18/2022 08:39:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/18/2022 08:39:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/18/2022 08:39:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=644
06/18/2022 08:39:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/18/2022 08:39:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/18/2022 08:39:18 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.8112572223246666 on epoch=649
06/18/2022 08:39:20 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/18/2022 08:39:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/18/2022 08:39:25 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/18/2022 08:39:28 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
06/18/2022 08:39:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
06/18/2022 08:39:31 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7621058558558558 on epoch=662
06/18/2022 08:39:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/18/2022 08:39:36 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/18/2022 08:39:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/18/2022 08:39:41 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
06/18/2022 08:39:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
06/18/2022 08:39:45 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.779265873015873 on epoch=674
06/18/2022 08:39:48 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/18/2022 08:39:50 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/18/2022 08:39:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=682
06/18/2022 08:39:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/18/2022 08:39:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/18/2022 08:39:59 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7942911255411256 on epoch=687
06/18/2022 08:40:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 08:40:04 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/18/2022 08:40:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/18/2022 08:40:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/18/2022 08:40:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/18/2022 08:40:13 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7768308080808081 on epoch=699
06/18/2022 08:40:15 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/18/2022 08:40:18 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/18/2022 08:40:20 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/18/2022 08:40:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 08:40:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=712
06/18/2022 08:40:26 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.8112572223246666 on epoch=712
06/18/2022 08:40:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/18/2022 08:40:31 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/18/2022 08:40:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/18/2022 08:40:36 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
06/18/2022 08:40:39 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/18/2022 08:40:40 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7768308080808081 on epoch=724
06/18/2022 08:40:43 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/18/2022 08:40:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
06/18/2022 08:40:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/18/2022 08:40:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 08:40:53 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/18/2022 08:40:54 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7954656862745099 on epoch=737
06/18/2022 08:40:56 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 08:40:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/18/2022 08:41:01 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/18/2022 08:41:04 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/18/2022 08:41:06 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/18/2022 08:41:07 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7954656862745099 on epoch=749
06/18/2022 08:41:07 - INFO - __main__ - save last model!
06/18/2022 08:41:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 08:41:08 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 08:41:08 - INFO - __main__ - Printing 3 examples
06/18/2022 08:41:08 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 08:41:08 - INFO - __main__ - ['others']
06/18/2022 08:41:08 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 08:41:08 - INFO - __main__ - ['others']
06/18/2022 08:41:08 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 08:41:08 - INFO - __main__ - ['others']
06/18/2022 08:41:08 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 08:41:08 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:41:08 - INFO - __main__ - Printing 3 examples
06/18/2022 08:41:08 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/18/2022 08:41:08 - INFO - __main__ - ['happy']
06/18/2022 08:41:08 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/18/2022 08:41:08 - INFO - __main__ - ['happy']
06/18/2022 08:41:08 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/18/2022 08:41:08 - INFO - __main__ - ['happy']
06/18/2022 08:41:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 08:41:08 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:41:08 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 08:41:08 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:41:08 - INFO - __main__ - Printing 3 examples
06/18/2022 08:41:08 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/18/2022 08:41:08 - INFO - __main__ - ['happy']
06/18/2022 08:41:08 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/18/2022 08:41:08 - INFO - __main__ - ['happy']
06/18/2022 08:41:08 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/18/2022 08:41:08 - INFO - __main__ - ['happy']
06/18/2022 08:41:08 - INFO - __main__ - Tokenizing Input ...
06/18/2022 08:41:08 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:41:08 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 08:41:10 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:41:15 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 08:41:23 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 08:41:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 08:41:24 - INFO - __main__ - Starting training!
06/18/2022 08:42:52 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_42_0.5_8_predictions.txt
06/18/2022 08:42:52 - INFO - __main__ - Classification-F1 on test data: 0.2138
06/18/2022 08:42:52 - INFO - __main__ - prefix=emo_16_42, lr=0.5, bsz=8, dev_performance=0.8112572223246666, test_performance=0.21383150813465687
06/18/2022 08:42:52 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.4, bsz=8 ...
06/18/2022 08:42:53 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:42:53 - INFO - __main__ - Printing 3 examples
06/18/2022 08:42:53 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/18/2022 08:42:53 - INFO - __main__ - ['happy']
06/18/2022 08:42:53 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/18/2022 08:42:53 - INFO - __main__ - ['happy']
06/18/2022 08:42:53 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/18/2022 08:42:53 - INFO - __main__ - ['happy']
06/18/2022 08:42:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 08:42:53 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:42:53 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 08:42:53 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:42:53 - INFO - __main__ - Printing 3 examples
06/18/2022 08:42:53 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/18/2022 08:42:53 - INFO - __main__ - ['happy']
06/18/2022 08:42:53 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/18/2022 08:42:53 - INFO - __main__ - ['happy']
06/18/2022 08:42:53 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/18/2022 08:42:53 - INFO - __main__ - ['happy']
06/18/2022 08:42:53 - INFO - __main__ - Tokenizing Input ...
06/18/2022 08:42:53 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:42:53 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 08:43:12 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 08:43:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 08:43:13 - INFO - __main__ - Starting training!
06/18/2022 08:43:16 - INFO - __main__ - Step 10 Global step 10 Train loss 3.87 on epoch=2
06/18/2022 08:43:18 - INFO - __main__ - Step 20 Global step 20 Train loss 2.92 on epoch=4
06/18/2022 08:43:21 - INFO - __main__ - Step 30 Global step 30 Train loss 2.14 on epoch=7
06/18/2022 08:43:23 - INFO - __main__ - Step 40 Global step 40 Train loss 1.66 on epoch=9
06/18/2022 08:43:26 - INFO - __main__ - Step 50 Global step 50 Train loss 1.22 on epoch=12
06/18/2022 08:43:27 - INFO - __main__ - Global step 50 Train loss 2.36 Classification-F1 0.2742094253722161 on epoch=12
06/18/2022 08:43:27 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2742094253722161 on epoch=12, global_step=50
06/18/2022 08:43:29 - INFO - __main__ - Step 60 Global step 60 Train loss 1.17 on epoch=14
06/18/2022 08:43:32 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=17
06/18/2022 08:43:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.83 on epoch=19
06/18/2022 08:43:37 - INFO - __main__ - Step 90 Global step 90 Train loss 0.68 on epoch=22
06/18/2022 08:43:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.67 on epoch=24
06/18/2022 08:43:40 - INFO - __main__ - Global step 100 Train loss 0.86 Classification-F1 0.40947324034310945 on epoch=24
06/18/2022 08:43:40 - INFO - __main__ - Saving model with best Classification-F1: 0.2742094253722161 -> 0.40947324034310945 on epoch=24, global_step=100
06/18/2022 08:43:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.62 on epoch=27
06/18/2022 08:43:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=29
06/18/2022 08:43:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=32
06/18/2022 08:43:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.71 on epoch=34
06/18/2022 08:43:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=37
06/18/2022 08:43:54 - INFO - __main__ - Global step 150 Train loss 0.58 Classification-F1 0.6162229583282215 on epoch=37
06/18/2022 08:43:54 - INFO - __main__ - Saving model with best Classification-F1: 0.40947324034310945 -> 0.6162229583282215 on epoch=37, global_step=150
06/18/2022 08:43:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=39
06/18/2022 08:43:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=42
06/18/2022 08:44:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.55 on epoch=44
06/18/2022 08:44:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=47
06/18/2022 08:44:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=49
06/18/2022 08:44:07 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.7369510366767835 on epoch=49
06/18/2022 08:44:07 - INFO - __main__ - Saving model with best Classification-F1: 0.6162229583282215 -> 0.7369510366767835 on epoch=49, global_step=200
06/18/2022 08:44:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=52
06/18/2022 08:44:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=54
06/18/2022 08:44:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=57
06/18/2022 08:44:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=59
06/18/2022 08:44:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=62
06/18/2022 08:44:21 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.7314855875831485 on epoch=62
06/18/2022 08:44:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.33 on epoch=64
06/18/2022 08:44:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.23 on epoch=67
06/18/2022 08:44:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=69
06/18/2022 08:44:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=72
06/18/2022 08:44:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=74
06/18/2022 08:44:34 - INFO - __main__ - Global step 300 Train loss 0.28 Classification-F1 0.7573684210526316 on epoch=74
06/18/2022 08:44:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7369510366767835 -> 0.7573684210526316 on epoch=74, global_step=300
06/18/2022 08:44:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=77
06/18/2022 08:44:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=79
06/18/2022 08:44:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=82
06/18/2022 08:44:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=84
06/18/2022 08:44:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=87
06/18/2022 08:44:48 - INFO - __main__ - Global step 350 Train loss 0.28 Classification-F1 0.7717948717948717 on epoch=87
06/18/2022 08:44:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7573684210526316 -> 0.7717948717948717 on epoch=87, global_step=350
06/18/2022 08:44:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=89
06/18/2022 08:44:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.19 on epoch=92
06/18/2022 08:44:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.18 on epoch=94
06/18/2022 08:44:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=97
06/18/2022 08:45:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=99
06/18/2022 08:45:01 - INFO - __main__ - Global step 400 Train loss 0.20 Classification-F1 0.7521739130434782 on epoch=99
06/18/2022 08:45:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.19 on epoch=102
06/18/2022 08:45:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=104
06/18/2022 08:45:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.20 on epoch=107
06/18/2022 08:45:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.13 on epoch=109
06/18/2022 08:45:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.17 on epoch=112
06/18/2022 08:45:15 - INFO - __main__ - Global step 450 Train loss 0.18 Classification-F1 0.7857349327937564 on epoch=112
06/18/2022 08:45:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7717948717948717 -> 0.7857349327937564 on epoch=112, global_step=450
06/18/2022 08:45:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=114
06/18/2022 08:45:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.11 on epoch=117
06/18/2022 08:45:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=119
06/18/2022 08:45:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.12 on epoch=122
06/18/2022 08:45:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=124
06/18/2022 08:45:28 - INFO - __main__ - Global step 500 Train loss 0.12 Classification-F1 0.7720588235294118 on epoch=124
06/18/2022 08:45:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=127
06/18/2022 08:45:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.18 on epoch=129
06/18/2022 08:45:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.09 on epoch=132
06/18/2022 08:45:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.08 on epoch=134
06/18/2022 08:45:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.14 on epoch=137
06/18/2022 08:45:41 - INFO - __main__ - Global step 550 Train loss 0.11 Classification-F1 0.7720588235294118 on epoch=137
06/18/2022 08:45:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.09 on epoch=139
06/18/2022 08:45:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=142
06/18/2022 08:45:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=144
06/18/2022 08:45:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=147
06/18/2022 08:45:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=149
06/18/2022 08:45:55 - INFO - __main__ - Global step 600 Train loss 0.10 Classification-F1 0.7218996689584926 on epoch=149
06/18/2022 08:45:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.09 on epoch=152
06/18/2022 08:46:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.09 on epoch=154
06/18/2022 08:46:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=157
06/18/2022 08:46:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=159
06/18/2022 08:46:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=162
06/18/2022 08:46:08 - INFO - __main__ - Global step 650 Train loss 0.09 Classification-F1 0.7075519232083077 on epoch=162
06/18/2022 08:46:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=164
06/18/2022 08:46:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=167
06/18/2022 08:46:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=169
06/18/2022 08:46:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.08 on epoch=172
06/18/2022 08:46:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=174
06/18/2022 08:46:22 - INFO - __main__ - Global step 700 Train loss 0.05 Classification-F1 0.7853472222222222 on epoch=174
06/18/2022 08:46:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=177
06/18/2022 08:46:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=179
06/18/2022 08:46:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=182
06/18/2022 08:46:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=184
06/18/2022 08:46:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=187
06/18/2022 08:46:35 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.7373366013071896 on epoch=187
06/18/2022 08:46:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=189
06/18/2022 08:46:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=192
06/18/2022 08:46:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=194
06/18/2022 08:46:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=197
06/18/2022 08:46:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=199
06/18/2022 08:46:48 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.777917358165837 on epoch=199
06/18/2022 08:46:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=202
06/18/2022 08:46:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=204
06/18/2022 08:46:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=207
06/18/2022 08:46:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=209
06/18/2022 08:47:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=212
06/18/2022 08:47:02 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.76195436006337 on epoch=212
06/18/2022 08:47:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=214
06/18/2022 08:47:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=217
06/18/2022 08:47:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=219
06/18/2022 08:47:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=222
06/18/2022 08:47:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=224
06/18/2022 08:47:15 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.7319690113807761 on epoch=224
06/18/2022 08:47:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=227
06/18/2022 08:47:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=229
06/18/2022 08:47:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=232
06/18/2022 08:47:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=234
06/18/2022 08:47:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=237
06/18/2022 08:47:28 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.737296494355318 on epoch=237
06/18/2022 08:47:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=239
06/18/2022 08:47:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
06/18/2022 08:47:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=244
06/18/2022 08:47:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=247
06/18/2022 08:47:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=249
06/18/2022 08:47:42 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.7943802521008404 on epoch=249
06/18/2022 08:47:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7857349327937564 -> 0.7943802521008404 on epoch=249, global_step=1000
06/18/2022 08:47:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=252
06/18/2022 08:47:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=254
06/18/2022 08:47:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
06/18/2022 08:47:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
06/18/2022 08:47:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=262
06/18/2022 08:47:55 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.7639299725993274 on epoch=262
06/18/2022 08:47:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
06/18/2022 08:48:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=267
06/18/2022 08:48:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
06/18/2022 08:48:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
06/18/2022 08:48:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=274
06/18/2022 08:48:09 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.7940158430143218 on epoch=274
06/18/2022 08:48:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
06/18/2022 08:48:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
06/18/2022 08:48:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=282
06/18/2022 08:48:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
06/18/2022 08:48:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
06/18/2022 08:48:22 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.7757575757575758 on epoch=287
06/18/2022 08:48:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
06/18/2022 08:48:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
06/18/2022 08:48:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=294
06/18/2022 08:48:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
06/18/2022 08:48:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=299
06/18/2022 08:48:36 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.7942911255411256 on epoch=299
06/18/2022 08:48:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
06/18/2022 08:48:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=304
06/18/2022 08:48:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
06/18/2022 08:48:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=309
06/18/2022 08:48:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=312
06/18/2022 08:48:49 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.7940158430143218 on epoch=312
06/18/2022 08:48:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
06/18/2022 08:48:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
06/18/2022 08:48:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
06/18/2022 08:48:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
06/18/2022 08:49:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
06/18/2022 08:49:02 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.779265873015873 on epoch=324
06/18/2022 08:49:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
06/18/2022 08:49:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
06/18/2022 08:49:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
06/18/2022 08:49:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
06/18/2022 08:49:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
06/18/2022 08:49:16 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.7566982037570273 on epoch=337
06/18/2022 08:49:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
06/18/2022 08:49:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
06/18/2022 08:49:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
06/18/2022 08:49:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
06/18/2022 08:49:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
06/18/2022 08:49:29 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.7942911255411256 on epoch=349
06/18/2022 08:49:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=352
06/18/2022 08:49:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
06/18/2022 08:49:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
06/18/2022 08:49:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
06/18/2022 08:49:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
06/18/2022 08:49:43 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.7942911255411256 on epoch=362
06/18/2022 08:49:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
06/18/2022 08:49:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
06/18/2022 08:49:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
06/18/2022 08:49:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
06/18/2022 08:49:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
06/18/2022 08:49:56 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.7942911255411256 on epoch=374
06/18/2022 08:49:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=377
06/18/2022 08:50:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=379
06/18/2022 08:50:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/18/2022 08:50:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=384
06/18/2022 08:50:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
06/18/2022 08:50:09 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.7942911255411256 on epoch=387
06/18/2022 08:50:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
06/18/2022 08:50:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
06/18/2022 08:50:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
06/18/2022 08:50:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
06/18/2022 08:50:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/18/2022 08:50:23 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.779265873015873 on epoch=399
06/18/2022 08:50:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/18/2022 08:50:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
06/18/2022 08:50:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
06/18/2022 08:50:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
06/18/2022 08:50:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
06/18/2022 08:50:36 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7942911255411256 on epoch=412
06/18/2022 08:50:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
06/18/2022 08:50:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
06/18/2022 08:50:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/18/2022 08:50:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
06/18/2022 08:50:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
06/18/2022 08:50:49 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.779265873015873 on epoch=424
06/18/2022 08:50:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
06/18/2022 08:50:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=429
06/18/2022 08:50:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/18/2022 08:50:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
06/18/2022 08:51:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
06/18/2022 08:51:03 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.779265873015873 on epoch=437
06/18/2022 08:51:05 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
06/18/2022 08:51:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
06/18/2022 08:51:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
06/18/2022 08:51:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
06/18/2022 08:51:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
06/18/2022 08:51:16 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7639299725993274 on epoch=449
06/18/2022 08:51:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
06/18/2022 08:51:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
06/18/2022 08:51:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/18/2022 08:51:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
06/18/2022 08:51:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
06/18/2022 08:51:30 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7942911255411256 on epoch=462
06/18/2022 08:51:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=464
06/18/2022 08:51:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=467
06/18/2022 08:51:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
06/18/2022 08:51:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
06/18/2022 08:51:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
06/18/2022 08:51:43 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.7231812840508492 on epoch=474
06/18/2022 08:51:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
06/18/2022 08:51:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=479
06/18/2022 08:51:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/18/2022 08:51:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/18/2022 08:51:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
06/18/2022 08:51:57 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.7392951251646904 on epoch=487
06/18/2022 08:51:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=489
06/18/2022 08:52:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/18/2022 08:52:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=494
06/18/2022 08:52:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
06/18/2022 08:52:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
06/18/2022 08:52:10 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.779265873015873 on epoch=499
06/18/2022 08:52:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/18/2022 08:52:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=504
06/18/2022 08:52:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
06/18/2022 08:52:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
06/18/2022 08:52:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
06/18/2022 08:52:23 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7225715421303657 on epoch=512
06/18/2022 08:52:26 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/18/2022 08:52:28 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
06/18/2022 08:52:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
06/18/2022 08:52:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
06/18/2022 08:52:36 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
06/18/2022 08:52:37 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7757575757575758 on epoch=524
06/18/2022 08:52:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
06/18/2022 08:52:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
06/18/2022 08:52:44 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/18/2022 08:52:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
06/18/2022 08:52:49 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
06/18/2022 08:52:50 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7571438365556012 on epoch=537
06/18/2022 08:52:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
06/18/2022 08:52:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/18/2022 08:52:58 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/18/2022 08:53:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/18/2022 08:53:03 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
06/18/2022 08:53:04 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7566982037570273 on epoch=549
06/18/2022 08:53:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
06/18/2022 08:53:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
06/18/2022 08:53:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/18/2022 08:53:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
06/18/2022 08:53:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
06/18/2022 08:53:17 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7616792929292928 on epoch=562
06/18/2022 08:53:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
06/18/2022 08:53:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
06/18/2022 08:53:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/18/2022 08:53:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/18/2022 08:53:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/18/2022 08:53:31 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.7285280293026473 on epoch=574
06/18/2022 08:53:33 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/18/2022 08:53:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/18/2022 08:53:38 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
06/18/2022 08:53:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=584
06/18/2022 08:53:43 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
06/18/2022 08:53:44 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7443066801619433 on epoch=587
06/18/2022 08:53:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
06/18/2022 08:53:49 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/18/2022 08:53:52 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/18/2022 08:53:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=597
06/18/2022 08:53:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/18/2022 08:53:58 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7582170688788336 on epoch=599
06/18/2022 08:54:00 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
06/18/2022 08:54:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=604
06/18/2022 08:54:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/18/2022 08:54:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
06/18/2022 08:54:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/18/2022 08:54:11 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7765931372549019 on epoch=612
06/18/2022 08:54:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/18/2022 08:54:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
06/18/2022 08:54:19 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
06/18/2022 08:54:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/18/2022 08:54:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/18/2022 08:54:25 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7422299922299922 on epoch=624
06/18/2022 08:54:27 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/18/2022 08:54:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/18/2022 08:54:32 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/18/2022 08:54:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/18/2022 08:54:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
06/18/2022 08:54:38 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7765931372549019 on epoch=637
06/18/2022 08:54:41 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/18/2022 08:54:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/18/2022 08:54:46 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/18/2022 08:54:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/18/2022 08:54:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/18/2022 08:54:52 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7582170688788336 on epoch=649
06/18/2022 08:54:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=652
06/18/2022 08:54:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/18/2022 08:54:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=657
06/18/2022 08:55:02 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 08:55:04 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/18/2022 08:55:05 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.75875504000504 on epoch=662
06/18/2022 08:55:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/18/2022 08:55:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/18/2022 08:55:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/18/2022 08:55:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/18/2022 08:55:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/18/2022 08:55:19 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.7765931372549019 on epoch=674
06/18/2022 08:55:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/18/2022 08:55:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/18/2022 08:55:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/18/2022 08:55:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=684
06/18/2022 08:55:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
06/18/2022 08:55:33 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7765931372549019 on epoch=687
06/18/2022 08:55:35 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 08:55:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/18/2022 08:55:40 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
06/18/2022 08:55:43 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/18/2022 08:55:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/18/2022 08:55:46 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7718497189085425 on epoch=699
06/18/2022 08:55:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/18/2022 08:55:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
06/18/2022 08:55:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/18/2022 08:55:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/18/2022 08:55:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
06/18/2022 08:56:00 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7571438365556012 on epoch=712
06/18/2022 08:56:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=714
06/18/2022 08:56:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/18/2022 08:56:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/18/2022 08:56:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
06/18/2022 08:56:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/18/2022 08:56:14 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7517156862745098 on epoch=724
06/18/2022 08:56:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=727
06/18/2022 08:56:19 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/18/2022 08:56:21 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/18/2022 08:56:24 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 08:56:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/18/2022 08:56:27 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7757575757575758 on epoch=737
06/18/2022 08:56:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 08:56:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/18/2022 08:56:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/18/2022 08:56:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/18/2022 08:56:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
06/18/2022 08:56:41 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7287878787878788 on epoch=749
06/18/2022 08:56:41 - INFO - __main__ - save last model!
06/18/2022 08:56:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 08:56:41 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 08:56:41 - INFO - __main__ - Printing 3 examples
06/18/2022 08:56:41 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 08:56:41 - INFO - __main__ - ['others']
06/18/2022 08:56:41 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 08:56:41 - INFO - __main__ - ['others']
06/18/2022 08:56:41 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 08:56:41 - INFO - __main__ - ['others']
06/18/2022 08:56:41 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 08:56:41 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:56:41 - INFO - __main__ - Printing 3 examples
06/18/2022 08:56:41 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/18/2022 08:56:41 - INFO - __main__ - ['happy']
06/18/2022 08:56:41 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/18/2022 08:56:41 - INFO - __main__ - ['happy']
06/18/2022 08:56:41 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/18/2022 08:56:41 - INFO - __main__ - ['happy']
06/18/2022 08:56:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 08:56:41 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:56:41 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 08:56:41 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:56:41 - INFO - __main__ - Printing 3 examples
06/18/2022 08:56:41 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/18/2022 08:56:41 - INFO - __main__ - ['happy']
06/18/2022 08:56:41 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/18/2022 08:56:41 - INFO - __main__ - ['happy']
06/18/2022 08:56:41 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/18/2022 08:56:41 - INFO - __main__ - ['happy']
06/18/2022 08:56:41 - INFO - __main__ - Tokenizing Input ...
06/18/2022 08:56:41 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:56:41 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 08:56:43 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:56:49 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 08:56:57 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 08:56:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 08:56:58 - INFO - __main__ - Starting training!
06/18/2022 08:58:21 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_42_0.4_8_predictions.txt
06/18/2022 08:58:22 - INFO - __main__ - Classification-F1 on test data: 0.1447
06/18/2022 08:58:22 - INFO - __main__ - prefix=emo_16_42, lr=0.4, bsz=8, dev_performance=0.7943802521008404, test_performance=0.14469284191799037
06/18/2022 08:58:22 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.3, bsz=8 ...
06/18/2022 08:58:23 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:58:23 - INFO - __main__ - Printing 3 examples
06/18/2022 08:58:23 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/18/2022 08:58:23 - INFO - __main__ - ['happy']
06/18/2022 08:58:23 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/18/2022 08:58:23 - INFO - __main__ - ['happy']
06/18/2022 08:58:23 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/18/2022 08:58:23 - INFO - __main__ - ['happy']
06/18/2022 08:58:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 08:58:23 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:58:23 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 08:58:23 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 08:58:23 - INFO - __main__ - Printing 3 examples
06/18/2022 08:58:23 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/18/2022 08:58:23 - INFO - __main__ - ['happy']
06/18/2022 08:58:23 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/18/2022 08:58:23 - INFO - __main__ - ['happy']
06/18/2022 08:58:23 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/18/2022 08:58:23 - INFO - __main__ - ['happy']
06/18/2022 08:58:23 - INFO - __main__ - Tokenizing Input ...
06/18/2022 08:58:23 - INFO - __main__ - Tokenizing Output ...
06/18/2022 08:58:23 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 08:58:42 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 08:58:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 08:58:42 - INFO - __main__ - Starting training!
06/18/2022 08:58:45 - INFO - __main__ - Step 10 Global step 10 Train loss 4.17 on epoch=2
06/18/2022 08:58:48 - INFO - __main__ - Step 20 Global step 20 Train loss 3.18 on epoch=4
06/18/2022 08:58:50 - INFO - __main__ - Step 30 Global step 30 Train loss 2.55 on epoch=7
06/18/2022 08:58:53 - INFO - __main__ - Step 40 Global step 40 Train loss 2.07 on epoch=9
06/18/2022 08:58:55 - INFO - __main__ - Step 50 Global step 50 Train loss 1.79 on epoch=12
06/18/2022 08:58:57 - INFO - __main__ - Global step 50 Train loss 2.75 Classification-F1 0.08567012789901346 on epoch=12
06/18/2022 08:58:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.08567012789901346 on epoch=12, global_step=50
06/18/2022 08:58:59 - INFO - __main__ - Step 60 Global step 60 Train loss 1.50 on epoch=14
06/18/2022 08:59:02 - INFO - __main__ - Step 70 Global step 70 Train loss 1.15 on epoch=17
06/18/2022 08:59:04 - INFO - __main__ - Step 80 Global step 80 Train loss 0.96 on epoch=19
06/18/2022 08:59:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.85 on epoch=22
06/18/2022 08:59:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.90 on epoch=24
06/18/2022 08:59:10 - INFO - __main__ - Global step 100 Train loss 1.07 Classification-F1 0.356890973443641 on epoch=24
06/18/2022 08:59:10 - INFO - __main__ - Saving model with best Classification-F1: 0.08567012789901346 -> 0.356890973443641 on epoch=24, global_step=100
06/18/2022 08:59:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.68 on epoch=27
06/18/2022 08:59:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.61 on epoch=29
06/18/2022 08:59:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.67 on epoch=32
06/18/2022 08:59:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.68 on epoch=34
06/18/2022 08:59:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.57 on epoch=37
06/18/2022 08:59:24 - INFO - __main__ - Global step 150 Train loss 0.64 Classification-F1 0.5195265423242468 on epoch=37
06/18/2022 08:59:24 - INFO - __main__ - Saving model with best Classification-F1: 0.356890973443641 -> 0.5195265423242468 on epoch=37, global_step=150
06/18/2022 08:59:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=39
06/18/2022 08:59:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=42
06/18/2022 08:59:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=44
06/18/2022 08:59:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=47
06/18/2022 08:59:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=49
06/18/2022 08:59:37 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.7641733870967742 on epoch=49
06/18/2022 08:59:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5195265423242468 -> 0.7641733870967742 on epoch=49, global_step=200
06/18/2022 08:59:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=52
06/18/2022 08:59:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=54
06/18/2022 08:59:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.65 on epoch=57
06/18/2022 08:59:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=59
06/18/2022 08:59:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=62
06/18/2022 08:59:51 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.6820887445887446 on epoch=62
06/18/2022 08:59:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=64
06/18/2022 08:59:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=67
06/18/2022 08:59:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=69
06/18/2022 09:00:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=72
06/18/2022 09:00:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=74
06/18/2022 09:00:04 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.7634906597774245 on epoch=74
06/18/2022 09:00:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=77
06/18/2022 09:00:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=79
06/18/2022 09:00:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=82
06/18/2022 09:00:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=84
06/18/2022 09:00:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=87
06/18/2022 09:00:18 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.7668220926322351 on epoch=87
06/18/2022 09:00:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7641733870967742 -> 0.7668220926322351 on epoch=87, global_step=350
06/18/2022 09:00:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=89
06/18/2022 09:00:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=92
06/18/2022 09:00:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=94
06/18/2022 09:00:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=97
06/18/2022 09:00:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=99
06/18/2022 09:00:31 - INFO - __main__ - Global step 400 Train loss 0.28 Classification-F1 0.8057725279106858 on epoch=99
06/18/2022 09:00:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7668220926322351 -> 0.8057725279106858 on epoch=99, global_step=400
06/18/2022 09:00:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=102
06/18/2022 09:00:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=104
06/18/2022 09:00:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=107
06/18/2022 09:00:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.18 on epoch=109
06/18/2022 09:00:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=112
06/18/2022 09:00:45 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.8052235691573927 on epoch=112
06/18/2022 09:00:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=114
06/18/2022 09:00:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=117
06/18/2022 09:00:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.18 on epoch=119
06/18/2022 09:00:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=122
06/18/2022 09:00:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=124
06/18/2022 09:00:58 - INFO - __main__ - Global step 500 Train loss 0.21 Classification-F1 0.8052235691573927 on epoch=124
06/18/2022 09:01:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=127
06/18/2022 09:01:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.19 on epoch=129
06/18/2022 09:01:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=132
06/18/2022 09:01:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=134
06/18/2022 09:01:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=137
06/18/2022 09:01:11 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.7904176093514329 on epoch=137
06/18/2022 09:01:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=139
06/18/2022 09:01:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.11 on epoch=142
06/18/2022 09:01:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=144
06/18/2022 09:01:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=147
06/18/2022 09:01:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=149
06/18/2022 09:01:25 - INFO - __main__ - Global step 600 Train loss 0.13 Classification-F1 0.7901475279106858 on epoch=149
06/18/2022 09:01:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.16 on epoch=152
06/18/2022 09:01:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=154
06/18/2022 09:01:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=157
06/18/2022 09:01:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=159
06/18/2022 09:01:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=162
06/18/2022 09:01:38 - INFO - __main__ - Global step 650 Train loss 0.14 Classification-F1 0.7519354392755928 on epoch=162
06/18/2022 09:01:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=164
06/18/2022 09:01:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=167
06/18/2022 09:01:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=169
06/18/2022 09:01:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=172
06/18/2022 09:01:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=174
06/18/2022 09:01:52 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.7907978043461914 on epoch=174
06/18/2022 09:01:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=177
06/18/2022 09:01:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=179
06/18/2022 09:01:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=182
06/18/2022 09:02:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=184
06/18/2022 09:02:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.16 on epoch=187
06/18/2022 09:02:05 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.8053071253071253 on epoch=187
06/18/2022 09:02:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=189
06/18/2022 09:02:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=192
06/18/2022 09:02:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=194
06/18/2022 09:02:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=197
06/18/2022 09:02:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.08 on epoch=199
06/18/2022 09:02:19 - INFO - __main__ - Global step 800 Train loss 0.08 Classification-F1 0.7763746057863705 on epoch=199
06/18/2022 09:02:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=202
06/18/2022 09:02:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=204
06/18/2022 09:02:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=207
06/18/2022 09:02:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=209
06/18/2022 09:02:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=212
06/18/2022 09:02:32 - INFO - __main__ - Global step 850 Train loss 0.06 Classification-F1 0.7763746057863705 on epoch=212
06/18/2022 09:02:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=214
06/18/2022 09:02:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=217
06/18/2022 09:02:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=219
06/18/2022 09:02:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=222
06/18/2022 09:02:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=224
06/18/2022 09:02:45 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.7577421271538919 on epoch=224
06/18/2022 09:02:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=227
06/18/2022 09:02:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=229
06/18/2022 09:02:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
06/18/2022 09:02:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=234
06/18/2022 09:02:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=237
06/18/2022 09:02:59 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.7577421271538919 on epoch=237
06/18/2022 09:03:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=239
06/18/2022 09:03:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
06/18/2022 09:03:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.10 on epoch=244
06/18/2022 09:03:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=247
06/18/2022 09:03:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=249
06/18/2022 09:03:13 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.7943802521008404 on epoch=249
06/18/2022 09:03:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=252
06/18/2022 09:03:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=254
06/18/2022 09:03:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
06/18/2022 09:03:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=259
06/18/2022 09:03:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=262
06/18/2022 09:03:26 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.8112572223246666 on epoch=262
06/18/2022 09:03:26 - INFO - __main__ - Saving model with best Classification-F1: 0.8057725279106858 -> 0.8112572223246666 on epoch=262, global_step=1050
06/18/2022 09:03:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
06/18/2022 09:03:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=267
06/18/2022 09:03:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
06/18/2022 09:03:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
06/18/2022 09:03:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=274
06/18/2022 09:03:40 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.7763746057863705 on epoch=274
06/18/2022 09:03:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=277
06/18/2022 09:03:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
06/18/2022 09:03:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=282
06/18/2022 09:03:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=284
06/18/2022 09:03:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
06/18/2022 09:03:53 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.7910804881393116 on epoch=287
06/18/2022 09:03:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
06/18/2022 09:03:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
06/18/2022 09:04:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
06/18/2022 09:04:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
06/18/2022 09:04:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
06/18/2022 09:04:06 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.7910804881393116 on epoch=299
06/18/2022 09:04:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
06/18/2022 09:04:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
06/18/2022 09:04:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
06/18/2022 09:04:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
06/18/2022 09:04:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
06/18/2022 09:04:20 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.7910804881393116 on epoch=312
06/18/2022 09:04:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
06/18/2022 09:04:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
06/18/2022 09:04:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
06/18/2022 09:04:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
06/18/2022 09:04:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=324
06/18/2022 09:04:33 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.7910804881393116 on epoch=324
06/18/2022 09:04:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
06/18/2022 09:04:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
06/18/2022 09:04:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
06/18/2022 09:04:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
06/18/2022 09:04:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
06/18/2022 09:04:47 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.7910804881393116 on epoch=337
06/18/2022 09:04:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
06/18/2022 09:04:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
06/18/2022 09:04:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
06/18/2022 09:04:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
06/18/2022 09:04:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
06/18/2022 09:05:00 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.7943802521008404 on epoch=349
06/18/2022 09:05:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
06/18/2022 09:05:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
06/18/2022 09:05:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
06/18/2022 09:05:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=359
06/18/2022 09:05:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=362
06/18/2022 09:05:13 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.7942911255411256 on epoch=362
06/18/2022 09:05:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
06/18/2022 09:05:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
06/18/2022 09:05:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
06/18/2022 09:05:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
06/18/2022 09:05:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
06/18/2022 09:05:27 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.7942911255411256 on epoch=374
06/18/2022 09:05:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
06/18/2022 09:05:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
06/18/2022 09:05:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/18/2022 09:05:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
06/18/2022 09:05:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
06/18/2022 09:05:40 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.8098175381263616 on epoch=387
06/18/2022 09:05:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
06/18/2022 09:05:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
06/18/2022 09:05:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
06/18/2022 09:05:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
06/18/2022 09:05:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
06/18/2022 09:05:53 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.7648624896608769 on epoch=399
06/18/2022 09:05:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/18/2022 09:05:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
06/18/2022 09:06:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
06/18/2022 09:06:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/18/2022 09:06:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
06/18/2022 09:06:07 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7724480095068331 on epoch=412
06/18/2022 09:06:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
06/18/2022 09:06:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
06/18/2022 09:06:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
06/18/2022 09:06:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
06/18/2022 09:06:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
06/18/2022 09:06:20 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.7768308080808081 on epoch=424
06/18/2022 09:06:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
06/18/2022 09:06:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/18/2022 09:06:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
06/18/2022 09:06:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
06/18/2022 09:06:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
06/18/2022 09:06:34 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.7774478381096028 on epoch=437
06/18/2022 09:06:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=439
06/18/2022 09:06:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=442
06/18/2022 09:06:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
06/18/2022 09:06:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
06/18/2022 09:06:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=449
06/18/2022 09:06:48 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.787372934431758 on epoch=449
06/18/2022 09:06:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
06/18/2022 09:06:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
06/18/2022 09:06:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
06/18/2022 09:06:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
06/18/2022 09:07:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
06/18/2022 09:07:01 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.791826923076923 on epoch=462
06/18/2022 09:07:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
06/18/2022 09:07:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/18/2022 09:07:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/18/2022 09:07:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
06/18/2022 09:07:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
06/18/2022 09:07:15 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.8269432773109244 on epoch=474
06/18/2022 09:07:15 - INFO - __main__ - Saving model with best Classification-F1: 0.8112572223246666 -> 0.8269432773109244 on epoch=474, global_step=1900
06/18/2022 09:07:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
06/18/2022 09:07:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
06/18/2022 09:07:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
06/18/2022 09:07:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
06/18/2022 09:07:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
06/18/2022 09:07:28 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.8098175381263616 on epoch=487
06/18/2022 09:07:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
06/18/2022 09:07:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
06/18/2022 09:07:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
06/18/2022 09:07:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/18/2022 09:07:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
06/18/2022 09:07:42 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7921537204625441 on epoch=499
06/18/2022 09:07:45 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
06/18/2022 09:07:47 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=504
06/18/2022 09:07:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
06/18/2022 09:07:52 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=509
06/18/2022 09:07:55 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/18/2022 09:07:56 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.8112572223246666 on epoch=512
06/18/2022 09:07:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/18/2022 09:08:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/18/2022 09:08:03 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
06/18/2022 09:08:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
06/18/2022 09:08:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
06/18/2022 09:08:09 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.8112572223246666 on epoch=524
06/18/2022 09:08:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
06/18/2022 09:08:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
06/18/2022 09:08:17 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
06/18/2022 09:08:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
06/18/2022 09:08:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
06/18/2022 09:08:23 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7931373243873243 on epoch=537
06/18/2022 09:08:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
06/18/2022 09:08:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
06/18/2022 09:08:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/18/2022 09:08:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
06/18/2022 09:08:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/18/2022 09:08:36 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.8098175381263616 on epoch=549
06/18/2022 09:08:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
06/18/2022 09:08:41 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=554
06/18/2022 09:08:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
06/18/2022 09:08:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
06/18/2022 09:08:49 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/18/2022 09:08:50 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.8269432773109244 on epoch=562
06/18/2022 09:08:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
06/18/2022 09:08:55 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
06/18/2022 09:08:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/18/2022 09:09:00 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/18/2022 09:09:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/18/2022 09:09:03 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.8098175381263616 on epoch=574
06/18/2022 09:09:06 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/18/2022 09:09:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/18/2022 09:09:11 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/18/2022 09:09:13 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
06/18/2022 09:09:16 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/18/2022 09:09:17 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7795026881720429 on epoch=587
06/18/2022 09:09:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=589
06/18/2022 09:09:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
06/18/2022 09:09:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/18/2022 09:09:27 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
06/18/2022 09:09:30 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/18/2022 09:09:31 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7942911255411256 on epoch=599
06/18/2022 09:09:33 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
06/18/2022 09:09:36 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/18/2022 09:09:38 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/18/2022 09:09:41 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
06/18/2022 09:09:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
06/18/2022 09:09:44 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.8112572223246666 on epoch=612
06/18/2022 09:09:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/18/2022 09:09:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
06/18/2022 09:09:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=619
06/18/2022 09:09:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/18/2022 09:09:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
06/18/2022 09:09:58 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.8098175381263616 on epoch=624
06/18/2022 09:10:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/18/2022 09:10:03 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
06/18/2022 09:10:06 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/18/2022 09:10:08 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=634
06/18/2022 09:10:11 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/18/2022 09:10:12 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.8112572223246666 on epoch=637
06/18/2022 09:10:14 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/18/2022 09:10:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/18/2022 09:10:19 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/18/2022 09:10:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/18/2022 09:10:24 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/18/2022 09:10:25 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.8112572223246666 on epoch=649
06/18/2022 09:10:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=652
06/18/2022 09:10:30 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=654
06/18/2022 09:10:33 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/18/2022 09:10:35 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
06/18/2022 09:10:38 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
06/18/2022 09:10:39 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.8243897306397306 on epoch=662
06/18/2022 09:10:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
06/18/2022 09:10:44 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/18/2022 09:10:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/18/2022 09:10:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
06/18/2022 09:10:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/18/2022 09:10:52 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.8415854978354979 on epoch=674
06/18/2022 09:10:53 - INFO - __main__ - Saving model with best Classification-F1: 0.8269432773109244 -> 0.8415854978354979 on epoch=674, global_step=2700
06/18/2022 09:10:55 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/18/2022 09:10:58 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/18/2022 09:11:00 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=682
06/18/2022 09:11:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/18/2022 09:11:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/18/2022 09:11:06 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.791826923076923 on epoch=687
06/18/2022 09:11:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
06/18/2022 09:11:11 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
06/18/2022 09:11:14 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/18/2022 09:11:16 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
06/18/2022 09:11:19 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/18/2022 09:11:20 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.791826923076923 on epoch=699
06/18/2022 09:11:22 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
06/18/2022 09:11:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/18/2022 09:11:27 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/18/2022 09:11:30 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/18/2022 09:11:32 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=712
06/18/2022 09:11:33 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.791826923076923 on epoch=712
06/18/2022 09:11:36 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/18/2022 09:11:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/18/2022 09:11:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=719
06/18/2022 09:11:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/18/2022 09:11:46 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/18/2022 09:11:47 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.7942911255411256 on epoch=724
06/18/2022 09:11:50 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/18/2022 09:11:52 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/18/2022 09:11:55 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/18/2022 09:11:57 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/18/2022 09:12:00 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/18/2022 09:12:01 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7942911255411256 on epoch=737
06/18/2022 09:12:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
06/18/2022 09:12:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/18/2022 09:12:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/18/2022 09:12:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/18/2022 09:12:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/18/2022 09:12:14 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7765931372549019 on epoch=749
06/18/2022 09:12:14 - INFO - __main__ - save last model!
06/18/2022 09:12:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 09:12:14 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:12:14 - INFO - __main__ - Printing 3 examples
06/18/2022 09:12:14 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/18/2022 09:12:14 - INFO - __main__ - ['happy']
06/18/2022 09:12:14 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/18/2022 09:12:14 - INFO - __main__ - ['happy']
06/18/2022 09:12:14 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/18/2022 09:12:14 - INFO - __main__ - ['happy']
06/18/2022 09:12:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 09:12:14 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 09:12:14 - INFO - __main__ - Printing 3 examples
06/18/2022 09:12:14 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 09:12:14 - INFO - __main__ - ['others']
06/18/2022 09:12:14 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 09:12:14 - INFO - __main__ - ['others']
06/18/2022 09:12:14 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 09:12:14 - INFO - __main__ - ['others']
06/18/2022 09:12:14 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 09:12:14 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:12:15 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 09:12:15 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:12:15 - INFO - __main__ - Printing 3 examples
06/18/2022 09:12:15 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/18/2022 09:12:15 - INFO - __main__ - ['happy']
06/18/2022 09:12:15 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/18/2022 09:12:15 - INFO - __main__ - ['happy']
06/18/2022 09:12:15 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/18/2022 09:12:15 - INFO - __main__ - ['happy']
06/18/2022 09:12:15 - INFO - __main__ - Tokenizing Input ...
06/18/2022 09:12:15 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:12:15 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 09:12:17 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:12:22 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 09:12:31 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 09:12:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 09:12:32 - INFO - __main__ - Starting training!
06/18/2022 09:13:57 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_42_0.3_8_predictions.txt
06/18/2022 09:13:57 - INFO - __main__ - Classification-F1 on test data: 0.1254
06/18/2022 09:13:57 - INFO - __main__ - prefix=emo_16_42, lr=0.3, bsz=8, dev_performance=0.8415854978354979, test_performance=0.12539920362286308
06/18/2022 09:13:57 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.2, bsz=8 ...
06/18/2022 09:13:58 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:13:58 - INFO - __main__ - Printing 3 examples
06/18/2022 09:13:58 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/18/2022 09:13:58 - INFO - __main__ - ['happy']
06/18/2022 09:13:58 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/18/2022 09:13:58 - INFO - __main__ - ['happy']
06/18/2022 09:13:58 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/18/2022 09:13:58 - INFO - __main__ - ['happy']
06/18/2022 09:13:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 09:13:58 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:13:58 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 09:13:58 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:13:58 - INFO - __main__ - Printing 3 examples
06/18/2022 09:13:58 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/18/2022 09:13:58 - INFO - __main__ - ['happy']
06/18/2022 09:13:58 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/18/2022 09:13:58 - INFO - __main__ - ['happy']
06/18/2022 09:13:58 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/18/2022 09:13:58 - INFO - __main__ - ['happy']
06/18/2022 09:13:58 - INFO - __main__ - Tokenizing Input ...
06/18/2022 09:13:58 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:13:59 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 09:14:17 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 09:14:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 09:14:18 - INFO - __main__ - Starting training!
06/18/2022 09:14:21 - INFO - __main__ - Step 10 Global step 10 Train loss 4.30 on epoch=2
06/18/2022 09:14:23 - INFO - __main__ - Step 20 Global step 20 Train loss 3.43 on epoch=4
06/18/2022 09:14:26 - INFO - __main__ - Step 30 Global step 30 Train loss 2.84 on epoch=7
06/18/2022 09:14:28 - INFO - __main__ - Step 40 Global step 40 Train loss 2.59 on epoch=9
06/18/2022 09:14:31 - INFO - __main__ - Step 50 Global step 50 Train loss 2.09 on epoch=12
06/18/2022 09:14:33 - INFO - __main__ - Global step 50 Train loss 3.05 Classification-F1 0.029333955804544037 on epoch=12
06/18/2022 09:14:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.029333955804544037 on epoch=12, global_step=50
06/18/2022 09:14:35 - INFO - __main__ - Step 60 Global step 60 Train loss 1.88 on epoch=14
06/18/2022 09:14:38 - INFO - __main__ - Step 70 Global step 70 Train loss 1.58 on epoch=17
06/18/2022 09:14:40 - INFO - __main__ - Step 80 Global step 80 Train loss 1.50 on epoch=19
06/18/2022 09:14:43 - INFO - __main__ - Step 90 Global step 90 Train loss 1.25 on epoch=22
06/18/2022 09:14:45 - INFO - __main__ - Step 100 Global step 100 Train loss 1.14 on epoch=24
06/18/2022 09:14:46 - INFO - __main__ - Global step 100 Train loss 1.47 Classification-F1 0.3002256130935694 on epoch=24
06/18/2022 09:14:46 - INFO - __main__ - Saving model with best Classification-F1: 0.029333955804544037 -> 0.3002256130935694 on epoch=24, global_step=100
06/18/2022 09:14:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.91 on epoch=27
06/18/2022 09:14:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.92 on epoch=29
06/18/2022 09:14:54 - INFO - __main__ - Step 130 Global step 130 Train loss 0.91 on epoch=32
06/18/2022 09:14:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.78 on epoch=34
06/18/2022 09:14:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.77 on epoch=37
06/18/2022 09:15:00 - INFO - __main__ - Global step 150 Train loss 0.86 Classification-F1 0.38494163936089437 on epoch=37
06/18/2022 09:15:00 - INFO - __main__ - Saving model with best Classification-F1: 0.3002256130935694 -> 0.38494163936089437 on epoch=37, global_step=150
06/18/2022 09:15:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.73 on epoch=39
06/18/2022 09:15:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.69 on epoch=42
06/18/2022 09:15:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.78 on epoch=44
06/18/2022 09:15:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.65 on epoch=47
06/18/2022 09:15:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.62 on epoch=49
06/18/2022 09:15:14 - INFO - __main__ - Global step 200 Train loss 0.69 Classification-F1 0.6724639445156331 on epoch=49
06/18/2022 09:15:14 - INFO - __main__ - Saving model with best Classification-F1: 0.38494163936089437 -> 0.6724639445156331 on epoch=49, global_step=200
06/18/2022 09:15:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.61 on epoch=52
06/18/2022 09:15:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.61 on epoch=54
06/18/2022 09:15:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.55 on epoch=57
06/18/2022 09:15:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=59
06/18/2022 09:15:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=62
06/18/2022 09:15:27 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.6227692075015123 on epoch=62
06/18/2022 09:15:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=64
06/18/2022 09:15:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=67
06/18/2022 09:15:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=69
06/18/2022 09:15:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.57 on epoch=72
06/18/2022 09:15:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=74
06/18/2022 09:15:41 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.7069215865552072 on epoch=74
06/18/2022 09:15:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6724639445156331 -> 0.7069215865552072 on epoch=74, global_step=300
06/18/2022 09:15:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=77
06/18/2022 09:15:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=79
06/18/2022 09:15:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=82
06/18/2022 09:15:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=84
06/18/2022 09:15:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=87
06/18/2022 09:15:54 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.7199582027168234 on epoch=87
06/18/2022 09:15:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7069215865552072 -> 0.7199582027168234 on epoch=87, global_step=350
06/18/2022 09:15:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=89
06/18/2022 09:15:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=92
06/18/2022 09:16:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=94
06/18/2022 09:16:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=97
06/18/2022 09:16:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=99
06/18/2022 09:16:08 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.7179476733465129 on epoch=99
06/18/2022 09:16:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=102
06/18/2022 09:16:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=104
06/18/2022 09:16:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=107
06/18/2022 09:16:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=109
06/18/2022 09:16:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.29 on epoch=112
06/18/2022 09:16:21 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.7179476733465129 on epoch=112
06/18/2022 09:16:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=114
06/18/2022 09:16:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=117
06/18/2022 09:16:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=119
06/18/2022 09:16:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=122
06/18/2022 09:16:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=124
06/18/2022 09:16:35 - INFO - __main__ - Global step 500 Train loss 0.33 Classification-F1 0.7365115505375003 on epoch=124
06/18/2022 09:16:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7199582027168234 -> 0.7365115505375003 on epoch=124, global_step=500
06/18/2022 09:16:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=127
06/18/2022 09:16:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=129
06/18/2022 09:16:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.29 on epoch=132
06/18/2022 09:16:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=134
06/18/2022 09:16:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=137
06/18/2022 09:16:48 - INFO - __main__ - Global step 550 Train loss 0.29 Classification-F1 0.7922983870967742 on epoch=137
06/18/2022 09:16:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7365115505375003 -> 0.7922983870967742 on epoch=137, global_step=550
06/18/2022 09:16:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=139
06/18/2022 09:16:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.29 on epoch=142
06/18/2022 09:16:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.30 on epoch=144
06/18/2022 09:16:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=147
06/18/2022 09:17:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=149
06/18/2022 09:17:02 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.7362735299017158 on epoch=149
06/18/2022 09:17:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=152
06/18/2022 09:17:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.29 on epoch=154
06/18/2022 09:17:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=157
06/18/2022 09:17:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=159
06/18/2022 09:17:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=162
06/18/2022 09:17:15 - INFO - __main__ - Global step 650 Train loss 0.24 Classification-F1 0.7517465626161277 on epoch=162
06/18/2022 09:17:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=164
06/18/2022 09:17:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=167
06/18/2022 09:17:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=169
06/18/2022 09:17:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=172
06/18/2022 09:17:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.17 on epoch=174
06/18/2022 09:17:29 - INFO - __main__ - Global step 700 Train loss 0.18 Classification-F1 0.7739098300073909 on epoch=174
06/18/2022 09:17:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.18 on epoch=177
06/18/2022 09:17:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=179
06/18/2022 09:17:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=182
06/18/2022 09:17:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=184
06/18/2022 09:17:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.30 on epoch=187
06/18/2022 09:17:42 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.7910105580693816 on epoch=187
06/18/2022 09:17:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=189
06/18/2022 09:17:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=192
06/18/2022 09:17:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=194
06/18/2022 09:17:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=197
06/18/2022 09:17:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=199
06/18/2022 09:17:56 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.8057725279106858 on epoch=199
06/18/2022 09:17:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7922983870967742 -> 0.8057725279106858 on epoch=199, global_step=800
06/18/2022 09:17:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=202
06/18/2022 09:18:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=204
06/18/2022 09:18:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=207
06/18/2022 09:18:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=209
06/18/2022 09:18:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=212
06/18/2022 09:18:09 - INFO - __main__ - Global step 850 Train loss 0.18 Classification-F1 0.7521739130434782 on epoch=212
06/18/2022 09:18:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=214
06/18/2022 09:18:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=217
06/18/2022 09:18:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=219
06/18/2022 09:18:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=222
06/18/2022 09:18:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=224
06/18/2022 09:18:23 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.7669795375265221 on epoch=224
06/18/2022 09:18:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=227
06/18/2022 09:18:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.11 on epoch=229
06/18/2022 09:18:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=232
06/18/2022 09:18:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=234
06/18/2022 09:18:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=237
06/18/2022 09:18:36 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.771969696969697 on epoch=237
06/18/2022 09:18:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=239
06/18/2022 09:18:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
06/18/2022 09:18:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.16 on epoch=244
06/18/2022 09:18:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=247
06/18/2022 09:18:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=249
06/18/2022 09:18:50 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.7575834748160725 on epoch=249
06/18/2022 09:18:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=252
06/18/2022 09:18:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=254
06/18/2022 09:18:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=257
06/18/2022 09:19:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=259
06/18/2022 09:19:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=262
06/18/2022 09:19:03 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.7380299924479448 on epoch=262
06/18/2022 09:19:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=264
06/18/2022 09:19:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=267
06/18/2022 09:19:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.14 on epoch=269
06/18/2022 09:19:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
06/18/2022 09:19:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=274
06/18/2022 09:19:17 - INFO - __main__ - Global step 1100 Train loss 0.13 Classification-F1 0.756969696969697 on epoch=274
06/18/2022 09:19:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=277
06/18/2022 09:19:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
06/18/2022 09:19:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=282
06/18/2022 09:19:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=284
06/18/2022 09:19:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=287
06/18/2022 09:19:30 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.771969696969697 on epoch=287
06/18/2022 09:19:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
06/18/2022 09:19:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=292
06/18/2022 09:19:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=294
06/18/2022 09:19:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=297
06/18/2022 09:19:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=299
06/18/2022 09:19:44 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.7381475225225225 on epoch=299
06/18/2022 09:19:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
06/18/2022 09:19:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=304
06/18/2022 09:19:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
06/18/2022 09:19:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
06/18/2022 09:19:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
06/18/2022 09:19:57 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.7373366013071896 on epoch=312
06/18/2022 09:20:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.14 on epoch=314
06/18/2022 09:20:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
06/18/2022 09:20:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=319
06/18/2022 09:20:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=322
06/18/2022 09:20:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
06/18/2022 09:20:11 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.7759289729877965 on epoch=324
06/18/2022 09:20:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=327
06/18/2022 09:20:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=329
06/18/2022 09:20:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=332
06/18/2022 09:20:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=334
06/18/2022 09:20:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
06/18/2022 09:20:24 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.7935446906035142 on epoch=337
06/18/2022 09:20:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
06/18/2022 09:20:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=342
06/18/2022 09:20:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
06/18/2022 09:20:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=347
06/18/2022 09:20:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
06/18/2022 09:20:38 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.7567873303167421 on epoch=349
06/18/2022 09:20:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
06/18/2022 09:20:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=354
06/18/2022 09:20:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=357
06/18/2022 09:20:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
06/18/2022 09:20:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
06/18/2022 09:20:52 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.7935446906035142 on epoch=362
06/18/2022 09:20:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
06/18/2022 09:20:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=367
06/18/2022 09:20:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
06/18/2022 09:21:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
06/18/2022 09:21:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=374
06/18/2022 09:21:05 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.7754010695187167 on epoch=374
06/18/2022 09:21:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=377
06/18/2022 09:21:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=379
06/18/2022 09:21:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
06/18/2022 09:21:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
06/18/2022 09:21:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
06/18/2022 09:21:19 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.7566982037570273 on epoch=387
06/18/2022 09:21:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
06/18/2022 09:21:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=392
06/18/2022 09:21:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
06/18/2022 09:21:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
06/18/2022 09:21:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=399
06/18/2022 09:21:32 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.7754010695187167 on epoch=399
06/18/2022 09:21:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=402
06/18/2022 09:21:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
06/18/2022 09:21:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
06/18/2022 09:21:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=409
06/18/2022 09:21:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=412
06/18/2022 09:21:46 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.7754010695187167 on epoch=412
06/18/2022 09:21:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=414
06/18/2022 09:21:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=417
06/18/2022 09:21:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
06/18/2022 09:21:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
06/18/2022 09:21:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
06/18/2022 09:21:59 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.8112572223246666 on epoch=424
06/18/2022 09:21:59 - INFO - __main__ - Saving model with best Classification-F1: 0.8057725279106858 -> 0.8112572223246666 on epoch=424, global_step=1700
06/18/2022 09:22:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
06/18/2022 09:22:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
06/18/2022 09:22:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=432
06/18/2022 09:22:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=434
06/18/2022 09:22:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
06/18/2022 09:22:13 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.7759289729877965 on epoch=437
06/18/2022 09:22:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
06/18/2022 09:22:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
06/18/2022 09:22:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
06/18/2022 09:22:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
06/18/2022 09:22:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
06/18/2022 09:22:27 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7759289729877965 on epoch=449
06/18/2022 09:22:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
06/18/2022 09:22:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=454
06/18/2022 09:22:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
06/18/2022 09:22:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
06/18/2022 09:22:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
06/18/2022 09:22:40 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.7942760942760942 on epoch=462
06/18/2022 09:22:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
06/18/2022 09:22:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
06/18/2022 09:22:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
06/18/2022 09:22:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
06/18/2022 09:22:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
06/18/2022 09:22:54 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7942760942760942 on epoch=474
06/18/2022 09:22:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
06/18/2022 09:22:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
06/18/2022 09:23:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
06/18/2022 09:23:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
06/18/2022 09:23:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
06/18/2022 09:23:07 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7759289729877965 on epoch=487
06/18/2022 09:23:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
06/18/2022 09:23:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
06/18/2022 09:23:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
06/18/2022 09:23:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=497
06/18/2022 09:23:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
06/18/2022 09:23:21 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7759289729877965 on epoch=499
06/18/2022 09:23:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
06/18/2022 09:23:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=504
06/18/2022 09:23:28 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.14 on epoch=507
06/18/2022 09:23:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
06/18/2022 09:23:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
06/18/2022 09:23:34 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.8121482683982684 on epoch=512
06/18/2022 09:23:34 - INFO - __main__ - Saving model with best Classification-F1: 0.8112572223246666 -> 0.8121482683982684 on epoch=512, global_step=2050
06/18/2022 09:23:37 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=514
06/18/2022 09:23:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=517
06/18/2022 09:23:42 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
06/18/2022 09:23:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
06/18/2022 09:23:47 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=524
06/18/2022 09:23:48 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.8121482683982684 on epoch=524
06/18/2022 09:23:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=527
06/18/2022 09:23:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
06/18/2022 09:23:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
06/18/2022 09:23:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
06/18/2022 09:24:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
06/18/2022 09:24:01 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.8121482683982684 on epoch=537
06/18/2022 09:24:04 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
06/18/2022 09:24:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
06/18/2022 09:24:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
06/18/2022 09:24:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
06/18/2022 09:24:14 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.10 on epoch=549
06/18/2022 09:24:14 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.8112572223246666 on epoch=549
06/18/2022 09:24:17 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
06/18/2022 09:24:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
06/18/2022 09:24:22 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
06/18/2022 09:24:24 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/18/2022 09:24:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=562
06/18/2022 09:24:28 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.7971230158730158 on epoch=562
06/18/2022 09:24:30 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=564
06/18/2022 09:24:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
06/18/2022 09:24:35 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
06/18/2022 09:24:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=572
06/18/2022 09:24:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
06/18/2022 09:24:41 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.7971230158730158 on epoch=574
06/18/2022 09:24:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/18/2022 09:24:46 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/18/2022 09:24:49 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
06/18/2022 09:24:51 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=584
06/18/2022 09:24:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
06/18/2022 09:24:55 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7965513399717254 on epoch=587
06/18/2022 09:24:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=589
06/18/2022 09:25:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
06/18/2022 09:25:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/18/2022 09:25:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=597
06/18/2022 09:25:07 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
06/18/2022 09:25:08 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.7763746057863705 on epoch=599
06/18/2022 09:25:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/18/2022 09:25:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
06/18/2022 09:25:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=607
06/18/2022 09:25:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
06/18/2022 09:25:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/18/2022 09:25:22 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7796593384828678 on epoch=612
06/18/2022 09:25:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/18/2022 09:25:27 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.13 on epoch=617
06/18/2022 09:25:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/18/2022 09:25:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=622
06/18/2022 09:25:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
06/18/2022 09:25:35 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.7227230572818808 on epoch=624
06/18/2022 09:25:38 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
06/18/2022 09:25:40 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
06/18/2022 09:25:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/18/2022 09:25:45 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
06/18/2022 09:25:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
06/18/2022 09:25:48 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7570588235294118 on epoch=637
06/18/2022 09:25:51 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/18/2022 09:25:53 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
06/18/2022 09:25:56 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
06/18/2022 09:25:59 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/18/2022 09:26:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
06/18/2022 09:26:02 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7759289729877965 on epoch=649
06/18/2022 09:26:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/18/2022 09:26:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/18/2022 09:26:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
06/18/2022 09:26:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
06/18/2022 09:26:15 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=662
06/18/2022 09:26:15 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.8112572223246666 on epoch=662
06/18/2022 09:26:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=664
06/18/2022 09:26:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=667
06/18/2022 09:26:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
06/18/2022 09:26:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
06/18/2022 09:26:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=674
06/18/2022 09:26:29 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.8121482683982684 on epoch=674
06/18/2022 09:26:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
06/18/2022 09:26:34 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
06/18/2022 09:26:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=682
06/18/2022 09:26:39 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=684
06/18/2022 09:26:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
06/18/2022 09:26:43 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.8121482683982684 on epoch=687
06/18/2022 09:26:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
06/18/2022 09:26:48 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/18/2022 09:26:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=694
06/18/2022 09:26:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=697
06/18/2022 09:26:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
06/18/2022 09:26:56 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.8121482683982684 on epoch=699
06/18/2022 09:26:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
06/18/2022 09:27:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=704
06/18/2022 09:27:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
06/18/2022 09:27:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 09:27:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
06/18/2022 09:27:09 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.8121482683982684 on epoch=712
06/18/2022 09:27:12 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/18/2022 09:27:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/18/2022 09:27:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/18/2022 09:27:20 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
06/18/2022 09:27:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
06/18/2022 09:27:23 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.8121482683982684 on epoch=724
06/18/2022 09:27:26 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/18/2022 09:27:28 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
06/18/2022 09:27:31 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.07 on epoch=732
06/18/2022 09:27:33 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 09:27:36 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/18/2022 09:27:36 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.8121482683982684 on epoch=737
06/18/2022 09:27:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=739
06/18/2022 09:27:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=742
06/18/2022 09:27:44 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
06/18/2022 09:27:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
06/18/2022 09:27:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/18/2022 09:27:50 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.8112572223246666 on epoch=749
06/18/2022 09:27:50 - INFO - __main__ - save last model!
06/18/2022 09:27:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 09:27:50 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 09:27:50 - INFO - __main__ - Printing 3 examples
06/18/2022 09:27:50 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 09:27:50 - INFO - __main__ - ['others']
06/18/2022 09:27:50 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 09:27:50 - INFO - __main__ - ['others']
06/18/2022 09:27:50 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 09:27:50 - INFO - __main__ - ['others']
06/18/2022 09:27:50 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 09:27:50 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:27:50 - INFO - __main__ - Printing 3 examples
06/18/2022 09:27:50 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/18/2022 09:27:50 - INFO - __main__ - ['others']
06/18/2022 09:27:50 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/18/2022 09:27:50 - INFO - __main__ - ['others']
06/18/2022 09:27:50 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/18/2022 09:27:50 - INFO - __main__ - ['others']
06/18/2022 09:27:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 09:27:50 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:27:50 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 09:27:50 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:27:50 - INFO - __main__ - Printing 3 examples
06/18/2022 09:27:50 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/18/2022 09:27:50 - INFO - __main__ - ['others']
06/18/2022 09:27:50 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/18/2022 09:27:50 - INFO - __main__ - ['others']
06/18/2022 09:27:50 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/18/2022 09:27:50 - INFO - __main__ - ['others']
06/18/2022 09:27:50 - INFO - __main__ - Tokenizing Input ...
06/18/2022 09:27:50 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:27:50 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 09:27:52 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:27:57 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 09:28:06 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 09:28:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 09:28:07 - INFO - __main__ - Starting training!
06/18/2022 09:29:16 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_42_0.2_8_predictions.txt
06/18/2022 09:29:16 - INFO - __main__ - Classification-F1 on test data: 0.1167
06/18/2022 09:29:16 - INFO - __main__ - prefix=emo_16_42, lr=0.2, bsz=8, dev_performance=0.8121482683982684, test_performance=0.11669952447364929
06/18/2022 09:29:16 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.5, bsz=8 ...
06/18/2022 09:29:17 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:29:17 - INFO - __main__ - Printing 3 examples
06/18/2022 09:29:17 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/18/2022 09:29:17 - INFO - __main__ - ['others']
06/18/2022 09:29:17 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/18/2022 09:29:17 - INFO - __main__ - ['others']
06/18/2022 09:29:17 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/18/2022 09:29:17 - INFO - __main__ - ['others']
06/18/2022 09:29:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 09:29:17 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:29:17 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 09:29:17 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:29:17 - INFO - __main__ - Printing 3 examples
06/18/2022 09:29:17 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/18/2022 09:29:17 - INFO - __main__ - ['others']
06/18/2022 09:29:17 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/18/2022 09:29:17 - INFO - __main__ - ['others']
06/18/2022 09:29:17 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/18/2022 09:29:17 - INFO - __main__ - ['others']
06/18/2022 09:29:17 - INFO - __main__ - Tokenizing Input ...
06/18/2022 09:29:17 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:29:17 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 09:29:32 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 09:29:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 09:29:33 - INFO - __main__ - Starting training!
06/18/2022 09:29:36 - INFO - __main__ - Step 10 Global step 10 Train loss 4.12 on epoch=2
06/18/2022 09:29:39 - INFO - __main__ - Step 20 Global step 20 Train loss 2.73 on epoch=4
06/18/2022 09:29:41 - INFO - __main__ - Step 30 Global step 30 Train loss 2.05 on epoch=7
06/18/2022 09:29:43 - INFO - __main__ - Step 40 Global step 40 Train loss 1.25 on epoch=9
06/18/2022 09:29:46 - INFO - __main__ - Step 50 Global step 50 Train loss 1.06 on epoch=12
06/18/2022 09:29:47 - INFO - __main__ - Global step 50 Train loss 2.24 Classification-F1 0.41918276845106117 on epoch=12
06/18/2022 09:29:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.41918276845106117 on epoch=12, global_step=50
06/18/2022 09:29:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.83 on epoch=14
06/18/2022 09:29:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.67 on epoch=17
06/18/2022 09:29:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.80 on epoch=19
06/18/2022 09:29:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=22
06/18/2022 09:29:59 - INFO - __main__ - Step 100 Global step 100 Train loss 0.58 on epoch=24
06/18/2022 09:30:00 - INFO - __main__ - Global step 100 Train loss 0.70 Classification-F1 0.5449628127112914 on epoch=24
06/18/2022 09:30:00 - INFO - __main__ - Saving model with best Classification-F1: 0.41918276845106117 -> 0.5449628127112914 on epoch=24, global_step=100
06/18/2022 09:30:02 - INFO - __main__ - Step 110 Global step 110 Train loss 0.59 on epoch=27
06/18/2022 09:30:04 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=29
06/18/2022 09:30:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=32
06/18/2022 09:30:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=34
06/18/2022 09:30:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=37
06/18/2022 09:30:12 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.5791021671826626 on epoch=37
06/18/2022 09:30:12 - INFO - __main__ - Saving model with best Classification-F1: 0.5449628127112914 -> 0.5791021671826626 on epoch=37, global_step=150
06/18/2022 09:30:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=39
06/18/2022 09:30:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=42
06/18/2022 09:30:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=44
06/18/2022 09:30:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.31 on epoch=47
06/18/2022 09:30:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.30 on epoch=49
06/18/2022 09:30:25 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.6030908757481287 on epoch=49
06/18/2022 09:30:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5791021671826626 -> 0.6030908757481287 on epoch=49, global_step=200
06/18/2022 09:30:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=52
06/18/2022 09:30:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=54
06/18/2022 09:30:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=57
06/18/2022 09:30:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.29 on epoch=59
06/18/2022 09:30:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=62
06/18/2022 09:30:37 - INFO - __main__ - Global step 250 Train loss 0.30 Classification-F1 0.6569235588972431 on epoch=62
06/18/2022 09:30:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6030908757481287 -> 0.6569235588972431 on epoch=62, global_step=250
06/18/2022 09:30:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=64
06/18/2022 09:30:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.28 on epoch=67
06/18/2022 09:30:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=69
06/18/2022 09:30:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=72
06/18/2022 09:30:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.21 on epoch=74
06/18/2022 09:30:50 - INFO - __main__ - Global step 300 Train loss 0.25 Classification-F1 0.6126402900596448 on epoch=74
06/18/2022 09:30:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.19 on epoch=77
06/18/2022 09:30:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=79
06/18/2022 09:30:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.17 on epoch=82
06/18/2022 09:30:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.19 on epoch=84
06/18/2022 09:31:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.13 on epoch=87
06/18/2022 09:31:03 - INFO - __main__ - Global step 350 Train loss 0.17 Classification-F1 0.6596153846153846 on epoch=87
06/18/2022 09:31:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6569235588972431 -> 0.6596153846153846 on epoch=87, global_step=350
06/18/2022 09:31:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.20 on epoch=89
06/18/2022 09:31:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.15 on epoch=92
06/18/2022 09:31:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=94
06/18/2022 09:31:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=97
06/18/2022 09:31:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.10 on epoch=99
06/18/2022 09:31:15 - INFO - __main__ - Global step 400 Train loss 0.19 Classification-F1 0.6617042440318303 on epoch=99
06/18/2022 09:31:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6596153846153846 -> 0.6617042440318303 on epoch=99, global_step=400
06/18/2022 09:31:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.14 on epoch=102
06/18/2022 09:31:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.19 on epoch=104
06/18/2022 09:31:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.11 on epoch=107
06/18/2022 09:31:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=109
06/18/2022 09:31:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.15 on epoch=112
06/18/2022 09:31:28 - INFO - __main__ - Global step 450 Train loss 0.14 Classification-F1 0.6610518292682928 on epoch=112
06/18/2022 09:31:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.16 on epoch=114
06/18/2022 09:31:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.14 on epoch=117
06/18/2022 09:31:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.08 on epoch=119
06/18/2022 09:31:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.10 on epoch=122
06/18/2022 09:31:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.14 on epoch=124
06/18/2022 09:31:41 - INFO - __main__ - Global step 500 Train loss 0.12 Classification-F1 0.6881044073598631 on epoch=124
06/18/2022 09:31:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6617042440318303 -> 0.6881044073598631 on epoch=124, global_step=500
06/18/2022 09:31:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.05 on epoch=127
06/18/2022 09:31:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=129
06/18/2022 09:31:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.05 on epoch=132
06/18/2022 09:31:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.16 on epoch=134
06/18/2022 09:31:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=137
06/18/2022 09:31:53 - INFO - __main__ - Global step 550 Train loss 0.09 Classification-F1 0.691009154315606 on epoch=137
06/18/2022 09:31:53 - INFO - __main__ - Saving model with best Classification-F1: 0.6881044073598631 -> 0.691009154315606 on epoch=137, global_step=550
06/18/2022 09:31:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.10 on epoch=139
06/18/2022 09:31:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=142
06/18/2022 09:32:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=144
06/18/2022 09:32:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.11 on epoch=147
06/18/2022 09:32:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.06 on epoch=149
06/18/2022 09:32:06 - INFO - __main__ - Global step 600 Train loss 0.08 Classification-F1 0.6617042440318303 on epoch=149
06/18/2022 09:32:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=152
06/18/2022 09:32:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=154
06/18/2022 09:32:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=157
06/18/2022 09:32:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=159
06/18/2022 09:32:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=162
06/18/2022 09:32:18 - INFO - __main__ - Global step 650 Train loss 0.06 Classification-F1 0.6596153846153846 on epoch=162
06/18/2022 09:32:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.07 on epoch=164
06/18/2022 09:32:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=167
06/18/2022 09:32:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.06 on epoch=169
06/18/2022 09:32:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=172
06/18/2022 09:32:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=174
06/18/2022 09:32:31 - INFO - __main__ - Global step 700 Train loss 0.06 Classification-F1 0.677296198637662 on epoch=174
06/18/2022 09:32:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=177
06/18/2022 09:32:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=179
06/18/2022 09:32:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=182
06/18/2022 09:32:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=184
06/18/2022 09:32:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=187
06/18/2022 09:32:44 - INFO - __main__ - Global step 750 Train loss 0.05 Classification-F1 0.6867500315776178 on epoch=187
06/18/2022 09:32:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=189
06/18/2022 09:32:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=192
06/18/2022 09:32:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=194
06/18/2022 09:32:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=197
06/18/2022 09:32:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=199
06/18/2022 09:32:56 - INFO - __main__ - Global step 800 Train loss 0.03 Classification-F1 0.6765350877192983 on epoch=199
06/18/2022 09:32:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=202
06/18/2022 09:33:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=204
06/18/2022 09:33:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=207
06/18/2022 09:33:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=209
06/18/2022 09:33:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=212
06/18/2022 09:33:09 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.7250398724082935 on epoch=212
06/18/2022 09:33:09 - INFO - __main__ - Saving model with best Classification-F1: 0.691009154315606 -> 0.7250398724082935 on epoch=212, global_step=850
06/18/2022 09:33:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=214
06/18/2022 09:33:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.11 on epoch=217
06/18/2022 09:33:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=219
06/18/2022 09:33:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=222
06/18/2022 09:33:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=224
06/18/2022 09:33:21 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.6867500315776178 on epoch=224
06/18/2022 09:33:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=227
06/18/2022 09:33:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=229
06/18/2022 09:33:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=232
06/18/2022 09:33:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=234
06/18/2022 09:33:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=237
06/18/2022 09:33:34 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.6466630930045565 on epoch=237
06/18/2022 09:33:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=239
06/18/2022 09:33:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
06/18/2022 09:33:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=244
06/18/2022 09:33:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=247
06/18/2022 09:33:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=249
06/18/2022 09:33:47 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.6617042440318303 on epoch=249
06/18/2022 09:33:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=252
06/18/2022 09:33:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=254
06/18/2022 09:33:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=257
06/18/2022 09:33:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=259
06/18/2022 09:33:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=262
06/18/2022 09:33:59 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.7533709106984969 on epoch=262
06/18/2022 09:33:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7250398724082935 -> 0.7533709106984969 on epoch=262, global_step=1050
06/18/2022 09:34:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
06/18/2022 09:34:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=267
06/18/2022 09:34:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
06/18/2022 09:34:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
06/18/2022 09:34:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
06/18/2022 09:34:12 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.6867500315776178 on epoch=274
06/18/2022 09:34:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
06/18/2022 09:34:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
06/18/2022 09:34:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=282
06/18/2022 09:34:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
06/18/2022 09:34:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
06/18/2022 09:34:25 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.6617042440318303 on epoch=287
06/18/2022 09:34:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
06/18/2022 09:34:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
06/18/2022 09:34:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
06/18/2022 09:34:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
06/18/2022 09:34:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
06/18/2022 09:34:38 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.6867500315776178 on epoch=299
06/18/2022 09:34:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
06/18/2022 09:34:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=304
06/18/2022 09:34:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
06/18/2022 09:34:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=309
06/18/2022 09:34:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
06/18/2022 09:34:50 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.6467948717948718 on epoch=312
06/18/2022 09:34:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
06/18/2022 09:34:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
06/18/2022 09:34:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=319
06/18/2022 09:35:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
06/18/2022 09:35:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
06/18/2022 09:35:03 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.6617042440318303 on epoch=324
06/18/2022 09:35:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
06/18/2022 09:35:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
06/18/2022 09:35:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
06/18/2022 09:35:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
06/18/2022 09:35:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
06/18/2022 09:35:16 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.6867500315776178 on epoch=337
06/18/2022 09:35:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
06/18/2022 09:35:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
06/18/2022 09:35:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
06/18/2022 09:35:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=347
06/18/2022 09:35:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
06/18/2022 09:35:29 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.7098304396122112 on epoch=349
06/18/2022 09:35:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
06/18/2022 09:35:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=354
06/18/2022 09:35:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
06/18/2022 09:35:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=359
06/18/2022 09:35:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
06/18/2022 09:35:42 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.6867500315776178 on epoch=362
06/18/2022 09:35:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
06/18/2022 09:35:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
06/18/2022 09:35:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
06/18/2022 09:35:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
06/18/2022 09:35:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
06/18/2022 09:35:55 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.6617042440318303 on epoch=374
06/18/2022 09:35:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
06/18/2022 09:35:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
06/18/2022 09:36:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/18/2022 09:36:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
06/18/2022 09:36:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
06/18/2022 09:36:07 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.6765350877192983 on epoch=387
06/18/2022 09:36:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
06/18/2022 09:36:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
06/18/2022 09:36:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
06/18/2022 09:36:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
06/18/2022 09:36:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
06/18/2022 09:36:20 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.7323417293432286 on epoch=399
06/18/2022 09:36:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/18/2022 09:36:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
06/18/2022 09:36:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
06/18/2022 09:36:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
06/18/2022 09:36:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
06/18/2022 09:36:33 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7323417293432286 on epoch=412
06/18/2022 09:36:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
06/18/2022 09:36:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
06/18/2022 09:36:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
06/18/2022 09:36:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
06/18/2022 09:36:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
06/18/2022 09:36:46 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.6617042440318303 on epoch=424
06/18/2022 09:36:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
06/18/2022 09:36:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
06/18/2022 09:36:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
06/18/2022 09:36:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
06/18/2022 09:36:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
06/18/2022 09:36:59 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.6867500315776178 on epoch=437
06/18/2022 09:37:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
06/18/2022 09:37:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
06/18/2022 09:37:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
06/18/2022 09:37:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
06/18/2022 09:37:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
06/18/2022 09:37:12 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.6767857142857143 on epoch=449
06/18/2022 09:37:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
06/18/2022 09:37:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
06/18/2022 09:37:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
06/18/2022 09:37:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
06/18/2022 09:37:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
06/18/2022 09:37:25 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6867500315776178 on epoch=462
06/18/2022 09:37:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/18/2022 09:37:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
06/18/2022 09:37:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=469
06/18/2022 09:37:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
06/18/2022 09:37:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
06/18/2022 09:37:37 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.6596153846153846 on epoch=474
06/18/2022 09:37:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
06/18/2022 09:37:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
06/18/2022 09:37:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
06/18/2022 09:37:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/18/2022 09:37:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
06/18/2022 09:37:50 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.6617042440318303 on epoch=487
06/18/2022 09:37:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
06/18/2022 09:37:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
06/18/2022 09:37:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=494
06/18/2022 09:38:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
06/18/2022 09:38:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
06/18/2022 09:38:03 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6867500315776178 on epoch=499
06/18/2022 09:38:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/18/2022 09:38:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
06/18/2022 09:38:10 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
06/18/2022 09:38:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
06/18/2022 09:38:15 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
06/18/2022 09:38:16 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7102090287208255 on epoch=512
06/18/2022 09:38:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
06/18/2022 09:38:21 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
06/18/2022 09:38:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
06/18/2022 09:38:26 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
06/18/2022 09:38:28 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/18/2022 09:38:29 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6767857142857143 on epoch=524
06/18/2022 09:38:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
06/18/2022 09:38:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
06/18/2022 09:38:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/18/2022 09:38:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
06/18/2022 09:38:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/18/2022 09:38:42 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.7310758082497213 on epoch=537
06/18/2022 09:38:44 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/18/2022 09:38:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/18/2022 09:38:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/18/2022 09:38:51 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
06/18/2022 09:38:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/18/2022 09:38:55 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.7670155993431855 on epoch=549
06/18/2022 09:38:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7533709106984969 -> 0.7670155993431855 on epoch=549, global_step=2200
06/18/2022 09:38:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/18/2022 09:39:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/18/2022 09:39:02 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/18/2022 09:39:04 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
06/18/2022 09:39:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
06/18/2022 09:39:08 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7224631180223285 on epoch=562
06/18/2022 09:39:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
06/18/2022 09:39:12 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
06/18/2022 09:39:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/18/2022 09:39:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/18/2022 09:39:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/18/2022 09:39:21 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.7657898665158194 on epoch=574
06/18/2022 09:39:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/18/2022 09:39:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/18/2022 09:39:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/18/2022 09:39:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
06/18/2022 09:39:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
06/18/2022 09:39:33 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.7528959276018099 on epoch=587
06/18/2022 09:39:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/18/2022 09:39:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/18/2022 09:39:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/18/2022 09:39:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/18/2022 09:39:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/18/2022 09:39:46 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.7093366778149387 on epoch=599
06/18/2022 09:39:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/18/2022 09:39:51 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/18/2022 09:39:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/18/2022 09:39:56 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/18/2022 09:39:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/18/2022 09:39:59 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.7528959276018099 on epoch=612
06/18/2022 09:40:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=614
06/18/2022 09:40:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/18/2022 09:40:06 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=619
06/18/2022 09:40:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/18/2022 09:40:11 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=624
06/18/2022 09:40:12 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.727570425313659 on epoch=624
06/18/2022 09:40:14 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/18/2022 09:40:17 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/18/2022 09:40:19 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/18/2022 09:40:22 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/18/2022 09:40:24 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/18/2022 09:40:25 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.7342961092961093 on epoch=637
06/18/2022 09:40:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/18/2022 09:40:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/18/2022 09:40:32 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/18/2022 09:40:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/18/2022 09:40:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/18/2022 09:40:38 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7325375773651636 on epoch=649
06/18/2022 09:40:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/18/2022 09:40:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
06/18/2022 09:40:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/18/2022 09:40:48 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 09:40:50 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/18/2022 09:40:51 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7528959276018099 on epoch=662
06/18/2022 09:40:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/18/2022 09:40:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/18/2022 09:40:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/18/2022 09:41:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/18/2022 09:41:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/18/2022 09:41:04 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.7047231353038976 on epoch=674
06/18/2022 09:41:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/18/2022 09:41:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
06/18/2022 09:41:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
06/18/2022 09:41:13 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/18/2022 09:41:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/18/2022 09:41:17 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.710602598908446 on epoch=687
06/18/2022 09:41:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 09:41:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/18/2022 09:41:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
06/18/2022 09:41:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/18/2022 09:41:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/18/2022 09:41:30 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7126574798199512 on epoch=699
06/18/2022 09:41:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/18/2022 09:41:34 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/18/2022 09:41:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/18/2022 09:41:39 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 09:41:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/18/2022 09:41:43 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6767857142857143 on epoch=712
06/18/2022 09:41:45 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/18/2022 09:41:47 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/18/2022 09:41:50 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/18/2022 09:41:52 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/18/2022 09:41:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/18/2022 09:41:56 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7018315018315019 on epoch=724
06/18/2022 09:41:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/18/2022 09:42:00 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=729
06/18/2022 09:42:03 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/18/2022 09:42:05 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 09:42:07 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/18/2022 09:42:08 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7102090287208255 on epoch=737
06/18/2022 09:42:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 09:42:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/18/2022 09:42:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/18/2022 09:42:18 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/18/2022 09:42:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/18/2022 09:42:21 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7342961092961093 on epoch=749
06/18/2022 09:42:21 - INFO - __main__ - save last model!
06/18/2022 09:42:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 09:42:21 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 09:42:21 - INFO - __main__ - Printing 3 examples
06/18/2022 09:42:21 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 09:42:21 - INFO - __main__ - ['others']
06/18/2022 09:42:21 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 09:42:21 - INFO - __main__ - ['others']
06/18/2022 09:42:21 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 09:42:21 - INFO - __main__ - ['others']
06/18/2022 09:42:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 09:42:21 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:42:21 - INFO - __main__ - Printing 3 examples
06/18/2022 09:42:21 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/18/2022 09:42:21 - INFO - __main__ - ['others']
06/18/2022 09:42:21 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/18/2022 09:42:21 - INFO - __main__ - ['others']
06/18/2022 09:42:21 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/18/2022 09:42:21 - INFO - __main__ - ['others']
06/18/2022 09:42:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 09:42:22 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:42:22 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 09:42:22 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:42:22 - INFO - __main__ - Printing 3 examples
06/18/2022 09:42:22 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/18/2022 09:42:22 - INFO - __main__ - ['others']
06/18/2022 09:42:22 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/18/2022 09:42:22 - INFO - __main__ - ['others']
06/18/2022 09:42:22 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/18/2022 09:42:22 - INFO - __main__ - ['others']
06/18/2022 09:42:22 - INFO - __main__ - Tokenizing Input ...
06/18/2022 09:42:22 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:42:22 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 09:42:23 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:42:29 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 09:42:40 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 09:42:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 09:42:41 - INFO - __main__ - Starting training!
06/18/2022 09:44:02 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_87_0.5_8_predictions.txt
06/18/2022 09:44:02 - INFO - __main__ - Classification-F1 on test data: 0.2130
06/18/2022 09:44:02 - INFO - __main__ - prefix=emo_16_87, lr=0.5, bsz=8, dev_performance=0.7670155993431855, test_performance=0.21302472011341828
06/18/2022 09:44:02 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.4, bsz=8 ...
06/18/2022 09:44:03 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:44:03 - INFO - __main__ - Printing 3 examples
06/18/2022 09:44:03 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/18/2022 09:44:03 - INFO - __main__ - ['others']
06/18/2022 09:44:03 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/18/2022 09:44:03 - INFO - __main__ - ['others']
06/18/2022 09:44:03 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/18/2022 09:44:03 - INFO - __main__ - ['others']
06/18/2022 09:44:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 09:44:03 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:44:04 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 09:44:04 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:44:04 - INFO - __main__ - Printing 3 examples
06/18/2022 09:44:04 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/18/2022 09:44:04 - INFO - __main__ - ['others']
06/18/2022 09:44:04 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/18/2022 09:44:04 - INFO - __main__ - ['others']
06/18/2022 09:44:04 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/18/2022 09:44:04 - INFO - __main__ - ['others']
06/18/2022 09:44:04 - INFO - __main__ - Tokenizing Input ...
06/18/2022 09:44:04 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:44:04 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 09:44:22 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 09:44:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 09:44:23 - INFO - __main__ - Starting training!
06/18/2022 09:44:26 - INFO - __main__ - Step 10 Global step 10 Train loss 4.10 on epoch=2
06/18/2022 09:44:28 - INFO - __main__ - Step 20 Global step 20 Train loss 2.94 on epoch=4
06/18/2022 09:44:31 - INFO - __main__ - Step 30 Global step 30 Train loss 2.22 on epoch=7
06/18/2022 09:44:33 - INFO - __main__ - Step 40 Global step 40 Train loss 1.71 on epoch=9
06/18/2022 09:44:35 - INFO - __main__ - Step 50 Global step 50 Train loss 1.29 on epoch=12
06/18/2022 09:44:37 - INFO - __main__ - Global step 50 Train loss 2.45 Classification-F1 0.19115479115479114 on epoch=12
06/18/2022 09:44:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.19115479115479114 on epoch=12, global_step=50
06/18/2022 09:44:39 - INFO - __main__ - Step 60 Global step 60 Train loss 1.02 on epoch=14
06/18/2022 09:44:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.83 on epoch=17
06/18/2022 09:44:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.80 on epoch=19
06/18/2022 09:44:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.65 on epoch=22
06/18/2022 09:44:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.58 on epoch=24
06/18/2022 09:44:50 - INFO - __main__ - Global step 100 Train loss 0.78 Classification-F1 0.4625704045058884 on epoch=24
06/18/2022 09:44:50 - INFO - __main__ - Saving model with best Classification-F1: 0.19115479115479114 -> 0.4625704045058884 on epoch=24, global_step=100
06/18/2022 09:44:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.58 on epoch=27
06/18/2022 09:44:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=29
06/18/2022 09:44:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=32
06/18/2022 09:45:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=34
06/18/2022 09:45:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=37
06/18/2022 09:45:03 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.4625704045058884 on epoch=37
06/18/2022 09:45:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=39
06/18/2022 09:45:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=42
06/18/2022 09:45:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.37 on epoch=44
06/18/2022 09:45:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=47
06/18/2022 09:45:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=49
06/18/2022 09:45:16 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.5081135791662108 on epoch=49
06/18/2022 09:45:16 - INFO - __main__ - Saving model with best Classification-F1: 0.4625704045058884 -> 0.5081135791662108 on epoch=49, global_step=200
06/18/2022 09:45:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=52
06/18/2022 09:45:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=54
06/18/2022 09:45:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=57
06/18/2022 09:45:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.36 on epoch=59
06/18/2022 09:45:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=62
06/18/2022 09:45:29 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.4750777426153096 on epoch=62
06/18/2022 09:45:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=64
06/18/2022 09:45:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=67
06/18/2022 09:45:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=69
06/18/2022 09:45:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=72
06/18/2022 09:45:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=74
06/18/2022 09:45:43 - INFO - __main__ - Global step 300 Train loss 0.29 Classification-F1 0.5096810207336523 on epoch=74
06/18/2022 09:45:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5081135791662108 -> 0.5096810207336523 on epoch=74, global_step=300
06/18/2022 09:45:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=77
06/18/2022 09:45:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=79
06/18/2022 09:45:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=82
06/18/2022 09:45:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=84
06/18/2022 09:45:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.16 on epoch=87
06/18/2022 09:45:56 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.5082769472856019 on epoch=87
06/18/2022 09:45:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=89
06/18/2022 09:46:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.18 on epoch=92
06/18/2022 09:46:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=94
06/18/2022 09:46:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.18 on epoch=97
06/18/2022 09:46:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.17 on epoch=99
06/18/2022 09:46:09 - INFO - __main__ - Global step 400 Train loss 0.20 Classification-F1 0.6743535327151626 on epoch=99
06/18/2022 09:46:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5096810207336523 -> 0.6743535327151626 on epoch=99, global_step=400
06/18/2022 09:46:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.18 on epoch=102
06/18/2022 09:46:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=104
06/18/2022 09:46:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.18 on epoch=107
06/18/2022 09:46:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.16 on epoch=109
06/18/2022 09:46:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.12 on epoch=112
06/18/2022 09:46:22 - INFO - __main__ - Global step 450 Train loss 0.17 Classification-F1 0.6439628653043288 on epoch=112
06/18/2022 09:46:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.17 on epoch=114
06/18/2022 09:46:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.13 on epoch=117
06/18/2022 09:46:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=119
06/18/2022 09:46:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.09 on epoch=122
06/18/2022 09:46:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=124
06/18/2022 09:46:35 - INFO - __main__ - Global step 500 Train loss 0.12 Classification-F1 0.6743535327151626 on epoch=124
06/18/2022 09:46:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.15 on epoch=127
06/18/2022 09:46:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.16 on epoch=129
06/18/2022 09:46:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=132
06/18/2022 09:46:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.14 on epoch=134
06/18/2022 09:46:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.08 on epoch=137
06/18/2022 09:46:48 - INFO - __main__ - Global step 550 Train loss 0.14 Classification-F1 0.6765350877192983 on epoch=137
06/18/2022 09:46:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6743535327151626 -> 0.6765350877192983 on epoch=137, global_step=550
06/18/2022 09:46:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.13 on epoch=139
06/18/2022 09:46:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.09 on epoch=142
06/18/2022 09:46:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.08 on epoch=144
06/18/2022 09:46:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.10 on epoch=147
06/18/2022 09:47:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=149
06/18/2022 09:47:02 - INFO - __main__ - Global step 600 Train loss 0.10 Classification-F1 0.6736885316740127 on epoch=149
06/18/2022 09:47:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.11 on epoch=152
06/18/2022 09:47:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=154
06/18/2022 09:47:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=157
06/18/2022 09:47:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=159
06/18/2022 09:47:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=162
06/18/2022 09:47:15 - INFO - __main__ - Global step 650 Train loss 0.09 Classification-F1 0.6743535327151626 on epoch=162
06/18/2022 09:47:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=164
06/18/2022 09:47:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=167
06/18/2022 09:47:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.11 on epoch=169
06/18/2022 09:47:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=172
06/18/2022 09:47:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=174
06/18/2022 09:47:28 - INFO - __main__ - Global step 700 Train loss 0.08 Classification-F1 0.685993208828523 on epoch=174
06/18/2022 09:47:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6765350877192983 -> 0.685993208828523 on epoch=174, global_step=700
06/18/2022 09:47:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=177
06/18/2022 09:47:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=179
06/18/2022 09:47:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=182
06/18/2022 09:47:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=184
06/18/2022 09:47:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=187
06/18/2022 09:47:41 - INFO - __main__ - Global step 750 Train loss 0.06 Classification-F1 0.6600668337510444 on epoch=187
06/18/2022 09:47:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=189
06/18/2022 09:47:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=192
06/18/2022 09:47:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=194
06/18/2022 09:47:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=197
06/18/2022 09:47:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=199
06/18/2022 09:47:54 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.7121740455356754 on epoch=199
06/18/2022 09:47:54 - INFO - __main__ - Saving model with best Classification-F1: 0.685993208828523 -> 0.7121740455356754 on epoch=199, global_step=800
06/18/2022 09:47:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=202
06/18/2022 09:47:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=204
06/18/2022 09:48:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=207
06/18/2022 09:48:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=209
06/18/2022 09:48:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=212
06/18/2022 09:48:08 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.7121740455356754 on epoch=212
06/18/2022 09:48:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=214
06/18/2022 09:48:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=217
06/18/2022 09:48:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=219
06/18/2022 09:48:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=222
06/18/2022 09:48:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
06/18/2022 09:48:21 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.7121740455356754 on epoch=224
06/18/2022 09:48:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=227
06/18/2022 09:48:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=229
06/18/2022 09:48:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=232
06/18/2022 09:48:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=234
06/18/2022 09:48:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=237
06/18/2022 09:48:34 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.7121740455356754 on epoch=237
06/18/2022 09:48:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=239
06/18/2022 09:48:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=242
06/18/2022 09:48:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=244
06/18/2022 09:48:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=247
06/18/2022 09:48:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=249
06/18/2022 09:48:47 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.6996523566981971 on epoch=249
06/18/2022 09:48:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
06/18/2022 09:48:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=254
06/18/2022 09:48:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
06/18/2022 09:48:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=259
06/18/2022 09:48:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=262
06/18/2022 09:49:00 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.7015808752650858 on epoch=262
06/18/2022 09:49:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
06/18/2022 09:49:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
06/18/2022 09:49:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=269
06/18/2022 09:49:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
06/18/2022 09:49:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=274
06/18/2022 09:49:13 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.6996523566981971 on epoch=274
06/18/2022 09:49:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
06/18/2022 09:49:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
06/18/2022 09:49:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
06/18/2022 09:49:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=284
06/18/2022 09:49:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
06/18/2022 09:49:26 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.7015808752650858 on epoch=287
06/18/2022 09:49:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
06/18/2022 09:49:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
06/18/2022 09:49:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
06/18/2022 09:49:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
06/18/2022 09:49:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
06/18/2022 09:49:40 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.6765350877192983 on epoch=299
06/18/2022 09:49:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
06/18/2022 09:49:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=304
06/18/2022 09:49:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
06/18/2022 09:49:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
06/18/2022 09:49:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
06/18/2022 09:49:53 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.6765350877192983 on epoch=312
06/18/2022 09:49:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
06/18/2022 09:49:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
06/18/2022 09:50:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=319
06/18/2022 09:50:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
06/18/2022 09:50:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
06/18/2022 09:50:06 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.7015808752650858 on epoch=324
06/18/2022 09:50:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
06/18/2022 09:50:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=329
06/18/2022 09:50:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
06/18/2022 09:50:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
06/18/2022 09:50:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
06/18/2022 09:50:19 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.7015808752650858 on epoch=337
06/18/2022 09:50:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
06/18/2022 09:50:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
06/18/2022 09:50:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
06/18/2022 09:50:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
06/18/2022 09:50:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
06/18/2022 09:50:33 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.7015808752650858 on epoch=349
06/18/2022 09:50:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=352
06/18/2022 09:50:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
06/18/2022 09:50:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
06/18/2022 09:50:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
06/18/2022 09:50:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
06/18/2022 09:50:46 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.7015808752650858 on epoch=362
06/18/2022 09:50:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
06/18/2022 09:50:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
06/18/2022 09:50:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=369
06/18/2022 09:50:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
06/18/2022 09:50:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
06/18/2022 09:50:59 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.7015808752650858 on epoch=374
06/18/2022 09:51:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
06/18/2022 09:51:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
06/18/2022 09:51:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=382
06/18/2022 09:51:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
06/18/2022 09:51:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
06/18/2022 09:51:13 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.7015808752650858 on epoch=387
06/18/2022 09:51:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
06/18/2022 09:51:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
06/18/2022 09:51:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
06/18/2022 09:51:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
06/18/2022 09:51:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
06/18/2022 09:51:26 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.6996523566981971 on epoch=399
06/18/2022 09:51:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/18/2022 09:51:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
06/18/2022 09:51:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
06/18/2022 09:51:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/18/2022 09:51:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
06/18/2022 09:51:39 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7015808752650858 on epoch=412
06/18/2022 09:51:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
06/18/2022 09:51:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
06/18/2022 09:51:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
06/18/2022 09:51:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
06/18/2022 09:51:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
06/18/2022 09:51:53 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.7015808752650858 on epoch=424
06/18/2022 09:51:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=427
06/18/2022 09:51:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
06/18/2022 09:52:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/18/2022 09:52:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
06/18/2022 09:52:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
06/18/2022 09:52:06 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.6996523566981971 on epoch=437
06/18/2022 09:52:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
06/18/2022 09:52:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
06/18/2022 09:52:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
06/18/2022 09:52:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
06/18/2022 09:52:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
06/18/2022 09:52:20 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6996523566981971 on epoch=449
06/18/2022 09:52:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
06/18/2022 09:52:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
06/18/2022 09:52:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/18/2022 09:52:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
06/18/2022 09:52:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
06/18/2022 09:52:33 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6996523566981971 on epoch=462
06/18/2022 09:52:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/18/2022 09:52:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
06/18/2022 09:52:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
06/18/2022 09:52:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
06/18/2022 09:52:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
06/18/2022 09:52:46 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.6996523566981971 on epoch=474
06/18/2022 09:52:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
06/18/2022 09:52:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
06/18/2022 09:52:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
06/18/2022 09:52:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/18/2022 09:52:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
06/18/2022 09:53:00 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7015808752650858 on epoch=487
06/18/2022 09:53:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
06/18/2022 09:53:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/18/2022 09:53:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
06/18/2022 09:53:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=497
06/18/2022 09:53:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
06/18/2022 09:53:13 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6996523566981971 on epoch=499
06/18/2022 09:53:16 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/18/2022 09:53:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
06/18/2022 09:53:21 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
06/18/2022 09:53:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/18/2022 09:53:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
06/18/2022 09:53:27 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6996523566981971 on epoch=512
06/18/2022 09:53:29 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
06/18/2022 09:53:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
06/18/2022 09:53:34 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
06/18/2022 09:53:37 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
06/18/2022 09:53:39 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/18/2022 09:53:40 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7015808752650858 on epoch=524
06/18/2022 09:53:43 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
06/18/2022 09:53:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
06/18/2022 09:53:48 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/18/2022 09:53:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
06/18/2022 09:53:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/18/2022 09:53:54 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.6867500315776178 on epoch=537
06/18/2022 09:53:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/18/2022 09:53:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
06/18/2022 09:54:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/18/2022 09:54:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/18/2022 09:54:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/18/2022 09:54:07 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6996523566981971 on epoch=549
06/18/2022 09:54:09 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/18/2022 09:54:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/18/2022 09:54:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/18/2022 09:54:17 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
06/18/2022 09:54:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
06/18/2022 09:54:20 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.7015808752650858 on epoch=562
06/18/2022 09:54:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
06/18/2022 09:54:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
06/18/2022 09:54:28 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/18/2022 09:54:30 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/18/2022 09:54:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/18/2022 09:54:34 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.6867500315776178 on epoch=574
06/18/2022 09:54:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/18/2022 09:54:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/18/2022 09:54:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
06/18/2022 09:54:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
06/18/2022 09:54:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
06/18/2022 09:54:47 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7015808752650858 on epoch=587
06/18/2022 09:54:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/18/2022 09:54:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/18/2022 09:54:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/18/2022 09:54:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/18/2022 09:55:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/18/2022 09:55:01 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.6867500315776178 on epoch=599
06/18/2022 09:55:03 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
06/18/2022 09:55:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/18/2022 09:55:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/18/2022 09:55:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=609
06/18/2022 09:55:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/18/2022 09:55:14 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.684920705479916 on epoch=612
06/18/2022 09:55:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/18/2022 09:55:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/18/2022 09:55:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
06/18/2022 09:55:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/18/2022 09:55:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/18/2022 09:55:28 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6996523566981971 on epoch=624
06/18/2022 09:55:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
06/18/2022 09:55:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=629
06/18/2022 09:55:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/18/2022 09:55:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/18/2022 09:55:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
06/18/2022 09:55:41 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.684920705479916 on epoch=637
06/18/2022 09:55:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/18/2022 09:55:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
06/18/2022 09:55:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
06/18/2022 09:55:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/18/2022 09:55:54 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=649
06/18/2022 09:55:55 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.6996523566981971 on epoch=649
06/18/2022 09:55:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
06/18/2022 09:56:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/18/2022 09:56:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/18/2022 09:56:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 09:56:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/18/2022 09:56:08 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6849142085984191 on epoch=662
06/18/2022 09:56:11 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/18/2022 09:56:13 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/18/2022 09:56:16 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
06/18/2022 09:56:18 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/18/2022 09:56:21 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/18/2022 09:56:22 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6849142085984191 on epoch=674
06/18/2022 09:56:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/18/2022 09:56:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
06/18/2022 09:56:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/18/2022 09:56:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/18/2022 09:56:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=687
06/18/2022 09:56:35 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6996523566981971 on epoch=687
06/18/2022 09:56:38 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 09:56:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/18/2022 09:56:43 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
06/18/2022 09:56:45 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/18/2022 09:56:48 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/18/2022 09:56:49 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.684920705479916 on epoch=699
06/18/2022 09:56:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/18/2022 09:56:54 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/18/2022 09:56:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/18/2022 09:56:59 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 09:57:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/18/2022 09:57:02 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.684920705479916 on epoch=712
06/18/2022 09:57:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/18/2022 09:57:07 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=717
06/18/2022 09:57:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/18/2022 09:57:12 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/18/2022 09:57:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/18/2022 09:57:16 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6996523566981971 on epoch=724
06/18/2022 09:57:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/18/2022 09:57:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/18/2022 09:57:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/18/2022 09:57:26 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 09:57:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/18/2022 09:57:29 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.684920705479916 on epoch=737
06/18/2022 09:57:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 09:57:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
06/18/2022 09:57:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
06/18/2022 09:57:39 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/18/2022 09:57:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
06/18/2022 09:57:43 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6996523566981971 on epoch=749
06/18/2022 09:57:43 - INFO - __main__ - save last model!
06/18/2022 09:57:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 09:57:43 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 09:57:43 - INFO - __main__ - Printing 3 examples
06/18/2022 09:57:43 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 09:57:43 - INFO - __main__ - ['others']
06/18/2022 09:57:43 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 09:57:43 - INFO - __main__ - ['others']
06/18/2022 09:57:43 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 09:57:43 - INFO - __main__ - ['others']
06/18/2022 09:57:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 09:57:43 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:57:43 - INFO - __main__ - Printing 3 examples
06/18/2022 09:57:43 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/18/2022 09:57:43 - INFO - __main__ - ['others']
06/18/2022 09:57:43 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/18/2022 09:57:43 - INFO - __main__ - ['others']
06/18/2022 09:57:43 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/18/2022 09:57:43 - INFO - __main__ - ['others']
06/18/2022 09:57:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 09:57:43 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:57:43 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 09:57:43 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:57:43 - INFO - __main__ - Printing 3 examples
06/18/2022 09:57:43 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/18/2022 09:57:43 - INFO - __main__ - ['others']
06/18/2022 09:57:43 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/18/2022 09:57:43 - INFO - __main__ - ['others']
06/18/2022 09:57:43 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/18/2022 09:57:43 - INFO - __main__ - ['others']
06/18/2022 09:57:43 - INFO - __main__ - Tokenizing Input ...
06/18/2022 09:57:43 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:57:43 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 09:57:45 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:57:51 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 09:57:59 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 09:58:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 09:58:00 - INFO - __main__ - Starting training!
06/18/2022 09:59:25 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_87_0.4_8_predictions.txt
06/18/2022 09:59:25 - INFO - __main__ - Classification-F1 on test data: 0.1777
06/18/2022 09:59:25 - INFO - __main__ - prefix=emo_16_87, lr=0.4, bsz=8, dev_performance=0.7121740455356754, test_performance=0.17772940375738178
06/18/2022 09:59:25 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.3, bsz=8 ...
06/18/2022 09:59:26 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:59:26 - INFO - __main__ - Printing 3 examples
06/18/2022 09:59:26 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/18/2022 09:59:26 - INFO - __main__ - ['others']
06/18/2022 09:59:26 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/18/2022 09:59:26 - INFO - __main__ - ['others']
06/18/2022 09:59:26 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/18/2022 09:59:26 - INFO - __main__ - ['others']
06/18/2022 09:59:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 09:59:26 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:59:26 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 09:59:26 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 09:59:26 - INFO - __main__ - Printing 3 examples
06/18/2022 09:59:26 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/18/2022 09:59:26 - INFO - __main__ - ['others']
06/18/2022 09:59:26 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/18/2022 09:59:26 - INFO - __main__ - ['others']
06/18/2022 09:59:26 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/18/2022 09:59:26 - INFO - __main__ - ['others']
06/18/2022 09:59:26 - INFO - __main__ - Tokenizing Input ...
06/18/2022 09:59:26 - INFO - __main__ - Tokenizing Output ...
06/18/2022 09:59:26 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 09:59:41 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 09:59:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 09:59:42 - INFO - __main__ - Starting training!
06/18/2022 09:59:45 - INFO - __main__ - Step 10 Global step 10 Train loss 4.07 on epoch=2
06/18/2022 09:59:48 - INFO - __main__ - Step 20 Global step 20 Train loss 3.13 on epoch=4
06/18/2022 09:59:50 - INFO - __main__ - Step 30 Global step 30 Train loss 2.47 on epoch=7
06/18/2022 09:59:52 - INFO - __main__ - Step 40 Global step 40 Train loss 1.98 on epoch=9
06/18/2022 09:59:55 - INFO - __main__ - Step 50 Global step 50 Train loss 1.68 on epoch=12
06/18/2022 09:59:56 - INFO - __main__ - Global step 50 Train loss 2.67 Classification-F1 0.13470463470463473 on epoch=12
06/18/2022 09:59:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13470463470463473 on epoch=12, global_step=50
06/18/2022 09:59:58 - INFO - __main__ - Step 60 Global step 60 Train loss 1.37 on epoch=14
06/18/2022 10:00:01 - INFO - __main__ - Step 70 Global step 70 Train loss 1.15 on epoch=17
06/18/2022 10:00:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.94 on epoch=19
06/18/2022 10:00:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=22
06/18/2022 10:00:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.66 on epoch=24
06/18/2022 10:00:08 - INFO - __main__ - Global step 100 Train loss 0.99 Classification-F1 0.4247619047619048 on epoch=24
06/18/2022 10:00:08 - INFO - __main__ - Saving model with best Classification-F1: 0.13470463470463473 -> 0.4247619047619048 on epoch=24, global_step=100
06/18/2022 10:00:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.79 on epoch=27
06/18/2022 10:00:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.71 on epoch=29
06/18/2022 10:00:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.61 on epoch=32
06/18/2022 10:00:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.65 on epoch=34
06/18/2022 10:00:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=37
06/18/2022 10:00:21 - INFO - __main__ - Global step 150 Train loss 0.66 Classification-F1 0.5433006535947713 on epoch=37
06/18/2022 10:00:21 - INFO - __main__ - Saving model with best Classification-F1: 0.4247619047619048 -> 0.5433006535947713 on epoch=37, global_step=150
06/18/2022 10:00:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=39
06/18/2022 10:00:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.58 on epoch=42
06/18/2022 10:00:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.61 on epoch=44
06/18/2022 10:00:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=47
06/18/2022 10:00:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=49
06/18/2022 10:00:34 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.5860339040866424 on epoch=49
06/18/2022 10:00:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5433006535947713 -> 0.5860339040866424 on epoch=49, global_step=200
06/18/2022 10:00:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=52
06/18/2022 10:00:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=54
06/18/2022 10:00:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=57
06/18/2022 10:00:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=59
06/18/2022 10:00:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=62
06/18/2022 10:00:46 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.6454268292682928 on epoch=62
06/18/2022 10:00:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5860339040866424 -> 0.6454268292682928 on epoch=62, global_step=250
06/18/2022 10:00:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=64
06/18/2022 10:00:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.33 on epoch=67
06/18/2022 10:00:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=69
06/18/2022 10:00:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=72
06/18/2022 10:00:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=74
06/18/2022 10:00:58 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.6307560903149139 on epoch=74
06/18/2022 10:01:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=77
06/18/2022 10:01:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=79
06/18/2022 10:01:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=82
06/18/2022 10:01:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=84
06/18/2022 10:01:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.29 on epoch=87
06/18/2022 10:01:11 - INFO - __main__ - Global step 350 Train loss 0.32 Classification-F1 0.6323593073593073 on epoch=87
06/18/2022 10:01:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=89
06/18/2022 10:01:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=92
06/18/2022 10:01:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.32 on epoch=94
06/18/2022 10:01:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=97
06/18/2022 10:01:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=99
06/18/2022 10:01:23 - INFO - __main__ - Global step 400 Train loss 0.32 Classification-F1 0.6287693223177094 on epoch=99
06/18/2022 10:01:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=102
06/18/2022 10:01:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=104
06/18/2022 10:01:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=107
06/18/2022 10:01:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=109
06/18/2022 10:01:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=112
06/18/2022 10:01:36 - INFO - __main__ - Global step 450 Train loss 0.29 Classification-F1 0.6414116349600222 on epoch=112
06/18/2022 10:01:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=114
06/18/2022 10:01:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=117
06/18/2022 10:01:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=119
06/18/2022 10:01:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=122
06/18/2022 10:01:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=124
06/18/2022 10:01:49 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.6714377406931964 on epoch=124
06/18/2022 10:01:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6454268292682928 -> 0.6714377406931964 on epoch=124, global_step=500
06/18/2022 10:01:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=127
06/18/2022 10:01:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.18 on epoch=129
06/18/2022 10:01:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=132
06/18/2022 10:01:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=134
06/18/2022 10:02:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.15 on epoch=137
06/18/2022 10:02:01 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.6714377406931964 on epoch=137
06/18/2022 10:02:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=139
06/18/2022 10:02:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=142
06/18/2022 10:02:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=144
06/18/2022 10:02:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=147
06/18/2022 10:02:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=149
06/18/2022 10:02:14 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.684250398724083 on epoch=149
06/18/2022 10:02:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6714377406931964 -> 0.684250398724083 on epoch=149, global_step=600
06/18/2022 10:02:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=152
06/18/2022 10:02:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=154
06/18/2022 10:02:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=157
06/18/2022 10:02:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=159
06/18/2022 10:02:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.15 on epoch=162
06/18/2022 10:02:26 - INFO - __main__ - Global step 650 Train loss 0.15 Classification-F1 0.6731353950103951 on epoch=162
06/18/2022 10:02:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=164
06/18/2022 10:02:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=167
06/18/2022 10:02:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=169
06/18/2022 10:02:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=172
06/18/2022 10:02:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=174
06/18/2022 10:02:39 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.685993208828523 on epoch=174
06/18/2022 10:02:39 - INFO - __main__ - Saving model with best Classification-F1: 0.684250398724083 -> 0.685993208828523 on epoch=174, global_step=700
06/18/2022 10:02:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=177
06/18/2022 10:02:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=179
06/18/2022 10:02:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=182
06/18/2022 10:02:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=184
06/18/2022 10:02:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=187
06/18/2022 10:02:51 - INFO - __main__ - Global step 750 Train loss 0.14 Classification-F1 0.6855276529738982 on epoch=187
06/18/2022 10:02:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=189
06/18/2022 10:02:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=192
06/18/2022 10:02:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.11 on epoch=194
06/18/2022 10:03:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=197
06/18/2022 10:03:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=199
06/18/2022 10:03:04 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.6855276529738982 on epoch=199
06/18/2022 10:03:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=202
06/18/2022 10:03:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=204
06/18/2022 10:03:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=207
06/18/2022 10:03:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=209
06/18/2022 10:03:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=212
06/18/2022 10:03:17 - INFO - __main__ - Global step 850 Train loss 0.09 Classification-F1 0.6864494416981063 on epoch=212
06/18/2022 10:03:17 - INFO - __main__ - Saving model with best Classification-F1: 0.685993208828523 -> 0.6864494416981063 on epoch=212, global_step=850
06/18/2022 10:03:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=214
06/18/2022 10:03:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=217
06/18/2022 10:03:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=219
06/18/2022 10:03:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=222
06/18/2022 10:03:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=224
06/18/2022 10:03:29 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.6864494416981063 on epoch=224
06/18/2022 10:03:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=227
06/18/2022 10:03:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=229
06/18/2022 10:03:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=232
06/18/2022 10:03:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=234
06/18/2022 10:03:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=237
06/18/2022 10:03:42 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.6765350877192983 on epoch=237
06/18/2022 10:03:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=239
06/18/2022 10:03:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=242
06/18/2022 10:03:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=244
06/18/2022 10:03:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=247
06/18/2022 10:03:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=249
06/18/2022 10:03:54 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.6743535327151626 on epoch=249
06/18/2022 10:03:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
06/18/2022 10:03:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=254
06/18/2022 10:04:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=257
06/18/2022 10:04:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=259
06/18/2022 10:04:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=262
06/18/2022 10:04:07 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.6765350877192983 on epoch=262
06/18/2022 10:04:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=264
06/18/2022 10:04:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
06/18/2022 10:04:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=269
06/18/2022 10:04:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
06/18/2022 10:04:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=274
06/18/2022 10:04:19 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.6765350877192983 on epoch=274
06/18/2022 10:04:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
06/18/2022 10:04:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
06/18/2022 10:04:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=282
06/18/2022 10:04:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=284
06/18/2022 10:04:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
06/18/2022 10:04:32 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.7015808752650858 on epoch=287
06/18/2022 10:04:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6864494416981063 -> 0.7015808752650858 on epoch=287, global_step=1150
06/18/2022 10:04:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=289
06/18/2022 10:04:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
06/18/2022 10:04:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
06/18/2022 10:04:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
06/18/2022 10:04:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=299
06/18/2022 10:04:45 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.6765350877192983 on epoch=299
06/18/2022 10:04:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
06/18/2022 10:04:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
06/18/2022 10:04:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
06/18/2022 10:04:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
06/18/2022 10:04:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
06/18/2022 10:04:58 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.6765350877192983 on epoch=312
06/18/2022 10:05:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=314
06/18/2022 10:05:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
06/18/2022 10:05:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
06/18/2022 10:05:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
06/18/2022 10:05:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=324
06/18/2022 10:05:10 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.6495934959349593 on epoch=324
06/18/2022 10:05:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
06/18/2022 10:05:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
06/18/2022 10:05:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
06/18/2022 10:05:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
06/18/2022 10:05:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
06/18/2022 10:05:23 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.6765350877192983 on epoch=337
06/18/2022 10:05:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=339
06/18/2022 10:05:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
06/18/2022 10:05:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=344
06/18/2022 10:05:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=347
06/18/2022 10:05:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
06/18/2022 10:05:35 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6765350877192983 on epoch=349
06/18/2022 10:05:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=352
06/18/2022 10:05:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
06/18/2022 10:05:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
06/18/2022 10:05:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
06/18/2022 10:05:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
06/18/2022 10:05:48 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.6765350877192983 on epoch=362
06/18/2022 10:05:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
06/18/2022 10:05:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
06/18/2022 10:05:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
06/18/2022 10:05:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
06/18/2022 10:06:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
06/18/2022 10:06:00 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.6765350877192983 on epoch=374
06/18/2022 10:06:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
06/18/2022 10:06:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
06/18/2022 10:06:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/18/2022 10:06:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
06/18/2022 10:06:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
06/18/2022 10:06:13 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.6765350877192983 on epoch=387
06/18/2022 10:06:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
06/18/2022 10:06:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
06/18/2022 10:06:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
06/18/2022 10:06:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
06/18/2022 10:06:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=399
06/18/2022 10:06:26 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.6765350877192983 on epoch=399
06/18/2022 10:06:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=402
06/18/2022 10:06:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
06/18/2022 10:06:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
06/18/2022 10:06:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/18/2022 10:06:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=412
06/18/2022 10:06:38 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.7015808752650858 on epoch=412
06/18/2022 10:06:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
06/18/2022 10:06:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
06/18/2022 10:06:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=419
06/18/2022 10:06:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
06/18/2022 10:06:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
06/18/2022 10:06:51 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.6765350877192983 on epoch=424
06/18/2022 10:06:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
06/18/2022 10:06:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/18/2022 10:06:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=432
06/18/2022 10:07:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
06/18/2022 10:07:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
06/18/2022 10:07:04 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7250398724082935 on epoch=437
06/18/2022 10:07:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7015808752650858 -> 0.7250398724082935 on epoch=437, global_step=1750
06/18/2022 10:07:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
06/18/2022 10:07:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
06/18/2022 10:07:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
06/18/2022 10:07:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
06/18/2022 10:07:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
06/18/2022 10:07:17 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6495934959349593 on epoch=449
06/18/2022 10:07:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
06/18/2022 10:07:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
06/18/2022 10:07:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/18/2022 10:07:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
06/18/2022 10:07:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
06/18/2022 10:07:29 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6347626522474914 on epoch=462
06/18/2022 10:07:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/18/2022 10:07:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/18/2022 10:07:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/18/2022 10:07:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
06/18/2022 10:07:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
06/18/2022 10:07:42 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6765350877192983 on epoch=474
06/18/2022 10:07:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
06/18/2022 10:07:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
06/18/2022 10:07:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/18/2022 10:07:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=484
06/18/2022 10:07:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
06/18/2022 10:07:55 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.6867500315776178 on epoch=487
06/18/2022 10:07:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
06/18/2022 10:07:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
06/18/2022 10:08:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
06/18/2022 10:08:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
06/18/2022 10:08:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
06/18/2022 10:08:07 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6765350877192983 on epoch=499
06/18/2022 10:08:09 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
06/18/2022 10:08:12 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
06/18/2022 10:08:14 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.21 on epoch=507
06/18/2022 10:08:17 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
06/18/2022 10:08:19 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/18/2022 10:08:20 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.6615330198946497 on epoch=512
06/18/2022 10:08:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/18/2022 10:08:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/18/2022 10:08:27 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
06/18/2022 10:08:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
06/18/2022 10:08:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/18/2022 10:08:33 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6495934959349593 on epoch=524
06/18/2022 10:08:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=527
06/18/2022 10:08:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.08 on epoch=529
06/18/2022 10:08:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/18/2022 10:08:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
06/18/2022 10:08:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/18/2022 10:08:45 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.6495934959349593 on epoch=537
06/18/2022 10:08:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/18/2022 10:08:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
06/18/2022 10:08:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
06/18/2022 10:08:55 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/18/2022 10:08:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
06/18/2022 10:08:58 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6495934959349593 on epoch=549
06/18/2022 10:09:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
06/18/2022 10:09:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
06/18/2022 10:09:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
06/18/2022 10:09:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/18/2022 10:09:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/18/2022 10:09:11 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6495934959349593 on epoch=562
06/18/2022 10:09:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
06/18/2022 10:09:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
06/18/2022 10:09:19 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/18/2022 10:09:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
06/18/2022 10:09:24 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/18/2022 10:09:25 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6495934959349593 on epoch=574
06/18/2022 10:09:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/18/2022 10:09:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=579
06/18/2022 10:09:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
06/18/2022 10:09:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
06/18/2022 10:09:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/18/2022 10:09:38 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7015808752650858 on epoch=587
06/18/2022 10:09:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/18/2022 10:09:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/18/2022 10:09:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/18/2022 10:09:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
06/18/2022 10:09:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
06/18/2022 10:09:51 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6765350877192983 on epoch=599
06/18/2022 10:09:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=602
06/18/2022 10:09:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/18/2022 10:09:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/18/2022 10:10:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
06/18/2022 10:10:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/18/2022 10:10:04 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6495934959349593 on epoch=612
06/18/2022 10:10:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/18/2022 10:10:09 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/18/2022 10:10:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/18/2022 10:10:14 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=622
06/18/2022 10:10:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
06/18/2022 10:10:18 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6765350877192983 on epoch=624
06/18/2022 10:10:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
06/18/2022 10:10:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
06/18/2022 10:10:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/18/2022 10:10:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
06/18/2022 10:10:30 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/18/2022 10:10:31 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6743535327151626 on epoch=637
06/18/2022 10:10:34 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/18/2022 10:10:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/18/2022 10:10:39 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
06/18/2022 10:10:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/18/2022 10:10:44 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/18/2022 10:10:45 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6495934959349593 on epoch=649
06/18/2022 10:10:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
06/18/2022 10:10:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/18/2022 10:10:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/18/2022 10:10:55 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 10:10:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
06/18/2022 10:10:58 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6743535327151626 on epoch=662
06/18/2022 10:11:01 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/18/2022 10:11:03 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/18/2022 10:11:06 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/18/2022 10:11:08 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/18/2022 10:11:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/18/2022 10:11:12 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6495934959349593 on epoch=674
06/18/2022 10:11:14 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/18/2022 10:11:17 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
06/18/2022 10:11:19 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
06/18/2022 10:11:21 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/18/2022 10:11:24 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
06/18/2022 10:11:25 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6471774193548387 on epoch=687
06/18/2022 10:11:27 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/18/2022 10:11:30 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
06/18/2022 10:11:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
06/18/2022 10:11:35 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/18/2022 10:11:37 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
06/18/2022 10:11:39 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7015808752650858 on epoch=699
06/18/2022 10:11:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
06/18/2022 10:11:43 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/18/2022 10:11:46 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/18/2022 10:11:48 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/18/2022 10:11:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/18/2022 10:11:52 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7015808752650858 on epoch=712
06/18/2022 10:11:54 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/18/2022 10:11:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/18/2022 10:11:59 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/18/2022 10:12:02 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
06/18/2022 10:12:04 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/18/2022 10:12:05 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7015808752650858 on epoch=724
06/18/2022 10:12:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
06/18/2022 10:12:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=729
06/18/2022 10:12:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/18/2022 10:12:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 10:12:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
06/18/2022 10:12:19 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.6743535327151626 on epoch=737
06/18/2022 10:12:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/18/2022 10:12:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/18/2022 10:12:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/18/2022 10:12:29 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/18/2022 10:12:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
06/18/2022 10:12:32 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7015808752650858 on epoch=749
06/18/2022 10:12:32 - INFO - __main__ - save last model!
06/18/2022 10:12:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 10:12:32 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 10:12:32 - INFO - __main__ - Printing 3 examples
06/18/2022 10:12:32 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 10:12:32 - INFO - __main__ - ['others']
06/18/2022 10:12:32 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 10:12:32 - INFO - __main__ - ['others']
06/18/2022 10:12:32 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 10:12:32 - INFO - __main__ - ['others']
06/18/2022 10:12:32 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 10:12:32 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 10:12:32 - INFO - __main__ - Printing 3 examples
06/18/2022 10:12:32 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/18/2022 10:12:32 - INFO - __main__ - ['others']
06/18/2022 10:12:32 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/18/2022 10:12:32 - INFO - __main__ - ['others']
06/18/2022 10:12:32 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/18/2022 10:12:32 - INFO - __main__ - ['others']
06/18/2022 10:12:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 10:12:32 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:12:32 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 10:12:32 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 10:12:32 - INFO - __main__ - Printing 3 examples
06/18/2022 10:12:32 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/18/2022 10:12:32 - INFO - __main__ - ['others']
06/18/2022 10:12:32 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/18/2022 10:12:32 - INFO - __main__ - ['others']
06/18/2022 10:12:32 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/18/2022 10:12:32 - INFO - __main__ - ['others']
06/18/2022 10:12:32 - INFO - __main__ - Tokenizing Input ...
06/18/2022 10:12:32 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:12:33 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 10:12:34 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:12:40 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 10:12:51 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 10:12:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 10:12:52 - INFO - __main__ - Starting training!
06/18/2022 10:14:13 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_87_0.3_8_predictions.txt
06/18/2022 10:14:13 - INFO - __main__ - Classification-F1 on test data: 0.1283
06/18/2022 10:14:13 - INFO - __main__ - prefix=emo_16_87, lr=0.3, bsz=8, dev_performance=0.7250398724082935, test_performance=0.12834319711019185
06/18/2022 10:14:13 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.2, bsz=8 ...
06/18/2022 10:14:14 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 10:14:14 - INFO - __main__ - Printing 3 examples
06/18/2022 10:14:14 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/18/2022 10:14:14 - INFO - __main__ - ['others']
06/18/2022 10:14:14 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/18/2022 10:14:14 - INFO - __main__ - ['others']
06/18/2022 10:14:14 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/18/2022 10:14:14 - INFO - __main__ - ['others']
06/18/2022 10:14:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 10:14:14 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:14:14 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
06/18/2022 10:14:14 - INFO - __main__ - Start tokenizing ... 64 instances
06/18/2022 10:14:14 - INFO - __main__ - Printing 3 examples
06/18/2022 10:14:14 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/18/2022 10:14:14 - INFO - __main__ - ['others']
06/18/2022 10:14:14 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/18/2022 10:14:14 - INFO - __main__ - ['others']
06/18/2022 10:14:14 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/18/2022 10:14:14 - INFO - __main__ - ['others']
06/18/2022 10:14:14 - INFO - __main__ - Tokenizing Input ...
06/18/2022 10:14:14 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:14:14 - INFO - __main__ - Loaded 64 examples from dev data
06/18/2022 10:14:33 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 10:14:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 10:14:33 - INFO - __main__ - Starting training!
06/18/2022 10:14:37 - INFO - __main__ - Step 10 Global step 10 Train loss 4.35 on epoch=2
06/18/2022 10:14:39 - INFO - __main__ - Step 20 Global step 20 Train loss 3.53 on epoch=4
06/18/2022 10:14:42 - INFO - __main__ - Step 30 Global step 30 Train loss 2.96 on epoch=7
06/18/2022 10:14:44 - INFO - __main__ - Step 40 Global step 40 Train loss 2.53 on epoch=9
06/18/2022 10:14:47 - INFO - __main__ - Step 50 Global step 50 Train loss 2.20 on epoch=12
06/18/2022 10:14:48 - INFO - __main__ - Global step 50 Train loss 3.12 Classification-F1 0.022162619988706944 on epoch=12
06/18/2022 10:14:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.022162619988706944 on epoch=12, global_step=50
06/18/2022 10:14:50 - INFO - __main__ - Step 60 Global step 60 Train loss 1.79 on epoch=14
06/18/2022 10:14:53 - INFO - __main__ - Step 70 Global step 70 Train loss 1.70 on epoch=17
06/18/2022 10:14:55 - INFO - __main__ - Step 80 Global step 80 Train loss 1.51 on epoch=19
06/18/2022 10:14:58 - INFO - __main__ - Step 90 Global step 90 Train loss 1.25 on epoch=22
06/18/2022 10:15:00 - INFO - __main__ - Step 100 Global step 100 Train loss 1.18 on epoch=24
06/18/2022 10:15:01 - INFO - __main__ - Global step 100 Train loss 1.49 Classification-F1 0.26274413371187566 on epoch=24
06/18/2022 10:15:02 - INFO - __main__ - Saving model with best Classification-F1: 0.022162619988706944 -> 0.26274413371187566 on epoch=24, global_step=100
06/18/2022 10:15:04 - INFO - __main__ - Step 110 Global step 110 Train loss 1.04 on epoch=27
06/18/2022 10:15:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.90 on epoch=29
06/18/2022 10:15:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.87 on epoch=32
06/18/2022 10:15:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.71 on epoch=34
06/18/2022 10:15:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.76 on epoch=37
06/18/2022 10:15:15 - INFO - __main__ - Global step 150 Train loss 0.85 Classification-F1 0.5714151317092494 on epoch=37
06/18/2022 10:15:15 - INFO - __main__ - Saving model with best Classification-F1: 0.26274413371187566 -> 0.5714151317092494 on epoch=37, global_step=150
06/18/2022 10:15:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.72 on epoch=39
06/18/2022 10:15:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.65 on epoch=42
06/18/2022 10:15:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.70 on epoch=44
06/18/2022 10:15:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=47
06/18/2022 10:15:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=49
06/18/2022 10:15:28 - INFO - __main__ - Global step 200 Train loss 0.65 Classification-F1 0.5902495306633292 on epoch=49
06/18/2022 10:15:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5714151317092494 -> 0.5902495306633292 on epoch=49, global_step=200
06/18/2022 10:15:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.62 on epoch=52
06/18/2022 10:15:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=54
06/18/2022 10:15:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=57
06/18/2022 10:15:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=59
06/18/2022 10:15:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=62
06/18/2022 10:15:42 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.5908010012515644 on epoch=62
06/18/2022 10:15:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5902495306633292 -> 0.5908010012515644 on epoch=62, global_step=250
06/18/2022 10:15:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=64
06/18/2022 10:15:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=67
06/18/2022 10:15:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.62 on epoch=69
06/18/2022 10:15:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=72
06/18/2022 10:15:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=74
06/18/2022 10:15:55 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.6038576508422231 on epoch=74
06/18/2022 10:15:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5908010012515644 -> 0.6038576508422231 on epoch=74, global_step=300
06/18/2022 10:15:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=77
06/18/2022 10:16:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=79
06/18/2022 10:16:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=82
06/18/2022 10:16:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.57 on epoch=84
06/18/2022 10:16:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=87
06/18/2022 10:16:08 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.6161616161616161 on epoch=87
06/18/2022 10:16:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6038576508422231 -> 0.6161616161616161 on epoch=87, global_step=350
06/18/2022 10:16:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=89
06/18/2022 10:16:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=92
06/18/2022 10:16:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.33 on epoch=94
06/18/2022 10:16:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=97
06/18/2022 10:16:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=99
06/18/2022 10:16:22 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.6307560903149139 on epoch=99
06/18/2022 10:16:22 - INFO - __main__ - Saving model with best Classification-F1: 0.6161616161616161 -> 0.6307560903149139 on epoch=99, global_step=400
06/18/2022 10:16:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=102
06/18/2022 10:16:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=104
06/18/2022 10:16:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=107
06/18/2022 10:16:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=109
06/18/2022 10:16:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=112
06/18/2022 10:16:35 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.6437817588368384 on epoch=112
06/18/2022 10:16:35 - INFO - __main__ - Saving model with best Classification-F1: 0.6307560903149139 -> 0.6437817588368384 on epoch=112, global_step=450
06/18/2022 10:16:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=114
06/18/2022 10:16:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=117
06/18/2022 10:16:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=119
06/18/2022 10:16:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=122
06/18/2022 10:16:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=124
06/18/2022 10:16:48 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.6434837876135702 on epoch=124
06/18/2022 10:16:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.33 on epoch=127
06/18/2022 10:16:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=129
06/18/2022 10:16:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=132
06/18/2022 10:16:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=134
06/18/2022 10:17:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=137
06/18/2022 10:17:02 - INFO - __main__ - Global step 550 Train loss 0.32 Classification-F1 0.6434001670843776 on epoch=137
06/18/2022 10:17:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=139
06/18/2022 10:17:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=142
06/18/2022 10:17:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=144
06/18/2022 10:17:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=147
06/18/2022 10:17:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=149
06/18/2022 10:17:15 - INFO - __main__ - Global step 600 Train loss 0.28 Classification-F1 0.642282835831223 on epoch=149
06/18/2022 10:17:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=152
06/18/2022 10:17:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=154
06/18/2022 10:17:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=157
06/18/2022 10:17:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.28 on epoch=159
06/18/2022 10:17:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=162
06/18/2022 10:17:28 - INFO - __main__ - Global step 650 Train loss 0.24 Classification-F1 0.6587510897994769 on epoch=162
06/18/2022 10:17:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6437817588368384 -> 0.6587510897994769 on epoch=162, global_step=650
06/18/2022 10:17:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=164
06/18/2022 10:17:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=167
06/18/2022 10:17:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=169
06/18/2022 10:17:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.28 on epoch=172
06/18/2022 10:17:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=174
06/18/2022 10:17:42 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.6560321843620502 on epoch=174
06/18/2022 10:17:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=177
06/18/2022 10:17:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=179
06/18/2022 10:17:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.18 on epoch=182
06/18/2022 10:17:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=184
06/18/2022 10:17:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=187
06/18/2022 10:17:55 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.6713052793273507 on epoch=187
06/18/2022 10:17:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6587510897994769 -> 0.6713052793273507 on epoch=187, global_step=750
06/18/2022 10:17:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=189
06/18/2022 10:18:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.17 on epoch=192
06/18/2022 10:18:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=194
06/18/2022 10:18:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=197
06/18/2022 10:18:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=199
06/18/2022 10:18:09 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.671255060728745 on epoch=199
06/18/2022 10:18:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=202
06/18/2022 10:18:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=204
06/18/2022 10:18:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=207
06/18/2022 10:18:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.13 on epoch=209
06/18/2022 10:18:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=212
06/18/2022 10:18:22 - INFO - __main__ - Global step 850 Train loss 0.16 Classification-F1 0.6713052793273507 on epoch=212
06/18/2022 10:18:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=214
06/18/2022 10:18:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=217
06/18/2022 10:18:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=219
06/18/2022 10:18:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=222
06/18/2022 10:18:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=224
06/18/2022 10:18:35 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.6598085496984002 on epoch=224
06/18/2022 10:18:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=227
06/18/2022 10:18:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.12 on epoch=229
06/18/2022 10:18:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=232
06/18/2022 10:18:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=234
06/18/2022 10:18:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=237
06/18/2022 10:18:49 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.6739583333333334 on epoch=237
06/18/2022 10:18:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6713052793273507 -> 0.6739583333333334 on epoch=237, global_step=950
06/18/2022 10:18:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=239
06/18/2022 10:18:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=242
06/18/2022 10:18:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=244
06/18/2022 10:18:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
06/18/2022 10:19:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=249
06/18/2022 10:19:02 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.6587510897994769 on epoch=249
06/18/2022 10:19:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=252
06/18/2022 10:19:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.14 on epoch=254
06/18/2022 10:19:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=257
06/18/2022 10:19:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=259
06/18/2022 10:19:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=262
06/18/2022 10:19:16 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.644078947368421 on epoch=262
06/18/2022 10:19:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=264
06/18/2022 10:19:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=267
06/18/2022 10:19:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=269
06/18/2022 10:19:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=272
06/18/2022 10:19:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
06/18/2022 10:19:29 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.6582245004570981 on epoch=274
06/18/2022 10:19:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=277
06/18/2022 10:19:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=279
06/18/2022 10:19:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=282
06/18/2022 10:19:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=284
06/18/2022 10:19:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=287
06/18/2022 10:19:42 - INFO - __main__ - Global step 1150 Train loss 0.12 Classification-F1 0.6582245004570981 on epoch=287
06/18/2022 10:19:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=289
06/18/2022 10:19:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=292
06/18/2022 10:19:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=294
06/18/2022 10:19:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=297
06/18/2022 10:19:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=299
06/18/2022 10:19:56 - INFO - __main__ - Global step 1200 Train loss 0.12 Classification-F1 0.6582245004570981 on epoch=299
06/18/2022 10:19:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=302
06/18/2022 10:20:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
06/18/2022 10:20:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=307
06/18/2022 10:20:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
06/18/2022 10:20:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=312
06/18/2022 10:20:09 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.6596153846153846 on epoch=312
06/18/2022 10:20:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=314
06/18/2022 10:20:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=317
06/18/2022 10:20:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=319
06/18/2022 10:20:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=322
06/18/2022 10:20:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=324
06/18/2022 10:20:23 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.6596153846153846 on epoch=324
06/18/2022 10:20:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
06/18/2022 10:20:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=329
06/18/2022 10:20:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=332
06/18/2022 10:20:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=334
06/18/2022 10:20:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
06/18/2022 10:20:36 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.6596153846153846 on epoch=337
06/18/2022 10:20:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
06/18/2022 10:20:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=342
06/18/2022 10:20:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
06/18/2022 10:20:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=347
06/18/2022 10:20:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
06/18/2022 10:20:50 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.6731353950103951 on epoch=349
06/18/2022 10:20:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=352
06/18/2022 10:20:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=354
06/18/2022 10:20:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=357
06/18/2022 10:21:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
06/18/2022 10:21:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=362
06/18/2022 10:21:03 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.6582245004570981 on epoch=362
06/18/2022 10:21:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
06/18/2022 10:21:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=367
06/18/2022 10:21:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
06/18/2022 10:21:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
06/18/2022 10:21:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
06/18/2022 10:21:16 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.6596153846153846 on epoch=374
06/18/2022 10:21:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.12 on epoch=377
06/18/2022 10:21:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=379
06/18/2022 10:21:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
06/18/2022 10:21:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=384
06/18/2022 10:21:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=387
06/18/2022 10:21:30 - INFO - __main__ - Global step 1550 Train loss 0.08 Classification-F1 0.6731353950103951 on epoch=387
06/18/2022 10:21:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=389
06/18/2022 10:21:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
06/18/2022 10:21:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=394
06/18/2022 10:21:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
06/18/2022 10:21:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
06/18/2022 10:21:43 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.6731353950103951 on epoch=399
06/18/2022 10:21:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=402
06/18/2022 10:21:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
06/18/2022 10:21:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
06/18/2022 10:21:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
06/18/2022 10:21:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
06/18/2022 10:21:57 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.6587510897994769 on epoch=412
06/18/2022 10:21:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
06/18/2022 10:22:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
06/18/2022 10:22:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
06/18/2022 10:22:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
06/18/2022 10:22:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=424
06/18/2022 10:22:10 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.6587510897994769 on epoch=424
06/18/2022 10:22:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
06/18/2022 10:22:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/18/2022 10:22:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=432
06/18/2022 10:22:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=434
06/18/2022 10:22:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
06/18/2022 10:22:23 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.6444380181222287 on epoch=437
06/18/2022 10:22:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=439
06/18/2022 10:22:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
06/18/2022 10:22:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=444
06/18/2022 10:22:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=447
06/18/2022 10:22:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
06/18/2022 10:22:37 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.6743535327151626 on epoch=449
06/18/2022 10:22:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6739583333333334 -> 0.6743535327151626 on epoch=449, global_step=1800
06/18/2022 10:22:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
06/18/2022 10:22:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=454
06/18/2022 10:22:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
06/18/2022 10:22:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=459
06/18/2022 10:22:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=462
06/18/2022 10:22:50 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.6731353950103951 on epoch=462
06/18/2022 10:22:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
06/18/2022 10:22:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
06/18/2022 10:22:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/18/2022 10:23:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=472
06/18/2022 10:23:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=474
06/18/2022 10:23:03 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.6743535327151626 on epoch=474
06/18/2022 10:23:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
06/18/2022 10:23:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=479
06/18/2022 10:23:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/18/2022 10:23:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=484
06/18/2022 10:23:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=487
06/18/2022 10:23:17 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6743535327151626 on epoch=487
06/18/2022 10:23:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
06/18/2022 10:23:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
06/18/2022 10:23:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
06/18/2022 10:23:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
06/18/2022 10:23:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
06/18/2022 10:23:30 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.6743535327151626 on epoch=499
06/18/2022 10:23:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=502
06/18/2022 10:23:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=504
06/18/2022 10:23:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
06/18/2022 10:23:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
06/18/2022 10:23:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=512
06/18/2022 10:23:44 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.6743535327151626 on epoch=512
06/18/2022 10:23:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
06/18/2022 10:23:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/18/2022 10:23:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
06/18/2022 10:23:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
06/18/2022 10:23:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
06/18/2022 10:23:57 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6731353950103951 on epoch=524
06/18/2022 10:24:00 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
06/18/2022 10:24:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
06/18/2022 10:24:05 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/18/2022 10:24:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=534
06/18/2022 10:24:10 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
06/18/2022 10:24:11 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6731353950103951 on epoch=537
06/18/2022 10:24:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=539
06/18/2022 10:24:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
06/18/2022 10:24:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
06/18/2022 10:24:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
06/18/2022 10:24:24 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/18/2022 10:24:25 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6596153846153846 on epoch=549
06/18/2022 10:24:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
06/18/2022 10:24:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
06/18/2022 10:24:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/18/2022 10:24:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
06/18/2022 10:24:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
06/18/2022 10:24:38 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6743535327151626 on epoch=562
06/18/2022 10:24:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
06/18/2022 10:24:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
06/18/2022 10:24:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=569
06/18/2022 10:24:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
06/18/2022 10:24:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
06/18/2022 10:24:52 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.6743535327151626 on epoch=574
06/18/2022 10:24:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/18/2022 10:24:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
06/18/2022 10:24:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
06/18/2022 10:25:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
06/18/2022 10:25:04 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=587
06/18/2022 10:25:05 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6849142085984191 on epoch=587
06/18/2022 10:25:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6743535327151626 -> 0.6849142085984191 on epoch=587, global_step=2350
06/18/2022 10:25:08 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
06/18/2022 10:25:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/18/2022 10:25:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=594
06/18/2022 10:25:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
06/18/2022 10:25:18 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
06/18/2022 10:25:19 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.6731353950103951 on epoch=599
06/18/2022 10:25:21 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.07 on epoch=602
06/18/2022 10:25:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/18/2022 10:25:26 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
06/18/2022 10:25:29 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
06/18/2022 10:25:31 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
06/18/2022 10:25:32 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.6743535327151626 on epoch=612
06/18/2022 10:25:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=614
06/18/2022 10:25:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=617
06/18/2022 10:25:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
06/18/2022 10:25:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=622
06/18/2022 10:25:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
06/18/2022 10:25:46 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.6743535327151626 on epoch=624
06/18/2022 10:25:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
06/18/2022 10:25:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
06/18/2022 10:25:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/18/2022 10:25:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=634
06/18/2022 10:25:59 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
06/18/2022 10:26:00 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6743535327151626 on epoch=637
06/18/2022 10:26:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
06/18/2022 10:26:05 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=642
06/18/2022 10:26:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
06/18/2022 10:26:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/18/2022 10:26:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/18/2022 10:26:13 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6596153846153846 on epoch=649
06/18/2022 10:26:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
06/18/2022 10:26:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=654
06/18/2022 10:26:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
06/18/2022 10:26:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/18/2022 10:26:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
06/18/2022 10:26:26 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6743535327151626 on epoch=662
06/18/2022 10:26:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
06/18/2022 10:26:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
06/18/2022 10:26:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
06/18/2022 10:26:36 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
06/18/2022 10:26:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
06/18/2022 10:26:40 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6743535327151626 on epoch=674
06/18/2022 10:26:42 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
06/18/2022 10:26:45 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
06/18/2022 10:26:47 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
06/18/2022 10:26:50 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/18/2022 10:26:52 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/18/2022 10:26:53 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6743535327151626 on epoch=687
06/18/2022 10:26:56 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=689
06/18/2022 10:26:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
06/18/2022 10:27:01 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
06/18/2022 10:27:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
06/18/2022 10:27:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/18/2022 10:27:07 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6743535327151626 on epoch=699
06/18/2022 10:27:09 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/18/2022 10:27:12 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=704
06/18/2022 10:27:14 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/18/2022 10:27:17 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/18/2022 10:27:19 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/18/2022 10:27:20 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6743535327151626 on epoch=712
06/18/2022 10:27:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
06/18/2022 10:27:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
06/18/2022 10:27:28 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=719
06/18/2022 10:27:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=722
06/18/2022 10:27:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
06/18/2022 10:27:34 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6743535327151626 on epoch=724
06/18/2022 10:27:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/18/2022 10:27:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/18/2022 10:27:41 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
06/18/2022 10:27:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/18/2022 10:27:46 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
06/18/2022 10:27:47 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6743535327151626 on epoch=737
06/18/2022 10:27:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
06/18/2022 10:27:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/18/2022 10:27:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
06/18/2022 10:27:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
06/18/2022 10:28:01 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
06/18/2022 10:28:02 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6743535327151626 on epoch=749
06/18/2022 10:28:02 - INFO - __main__ - save last model!
06/18/2022 10:28:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 10:28:02 - INFO - __main__ - Start tokenizing ... 5509 instances
06/18/2022 10:28:02 - INFO - __main__ - Printing 3 examples
06/18/2022 10:28:02 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/18/2022 10:28:02 - INFO - __main__ - ['others']
06/18/2022 10:28:02 - INFO - __main__ -  [emo] what you like very little things ok
06/18/2022 10:28:02 - INFO - __main__ - ['others']
06/18/2022 10:28:02 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/18/2022 10:28:02 - INFO - __main__ - ['others']
06/18/2022 10:28:02 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 10:28:04 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:28:09 - INFO - __main__ - Loaded 5509 examples from test data
06/18/2022 10:29:37 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-emo/emo_16_87_0.2_8_predictions.txt
06/18/2022 10:29:37 - INFO - __main__ - Classification-F1 on test data: 0.1582
06/18/2022 10:29:38 - INFO - __main__ - prefix=emo_16_87, lr=0.2, bsz=8, dev_performance=0.6849142085984191, test_performance=0.15817563815643582
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (85319): No such process
Task: yelp_polarity, Checkpoint: models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt, Identifier: T5-large-multitask-cls2cls-5e-1-4-10-up64shot
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/singletask_from_multi_cls2cls.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/singletask_from_multi_cls2cls.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-yelp_polarity/log.txt'
06/18/2022 10:29:44 - INFO - __main__ - Namespace(task_dir='data/yelp_polarity/', task_name='yelp_polarity', identifier='T5-large-multitask-cls2cls-5e-1-4-10-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-yelp_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/18/2022 10:29:44 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-yelp_polarity
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_multi_cls2cls.py', '--local_rank=1', '--task_dir', 'data/yelp_polarity/', '--task_name', 'yelp_polarity', '--identifier', 'T5-large-multitask-cls2cls-5e-1-4-10-up64shot', '--checkpoint', 'models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-yelp_polarity', '--cuda', '4,5', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 85325
Killing subprocess 85326
++++++++++++++++++++++++++++++
kill: (85332): No such process
Task: ethos-religion, Checkpoint: models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt, Identifier: T5-large-multitask-cls2cls-5e-1-4-10-up64shot
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/singletask_from_multi_cls2cls.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/singletask_from_multi_cls2cls.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-ethos-religion/log.txt'
06/18/2022 10:29:48 - INFO - __main__ - Namespace(task_dir='data/ethos-religion/', task_name='ethos-religion', identifier='T5-large-multitask-cls2cls-5e-1-4-10-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-ethos-religion', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/18/2022 10:29:48 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-ethos-religion
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_multi_cls2cls.py', '--local_rank=1', '--task_dir', 'data/ethos-religion/', '--task_name', 'ethos-religion', '--identifier', 'T5-large-multitask-cls2cls-5e-1-4-10-up64shot', '--checkpoint', 'models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-ethos-religion', '--cuda', '4,5', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 85338
Killing subprocess 85339
++++++++++++++++++++++++++++++
kill: (85345): No such process
Task: amazon_polarity, Checkpoint: models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt, Identifier: T5-large-multitask-cls2cls-5e-1-4-10-up64shot
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/singletask_from_multi_cls2cls.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/singletask_from_multi_cls2cls.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-amazon_polarity/log.txt'
06/18/2022 10:29:52 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-multitask-cls2cls-5e-1-4-10-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/18/2022 10:29:52 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-amazon_polarity
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_multi_cls2cls.py', '--local_rank=1', '--task_dir', 'data/amazon_polarity/', '--task_name', 'amazon_polarity', '--identifier', 'T5-large-multitask-cls2cls-5e-1-4-10-up64shot', '--checkpoint', 'models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-amazon_polarity', '--cuda', '4,5', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 85351
Killing subprocess 85352
++++++++++++++++++++++++++++++
kill: (85356): No such process
Task: tab_fact, Checkpoint: models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt, Identifier: T5-large-multitask-cls2cls-5e-1-4-10-up64shot
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/singletask_from_multi_cls2cls.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/singletask_from_multi_cls2cls.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_multi/models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-tab_fact/log.txt'
06/18/2022 10:29:55 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-large-multitask-cls2cls-5e-1-4-10-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/18/2022 10:29:55 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-tab_fact
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_multi_cls2cls.py', '--local_rank=1', '--task_dir', 'data/tab_fact/', '--task_name', 'tab_fact', '--identifier', 'T5-large-multitask-cls2cls-5e-1-4-10-up64shot', '--checkpoint', 'models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-tab_fact', '--cuda', '4,5', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 85362
Killing subprocess 85363
++++++++++++++++++++++++++++++
kill: (85367): No such process
Task: anli, Checkpoint: models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt, Identifier: T5-large-multitask-cls2cls-5e-1-4-10-up64shot
06/18/2022 10:29:58 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-multitask-cls2cls-5e-1-4-10-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/18/2022 10:29:58 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli
06/18/2022 10:29:58 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-multitask-cls2cls-5e-1-4-10-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-10-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/18/2022 10:29:58 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli
06/18/2022 10:29:58 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/18/2022 10:29:58 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/18/2022 10:29:58 - INFO - __main__ - args.device: cuda:0
06/18/2022 10:29:58 - INFO - __main__ - Using 2 gpus
06/18/2022 10:29:58 - INFO - __main__ - args.device: cuda:1
06/18/2022 10:29:58 - INFO - __main__ - Using 2 gpus
06/18/2022 10:29:58 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
06/18/2022 10:29:58 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
06/18/2022 10:30:03 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.5, bsz=8 ...
06/18/2022 10:30:04 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 10:30:04 - INFO - __main__ - Printing 3 examples
06/18/2022 10:30:04 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/18/2022 10:30:04 - INFO - __main__ - ['neutral']
06/18/2022 10:30:04 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/18/2022 10:30:04 - INFO - __main__ - ['neutral']
06/18/2022 10:30:04 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/18/2022 10:30:04 - INFO - __main__ - ['neutral']
06/18/2022 10:30:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 10:30:04 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 10:30:04 - INFO - __main__ - Printing 3 examples
06/18/2022 10:30:04 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/18/2022 10:30:04 - INFO - __main__ - ['neutral']
06/18/2022 10:30:04 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/18/2022 10:30:04 - INFO - __main__ - ['neutral']
06/18/2022 10:30:04 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/18/2022 10:30:04 - INFO - __main__ - ['neutral']
06/18/2022 10:30:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 10:30:04 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:30:04 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:30:04 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 10:30:04 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 10:30:04 - INFO - __main__ - Printing 3 examples
06/18/2022 10:30:04 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/18/2022 10:30:04 - INFO - __main__ - ['neutral']
06/18/2022 10:30:04 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/18/2022 10:30:04 - INFO - __main__ - ['neutral']
06/18/2022 10:30:04 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/18/2022 10:30:04 - INFO - __main__ - ['neutral']
06/18/2022 10:30:04 - INFO - __main__ - Tokenizing Input ...
06/18/2022 10:30:04 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 10:30:04 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 10:30:04 - INFO - __main__ - Printing 3 examples
06/18/2022 10:30:04 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/18/2022 10:30:04 - INFO - __main__ - ['neutral']
06/18/2022 10:30:04 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/18/2022 10:30:04 - INFO - __main__ - ['neutral']
06/18/2022 10:30:04 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/18/2022 10:30:04 - INFO - __main__ - ['neutral']
06/18/2022 10:30:04 - INFO - __main__ - Tokenizing Input ...
06/18/2022 10:30:04 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:30:04 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:30:04 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 10:30:04 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 10:30:22 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 10:30:22 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 10:30:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 10:30:23 - INFO - __main__ - Starting training!
06/18/2022 10:30:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 10:30:27 - INFO - __main__ - Starting training!
06/18/2022 10:30:31 - INFO - __main__ - Step 10 Global step 10 Train loss 0.66 on epoch=3
06/18/2022 10:30:33 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=6
06/18/2022 10:30:36 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=9
06/18/2022 10:30:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=13
06/18/2022 10:30:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=16
06/18/2022 10:30:42 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=16
06/18/2022 10:30:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/18/2022 10:30:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=19
06/18/2022 10:30:47 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=23
06/18/2022 10:30:50 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=26
06/18/2022 10:30:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=29
06/18/2022 10:30:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=33
06/18/2022 10:30:56 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=33
06/18/2022 10:30:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
06/18/2022 10:31:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=39
06/18/2022 10:31:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=43
06/18/2022 10:31:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=46
06/18/2022 10:31:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=49
06/18/2022 10:31:10 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.2871689026487788 on epoch=49
06/18/2022 10:31:10 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2871689026487788 on epoch=49, global_step=150
06/18/2022 10:31:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
06/18/2022 10:31:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=56
06/18/2022 10:31:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=59
06/18/2022 10:31:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=63
06/18/2022 10:31:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=66
06/18/2022 10:31:24 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.41245791245791247 on epoch=66
06/18/2022 10:31:24 - INFO - __main__ - Saving model with best Classification-F1: 0.2871689026487788 -> 0.41245791245791247 on epoch=66, global_step=200
06/18/2022 10:31:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=69
06/18/2022 10:31:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=73
06/18/2022 10:31:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=76
06/18/2022 10:31:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.58 on epoch=79
06/18/2022 10:31:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=83
06/18/2022 10:31:38 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.38973008558262023 on epoch=83
06/18/2022 10:31:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=86
06/18/2022 10:31:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=89
06/18/2022 10:31:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=93
06/18/2022 10:31:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=96
06/18/2022 10:31:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=99
06/18/2022 10:31:52 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.3758169934640523 on epoch=99
06/18/2022 10:31:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=103
06/18/2022 10:31:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=106
06/18/2022 10:31:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.30 on epoch=109
06/18/2022 10:32:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.31 on epoch=113
06/18/2022 10:32:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=116
06/18/2022 10:32:06 - INFO - __main__ - Global step 350 Train loss 0.33 Classification-F1 0.4935271405859641 on epoch=116
06/18/2022 10:32:06 - INFO - __main__ - Saving model with best Classification-F1: 0.41245791245791247 -> 0.4935271405859641 on epoch=116, global_step=350
06/18/2022 10:32:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=119
06/18/2022 10:32:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=123
06/18/2022 10:32:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=126
06/18/2022 10:32:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=129
06/18/2022 10:32:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=133
06/18/2022 10:32:20 - INFO - __main__ - Global step 400 Train loss 0.29 Classification-F1 0.3682247092385342 on epoch=133
06/18/2022 10:32:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.29 on epoch=136
06/18/2022 10:32:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=139
06/18/2022 10:32:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=143
06/18/2022 10:32:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=146
06/18/2022 10:32:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=149
06/18/2022 10:32:33 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.4569646040234276 on epoch=149
06/18/2022 10:32:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=153
06/18/2022 10:32:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=156
06/18/2022 10:32:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=159
06/18/2022 10:32:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=163
06/18/2022 10:32:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=166
06/18/2022 10:32:47 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.5250257997936016 on epoch=166
06/18/2022 10:32:48 - INFO - __main__ - Saving model with best Classification-F1: 0.4935271405859641 -> 0.5250257997936016 on epoch=166, global_step=500
06/18/2022 10:32:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=169
06/18/2022 10:32:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=173
06/18/2022 10:32:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.16 on epoch=176
06/18/2022 10:32:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.20 on epoch=179
06/18/2022 10:33:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=183
06/18/2022 10:33:02 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3920393120393121 on epoch=183
06/18/2022 10:33:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=186
06/18/2022 10:33:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=189
06/18/2022 10:33:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=193
06/18/2022 10:33:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.18 on epoch=196
06/18/2022 10:33:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=199
06/18/2022 10:33:16 - INFO - __main__ - Global step 600 Train loss 0.17 Classification-F1 0.3570234113712375 on epoch=199
06/18/2022 10:33:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=203
06/18/2022 10:33:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=206
06/18/2022 10:33:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=209
06/18/2022 10:33:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.09 on epoch=213
06/18/2022 10:33:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=216
06/18/2022 10:33:30 - INFO - __main__ - Global step 650 Train loss 0.14 Classification-F1 0.28652828652828655 on epoch=216
06/18/2022 10:33:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=219
06/18/2022 10:33:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=223
06/18/2022 10:33:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=226
06/18/2022 10:33:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=229
06/18/2022 10:33:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=233
06/18/2022 10:33:44 - INFO - __main__ - Global step 700 Train loss 0.10 Classification-F1 0.18307512910211562 on epoch=233
06/18/2022 10:33:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.15 on epoch=236
06/18/2022 10:33:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=239
06/18/2022 10:33:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=243
06/18/2022 10:33:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=246
06/18/2022 10:33:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=249
06/18/2022 10:33:58 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.3067101270091042 on epoch=249
06/18/2022 10:34:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=253
06/18/2022 10:34:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=256
06/18/2022 10:34:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=259
06/18/2022 10:34:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=263
06/18/2022 10:34:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=266
06/18/2022 10:34:12 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.3025 on epoch=266
06/18/2022 10:34:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=269
06/18/2022 10:34:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=273
06/18/2022 10:34:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=276
06/18/2022 10:34:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=279
06/18/2022 10:34:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=283
06/18/2022 10:34:26 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.3041409809702493 on epoch=283
06/18/2022 10:34:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=286
06/18/2022 10:34:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=289
06/18/2022 10:34:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=293
06/18/2022 10:34:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=296
06/18/2022 10:34:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=299
06/18/2022 10:34:40 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.23254901960784316 on epoch=299
06/18/2022 10:34:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=303
06/18/2022 10:34:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=306
06/18/2022 10:34:48 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=309
06/18/2022 10:34:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=313
06/18/2022 10:34:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=316
06/18/2022 10:34:54 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.20747126436781613 on epoch=316
06/18/2022 10:34:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=319
06/18/2022 10:34:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=323
06/18/2022 10:35:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
06/18/2022 10:35:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=329
06/18/2022 10:35:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=333
06/18/2022 10:35:08 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.2757957957957958 on epoch=333
06/18/2022 10:35:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=336
06/18/2022 10:35:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=339
06/18/2022 10:35:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=343
06/18/2022 10:35:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=346
06/18/2022 10:35:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=349
06/18/2022 10:35:22 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.20736842105263162 on epoch=349
06/18/2022 10:35:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=353
06/18/2022 10:35:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=356
06/18/2022 10:35:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=359
06/18/2022 10:35:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=363
06/18/2022 10:35:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=366
06/18/2022 10:35:36 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.23014251997095134 on epoch=366
06/18/2022 10:35:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=369
06/18/2022 10:35:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=373
06/18/2022 10:35:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=376
06/18/2022 10:35:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
06/18/2022 10:35:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
06/18/2022 10:35:50 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.3672801213123794 on epoch=383
06/18/2022 10:35:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=386
06/18/2022 10:35:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=389
06/18/2022 10:35:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=393
06/18/2022 10:36:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=396
06/18/2022 10:36:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
06/18/2022 10:36:04 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.21649904214559387 on epoch=399
06/18/2022 10:36:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=403
06/18/2022 10:36:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=406
06/18/2022 10:36:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=409
06/18/2022 10:36:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=413
06/18/2022 10:36:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=416
06/18/2022 10:36:19 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.22232686177919445 on epoch=416
06/18/2022 10:36:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
06/18/2022 10:36:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=423
06/18/2022 10:36:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
06/18/2022 10:36:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
06/18/2022 10:36:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=433
06/18/2022 10:36:33 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.2733333333333333 on epoch=433
06/18/2022 10:36:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
06/18/2022 10:36:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
06/18/2022 10:36:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
06/18/2022 10:36:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
06/18/2022 10:36:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
06/18/2022 10:36:47 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.2061728395061728 on epoch=449
06/18/2022 10:36:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=453
06/18/2022 10:36:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=456
06/18/2022 10:36:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
06/18/2022 10:36:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
06/18/2022 10:37:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=466
06/18/2022 10:37:01 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.16104497354497355 on epoch=466
06/18/2022 10:37:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
06/18/2022 10:37:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=473
06/18/2022 10:37:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
06/18/2022 10:37:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=479
06/18/2022 10:37:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
06/18/2022 10:37:15 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.29655172413793107 on epoch=483
06/18/2022 10:37:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
06/18/2022 10:37:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=489
06/18/2022 10:37:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=493
06/18/2022 10:37:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=496
06/18/2022 10:37:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
06/18/2022 10:37:29 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.34493087557603686 on epoch=499
06/18/2022 10:37:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=503
06/18/2022 10:37:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
06/18/2022 10:37:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
06/18/2022 10:37:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=513
06/18/2022 10:37:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
06/18/2022 10:37:43 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.2956043956043956 on epoch=516
06/18/2022 10:37:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
06/18/2022 10:37:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=523
06/18/2022 10:37:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
06/18/2022 10:37:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
06/18/2022 10:37:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/18/2022 10:37:57 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.5370370370370371 on epoch=533
06/18/2022 10:37:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5250257997936016 -> 0.5370370370370371 on epoch=533, global_step=1600
06/18/2022 10:38:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
06/18/2022 10:38:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
06/18/2022 10:38:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
06/18/2022 10:38:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
06/18/2022 10:38:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=549
06/18/2022 10:38:11 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.26060606060606056 on epoch=549
06/18/2022 10:38:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
06/18/2022 10:38:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
06/18/2022 10:38:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
06/18/2022 10:38:22 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
06/18/2022 10:38:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
06/18/2022 10:38:26 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.2439153439153439 on epoch=566
06/18/2022 10:38:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
06/18/2022 10:38:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
06/18/2022 10:38:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/18/2022 10:38:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=579
06/18/2022 10:38:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
06/18/2022 10:38:40 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.24868421052631579 on epoch=583
06/18/2022 10:38:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
06/18/2022 10:38:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
06/18/2022 10:38:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=593
06/18/2022 10:38:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=596
06/18/2022 10:38:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=599
06/18/2022 10:38:54 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.4644945697577276 on epoch=599
06/18/2022 10:38:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/18/2022 10:38:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
06/18/2022 10:39:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
06/18/2022 10:39:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/18/2022 10:39:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=616
06/18/2022 10:39:08 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.1951530612244898 on epoch=616
06/18/2022 10:39:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/18/2022 10:39:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
06/18/2022 10:39:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/18/2022 10:39:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
06/18/2022 10:39:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
06/18/2022 10:39:22 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.2778264192898339 on epoch=633
06/18/2022 10:39:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=636
06/18/2022 10:39:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
06/18/2022 10:39:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/18/2022 10:39:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
06/18/2022 10:39:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/18/2022 10:39:36 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.2923850574712644 on epoch=649
06/18/2022 10:39:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/18/2022 10:39:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/18/2022 10:39:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/18/2022 10:39:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/18/2022 10:39:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/18/2022 10:39:50 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.3458725182863114 on epoch=666
06/18/2022 10:39:53 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=669
06/18/2022 10:39:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
06/18/2022 10:39:58 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
06/18/2022 10:40:01 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/18/2022 10:40:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/18/2022 10:40:05 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.23849170234042316 on epoch=683
06/18/2022 10:40:08 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
06/18/2022 10:40:10 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
06/18/2022 10:40:13 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/18/2022 10:40:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
06/18/2022 10:40:18 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/18/2022 10:40:20 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.271889400921659 on epoch=699
06/18/2022 10:40:22 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
06/18/2022 10:40:25 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/18/2022 10:40:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
06/18/2022 10:40:30 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
06/18/2022 10:40:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.11 on epoch=716
06/18/2022 10:40:34 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.25353925353925355 on epoch=716
06/18/2022 10:40:37 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/18/2022 10:40:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/18/2022 10:40:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=726
06/18/2022 10:40:45 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=729
06/18/2022 10:40:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
06/18/2022 10:40:49 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.21351766513056836 on epoch=733
06/18/2022 10:40:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=736
06/18/2022 10:40:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
06/18/2022 10:40:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/18/2022 10:41:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/18/2022 10:41:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
06/18/2022 10:41:04 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.20963023904200376 on epoch=749
06/18/2022 10:41:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=753
06/18/2022 10:41:09 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/18/2022 10:41:12 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=759
06/18/2022 10:41:14 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
06/18/2022 10:41:17 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/18/2022 10:41:18 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.22261904761904758 on epoch=766
06/18/2022 10:41:21 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
06/18/2022 10:41:24 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=773
06/18/2022 10:41:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
06/18/2022 10:41:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=779
06/18/2022 10:41:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/18/2022 10:41:33 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.2147028444303194 on epoch=783
06/18/2022 10:41:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
06/18/2022 10:41:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/18/2022 10:41:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
06/18/2022 10:41:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
06/18/2022 10:41:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/18/2022 10:41:48 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.20254553434276937 on epoch=799
06/18/2022 10:41:50 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/18/2022 10:41:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
06/18/2022 10:41:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=809
06/18/2022 10:41:58 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/18/2022 10:42:01 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=816
06/18/2022 10:42:02 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.269668458781362 on epoch=816
06/18/2022 10:42:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/18/2022 10:42:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/18/2022 10:42:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=826
06/18/2022 10:42:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/18/2022 10:42:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/18/2022 10:42:17 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.17324805339265853 on epoch=833
06/18/2022 10:42:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
06/18/2022 10:42:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
06/18/2022 10:42:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/18/2022 10:42:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/18/2022 10:42:30 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/18/2022 10:42:32 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.2445927573847485 on epoch=849
06/18/2022 10:42:34 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/18/2022 10:42:37 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/18/2022 10:42:40 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/18/2022 10:42:42 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/18/2022 10:42:45 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/18/2022 10:42:46 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.21695402298850575 on epoch=866
06/18/2022 10:42:49 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/18/2022 10:42:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/18/2022 10:42:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
06/18/2022 10:42:57 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
06/18/2022 10:43:00 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/18/2022 10:43:01 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.3676942634673447 on epoch=883
06/18/2022 10:43:04 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/18/2022 10:43:06 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/18/2022 10:43:09 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=893
06/18/2022 10:43:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/18/2022 10:43:14 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
06/18/2022 10:43:16 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.37241379310344824 on epoch=899
06/18/2022 10:43:18 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/18/2022 10:43:21 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
06/18/2022 10:43:24 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=909
06/18/2022 10:43:26 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/18/2022 10:43:29 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/18/2022 10:43:31 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.32459770114942527 on epoch=916
06/18/2022 10:43:33 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
06/18/2022 10:43:36 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/18/2022 10:43:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/18/2022 10:43:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
06/18/2022 10:43:44 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=933
06/18/2022 10:43:45 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.30793692337891776 on epoch=933
06/18/2022 10:43:48 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/18/2022 10:43:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/18/2022 10:43:53 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/18/2022 10:43:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/18/2022 10:43:58 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/18/2022 10:44:00 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.3722804972804973 on epoch=949
06/18/2022 10:44:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/18/2022 10:44:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=956
06/18/2022 10:44:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/18/2022 10:44:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
06/18/2022 10:44:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/18/2022 10:44:14 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5056029333003749 on epoch=966
06/18/2022 10:44:17 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/18/2022 10:44:20 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/18/2022 10:44:22 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/18/2022 10:44:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
06/18/2022 10:44:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=983
06/18/2022 10:44:29 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.5441993464052288 on epoch=983
06/18/2022 10:44:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5370370370370371 -> 0.5441993464052288 on epoch=983, global_step=2950
06/18/2022 10:44:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/18/2022 10:44:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=989
06/18/2022 10:44:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
06/18/2022 10:44:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/18/2022 10:44:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/18/2022 10:44:44 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 10:44:44 - INFO - __main__ - Printing 3 examples
06/18/2022 10:44:44 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/18/2022 10:44:44 - INFO - __main__ - ['neutral']
06/18/2022 10:44:44 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/18/2022 10:44:44 - INFO - __main__ - ['neutral']
06/18/2022 10:44:44 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/18/2022 10:44:44 - INFO - __main__ - ['neutral']
06/18/2022 10:44:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 10:44:44 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:44:44 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.37901785714285713 on epoch=999
06/18/2022 10:44:44 - INFO - __main__ - save last model!
06/18/2022 10:44:44 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 10:44:44 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 10:44:44 - INFO - __main__ - Printing 3 examples
06/18/2022 10:44:44 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/18/2022 10:44:44 - INFO - __main__ - ['neutral']
06/18/2022 10:44:44 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/18/2022 10:44:44 - INFO - __main__ - ['neutral']
06/18/2022 10:44:44 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/18/2022 10:44:44 - INFO - __main__ - ['neutral']
06/18/2022 10:44:44 - INFO - __main__ - Tokenizing Input ...
06/18/2022 10:44:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 10:44:44 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 10:44:44 - INFO - __main__ - Printing 3 examples
06/18/2022 10:44:44 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 10:44:44 - INFO - __main__ - ['contradiction']
06/18/2022 10:44:44 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 10:44:44 - INFO - __main__ - ['entailment']
06/18/2022 10:44:44 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 10:44:44 - INFO - __main__ - ['contradiction']
06/18/2022 10:44:44 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 10:44:44 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:44:44 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 10:44:44 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:44:45 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 10:45:01 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 10:45:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 10:45:02 - INFO - __main__ - Starting training!
06/18/2022 10:45:16 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_100_0.5_8_predictions.txt
06/18/2022 10:45:16 - INFO - __main__ - Classification-F1 on test data: 0.0364
06/18/2022 10:45:16 - INFO - __main__ - prefix=anli_16_100, lr=0.5, bsz=8, dev_performance=0.5441993464052288, test_performance=0.03641969900791656
06/18/2022 10:45:16 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.4, bsz=8 ...
06/18/2022 10:45:17 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 10:45:17 - INFO - __main__ - Printing 3 examples
06/18/2022 10:45:17 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/18/2022 10:45:17 - INFO - __main__ - ['neutral']
06/18/2022 10:45:17 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/18/2022 10:45:17 - INFO - __main__ - ['neutral']
06/18/2022 10:45:17 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/18/2022 10:45:17 - INFO - __main__ - ['neutral']
06/18/2022 10:45:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 10:45:17 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:45:17 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 10:45:17 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 10:45:17 - INFO - __main__ - Printing 3 examples
06/18/2022 10:45:17 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/18/2022 10:45:17 - INFO - __main__ - ['neutral']
06/18/2022 10:45:17 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/18/2022 10:45:17 - INFO - __main__ - ['neutral']
06/18/2022 10:45:17 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/18/2022 10:45:17 - INFO - __main__ - ['neutral']
06/18/2022 10:45:17 - INFO - __main__ - Tokenizing Input ...
06/18/2022 10:45:17 - INFO - __main__ - Tokenizing Output ...
06/18/2022 10:45:17 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 10:45:34 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 10:45:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 10:45:34 - INFO - __main__ - Starting training!
06/18/2022 10:45:38 - INFO - __main__ - Step 10 Global step 10 Train loss 0.66 on epoch=3
06/18/2022 10:45:40 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=6
06/18/2022 10:45:43 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=9
06/18/2022 10:45:45 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=13
06/18/2022 10:45:48 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=16
06/18/2022 10:45:49 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.1837474120082816 on epoch=16
06/18/2022 10:45:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1837474120082816 on epoch=16, global_step=50
06/18/2022 10:45:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=19
06/18/2022 10:45:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=23
06/18/2022 10:45:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
06/18/2022 10:46:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=29
06/18/2022 10:46:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=33
06/18/2022 10:46:04 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=33
06/18/2022 10:46:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=36
06/18/2022 10:46:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=39
06/18/2022 10:46:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=43
06/18/2022 10:46:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=46
06/18/2022 10:46:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=49
06/18/2022 10:46:18 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=49
06/18/2022 10:46:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=53
06/18/2022 10:46:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=56
06/18/2022 10:46:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=59
06/18/2022 10:46:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=63
06/18/2022 10:46:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=66
06/18/2022 10:46:32 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.27409102466310703 on epoch=66
06/18/2022 10:46:32 - INFO - __main__ - Saving model with best Classification-F1: 0.1837474120082816 -> 0.27409102466310703 on epoch=66, global_step=200
06/18/2022 10:46:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=69
06/18/2022 10:46:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=73
06/18/2022 10:46:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=76
06/18/2022 10:46:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
06/18/2022 10:46:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
06/18/2022 10:46:47 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.3272727272727273 on epoch=83
06/18/2022 10:46:47 - INFO - __main__ - Saving model with best Classification-F1: 0.27409102466310703 -> 0.3272727272727273 on epoch=83, global_step=250
06/18/2022 10:46:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=86
06/18/2022 10:46:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=89
06/18/2022 10:46:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=93
06/18/2022 10:46:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.37 on epoch=96
06/18/2022 10:47:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=99
06/18/2022 10:47:01 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.40972222222222215 on epoch=99
06/18/2022 10:47:01 - INFO - __main__ - Saving model with best Classification-F1: 0.3272727272727273 -> 0.40972222222222215 on epoch=99, global_step=300
06/18/2022 10:47:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=103
06/18/2022 10:47:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=106
06/18/2022 10:47:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=109
06/18/2022 10:47:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=113
06/18/2022 10:47:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=116
06/18/2022 10:47:15 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.3742528735632184 on epoch=116
06/18/2022 10:47:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=119
06/18/2022 10:47:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=123
06/18/2022 10:47:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=126
06/18/2022 10:47:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=129
06/18/2022 10:47:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=133
06/18/2022 10:47:30 - INFO - __main__ - Global step 400 Train loss 0.35 Classification-F1 0.2442830239440409 on epoch=133
06/18/2022 10:47:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=136
06/18/2022 10:47:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=139
06/18/2022 10:47:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=143
06/18/2022 10:47:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.32 on epoch=146
06/18/2022 10:47:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=149
06/18/2022 10:47:44 - INFO - __main__ - Global step 450 Train loss 0.31 Classification-F1 0.3015873015873016 on epoch=149
06/18/2022 10:47:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=153
06/18/2022 10:47:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=156
06/18/2022 10:47:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=159
06/18/2022 10:47:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=163
06/18/2022 10:47:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=166
06/18/2022 10:47:58 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.35137701804368465 on epoch=166
06/18/2022 10:48:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=169
06/18/2022 10:48:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.28 on epoch=173
06/18/2022 10:48:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.28 on epoch=176
06/18/2022 10:48:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=179
06/18/2022 10:48:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=183
06/18/2022 10:48:12 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.3717948717948718 on epoch=183
06/18/2022 10:48:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.31 on epoch=186
06/18/2022 10:48:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=189
06/18/2022 10:48:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=193
06/18/2022 10:48:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=196
06/18/2022 10:48:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=199
06/18/2022 10:48:26 - INFO - __main__ - Global step 600 Train loss 0.25 Classification-F1 0.3700698670272707 on epoch=199
06/18/2022 10:48:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=203
06/18/2022 10:48:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=206
06/18/2022 10:48:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=209
06/18/2022 10:48:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=213
06/18/2022 10:48:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=216
06/18/2022 10:48:41 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.39249203955086304 on epoch=216
06/18/2022 10:48:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=219
06/18/2022 10:48:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=223
06/18/2022 10:48:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=226
06/18/2022 10:48:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=229
06/18/2022 10:48:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=233
06/18/2022 10:48:55 - INFO - __main__ - Global step 700 Train loss 0.17 Classification-F1 0.29824016563147 on epoch=233
06/18/2022 10:48:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.15 on epoch=236
06/18/2022 10:49:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=239
06/18/2022 10:49:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=243
06/18/2022 10:49:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=246
06/18/2022 10:49:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=249
06/18/2022 10:49:10 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.3631549609810479 on epoch=249
06/18/2022 10:49:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=253
06/18/2022 10:49:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=256
06/18/2022 10:49:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=259
06/18/2022 10:49:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=263
06/18/2022 10:49:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=266
06/18/2022 10:49:24 - INFO - __main__ - Global step 800 Train loss 0.13 Classification-F1 0.2505081300813008 on epoch=266
06/18/2022 10:49:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=269
06/18/2022 10:49:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=273
06/18/2022 10:49:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=276
06/18/2022 10:49:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=279
06/18/2022 10:49:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=283
06/18/2022 10:49:39 - INFO - __main__ - Global step 850 Train loss 0.13 Classification-F1 0.25862998548420546 on epoch=283
06/18/2022 10:49:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=286
06/18/2022 10:49:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=289
06/18/2022 10:49:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=293
06/18/2022 10:49:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=296
06/18/2022 10:49:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=299
06/18/2022 10:49:53 - INFO - __main__ - Global step 900 Train loss 0.13 Classification-F1 0.4110835401157982 on epoch=299
06/18/2022 10:49:53 - INFO - __main__ - Saving model with best Classification-F1: 0.40972222222222215 -> 0.4110835401157982 on epoch=299, global_step=900
06/18/2022 10:49:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=303
06/18/2022 10:49:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=306
06/18/2022 10:50:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=309
06/18/2022 10:50:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=313
06/18/2022 10:50:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=316
06/18/2022 10:50:08 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.38275754746342977 on epoch=316
06/18/2022 10:50:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=319
06/18/2022 10:50:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=323
06/18/2022 10:50:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=326
06/18/2022 10:50:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=329
06/18/2022 10:50:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=333
06/18/2022 10:50:22 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.46406371406371405 on epoch=333
06/18/2022 10:50:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4110835401157982 -> 0.46406371406371405 on epoch=333, global_step=1000
06/18/2022 10:50:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=336
06/18/2022 10:50:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=339
06/18/2022 10:50:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=343
06/18/2022 10:50:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=346
06/18/2022 10:50:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=349
06/18/2022 10:50:37 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.2931789416070879 on epoch=349
06/18/2022 10:50:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=353
06/18/2022 10:50:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=356
06/18/2022 10:50:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=359
06/18/2022 10:50:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=363
06/18/2022 10:50:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=366
06/18/2022 10:50:51 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.4325889164598842 on epoch=366
06/18/2022 10:50:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=369
06/18/2022 10:50:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=373
06/18/2022 10:50:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=376
06/18/2022 10:51:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=379
06/18/2022 10:51:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=383
06/18/2022 10:51:06 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.4200501253132832 on epoch=383
06/18/2022 10:51:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=386
06/18/2022 10:51:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=389
06/18/2022 10:51:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=393
06/18/2022 10:51:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=396
06/18/2022 10:51:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
06/18/2022 10:51:20 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.42060207991242476 on epoch=399
06/18/2022 10:51:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=403
06/18/2022 10:51:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=406
06/18/2022 10:51:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=409
06/18/2022 10:51:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=413
06/18/2022 10:51:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
06/18/2022 10:51:35 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.3036354326676908 on epoch=416
06/18/2022 10:51:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=419
06/18/2022 10:51:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=423
06/18/2022 10:51:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=426
06/18/2022 10:51:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=429
06/18/2022 10:51:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=433
06/18/2022 10:51:49 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.2154945054945055 on epoch=433
06/18/2022 10:51:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=436
06/18/2022 10:51:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=439
06/18/2022 10:51:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=443
06/18/2022 10:52:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=446
06/18/2022 10:52:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=449
06/18/2022 10:52:04 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.2603686635944701 on epoch=449
06/18/2022 10:52:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=453
06/18/2022 10:52:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
06/18/2022 10:52:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
06/18/2022 10:52:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=463
06/18/2022 10:52:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=466
06/18/2022 10:52:18 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.18516483516483517 on epoch=466
06/18/2022 10:52:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=469
06/18/2022 10:52:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
06/18/2022 10:52:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=476
06/18/2022 10:52:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=479
06/18/2022 10:52:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=483
06/18/2022 10:52:33 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.3114987714987715 on epoch=483
06/18/2022 10:52:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
06/18/2022 10:52:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=489
06/18/2022 10:52:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
06/18/2022 10:52:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
06/18/2022 10:52:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=499
06/18/2022 10:52:47 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.21505376344086022 on epoch=499
06/18/2022 10:52:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
06/18/2022 10:52:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=506
06/18/2022 10:52:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
06/18/2022 10:52:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
06/18/2022 10:53:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=516
06/18/2022 10:53:02 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.4372567783094099 on epoch=516
06/18/2022 10:53:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
06/18/2022 10:53:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
06/18/2022 10:53:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
06/18/2022 10:53:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=529
06/18/2022 10:53:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
06/18/2022 10:53:16 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.2633333333333333 on epoch=533
06/18/2022 10:53:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
06/18/2022 10:53:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
06/18/2022 10:53:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
06/18/2022 10:53:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
06/18/2022 10:53:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=549
06/18/2022 10:53:31 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.21298637093965236 on epoch=549
06/18/2022 10:53:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
06/18/2022 10:53:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
06/18/2022 10:53:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=559
06/18/2022 10:53:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
06/18/2022 10:53:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
06/18/2022 10:53:45 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.4791666666666667 on epoch=566
06/18/2022 10:53:45 - INFO - __main__ - Saving model with best Classification-F1: 0.46406371406371405 -> 0.4791666666666667 on epoch=566, global_step=1700
06/18/2022 10:53:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
06/18/2022 10:53:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
06/18/2022 10:53:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/18/2022 10:53:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
06/18/2022 10:53:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
06/18/2022 10:54:00 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.4807551319648094 on epoch=583
06/18/2022 10:54:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4791666666666667 -> 0.4807551319648094 on epoch=583, global_step=1750
06/18/2022 10:54:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
06/18/2022 10:54:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=589
06/18/2022 10:54:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
06/18/2022 10:54:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
06/18/2022 10:54:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
06/18/2022 10:54:14 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.3028673835125448 on epoch=599
06/18/2022 10:54:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
06/18/2022 10:54:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
06/18/2022 10:54:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/18/2022 10:54:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/18/2022 10:54:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=616
06/18/2022 10:54:29 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.20416666666666666 on epoch=616
06/18/2022 10:54:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/18/2022 10:54:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
06/18/2022 10:54:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
06/18/2022 10:54:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
06/18/2022 10:54:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/18/2022 10:54:44 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.29339080459770117 on epoch=633
06/18/2022 10:54:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
06/18/2022 10:54:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
06/18/2022 10:54:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=643
06/18/2022 10:54:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
06/18/2022 10:54:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/18/2022 10:54:58 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.33840996168582377 on epoch=649
06/18/2022 10:55:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/18/2022 10:55:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/18/2022 10:55:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/18/2022 10:55:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/18/2022 10:55:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
06/18/2022 10:55:13 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.17705013039698642 on epoch=666
06/18/2022 10:55:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
06/18/2022 10:55:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
06/18/2022 10:55:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/18/2022 10:55:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=679
06/18/2022 10:55:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
06/18/2022 10:55:27 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.26031746031746034 on epoch=683
06/18/2022 10:55:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=686
06/18/2022 10:55:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/18/2022 10:55:35 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=693
06/18/2022 10:55:38 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=696
06/18/2022 10:55:40 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=699
06/18/2022 10:55:42 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.12439215686274512 on epoch=699
06/18/2022 10:55:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
06/18/2022 10:55:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
06/18/2022 10:55:50 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/18/2022 10:55:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
06/18/2022 10:55:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/18/2022 10:55:56 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.3313685037822969 on epoch=716
06/18/2022 10:55:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=719
06/18/2022 10:56:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/18/2022 10:56:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
06/18/2022 10:56:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
06/18/2022 10:56:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
06/18/2022 10:56:11 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.27 on epoch=733
06/18/2022 10:56:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=736
06/18/2022 10:56:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/18/2022 10:56:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/18/2022 10:56:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=746
06/18/2022 10:56:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
06/18/2022 10:56:25 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.33901160019068804 on epoch=749
06/18/2022 10:56:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
06/18/2022 10:56:31 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/18/2022 10:56:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=759
06/18/2022 10:56:36 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
06/18/2022 10:56:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/18/2022 10:56:40 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.35714285714285715 on epoch=766
06/18/2022 10:56:43 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
06/18/2022 10:56:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/18/2022 10:56:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
06/18/2022 10:56:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=779
06/18/2022 10:56:53 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/18/2022 10:56:54 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.2746031746031746 on epoch=783
06/18/2022 10:56:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=786
06/18/2022 10:57:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/18/2022 10:57:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
06/18/2022 10:57:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=796
06/18/2022 10:57:07 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/18/2022 10:57:09 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.22307561017238434 on epoch=799
06/18/2022 10:57:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=803
06/18/2022 10:57:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/18/2022 10:57:17 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/18/2022 10:57:19 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/18/2022 10:57:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/18/2022 10:57:23 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.2959875760649087 on epoch=816
06/18/2022 10:57:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/18/2022 10:57:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/18/2022 10:57:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/18/2022 10:57:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/18/2022 10:57:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=833
06/18/2022 10:57:38 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.2821896383186705 on epoch=833
06/18/2022 10:57:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=836
06/18/2022 10:57:43 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/18/2022 10:57:46 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/18/2022 10:57:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
06/18/2022 10:57:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/18/2022 10:57:52 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.2814516129032258 on epoch=849
06/18/2022 10:57:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/18/2022 10:57:58 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/18/2022 10:58:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
06/18/2022 10:58:03 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/18/2022 10:58:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
06/18/2022 10:58:07 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.22373905911169537 on epoch=866
06/18/2022 10:58:09 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
06/18/2022 10:58:12 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/18/2022 10:58:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/18/2022 10:58:17 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/18/2022 10:58:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=883
06/18/2022 10:58:21 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.18617511520737323 on epoch=883
06/18/2022 10:58:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=886
06/18/2022 10:58:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/18/2022 10:58:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
06/18/2022 10:58:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
06/18/2022 10:58:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=899
06/18/2022 10:58:35 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.2896031746031746 on epoch=899
06/18/2022 10:58:38 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=903
06/18/2022 10:58:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/18/2022 10:58:43 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
06/18/2022 10:58:46 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=913
06/18/2022 10:58:48 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/18/2022 10:58:50 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.15698924731182795 on epoch=916
06/18/2022 10:58:52 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/18/2022 10:58:55 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/18/2022 10:58:57 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/18/2022 10:59:00 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/18/2022 10:59:03 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
06/18/2022 10:59:04 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.2356902356902357 on epoch=933
06/18/2022 10:59:07 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/18/2022 10:59:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=939
06/18/2022 10:59:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/18/2022 10:59:14 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/18/2022 10:59:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
06/18/2022 10:59:18 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.2356902356902357 on epoch=949
06/18/2022 10:59:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
06/18/2022 10:59:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/18/2022 10:59:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/18/2022 10:59:29 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/18/2022 10:59:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
06/18/2022 10:59:33 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.23683021595735068 on epoch=966
06/18/2022 10:59:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=969
06/18/2022 10:59:38 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
06/18/2022 10:59:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=976
06/18/2022 10:59:43 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/18/2022 10:59:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/18/2022 10:59:47 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.4469507101086048 on epoch=983
06/18/2022 10:59:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
06/18/2022 10:59:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/18/2022 10:59:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
06/18/2022 10:59:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
06/18/2022 11:00:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/18/2022 11:00:01 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:00:01 - INFO - __main__ - Printing 3 examples
06/18/2022 11:00:01 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/18/2022 11:00:01 - INFO - __main__ - ['neutral']
06/18/2022 11:00:01 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/18/2022 11:00:01 - INFO - __main__ - ['neutral']
06/18/2022 11:00:01 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/18/2022 11:00:01 - INFO - __main__ - ['neutral']
06/18/2022 11:00:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 11:00:01 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:00:01 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.23671594982078856 on epoch=999
06/18/2022 11:00:01 - INFO - __main__ - save last model!
06/18/2022 11:00:01 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 11:00:01 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:00:01 - INFO - __main__ - Printing 3 examples
06/18/2022 11:00:01 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/18/2022 11:00:01 - INFO - __main__ - ['neutral']
06/18/2022 11:00:01 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/18/2022 11:00:01 - INFO - __main__ - ['neutral']
06/18/2022 11:00:01 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/18/2022 11:00:01 - INFO - __main__ - ['neutral']
06/18/2022 11:00:01 - INFO - __main__ - Tokenizing Input ...
06/18/2022 11:00:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 11:00:01 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 11:00:01 - INFO - __main__ - Printing 3 examples
06/18/2022 11:00:01 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 11:00:01 - INFO - __main__ - ['contradiction']
06/18/2022 11:00:01 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 11:00:01 - INFO - __main__ - ['entailment']
06/18/2022 11:00:01 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 11:00:01 - INFO - __main__ - ['contradiction']
06/18/2022 11:00:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 11:00:01 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:00:01 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 11:00:02 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:00:03 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 11:00:16 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 11:00:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 11:00:17 - INFO - __main__ - Starting training!
06/18/2022 11:00:33 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_100_0.4_8_predictions.txt
06/18/2022 11:00:34 - INFO - __main__ - Classification-F1 on test data: 0.0273
06/18/2022 11:00:34 - INFO - __main__ - prefix=anli_16_100, lr=0.4, bsz=8, dev_performance=0.4807551319648094, test_performance=0.02728094191939565
06/18/2022 11:00:34 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.3, bsz=8 ...
06/18/2022 11:00:35 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:00:35 - INFO - __main__ - Printing 3 examples
06/18/2022 11:00:35 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/18/2022 11:00:35 - INFO - __main__ - ['neutral']
06/18/2022 11:00:35 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/18/2022 11:00:35 - INFO - __main__ - ['neutral']
06/18/2022 11:00:35 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/18/2022 11:00:35 - INFO - __main__ - ['neutral']
06/18/2022 11:00:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 11:00:35 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:00:35 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 11:00:35 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:00:35 - INFO - __main__ - Printing 3 examples
06/18/2022 11:00:35 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/18/2022 11:00:35 - INFO - __main__ - ['neutral']
06/18/2022 11:00:35 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/18/2022 11:00:35 - INFO - __main__ - ['neutral']
06/18/2022 11:00:35 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/18/2022 11:00:35 - INFO - __main__ - ['neutral']
06/18/2022 11:00:35 - INFO - __main__ - Tokenizing Input ...
06/18/2022 11:00:35 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:00:35 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 11:00:50 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 11:00:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 11:00:51 - INFO - __main__ - Starting training!
06/18/2022 11:00:54 - INFO - __main__ - Step 10 Global step 10 Train loss 0.80 on epoch=3
06/18/2022 11:00:57 - INFO - __main__ - Step 20 Global step 20 Train loss 0.49 on epoch=6
06/18/2022 11:00:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=9
06/18/2022 11:01:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=13
06/18/2022 11:01:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=16
06/18/2022 11:01:06 - INFO - __main__ - Global step 50 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=16
06/18/2022 11:01:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/18/2022 11:01:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=19
06/18/2022 11:01:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=23
06/18/2022 11:01:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=26
06/18/2022 11:01:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=29
06/18/2022 11:01:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=33
06/18/2022 11:01:20 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=33
06/18/2022 11:01:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=36
06/18/2022 11:01:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=39
06/18/2022 11:01:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=43
06/18/2022 11:01:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=46
06/18/2022 11:01:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=49
06/18/2022 11:01:34 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.1693121693121693 on epoch=49
06/18/2022 11:01:35 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1693121693121693 on epoch=49, global_step=150
06/18/2022 11:01:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
06/18/2022 11:01:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=56
06/18/2022 11:01:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=59
06/18/2022 11:01:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=63
06/18/2022 11:01:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=66
06/18/2022 11:01:49 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.3240928555330178 on epoch=66
06/18/2022 11:01:49 - INFO - __main__ - Saving model with best Classification-F1: 0.1693121693121693 -> 0.3240928555330178 on epoch=66, global_step=200
06/18/2022 11:01:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=69
06/18/2022 11:01:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=73
06/18/2022 11:01:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=76
06/18/2022 11:01:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=79
06/18/2022 11:02:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
06/18/2022 11:02:03 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.17486338797814208 on epoch=83
06/18/2022 11:02:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=86
06/18/2022 11:02:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=89
06/18/2022 11:02:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=93
06/18/2022 11:02:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=96
06/18/2022 11:02:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=99
06/18/2022 11:02:17 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.30002329373398556 on epoch=99
06/18/2022 11:02:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=103
06/18/2022 11:02:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
06/18/2022 11:02:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=109
06/18/2022 11:02:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=113
06/18/2022 11:02:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=116
06/18/2022 11:02:32 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.3467193328944482 on epoch=116
06/18/2022 11:02:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3240928555330178 -> 0.3467193328944482 on epoch=116, global_step=350
06/18/2022 11:02:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=119
06/18/2022 11:02:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=123
06/18/2022 11:02:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=126
06/18/2022 11:02:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=129
06/18/2022 11:02:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=133
06/18/2022 11:02:46 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.32573599240265905 on epoch=133
06/18/2022 11:02:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=136
06/18/2022 11:02:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=139
06/18/2022 11:02:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=143
06/18/2022 11:02:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=146
06/18/2022 11:02:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=149
06/18/2022 11:03:00 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.39365079365079364 on epoch=149
06/18/2022 11:03:00 - INFO - __main__ - Saving model with best Classification-F1: 0.3467193328944482 -> 0.39365079365079364 on epoch=149, global_step=450
06/18/2022 11:03:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=153
06/18/2022 11:03:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=156
06/18/2022 11:03:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=159
06/18/2022 11:03:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.33 on epoch=163
06/18/2022 11:03:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=166
06/18/2022 11:03:14 - INFO - __main__ - Global step 500 Train loss 0.35 Classification-F1 0.3717948717948718 on epoch=166
06/18/2022 11:03:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=169
06/18/2022 11:03:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=173
06/18/2022 11:03:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=176
06/18/2022 11:03:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=179
06/18/2022 11:03:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=183
06/18/2022 11:03:29 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.41245791245791247 on epoch=183
06/18/2022 11:03:29 - INFO - __main__ - Saving model with best Classification-F1: 0.39365079365079364 -> 0.41245791245791247 on epoch=183, global_step=550
06/18/2022 11:03:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=186
06/18/2022 11:03:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=189
06/18/2022 11:03:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=193
06/18/2022 11:03:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=196
06/18/2022 11:03:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=199
06/18/2022 11:03:43 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.35418009886094987 on epoch=199
06/18/2022 11:03:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.30 on epoch=203
06/18/2022 11:03:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=206
06/18/2022 11:03:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=209
06/18/2022 11:03:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.31 on epoch=213
06/18/2022 11:03:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=216
06/18/2022 11:03:57 - INFO - __main__ - Global step 650 Train loss 0.31 Classification-F1 0.4083546462063086 on epoch=216
06/18/2022 11:04:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=219
06/18/2022 11:04:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.27 on epoch=223
06/18/2022 11:04:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=226
06/18/2022 11:04:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.28 on epoch=229
06/18/2022 11:04:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=233
06/18/2022 11:04:11 - INFO - __main__ - Global step 700 Train loss 0.27 Classification-F1 0.35978835978835977 on epoch=233
06/18/2022 11:04:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.30 on epoch=236
06/18/2022 11:04:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=239
06/18/2022 11:04:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=243
06/18/2022 11:04:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=246
06/18/2022 11:04:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=249
06/18/2022 11:04:26 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.37878787878787873 on epoch=249
06/18/2022 11:04:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=253
06/18/2022 11:04:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=256
06/18/2022 11:04:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=259
06/18/2022 11:04:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=263
06/18/2022 11:04:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=266
06/18/2022 11:04:40 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.37878787878787873 on epoch=266
06/18/2022 11:04:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=269
06/18/2022 11:04:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=273
06/18/2022 11:04:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=276
06/18/2022 11:04:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=279
06/18/2022 11:04:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.16 on epoch=283
06/18/2022 11:04:54 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.4507936507936508 on epoch=283
06/18/2022 11:04:54 - INFO - __main__ - Saving model with best Classification-F1: 0.41245791245791247 -> 0.4507936507936508 on epoch=283, global_step=850
06/18/2022 11:04:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.17 on epoch=286
06/18/2022 11:05:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=289
06/18/2022 11:05:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=293
06/18/2022 11:05:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=296
06/18/2022 11:05:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=299
06/18/2022 11:05:09 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.41197554290533417 on epoch=299
06/18/2022 11:05:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=303
06/18/2022 11:05:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=306
06/18/2022 11:05:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=309
06/18/2022 11:05:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=313
06/18/2022 11:05:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.15 on epoch=316
06/18/2022 11:05:23 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.4410774410774412 on epoch=316
06/18/2022 11:05:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=319
06/18/2022 11:05:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=323
06/18/2022 11:05:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=326
06/18/2022 11:05:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=329
06/18/2022 11:05:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=333
06/18/2022 11:05:38 - INFO - __main__ - Global step 1000 Train loss 0.15 Classification-F1 0.4987468671679198 on epoch=333
06/18/2022 11:05:38 - INFO - __main__ - Saving model with best Classification-F1: 0.4507936507936508 -> 0.4987468671679198 on epoch=333, global_step=1000
06/18/2022 11:05:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=336
06/18/2022 11:05:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=339
06/18/2022 11:05:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=343
06/18/2022 11:05:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=346
06/18/2022 11:05:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=349
06/18/2022 11:05:52 - INFO - __main__ - Global step 1050 Train loss 0.11 Classification-F1 0.4320261437908497 on epoch=349
06/18/2022 11:05:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=353
06/18/2022 11:05:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=356
06/18/2022 11:06:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=359
06/18/2022 11:06:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=363
06/18/2022 11:06:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=366
06/18/2022 11:06:07 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.44733044733044736 on epoch=366
06/18/2022 11:06:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=369
06/18/2022 11:06:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=373
06/18/2022 11:06:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=376
06/18/2022 11:06:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.10 on epoch=379
06/18/2022 11:06:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=383
06/18/2022 11:06:21 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.3604554865424431 on epoch=383
06/18/2022 11:06:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=386
06/18/2022 11:06:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=389
06/18/2022 11:06:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=393
06/18/2022 11:06:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=396
06/18/2022 11:06:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=399
06/18/2022 11:06:36 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.3124727211683733 on epoch=399
06/18/2022 11:06:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.11 on epoch=403
06/18/2022 11:06:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=406
06/18/2022 11:06:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=409
06/18/2022 11:06:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=413
06/18/2022 11:06:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=416
06/18/2022 11:06:50 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.43810437735291946 on epoch=416
06/18/2022 11:06:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.10 on epoch=419
06/18/2022 11:06:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=423
06/18/2022 11:06:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.13 on epoch=426
06/18/2022 11:07:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=429
06/18/2022 11:07:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=433
06/18/2022 11:07:04 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.31807359307359306 on epoch=433
06/18/2022 11:07:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=436
06/18/2022 11:07:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=439
06/18/2022 11:07:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=443
06/18/2022 11:07:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=446
06/18/2022 11:07:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=449
06/18/2022 11:07:19 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.5007130124777184 on epoch=449
06/18/2022 11:07:19 - INFO - __main__ - Saving model with best Classification-F1: 0.4987468671679198 -> 0.5007130124777184 on epoch=449, global_step=1350
06/18/2022 11:07:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=453
06/18/2022 11:07:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=456
06/18/2022 11:07:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=459
06/18/2022 11:07:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=463
06/18/2022 11:07:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=466
06/18/2022 11:07:33 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.4671717171717172 on epoch=466
06/18/2022 11:07:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=469
06/18/2022 11:07:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=473
06/18/2022 11:07:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=476
06/18/2022 11:07:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=479
06/18/2022 11:07:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
06/18/2022 11:07:48 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.35119047619047616 on epoch=483
06/18/2022 11:07:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=486
06/18/2022 11:07:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=489
06/18/2022 11:07:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=493
06/18/2022 11:07:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=496
06/18/2022 11:08:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=499
06/18/2022 11:08:02 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.32147588765235824 on epoch=499
06/18/2022 11:08:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=503
06/18/2022 11:08:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
06/18/2022 11:08:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=509
06/18/2022 11:08:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=513
06/18/2022 11:08:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
06/18/2022 11:08:17 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.26003276003276 on epoch=516
06/18/2022 11:08:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
06/18/2022 11:08:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=523
06/18/2022 11:08:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
06/18/2022 11:08:27 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
06/18/2022 11:08:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=533
06/18/2022 11:08:31 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.24336609336609336 on epoch=533
06/18/2022 11:08:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=536
06/18/2022 11:08:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=539
06/18/2022 11:08:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
06/18/2022 11:08:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
06/18/2022 11:08:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=549
06/18/2022 11:08:46 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.25072463768115943 on epoch=549
06/18/2022 11:08:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
06/18/2022 11:08:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=556
06/18/2022 11:08:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=559
06/18/2022 11:08:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
06/18/2022 11:08:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
06/18/2022 11:09:00 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.27714285714285714 on epoch=566
06/18/2022 11:09:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=569
06/18/2022 11:09:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=573
06/18/2022 11:09:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=576
06/18/2022 11:09:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=579
06/18/2022 11:09:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=583
06/18/2022 11:09:14 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.22417711598746082 on epoch=583
06/18/2022 11:09:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
06/18/2022 11:09:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=589
06/18/2022 11:09:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=593
06/18/2022 11:09:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
06/18/2022 11:09:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
06/18/2022 11:09:29 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.3254608294930876 on epoch=599
06/18/2022 11:09:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=603
06/18/2022 11:09:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=606
06/18/2022 11:09:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=609
06/18/2022 11:09:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=613
06/18/2022 11:09:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
06/18/2022 11:09:43 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.2875064004096262 on epoch=616
06/18/2022 11:09:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=619
06/18/2022 11:09:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=623
06/18/2022 11:09:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
06/18/2022 11:09:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
06/18/2022 11:09:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
06/18/2022 11:09:58 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.20555555555555557 on epoch=633
06/18/2022 11:10:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
06/18/2022 11:10:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=639
06/18/2022 11:10:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/18/2022 11:10:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=646
06/18/2022 11:10:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
06/18/2022 11:10:12 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.3114285714285714 on epoch=649
06/18/2022 11:10:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=653
06/18/2022 11:10:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=656
06/18/2022 11:10:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=659
06/18/2022 11:10:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=663
06/18/2022 11:10:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/18/2022 11:10:27 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.37175678352148944 on epoch=666
06/18/2022 11:10:29 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=669
06/18/2022 11:10:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=673
06/18/2022 11:10:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=676
06/18/2022 11:10:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
06/18/2022 11:10:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
06/18/2022 11:10:41 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.23909882232462878 on epoch=683
06/18/2022 11:10:44 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=686
06/18/2022 11:10:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
06/18/2022 11:10:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=693
06/18/2022 11:10:52 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=696
06/18/2022 11:10:54 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=699
06/18/2022 11:10:56 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.26883782883782886 on epoch=699
06/18/2022 11:10:58 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=703
06/18/2022 11:11:01 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
06/18/2022 11:11:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/18/2022 11:11:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/18/2022 11:11:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/18/2022 11:11:10 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.23796225893412593 on epoch=716
06/18/2022 11:11:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=719
06/18/2022 11:11:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=723
06/18/2022 11:11:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
06/18/2022 11:11:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
06/18/2022 11:11:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=733
06/18/2022 11:11:25 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.46093189964157705 on epoch=733
06/18/2022 11:11:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/18/2022 11:11:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
06/18/2022 11:11:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=743
06/18/2022 11:11:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/18/2022 11:11:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
06/18/2022 11:11:39 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.3062049062049062 on epoch=749
06/18/2022 11:11:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
06/18/2022 11:11:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/18/2022 11:11:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=759
06/18/2022 11:11:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/18/2022 11:11:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=766
06/18/2022 11:11:54 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.23217893217893218 on epoch=766
06/18/2022 11:11:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
06/18/2022 11:11:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=773
06/18/2022 11:12:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
06/18/2022 11:12:04 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=779
06/18/2022 11:12:07 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/18/2022 11:12:08 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.15816729917773562 on epoch=783
06/18/2022 11:12:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
06/18/2022 11:12:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=789
06/18/2022 11:12:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=793
06/18/2022 11:12:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
06/18/2022 11:12:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/18/2022 11:12:22 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.268100358422939 on epoch=799
06/18/2022 11:12:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=803
06/18/2022 11:12:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=806
06/18/2022 11:12:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/18/2022 11:12:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=813
06/18/2022 11:12:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=816
06/18/2022 11:12:36 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.3213967405143876 on epoch=816
06/18/2022 11:12:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/18/2022 11:12:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
06/18/2022 11:12:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/18/2022 11:12:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/18/2022 11:12:49 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
06/18/2022 11:12:51 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.3169772256728779 on epoch=833
06/18/2022 11:12:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
06/18/2022 11:12:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/18/2022 11:12:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
06/18/2022 11:13:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
06/18/2022 11:13:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=849
06/18/2022 11:13:05 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.20661630036630033 on epoch=849
06/18/2022 11:13:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
06/18/2022 11:13:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=856
06/18/2022 11:13:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=859
06/18/2022 11:13:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/18/2022 11:13:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
06/18/2022 11:13:19 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.14684095860566448 on epoch=866
06/18/2022 11:13:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
06/18/2022 11:13:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=873
06/18/2022 11:13:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/18/2022 11:13:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=879
06/18/2022 11:13:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
06/18/2022 11:13:33 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.15555555555555553 on epoch=883
06/18/2022 11:13:36 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=886
06/18/2022 11:13:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
06/18/2022 11:13:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
06/18/2022 11:13:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=896
06/18/2022 11:13:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=899
06/18/2022 11:13:47 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.1791800356506239 on epoch=899
06/18/2022 11:13:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
06/18/2022 11:13:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
06/18/2022 11:13:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
06/18/2022 11:13:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
06/18/2022 11:14:00 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/18/2022 11:14:02 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.15384615384615385 on epoch=916
06/18/2022 11:14:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
06/18/2022 11:14:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=923
06/18/2022 11:14:09 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=926
06/18/2022 11:14:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/18/2022 11:14:14 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=933
06/18/2022 11:14:16 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.255241935483871 on epoch=933
06/18/2022 11:14:18 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
06/18/2022 11:14:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=939
06/18/2022 11:14:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=943
06/18/2022 11:14:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=946
06/18/2022 11:14:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=949
06/18/2022 11:14:30 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.3272375760649087 on epoch=949
06/18/2022 11:14:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
06/18/2022 11:14:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=956
06/18/2022 11:14:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
06/18/2022 11:14:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/18/2022 11:14:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
06/18/2022 11:14:44 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.31138975966562177 on epoch=966
06/18/2022 11:14:47 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/18/2022 11:14:49 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
06/18/2022 11:14:52 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
06/18/2022 11:14:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
06/18/2022 11:14:57 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
06/18/2022 11:14:59 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.4652583618100859 on epoch=983
06/18/2022 11:15:01 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=986
06/18/2022 11:15:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/18/2022 11:15:06 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/18/2022 11:15:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
06/18/2022 11:15:11 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
06/18/2022 11:15:13 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:15:13 - INFO - __main__ - Printing 3 examples
06/18/2022 11:15:13 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/18/2022 11:15:13 - INFO - __main__ - ['neutral']
06/18/2022 11:15:13 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/18/2022 11:15:13 - INFO - __main__ - ['neutral']
06/18/2022 11:15:13 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/18/2022 11:15:13 - INFO - __main__ - ['neutral']
06/18/2022 11:15:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 11:15:13 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.25636565881282236 on epoch=999
06/18/2022 11:15:13 - INFO - __main__ - save last model!
06/18/2022 11:15:13 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:15:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 11:15:13 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 11:15:13 - INFO - __main__ - Printing 3 examples
06/18/2022 11:15:13 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 11:15:13 - INFO - __main__ - ['contradiction']
06/18/2022 11:15:13 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 11:15:13 - INFO - __main__ - ['entailment']
06/18/2022 11:15:13 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 11:15:13 - INFO - __main__ - ['contradiction']
06/18/2022 11:15:13 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 11:15:13 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 11:15:13 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:15:13 - INFO - __main__ - Printing 3 examples
06/18/2022 11:15:13 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/18/2022 11:15:13 - INFO - __main__ - ['neutral']
06/18/2022 11:15:13 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/18/2022 11:15:13 - INFO - __main__ - ['neutral']
06/18/2022 11:15:13 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/18/2022 11:15:13 - INFO - __main__ - ['neutral']
06/18/2022 11:15:13 - INFO - __main__ - Tokenizing Input ...
06/18/2022 11:15:13 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:15:13 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 11:15:13 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:15:14 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 11:15:28 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 11:15:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 11:15:29 - INFO - __main__ - Starting training!
06/18/2022 11:15:44 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_100_0.3_8_predictions.txt
06/18/2022 11:15:44 - INFO - __main__ - Classification-F1 on test data: 0.0189
06/18/2022 11:15:45 - INFO - __main__ - prefix=anli_16_100, lr=0.3, bsz=8, dev_performance=0.5007130124777184, test_performance=0.018899299889816045
06/18/2022 11:15:45 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.2, bsz=8 ...
06/18/2022 11:15:45 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:15:45 - INFO - __main__ - Printing 3 examples
06/18/2022 11:15:45 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/18/2022 11:15:45 - INFO - __main__ - ['neutral']
06/18/2022 11:15:45 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/18/2022 11:15:45 - INFO - __main__ - ['neutral']
06/18/2022 11:15:45 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/18/2022 11:15:45 - INFO - __main__ - ['neutral']
06/18/2022 11:15:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 11:15:45 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:15:46 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 11:15:46 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:15:46 - INFO - __main__ - Printing 3 examples
06/18/2022 11:15:46 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/18/2022 11:15:46 - INFO - __main__ - ['neutral']
06/18/2022 11:15:46 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/18/2022 11:15:46 - INFO - __main__ - ['neutral']
06/18/2022 11:15:46 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/18/2022 11:15:46 - INFO - __main__ - ['neutral']
06/18/2022 11:15:46 - INFO - __main__ - Tokenizing Input ...
06/18/2022 11:15:46 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:15:46 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 11:16:01 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 11:16:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 11:16:01 - INFO - __main__ - Starting training!
06/18/2022 11:16:05 - INFO - __main__ - Step 10 Global step 10 Train loss 0.81 on epoch=3
06/18/2022 11:16:07 - INFO - __main__ - Step 20 Global step 20 Train loss 0.58 on epoch=6
06/18/2022 11:16:10 - INFO - __main__ - Step 30 Global step 30 Train loss 0.57 on epoch=9
06/18/2022 11:16:12 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=13
06/18/2022 11:16:15 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=16
06/18/2022 11:16:16 - INFO - __main__ - Global step 50 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=16
06/18/2022 11:16:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/18/2022 11:16:19 - INFO - __main__ - Step 60 Global step 60 Train loss 0.52 on epoch=19
06/18/2022 11:16:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.51 on epoch=23
06/18/2022 11:16:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=26
06/18/2022 11:16:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=29
06/18/2022 11:16:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=33
06/18/2022 11:16:30 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=33
06/18/2022 11:16:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=36
06/18/2022 11:16:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=39
06/18/2022 11:16:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=43
06/18/2022 11:16:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=46
06/18/2022 11:16:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=49
06/18/2022 11:16:44 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.1693121693121693 on epoch=49
06/18/2022 11:16:44 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1693121693121693 on epoch=49, global_step=150
06/18/2022 11:16:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=53
06/18/2022 11:16:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=56
06/18/2022 11:16:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=59
06/18/2022 11:16:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
06/18/2022 11:16:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=66
06/18/2022 11:16:58 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.1693121693121693 on epoch=66
06/18/2022 11:17:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=69
06/18/2022 11:17:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=73
06/18/2022 11:17:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=76
06/18/2022 11:17:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=79
06/18/2022 11:17:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=83
06/18/2022 11:17:12 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.1693121693121693 on epoch=83
06/18/2022 11:17:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=86
06/18/2022 11:17:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=89
06/18/2022 11:17:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
06/18/2022 11:17:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=96
06/18/2022 11:17:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=99
06/18/2022 11:17:26 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.17204301075268816 on epoch=99
06/18/2022 11:17:26 - INFO - __main__ - Saving model with best Classification-F1: 0.1693121693121693 -> 0.17204301075268816 on epoch=99, global_step=300
06/18/2022 11:17:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=103
06/18/2022 11:17:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=106
06/18/2022 11:17:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=109
06/18/2022 11:17:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=113
06/18/2022 11:17:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
06/18/2022 11:17:40 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.17204301075268816 on epoch=116
06/18/2022 11:17:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=119
06/18/2022 11:17:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=123
06/18/2022 11:17:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=126
06/18/2022 11:17:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=129
06/18/2022 11:17:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=133
06/18/2022 11:17:54 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.2442830239440409 on epoch=133
06/18/2022 11:17:54 - INFO - __main__ - Saving model with best Classification-F1: 0.17204301075268816 -> 0.2442830239440409 on epoch=133, global_step=400
06/18/2022 11:17:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=136
06/18/2022 11:17:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=139
06/18/2022 11:18:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=143
06/18/2022 11:18:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=146
06/18/2022 11:18:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=149
06/18/2022 11:18:08 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.24451410658307213 on epoch=149
06/18/2022 11:18:08 - INFO - __main__ - Saving model with best Classification-F1: 0.2442830239440409 -> 0.24451410658307213 on epoch=149, global_step=450
06/18/2022 11:18:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=153
06/18/2022 11:18:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=156
06/18/2022 11:18:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.51 on epoch=159
06/18/2022 11:18:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=163
06/18/2022 11:18:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=166
06/18/2022 11:18:22 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.3688888888888888 on epoch=166
06/18/2022 11:18:22 - INFO - __main__ - Saving model with best Classification-F1: 0.24451410658307213 -> 0.3688888888888888 on epoch=166, global_step=500
06/18/2022 11:18:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=169
06/18/2022 11:18:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=173
06/18/2022 11:18:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=176
06/18/2022 11:18:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=179
06/18/2022 11:18:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=183
06/18/2022 11:18:36 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.4289705566301311 on epoch=183
06/18/2022 11:18:36 - INFO - __main__ - Saving model with best Classification-F1: 0.3688888888888888 -> 0.4289705566301311 on epoch=183, global_step=550
06/18/2022 11:18:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=186
06/18/2022 11:18:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=189
06/18/2022 11:18:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=193
06/18/2022 11:18:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=196
06/18/2022 11:18:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=199
06/18/2022 11:18:50 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.39111111111111113 on epoch=199
06/18/2022 11:18:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=203
06/18/2022 11:18:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=206
06/18/2022 11:18:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=209
06/18/2022 11:19:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=213
06/18/2022 11:19:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=216
06/18/2022 11:19:05 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.50940015645898 on epoch=216
06/18/2022 11:19:05 - INFO - __main__ - Saving model with best Classification-F1: 0.4289705566301311 -> 0.50940015645898 on epoch=216, global_step=650
06/18/2022 11:19:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=219
06/18/2022 11:19:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=223
06/18/2022 11:19:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.33 on epoch=226
06/18/2022 11:19:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=229
06/18/2022 11:19:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=233
06/18/2022 11:19:19 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.4536660408844389 on epoch=233
06/18/2022 11:19:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=236
06/18/2022 11:19:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=239
06/18/2022 11:19:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=243
06/18/2022 11:19:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=246
06/18/2022 11:19:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=249
06/18/2022 11:19:33 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.40972222222222215 on epoch=249
06/18/2022 11:19:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=253
06/18/2022 11:19:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.30 on epoch=256
06/18/2022 11:19:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=259
06/18/2022 11:19:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=263
06/18/2022 11:19:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=266
06/18/2022 11:19:47 - INFO - __main__ - Global step 800 Train loss 0.31 Classification-F1 0.38973008558262023 on epoch=266
06/18/2022 11:19:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=269
06/18/2022 11:19:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=273
06/18/2022 11:19:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=276
06/18/2022 11:19:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=279
06/18/2022 11:20:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.28 on epoch=283
06/18/2022 11:20:01 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.4529179260637062 on epoch=283
06/18/2022 11:20:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.30 on epoch=286
06/18/2022 11:20:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=289
06/18/2022 11:20:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.29 on epoch=293
06/18/2022 11:20:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=296
06/18/2022 11:20:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=299
06/18/2022 11:20:15 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.4334809192494202 on epoch=299
06/18/2022 11:20:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.25 on epoch=303
06/18/2022 11:20:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.28 on epoch=306
06/18/2022 11:20:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.27 on epoch=309
06/18/2022 11:20:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=313
06/18/2022 11:20:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=316
06/18/2022 11:20:30 - INFO - __main__ - Global step 950 Train loss 0.27 Classification-F1 0.42751322751322746 on epoch=316
06/18/2022 11:20:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.30 on epoch=319
06/18/2022 11:20:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=323
06/18/2022 11:20:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=326
06/18/2022 11:20:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=329
06/18/2022 11:20:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=333
06/18/2022 11:20:43 - INFO - __main__ - Global step 1000 Train loss 0.29 Classification-F1 0.3479853479853479 on epoch=333
06/18/2022 11:20:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=336
06/18/2022 11:20:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=339
06/18/2022 11:20:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.25 on epoch=343
06/18/2022 11:20:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.28 on epoch=346
06/18/2022 11:20:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.28 on epoch=349
06/18/2022 11:20:58 - INFO - __main__ - Global step 1050 Train loss 0.26 Classification-F1 0.41269841269841273 on epoch=349
06/18/2022 11:21:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=353
06/18/2022 11:21:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=356
06/18/2022 11:21:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=359
06/18/2022 11:21:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=363
06/18/2022 11:21:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=366
06/18/2022 11:21:12 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.4156436520968541 on epoch=366
06/18/2022 11:21:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=369
06/18/2022 11:21:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=373
06/18/2022 11:21:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=376
06/18/2022 11:21:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=379
06/18/2022 11:21:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=383
06/18/2022 11:21:26 - INFO - __main__ - Global step 1150 Train loss 0.23 Classification-F1 0.46804632705795496 on epoch=383
06/18/2022 11:21:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=386
06/18/2022 11:21:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=389
06/18/2022 11:21:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=393
06/18/2022 11:21:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=396
06/18/2022 11:21:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=399
06/18/2022 11:21:40 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.42861257109943124 on epoch=399
06/18/2022 11:21:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.19 on epoch=403
06/18/2022 11:21:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=406
06/18/2022 11:21:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=409
06/18/2022 11:21:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=413
06/18/2022 11:21:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=416
06/18/2022 11:21:54 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.47089947089947093 on epoch=416
06/18/2022 11:21:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.25 on epoch=419
06/18/2022 11:21:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=423
06/18/2022 11:22:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=426
06/18/2022 11:22:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=429
06/18/2022 11:22:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=433
06/18/2022 11:22:09 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.42397660818713456 on epoch=433
06/18/2022 11:22:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=436
06/18/2022 11:22:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=439
06/18/2022 11:22:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=443
06/18/2022 11:22:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=446
06/18/2022 11:22:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.11 on epoch=449
06/18/2022 11:22:23 - INFO - __main__ - Global step 1350 Train loss 0.17 Classification-F1 0.4627168696936139 on epoch=449
06/18/2022 11:22:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=453
06/18/2022 11:22:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.14 on epoch=456
06/18/2022 11:22:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=459
06/18/2022 11:22:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.14 on epoch=463
06/18/2022 11:22:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=466
06/18/2022 11:22:37 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.44721299372462164 on epoch=466
06/18/2022 11:22:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=469
06/18/2022 11:22:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=473
06/18/2022 11:22:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=476
06/18/2022 11:22:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=479
06/18/2022 11:22:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=483
06/18/2022 11:22:51 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.3035582303874987 on epoch=483
06/18/2022 11:22:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=486
06/18/2022 11:22:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=489
06/18/2022 11:22:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=493
06/18/2022 11:23:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=496
06/18/2022 11:23:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=499
06/18/2022 11:23:05 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.3002032520325203 on epoch=499
06/18/2022 11:23:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=503
06/18/2022 11:23:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=506
06/18/2022 11:23:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=509
06/18/2022 11:23:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=513
06/18/2022 11:23:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=516
06/18/2022 11:23:20 - INFO - __main__ - Global step 1550 Train loss 0.12 Classification-F1 0.24333333333333332 on epoch=516
06/18/2022 11:23:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=519
06/18/2022 11:23:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=523
06/18/2022 11:23:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=526
06/18/2022 11:23:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=529
06/18/2022 11:23:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=533
06/18/2022 11:23:34 - INFO - __main__ - Global step 1600 Train loss 0.11 Classification-F1 0.27725274725274723 on epoch=533
06/18/2022 11:23:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=536
06/18/2022 11:23:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=539
06/18/2022 11:23:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.10 on epoch=543
06/18/2022 11:23:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=546
06/18/2022 11:23:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=549
06/18/2022 11:23:48 - INFO - __main__ - Global step 1650 Train loss 0.12 Classification-F1 0.3683423913043478 on epoch=549
06/18/2022 11:23:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=553
06/18/2022 11:23:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=556
06/18/2022 11:23:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=559
06/18/2022 11:23:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.12 on epoch=563
06/18/2022 11:24:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=566
06/18/2022 11:24:02 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.2847434819175778 on epoch=566
06/18/2022 11:24:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=569
06/18/2022 11:24:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=573
06/18/2022 11:24:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=576
06/18/2022 11:24:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=579
06/18/2022 11:24:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=583
06/18/2022 11:24:17 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.4611111111111111 on epoch=583
06/18/2022 11:24:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=586
06/18/2022 11:24:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=589
06/18/2022 11:24:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=593
06/18/2022 11:24:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=596
06/18/2022 11:24:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=599
06/18/2022 11:24:31 - INFO - __main__ - Global step 1800 Train loss 0.09 Classification-F1 0.46968633705475815 on epoch=599
06/18/2022 11:24:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=603
06/18/2022 11:24:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=606
06/18/2022 11:24:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=609
06/18/2022 11:24:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=613
06/18/2022 11:24:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=616
06/18/2022 11:24:45 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.26522972972972975 on epoch=616
06/18/2022 11:24:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=619
06/18/2022 11:24:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=623
06/18/2022 11:24:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=626
06/18/2022 11:24:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=629
06/18/2022 11:24:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=633
06/18/2022 11:24:59 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.3153532608695652 on epoch=633
06/18/2022 11:25:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=636
06/18/2022 11:25:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=639
06/18/2022 11:25:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=643
06/18/2022 11:25:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=646
06/18/2022 11:25:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=649
06/18/2022 11:25:14 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.40035412158174305 on epoch=649
06/18/2022 11:25:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=653
06/18/2022 11:25:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=656
06/18/2022 11:25:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
06/18/2022 11:25:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=663
06/18/2022 11:25:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=666
06/18/2022 11:25:28 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.2730072463768116 on epoch=666
06/18/2022 11:25:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=669
06/18/2022 11:25:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=673
06/18/2022 11:25:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=676
06/18/2022 11:25:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=679
06/18/2022 11:25:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.10 on epoch=683
06/18/2022 11:25:42 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.24220196353436188 on epoch=683
06/18/2022 11:25:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=686
06/18/2022 11:25:47 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=689
06/18/2022 11:25:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=693
06/18/2022 11:25:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
06/18/2022 11:25:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=699
06/18/2022 11:25:57 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.2378595317725752 on epoch=699
06/18/2022 11:25:59 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=703
06/18/2022 11:26:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=706
06/18/2022 11:26:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=709
06/18/2022 11:26:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
06/18/2022 11:26:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=716
06/18/2022 11:26:11 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.2510769230769231 on epoch=716
06/18/2022 11:26:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=719
06/18/2022 11:26:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=723
06/18/2022 11:26:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=726
06/18/2022 11:26:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=729
06/18/2022 11:26:24 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=733
06/18/2022 11:26:25 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.2920405405405405 on epoch=733
06/18/2022 11:26:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=736
06/18/2022 11:26:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=739
06/18/2022 11:26:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=743
06/18/2022 11:26:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=746
06/18/2022 11:26:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=749
06/18/2022 11:26:39 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.43831168831168826 on epoch=749
06/18/2022 11:26:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
06/18/2022 11:26:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=756
06/18/2022 11:26:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=759
06/18/2022 11:26:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=763
06/18/2022 11:26:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=766
06/18/2022 11:26:53 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.4084552845528455 on epoch=766
06/18/2022 11:26:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=769
06/18/2022 11:26:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.10 on epoch=773
06/18/2022 11:27:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=776
06/18/2022 11:27:04 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=779
06/18/2022 11:27:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=783
06/18/2022 11:27:08 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.27434210526315794 on epoch=783
06/18/2022 11:27:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
06/18/2022 11:27:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=789
06/18/2022 11:27:15 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=793
06/18/2022 11:27:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=796
06/18/2022 11:27:20 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=799
06/18/2022 11:27:22 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.2839021164021164 on epoch=799
06/18/2022 11:27:24 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=803
06/18/2022 11:27:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=806
06/18/2022 11:27:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=809
06/18/2022 11:27:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=813
06/18/2022 11:27:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=816
06/18/2022 11:27:36 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.2760405405405405 on epoch=816
06/18/2022 11:27:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
06/18/2022 11:27:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=823
06/18/2022 11:27:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=826
06/18/2022 11:27:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=829
06/18/2022 11:27:49 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=833
06/18/2022 11:27:51 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.22402777777777774 on epoch=833
06/18/2022 11:27:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=836
06/18/2022 11:27:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=839
06/18/2022 11:27:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=843
06/18/2022 11:28:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=846
06/18/2022 11:28:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=849
06/18/2022 11:28:05 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.2360544687910417 on epoch=849
06/18/2022 11:28:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=853
06/18/2022 11:28:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=856
06/18/2022 11:28:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=859
06/18/2022 11:28:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
06/18/2022 11:28:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=866
06/18/2022 11:28:19 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.21144595359366158 on epoch=866
06/18/2022 11:28:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=869
06/18/2022 11:28:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=873
06/18/2022 11:28:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=876
06/18/2022 11:28:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=879
06/18/2022 11:28:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=883
06/18/2022 11:28:33 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.1975117975117975 on epoch=883
06/18/2022 11:28:36 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=886
06/18/2022 11:28:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=889
06/18/2022 11:28:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
06/18/2022 11:28:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=896
06/18/2022 11:28:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=899
06/18/2022 11:28:48 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.23790849673202613 on epoch=899
06/18/2022 11:28:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=903
06/18/2022 11:28:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=906
06/18/2022 11:28:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=909
06/18/2022 11:28:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=913
06/18/2022 11:29:00 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/18/2022 11:29:02 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.3366572024766531 on epoch=916
06/18/2022 11:29:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=919
06/18/2022 11:29:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=923
06/18/2022 11:29:09 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=926
06/18/2022 11:29:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=929
06/18/2022 11:29:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=933
06/18/2022 11:29:16 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.3258848994143112 on epoch=933
06/18/2022 11:29:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
06/18/2022 11:29:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=939
06/18/2022 11:29:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=943
06/18/2022 11:29:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=946
06/18/2022 11:29:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=949
06/18/2022 11:29:30 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.22852930852930853 on epoch=949
06/18/2022 11:29:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
06/18/2022 11:29:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=956
06/18/2022 11:29:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
06/18/2022 11:29:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/18/2022 11:29:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
06/18/2022 11:29:45 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.22269144144144146 on epoch=966
06/18/2022 11:29:47 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=969
06/18/2022 11:29:50 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
06/18/2022 11:29:52 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=976
06/18/2022 11:29:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
06/18/2022 11:29:57 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=983
06/18/2022 11:29:59 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.22109988776655443 on epoch=983
06/18/2022 11:30:01 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=986
06/18/2022 11:30:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=989
06/18/2022 11:30:06 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
06/18/2022 11:30:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=996
06/18/2022 11:30:12 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
06/18/2022 11:30:13 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.24178113553113553 on epoch=999
06/18/2022 11:30:13 - INFO - __main__ - save last model!
06/18/2022 11:30:13 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:30:13 - INFO - __main__ - Printing 3 examples
06/18/2022 11:30:13 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/18/2022 11:30:13 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:13 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/18/2022 11:30:13 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:13 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/18/2022 11:30:13 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 11:30:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 11:30:13 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:30:13 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 11:30:13 - INFO - __main__ - Printing 3 examples
06/18/2022 11:30:13 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 11:30:13 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:13 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 11:30:13 - INFO - __main__ - ['entailment']
06/18/2022 11:30:13 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 11:30:13 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:13 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 11:30:13 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 11:30:13 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:30:13 - INFO - __main__ - Printing 3 examples
06/18/2022 11:30:13 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/18/2022 11:30:13 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:13 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/18/2022 11:30:13 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:13 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/18/2022 11:30:13 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:13 - INFO - __main__ - Tokenizing Input ...
06/18/2022 11:30:13 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:30:13 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 11:30:14 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:30:15 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 11:30:32 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 11:30:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 11:30:33 - INFO - __main__ - Starting training!
06/18/2022 11:30:45 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_100_0.2_8_predictions.txt
06/18/2022 11:30:45 - INFO - __main__ - Classification-F1 on test data: 0.0192
06/18/2022 11:30:45 - INFO - __main__ - prefix=anli_16_100, lr=0.2, bsz=8, dev_performance=0.50940015645898, test_performance=0.019231889119434107
06/18/2022 11:30:45 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.5, bsz=8 ...
06/18/2022 11:30:46 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:30:46 - INFO - __main__ - Printing 3 examples
06/18/2022 11:30:46 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/18/2022 11:30:46 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:46 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/18/2022 11:30:46 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:46 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/18/2022 11:30:46 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 11:30:46 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:30:46 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 11:30:46 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:30:46 - INFO - __main__ - Printing 3 examples
06/18/2022 11:30:46 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/18/2022 11:30:46 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:46 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/18/2022 11:30:46 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:46 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/18/2022 11:30:46 - INFO - __main__ - ['contradiction']
06/18/2022 11:30:46 - INFO - __main__ - Tokenizing Input ...
06/18/2022 11:30:46 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:30:46 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 11:31:04 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 11:31:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 11:31:05 - INFO - __main__ - Starting training!
06/18/2022 11:31:09 - INFO - __main__ - Step 10 Global step 10 Train loss 0.68 on epoch=3
06/18/2022 11:31:11 - INFO - __main__ - Step 20 Global step 20 Train loss 0.48 on epoch=6
06/18/2022 11:31:14 - INFO - __main__ - Step 30 Global step 30 Train loss 0.45 on epoch=9
06/18/2022 11:31:17 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=13
06/18/2022 11:31:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=16
06/18/2022 11:31:21 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.36224781507800374 on epoch=16
06/18/2022 11:31:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.36224781507800374 on epoch=16, global_step=50
06/18/2022 11:31:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=19
06/18/2022 11:31:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.39 on epoch=23
06/18/2022 11:31:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=26
06/18/2022 11:31:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=29
06/18/2022 11:31:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=33
06/18/2022 11:31:35 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.27602905569007263 on epoch=33
06/18/2022 11:31:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=36
06/18/2022 11:31:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=39
06/18/2022 11:31:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=43
06/18/2022 11:31:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.39 on epoch=46
06/18/2022 11:31:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.39 on epoch=49
06/18/2022 11:31:50 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.38492063492063494 on epoch=49
06/18/2022 11:31:50 - INFO - __main__ - Saving model with best Classification-F1: 0.36224781507800374 -> 0.38492063492063494 on epoch=49, global_step=150
06/18/2022 11:31:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=53
06/18/2022 11:31:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=56
06/18/2022 11:31:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=59
06/18/2022 11:32:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=63
06/18/2022 11:32:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=66
06/18/2022 11:32:04 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3657262277951933 on epoch=66
06/18/2022 11:32:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=69
06/18/2022 11:32:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=73
06/18/2022 11:32:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=76
06/18/2022 11:32:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=79
06/18/2022 11:32:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.33 on epoch=83
06/18/2022 11:32:19 - INFO - __main__ - Global step 250 Train loss 0.36 Classification-F1 0.3657262277951933 on epoch=83
06/18/2022 11:32:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.29 on epoch=86
06/18/2022 11:32:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.31 on epoch=89
06/18/2022 11:32:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.34 on epoch=93
06/18/2022 11:32:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=96
06/18/2022 11:32:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.33 on epoch=99
06/18/2022 11:32:33 - INFO - __main__ - Global step 300 Train loss 0.31 Classification-F1 0.4444444444444445 on epoch=99
06/18/2022 11:32:33 - INFO - __main__ - Saving model with best Classification-F1: 0.38492063492063494 -> 0.4444444444444445 on epoch=99, global_step=300
06/18/2022 11:32:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=103
06/18/2022 11:32:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=106
06/18/2022 11:32:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=109
06/18/2022 11:32:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=113
06/18/2022 11:32:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=116
06/18/2022 11:32:47 - INFO - __main__ - Global step 350 Train loss 0.29 Classification-F1 0.4432234432234432 on epoch=116
06/18/2022 11:32:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=119
06/18/2022 11:32:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.29 on epoch=123
06/18/2022 11:32:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=126
06/18/2022 11:32:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=129
06/18/2022 11:33:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=133
06/18/2022 11:33:02 - INFO - __main__ - Global step 400 Train loss 0.26 Classification-F1 0.44817144188213365 on epoch=133
06/18/2022 11:33:02 - INFO - __main__ - Saving model with best Classification-F1: 0.4444444444444445 -> 0.44817144188213365 on epoch=133, global_step=400
06/18/2022 11:33:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=136
06/18/2022 11:33:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=139
06/18/2022 11:33:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.20 on epoch=143
06/18/2022 11:33:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=146
06/18/2022 11:33:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=149
06/18/2022 11:33:16 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.41269841269841273 on epoch=149
06/18/2022 11:33:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=153
06/18/2022 11:33:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=156
06/18/2022 11:33:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=159
06/18/2022 11:33:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.18 on epoch=163
06/18/2022 11:33:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.16 on epoch=166
06/18/2022 11:33:31 - INFO - __main__ - Global step 500 Train loss 0.18 Classification-F1 0.48501239576290284 on epoch=166
06/18/2022 11:33:31 - INFO - __main__ - Saving model with best Classification-F1: 0.44817144188213365 -> 0.48501239576290284 on epoch=166, global_step=500
06/18/2022 11:33:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=169
06/18/2022 11:33:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=173
06/18/2022 11:33:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.29 on epoch=176
06/18/2022 11:33:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.16 on epoch=179
06/18/2022 11:33:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.15 on epoch=183
06/18/2022 11:33:45 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3458333333333333 on epoch=183
06/18/2022 11:33:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.13 on epoch=186
06/18/2022 11:33:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=189
06/18/2022 11:33:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=193
06/18/2022 11:33:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.12 on epoch=196
06/18/2022 11:33:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=199
06/18/2022 11:34:00 - INFO - __main__ - Global step 600 Train loss 0.15 Classification-F1 0.43922127255460586 on epoch=199
06/18/2022 11:34:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=203
06/18/2022 11:34:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=206
06/18/2022 11:34:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=209
06/18/2022 11:34:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=213
06/18/2022 11:34:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=216
06/18/2022 11:34:14 - INFO - __main__ - Global step 650 Train loss 0.12 Classification-F1 0.3179347826086957 on epoch=216
06/18/2022 11:34:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=219
06/18/2022 11:34:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=223
06/18/2022 11:34:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=226
06/18/2022 11:34:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=229
06/18/2022 11:34:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.10 on epoch=233
06/18/2022 11:34:29 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.31875 on epoch=233
06/18/2022 11:34:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=236
06/18/2022 11:34:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=239
06/18/2022 11:34:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=243
06/18/2022 11:34:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=246
06/18/2022 11:34:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=249
06/18/2022 11:34:43 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.3179347826086957 on epoch=249
06/18/2022 11:34:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.15 on epoch=253
06/18/2022 11:34:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=256
06/18/2022 11:34:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.14 on epoch=259
06/18/2022 11:34:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=263
06/18/2022 11:34:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.11 on epoch=266
06/18/2022 11:34:58 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.39182908545727135 on epoch=266
06/18/2022 11:35:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=269
06/18/2022 11:35:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=273
06/18/2022 11:35:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=276
06/18/2022 11:35:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=279
06/18/2022 11:35:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=283
06/18/2022 11:35:13 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.30972222222222223 on epoch=283
06/18/2022 11:35:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=286
06/18/2022 11:35:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=289
06/18/2022 11:35:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=293
06/18/2022 11:35:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=296
06/18/2022 11:35:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=299
06/18/2022 11:35:27 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.32473759722117346 on epoch=299
06/18/2022 11:35:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=303
06/18/2022 11:35:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=306
06/18/2022 11:35:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=309
06/18/2022 11:35:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=313
06/18/2022 11:35:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=316
06/18/2022 11:35:42 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.37008995502248876 on epoch=316
06/18/2022 11:35:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=319
06/18/2022 11:35:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=323
06/18/2022 11:35:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=326
06/18/2022 11:35:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=329
06/18/2022 11:35:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=333
06/18/2022 11:35:56 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.3052083333333333 on epoch=333
06/18/2022 11:35:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=336
06/18/2022 11:36:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=339
06/18/2022 11:36:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=343
06/18/2022 11:36:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=346
06/18/2022 11:36:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=349
06/18/2022 11:36:11 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.3384920634920635 on epoch=349
06/18/2022 11:36:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=353
06/18/2022 11:36:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=356
06/18/2022 11:36:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=359
06/18/2022 11:36:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=363
06/18/2022 11:36:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=366
06/18/2022 11:36:25 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.22566137566137567 on epoch=366
06/18/2022 11:36:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=369
06/18/2022 11:36:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
06/18/2022 11:36:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
06/18/2022 11:36:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=379
06/18/2022 11:36:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=383
06/18/2022 11:36:40 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.23697263993316628 on epoch=383
06/18/2022 11:36:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=386
06/18/2022 11:36:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=389
06/18/2022 11:36:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=393
06/18/2022 11:36:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=396
06/18/2022 11:36:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
06/18/2022 11:36:54 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.29409722222222223 on epoch=399
06/18/2022 11:36:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=403
06/18/2022 11:37:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=406
06/18/2022 11:37:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=409
06/18/2022 11:37:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=413
06/18/2022 11:37:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
06/18/2022 11:37:09 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.22420794032662517 on epoch=416
06/18/2022 11:37:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=419
06/18/2022 11:37:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=423
06/18/2022 11:37:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
06/18/2022 11:37:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=429
06/18/2022 11:37:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=433
06/18/2022 11:37:24 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.20185344609307743 on epoch=433
06/18/2022 11:37:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=436
06/18/2022 11:37:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
06/18/2022 11:37:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=443
06/18/2022 11:37:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
06/18/2022 11:37:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
06/18/2022 11:37:38 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.2853921901528013 on epoch=449
06/18/2022 11:37:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=453
06/18/2022 11:37:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
06/18/2022 11:37:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
06/18/2022 11:37:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
06/18/2022 11:37:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
06/18/2022 11:37:53 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.3723196881091618 on epoch=466
06/18/2022 11:37:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
06/18/2022 11:37:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=473
06/18/2022 11:38:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
06/18/2022 11:38:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=479
06/18/2022 11:38:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
06/18/2022 11:38:08 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.3009850830284267 on epoch=483
06/18/2022 11:38:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
06/18/2022 11:38:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
06/18/2022 11:38:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
06/18/2022 11:38:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
06/18/2022 11:38:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
06/18/2022 11:38:23 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.2158119658119658 on epoch=499
06/18/2022 11:38:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
06/18/2022 11:38:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
06/18/2022 11:38:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=509
06/18/2022 11:38:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=513
06/18/2022 11:38:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
06/18/2022 11:38:37 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.2966666666666667 on epoch=516
06/18/2022 11:38:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=519
06/18/2022 11:38:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
06/18/2022 11:38:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
06/18/2022 11:38:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
06/18/2022 11:38:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
06/18/2022 11:38:52 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.2515017162471396 on epoch=533
06/18/2022 11:38:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=536
06/18/2022 11:38:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
06/18/2022 11:39:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
06/18/2022 11:39:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/18/2022 11:39:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
06/18/2022 11:39:06 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.2515017162471396 on epoch=549
06/18/2022 11:39:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
06/18/2022 11:39:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=556
06/18/2022 11:39:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
06/18/2022 11:39:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
06/18/2022 11:39:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
06/18/2022 11:39:21 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.3620923913043478 on epoch=566
06/18/2022 11:39:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
06/18/2022 11:39:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
06/18/2022 11:39:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/18/2022 11:39:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/18/2022 11:39:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
06/18/2022 11:39:36 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.25066454013822437 on epoch=583
06/18/2022 11:39:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=586
06/18/2022 11:39:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=589
06/18/2022 11:39:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/18/2022 11:39:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=596
06/18/2022 11:39:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
06/18/2022 11:39:50 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.28018648018648024 on epoch=599
06/18/2022 11:39:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
06/18/2022 11:39:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/18/2022 11:39:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=609
06/18/2022 11:40:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/18/2022 11:40:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
06/18/2022 11:40:05 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.25213675213675213 on epoch=616
06/18/2022 11:40:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/18/2022 11:40:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
06/18/2022 11:40:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/18/2022 11:40:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
06/18/2022 11:40:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/18/2022 11:40:20 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.24005945745076177 on epoch=633
06/18/2022 11:40:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/18/2022 11:40:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
06/18/2022 11:40:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/18/2022 11:40:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/18/2022 11:40:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
06/18/2022 11:40:35 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.28956376956376956 on epoch=649
06/18/2022 11:40:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/18/2022 11:40:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/18/2022 11:40:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
06/18/2022 11:40:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/18/2022 11:40:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/18/2022 11:40:49 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.28018648018648024 on epoch=666
06/18/2022 11:40:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/18/2022 11:40:55 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
06/18/2022 11:40:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/18/2022 11:41:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=679
06/18/2022 11:41:02 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/18/2022 11:41:04 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.20268620268620266 on epoch=683
06/18/2022 11:41:07 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/18/2022 11:41:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/18/2022 11:41:12 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=693
06/18/2022 11:41:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
06/18/2022 11:41:17 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/18/2022 11:41:18 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.2833333333333333 on epoch=699
06/18/2022 11:41:21 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/18/2022 11:41:24 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/18/2022 11:41:26 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
06/18/2022 11:41:29 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/18/2022 11:41:32 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/18/2022 11:41:33 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.23647522806008078 on epoch=716
06/18/2022 11:41:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
06/18/2022 11:41:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
06/18/2022 11:41:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
06/18/2022 11:41:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/18/2022 11:41:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
06/18/2022 11:41:48 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.21894736842105264 on epoch=733
06/18/2022 11:41:50 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/18/2022 11:41:53 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/18/2022 11:41:56 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/18/2022 11:41:58 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/18/2022 11:42:01 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/18/2022 11:42:02 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.38502258610954265 on epoch=749
06/18/2022 11:42:05 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/18/2022 11:42:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/18/2022 11:42:10 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=759
06/18/2022 11:42:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/18/2022 11:42:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/18/2022 11:42:17 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.24485199485199485 on epoch=766
06/18/2022 11:42:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/18/2022 11:42:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/18/2022 11:42:25 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
06/18/2022 11:42:28 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/18/2022 11:42:30 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/18/2022 11:42:32 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.27669172932330827 on epoch=783
06/18/2022 11:42:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=786
06/18/2022 11:42:37 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/18/2022 11:42:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=793
06/18/2022 11:42:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/18/2022 11:42:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/18/2022 11:42:47 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.260273646071701 on epoch=799
06/18/2022 11:42:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=803
06/18/2022 11:42:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
06/18/2022 11:42:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/18/2022 11:42:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/18/2022 11:43:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/18/2022 11:43:01 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.2982857142857143 on epoch=816
06/18/2022 11:43:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/18/2022 11:43:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/18/2022 11:43:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/18/2022 11:43:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/18/2022 11:43:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=833
06/18/2022 11:43:16 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.28311688311688316 on epoch=833
06/18/2022 11:43:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=836
06/18/2022 11:43:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/18/2022 11:43:24 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
06/18/2022 11:43:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/18/2022 11:43:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/18/2022 11:43:31 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.3504464285714286 on epoch=849
06/18/2022 11:43:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/18/2022 11:43:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
06/18/2022 11:43:39 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/18/2022 11:43:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/18/2022 11:43:44 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/18/2022 11:43:45 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.3009620826259196 on epoch=866
06/18/2022 11:43:48 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/18/2022 11:43:51 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=873
06/18/2022 11:43:53 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/18/2022 11:43:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/18/2022 11:43:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
06/18/2022 11:44:00 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.20957220637801402 on epoch=883
06/18/2022 11:44:03 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=886
06/18/2022 11:44:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=889
06/18/2022 11:44:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=893
06/18/2022 11:44:11 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/18/2022 11:44:14 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/18/2022 11:44:15 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.20659484369161785 on epoch=899
06/18/2022 11:44:18 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/18/2022 11:44:20 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/18/2022 11:44:23 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/18/2022 11:44:26 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
06/18/2022 11:44:28 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/18/2022 11:44:30 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.29826302729528537 on epoch=916
06/18/2022 11:44:32 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=919
06/18/2022 11:44:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=923
06/18/2022 11:44:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
06/18/2022 11:44:40 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/18/2022 11:44:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/18/2022 11:44:44 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.27883575883575884 on epoch=933
06/18/2022 11:44:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/18/2022 11:44:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/18/2022 11:44:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/18/2022 11:44:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/18/2022 11:44:58 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/18/2022 11:44:59 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.29044289044289046 on epoch=949
06/18/2022 11:45:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=953
06/18/2022 11:45:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
06/18/2022 11:45:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/18/2022 11:45:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/18/2022 11:45:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
06/18/2022 11:45:14 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.26043956043956046 on epoch=966
06/18/2022 11:45:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/18/2022 11:45:19 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/18/2022 11:45:22 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/18/2022 11:45:24 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/18/2022 11:45:27 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
06/18/2022 11:45:28 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.3536633146140786 on epoch=983
06/18/2022 11:45:31 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/18/2022 11:45:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/18/2022 11:45:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/18/2022 11:45:39 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/18/2022 11:45:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=999
06/18/2022 11:45:43 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.35512820512820514 on epoch=999
06/18/2022 11:45:43 - INFO - __main__ - save last model!
06/18/2022 11:45:43 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:45:43 - INFO - __main__ - Printing 3 examples
06/18/2022 11:45:43 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/18/2022 11:45:43 - INFO - __main__ - ['contradiction']
06/18/2022 11:45:43 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/18/2022 11:45:43 - INFO - __main__ - ['contradiction']
06/18/2022 11:45:43 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/18/2022 11:45:43 - INFO - __main__ - ['contradiction']
06/18/2022 11:45:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 11:45:43 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:45:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 11:45:43 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 11:45:43 - INFO - __main__ - Printing 3 examples
06/18/2022 11:45:43 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 11:45:43 - INFO - __main__ - ['contradiction']
06/18/2022 11:45:43 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 11:45:43 - INFO - __main__ - ['entailment']
06/18/2022 11:45:43 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 11:45:43 - INFO - __main__ - ['contradiction']
06/18/2022 11:45:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 11:45:43 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 11:45:43 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:45:43 - INFO - __main__ - Printing 3 examples
06/18/2022 11:45:43 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/18/2022 11:45:43 - INFO - __main__ - ['contradiction']
06/18/2022 11:45:43 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/18/2022 11:45:43 - INFO - __main__ - ['contradiction']
06/18/2022 11:45:43 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/18/2022 11:45:43 - INFO - __main__ - ['contradiction']
06/18/2022 11:45:43 - INFO - __main__ - Tokenizing Input ...
06/18/2022 11:45:43 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:45:43 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 11:45:44 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:45:45 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 11:46:02 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 11:46:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 11:46:03 - INFO - __main__ - Starting training!
06/18/2022 11:46:14 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_13_0.5_8_predictions.txt
06/18/2022 11:46:14 - INFO - __main__ - Classification-F1 on test data: 0.0490
06/18/2022 11:46:15 - INFO - __main__ - prefix=anli_16_13, lr=0.5, bsz=8, dev_performance=0.48501239576290284, test_performance=0.04898411810348696
06/18/2022 11:46:15 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.4, bsz=8 ...
06/18/2022 11:46:16 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:46:16 - INFO - __main__ - Printing 3 examples
06/18/2022 11:46:16 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/18/2022 11:46:16 - INFO - __main__ - ['contradiction']
06/18/2022 11:46:16 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/18/2022 11:46:16 - INFO - __main__ - ['contradiction']
06/18/2022 11:46:16 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/18/2022 11:46:16 - INFO - __main__ - ['contradiction']
06/18/2022 11:46:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 11:46:16 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:46:16 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 11:46:16 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 11:46:16 - INFO - __main__ - Printing 3 examples
06/18/2022 11:46:16 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/18/2022 11:46:16 - INFO - __main__ - ['contradiction']
06/18/2022 11:46:16 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/18/2022 11:46:16 - INFO - __main__ - ['contradiction']
06/18/2022 11:46:16 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/18/2022 11:46:16 - INFO - __main__ - ['contradiction']
06/18/2022 11:46:16 - INFO - __main__ - Tokenizing Input ...
06/18/2022 11:46:16 - INFO - __main__ - Tokenizing Output ...
06/18/2022 11:46:16 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 11:46:31 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 11:46:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 11:46:32 - INFO - __main__ - Starting training!
06/18/2022 11:46:35 - INFO - __main__ - Step 10 Global step 10 Train loss 0.68 on epoch=3
06/18/2022 11:46:38 - INFO - __main__ - Step 20 Global step 20 Train loss 0.47 on epoch=6
06/18/2022 11:46:41 - INFO - __main__ - Step 30 Global step 30 Train loss 0.46 on epoch=9
06/18/2022 11:46:43 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=13
06/18/2022 11:46:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=16
06/18/2022 11:46:47 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.24611708482676223 on epoch=16
06/18/2022 11:46:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.24611708482676223 on epoch=16, global_step=50
06/18/2022 11:46:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=19
06/18/2022 11:46:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=23
06/18/2022 11:46:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=26
06/18/2022 11:46:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=29
06/18/2022 11:47:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=33
06/18/2022 11:47:01 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.28012654587287894 on epoch=33
06/18/2022 11:47:01 - INFO - __main__ - Saving model with best Classification-F1: 0.24611708482676223 -> 0.28012654587287894 on epoch=33, global_step=100
06/18/2022 11:47:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=36
06/18/2022 11:47:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=39
06/18/2022 11:47:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
06/18/2022 11:47:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=46
06/18/2022 11:47:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=49
06/18/2022 11:47:15 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.27777777777777773 on epoch=49
06/18/2022 11:47:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=53
06/18/2022 11:47:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=56
06/18/2022 11:47:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=59
06/18/2022 11:47:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
06/18/2022 11:47:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=66
06/18/2022 11:47:30 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.3111111111111111 on epoch=66
06/18/2022 11:47:30 - INFO - __main__ - Saving model with best Classification-F1: 0.28012654587287894 -> 0.3111111111111111 on epoch=66, global_step=200
06/18/2022 11:47:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=69
06/18/2022 11:47:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=73
06/18/2022 11:47:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=76
06/18/2022 11:47:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=79
06/18/2022 11:47:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=83
06/18/2022 11:47:44 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.3111111111111111 on epoch=83
06/18/2022 11:47:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=86
06/18/2022 11:47:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=89
06/18/2022 11:47:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.30 on epoch=93
06/18/2022 11:47:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.34 on epoch=96
06/18/2022 11:47:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=99
06/18/2022 11:47:58 - INFO - __main__ - Global step 300 Train loss 0.35 Classification-F1 0.3657262277951933 on epoch=99
06/18/2022 11:47:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3111111111111111 -> 0.3657262277951933 on epoch=99, global_step=300
06/18/2022 11:48:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.33 on epoch=103
06/18/2022 11:48:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=106
06/18/2022 11:48:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=109
06/18/2022 11:48:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.32 on epoch=113
06/18/2022 11:48:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=116
06/18/2022 11:48:12 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.3111111111111111 on epoch=116
06/18/2022 11:48:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=119
06/18/2022 11:48:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=123
06/18/2022 11:48:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=126
06/18/2022 11:48:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=129
06/18/2022 11:48:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.32 on epoch=133
06/18/2022 11:48:26 - INFO - __main__ - Global step 400 Train loss 0.33 Classification-F1 0.41269841269841273 on epoch=133
06/18/2022 11:48:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3657262277951933 -> 0.41269841269841273 on epoch=133, global_step=400
06/18/2022 11:48:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=136
06/18/2022 11:48:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=139
06/18/2022 11:48:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=143
06/18/2022 11:48:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=146
06/18/2022 11:48:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=149
06/18/2022 11:48:40 - INFO - __main__ - Global step 450 Train loss 0.26 Classification-F1 0.4339393939393939 on epoch=149
06/18/2022 11:48:40 - INFO - __main__ - Saving model with best Classification-F1: 0.41269841269841273 -> 0.4339393939393939 on epoch=149, global_step=450
06/18/2022 11:48:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=153
06/18/2022 11:48:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=156
06/18/2022 11:48:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=159
06/18/2022 11:48:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=163
06/18/2022 11:48:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=166
06/18/2022 11:48:54 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.4339393939393939 on epoch=166
06/18/2022 11:48:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=169
06/18/2022 11:49:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.18 on epoch=173
06/18/2022 11:49:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=176
06/18/2022 11:49:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.20 on epoch=179
06/18/2022 11:49:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=183
06/18/2022 11:49:09 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.45394112060778724 on epoch=183
06/18/2022 11:49:09 - INFO - __main__ - Saving model with best Classification-F1: 0.4339393939393939 -> 0.45394112060778724 on epoch=183, global_step=550
06/18/2022 11:49:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=186
06/18/2022 11:49:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.15 on epoch=189
06/18/2022 11:49:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=193
06/18/2022 11:49:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=196
06/18/2022 11:49:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=199
06/18/2022 11:49:23 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.41269841269841273 on epoch=199
06/18/2022 11:49:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=203
06/18/2022 11:49:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=206
06/18/2022 11:49:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=209
06/18/2022 11:49:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=213
06/18/2022 11:49:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=216
06/18/2022 11:49:37 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.3188888888888889 on epoch=216
06/18/2022 11:49:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=219
06/18/2022 11:49:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=223
06/18/2022 11:49:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=226
06/18/2022 11:49:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=229
06/18/2022 11:49:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=233
06/18/2022 11:49:51 - INFO - __main__ - Global step 700 Train loss 0.14 Classification-F1 0.3158263305322129 on epoch=233
06/18/2022 11:49:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=236
06/18/2022 11:49:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=239
06/18/2022 11:49:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.18 on epoch=243
06/18/2022 11:50:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=246
06/18/2022 11:50:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=249
06/18/2022 11:50:05 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.3185714285714286 on epoch=249
06/18/2022 11:50:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=253
06/18/2022 11:50:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=256
06/18/2022 11:50:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.14 on epoch=259
06/18/2022 11:50:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=263
06/18/2022 11:50:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=266
06/18/2022 11:50:19 - INFO - __main__ - Global step 800 Train loss 0.14 Classification-F1 0.3185714285714286 on epoch=266
06/18/2022 11:50:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=269
06/18/2022 11:50:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=273
06/18/2022 11:50:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=276
06/18/2022 11:50:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=279
06/18/2022 11:50:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=283
06/18/2022 11:50:34 - INFO - __main__ - Global step 850 Train loss 0.13 Classification-F1 0.28553549079206153 on epoch=283
06/18/2022 11:50:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=286
06/18/2022 11:50:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=289
06/18/2022 11:50:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=293
06/18/2022 11:50:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=296
06/18/2022 11:50:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=299
06/18/2022 11:50:48 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.35218253968253965 on epoch=299
06/18/2022 11:50:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=303
06/18/2022 11:50:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=306
06/18/2022 11:50:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=309
06/18/2022 11:50:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=313
06/18/2022 11:51:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=316
06/18/2022 11:51:02 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.19102103083796448 on epoch=316
06/18/2022 11:51:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=319
06/18/2022 11:51:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=323
06/18/2022 11:51:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.08 on epoch=326
06/18/2022 11:51:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=329
06/18/2022 11:51:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=333
06/18/2022 11:51:16 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.2984126984126984 on epoch=333
06/18/2022 11:51:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=336
06/18/2022 11:51:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=339
06/18/2022 11:51:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=343
06/18/2022 11:51:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=346
06/18/2022 11:51:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=349
06/18/2022 11:51:31 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.2851171816689058 on epoch=349
06/18/2022 11:51:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=353
06/18/2022 11:51:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=356
06/18/2022 11:51:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=359
06/18/2022 11:51:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=363
06/18/2022 11:51:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=366
06/18/2022 11:51:45 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.26466049382716045 on epoch=366
06/18/2022 11:51:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=369
06/18/2022 11:51:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=373
06/18/2022 11:51:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=376
06/18/2022 11:51:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=379
06/18/2022 11:51:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=383
06/18/2022 11:52:00 - INFO - __main__ - Global step 1150 Train loss 0.06 Classification-F1 0.23770347855454235 on epoch=383
06/18/2022 11:52:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=386
06/18/2022 11:52:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=389
06/18/2022 11:52:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=393
06/18/2022 11:52:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.11 on epoch=396
06/18/2022 11:52:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=399
06/18/2022 11:52:14 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.23010033444816055 on epoch=399
06/18/2022 11:52:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=403
06/18/2022 11:52:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=406
06/18/2022 11:52:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=409
06/18/2022 11:52:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=413
06/18/2022 11:52:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=416
06/18/2022 11:52:28 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.2060547504025765 on epoch=416
06/18/2022 11:52:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=419
06/18/2022 11:52:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=423
06/18/2022 11:52:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=426
06/18/2022 11:52:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=429
06/18/2022 11:52:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=433
06/18/2022 11:52:43 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.25 on epoch=433
06/18/2022 11:52:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=436
06/18/2022 11:52:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=439
06/18/2022 11:52:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=443
06/18/2022 11:52:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=446
06/18/2022 11:52:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
06/18/2022 11:52:57 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.25032621744349914 on epoch=449
06/18/2022 11:53:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=453
06/18/2022 11:53:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=456
06/18/2022 11:53:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
06/18/2022 11:53:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=463
06/18/2022 11:53:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=466
06/18/2022 11:53:12 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.2473383458646617 on epoch=466
06/18/2022 11:53:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
06/18/2022 11:53:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=473
06/18/2022 11:53:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=476
06/18/2022 11:53:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=479
06/18/2022 11:53:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=483
06/18/2022 11:53:26 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.21695402298850575 on epoch=483
06/18/2022 11:53:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
06/18/2022 11:53:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=489
06/18/2022 11:53:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=493
06/18/2022 11:53:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
06/18/2022 11:53:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=499
06/18/2022 11:53:40 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.1742278401317303 on epoch=499
06/18/2022 11:53:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=503
06/18/2022 11:53:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=506
06/18/2022 11:53:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=509
06/18/2022 11:53:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
06/18/2022 11:53:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
06/18/2022 11:53:55 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.2436909294512878 on epoch=516
06/18/2022 11:53:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=519
06/18/2022 11:54:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=523
06/18/2022 11:54:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
06/18/2022 11:54:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=529
06/18/2022 11:54:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/18/2022 11:54:09 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.24518518518518517 on epoch=533
06/18/2022 11:54:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
06/18/2022 11:54:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
06/18/2022 11:54:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
06/18/2022 11:54:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/18/2022 11:54:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=549
06/18/2022 11:54:24 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.2701464254952627 on epoch=549
06/18/2022 11:54:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
06/18/2022 11:54:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=556
06/18/2022 11:54:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
06/18/2022 11:54:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
06/18/2022 11:54:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
06/18/2022 11:54:38 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.1833558254610886 on epoch=566
06/18/2022 11:54:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=569
06/18/2022 11:54:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=573
06/18/2022 11:54:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=576
06/18/2022 11:54:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/18/2022 11:54:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
06/18/2022 11:54:52 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.20307577454273984 on epoch=583
06/18/2022 11:54:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
06/18/2022 11:54:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
06/18/2022 11:55:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/18/2022 11:55:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=596
06/18/2022 11:55:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=599
06/18/2022 11:55:07 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.23219373219373218 on epoch=599
06/18/2022 11:55:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=603
06/18/2022 11:55:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
06/18/2022 11:55:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=609
06/18/2022 11:55:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
06/18/2022 11:55:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
06/18/2022 11:55:21 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.19284188034188032 on epoch=616
06/18/2022 11:55:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/18/2022 11:55:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
06/18/2022 11:55:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
06/18/2022 11:55:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
06/18/2022 11:55:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
06/18/2022 11:55:36 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.1502330289049995 on epoch=633
06/18/2022 11:55:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
06/18/2022 11:55:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=639
06/18/2022 11:55:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/18/2022 11:55:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
06/18/2022 11:55:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/18/2022 11:55:50 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.1716948901771423 on epoch=649
06/18/2022 11:55:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/18/2022 11:55:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/18/2022 11:55:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/18/2022 11:56:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/18/2022 11:56:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/18/2022 11:56:04 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.2060547504025765 on epoch=666
06/18/2022 11:56:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
06/18/2022 11:56:09 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
06/18/2022 11:56:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
06/18/2022 11:56:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
06/18/2022 11:56:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/18/2022 11:56:18 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.1333502024291498 on epoch=683
06/18/2022 11:56:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=686
06/18/2022 11:56:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
06/18/2022 11:56:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/18/2022 11:56:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
06/18/2022 11:56:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/18/2022 11:56:33 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.21515151515151512 on epoch=699
06/18/2022 11:56:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/18/2022 11:56:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/18/2022 11:56:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/18/2022 11:56:43 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/18/2022 11:56:46 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=716
06/18/2022 11:56:47 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.26755555555555555 on epoch=716
06/18/2022 11:56:50 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/18/2022 11:56:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
06/18/2022 11:56:55 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/18/2022 11:56:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/18/2022 11:57:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=733
06/18/2022 11:57:01 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.25054945054945055 on epoch=733
06/18/2022 11:57:04 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/18/2022 11:57:06 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
06/18/2022 11:57:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/18/2022 11:57:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
06/18/2022 11:57:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/18/2022 11:57:15 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.26140711462450594 on epoch=749
06/18/2022 11:57:18 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=753
06/18/2022 11:57:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/18/2022 11:57:23 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/18/2022 11:57:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/18/2022 11:57:28 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/18/2022 11:57:30 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.20454545454545456 on epoch=766
06/18/2022 11:57:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/18/2022 11:57:35 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/18/2022 11:57:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/18/2022 11:57:40 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=779
06/18/2022 11:57:43 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/18/2022 11:57:44 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.19968730456535333 on epoch=783
06/18/2022 11:57:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/18/2022 11:57:49 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/18/2022 11:57:52 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/18/2022 11:57:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
06/18/2022 11:57:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/18/2022 11:57:58 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.2753246753246753 on epoch=799
06/18/2022 11:58:01 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
06/18/2022 11:58:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
06/18/2022 11:58:06 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/18/2022 11:58:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=813
06/18/2022 11:58:11 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/18/2022 11:58:12 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.24150036954915002 on epoch=816
06/18/2022 11:58:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
06/18/2022 11:58:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/18/2022 11:58:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=826
06/18/2022 11:58:23 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=829
06/18/2022 11:58:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/18/2022 11:58:27 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.29293023255813955 on epoch=833
06/18/2022 11:58:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
06/18/2022 11:58:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=839
06/18/2022 11:58:34 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
06/18/2022 11:58:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
06/18/2022 11:58:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/18/2022 11:58:41 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.1986236355007119 on epoch=849
06/18/2022 11:58:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/18/2022 11:58:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/18/2022 11:58:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/18/2022 11:58:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/18/2022 11:58:54 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
06/18/2022 11:58:55 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.19666048237476805 on epoch=866
06/18/2022 11:58:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/18/2022 11:59:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/18/2022 11:59:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
06/18/2022 11:59:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/18/2022 11:59:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=883
06/18/2022 11:59:09 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.24558774558774557 on epoch=883
06/18/2022 11:59:12 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
06/18/2022 11:59:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=889
06/18/2022 11:59:17 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
06/18/2022 11:59:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
06/18/2022 11:59:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=899
06/18/2022 11:59:24 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.1776785714285714 on epoch=899
06/18/2022 11:59:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
06/18/2022 11:59:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/18/2022 11:59:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
06/18/2022 11:59:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/18/2022 11:59:37 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/18/2022 11:59:38 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.16228433402346445 on epoch=916
06/18/2022 11:59:41 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
06/18/2022 11:59:43 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=923
06/18/2022 11:59:46 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/18/2022 11:59:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/18/2022 11:59:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/18/2022 11:59:52 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.18419080919080918 on epoch=933
06/18/2022 11:59:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/18/2022 11:59:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
06/18/2022 12:00:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
06/18/2022 12:00:02 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=946
06/18/2022 12:00:05 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/18/2022 12:00:06 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.18591934381408065 on epoch=949
06/18/2022 12:00:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/18/2022 12:00:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
06/18/2022 12:00:14 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/18/2022 12:00:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/18/2022 12:00:19 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
06/18/2022 12:00:21 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.1879434850863422 on epoch=966
06/18/2022 12:00:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/18/2022 12:00:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
06/18/2022 12:00:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/18/2022 12:00:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/18/2022 12:00:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/18/2022 12:00:35 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.20337662337662338 on epoch=983
06/18/2022 12:00:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/18/2022 12:00:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/18/2022 12:00:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/18/2022 12:00:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
06/18/2022 12:00:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/18/2022 12:00:49 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.16074534161490686 on epoch=999
06/18/2022 12:00:49 - INFO - __main__ - save last model!
06/18/2022 12:00:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 12:00:49 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 12:00:49 - INFO - __main__ - Printing 3 examples
06/18/2022 12:00:49 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 12:00:49 - INFO - __main__ - ['contradiction']
06/18/2022 12:00:49 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 12:00:49 - INFO - __main__ - ['entailment']
06/18/2022 12:00:49 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 12:00:49 - INFO - __main__ - ['contradiction']
06/18/2022 12:00:49 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 12:00:50 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:00:50 - INFO - __main__ - Printing 3 examples
06/18/2022 12:00:50 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/18/2022 12:00:50 - INFO - __main__ - ['contradiction']
06/18/2022 12:00:50 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/18/2022 12:00:50 - INFO - __main__ - ['contradiction']
06/18/2022 12:00:50 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/18/2022 12:00:50 - INFO - __main__ - ['contradiction']
06/18/2022 12:00:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 12:00:50 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:00:50 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 12:00:50 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:00:50 - INFO - __main__ - Printing 3 examples
06/18/2022 12:00:50 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/18/2022 12:00:50 - INFO - __main__ - ['contradiction']
06/18/2022 12:00:50 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/18/2022 12:00:50 - INFO - __main__ - ['contradiction']
06/18/2022 12:00:50 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/18/2022 12:00:50 - INFO - __main__ - ['contradiction']
06/18/2022 12:00:50 - INFO - __main__ - Tokenizing Input ...
06/18/2022 12:00:50 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:00:50 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 12:00:50 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:00:51 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 12:01:05 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 12:01:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 12:01:06 - INFO - __main__ - Starting training!
06/18/2022 12:01:21 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_13_0.4_8_predictions.txt
06/18/2022 12:01:21 - INFO - __main__ - Classification-F1 on test data: 0.0484
06/18/2022 12:01:21 - INFO - __main__ - prefix=anli_16_13, lr=0.4, bsz=8, dev_performance=0.45394112060778724, test_performance=0.04842443033499346
06/18/2022 12:01:21 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.3, bsz=8 ...
06/18/2022 12:01:22 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:01:22 - INFO - __main__ - Printing 3 examples
06/18/2022 12:01:22 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/18/2022 12:01:22 - INFO - __main__ - ['contradiction']
06/18/2022 12:01:22 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/18/2022 12:01:22 - INFO - __main__ - ['contradiction']
06/18/2022 12:01:22 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/18/2022 12:01:22 - INFO - __main__ - ['contradiction']
06/18/2022 12:01:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 12:01:22 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:01:22 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 12:01:22 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:01:22 - INFO - __main__ - Printing 3 examples
06/18/2022 12:01:22 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/18/2022 12:01:22 - INFO - __main__ - ['contradiction']
06/18/2022 12:01:22 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/18/2022 12:01:22 - INFO - __main__ - ['contradiction']
06/18/2022 12:01:22 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/18/2022 12:01:22 - INFO - __main__ - ['contradiction']
06/18/2022 12:01:22 - INFO - __main__ - Tokenizing Input ...
06/18/2022 12:01:23 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:01:23 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 12:01:38 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 12:01:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 12:01:39 - INFO - __main__ - Starting training!
06/18/2022 12:01:42 - INFO - __main__ - Step 10 Global step 10 Train loss 0.69 on epoch=3
06/18/2022 12:01:45 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=6
06/18/2022 12:01:48 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=9
06/18/2022 12:01:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=13
06/18/2022 12:01:53 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=16
06/18/2022 12:01:54 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.2504743833017078 on epoch=16
06/18/2022 12:01:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2504743833017078 on epoch=16, global_step=50
06/18/2022 12:01:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=19
06/18/2022 12:01:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=23
06/18/2022 12:02:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
06/18/2022 12:02:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=29
06/18/2022 12:02:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=33
06/18/2022 12:02:08 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.24611708482676223 on epoch=33
06/18/2022 12:02:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=36
06/18/2022 12:02:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=39
06/18/2022 12:02:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=43
06/18/2022 12:02:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=46
06/18/2022 12:02:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
06/18/2022 12:02:23 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.2169934640522876 on epoch=49
06/18/2022 12:02:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=53
06/18/2022 12:02:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.36 on epoch=56
06/18/2022 12:02:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=59
06/18/2022 12:02:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=63
06/18/2022 12:02:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=66
06/18/2022 12:02:37 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.2839080459770115 on epoch=66
06/18/2022 12:02:37 - INFO - __main__ - Saving model with best Classification-F1: 0.2504743833017078 -> 0.2839080459770115 on epoch=66, global_step=200
06/18/2022 12:02:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=69
06/18/2022 12:02:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=73
06/18/2022 12:02:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=76
06/18/2022 12:02:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=79
06/18/2022 12:02:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=83
06/18/2022 12:02:51 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.38492063492063494 on epoch=83
06/18/2022 12:02:51 - INFO - __main__ - Saving model with best Classification-F1: 0.2839080459770115 -> 0.38492063492063494 on epoch=83, global_step=250
06/18/2022 12:02:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=86
06/18/2022 12:02:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=89
06/18/2022 12:02:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.36 on epoch=93
06/18/2022 12:03:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=96
06/18/2022 12:03:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=99
06/18/2022 12:03:05 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3610475464022375 on epoch=99
06/18/2022 12:03:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=103
06/18/2022 12:03:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=106
06/18/2022 12:03:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=109
06/18/2022 12:03:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=113
06/18/2022 12:03:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.34 on epoch=116
06/18/2022 12:03:19 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.3395211191821361 on epoch=116
06/18/2022 12:03:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=119
06/18/2022 12:03:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.32 on epoch=123
06/18/2022 12:03:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=126
06/18/2022 12:03:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=129
06/18/2022 12:03:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=133
06/18/2022 12:03:34 - INFO - __main__ - Global step 400 Train loss 0.34 Classification-F1 0.41269841269841273 on epoch=133
06/18/2022 12:03:34 - INFO - __main__ - Saving model with best Classification-F1: 0.38492063492063494 -> 0.41269841269841273 on epoch=133, global_step=400
06/18/2022 12:03:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=136
06/18/2022 12:03:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=139
06/18/2022 12:03:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=143
06/18/2022 12:03:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=146
06/18/2022 12:03:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=149
06/18/2022 12:03:48 - INFO - __main__ - Global step 450 Train loss 0.30 Classification-F1 0.4339393939393939 on epoch=149
06/18/2022 12:03:48 - INFO - __main__ - Saving model with best Classification-F1: 0.41269841269841273 -> 0.4339393939393939 on epoch=149, global_step=450
06/18/2022 12:03:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=153
06/18/2022 12:03:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=156
06/18/2022 12:03:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=159
06/18/2022 12:03:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=163
06/18/2022 12:04:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=166
06/18/2022 12:04:02 - INFO - __main__ - Global step 500 Train loss 0.29 Classification-F1 0.3689526847421584 on epoch=166
06/18/2022 12:04:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=169
06/18/2022 12:04:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=173
06/18/2022 12:04:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=176
06/18/2022 12:04:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=179
06/18/2022 12:04:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=183
06/18/2022 12:04:16 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.43753086419753084 on epoch=183
06/18/2022 12:04:17 - INFO - __main__ - Saving model with best Classification-F1: 0.4339393939393939 -> 0.43753086419753084 on epoch=183, global_step=550
06/18/2022 12:04:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=186
06/18/2022 12:04:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.31 on epoch=189
06/18/2022 12:04:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.30 on epoch=193
06/18/2022 12:04:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=196
06/18/2022 12:04:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=199
06/18/2022 12:04:31 - INFO - __main__ - Global step 600 Train loss 0.27 Classification-F1 0.49688380431998636 on epoch=199
06/18/2022 12:04:31 - INFO - __main__ - Saving model with best Classification-F1: 0.43753086419753084 -> 0.49688380431998636 on epoch=199, global_step=600
06/18/2022 12:04:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=203
06/18/2022 12:04:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=206
06/18/2022 12:04:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=209
06/18/2022 12:04:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=213
06/18/2022 12:04:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=216
06/18/2022 12:04:45 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.45394112060778724 on epoch=216
06/18/2022 12:04:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=219
06/18/2022 12:04:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=223
06/18/2022 12:04:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=226
06/18/2022 12:04:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=229
06/18/2022 12:04:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=233
06/18/2022 12:04:59 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.4355928255299324 on epoch=233
06/18/2022 12:05:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=236
06/18/2022 12:05:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=239
06/18/2022 12:05:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=243
06/18/2022 12:05:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=246
06/18/2022 12:05:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=249
06/18/2022 12:05:14 - INFO - __main__ - Global step 750 Train loss 0.19 Classification-F1 0.4355928255299324 on epoch=249
06/18/2022 12:05:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=253
06/18/2022 12:05:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.17 on epoch=256
06/18/2022 12:05:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=259
06/18/2022 12:05:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.16 on epoch=263
06/18/2022 12:05:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=266
06/18/2022 12:05:28 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.42348008385744235 on epoch=266
06/18/2022 12:05:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=269
06/18/2022 12:05:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.15 on epoch=273
06/18/2022 12:05:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=276
06/18/2022 12:05:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.17 on epoch=279
06/18/2022 12:05:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.15 on epoch=283
06/18/2022 12:05:42 - INFO - __main__ - Global step 850 Train loss 0.16 Classification-F1 0.4856084656084656 on epoch=283
06/18/2022 12:05:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=286
06/18/2022 12:05:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=289
06/18/2022 12:05:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=293
06/18/2022 12:05:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=296
06/18/2022 12:05:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=299
06/18/2022 12:05:57 - INFO - __main__ - Global step 900 Train loss 0.16 Classification-F1 0.40740740740740744 on epoch=299
06/18/2022 12:05:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=303
06/18/2022 12:06:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=306
06/18/2022 12:06:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=309
06/18/2022 12:06:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=313
06/18/2022 12:06:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=316
06/18/2022 12:06:11 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.3953689167974882 on epoch=316
06/18/2022 12:06:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=319
06/18/2022 12:06:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=323
06/18/2022 12:06:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=326
06/18/2022 12:06:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=329
06/18/2022 12:06:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=333
06/18/2022 12:06:25 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.32877406281661603 on epoch=333
06/18/2022 12:06:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=336
06/18/2022 12:06:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=339
06/18/2022 12:06:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=343
06/18/2022 12:06:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=346
06/18/2022 12:06:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=349
06/18/2022 12:06:39 - INFO - __main__ - Global step 1050 Train loss 0.13 Classification-F1 0.3250718390804598 on epoch=349
06/18/2022 12:06:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=353
06/18/2022 12:06:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=356
06/18/2022 12:06:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=359
06/18/2022 12:06:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.15 on epoch=363
06/18/2022 12:06:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.11 on epoch=366
06/18/2022 12:06:54 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.384726443768997 on epoch=366
06/18/2022 12:06:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=369
06/18/2022 12:06:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=373
06/18/2022 12:07:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=376
06/18/2022 12:07:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=379
06/18/2022 12:07:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=383
06/18/2022 12:07:08 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.37415356151711376 on epoch=383
06/18/2022 12:07:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=386
06/18/2022 12:07:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.15 on epoch=389
06/18/2022 12:07:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=393
06/18/2022 12:07:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=396
06/18/2022 12:07:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=399
06/18/2022 12:07:22 - INFO - __main__ - Global step 1200 Train loss 0.10 Classification-F1 0.3813405797101449 on epoch=399
06/18/2022 12:07:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=403
06/18/2022 12:07:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=406
06/18/2022 12:07:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=409
06/18/2022 12:07:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=413
06/18/2022 12:07:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=416
06/18/2022 12:07:37 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.3978114478114477 on epoch=416
06/18/2022 12:07:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=419
06/18/2022 12:07:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=423
06/18/2022 12:07:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=426
06/18/2022 12:07:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=429
06/18/2022 12:07:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=433
06/18/2022 12:07:51 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.37063492063492065 on epoch=433
06/18/2022 12:07:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=436
06/18/2022 12:07:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=439
06/18/2022 12:07:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=443
06/18/2022 12:08:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=446
06/18/2022 12:08:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=449
06/18/2022 12:08:05 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.39617673992673996 on epoch=449
06/18/2022 12:08:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=453
06/18/2022 12:08:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=456
06/18/2022 12:08:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=459
06/18/2022 12:08:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.14 on epoch=463
06/18/2022 12:08:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=466
06/18/2022 12:08:20 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.29233363535689116 on epoch=466
06/18/2022 12:08:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=469
06/18/2022 12:08:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=473
06/18/2022 12:08:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=476
06/18/2022 12:08:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=479
06/18/2022 12:08:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=483
06/18/2022 12:08:34 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.280672268907563 on epoch=483
06/18/2022 12:08:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=486
06/18/2022 12:08:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=489
06/18/2022 12:08:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
06/18/2022 12:08:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=496
06/18/2022 12:08:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=499
06/18/2022 12:08:49 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.37365894974590624 on epoch=499
06/18/2022 12:08:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=503
06/18/2022 12:08:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
06/18/2022 12:08:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=509
06/18/2022 12:08:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
06/18/2022 12:09:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=516
06/18/2022 12:09:03 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.411371237458194 on epoch=516
06/18/2022 12:09:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
06/18/2022 12:09:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=523
06/18/2022 12:09:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=526
06/18/2022 12:09:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=529
06/18/2022 12:09:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=533
06/18/2022 12:09:17 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.3324712002972873 on epoch=533
06/18/2022 12:09:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=536
06/18/2022 12:09:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=539
06/18/2022 12:09:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=543
06/18/2022 12:09:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=546
06/18/2022 12:09:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=549
06/18/2022 12:09:32 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.3012987012987013 on epoch=549
06/18/2022 12:09:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=553
06/18/2022 12:09:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=556
06/18/2022 12:09:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=559
06/18/2022 12:09:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=563
06/18/2022 12:09:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
06/18/2022 12:09:46 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.4487179487179487 on epoch=566
06/18/2022 12:09:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=569
06/18/2022 12:09:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=573
06/18/2022 12:09:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=576
06/18/2022 12:09:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=579
06/18/2022 12:09:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
06/18/2022 12:10:01 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.23009404388714735 on epoch=583
06/18/2022 12:10:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
06/18/2022 12:10:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
06/18/2022 12:10:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
06/18/2022 12:10:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=596
06/18/2022 12:10:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=599
06/18/2022 12:10:15 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.3915160926030491 on epoch=599
06/18/2022 12:10:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
06/18/2022 12:10:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
06/18/2022 12:10:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=609
06/18/2022 12:10:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
06/18/2022 12:10:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=616
06/18/2022 12:10:29 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.2580304719435154 on epoch=616
06/18/2022 12:10:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=619
06/18/2022 12:10:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
06/18/2022 12:10:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=626
06/18/2022 12:10:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
06/18/2022 12:10:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=633
06/18/2022 12:10:44 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.25982608695652176 on epoch=633
06/18/2022 12:10:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=636
06/18/2022 12:10:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=639
06/18/2022 12:10:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=643
06/18/2022 12:10:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
06/18/2022 12:10:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
06/18/2022 12:10:58 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.28311688311688316 on epoch=649
06/18/2022 12:11:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=653
06/18/2022 12:11:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=656
06/18/2022 12:11:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/18/2022 12:11:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=663
06/18/2022 12:11:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=666
06/18/2022 12:11:13 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.27755555555555556 on epoch=666
06/18/2022 12:11:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
06/18/2022 12:11:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
06/18/2022 12:11:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
06/18/2022 12:11:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=679
06/18/2022 12:11:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=683
06/18/2022 12:11:27 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.3471819645732689 on epoch=683
06/18/2022 12:11:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=686
06/18/2022 12:11:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
06/18/2022 12:11:35 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=693
06/18/2022 12:11:37 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
06/18/2022 12:11:40 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/18/2022 12:11:41 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.2993615080571602 on epoch=699
06/18/2022 12:11:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=703
06/18/2022 12:11:46 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
06/18/2022 12:11:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
06/18/2022 12:11:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/18/2022 12:11:54 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=716
06/18/2022 12:11:56 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.27587412587412585 on epoch=716
06/18/2022 12:11:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/18/2022 12:12:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
06/18/2022 12:12:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=726
06/18/2022 12:12:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
06/18/2022 12:12:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
06/18/2022 12:12:10 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.2425925925925926 on epoch=733
06/18/2022 12:12:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
06/18/2022 12:12:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=739
06/18/2022 12:12:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=743
06/18/2022 12:12:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/18/2022 12:12:23 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=749
06/18/2022 12:12:24 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.37558839627805146 on epoch=749
06/18/2022 12:12:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
06/18/2022 12:12:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
06/18/2022 12:12:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/18/2022 12:12:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
06/18/2022 12:12:37 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=766
06/18/2022 12:12:39 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.22852613314359185 on epoch=766
06/18/2022 12:12:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
06/18/2022 12:12:44 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/18/2022 12:12:47 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=776
06/18/2022 12:12:49 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=779
06/18/2022 12:12:52 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
06/18/2022 12:12:53 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.253015873015873 on epoch=783
06/18/2022 12:12:56 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
06/18/2022 12:12:58 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/18/2022 12:13:01 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
06/18/2022 12:13:03 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=796
06/18/2022 12:13:06 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/18/2022 12:13:07 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.27206349206349206 on epoch=799
06/18/2022 12:13:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/18/2022 12:13:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
06/18/2022 12:13:15 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/18/2022 12:13:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/18/2022 12:13:20 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/18/2022 12:13:22 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.34835423197492166 on epoch=816
06/18/2022 12:13:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=819
06/18/2022 12:13:27 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
06/18/2022 12:13:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/18/2022 12:13:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/18/2022 12:13:35 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=833
06/18/2022 12:13:36 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.26136736826392 on epoch=833
06/18/2022 12:13:38 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/18/2022 12:13:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/18/2022 12:13:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/18/2022 12:13:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
06/18/2022 12:13:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.08 on epoch=849
06/18/2022 12:13:50 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.2962121212121212 on epoch=849
06/18/2022 12:13:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
06/18/2022 12:13:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
06/18/2022 12:13:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
06/18/2022 12:14:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/18/2022 12:14:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=866
06/18/2022 12:14:05 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.280414987311539 on epoch=866
06/18/2022 12:14:07 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
06/18/2022 12:14:10 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
06/18/2022 12:14:13 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=876
06/18/2022 12:14:15 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
06/18/2022 12:14:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=883
06/18/2022 12:14:19 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.22852613314359185 on epoch=883
06/18/2022 12:14:22 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
06/18/2022 12:14:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
06/18/2022 12:14:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=893
06/18/2022 12:14:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/18/2022 12:14:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
06/18/2022 12:14:33 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.362012987012987 on epoch=899
06/18/2022 12:14:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
06/18/2022 12:14:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=906
06/18/2022 12:14:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
06/18/2022 12:14:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/18/2022 12:14:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/18/2022 12:14:48 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.3366977225672878 on epoch=916
06/18/2022 12:14:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=919
06/18/2022 12:14:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/18/2022 12:14:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
06/18/2022 12:14:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/18/2022 12:15:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
06/18/2022 12:15:02 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.38301282051282054 on epoch=933
06/18/2022 12:15:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=936
06/18/2022 12:15:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=939
06/18/2022 12:15:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=943
06/18/2022 12:15:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=946
06/18/2022 12:15:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
06/18/2022 12:15:16 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.34835423197492166 on epoch=949
06/18/2022 12:15:19 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/18/2022 12:15:22 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/18/2022 12:15:24 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/18/2022 12:15:27 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/18/2022 12:15:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/18/2022 12:15:31 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.34835423197492166 on epoch=966
06/18/2022 12:15:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/18/2022 12:15:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=973
06/18/2022 12:15:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=976
06/18/2022 12:15:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/18/2022 12:15:44 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/18/2022 12:15:45 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.3430527289546716 on epoch=983
06/18/2022 12:15:48 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/18/2022 12:15:50 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
06/18/2022 12:15:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
06/18/2022 12:15:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/18/2022 12:15:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
06/18/2022 12:15:59 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.3617243867243868 on epoch=999
06/18/2022 12:15:59 - INFO - __main__ - save last model!
06/18/2022 12:15:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 12:15:59 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 12:15:59 - INFO - __main__ - Printing 3 examples
06/18/2022 12:15:59 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 12:15:59 - INFO - __main__ - ['contradiction']
06/18/2022 12:15:59 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 12:15:59 - INFO - __main__ - ['entailment']
06/18/2022 12:15:59 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 12:15:59 - INFO - __main__ - ['contradiction']
06/18/2022 12:15:59 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 12:16:00 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:16:00 - INFO - __main__ - Printing 3 examples
06/18/2022 12:16:00 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/18/2022 12:16:00 - INFO - __main__ - ['contradiction']
06/18/2022 12:16:00 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/18/2022 12:16:00 - INFO - __main__ - ['contradiction']
06/18/2022 12:16:00 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/18/2022 12:16:00 - INFO - __main__ - ['contradiction']
06/18/2022 12:16:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 12:16:00 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:16:00 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 12:16:00 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:16:00 - INFO - __main__ - Printing 3 examples
06/18/2022 12:16:00 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/18/2022 12:16:00 - INFO - __main__ - ['contradiction']
06/18/2022 12:16:00 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/18/2022 12:16:00 - INFO - __main__ - ['contradiction']
06/18/2022 12:16:00 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/18/2022 12:16:00 - INFO - __main__ - ['contradiction']
06/18/2022 12:16:00 - INFO - __main__ - Tokenizing Input ...
06/18/2022 12:16:00 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:16:00 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 12:16:00 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:16:01 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 12:16:15 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 12:16:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 12:16:15 - INFO - __main__ - Starting training!
06/18/2022 12:16:30 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_13_0.3_8_predictions.txt
06/18/2022 12:16:30 - INFO - __main__ - Classification-F1 on test data: 0.0449
06/18/2022 12:16:31 - INFO - __main__ - prefix=anli_16_13, lr=0.3, bsz=8, dev_performance=0.49688380431998636, test_performance=0.044887972968896656
06/18/2022 12:16:31 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.2, bsz=8 ...
06/18/2022 12:16:32 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:16:32 - INFO - __main__ - Printing 3 examples
06/18/2022 12:16:32 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/18/2022 12:16:32 - INFO - __main__ - ['contradiction']
06/18/2022 12:16:32 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/18/2022 12:16:32 - INFO - __main__ - ['contradiction']
06/18/2022 12:16:32 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/18/2022 12:16:32 - INFO - __main__ - ['contradiction']
06/18/2022 12:16:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 12:16:32 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:16:32 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 12:16:32 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:16:32 - INFO - __main__ - Printing 3 examples
06/18/2022 12:16:32 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/18/2022 12:16:32 - INFO - __main__ - ['contradiction']
06/18/2022 12:16:32 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/18/2022 12:16:32 - INFO - __main__ - ['contradiction']
06/18/2022 12:16:32 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/18/2022 12:16:32 - INFO - __main__ - ['contradiction']
06/18/2022 12:16:32 - INFO - __main__ - Tokenizing Input ...
06/18/2022 12:16:32 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:16:32 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 12:16:47 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 12:16:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 12:16:48 - INFO - __main__ - Starting training!
06/18/2022 12:16:51 - INFO - __main__ - Step 10 Global step 10 Train loss 0.80 on epoch=3
06/18/2022 12:16:54 - INFO - __main__ - Step 20 Global step 20 Train loss 0.55 on epoch=6
06/18/2022 12:16:56 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=9
06/18/2022 12:16:59 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=13
06/18/2022 12:17:02 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=16
06/18/2022 12:17:03 - INFO - __main__ - Global step 50 Train loss 0.57 Classification-F1 0.2085278555866791 on epoch=16
06/18/2022 12:17:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2085278555866791 on epoch=16, global_step=50
06/18/2022 12:17:05 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=19
06/18/2022 12:17:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=23
06/18/2022 12:17:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=26
06/18/2022 12:17:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=29
06/18/2022 12:17:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=33
06/18/2022 12:17:17 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.24611708482676223 on epoch=33
06/18/2022 12:17:17 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.24611708482676223 on epoch=33, global_step=100
06/18/2022 12:17:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=36
06/18/2022 12:17:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=39
06/18/2022 12:17:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
06/18/2022 12:17:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=46
06/18/2022 12:17:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=49
06/18/2022 12:17:31 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.24611708482676223 on epoch=49
06/18/2022 12:17:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=53
06/18/2022 12:17:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=56
06/18/2022 12:17:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=59
06/18/2022 12:17:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=63
06/18/2022 12:17:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=66
06/18/2022 12:17:45 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.28012654587287894 on epoch=66
06/18/2022 12:17:45 - INFO - __main__ - Saving model with best Classification-F1: 0.24611708482676223 -> 0.28012654587287894 on epoch=66, global_step=200
06/18/2022 12:17:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=69
06/18/2022 12:17:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=73
06/18/2022 12:17:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=76
06/18/2022 12:17:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=79
06/18/2022 12:17:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=83
06/18/2022 12:18:00 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.24611708482676223 on epoch=83
06/18/2022 12:18:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=86
06/18/2022 12:18:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=89
06/18/2022 12:18:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=93
06/18/2022 12:18:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=96
06/18/2022 12:18:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=99
06/18/2022 12:18:14 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.27777777777777773 on epoch=99
06/18/2022 12:18:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=103
06/18/2022 12:18:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=106
06/18/2022 12:18:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
06/18/2022 12:18:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=113
06/18/2022 12:18:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=116
06/18/2022 12:18:28 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.27777777777777773 on epoch=116
06/18/2022 12:18:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=119
06/18/2022 12:18:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=123
06/18/2022 12:18:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=126
06/18/2022 12:18:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=129
06/18/2022 12:18:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=133
06/18/2022 12:18:42 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.33542319749216304 on epoch=133
06/18/2022 12:18:42 - INFO - __main__ - Saving model with best Classification-F1: 0.28012654587287894 -> 0.33542319749216304 on epoch=133, global_step=400
06/18/2022 12:18:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=136
06/18/2022 12:18:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=139
06/18/2022 12:18:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=143
06/18/2022 12:18:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=146
06/18/2022 12:18:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=149
06/18/2022 12:18:56 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.3395211191821361 on epoch=149
06/18/2022 12:18:56 - INFO - __main__ - Saving model with best Classification-F1: 0.33542319749216304 -> 0.3395211191821361 on epoch=149, global_step=450
06/18/2022 12:18:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=153
06/18/2022 12:19:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.35 on epoch=156
06/18/2022 12:19:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=159
06/18/2022 12:19:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.33 on epoch=163
06/18/2022 12:19:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=166
06/18/2022 12:19:11 - INFO - __main__ - Global step 500 Train loss 0.36 Classification-F1 0.3689526847421584 on epoch=166
06/18/2022 12:19:11 - INFO - __main__ - Saving model with best Classification-F1: 0.3395211191821361 -> 0.3689526847421584 on epoch=166, global_step=500
06/18/2022 12:19:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=169
06/18/2022 12:19:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=173
06/18/2022 12:19:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=176
06/18/2022 12:19:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=179
06/18/2022 12:19:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=183
06/18/2022 12:19:25 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.3689526847421584 on epoch=183
06/18/2022 12:19:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=186
06/18/2022 12:19:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=189
06/18/2022 12:19:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.34 on epoch=193
06/18/2022 12:19:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=196
06/18/2022 12:19:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=199
06/18/2022 12:19:39 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.3689526847421584 on epoch=199
06/18/2022 12:19:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=203
06/18/2022 12:19:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=206
06/18/2022 12:19:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=209
06/18/2022 12:19:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=213
06/18/2022 12:19:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=216
06/18/2022 12:19:53 - INFO - __main__ - Global step 650 Train loss 0.33 Classification-F1 0.3900330536486143 on epoch=216
06/18/2022 12:19:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3689526847421584 -> 0.3900330536486143 on epoch=216, global_step=650
06/18/2022 12:19:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=219
06/18/2022 12:19:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=223
06/18/2022 12:20:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=226
06/18/2022 12:20:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=229
06/18/2022 12:20:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=233
06/18/2022 12:20:07 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.41269841269841273 on epoch=233
06/18/2022 12:20:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3900330536486143 -> 0.41269841269841273 on epoch=233, global_step=700
06/18/2022 12:20:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=236
06/18/2022 12:20:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=239
06/18/2022 12:20:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=243
06/18/2022 12:20:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=246
06/18/2022 12:20:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=249
06/18/2022 12:20:22 - INFO - __main__ - Global step 750 Train loss 0.29 Classification-F1 0.41269841269841273 on epoch=249
06/18/2022 12:20:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=253
06/18/2022 12:20:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.26 on epoch=256
06/18/2022 12:20:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=259
06/18/2022 12:20:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=263
06/18/2022 12:20:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=266
06/18/2022 12:20:36 - INFO - __main__ - Global step 800 Train loss 0.27 Classification-F1 0.41269841269841273 on epoch=266
06/18/2022 12:20:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=269
06/18/2022 12:20:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=273
06/18/2022 12:20:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=276
06/18/2022 12:20:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=279
06/18/2022 12:20:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=283
06/18/2022 12:20:50 - INFO - __main__ - Global step 850 Train loss 0.27 Classification-F1 0.455377302436126 on epoch=283
06/18/2022 12:20:50 - INFO - __main__ - Saving model with best Classification-F1: 0.41269841269841273 -> 0.455377302436126 on epoch=283, global_step=850
06/18/2022 12:20:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=286
06/18/2022 12:20:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=289
06/18/2022 12:20:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=293
06/18/2022 12:21:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=296
06/18/2022 12:21:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=299
06/18/2022 12:21:05 - INFO - __main__ - Global step 900 Train loss 0.25 Classification-F1 0.4767465504720407 on epoch=299
06/18/2022 12:21:05 - INFO - __main__ - Saving model with best Classification-F1: 0.455377302436126 -> 0.4767465504720407 on epoch=299, global_step=900
06/18/2022 12:21:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=303
06/18/2022 12:21:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=306
06/18/2022 12:21:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=309
06/18/2022 12:21:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=313
06/18/2022 12:21:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=316
06/18/2022 12:21:19 - INFO - __main__ - Global step 950 Train loss 0.25 Classification-F1 0.455377302436126 on epoch=316
06/18/2022 12:21:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=319
06/18/2022 12:21:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=323
06/18/2022 12:21:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=326
06/18/2022 12:21:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=329
06/18/2022 12:21:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=333
06/18/2022 12:21:33 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.4586641623267595 on epoch=333
06/18/2022 12:21:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=336
06/18/2022 12:21:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=339
06/18/2022 12:21:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=343
06/18/2022 12:21:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=346
06/18/2022 12:21:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=349
06/18/2022 12:21:48 - INFO - __main__ - Global step 1050 Train loss 0.23 Classification-F1 0.43753086419753084 on epoch=349
06/18/2022 12:21:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=353
06/18/2022 12:21:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=356
06/18/2022 12:21:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=359
06/18/2022 12:21:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=363
06/18/2022 12:22:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=366
06/18/2022 12:22:02 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.4553376906318083 on epoch=366
06/18/2022 12:22:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=369
06/18/2022 12:22:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=373
06/18/2022 12:22:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=376
06/18/2022 12:22:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=379
06/18/2022 12:22:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=383
06/18/2022 12:22:17 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.4192852501624431 on epoch=383
06/18/2022 12:22:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=386
06/18/2022 12:22:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=389
06/18/2022 12:22:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=393
06/18/2022 12:22:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=396
06/18/2022 12:22:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=399
06/18/2022 12:22:31 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.46365914786967416 on epoch=399
06/18/2022 12:22:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.15 on epoch=403
06/18/2022 12:22:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=406
06/18/2022 12:22:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=409
06/18/2022 12:22:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=413
06/18/2022 12:22:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=416
06/18/2022 12:22:46 - INFO - __main__ - Global step 1250 Train loss 0.16 Classification-F1 0.4557674104124409 on epoch=416
06/18/2022 12:22:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=419
06/18/2022 12:22:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.18 on epoch=423
06/18/2022 12:22:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=426
06/18/2022 12:22:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=429
06/18/2022 12:22:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=433
06/18/2022 12:23:00 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.3387706855791962 on epoch=433
06/18/2022 12:23:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=436
06/18/2022 12:23:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=439
06/18/2022 12:23:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=443
06/18/2022 12:23:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=446
06/18/2022 12:23:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=449
06/18/2022 12:23:15 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.294862772695285 on epoch=449
06/18/2022 12:23:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.12 on epoch=453
06/18/2022 12:23:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=456
06/18/2022 12:23:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=459
06/18/2022 12:23:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=463
06/18/2022 12:23:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=466
06/18/2022 12:23:29 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.4053030303030303 on epoch=466
06/18/2022 12:23:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=469
06/18/2022 12:23:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=473
06/18/2022 12:23:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=476
06/18/2022 12:23:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=479
06/18/2022 12:23:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=483
06/18/2022 12:23:44 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.29931972789115646 on epoch=483
06/18/2022 12:23:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=486
06/18/2022 12:23:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=489
06/18/2022 12:23:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=493
06/18/2022 12:23:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.12 on epoch=496
06/18/2022 12:23:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=499
06/18/2022 12:23:58 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.3158263305322129 on epoch=499
06/18/2022 12:24:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=503
06/18/2022 12:24:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.10 on epoch=506
06/18/2022 12:24:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.11 on epoch=509
06/18/2022 12:24:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.11 on epoch=513
06/18/2022 12:24:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=516
06/18/2022 12:24:13 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.3540494458653027 on epoch=516
06/18/2022 12:24:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=519
06/18/2022 12:24:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=523
06/18/2022 12:24:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=526
06/18/2022 12:24:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=529
06/18/2022 12:24:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=533
06/18/2022 12:24:27 - INFO - __main__ - Global step 1600 Train loss 0.11 Classification-F1 0.3680031073324414 on epoch=533
06/18/2022 12:24:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=536
06/18/2022 12:24:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=539
06/18/2022 12:24:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=543
06/18/2022 12:24:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=546
06/18/2022 12:24:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=549
06/18/2022 12:24:42 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.3524154589371981 on epoch=549
06/18/2022 12:24:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=553
06/18/2022 12:24:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=556
06/18/2022 12:24:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=559
06/18/2022 12:24:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=563
06/18/2022 12:24:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.09 on epoch=566
06/18/2022 12:24:56 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.3540494458653027 on epoch=566
06/18/2022 12:24:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=569
06/18/2022 12:25:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=573
06/18/2022 12:25:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=576
06/18/2022 12:25:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=579
06/18/2022 12:25:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=583
06/18/2022 12:25:10 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.3889082462253194 on epoch=583
06/18/2022 12:25:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=586
06/18/2022 12:25:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=589
06/18/2022 12:25:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=593
06/18/2022 12:25:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=596
06/18/2022 12:25:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=599
06/18/2022 12:25:25 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.3429824561403509 on epoch=599
06/18/2022 12:25:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=603
06/18/2022 12:25:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=606
06/18/2022 12:25:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=609
06/18/2022 12:25:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.10 on epoch=613
06/18/2022 12:25:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=616
06/18/2022 12:25:40 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.35555555555555557 on epoch=616
06/18/2022 12:25:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=619
06/18/2022 12:25:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=623
06/18/2022 12:25:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=626
06/18/2022 12:25:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=629
06/18/2022 12:25:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=633
06/18/2022 12:25:54 - INFO - __main__ - Global step 1900 Train loss 0.08 Classification-F1 0.3167748917748918 on epoch=633
06/18/2022 12:25:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=636
06/18/2022 12:25:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=639
06/18/2022 12:26:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=643
06/18/2022 12:26:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=646
06/18/2022 12:26:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=649
06/18/2022 12:26:09 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.24286634460547507 on epoch=649
06/18/2022 12:26:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=653
06/18/2022 12:26:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=656
06/18/2022 12:26:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=659
06/18/2022 12:26:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=663
06/18/2022 12:26:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=666
06/18/2022 12:26:23 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.3052910052910053 on epoch=666
06/18/2022 12:26:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.11 on epoch=669
06/18/2022 12:26:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=673
06/18/2022 12:26:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=676
06/18/2022 12:26:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=679
06/18/2022 12:26:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=683
06/18/2022 12:26:37 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.35496453900709224 on epoch=683
06/18/2022 12:26:40 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
06/18/2022 12:26:43 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=689
06/18/2022 12:26:45 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.08 on epoch=693
06/18/2022 12:26:48 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=696
06/18/2022 12:26:51 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=699
06/18/2022 12:26:52 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.21986427421210034 on epoch=699
06/18/2022 12:26:55 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.12 on epoch=703
06/18/2022 12:26:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=706
06/18/2022 12:27:00 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=709
06/18/2022 12:27:03 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=713
06/18/2022 12:27:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=716
06/18/2022 12:27:07 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.24419279907084787 on epoch=716
06/18/2022 12:27:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=719
06/18/2022 12:27:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=723
06/18/2022 12:27:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=726
06/18/2022 12:27:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=729
06/18/2022 12:27:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=733
06/18/2022 12:27:21 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.2377067563779086 on epoch=733
06/18/2022 12:27:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=736
06/18/2022 12:27:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=739
06/18/2022 12:27:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=743
06/18/2022 12:27:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.09 on epoch=746
06/18/2022 12:27:34 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
06/18/2022 12:27:36 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.258714969241285 on epoch=749
06/18/2022 12:27:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=753
06/18/2022 12:27:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=756
06/18/2022 12:27:43 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=759
06/18/2022 12:27:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=763
06/18/2022 12:27:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=766
06/18/2022 12:27:50 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.36288998357963875 on epoch=766
06/18/2022 12:27:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=769
06/18/2022 12:27:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=773
06/18/2022 12:27:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=776
06/18/2022 12:28:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=779
06/18/2022 12:28:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=783
06/18/2022 12:28:05 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.24759122364363387 on epoch=783
06/18/2022 12:28:07 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=786
06/18/2022 12:28:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=789
06/18/2022 12:28:12 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=793
06/18/2022 12:28:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=796
06/18/2022 12:28:18 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=799
06/18/2022 12:28:19 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.2870707070707071 on epoch=799
06/18/2022 12:28:22 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=803
06/18/2022 12:28:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
06/18/2022 12:28:27 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=809
06/18/2022 12:28:29 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
06/18/2022 12:28:32 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=816
06/18/2022 12:28:33 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.17152435686918446 on epoch=816
06/18/2022 12:28:36 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=819
06/18/2022 12:28:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=823
06/18/2022 12:28:41 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=826
06/18/2022 12:28:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/18/2022 12:28:46 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=833
06/18/2022 12:28:48 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.16524122807017544 on epoch=833
06/18/2022 12:28:50 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=836
06/18/2022 12:28:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=839
06/18/2022 12:28:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=843
06/18/2022 12:28:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=846
06/18/2022 12:29:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
06/18/2022 12:29:02 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.15891472868217055 on epoch=849
06/18/2022 12:29:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=853
06/18/2022 12:29:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=856
06/18/2022 12:29:10 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=859
06/18/2022 12:29:12 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=863
06/18/2022 12:29:15 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=866
06/18/2022 12:29:16 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.1402048042874916 on epoch=866
06/18/2022 12:29:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=869
06/18/2022 12:29:22 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=873
06/18/2022 12:29:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=876
06/18/2022 12:29:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=879
06/18/2022 12:29:29 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=883
06/18/2022 12:29:31 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.18121693121693122 on epoch=883
06/18/2022 12:29:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=886
06/18/2022 12:29:36 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=889
06/18/2022 12:29:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=893
06/18/2022 12:29:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=896
06/18/2022 12:29:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=899
06/18/2022 12:29:46 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.21646974579305406 on epoch=899
06/18/2022 12:29:48 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=903
06/18/2022 12:29:51 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=906
06/18/2022 12:29:54 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
06/18/2022 12:29:56 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
06/18/2022 12:29:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/18/2022 12:30:00 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.1813357148723002 on epoch=916
06/18/2022 12:30:03 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=919
06/18/2022 12:30:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=923
06/18/2022 12:30:08 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=926
06/18/2022 12:30:11 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=929
06/18/2022 12:30:14 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=933
06/18/2022 12:30:15 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.19072306665095634 on epoch=933
06/18/2022 12:30:18 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=936
06/18/2022 12:30:20 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=939
06/18/2022 12:30:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=943
06/18/2022 12:30:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
06/18/2022 12:30:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=949
06/18/2022 12:30:30 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.2074074074074074 on epoch=949
06/18/2022 12:30:32 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
06/18/2022 12:30:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=956
06/18/2022 12:30:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=959
06/18/2022 12:30:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=963
06/18/2022 12:30:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=966
06/18/2022 12:30:45 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.20544217687074826 on epoch=966
06/18/2022 12:30:47 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=969
06/18/2022 12:30:50 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=973
06/18/2022 12:30:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=976
06/18/2022 12:30:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=979
06/18/2022 12:30:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=983
06/18/2022 12:30:59 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.15506715506715507 on epoch=983
06/18/2022 12:31:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=986
06/18/2022 12:31:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=989
06/18/2022 12:31:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
06/18/2022 12:31:10 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
06/18/2022 12:31:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
06/18/2022 12:31:14 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:31:14 - INFO - __main__ - Printing 3 examples
06/18/2022 12:31:14 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/18/2022 12:31:14 - INFO - __main__ - ['entailment']
06/18/2022 12:31:14 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/18/2022 12:31:14 - INFO - __main__ - ['entailment']
06/18/2022 12:31:14 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/18/2022 12:31:14 - INFO - __main__ - ['entailment']
06/18/2022 12:31:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 12:31:14 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.14576719576719577 on epoch=999
06/18/2022 12:31:14 - INFO - __main__ - save last model!
06/18/2022 12:31:14 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:31:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 12:31:14 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 12:31:14 - INFO - __main__ - Printing 3 examples
06/18/2022 12:31:14 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 12:31:14 - INFO - __main__ - ['contradiction']
06/18/2022 12:31:14 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 12:31:14 - INFO - __main__ - ['entailment']
06/18/2022 12:31:14 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 12:31:14 - INFO - __main__ - ['contradiction']
06/18/2022 12:31:14 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 12:31:14 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 12:31:14 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:31:14 - INFO - __main__ - Printing 3 examples
06/18/2022 12:31:14 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/18/2022 12:31:14 - INFO - __main__ - ['entailment']
06/18/2022 12:31:14 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/18/2022 12:31:14 - INFO - __main__ - ['entailment']
06/18/2022 12:31:14 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/18/2022 12:31:14 - INFO - __main__ - ['entailment']
06/18/2022 12:31:14 - INFO - __main__ - Tokenizing Input ...
06/18/2022 12:31:14 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:31:14 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 12:31:15 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:31:16 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 12:31:33 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 12:31:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 12:31:34 - INFO - __main__ - Starting training!
06/18/2022 12:31:46 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_13_0.2_8_predictions.txt
06/18/2022 12:31:46 - INFO - __main__ - Classification-F1 on test data: 0.0341
06/18/2022 12:31:46 - INFO - __main__ - prefix=anli_16_13, lr=0.2, bsz=8, dev_performance=0.4767465504720407, test_performance=0.03406830473136323
06/18/2022 12:31:46 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.5, bsz=8 ...
06/18/2022 12:31:47 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:31:47 - INFO - __main__ - Printing 3 examples
06/18/2022 12:31:47 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/18/2022 12:31:47 - INFO - __main__ - ['entailment']
06/18/2022 12:31:47 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/18/2022 12:31:47 - INFO - __main__ - ['entailment']
06/18/2022 12:31:47 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/18/2022 12:31:47 - INFO - __main__ - ['entailment']
06/18/2022 12:31:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 12:31:47 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:31:47 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 12:31:47 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:31:47 - INFO - __main__ - Printing 3 examples
06/18/2022 12:31:47 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/18/2022 12:31:47 - INFO - __main__ - ['entailment']
06/18/2022 12:31:47 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/18/2022 12:31:47 - INFO - __main__ - ['entailment']
06/18/2022 12:31:47 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/18/2022 12:31:47 - INFO - __main__ - ['entailment']
06/18/2022 12:31:47 - INFO - __main__ - Tokenizing Input ...
06/18/2022 12:31:47 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:31:47 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 12:32:02 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 12:32:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 12:32:03 - INFO - __main__ - Starting training!
06/18/2022 12:32:06 - INFO - __main__ - Step 10 Global step 10 Train loss 0.83 on epoch=3
06/18/2022 12:32:09 - INFO - __main__ - Step 20 Global step 20 Train loss 0.48 on epoch=6
06/18/2022 12:32:12 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=9
06/18/2022 12:32:14 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=13
06/18/2022 12:32:17 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=16
06/18/2022 12:32:18 - INFO - __main__ - Global step 50 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=16
06/18/2022 12:32:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/18/2022 12:32:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=19
06/18/2022 12:32:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=23
06/18/2022 12:32:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=26
06/18/2022 12:32:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=29
06/18/2022 12:32:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=33
06/18/2022 12:32:33 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.19122257053291536 on epoch=33
06/18/2022 12:32:33 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.19122257053291536 on epoch=33, global_step=100
06/18/2022 12:32:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=36
06/18/2022 12:32:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=39
06/18/2022 12:32:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
06/18/2022 12:32:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=46
06/18/2022 12:32:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=49
06/18/2022 12:32:47 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=49
06/18/2022 12:32:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=53
06/18/2022 12:32:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=56
06/18/2022 12:32:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=59
06/18/2022 12:32:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=63
06/18/2022 12:33:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=66
06/18/2022 12:33:01 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.18888888888888888 on epoch=66
06/18/2022 12:33:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=69
06/18/2022 12:33:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=73
06/18/2022 12:33:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=76
06/18/2022 12:33:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
06/18/2022 12:33:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=83
06/18/2022 12:33:16 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.15873015873015875 on epoch=83
06/18/2022 12:33:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=86
06/18/2022 12:33:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=89
06/18/2022 12:33:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=93
06/18/2022 12:33:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=96
06/18/2022 12:33:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=99
06/18/2022 12:33:30 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.2085278555866791 on epoch=99
06/18/2022 12:33:30 - INFO - __main__ - Saving model with best Classification-F1: 0.19122257053291536 -> 0.2085278555866791 on epoch=99, global_step=300
06/18/2022 12:33:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=103
06/18/2022 12:33:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
06/18/2022 12:33:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=109
06/18/2022 12:33:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=113
06/18/2022 12:33:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=116
06/18/2022 12:33:44 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.24611708482676223 on epoch=116
06/18/2022 12:33:44 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.24611708482676223 on epoch=116, global_step=350
06/18/2022 12:33:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=119
06/18/2022 12:33:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=123
06/18/2022 12:33:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=126
06/18/2022 12:33:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=129
06/18/2022 12:33:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=133
06/18/2022 12:33:59 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.23298358891579232 on epoch=133
06/18/2022 12:34:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=136
06/18/2022 12:34:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=139
06/18/2022 12:34:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=143
06/18/2022 12:34:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=146
06/18/2022 12:34:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=149
06/18/2022 12:34:13 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.19999999999999998 on epoch=149
06/18/2022 12:34:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=153
06/18/2022 12:34:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=156
06/18/2022 12:34:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=159
06/18/2022 12:34:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.32 on epoch=163
06/18/2022 12:34:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=166
06/18/2022 12:34:28 - INFO - __main__ - Global step 500 Train loss 0.31 Classification-F1 0.23301985370950887 on epoch=166
06/18/2022 12:34:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=169
06/18/2022 12:34:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.28 on epoch=173
06/18/2022 12:34:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=176
06/18/2022 12:34:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=179
06/18/2022 12:34:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=183
06/18/2022 12:34:42 - INFO - __main__ - Global step 550 Train loss 0.29 Classification-F1 0.2504743833017078 on epoch=183
06/18/2022 12:34:42 - INFO - __main__ - Saving model with best Classification-F1: 0.24611708482676223 -> 0.2504743833017078 on epoch=183, global_step=550
06/18/2022 12:34:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.31 on epoch=186
06/18/2022 12:34:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=189
06/18/2022 12:34:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=193
06/18/2022 12:34:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=196
06/18/2022 12:34:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=199
06/18/2022 12:34:57 - INFO - __main__ - Global step 600 Train loss 0.27 Classification-F1 0.26332288401253917 on epoch=199
06/18/2022 12:34:57 - INFO - __main__ - Saving model with best Classification-F1: 0.2504743833017078 -> 0.26332288401253917 on epoch=199, global_step=600
06/18/2022 12:34:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=203
06/18/2022 12:35:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=206
06/18/2022 12:35:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=209
06/18/2022 12:35:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=213
06/18/2022 12:35:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.15 on epoch=216
06/18/2022 12:35:11 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.2774327122153209 on epoch=216
06/18/2022 12:35:11 - INFO - __main__ - Saving model with best Classification-F1: 0.26332288401253917 -> 0.2774327122153209 on epoch=216, global_step=650
06/18/2022 12:35:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=219
06/18/2022 12:35:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=223
06/18/2022 12:35:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=226
06/18/2022 12:35:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=229
06/18/2022 12:35:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=233
06/18/2022 12:35:26 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.47105508870214746 on epoch=233
06/18/2022 12:35:26 - INFO - __main__ - Saving model with best Classification-F1: 0.2774327122153209 -> 0.47105508870214746 on epoch=233, global_step=700
06/18/2022 12:35:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=236
06/18/2022 12:35:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=239
06/18/2022 12:35:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=243
06/18/2022 12:35:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=246
06/18/2022 12:35:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.16 on epoch=249
06/18/2022 12:35:40 - INFO - __main__ - Global step 750 Train loss 0.18 Classification-F1 0.2504743833017078 on epoch=249
06/18/2022 12:35:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=253
06/18/2022 12:35:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=256
06/18/2022 12:35:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=259
06/18/2022 12:35:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=263
06/18/2022 12:35:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=266
06/18/2022 12:35:55 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.33682058682058685 on epoch=266
06/18/2022 12:35:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=269
06/18/2022 12:36:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.13 on epoch=273
06/18/2022 12:36:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=276
06/18/2022 12:36:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=279
06/18/2022 12:36:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=283
06/18/2022 12:36:09 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.3234567901234568 on epoch=283
06/18/2022 12:36:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=286
06/18/2022 12:36:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=289
06/18/2022 12:36:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=293
06/18/2022 12:36:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=296
06/18/2022 12:36:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=299
06/18/2022 12:36:24 - INFO - __main__ - Global step 900 Train loss 0.11 Classification-F1 0.32380952380952377 on epoch=299
06/18/2022 12:36:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=303
06/18/2022 12:36:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=306
06/18/2022 12:36:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=309
06/18/2022 12:36:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=313
06/18/2022 12:36:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=316
06/18/2022 12:36:39 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.3880676328502415 on epoch=316
06/18/2022 12:36:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=319
06/18/2022 12:36:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=323
06/18/2022 12:36:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=326
06/18/2022 12:36:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=329
06/18/2022 12:36:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=333
06/18/2022 12:36:53 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.3895652173913044 on epoch=333
06/18/2022 12:36:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=336
06/18/2022 12:36:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=339
06/18/2022 12:37:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=343
06/18/2022 12:37:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=346
06/18/2022 12:37:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=349
06/18/2022 12:37:08 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.4689458689458689 on epoch=349
06/18/2022 12:37:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=353
06/18/2022 12:37:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=356
06/18/2022 12:37:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=359
06/18/2022 12:37:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=363
06/18/2022 12:37:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.11 on epoch=366
06/18/2022 12:37:22 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.357953052988514 on epoch=366
06/18/2022 12:37:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=369
06/18/2022 12:37:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=373
06/18/2022 12:37:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=376
06/18/2022 12:37:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=379
06/18/2022 12:37:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
06/18/2022 12:37:37 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.3849206349206349 on epoch=383
06/18/2022 12:37:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=386
06/18/2022 12:37:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=389
06/18/2022 12:37:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=393
06/18/2022 12:37:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=396
06/18/2022 12:37:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=399
06/18/2022 12:37:51 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.3691497975708502 on epoch=399
06/18/2022 12:37:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=403
06/18/2022 12:37:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=406
06/18/2022 12:37:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=409
06/18/2022 12:38:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=413
06/18/2022 12:38:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=416
06/18/2022 12:38:06 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.3762428048142334 on epoch=416
06/18/2022 12:38:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=419
06/18/2022 12:38:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=423
06/18/2022 12:38:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=426
06/18/2022 12:38:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=429
06/18/2022 12:38:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=433
06/18/2022 12:38:20 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.28547008547008546 on epoch=433
06/18/2022 12:38:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
06/18/2022 12:38:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=439
06/18/2022 12:38:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
06/18/2022 12:38:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
06/18/2022 12:38:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
06/18/2022 12:38:35 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.20657080366699243 on epoch=449
06/18/2022 12:38:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=453
06/18/2022 12:38:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=456
06/18/2022 12:38:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
06/18/2022 12:38:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=463
06/18/2022 12:38:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=466
06/18/2022 12:38:49 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.19089445176401695 on epoch=466
06/18/2022 12:38:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=469
06/18/2022 12:38:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=473
06/18/2022 12:38:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=476
06/18/2022 12:39:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
06/18/2022 12:39:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=483
06/18/2022 12:39:04 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.24652320304494219 on epoch=483
06/18/2022 12:39:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
06/18/2022 12:39:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=489
06/18/2022 12:39:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
06/18/2022 12:39:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=496
06/18/2022 12:39:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
06/18/2022 12:39:18 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.22244563946691603 on epoch=499
06/18/2022 12:39:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
06/18/2022 12:39:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
06/18/2022 12:39:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
06/18/2022 12:39:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
06/18/2022 12:39:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
06/18/2022 12:39:33 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.23857808857808854 on epoch=516
06/18/2022 12:39:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
06/18/2022 12:39:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
06/18/2022 12:39:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
06/18/2022 12:39:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
06/18/2022 12:39:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/18/2022 12:39:47 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.18704034983104748 on epoch=533
06/18/2022 12:39:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
06/18/2022 12:39:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=539
06/18/2022 12:39:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=543
06/18/2022 12:39:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/18/2022 12:40:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
06/18/2022 12:40:02 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.23833598151090568 on epoch=549
06/18/2022 12:40:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
06/18/2022 12:40:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
06/18/2022 12:40:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
06/18/2022 12:40:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=563
06/18/2022 12:40:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
06/18/2022 12:40:17 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.20095238095238097 on epoch=566
06/18/2022 12:40:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
06/18/2022 12:40:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
06/18/2022 12:40:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
06/18/2022 12:40:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=579
06/18/2022 12:40:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
06/18/2022 12:40:31 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.2540151515151515 on epoch=583
06/18/2022 12:40:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/18/2022 12:40:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
06/18/2022 12:40:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
06/18/2022 12:40:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
06/18/2022 12:40:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
06/18/2022 12:40:46 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.3180952380952381 on epoch=599
06/18/2022 12:40:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
06/18/2022 12:40:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
06/18/2022 12:40:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=609
06/18/2022 12:40:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=613
06/18/2022 12:40:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
06/18/2022 12:41:00 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.16119929453262785 on epoch=616
06/18/2022 12:41:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/18/2022 12:41:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
06/18/2022 12:41:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/18/2022 12:41:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/18/2022 12:41:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=633
06/18/2022 12:41:15 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.1742124542124542 on epoch=633
06/18/2022 12:41:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/18/2022 12:41:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/18/2022 12:41:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/18/2022 12:41:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=646
06/18/2022 12:41:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/18/2022 12:41:29 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.1610549943883277 on epoch=649
06/18/2022 12:41:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/18/2022 12:41:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/18/2022 12:41:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/18/2022 12:41:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
06/18/2022 12:41:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/18/2022 12:41:44 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.15476190476190477 on epoch=666
06/18/2022 12:41:46 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=669
06/18/2022 12:41:49 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
06/18/2022 12:41:52 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/18/2022 12:41:54 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/18/2022 12:41:57 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/18/2022 12:41:58 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.1339943384761312 on epoch=683
06/18/2022 12:42:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/18/2022 12:42:03 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
06/18/2022 12:42:06 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/18/2022 12:42:09 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
06/18/2022 12:42:11 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/18/2022 12:42:13 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.15257705257705256 on epoch=699
06/18/2022 12:42:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
06/18/2022 12:42:18 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/18/2022 12:42:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/18/2022 12:42:23 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/18/2022 12:42:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/18/2022 12:42:27 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.10845454465749581 on epoch=716
06/18/2022 12:42:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/18/2022 12:42:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
06/18/2022 12:42:35 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/18/2022 12:42:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/18/2022 12:42:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
06/18/2022 12:42:42 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.2448485059795683 on epoch=733
06/18/2022 12:42:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=736
06/18/2022 12:42:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/18/2022 12:42:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/18/2022 12:42:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/18/2022 12:42:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
06/18/2022 12:42:56 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.265611858704794 on epoch=749
06/18/2022 12:42:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/18/2022 12:43:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/18/2022 12:43:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=759
06/18/2022 12:43:07 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/18/2022 12:43:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/18/2022 12:43:11 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.1722639933166249 on epoch=766
06/18/2022 12:43:13 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/18/2022 12:43:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/18/2022 12:43:19 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
06/18/2022 12:43:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/18/2022 12:43:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/18/2022 12:43:26 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.15249101318066832 on epoch=783
06/18/2022 12:43:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
06/18/2022 12:43:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=789
06/18/2022 12:43:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/18/2022 12:43:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/18/2022 12:43:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=799
06/18/2022 12:43:40 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.23754497354497356 on epoch=799
06/18/2022 12:43:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
06/18/2022 12:43:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/18/2022 12:43:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/18/2022 12:43:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/18/2022 12:43:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/18/2022 12:43:55 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.2666249739420471 on epoch=816
06/18/2022 12:43:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/18/2022 12:44:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
06/18/2022 12:44:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/18/2022 12:44:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/18/2022 12:44:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=833
06/18/2022 12:44:09 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.21949531949531947 on epoch=833
06/18/2022 12:44:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/18/2022 12:44:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/18/2022 12:44:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/18/2022 12:44:20 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/18/2022 12:44:23 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
06/18/2022 12:44:24 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.11667287977632804 on epoch=849
06/18/2022 12:44:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/18/2022 12:44:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
06/18/2022 12:44:32 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/18/2022 12:44:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/18/2022 12:44:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/18/2022 12:44:38 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.17340067340067336 on epoch=866
06/18/2022 12:44:41 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/18/2022 12:44:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/18/2022 12:44:46 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/18/2022 12:44:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/18/2022 12:44:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/18/2022 12:44:53 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.1246796365330848 on epoch=883
06/18/2022 12:44:56 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/18/2022 12:44:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/18/2022 12:45:01 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/18/2022 12:45:04 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/18/2022 12:45:06 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/18/2022 12:45:08 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.26387986180002054 on epoch=899
06/18/2022 12:45:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/18/2022 12:45:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
06/18/2022 12:45:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/18/2022 12:45:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/18/2022 12:45:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/18/2022 12:45:22 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.11506905256905256 on epoch=916
06/18/2022 12:45:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/18/2022 12:45:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/18/2022 12:45:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/18/2022 12:45:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=929
06/18/2022 12:45:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/18/2022 12:45:37 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.10250000000000001 on epoch=933
06/18/2022 12:45:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/18/2022 12:45:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/18/2022 12:45:45 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/18/2022 12:45:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/18/2022 12:45:50 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=949
06/18/2022 12:45:51 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.1320333801536809 on epoch=949
06/18/2022 12:45:54 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/18/2022 12:45:56 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/18/2022 12:45:59 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/18/2022 12:46:02 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/18/2022 12:46:04 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/18/2022 12:46:06 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.13556064073226542 on epoch=966
06/18/2022 12:46:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/18/2022 12:46:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/18/2022 12:46:14 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/18/2022 12:46:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/18/2022 12:46:19 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
06/18/2022 12:46:20 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.2068760611205433 on epoch=983
06/18/2022 12:46:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=986
06/18/2022 12:46:26 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/18/2022 12:46:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/18/2022 12:46:31 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/18/2022 12:46:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/18/2022 12:46:35 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:46:35 - INFO - __main__ - Printing 3 examples
06/18/2022 12:46:35 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/18/2022 12:46:35 - INFO - __main__ - ['entailment']
06/18/2022 12:46:35 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/18/2022 12:46:35 - INFO - __main__ - ['entailment']
06/18/2022 12:46:35 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/18/2022 12:46:35 - INFO - __main__ - ['entailment']
06/18/2022 12:46:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 12:46:35 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.17311376363100503 on epoch=999
06/18/2022 12:46:35 - INFO - __main__ - save last model!
06/18/2022 12:46:35 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:46:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 12:46:35 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 12:46:35 - INFO - __main__ - Printing 3 examples
06/18/2022 12:46:35 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 12:46:35 - INFO - __main__ - ['contradiction']
06/18/2022 12:46:35 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 12:46:35 - INFO - __main__ - ['entailment']
06/18/2022 12:46:35 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 12:46:35 - INFO - __main__ - ['contradiction']
06/18/2022 12:46:35 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 12:46:35 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 12:46:35 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:46:35 - INFO - __main__ - Printing 3 examples
06/18/2022 12:46:35 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/18/2022 12:46:35 - INFO - __main__ - ['entailment']
06/18/2022 12:46:35 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/18/2022 12:46:35 - INFO - __main__ - ['entailment']
06/18/2022 12:46:35 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/18/2022 12:46:35 - INFO - __main__ - ['entailment']
06/18/2022 12:46:35 - INFO - __main__ - Tokenizing Input ...
06/18/2022 12:46:35 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:46:35 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 12:46:36 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:46:37 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 12:46:50 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 12:46:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 12:46:51 - INFO - __main__ - Starting training!
06/18/2022 12:47:06 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_21_0.5_8_predictions.txt
06/18/2022 12:47:06 - INFO - __main__ - Classification-F1 on test data: 0.0373
06/18/2022 12:47:07 - INFO - __main__ - prefix=anli_16_21, lr=0.5, bsz=8, dev_performance=0.47105508870214746, test_performance=0.03734827670310371
06/18/2022 12:47:07 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.4, bsz=8 ...
06/18/2022 12:47:08 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:47:08 - INFO - __main__ - Printing 3 examples
06/18/2022 12:47:08 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/18/2022 12:47:08 - INFO - __main__ - ['entailment']
06/18/2022 12:47:08 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/18/2022 12:47:08 - INFO - __main__ - ['entailment']
06/18/2022 12:47:08 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/18/2022 12:47:08 - INFO - __main__ - ['entailment']
06/18/2022 12:47:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 12:47:08 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:47:08 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 12:47:08 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 12:47:08 - INFO - __main__ - Printing 3 examples
06/18/2022 12:47:08 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/18/2022 12:47:08 - INFO - __main__ - ['entailment']
06/18/2022 12:47:08 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/18/2022 12:47:08 - INFO - __main__ - ['entailment']
06/18/2022 12:47:08 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/18/2022 12:47:08 - INFO - __main__ - ['entailment']
06/18/2022 12:47:08 - INFO - __main__ - Tokenizing Input ...
06/18/2022 12:47:08 - INFO - __main__ - Tokenizing Output ...
06/18/2022 12:47:08 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 12:47:23 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 12:47:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 12:47:24 - INFO - __main__ - Starting training!
06/18/2022 12:47:27 - INFO - __main__ - Step 10 Global step 10 Train loss 0.80 on epoch=3
06/18/2022 12:47:30 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=6
06/18/2022 12:47:32 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=9
06/18/2022 12:47:35 - INFO - __main__ - Step 40 Global step 40 Train loss 0.52 on epoch=13
06/18/2022 12:47:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.46 on epoch=16
06/18/2022 12:47:39 - INFO - __main__ - Global step 50 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=16
06/18/2022 12:47:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/18/2022 12:47:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.58 on epoch=19
06/18/2022 12:47:44 - INFO - __main__ - Step 70 Global step 70 Train loss 0.51 on epoch=23
06/18/2022 12:47:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
06/18/2022 12:47:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=29
06/18/2022 12:47:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=33
06/18/2022 12:47:53 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=33
06/18/2022 12:47:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=36
06/18/2022 12:47:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=39
06/18/2022 12:48:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=43
06/18/2022 12:48:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=46
06/18/2022 12:48:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
06/18/2022 12:48:08 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=49
06/18/2022 12:48:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
06/18/2022 12:48:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
06/18/2022 12:48:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=59
06/18/2022 12:48:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
06/18/2022 12:48:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=66
06/18/2022 12:48:22 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.2085278555866791 on epoch=66
06/18/2022 12:48:22 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2085278555866791 on epoch=66, global_step=200
06/18/2022 12:48:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=69
06/18/2022 12:48:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=73
06/18/2022 12:48:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=76
06/18/2022 12:48:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=79
06/18/2022 12:48:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=83
06/18/2022 12:48:36 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.2085278555866791 on epoch=83
06/18/2022 12:48:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=86
06/18/2022 12:48:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=89
06/18/2022 12:48:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=93
06/18/2022 12:48:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=96
06/18/2022 12:48:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=99
06/18/2022 12:48:50 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.2085278555866791 on epoch=99
06/18/2022 12:48:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=103
06/18/2022 12:48:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=106
06/18/2022 12:48:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=109
06/18/2022 12:49:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=113
06/18/2022 12:49:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=116
06/18/2022 12:49:05 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.2085278555866791 on epoch=116
06/18/2022 12:49:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=119
06/18/2022 12:49:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=123
06/18/2022 12:49:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=126
06/18/2022 12:49:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=129
06/18/2022 12:49:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=133
06/18/2022 12:49:19 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.2450388265746333 on epoch=133
06/18/2022 12:49:19 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.2450388265746333 on epoch=133, global_step=400
06/18/2022 12:49:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=136
06/18/2022 12:49:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=139
06/18/2022 12:49:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=143
06/18/2022 12:49:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=146
06/18/2022 12:49:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=149
06/18/2022 12:49:33 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.2119004250151791 on epoch=149
06/18/2022 12:49:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=153
06/18/2022 12:49:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=156
06/18/2022 12:49:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=159
06/18/2022 12:49:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=163
06/18/2022 12:49:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=166
06/18/2022 12:49:48 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.2518518518518518 on epoch=166
06/18/2022 12:49:48 - INFO - __main__ - Saving model with best Classification-F1: 0.2450388265746333 -> 0.2518518518518518 on epoch=166, global_step=500
06/18/2022 12:49:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=169
06/18/2022 12:49:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=173
06/18/2022 12:49:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=176
06/18/2022 12:49:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=179
06/18/2022 12:50:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=183
06/18/2022 12:50:02 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.2906432748538012 on epoch=183
06/18/2022 12:50:02 - INFO - __main__ - Saving model with best Classification-F1: 0.2518518518518518 -> 0.2906432748538012 on epoch=183, global_step=550
06/18/2022 12:50:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.34 on epoch=186
06/18/2022 12:50:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.33 on epoch=189
06/18/2022 12:50:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=193
06/18/2022 12:50:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=196
06/18/2022 12:50:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.33 on epoch=199
06/18/2022 12:50:17 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.25486976399755307 on epoch=199
06/18/2022 12:50:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=203
06/18/2022 12:50:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=206
06/18/2022 12:50:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.31 on epoch=209
06/18/2022 12:50:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=213
06/18/2022 12:50:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=216
06/18/2022 12:50:31 - INFO - __main__ - Global step 650 Train loss 0.31 Classification-F1 0.28571428571428575 on epoch=216
06/18/2022 12:50:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=219
06/18/2022 12:50:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=223
06/18/2022 12:50:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=226
06/18/2022 12:50:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=229
06/18/2022 12:50:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.26 on epoch=233
06/18/2022 12:50:46 - INFO - __main__ - Global step 700 Train loss 0.30 Classification-F1 0.3145299145299145 on epoch=233
06/18/2022 12:50:46 - INFO - __main__ - Saving model with best Classification-F1: 0.2906432748538012 -> 0.3145299145299145 on epoch=233, global_step=700
06/18/2022 12:50:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=236
06/18/2022 12:50:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=239
06/18/2022 12:50:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=243
06/18/2022 12:50:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.27 on epoch=246
06/18/2022 12:50:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=249
06/18/2022 12:51:01 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.35488105004101717 on epoch=249
06/18/2022 12:51:01 - INFO - __main__ - Saving model with best Classification-F1: 0.3145299145299145 -> 0.35488105004101717 on epoch=249, global_step=750
06/18/2022 12:51:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=253
06/18/2022 12:51:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=256
06/18/2022 12:51:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=259
06/18/2022 12:51:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.29 on epoch=263
06/18/2022 12:51:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=266
06/18/2022 12:51:15 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.41705314009661837 on epoch=266
06/18/2022 12:51:15 - INFO - __main__ - Saving model with best Classification-F1: 0.35488105004101717 -> 0.41705314009661837 on epoch=266, global_step=800
06/18/2022 12:51:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=269
06/18/2022 12:51:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=273
06/18/2022 12:51:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=276
06/18/2022 12:51:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=279
06/18/2022 12:51:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=283
06/18/2022 12:51:30 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.38386655728739405 on epoch=283
06/18/2022 12:51:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=286
06/18/2022 12:51:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=289
06/18/2022 12:51:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=293
06/18/2022 12:51:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=296
06/18/2022 12:51:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=299
06/18/2022 12:51:44 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.30349206349206354 on epoch=299
06/18/2022 12:51:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=303
06/18/2022 12:51:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=306
06/18/2022 12:51:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=309
06/18/2022 12:51:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=313
06/18/2022 12:51:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=316
06/18/2022 12:51:59 - INFO - __main__ - Global step 950 Train loss 0.18 Classification-F1 0.35653235653235654 on epoch=316
06/18/2022 12:52:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=319
06/18/2022 12:52:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=323
06/18/2022 12:52:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=326
06/18/2022 12:52:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=329
06/18/2022 12:52:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=333
06/18/2022 12:52:13 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.32177328843995506 on epoch=333
06/18/2022 12:52:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=336
06/18/2022 12:52:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.10 on epoch=339
06/18/2022 12:52:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=343
06/18/2022 12:52:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=346
06/18/2022 12:52:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=349
06/18/2022 12:52:28 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.2070538157494679 on epoch=349
06/18/2022 12:52:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=353
06/18/2022 12:52:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.15 on epoch=356
06/18/2022 12:52:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.11 on epoch=359
06/18/2022 12:52:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=363
06/18/2022 12:52:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=366
06/18/2022 12:52:42 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.16608270956097043 on epoch=366
06/18/2022 12:52:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=369
06/18/2022 12:52:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=373
06/18/2022 12:52:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=376
06/18/2022 12:52:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=379
06/18/2022 12:52:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=383
06/18/2022 12:52:57 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.1276803118908382 on epoch=383
06/18/2022 12:53:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=386
06/18/2022 12:53:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=389
06/18/2022 12:53:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=393
06/18/2022 12:53:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=396
06/18/2022 12:53:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=399
06/18/2022 12:53:11 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.18610526315789475 on epoch=399
06/18/2022 12:53:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=403
06/18/2022 12:53:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=406
06/18/2022 12:53:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=409
06/18/2022 12:53:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=413
06/18/2022 12:53:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=416
06/18/2022 12:53:26 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.15791805094130676 on epoch=416
06/18/2022 12:53:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=419
06/18/2022 12:53:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=423
06/18/2022 12:53:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=426
06/18/2022 12:53:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=429
06/18/2022 12:53:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=433
06/18/2022 12:53:41 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.205823754789272 on epoch=433
06/18/2022 12:53:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=436
06/18/2022 12:53:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=439
06/18/2022 12:53:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=443
06/18/2022 12:53:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=446
06/18/2022 12:53:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=449
06/18/2022 12:53:55 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.2457051720209615 on epoch=449
06/18/2022 12:53:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=453
06/18/2022 12:54:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=456
06/18/2022 12:54:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=459
06/18/2022 12:54:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=463
06/18/2022 12:54:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=466
06/18/2022 12:54:10 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.2205403791610688 on epoch=466
06/18/2022 12:54:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=469
06/18/2022 12:54:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=473
06/18/2022 12:54:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=476
06/18/2022 12:54:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=479
06/18/2022 12:54:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=483
06/18/2022 12:54:24 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.18390616475722857 on epoch=483
06/18/2022 12:54:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=486
06/18/2022 12:54:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=489
06/18/2022 12:54:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=493
06/18/2022 12:54:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
06/18/2022 12:54:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=499
06/18/2022 12:54:39 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.21054421768707482 on epoch=499
06/18/2022 12:54:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=503
06/18/2022 12:54:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
06/18/2022 12:54:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
06/18/2022 12:54:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=513
06/18/2022 12:54:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
06/18/2022 12:54:53 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.2812675897782281 on epoch=516
06/18/2022 12:54:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
06/18/2022 12:54:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=523
06/18/2022 12:55:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
06/18/2022 12:55:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=529
06/18/2022 12:55:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
06/18/2022 12:55:08 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.19386363636363635 on epoch=533
06/18/2022 12:55:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=536
06/18/2022 12:55:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
06/18/2022 12:55:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
06/18/2022 12:55:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/18/2022 12:55:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=549
06/18/2022 12:55:22 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.20674727571279297 on epoch=549
06/18/2022 12:55:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=553
06/18/2022 12:55:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=556
06/18/2022 12:55:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=559
06/18/2022 12:55:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=563
06/18/2022 12:55:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
06/18/2022 12:55:37 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.29296066252587993 on epoch=566
06/18/2022 12:55:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
06/18/2022 12:55:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=573
06/18/2022 12:55:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/18/2022 12:55:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/18/2022 12:55:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
06/18/2022 12:55:51 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.24732441471571903 on epoch=583
06/18/2022 12:55:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=586
06/18/2022 12:55:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=589
06/18/2022 12:55:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
06/18/2022 12:56:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
06/18/2022 12:56:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
06/18/2022 12:56:06 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.29190588765056846 on epoch=599
06/18/2022 12:56:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
06/18/2022 12:56:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
06/18/2022 12:56:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
06/18/2022 12:56:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
06/18/2022 12:56:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
06/18/2022 12:56:20 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.205823754789272 on epoch=616
06/18/2022 12:56:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=619
06/18/2022 12:56:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
06/18/2022 12:56:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
06/18/2022 12:56:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
06/18/2022 12:56:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=633
06/18/2022 12:56:35 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.1682355112587671 on epoch=633
06/18/2022 12:56:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
06/18/2022 12:56:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
06/18/2022 12:56:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=643
06/18/2022 12:56:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
06/18/2022 12:56:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=649
06/18/2022 12:56:50 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.167003984165171 on epoch=649
06/18/2022 12:56:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
06/18/2022 12:56:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/18/2022 12:56:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/18/2022 12:57:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/18/2022 12:57:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/18/2022 12:57:04 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.17246827246827245 on epoch=666
06/18/2022 12:57:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
06/18/2022 12:57:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=673
06/18/2022 12:57:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
06/18/2022 12:57:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=679
06/18/2022 12:57:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=683
06/18/2022 12:57:19 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.21636363636363637 on epoch=683
06/18/2022 12:57:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=686
06/18/2022 12:57:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
06/18/2022 12:57:27 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/18/2022 12:57:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
06/18/2022 12:57:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/18/2022 12:57:34 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.21190865871716932 on epoch=699
06/18/2022 12:57:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=703
06/18/2022 12:57:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/18/2022 12:57:41 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/18/2022 12:57:44 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/18/2022 12:57:47 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/18/2022 12:57:48 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.17248677248677247 on epoch=716
06/18/2022 12:57:51 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/18/2022 12:57:53 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
06/18/2022 12:57:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
06/18/2022 12:57:59 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
06/18/2022 12:58:01 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
06/18/2022 12:58:03 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.20525252525252524 on epoch=733
06/18/2022 12:58:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
06/18/2022 12:58:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/18/2022 12:58:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/18/2022 12:58:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/18/2022 12:58:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
06/18/2022 12:58:17 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.1739112739112739 on epoch=749
06/18/2022 12:58:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
06/18/2022 12:58:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=756
06/18/2022 12:58:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/18/2022 12:58:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
06/18/2022 12:58:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/18/2022 12:58:32 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.14986600700886415 on epoch=766
06/18/2022 12:58:35 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
06/18/2022 12:58:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/18/2022 12:58:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
06/18/2022 12:58:42 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/18/2022 12:58:45 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/18/2022 12:58:47 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.22006734006734008 on epoch=783
06/18/2022 12:58:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
06/18/2022 12:58:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=789
06/18/2022 12:58:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
06/18/2022 12:58:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/18/2022 12:59:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/18/2022 12:59:01 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.28934137757667167 on epoch=799
06/18/2022 12:59:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
06/18/2022 12:59:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=806
06/18/2022 12:59:09 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=809
06/18/2022 12:59:12 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
06/18/2022 12:59:14 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/18/2022 12:59:16 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.2098765432098765 on epoch=816
06/18/2022 12:59:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/18/2022 12:59:21 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=823
06/18/2022 12:59:23 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=826
06/18/2022 12:59:26 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=829
06/18/2022 12:59:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
06/18/2022 12:59:30 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.2492063492063492 on epoch=833
06/18/2022 12:59:33 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
06/18/2022 12:59:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
06/18/2022 12:59:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=843
06/18/2022 12:59:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
06/18/2022 12:59:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/18/2022 12:59:45 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.19359576736289336 on epoch=849
06/18/2022 12:59:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
06/18/2022 12:59:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/18/2022 12:59:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/18/2022 12:59:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/18/2022 12:59:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/18/2022 12:59:59 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.21209372637944068 on epoch=866
06/18/2022 13:00:02 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/18/2022 13:00:04 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
06/18/2022 13:00:07 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
06/18/2022 13:00:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
06/18/2022 13:00:12 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
06/18/2022 13:00:14 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.17636988764691497 on epoch=883
06/18/2022 13:00:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=886
06/18/2022 13:00:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/18/2022 13:00:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
06/18/2022 13:00:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
06/18/2022 13:00:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/18/2022 13:00:28 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.1904040404040404 on epoch=899
06/18/2022 13:00:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=903
06/18/2022 13:00:33 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=906
06/18/2022 13:00:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/18/2022 13:00:39 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/18/2022 13:00:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/18/2022 13:00:43 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.1880134050346816 on epoch=916
06/18/2022 13:00:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=919
06/18/2022 13:00:48 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=923
06/18/2022 13:00:51 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
06/18/2022 13:00:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
06/18/2022 13:00:56 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/18/2022 13:00:57 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.19637606837606833 on epoch=933
06/18/2022 13:01:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
06/18/2022 13:01:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/18/2022 13:01:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=943
06/18/2022 13:01:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
06/18/2022 13:01:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=949
06/18/2022 13:01:12 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.21128451380552224 on epoch=949
06/18/2022 13:01:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/18/2022 13:01:17 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
06/18/2022 13:01:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/18/2022 13:01:22 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/18/2022 13:01:25 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/18/2022 13:01:26 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.19860187553282183 on epoch=966
06/18/2022 13:01:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/18/2022 13:01:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=973
06/18/2022 13:01:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=976
06/18/2022 13:01:37 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/18/2022 13:01:40 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/18/2022 13:01:41 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.20312124849939978 on epoch=983
06/18/2022 13:01:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
06/18/2022 13:01:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/18/2022 13:01:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/18/2022 13:01:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/18/2022 13:01:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/18/2022 13:01:55 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.16518518518518518 on epoch=999
06/18/2022 13:01:55 - INFO - __main__ - save last model!
06/18/2022 13:01:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 13:01:55 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:01:55 - INFO - __main__ - Printing 3 examples
06/18/2022 13:01:55 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/18/2022 13:01:55 - INFO - __main__ - ['entailment']
06/18/2022 13:01:55 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/18/2022 13:01:55 - INFO - __main__ - ['entailment']
06/18/2022 13:01:55 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/18/2022 13:01:55 - INFO - __main__ - ['entailment']
06/18/2022 13:01:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 13:01:55 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 13:01:55 - INFO - __main__ - Printing 3 examples
06/18/2022 13:01:55 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 13:01:55 - INFO - __main__ - ['contradiction']
06/18/2022 13:01:55 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 13:01:55 - INFO - __main__ - ['entailment']
06/18/2022 13:01:55 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 13:01:55 - INFO - __main__ - ['contradiction']
06/18/2022 13:01:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 13:01:55 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:01:56 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 13:01:56 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:01:56 - INFO - __main__ - Printing 3 examples
06/18/2022 13:01:56 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/18/2022 13:01:56 - INFO - __main__ - ['entailment']
06/18/2022 13:01:56 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/18/2022 13:01:56 - INFO - __main__ - ['entailment']
06/18/2022 13:01:56 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/18/2022 13:01:56 - INFO - __main__ - ['entailment']
06/18/2022 13:01:56 - INFO - __main__ - Tokenizing Input ...
06/18/2022 13:01:56 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:01:56 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 13:01:56 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:01:57 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 13:02:11 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 13:02:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 13:02:11 - INFO - __main__ - Starting training!
06/18/2022 13:02:27 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_21_0.4_8_predictions.txt
06/18/2022 13:02:27 - INFO - __main__ - Classification-F1 on test data: 0.0557
06/18/2022 13:02:27 - INFO - __main__ - prefix=anli_16_21, lr=0.4, bsz=8, dev_performance=0.41705314009661837, test_performance=0.055652599037544645
06/18/2022 13:02:27 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.3, bsz=8 ...
06/18/2022 13:02:28 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:02:28 - INFO - __main__ - Printing 3 examples
06/18/2022 13:02:28 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/18/2022 13:02:28 - INFO - __main__ - ['entailment']
06/18/2022 13:02:28 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/18/2022 13:02:28 - INFO - __main__ - ['entailment']
06/18/2022 13:02:28 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/18/2022 13:02:28 - INFO - __main__ - ['entailment']
06/18/2022 13:02:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 13:02:28 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:02:28 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 13:02:28 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:02:28 - INFO - __main__ - Printing 3 examples
06/18/2022 13:02:28 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/18/2022 13:02:28 - INFO - __main__ - ['entailment']
06/18/2022 13:02:28 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/18/2022 13:02:28 - INFO - __main__ - ['entailment']
06/18/2022 13:02:28 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/18/2022 13:02:28 - INFO - __main__ - ['entailment']
06/18/2022 13:02:28 - INFO - __main__ - Tokenizing Input ...
06/18/2022 13:02:28 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:02:28 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 13:02:43 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 13:02:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 13:02:44 - INFO - __main__ - Starting training!
06/18/2022 13:02:48 - INFO - __main__ - Step 10 Global step 10 Train loss 0.79 on epoch=3
06/18/2022 13:02:50 - INFO - __main__ - Step 20 Global step 20 Train loss 0.55 on epoch=6
06/18/2022 13:02:53 - INFO - __main__ - Step 30 Global step 30 Train loss 0.59 on epoch=9
06/18/2022 13:02:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=13
06/18/2022 13:02:58 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=16
06/18/2022 13:02:59 - INFO - __main__ - Global step 50 Train loss 0.61 Classification-F1 0.1983273596176822 on epoch=16
06/18/2022 13:02:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1983273596176822 on epoch=16, global_step=50
06/18/2022 13:03:02 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=19
06/18/2022 13:03:05 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=23
06/18/2022 13:03:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=26
06/18/2022 13:03:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=29
06/18/2022 13:03:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=33
06/18/2022 13:03:14 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.1983273596176822 on epoch=33
06/18/2022 13:03:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=36
06/18/2022 13:03:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.59 on epoch=39
06/18/2022 13:03:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=43
06/18/2022 13:03:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=46
06/18/2022 13:03:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=49
06/18/2022 13:03:28 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=49
06/18/2022 13:03:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=53
06/18/2022 13:03:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=56
06/18/2022 13:03:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=59
06/18/2022 13:03:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=63
06/18/2022 13:03:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=66
06/18/2022 13:03:42 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.1983273596176822 on epoch=66
06/18/2022 13:03:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=69
06/18/2022 13:03:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=73
06/18/2022 13:03:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=76
06/18/2022 13:03:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=79
06/18/2022 13:03:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=83
06/18/2022 13:03:57 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.1983273596176822 on epoch=83
06/18/2022 13:03:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=86
06/18/2022 13:04:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=89
06/18/2022 13:04:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=93
06/18/2022 13:04:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=96
06/18/2022 13:04:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=99
06/18/2022 13:04:11 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.24018714954105502 on epoch=99
06/18/2022 13:04:11 - INFO - __main__ - Saving model with best Classification-F1: 0.1983273596176822 -> 0.24018714954105502 on epoch=99, global_step=300
06/18/2022 13:04:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=103
06/18/2022 13:04:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=106
06/18/2022 13:04:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=109
06/18/2022 13:04:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=113
06/18/2022 13:04:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=116
06/18/2022 13:04:25 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.2085278555866791 on epoch=116
06/18/2022 13:04:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=119
06/18/2022 13:04:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=123
06/18/2022 13:04:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=126
06/18/2022 13:04:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=129
06/18/2022 13:04:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=133
06/18/2022 13:04:40 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.1990221455277538 on epoch=133
06/18/2022 13:04:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=136
06/18/2022 13:04:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=139
06/18/2022 13:04:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=143
06/18/2022 13:04:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=146
06/18/2022 13:04:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=149
06/18/2022 13:04:54 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.20908004778972522 on epoch=149
06/18/2022 13:04:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=153
06/18/2022 13:04:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=156
06/18/2022 13:05:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=159
06/18/2022 13:05:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=163
06/18/2022 13:05:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=166
06/18/2022 13:05:09 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.20908004778972522 on epoch=166
06/18/2022 13:05:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=169
06/18/2022 13:05:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=173
06/18/2022 13:05:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=176
06/18/2022 13:05:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=179
06/18/2022 13:05:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=183
06/18/2022 13:05:23 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.2099511072763877 on epoch=183
06/18/2022 13:05:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=186
06/18/2022 13:05:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=189
06/18/2022 13:05:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=193
06/18/2022 13:05:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=196
06/18/2022 13:05:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=199
06/18/2022 13:05:37 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.2099511072763877 on epoch=199
06/18/2022 13:05:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=203
06/18/2022 13:05:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=206
06/18/2022 13:05:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=209
06/18/2022 13:05:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=213
06/18/2022 13:05:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=216
06/18/2022 13:05:51 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.27602905569007263 on epoch=216
06/18/2022 13:05:51 - INFO - __main__ - Saving model with best Classification-F1: 0.24018714954105502 -> 0.27602905569007263 on epoch=216, global_step=650
06/18/2022 13:05:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=219
06/18/2022 13:05:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=223
06/18/2022 13:05:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=226
06/18/2022 13:06:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=229
06/18/2022 13:06:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=233
06/18/2022 13:06:06 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.33793103448275863 on epoch=233
06/18/2022 13:06:06 - INFO - __main__ - Saving model with best Classification-F1: 0.27602905569007263 -> 0.33793103448275863 on epoch=233, global_step=700
06/18/2022 13:06:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=236
06/18/2022 13:06:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=239
06/18/2022 13:06:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.35 on epoch=243
06/18/2022 13:06:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=246
06/18/2022 13:06:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=249
06/18/2022 13:06:20 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.24893746205221615 on epoch=249
06/18/2022 13:06:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=253
06/18/2022 13:06:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=256
06/18/2022 13:06:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=259
06/18/2022 13:06:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.31 on epoch=263
06/18/2022 13:06:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.34 on epoch=266
06/18/2022 13:06:35 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.4302054154995331 on epoch=266
06/18/2022 13:06:35 - INFO - __main__ - Saving model with best Classification-F1: 0.33793103448275863 -> 0.4302054154995331 on epoch=266, global_step=800
06/18/2022 13:06:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.34 on epoch=269
06/18/2022 13:06:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.32 on epoch=273
06/18/2022 13:06:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=276
06/18/2022 13:06:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=279
06/18/2022 13:06:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=283
06/18/2022 13:06:50 - INFO - __main__ - Global step 850 Train loss 0.30 Classification-F1 0.3746498599439776 on epoch=283
06/18/2022 13:06:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=286
06/18/2022 13:06:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=289
06/18/2022 13:06:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.28 on epoch=293
06/18/2022 13:07:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=296
06/18/2022 13:07:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=299
06/18/2022 13:07:04 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.28815314832672595 on epoch=299
06/18/2022 13:07:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=303
06/18/2022 13:07:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.28 on epoch=306
06/18/2022 13:07:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.28 on epoch=309
06/18/2022 13:07:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=313
06/18/2022 13:07:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.28 on epoch=316
06/18/2022 13:07:19 - INFO - __main__ - Global step 950 Train loss 0.28 Classification-F1 0.4209235209235209 on epoch=316
06/18/2022 13:07:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=319
06/18/2022 13:07:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=323
06/18/2022 13:07:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=326
06/18/2022 13:07:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.29 on epoch=329
06/18/2022 13:07:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=333
06/18/2022 13:07:33 - INFO - __main__ - Global step 1000 Train loss 0.25 Classification-F1 0.32425892316999394 on epoch=333
06/18/2022 13:07:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=336
06/18/2022 13:07:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=339
06/18/2022 13:07:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=343
06/18/2022 13:07:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=346
06/18/2022 13:07:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=349
06/18/2022 13:07:48 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.32568542568542574 on epoch=349
06/18/2022 13:07:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.23 on epoch=353
06/18/2022 13:07:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=356
06/18/2022 13:07:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=359
06/18/2022 13:07:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=363
06/18/2022 13:08:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=366
06/18/2022 13:08:02 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.3607650365850213 on epoch=366
06/18/2022 13:08:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=369
06/18/2022 13:08:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=373
06/18/2022 13:08:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=376
06/18/2022 13:08:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=379
06/18/2022 13:08:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=383
06/18/2022 13:08:17 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.29295460874408247 on epoch=383
06/18/2022 13:08:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=386
06/18/2022 13:08:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=389
06/18/2022 13:08:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=393
06/18/2022 13:08:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=396
06/18/2022 13:08:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=399
06/18/2022 13:08:31 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.3549382716049383 on epoch=399
06/18/2022 13:08:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=403
06/18/2022 13:08:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=406
06/18/2022 13:08:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=409
06/18/2022 13:08:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=413
06/18/2022 13:08:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=416
06/18/2022 13:08:46 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.51435325903411 on epoch=416
06/18/2022 13:08:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4302054154995331 -> 0.51435325903411 on epoch=416, global_step=1250
06/18/2022 13:08:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=419
06/18/2022 13:08:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=423
06/18/2022 13:08:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=426
06/18/2022 13:08:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=429
06/18/2022 13:08:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=433
06/18/2022 13:09:00 - INFO - __main__ - Global step 1300 Train loss 0.15 Classification-F1 0.4826072272880783 on epoch=433
06/18/2022 13:09:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=436
06/18/2022 13:09:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=439
06/18/2022 13:09:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=443
06/18/2022 13:09:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=446
06/18/2022 13:09:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=449
06/18/2022 13:09:15 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.38915032679738565 on epoch=449
06/18/2022 13:09:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=453
06/18/2022 13:09:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.14 on epoch=456
06/18/2022 13:09:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=459
06/18/2022 13:09:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=463
06/18/2022 13:09:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=466
06/18/2022 13:09:30 - INFO - __main__ - Global step 1400 Train loss 0.14 Classification-F1 0.3033724340175953 on epoch=466
06/18/2022 13:09:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.10 on epoch=469
06/18/2022 13:09:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=473
06/18/2022 13:09:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=476
06/18/2022 13:09:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=479
06/18/2022 13:09:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=483
06/18/2022 13:09:44 - INFO - __main__ - Global step 1450 Train loss 0.10 Classification-F1 0.3756357670221493 on epoch=483
06/18/2022 13:09:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=486
06/18/2022 13:09:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.12 on epoch=489
06/18/2022 13:09:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=493
06/18/2022 13:09:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=496
06/18/2022 13:09:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=499
06/18/2022 13:09:59 - INFO - __main__ - Global step 1500 Train loss 0.09 Classification-F1 0.23280991083405667 on epoch=499
06/18/2022 13:10:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=503
06/18/2022 13:10:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=506
06/18/2022 13:10:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.11 on epoch=509
06/18/2022 13:10:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=513
06/18/2022 13:10:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=516
06/18/2022 13:10:13 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.48346560846560843 on epoch=516
06/18/2022 13:10:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=519
06/18/2022 13:10:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=523
06/18/2022 13:10:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=526
06/18/2022 13:10:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=529
06/18/2022 13:10:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=533
06/18/2022 13:10:28 - INFO - __main__ - Global step 1600 Train loss 0.11 Classification-F1 0.4587742504409171 on epoch=533
06/18/2022 13:10:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=536
06/18/2022 13:10:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.11 on epoch=539
06/18/2022 13:10:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=543
06/18/2022 13:10:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=546
06/18/2022 13:10:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=549
06/18/2022 13:10:42 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.25104427736006685 on epoch=549
06/18/2022 13:10:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=553
06/18/2022 13:10:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=556
06/18/2022 13:10:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=559
06/18/2022 13:10:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=563
06/18/2022 13:10:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=566
06/18/2022 13:10:57 - INFO - __main__ - Global step 1700 Train loss 0.07 Classification-F1 0.31338080959520237 on epoch=566
06/18/2022 13:11:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=569
06/18/2022 13:11:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=573
06/18/2022 13:11:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=576
06/18/2022 13:11:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=579
06/18/2022 13:11:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=583
06/18/2022 13:11:12 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.2698934837092732 on epoch=583
06/18/2022 13:11:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=586
06/18/2022 13:11:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=589
06/18/2022 13:11:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=593
06/18/2022 13:11:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=596
06/18/2022 13:11:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=599
06/18/2022 13:11:26 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.253167701863354 on epoch=599
06/18/2022 13:11:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=603
06/18/2022 13:11:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=606
06/18/2022 13:11:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=609
06/18/2022 13:11:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=613
06/18/2022 13:11:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=616
06/18/2022 13:11:41 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.2235933806146572 on epoch=616
06/18/2022 13:11:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=619
06/18/2022 13:11:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=623
06/18/2022 13:11:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=626
06/18/2022 13:11:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=629
06/18/2022 13:11:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=633
06/18/2022 13:11:55 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.43639156542382346 on epoch=633
06/18/2022 13:11:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
06/18/2022 13:12:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=639
06/18/2022 13:12:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=643
06/18/2022 13:12:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=646
06/18/2022 13:12:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=649
06/18/2022 13:12:10 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.2471453358671404 on epoch=649
06/18/2022 13:12:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=653
06/18/2022 13:12:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
06/18/2022 13:12:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=659
06/18/2022 13:12:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=663
06/18/2022 13:12:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=666
06/18/2022 13:12:24 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.20761377212990118 on epoch=666
06/18/2022 13:12:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=669
06/18/2022 13:12:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=673
06/18/2022 13:12:32 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=676
06/18/2022 13:12:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=679
06/18/2022 13:12:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=683
06/18/2022 13:12:39 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.21657848324514992 on epoch=683
06/18/2022 13:12:42 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=686
06/18/2022 13:12:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=689
06/18/2022 13:12:47 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=693
06/18/2022 13:12:50 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=696
06/18/2022 13:12:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=699
06/18/2022 13:12:54 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.24453271703018925 on epoch=699
06/18/2022 13:12:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=703
06/18/2022 13:12:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=706
06/18/2022 13:13:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=709
06/18/2022 13:13:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
06/18/2022 13:13:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=716
06/18/2022 13:13:08 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.253167701863354 on epoch=716
06/18/2022 13:13:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=719
06/18/2022 13:13:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
06/18/2022 13:13:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=726
06/18/2022 13:13:19 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=729
06/18/2022 13:13:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=733
06/18/2022 13:13:23 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.2538350426115845 on epoch=733
06/18/2022 13:13:26 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=736
06/18/2022 13:13:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=739
06/18/2022 13:13:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=743
06/18/2022 13:13:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
06/18/2022 13:13:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=749
06/18/2022 13:13:37 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.25322301024428684 on epoch=749
06/18/2022 13:13:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=753
06/18/2022 13:13:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
06/18/2022 13:13:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=759
06/18/2022 13:13:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=763
06/18/2022 13:13:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/18/2022 13:13:52 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.20083199141170152 on epoch=766
06/18/2022 13:13:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=769
06/18/2022 13:13:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=773
06/18/2022 13:14:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
06/18/2022 13:14:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/18/2022 13:14:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/18/2022 13:14:07 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.3024891774891775 on epoch=783
06/18/2022 13:14:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=786
06/18/2022 13:14:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=789
06/18/2022 13:14:15 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
06/18/2022 13:14:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=796
06/18/2022 13:14:20 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=799
06/18/2022 13:14:21 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.31338080959520237 on epoch=799
06/18/2022 13:14:24 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
06/18/2022 13:14:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
06/18/2022 13:14:29 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=809
06/18/2022 13:14:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
06/18/2022 13:14:34 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/18/2022 13:14:36 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.24412698412698414 on epoch=816
06/18/2022 13:14:38 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=819
06/18/2022 13:14:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=823
06/18/2022 13:14:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=826
06/18/2022 13:14:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/18/2022 13:14:49 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=833
06/18/2022 13:14:50 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.2529560616517138 on epoch=833
06/18/2022 13:14:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
06/18/2022 13:14:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=839
06/18/2022 13:14:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=843
06/18/2022 13:15:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=846
06/18/2022 13:15:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/18/2022 13:15:05 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.30323652341656837 on epoch=849
06/18/2022 13:15:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
06/18/2022 13:15:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
06/18/2022 13:15:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=859
06/18/2022 13:15:16 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/18/2022 13:15:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
06/18/2022 13:15:20 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.20664058214795042 on epoch=866
06/18/2022 13:15:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=869
06/18/2022 13:15:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=873
06/18/2022 13:15:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=876
06/18/2022 13:15:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=879
06/18/2022 13:15:33 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=883
06/18/2022 13:15:34 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.24452107279693486 on epoch=883
06/18/2022 13:15:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
06/18/2022 13:15:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
06/18/2022 13:15:42 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
06/18/2022 13:15:45 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=896
06/18/2022 13:15:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=899
06/18/2022 13:15:49 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.2580730897009967 on epoch=899
06/18/2022 13:15:51 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=903
06/18/2022 13:15:54 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=906
06/18/2022 13:15:57 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
06/18/2022 13:15:59 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=913
06/18/2022 13:16:02 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/18/2022 13:16:03 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.23828529194382853 on epoch=916
06/18/2022 13:16:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=919
06/18/2022 13:16:09 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
06/18/2022 13:16:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=926
06/18/2022 13:16:14 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
06/18/2022 13:16:17 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=933
06/18/2022 13:16:18 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.24429019256605464 on epoch=933
06/18/2022 13:16:21 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
06/18/2022 13:16:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=939
06/18/2022 13:16:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
06/18/2022 13:16:29 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=946
06/18/2022 13:16:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=949
06/18/2022 13:16:33 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.253015873015873 on epoch=949
06/18/2022 13:16:35 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/18/2022 13:16:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=956
06/18/2022 13:16:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
06/18/2022 13:16:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/18/2022 13:16:46 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=966
06/18/2022 13:16:47 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.253015873015873 on epoch=966
06/18/2022 13:16:50 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=969
06/18/2022 13:16:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
06/18/2022 13:16:55 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/18/2022 13:16:58 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/18/2022 13:17:00 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
06/18/2022 13:17:02 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.21540804874138209 on epoch=983
06/18/2022 13:17:04 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=986
06/18/2022 13:17:07 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
06/18/2022 13:17:10 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
06/18/2022 13:17:12 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=996
06/18/2022 13:17:15 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=999
06/18/2022 13:17:16 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:17:16 - INFO - __main__ - Printing 3 examples
06/18/2022 13:17:16 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/18/2022 13:17:16 - INFO - __main__ - ['entailment']
06/18/2022 13:17:16 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/18/2022 13:17:16 - INFO - __main__ - ['entailment']
06/18/2022 13:17:16 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/18/2022 13:17:16 - INFO - __main__ - ['entailment']
06/18/2022 13:17:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 13:17:16 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.14833271810015997 on epoch=999
06/18/2022 13:17:16 - INFO - __main__ - save last model!
06/18/2022 13:17:16 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:17:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 13:17:16 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 13:17:16 - INFO - __main__ - Printing 3 examples
06/18/2022 13:17:16 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 13:17:16 - INFO - __main__ - ['contradiction']
06/18/2022 13:17:16 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 13:17:16 - INFO - __main__ - ['entailment']
06/18/2022 13:17:16 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 13:17:16 - INFO - __main__ - ['contradiction']
06/18/2022 13:17:16 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 13:17:16 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 13:17:16 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:17:16 - INFO - __main__ - Printing 3 examples
06/18/2022 13:17:16 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/18/2022 13:17:16 - INFO - __main__ - ['entailment']
06/18/2022 13:17:16 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/18/2022 13:17:16 - INFO - __main__ - ['entailment']
06/18/2022 13:17:16 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/18/2022 13:17:16 - INFO - __main__ - ['entailment']
06/18/2022 13:17:16 - INFO - __main__ - Tokenizing Input ...
06/18/2022 13:17:16 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:17:17 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 13:17:17 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:17:18 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 13:17:35 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 13:17:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 13:17:36 - INFO - __main__ - Starting training!
06/18/2022 13:17:48 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_21_0.3_8_predictions.txt
06/18/2022 13:17:48 - INFO - __main__ - Classification-F1 on test data: 0.0269
06/18/2022 13:17:48 - INFO - __main__ - prefix=anli_16_21, lr=0.3, bsz=8, dev_performance=0.51435325903411, test_performance=0.026875282174179265
06/18/2022 13:17:48 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.2, bsz=8 ...
06/18/2022 13:17:49 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:17:49 - INFO - __main__ - Printing 3 examples
06/18/2022 13:17:49 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/18/2022 13:17:49 - INFO - __main__ - ['entailment']
06/18/2022 13:17:49 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/18/2022 13:17:49 - INFO - __main__ - ['entailment']
06/18/2022 13:17:49 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/18/2022 13:17:49 - INFO - __main__ - ['entailment']
06/18/2022 13:17:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 13:17:49 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:17:49 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 13:17:49 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:17:49 - INFO - __main__ - Printing 3 examples
06/18/2022 13:17:49 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/18/2022 13:17:49 - INFO - __main__ - ['entailment']
06/18/2022 13:17:49 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/18/2022 13:17:49 - INFO - __main__ - ['entailment']
06/18/2022 13:17:49 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/18/2022 13:17:49 - INFO - __main__ - ['entailment']
06/18/2022 13:17:49 - INFO - __main__ - Tokenizing Input ...
06/18/2022 13:17:49 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:17:49 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 13:18:08 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 13:18:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 13:18:08 - INFO - __main__ - Starting training!
06/18/2022 13:18:12 - INFO - __main__ - Step 10 Global step 10 Train loss 1.10 on epoch=3
06/18/2022 13:18:14 - INFO - __main__ - Step 20 Global step 20 Train loss 0.62 on epoch=6
06/18/2022 13:18:17 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=9
06/18/2022 13:18:20 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=13
06/18/2022 13:18:22 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=16
06/18/2022 13:18:24 - INFO - __main__ - Global step 50 Train loss 0.67 Classification-F1 0.2085278555866791 on epoch=16
06/18/2022 13:18:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2085278555866791 on epoch=16, global_step=50
06/18/2022 13:18:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=19
06/18/2022 13:18:29 - INFO - __main__ - Step 70 Global step 70 Train loss 0.51 on epoch=23
06/18/2022 13:18:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=26
06/18/2022 13:18:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=29
06/18/2022 13:18:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=33
06/18/2022 13:18:38 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.20050600885515493 on epoch=33
06/18/2022 13:18:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.56 on epoch=36
06/18/2022 13:18:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=39
06/18/2022 13:18:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=43
06/18/2022 13:18:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=46
06/18/2022 13:18:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=49
06/18/2022 13:18:52 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.2085278555866791 on epoch=49
06/18/2022 13:18:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=53
06/18/2022 13:18:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
06/18/2022 13:19:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=59
06/18/2022 13:19:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=63
06/18/2022 13:19:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=66
06/18/2022 13:19:06 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.2085278555866791 on epoch=66
06/18/2022 13:19:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=69
06/18/2022 13:19:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=73
06/18/2022 13:19:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=76
06/18/2022 13:19:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=79
06/18/2022 13:19:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.52 on epoch=83
06/18/2022 13:19:20 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.2085278555866791 on epoch=83
06/18/2022 13:19:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=86
06/18/2022 13:19:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.56 on epoch=89
06/18/2022 13:19:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=93
06/18/2022 13:19:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=96
06/18/2022 13:19:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=99
06/18/2022 13:19:34 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=99
06/18/2022 13:19:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=103
06/18/2022 13:19:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=106
06/18/2022 13:19:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=109
06/18/2022 13:19:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=113
06/18/2022 13:19:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
06/18/2022 13:19:48 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.1983273596176822 on epoch=116
06/18/2022 13:19:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=119
06/18/2022 13:19:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=123
06/18/2022 13:19:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=126
06/18/2022 13:19:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=129
06/18/2022 13:20:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=133
06/18/2022 13:20:02 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.1990221455277538 on epoch=133
06/18/2022 13:20:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=136
06/18/2022 13:20:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=139
06/18/2022 13:20:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=143
06/18/2022 13:20:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=146
06/18/2022 13:20:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=149
06/18/2022 13:20:16 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.1990221455277538 on epoch=149
06/18/2022 13:20:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=153
06/18/2022 13:20:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=156
06/18/2022 13:20:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=159
06/18/2022 13:20:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=163
06/18/2022 13:20:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=166
06/18/2022 13:20:30 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=166
06/18/2022 13:20:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=169
06/18/2022 13:20:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=173
06/18/2022 13:20:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=176
06/18/2022 13:20:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=179
06/18/2022 13:20:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=183
06/18/2022 13:20:44 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=183
06/18/2022 13:20:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=186
06/18/2022 13:20:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=189
06/18/2022 13:20:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=193
06/18/2022 13:20:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=196
06/18/2022 13:20:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.53 on epoch=199
06/18/2022 13:20:58 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=199
06/18/2022 13:21:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.50 on epoch=203
06/18/2022 13:21:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=206
06/18/2022 13:21:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=209
06/18/2022 13:21:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=213
06/18/2022 13:21:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=216
06/18/2022 13:21:12 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.1693121693121693 on epoch=216
06/18/2022 13:21:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=219
06/18/2022 13:21:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=223
06/18/2022 13:21:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=226
06/18/2022 13:21:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=229
06/18/2022 13:21:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=233
06/18/2022 13:21:26 - INFO - __main__ - Global step 700 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=233
06/18/2022 13:21:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=236
06/18/2022 13:21:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=239
06/18/2022 13:21:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=243
06/18/2022 13:21:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=246
06/18/2022 13:21:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=249
06/18/2022 13:21:41 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.2450388265746333 on epoch=249
06/18/2022 13:21:41 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.2450388265746333 on epoch=249, global_step=750
06/18/2022 13:21:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=253
06/18/2022 13:21:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=256
06/18/2022 13:21:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=259
06/18/2022 13:21:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=263
06/18/2022 13:21:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=266
06/18/2022 13:21:55 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.24745762711864408 on epoch=266
06/18/2022 13:21:55 - INFO - __main__ - Saving model with best Classification-F1: 0.2450388265746333 -> 0.24745762711864408 on epoch=266, global_step=800
06/18/2022 13:21:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=269
06/18/2022 13:22:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=273
06/18/2022 13:22:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=276
06/18/2022 13:22:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=279
06/18/2022 13:22:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=283
06/18/2022 13:22:09 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.32936507936507936 on epoch=283
06/18/2022 13:22:09 - INFO - __main__ - Saving model with best Classification-F1: 0.24745762711864408 -> 0.32936507936507936 on epoch=283, global_step=850
06/18/2022 13:22:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=286
06/18/2022 13:22:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=289
06/18/2022 13:22:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=293
06/18/2022 13:22:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=296
06/18/2022 13:22:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=299
06/18/2022 13:22:23 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.3469184599619382 on epoch=299
06/18/2022 13:22:23 - INFO - __main__ - Saving model with best Classification-F1: 0.32936507936507936 -> 0.3469184599619382 on epoch=299, global_step=900
06/18/2022 13:22:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=303
06/18/2022 13:22:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.42 on epoch=306
06/18/2022 13:22:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=309
06/18/2022 13:22:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=313
06/18/2022 13:22:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=316
06/18/2022 13:22:38 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.33086419753086416 on epoch=316
06/18/2022 13:22:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=319
06/18/2022 13:22:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=323
06/18/2022 13:22:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=326
06/18/2022 13:22:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=329
06/18/2022 13:22:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=333
06/18/2022 13:22:52 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.3716282320055904 on epoch=333
06/18/2022 13:22:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3469184599619382 -> 0.3716282320055904 on epoch=333, global_step=1000
06/18/2022 13:22:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=336
06/18/2022 13:22:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=339
06/18/2022 13:23:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=343
06/18/2022 13:23:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=346
06/18/2022 13:23:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=349
06/18/2022 13:23:06 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.39433551198257083 on epoch=349
06/18/2022 13:23:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3716282320055904 -> 0.39433551198257083 on epoch=349, global_step=1050
06/18/2022 13:23:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=353
06/18/2022 13:23:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=356
06/18/2022 13:23:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=359
06/18/2022 13:23:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=363
06/18/2022 13:23:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=366
06/18/2022 13:23:20 - INFO - __main__ - Global step 1100 Train loss 0.32 Classification-F1 0.3932275132275132 on epoch=366
06/18/2022 13:23:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=369
06/18/2022 13:23:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=373
06/18/2022 13:23:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=376
06/18/2022 13:23:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=379
06/18/2022 13:23:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=383
06/18/2022 13:23:35 - INFO - __main__ - Global step 1150 Train loss 0.35 Classification-F1 0.4156436520968541 on epoch=383
06/18/2022 13:23:35 - INFO - __main__ - Saving model with best Classification-F1: 0.39433551198257083 -> 0.4156436520968541 on epoch=383, global_step=1150
06/18/2022 13:23:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=386
06/18/2022 13:23:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=389
06/18/2022 13:23:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=393
06/18/2022 13:23:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=396
06/18/2022 13:23:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=399
06/18/2022 13:23:49 - INFO - __main__ - Global step 1200 Train loss 0.32 Classification-F1 0.4156436520968541 on epoch=399
06/18/2022 13:23:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=403
06/18/2022 13:23:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.35 on epoch=406
06/18/2022 13:23:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=409
06/18/2022 13:23:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=413
06/18/2022 13:24:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.34 on epoch=416
06/18/2022 13:24:03 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.44192834138486314 on epoch=416
06/18/2022 13:24:03 - INFO - __main__ - Saving model with best Classification-F1: 0.4156436520968541 -> 0.44192834138486314 on epoch=416, global_step=1250
06/18/2022 13:24:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=419
06/18/2022 13:24:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=423
06/18/2022 13:24:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=426
06/18/2022 13:24:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=429
06/18/2022 13:24:16 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=433
06/18/2022 13:24:17 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.44192834138486314 on epoch=433
06/18/2022 13:24:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.27 on epoch=436
06/18/2022 13:24:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=439
06/18/2022 13:24:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=443
06/18/2022 13:24:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=446
06/18/2022 13:24:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.26 on epoch=449
06/18/2022 13:24:32 - INFO - __main__ - Global step 1350 Train loss 0.29 Classification-F1 0.44335138158061965 on epoch=449
06/18/2022 13:24:32 - INFO - __main__ - Saving model with best Classification-F1: 0.44192834138486314 -> 0.44335138158061965 on epoch=449, global_step=1350
06/18/2022 13:24:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=453
06/18/2022 13:24:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=456
06/18/2022 13:24:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=459
06/18/2022 13:24:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.25 on epoch=463
06/18/2022 13:24:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=466
06/18/2022 13:24:46 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.43094202898550726 on epoch=466
06/18/2022 13:24:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.23 on epoch=469
06/18/2022 13:24:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=473
06/18/2022 13:24:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=476
06/18/2022 13:24:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=479
06/18/2022 13:24:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=483
06/18/2022 13:25:00 - INFO - __main__ - Global step 1450 Train loss 0.25 Classification-F1 0.4678558156819026 on epoch=483
06/18/2022 13:25:00 - INFO - __main__ - Saving model with best Classification-F1: 0.44335138158061965 -> 0.4678558156819026 on epoch=483, global_step=1450
06/18/2022 13:25:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.26 on epoch=486
06/18/2022 13:25:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.24 on epoch=489
06/18/2022 13:25:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=493
06/18/2022 13:25:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=496
06/18/2022 13:25:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=499
06/18/2022 13:25:15 - INFO - __main__ - Global step 1500 Train loss 0.25 Classification-F1 0.4183235867446393 on epoch=499
06/18/2022 13:25:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=503
06/18/2022 13:25:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=506
06/18/2022 13:25:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=509
06/18/2022 13:25:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=513
06/18/2022 13:25:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=516
06/18/2022 13:25:29 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.4360676804454915 on epoch=516
06/18/2022 13:25:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=519
06/18/2022 13:25:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=523
06/18/2022 13:25:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.28 on epoch=526
06/18/2022 13:25:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=529
06/18/2022 13:25:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=533
06/18/2022 13:25:43 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.496231884057971 on epoch=533
06/18/2022 13:25:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4678558156819026 -> 0.496231884057971 on epoch=533, global_step=1600
06/18/2022 13:25:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=536
06/18/2022 13:25:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.24 on epoch=539
06/18/2022 13:25:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=543
06/18/2022 13:25:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=546
06/18/2022 13:25:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=549
06/18/2022 13:25:57 - INFO - __main__ - Global step 1650 Train loss 0.23 Classification-F1 0.3741258741258741 on epoch=549
06/18/2022 13:26:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=553
06/18/2022 13:26:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=556
06/18/2022 13:26:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=559
06/18/2022 13:26:08 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.29 on epoch=563
06/18/2022 13:26:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=566
06/18/2022 13:26:12 - INFO - __main__ - Global step 1700 Train loss 0.23 Classification-F1 0.42857142857142855 on epoch=566
06/18/2022 13:26:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.20 on epoch=569
06/18/2022 13:26:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=573
06/18/2022 13:26:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=576
06/18/2022 13:26:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.16 on epoch=579
06/18/2022 13:26:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=583
06/18/2022 13:26:26 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.4553818052214203 on epoch=583
06/18/2022 13:26:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.17 on epoch=586
06/18/2022 13:26:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=589
06/18/2022 13:26:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=593
06/18/2022 13:26:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=596
06/18/2022 13:26:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=599
06/18/2022 13:26:40 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.44413711583924353 on epoch=599
06/18/2022 13:26:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=603
06/18/2022 13:26:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=606
06/18/2022 13:26:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=609
06/18/2022 13:26:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.17 on epoch=613
06/18/2022 13:26:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=616
06/18/2022 13:26:55 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.41884316308868735 on epoch=616
06/18/2022 13:26:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=619
06/18/2022 13:27:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=623
06/18/2022 13:27:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.16 on epoch=626
06/18/2022 13:27:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=629
06/18/2022 13:27:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=633
06/18/2022 13:27:09 - INFO - __main__ - Global step 1900 Train loss 0.16 Classification-F1 0.4051294617332353 on epoch=633
06/18/2022 13:27:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=636
06/18/2022 13:27:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=639
06/18/2022 13:27:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.15 on epoch=643
06/18/2022 13:27:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=646
06/18/2022 13:27:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=649
06/18/2022 13:27:23 - INFO - __main__ - Global step 1950 Train loss 0.16 Classification-F1 0.38805194805194804 on epoch=649
06/18/2022 13:27:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=653
06/18/2022 13:27:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=656
06/18/2022 13:27:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=659
06/18/2022 13:27:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=663
06/18/2022 13:27:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=666
06/18/2022 13:27:38 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.4699925678186548 on epoch=666
06/18/2022 13:27:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.15 on epoch=669
06/18/2022 13:27:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=673
06/18/2022 13:27:45 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=676
06/18/2022 13:27:48 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=679
06/18/2022 13:27:50 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=683
06/18/2022 13:27:52 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.4444444444444445 on epoch=683
06/18/2022 13:27:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.08 on epoch=686
06/18/2022 13:27:57 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.13 on epoch=689
06/18/2022 13:28:00 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.14 on epoch=693
06/18/2022 13:28:02 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=696
06/18/2022 13:28:05 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.17 on epoch=699
06/18/2022 13:28:06 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.5020334793843387 on epoch=699
06/18/2022 13:28:06 - INFO - __main__ - Saving model with best Classification-F1: 0.496231884057971 -> 0.5020334793843387 on epoch=699, global_step=2100
06/18/2022 13:28:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=703
06/18/2022 13:28:11 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.14 on epoch=706
06/18/2022 13:28:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=709
06/18/2022 13:28:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=713
06/18/2022 13:28:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=716
06/18/2022 13:28:20 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.38805194805194804 on epoch=716
06/18/2022 13:28:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=719
06/18/2022 13:28:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=723
06/18/2022 13:28:28 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=726
06/18/2022 13:28:31 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.13 on epoch=729
06/18/2022 13:28:33 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.13 on epoch=733
06/18/2022 13:28:35 - INFO - __main__ - Global step 2200 Train loss 0.13 Classification-F1 0.3885714285714286 on epoch=733
06/18/2022 13:28:37 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.09 on epoch=736
06/18/2022 13:28:40 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=739
06/18/2022 13:28:42 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.13 on epoch=743
06/18/2022 13:28:45 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=746
06/18/2022 13:28:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.11 on epoch=749
06/18/2022 13:28:49 - INFO - __main__ - Global step 2250 Train loss 0.12 Classification-F1 0.27733395989974935 on epoch=749
06/18/2022 13:28:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.10 on epoch=753
06/18/2022 13:28:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.09 on epoch=756
06/18/2022 13:28:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=759
06/18/2022 13:28:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=763
06/18/2022 13:29:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=766
06/18/2022 13:29:03 - INFO - __main__ - Global step 2300 Train loss 0.10 Classification-F1 0.31352813852813854 on epoch=766
06/18/2022 13:29:06 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.15 on epoch=769
06/18/2022 13:29:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.07 on epoch=773
06/18/2022 13:29:11 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.14 on epoch=776
06/18/2022 13:29:14 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=779
06/18/2022 13:29:16 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=783
06/18/2022 13:29:18 - INFO - __main__ - Global step 2350 Train loss 0.11 Classification-F1 0.26955465587044536 on epoch=783
06/18/2022 13:29:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=786
06/18/2022 13:29:23 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=789
06/18/2022 13:29:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=793
06/18/2022 13:29:28 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=796
06/18/2022 13:29:31 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.13 on epoch=799
06/18/2022 13:29:32 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.4271179401993355 on epoch=799
06/18/2022 13:29:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.09 on epoch=803
06/18/2022 13:29:37 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=806
06/18/2022 13:29:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=809
06/18/2022 13:29:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=813
06/18/2022 13:29:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=816
06/18/2022 13:29:46 - INFO - __main__ - Global step 2450 Train loss 0.10 Classification-F1 0.25944816053511705 on epoch=816
06/18/2022 13:29:49 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=819
06/18/2022 13:29:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=823
06/18/2022 13:29:54 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.11 on epoch=826
06/18/2022 13:29:57 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=829
06/18/2022 13:29:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=833
06/18/2022 13:30:00 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.25936234817813764 on epoch=833
06/18/2022 13:30:03 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=836
06/18/2022 13:30:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=839
06/18/2022 13:30:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=843
06/18/2022 13:30:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=846
06/18/2022 13:30:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.08 on epoch=849
06/18/2022 13:30:15 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.20966379158598833 on epoch=849
06/18/2022 13:30:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=853
06/18/2022 13:30:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=856
06/18/2022 13:30:23 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=859
06/18/2022 13:30:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=863
06/18/2022 13:30:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=866
06/18/2022 13:30:29 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.21400000000000002 on epoch=866
06/18/2022 13:30:32 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=869
06/18/2022 13:30:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=873
06/18/2022 13:30:37 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=876
06/18/2022 13:30:40 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=879
06/18/2022 13:30:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=883
06/18/2022 13:30:44 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.2845761381475667 on epoch=883
06/18/2022 13:30:46 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=886
06/18/2022 13:30:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=889
06/18/2022 13:30:51 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=893
06/18/2022 13:30:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=896
06/18/2022 13:30:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.08 on epoch=899
06/18/2022 13:30:58 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.30872388481084134 on epoch=899
06/18/2022 13:31:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=903
06/18/2022 13:31:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=906
06/18/2022 13:31:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=909
06/18/2022 13:31:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=913
06/18/2022 13:31:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=916
06/18/2022 13:31:13 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.30872388481084134 on epoch=916
06/18/2022 13:31:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=919
06/18/2022 13:31:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.07 on epoch=923
06/18/2022 13:31:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=926
06/18/2022 13:31:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=929
06/18/2022 13:31:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=933
06/18/2022 13:31:27 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.22947887758248536 on epoch=933
06/18/2022 13:31:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=936
06/18/2022 13:31:33 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=939
06/18/2022 13:31:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.09 on epoch=943
06/18/2022 13:31:38 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=946
06/18/2022 13:31:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=949
06/18/2022 13:31:42 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.30225442834138483 on epoch=949
06/18/2022 13:31:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=953
06/18/2022 13:31:47 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=956
06/18/2022 13:31:50 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=959
06/18/2022 13:31:52 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
06/18/2022 13:31:55 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=966
06/18/2022 13:31:56 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.2749655172413793 on epoch=966
06/18/2022 13:31:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=969
06/18/2022 13:32:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=973
06/18/2022 13:32:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=976
06/18/2022 13:32:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=979
06/18/2022 13:32:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=983
06/18/2022 13:32:11 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.22993006993006992 on epoch=983
06/18/2022 13:32:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=986
06/18/2022 13:32:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=989
06/18/2022 13:32:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=993
06/18/2022 13:32:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=996
06/18/2022 13:32:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=999
06/18/2022 13:32:25 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.2840816326530612 on epoch=999
06/18/2022 13:32:25 - INFO - __main__ - save last model!
06/18/2022 13:32:25 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:32:25 - INFO - __main__ - Printing 3 examples
06/18/2022 13:32:25 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/18/2022 13:32:25 - INFO - __main__ - ['neutral']
06/18/2022 13:32:25 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/18/2022 13:32:25 - INFO - __main__ - ['neutral']
06/18/2022 13:32:25 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/18/2022 13:32:25 - INFO - __main__ - ['neutral']
06/18/2022 13:32:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 13:32:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 13:32:25 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:32:25 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 13:32:25 - INFO - __main__ - Printing 3 examples
06/18/2022 13:32:25 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 13:32:25 - INFO - __main__ - ['contradiction']
06/18/2022 13:32:25 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 13:32:25 - INFO - __main__ - ['entailment']
06/18/2022 13:32:25 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 13:32:25 - INFO - __main__ - ['contradiction']
06/18/2022 13:32:25 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 13:32:25 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 13:32:25 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:32:25 - INFO - __main__ - Printing 3 examples
06/18/2022 13:32:25 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/18/2022 13:32:25 - INFO - __main__ - ['neutral']
06/18/2022 13:32:25 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/18/2022 13:32:25 - INFO - __main__ - ['neutral']
06/18/2022 13:32:25 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/18/2022 13:32:25 - INFO - __main__ - ['neutral']
06/18/2022 13:32:25 - INFO - __main__ - Tokenizing Input ...
06/18/2022 13:32:26 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:32:26 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 13:32:26 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:32:27 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 13:32:44 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 13:32:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 13:32:45 - INFO - __main__ - Starting training!
06/18/2022 13:32:57 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_21_0.2_8_predictions.txt
06/18/2022 13:32:57 - INFO - __main__ - Classification-F1 on test data: 0.0672
06/18/2022 13:32:57 - INFO - __main__ - prefix=anli_16_21, lr=0.2, bsz=8, dev_performance=0.5020334793843387, test_performance=0.06722348337815501
06/18/2022 13:32:57 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.5, bsz=8 ...
06/18/2022 13:32:58 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:32:58 - INFO - __main__ - Printing 3 examples
06/18/2022 13:32:58 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/18/2022 13:32:58 - INFO - __main__ - ['neutral']
06/18/2022 13:32:58 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/18/2022 13:32:58 - INFO - __main__ - ['neutral']
06/18/2022 13:32:58 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/18/2022 13:32:58 - INFO - __main__ - ['neutral']
06/18/2022 13:32:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 13:32:58 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:32:58 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 13:32:58 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:32:58 - INFO - __main__ - Printing 3 examples
06/18/2022 13:32:58 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/18/2022 13:32:58 - INFO - __main__ - ['neutral']
06/18/2022 13:32:58 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/18/2022 13:32:58 - INFO - __main__ - ['neutral']
06/18/2022 13:32:58 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/18/2022 13:32:58 - INFO - __main__ - ['neutral']
06/18/2022 13:32:58 - INFO - __main__ - Tokenizing Input ...
06/18/2022 13:32:58 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:32:58 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 13:33:13 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 13:33:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 13:33:14 - INFO - __main__ - Starting training!
06/18/2022 13:33:18 - INFO - __main__ - Step 10 Global step 10 Train loss 0.77 on epoch=3
06/18/2022 13:33:20 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=6
06/18/2022 13:33:23 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=9
06/18/2022 13:33:26 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=13
06/18/2022 13:33:28 - INFO - __main__ - Step 50 Global step 50 Train loss 0.45 on epoch=16
06/18/2022 13:33:30 - INFO - __main__ - Global step 50 Train loss 0.56 Classification-F1 0.29601782233361185 on epoch=16
06/18/2022 13:33:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.29601782233361185 on epoch=16, global_step=50
06/18/2022 13:33:32 - INFO - __main__ - Step 60 Global step 60 Train loss 0.45 on epoch=19
06/18/2022 13:33:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=23
06/18/2022 13:33:38 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=26
06/18/2022 13:33:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.38 on epoch=29
06/18/2022 13:33:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=33
06/18/2022 13:33:44 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.20370370370370372 on epoch=33
06/18/2022 13:33:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=36
06/18/2022 13:33:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=39
06/18/2022 13:33:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=43
06/18/2022 13:33:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=46
06/18/2022 13:33:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=49
06/18/2022 13:33:59 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.15555555555555556 on epoch=49
06/18/2022 13:34:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
06/18/2022 13:34:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=56
06/18/2022 13:34:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=59
06/18/2022 13:34:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=63
06/18/2022 13:34:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=66
06/18/2022 13:34:13 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.1990221455277538 on epoch=66
06/18/2022 13:34:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
06/18/2022 13:34:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=73
06/18/2022 13:34:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=76
06/18/2022 13:34:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=79
06/18/2022 13:34:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=83
06/18/2022 13:34:27 - INFO - __main__ - Global step 250 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=83
06/18/2022 13:34:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=86
06/18/2022 13:34:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=89
06/18/2022 13:34:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=93
06/18/2022 13:34:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=96
06/18/2022 13:34:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=99
06/18/2022 13:34:41 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.25069921179760996 on epoch=99
06/18/2022 13:34:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=103
06/18/2022 13:34:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=106
06/18/2022 13:34:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=109
06/18/2022 13:34:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=113
06/18/2022 13:34:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=116
06/18/2022 13:34:56 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.25182863113897597 on epoch=116
06/18/2022 13:34:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.31 on epoch=119
06/18/2022 13:35:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=123
06/18/2022 13:35:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=126
06/18/2022 13:35:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=129
06/18/2022 13:35:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=133
06/18/2022 13:35:10 - INFO - __main__ - Global step 400 Train loss 0.32 Classification-F1 0.26332288401253917 on epoch=133
06/18/2022 13:35:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=136
06/18/2022 13:35:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=139
06/18/2022 13:35:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=143
06/18/2022 13:35:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=146
06/18/2022 13:35:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=149
06/18/2022 13:35:24 - INFO - __main__ - Global step 450 Train loss 0.31 Classification-F1 0.31854450942170237 on epoch=149
06/18/2022 13:35:24 - INFO - __main__ - Saving model with best Classification-F1: 0.29601782233361185 -> 0.31854450942170237 on epoch=149, global_step=450
06/18/2022 13:35:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=153
06/18/2022 13:35:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=156
06/18/2022 13:35:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=159
06/18/2022 13:35:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.19 on epoch=163
06/18/2022 13:35:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=166
06/18/2022 13:35:39 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.27730294396961064 on epoch=166
06/18/2022 13:35:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=169
06/18/2022 13:35:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.19 on epoch=173
06/18/2022 13:35:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=176
06/18/2022 13:35:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=179
06/18/2022 13:35:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=183
06/18/2022 13:35:53 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.2819430652101953 on epoch=183
06/18/2022 13:35:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=186
06/18/2022 13:35:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=189
06/18/2022 13:36:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=193
06/18/2022 13:36:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=196
06/18/2022 13:36:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=199
06/18/2022 13:36:07 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.3043057571359458 on epoch=199
06/18/2022 13:36:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=203
06/18/2022 13:36:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=206
06/18/2022 13:36:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=209
06/18/2022 13:36:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=213
06/18/2022 13:36:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=216
06/18/2022 13:36:22 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.22066666666666665 on epoch=216
06/18/2022 13:36:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=219
06/18/2022 13:36:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=223
06/18/2022 13:36:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.15 on epoch=226
06/18/2022 13:36:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=229
06/18/2022 13:36:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=233
06/18/2022 13:36:36 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.1730526315789474 on epoch=233
06/18/2022 13:36:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=236
06/18/2022 13:36:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.15 on epoch=239
06/18/2022 13:36:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=243
06/18/2022 13:36:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.16 on epoch=246
06/18/2022 13:36:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=249
06/18/2022 13:36:50 - INFO - __main__ - Global step 750 Train loss 0.15 Classification-F1 0.21002906976744184 on epoch=249
06/18/2022 13:36:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=253
06/18/2022 13:36:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.09 on epoch=256
06/18/2022 13:36:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=259
06/18/2022 13:37:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=263
06/18/2022 13:37:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=266
06/18/2022 13:37:05 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.1849509588640023 on epoch=266
06/18/2022 13:37:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=269
06/18/2022 13:37:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=273
06/18/2022 13:37:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=276
06/18/2022 13:37:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=279
06/18/2022 13:37:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=283
06/18/2022 13:37:19 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.13096153846153846 on epoch=283
06/18/2022 13:37:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.11 on epoch=286
06/18/2022 13:37:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=289
06/18/2022 13:37:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=293
06/18/2022 13:37:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=296
06/18/2022 13:37:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=299
06/18/2022 13:37:34 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.10449275362318843 on epoch=299
06/18/2022 13:37:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=303
06/18/2022 13:37:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=306
06/18/2022 13:37:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=309
06/18/2022 13:37:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=313
06/18/2022 13:37:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=316
06/18/2022 13:37:48 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.11507580436530596 on epoch=316
06/18/2022 13:37:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=319
06/18/2022 13:37:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=323
06/18/2022 13:37:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=326
06/18/2022 13:37:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=329
06/18/2022 13:38:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=333
06/18/2022 13:38:02 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.14675143123418985 on epoch=333
06/18/2022 13:38:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
06/18/2022 13:38:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=339
06/18/2022 13:38:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=343
06/18/2022 13:38:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=346
06/18/2022 13:38:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=349
06/18/2022 13:38:16 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.1316883116883117 on epoch=349
06/18/2022 13:38:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=353
06/18/2022 13:38:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=356
06/18/2022 13:38:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=359
06/18/2022 13:38:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=363
06/18/2022 13:38:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=366
06/18/2022 13:38:31 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.1431992337164751 on epoch=366
06/18/2022 13:38:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=369
06/18/2022 13:38:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=373
06/18/2022 13:38:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=376
06/18/2022 13:38:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=379
06/18/2022 13:38:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=383
06/18/2022 13:38:45 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.11589135502178978 on epoch=383
06/18/2022 13:38:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=386
06/18/2022 13:38:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=389
06/18/2022 13:38:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=393
06/18/2022 13:38:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=396
06/18/2022 13:38:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
06/18/2022 13:39:00 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.14500451671183381 on epoch=399
06/18/2022 13:39:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=403
06/18/2022 13:39:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=406
06/18/2022 13:39:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=409
06/18/2022 13:39:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=413
06/18/2022 13:39:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
06/18/2022 13:39:14 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.11083263714842662 on epoch=416
06/18/2022 13:39:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=419
06/18/2022 13:39:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=423
06/18/2022 13:39:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=426
06/18/2022 13:39:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=429
06/18/2022 13:39:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=433
06/18/2022 13:39:28 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.1246179631272799 on epoch=433
06/18/2022 13:39:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=436
06/18/2022 13:39:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=439
06/18/2022 13:39:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
06/18/2022 13:39:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
06/18/2022 13:39:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=449
06/18/2022 13:39:43 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.14604377104377106 on epoch=449
06/18/2022 13:39:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
06/18/2022 13:39:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=456
06/18/2022 13:39:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=459
06/18/2022 13:39:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=463
06/18/2022 13:39:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=466
06/18/2022 13:39:57 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.20297408102286152 on epoch=466
06/18/2022 13:40:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
06/18/2022 13:40:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=473
06/18/2022 13:40:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=476
06/18/2022 13:40:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
06/18/2022 13:40:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=483
06/18/2022 13:40:11 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.18434270259828067 on epoch=483
06/18/2022 13:40:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
06/18/2022 13:40:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
06/18/2022 13:40:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=493
06/18/2022 13:40:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
06/18/2022 13:40:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=499
06/18/2022 13:40:26 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.15712858570001428 on epoch=499
06/18/2022 13:40:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
06/18/2022 13:40:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
06/18/2022 13:40:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
06/18/2022 13:40:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
06/18/2022 13:40:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
06/18/2022 13:40:40 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.16514008620689655 on epoch=516
06/18/2022 13:40:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
06/18/2022 13:40:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
06/18/2022 13:40:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
06/18/2022 13:40:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
06/18/2022 13:40:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=533
06/18/2022 13:40:55 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.19343602880866506 on epoch=533
06/18/2022 13:40:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=536
06/18/2022 13:41:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=539
06/18/2022 13:41:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
06/18/2022 13:41:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=546
06/18/2022 13:41:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=549
06/18/2022 13:41:09 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.14814814814814814 on epoch=549
06/18/2022 13:41:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
06/18/2022 13:41:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
06/18/2022 13:41:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=559
06/18/2022 13:41:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
06/18/2022 13:41:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=566
06/18/2022 13:41:24 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.21036437246963563 on epoch=566
06/18/2022 13:41:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
06/18/2022 13:41:29 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
06/18/2022 13:41:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=576
06/18/2022 13:41:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/18/2022 13:41:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
06/18/2022 13:41:38 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.15591175741551683 on epoch=583
06/18/2022 13:41:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
06/18/2022 13:41:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
06/18/2022 13:41:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/18/2022 13:41:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
06/18/2022 13:41:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
06/18/2022 13:41:52 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.12426681812720498 on epoch=599
06/18/2022 13:41:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=603
06/18/2022 13:41:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
06/18/2022 13:42:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
06/18/2022 13:42:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/18/2022 13:42:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
06/18/2022 13:42:06 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.13288084464555053 on epoch=616
06/18/2022 13:42:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=619
06/18/2022 13:42:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=623
06/18/2022 13:42:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/18/2022 13:42:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
06/18/2022 13:42:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/18/2022 13:42:21 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.3029411764705882 on epoch=633
06/18/2022 13:42:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
06/18/2022 13:42:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=639
06/18/2022 13:42:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/18/2022 13:42:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
06/18/2022 13:42:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
06/18/2022 13:42:35 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.18532272325375773 on epoch=649
06/18/2022 13:42:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/18/2022 13:42:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/18/2022 13:42:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/18/2022 13:42:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
06/18/2022 13:42:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
06/18/2022 13:42:49 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.18660049627791564 on epoch=666
06/18/2022 13:42:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/18/2022 13:42:54 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
06/18/2022 13:42:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
06/18/2022 13:42:59 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
06/18/2022 13:43:02 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=683
06/18/2022 13:43:03 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.16306306306306304 on epoch=683
06/18/2022 13:43:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/18/2022 13:43:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
06/18/2022 13:43:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/18/2022 13:43:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/18/2022 13:43:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/18/2022 13:43:18 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.13493526046371576 on epoch=699
06/18/2022 13:43:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
06/18/2022 13:43:23 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
06/18/2022 13:43:26 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=709
06/18/2022 13:43:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/18/2022 13:43:31 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/18/2022 13:43:32 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.17521367521367523 on epoch=716
06/18/2022 13:43:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/18/2022 13:43:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
06/18/2022 13:43:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/18/2022 13:43:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=729
06/18/2022 13:43:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/18/2022 13:43:46 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.15759637188208614 on epoch=733
06/18/2022 13:43:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/18/2022 13:43:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
06/18/2022 13:43:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/18/2022 13:43:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/18/2022 13:43:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/18/2022 13:44:01 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.20346475507765827 on epoch=749
06/18/2022 13:44:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
06/18/2022 13:44:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/18/2022 13:44:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
06/18/2022 13:44:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
06/18/2022 13:44:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/18/2022 13:44:15 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.20421165582455908 on epoch=766
06/18/2022 13:44:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=769
06/18/2022 13:44:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/18/2022 13:44:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/18/2022 13:44:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=779
06/18/2022 13:44:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/18/2022 13:44:29 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.13626373626373625 on epoch=783
06/18/2022 13:44:32 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/18/2022 13:44:34 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/18/2022 13:44:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=793
06/18/2022 13:44:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/18/2022 13:44:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/18/2022 13:44:44 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.21720085470085468 on epoch=799
06/18/2022 13:44:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
06/18/2022 13:44:49 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
06/18/2022 13:44:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/18/2022 13:44:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=813
06/18/2022 13:44:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=816
06/18/2022 13:44:58 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.2548283752860412 on epoch=816
06/18/2022 13:45:01 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=819
06/18/2022 13:45:03 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
06/18/2022 13:45:06 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/18/2022 13:45:08 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=829
06/18/2022 13:45:11 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=833
06/18/2022 13:45:12 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.1316137566137566 on epoch=833
06/18/2022 13:45:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/18/2022 13:45:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
06/18/2022 13:45:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
06/18/2022 13:45:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=846
06/18/2022 13:45:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/18/2022 13:45:27 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.13597848511641616 on epoch=849
06/18/2022 13:45:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/18/2022 13:45:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
06/18/2022 13:45:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
06/18/2022 13:45:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/18/2022 13:45:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
06/18/2022 13:45:41 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.13960956029921545 on epoch=866
06/18/2022 13:45:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/18/2022 13:45:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/18/2022 13:45:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
06/18/2022 13:45:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=879
06/18/2022 13:45:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/18/2022 13:45:55 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.1078698553948832 on epoch=883
06/18/2022 13:45:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/18/2022 13:46:00 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=889
06/18/2022 13:46:03 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/18/2022 13:46:06 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/18/2022 13:46:08 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
06/18/2022 13:46:10 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.11532738095238095 on epoch=899
06/18/2022 13:46:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/18/2022 13:46:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/18/2022 13:46:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=909
06/18/2022 13:46:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
06/18/2022 13:46:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/18/2022 13:46:24 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.14691558441558442 on epoch=916
06/18/2022 13:46:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/18/2022 13:46:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/18/2022 13:46:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/18/2022 13:46:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/18/2022 13:46:37 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/18/2022 13:46:38 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.17904040404040403 on epoch=933
06/18/2022 13:46:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/18/2022 13:46:43 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=939
06/18/2022 13:46:46 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/18/2022 13:46:49 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
06/18/2022 13:46:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/18/2022 13:46:53 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.11192511192511193 on epoch=949
06/18/2022 13:46:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/18/2022 13:46:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/18/2022 13:47:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/18/2022 13:47:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/18/2022 13:47:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
06/18/2022 13:47:07 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.15439060614499212 on epoch=966
06/18/2022 13:47:09 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/18/2022 13:47:12 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/18/2022 13:47:15 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/18/2022 13:47:17 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/18/2022 13:47:20 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
06/18/2022 13:47:21 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.09442396313364057 on epoch=983
06/18/2022 13:47:24 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/18/2022 13:47:26 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/18/2022 13:47:29 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/18/2022 13:47:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=996
06/18/2022 13:47:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/18/2022 13:47:35 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:47:35 - INFO - __main__ - Printing 3 examples
06/18/2022 13:47:35 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/18/2022 13:47:35 - INFO - __main__ - ['neutral']
06/18/2022 13:47:35 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/18/2022 13:47:35 - INFO - __main__ - ['neutral']
06/18/2022 13:47:35 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/18/2022 13:47:35 - INFO - __main__ - ['neutral']
06/18/2022 13:47:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 13:47:35 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:47:35 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 13:47:35 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:47:35 - INFO - __main__ - Printing 3 examples
06/18/2022 13:47:35 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/18/2022 13:47:35 - INFO - __main__ - ['neutral']
06/18/2022 13:47:35 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/18/2022 13:47:35 - INFO - __main__ - ['neutral']
06/18/2022 13:47:35 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/18/2022 13:47:35 - INFO - __main__ - ['neutral']
06/18/2022 13:47:35 - INFO - __main__ - Tokenizing Input ...
06/18/2022 13:47:35 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:47:36 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.12910052910052908 on epoch=999
06/18/2022 13:47:36 - INFO - __main__ - save last model!
06/18/2022 13:47:36 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 13:47:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 13:47:36 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 13:47:36 - INFO - __main__ - Printing 3 examples
06/18/2022 13:47:36 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 13:47:36 - INFO - __main__ - ['contradiction']
06/18/2022 13:47:36 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 13:47:36 - INFO - __main__ - ['entailment']
06/18/2022 13:47:36 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 13:47:36 - INFO - __main__ - ['contradiction']
06/18/2022 13:47:36 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 13:47:36 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:47:37 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 13:47:54 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 13:47:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 13:47:55 - INFO - __main__ - Starting training!
06/18/2022 13:48:07 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_42_0.5_8_predictions.txt
06/18/2022 13:48:07 - INFO - __main__ - Classification-F1 on test data: 0.0369
06/18/2022 13:48:07 - INFO - __main__ - prefix=anli_16_42, lr=0.5, bsz=8, dev_performance=0.31854450942170237, test_performance=0.03686944585134999
06/18/2022 13:48:07 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.4, bsz=8 ...
06/18/2022 13:48:08 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:48:08 - INFO - __main__ - Printing 3 examples
06/18/2022 13:48:08 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/18/2022 13:48:08 - INFO - __main__ - ['neutral']
06/18/2022 13:48:08 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/18/2022 13:48:08 - INFO - __main__ - ['neutral']
06/18/2022 13:48:08 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/18/2022 13:48:08 - INFO - __main__ - ['neutral']
06/18/2022 13:48:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 13:48:08 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:48:08 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 13:48:08 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 13:48:08 - INFO - __main__ - Printing 3 examples
06/18/2022 13:48:08 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/18/2022 13:48:08 - INFO - __main__ - ['neutral']
06/18/2022 13:48:08 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/18/2022 13:48:08 - INFO - __main__ - ['neutral']
06/18/2022 13:48:08 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/18/2022 13:48:08 - INFO - __main__ - ['neutral']
06/18/2022 13:48:08 - INFO - __main__ - Tokenizing Input ...
06/18/2022 13:48:08 - INFO - __main__ - Tokenizing Output ...
06/18/2022 13:48:08 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 13:48:27 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 13:48:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 13:48:28 - INFO - __main__ - Starting training!
06/18/2022 13:48:31 - INFO - __main__ - Step 10 Global step 10 Train loss 0.80 on epoch=3
06/18/2022 13:48:34 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=6
06/18/2022 13:48:36 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=9
06/18/2022 13:48:39 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=13
06/18/2022 13:48:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=16
06/18/2022 13:48:43 - INFO - __main__ - Global step 50 Train loss 0.58 Classification-F1 0.23410986482599946 on epoch=16
06/18/2022 13:48:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.23410986482599946 on epoch=16, global_step=50
06/18/2022 13:48:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=19
06/18/2022 13:48:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.51 on epoch=23
06/18/2022 13:48:50 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=26
06/18/2022 13:48:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=29
06/18/2022 13:48:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=33
06/18/2022 13:48:57 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.24018714954105505 on epoch=33
06/18/2022 13:48:57 - INFO - __main__ - Saving model with best Classification-F1: 0.23410986482599946 -> 0.24018714954105505 on epoch=33, global_step=100
06/18/2022 13:48:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=36
06/18/2022 13:49:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=39
06/18/2022 13:49:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=43
06/18/2022 13:49:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=46
06/18/2022 13:49:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=49
06/18/2022 13:49:11 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.20908004778972522 on epoch=49
06/18/2022 13:49:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.56 on epoch=53
06/18/2022 13:49:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=56
06/18/2022 13:49:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=59
06/18/2022 13:49:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=63
06/18/2022 13:49:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=66
06/18/2022 13:49:25 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16129032258064516 on epoch=66
06/18/2022 13:49:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
06/18/2022 13:49:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
06/18/2022 13:49:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=76
06/18/2022 13:49:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
06/18/2022 13:49:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=83
06/18/2022 13:49:40 - INFO - __main__ - Global step 250 Train loss 0.44 Classification-F1 0.18888888888888888 on epoch=83
06/18/2022 13:49:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=86
06/18/2022 13:49:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=89
06/18/2022 13:49:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=93
06/18/2022 13:49:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=96
06/18/2022 13:49:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=99
06/18/2022 13:49:54 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.23329622758194188 on epoch=99
06/18/2022 13:49:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=103
06/18/2022 13:49:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=106
06/18/2022 13:50:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=109
06/18/2022 13:50:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=113
06/18/2022 13:50:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=116
06/18/2022 13:50:08 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.23226616446955428 on epoch=116
06/18/2022 13:50:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=119
06/18/2022 13:50:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=123
06/18/2022 13:50:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=126
06/18/2022 13:50:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=129
06/18/2022 13:50:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=133
06/18/2022 13:50:23 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.211258697027198 on epoch=133
06/18/2022 13:50:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.31 on epoch=136
06/18/2022 13:50:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=139
06/18/2022 13:50:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=143
06/18/2022 13:50:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=146
06/18/2022 13:50:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=149
06/18/2022 13:50:37 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.3432679738562092 on epoch=149
06/18/2022 13:50:37 - INFO - __main__ - Saving model with best Classification-F1: 0.24018714954105505 -> 0.3432679738562092 on epoch=149, global_step=450
06/18/2022 13:50:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=153
06/18/2022 13:50:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=156
06/18/2022 13:50:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=159
06/18/2022 13:50:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=163
06/18/2022 13:50:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.29 on epoch=166
06/18/2022 13:50:51 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.211258697027198 on epoch=166
06/18/2022 13:50:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=169
06/18/2022 13:50:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=173
06/18/2022 13:50:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=176
06/18/2022 13:51:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.30 on epoch=179
06/18/2022 13:51:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=183
06/18/2022 13:51:05 - INFO - __main__ - Global step 550 Train loss 0.26 Classification-F1 0.33322748802624963 on epoch=183
06/18/2022 13:51:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=186
06/18/2022 13:51:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=189
06/18/2022 13:51:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=193
06/18/2022 13:51:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=196
06/18/2022 13:51:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=199
06/18/2022 13:51:19 - INFO - __main__ - Global step 600 Train loss 0.25 Classification-F1 0.3379546076018594 on epoch=199
06/18/2022 13:51:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=203
06/18/2022 13:51:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=206
06/18/2022 13:51:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=209
06/18/2022 13:51:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=213
06/18/2022 13:51:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=216
06/18/2022 13:51:34 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.28515529469307893 on epoch=216
06/18/2022 13:51:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=219
06/18/2022 13:51:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.17 on epoch=223
06/18/2022 13:51:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=226
06/18/2022 13:51:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=229
06/18/2022 13:51:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=233
06/18/2022 13:51:48 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.3040123456790123 on epoch=233
06/18/2022 13:51:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.18 on epoch=236
06/18/2022 13:51:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=239
06/18/2022 13:51:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=243
06/18/2022 13:51:58 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=246
06/18/2022 13:52:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=249
06/18/2022 13:52:02 - INFO - __main__ - Global step 750 Train loss 0.18 Classification-F1 0.2970819304152637 on epoch=249
06/18/2022 13:52:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=253
06/18/2022 13:52:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=256
06/18/2022 13:52:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=259
06/18/2022 13:52:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=263
06/18/2022 13:52:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=266
06/18/2022 13:52:16 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.3666666666666667 on epoch=266
06/18/2022 13:52:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3432679738562092 -> 0.3666666666666667 on epoch=266, global_step=800
06/18/2022 13:52:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=269
06/18/2022 13:52:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=273
06/18/2022 13:52:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=276
06/18/2022 13:52:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=279
06/18/2022 13:52:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=283
06/18/2022 13:52:31 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.33811802232854865 on epoch=283
06/18/2022 13:52:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=286
06/18/2022 13:52:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=289
06/18/2022 13:52:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=293
06/18/2022 13:52:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=296
06/18/2022 13:52:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=299
06/18/2022 13:52:45 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.28689133665530203 on epoch=299
06/18/2022 13:52:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=303
06/18/2022 13:52:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=306
06/18/2022 13:52:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=309
06/18/2022 13:52:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=313
06/18/2022 13:52:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=316
06/18/2022 13:52:59 - INFO - __main__ - Global step 950 Train loss 0.13 Classification-F1 0.21358533206969454 on epoch=316
06/18/2022 13:53:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.14 on epoch=319
06/18/2022 13:53:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=323
06/18/2022 13:53:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=326
06/18/2022 13:53:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=329
06/18/2022 13:53:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=333
06/18/2022 13:53:13 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.17412244897959184 on epoch=333
06/18/2022 13:53:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=336
06/18/2022 13:53:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=339
06/18/2022 13:53:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=343
06/18/2022 13:53:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=346
06/18/2022 13:53:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=349
06/18/2022 13:53:28 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.3116883116883116 on epoch=349
06/18/2022 13:53:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=353
06/18/2022 13:53:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=356
06/18/2022 13:53:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=359
06/18/2022 13:53:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=363
06/18/2022 13:53:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=366
06/18/2022 13:53:42 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.15506172839506172 on epoch=366
06/18/2022 13:53:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=369
06/18/2022 13:53:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=373
06/18/2022 13:53:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=376
06/18/2022 13:53:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=379
06/18/2022 13:53:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=383
06/18/2022 13:53:56 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.12454042081949059 on epoch=383
06/18/2022 13:53:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=386
06/18/2022 13:54:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=389
06/18/2022 13:54:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=393
06/18/2022 13:54:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=396
06/18/2022 13:54:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=399
06/18/2022 13:54:10 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.1350231737828637 on epoch=399
06/18/2022 13:54:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=403
06/18/2022 13:54:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=406
06/18/2022 13:54:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.10 on epoch=409
06/18/2022 13:54:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=413
06/18/2022 13:54:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=416
06/18/2022 13:54:25 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.11150754007896864 on epoch=416
06/18/2022 13:54:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=419
06/18/2022 13:54:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=423
06/18/2022 13:54:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=426
06/18/2022 13:54:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=429
06/18/2022 13:54:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=433
06/18/2022 13:54:39 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.17126216077828982 on epoch=433
06/18/2022 13:54:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=436
06/18/2022 13:54:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=439
06/18/2022 13:54:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=443
06/18/2022 13:54:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=446
06/18/2022 13:54:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=449
06/18/2022 13:54:53 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.16595655806182122 on epoch=449
06/18/2022 13:54:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=453
06/18/2022 13:54:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=456
06/18/2022 13:55:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=459
06/18/2022 13:55:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=463
06/18/2022 13:55:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=466
06/18/2022 13:55:08 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.10652595214020895 on epoch=466
06/18/2022 13:55:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=469
06/18/2022 13:55:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=473
06/18/2022 13:55:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=476
06/18/2022 13:55:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=479
06/18/2022 13:55:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=483
06/18/2022 13:55:22 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.12244897959183675 on epoch=483
06/18/2022 13:55:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=486
06/18/2022 13:55:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
06/18/2022 13:55:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=493
06/18/2022 13:55:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=496
06/18/2022 13:55:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=499
06/18/2022 13:55:37 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.13809523809523808 on epoch=499
06/18/2022 13:55:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=503
06/18/2022 13:55:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
06/18/2022 13:55:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=509
06/18/2022 13:55:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=513
06/18/2022 13:55:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
06/18/2022 13:55:51 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.12099358974358974 on epoch=516
06/18/2022 13:55:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=519
06/18/2022 13:55:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
06/18/2022 13:55:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=526
06/18/2022 13:56:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=529
06/18/2022 13:56:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=533
06/18/2022 13:56:06 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.13873626373626374 on epoch=533
06/18/2022 13:56:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=536
06/18/2022 13:56:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
06/18/2022 13:56:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=543
06/18/2022 13:56:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/18/2022 13:56:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=549
06/18/2022 13:56:20 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.11157346192735475 on epoch=549
06/18/2022 13:56:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
06/18/2022 13:56:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
06/18/2022 13:56:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=559
06/18/2022 13:56:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=563
06/18/2022 13:56:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=566
06/18/2022 13:56:34 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.11983822042467136 on epoch=566
06/18/2022 13:56:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
06/18/2022 13:56:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
06/18/2022 13:56:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=576
06/18/2022 13:56:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=579
06/18/2022 13:56:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
06/18/2022 13:56:49 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.13983739837398376 on epoch=583
06/18/2022 13:56:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=586
06/18/2022 13:56:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=589
06/18/2022 13:56:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=593
06/18/2022 13:56:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=596
06/18/2022 13:57:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
06/18/2022 13:57:03 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.10300366300366301 on epoch=599
06/18/2022 13:57:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=603
06/18/2022 13:57:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
06/18/2022 13:57:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=609
06/18/2022 13:57:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/18/2022 13:57:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=616
06/18/2022 13:57:17 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.10285714285714287 on epoch=616
06/18/2022 13:57:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/18/2022 13:57:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=623
06/18/2022 13:57:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=626
06/18/2022 13:57:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
06/18/2022 13:57:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/18/2022 13:57:32 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.15542448074208512 on epoch=633
06/18/2022 13:57:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=636
06/18/2022 13:57:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=639
06/18/2022 13:57:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/18/2022 13:57:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=646
06/18/2022 13:57:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/18/2022 13:57:46 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.17423901634427952 on epoch=649
06/18/2022 13:57:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/18/2022 13:57:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
06/18/2022 13:57:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
06/18/2022 13:57:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=663
06/18/2022 13:57:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=666
06/18/2022 13:58:00 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.1335042735042735 on epoch=666
06/18/2022 13:58:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
06/18/2022 13:58:06 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=673
06/18/2022 13:58:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
06/18/2022 13:58:11 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=679
06/18/2022 13:58:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=683
06/18/2022 13:58:15 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.15389036251105215 on epoch=683
06/18/2022 13:58:17 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=686
06/18/2022 13:58:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
06/18/2022 13:58:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=693
06/18/2022 13:58:25 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
06/18/2022 13:58:28 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/18/2022 13:58:29 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.13711180124223601 on epoch=699
06/18/2022 13:58:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=703
06/18/2022 13:58:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/18/2022 13:58:37 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=709
06/18/2022 13:58:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/18/2022 13:58:42 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=716
06/18/2022 13:58:44 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.12622392507449978 on epoch=716
06/18/2022 13:58:46 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=719
06/18/2022 13:58:49 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=723
06/18/2022 13:58:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=726
06/18/2022 13:58:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=729
06/18/2022 13:58:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
06/18/2022 13:58:58 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.1886028886028886 on epoch=733
06/18/2022 13:59:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=736
06/18/2022 13:59:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
06/18/2022 13:59:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=743
06/18/2022 13:59:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/18/2022 13:59:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
06/18/2022 13:59:12 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.14514712340799296 on epoch=749
06/18/2022 13:59:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
06/18/2022 13:59:18 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/18/2022 13:59:20 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
06/18/2022 13:59:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
06/18/2022 13:59:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=766
06/18/2022 13:59:27 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.10052910052910051 on epoch=766
06/18/2022 13:59:29 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
06/18/2022 13:59:32 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=773
06/18/2022 13:59:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
06/18/2022 13:59:37 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/18/2022 13:59:39 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/18/2022 13:59:41 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.0970271833429728 on epoch=783
06/18/2022 13:59:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
06/18/2022 13:59:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=789
06/18/2022 13:59:49 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=793
06/18/2022 13:59:51 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
06/18/2022 13:59:54 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/18/2022 13:59:55 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.10409035409035407 on epoch=799
06/18/2022 13:59:58 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
06/18/2022 14:00:00 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
06/18/2022 14:00:03 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=809
06/18/2022 14:00:05 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/18/2022 14:00:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/18/2022 14:00:09 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.11866332497911444 on epoch=816
06/18/2022 14:00:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/18/2022 14:00:14 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
06/18/2022 14:00:17 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/18/2022 14:00:20 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=829
06/18/2022 14:00:22 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=833
06/18/2022 14:00:24 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.14013464013464014 on epoch=833
06/18/2022 14:00:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/18/2022 14:00:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
06/18/2022 14:00:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/18/2022 14:00:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
06/18/2022 14:00:36 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/18/2022 14:00:38 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.11752953813104187 on epoch=849
06/18/2022 14:00:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
06/18/2022 14:00:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/18/2022 14:00:46 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
06/18/2022 14:00:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/18/2022 14:00:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/18/2022 14:00:52 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.1374189650051719 on epoch=866
06/18/2022 14:00:55 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
06/18/2022 14:00:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/18/2022 14:01:00 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/18/2022 14:01:03 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=879
06/18/2022 14:01:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=883
06/18/2022 14:01:07 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.11895201323772753 on epoch=883
06/18/2022 14:01:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/18/2022 14:01:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
06/18/2022 14:01:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=893
06/18/2022 14:01:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=896
06/18/2022 14:01:20 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/18/2022 14:01:21 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.10912639484068055 on epoch=899
06/18/2022 14:01:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=903
06/18/2022 14:01:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=906
06/18/2022 14:01:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
06/18/2022 14:01:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
06/18/2022 14:01:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/18/2022 14:01:35 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.14174603174603176 on epoch=916
06/18/2022 14:01:38 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
06/18/2022 14:01:41 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=923
06/18/2022 14:01:43 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
06/18/2022 14:01:46 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
06/18/2022 14:01:48 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/18/2022 14:01:50 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.12636445969779303 on epoch=933
06/18/2022 14:01:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/18/2022 14:01:55 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
06/18/2022 14:01:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/18/2022 14:02:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
06/18/2022 14:02:03 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=949
06/18/2022 14:02:04 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.10959939531368103 on epoch=949
06/18/2022 14:02:07 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/18/2022 14:02:09 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
06/18/2022 14:02:12 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/18/2022 14:02:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/18/2022 14:02:17 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/18/2022 14:02:18 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.10029051408361753 on epoch=966
06/18/2022 14:02:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/18/2022 14:02:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
06/18/2022 14:02:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
06/18/2022 14:02:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/18/2022 14:02:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=983
06/18/2022 14:02:32 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.10188326889186984 on epoch=983
06/18/2022 14:02:35 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
06/18/2022 14:02:38 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
06/18/2022 14:02:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
06/18/2022 14:02:43 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/18/2022 14:02:45 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=999
06/18/2022 14:02:47 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 14:02:47 - INFO - __main__ - Printing 3 examples
06/18/2022 14:02:47 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/18/2022 14:02:47 - INFO - __main__ - ['neutral']
06/18/2022 14:02:47 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/18/2022 14:02:47 - INFO - __main__ - ['neutral']
06/18/2022 14:02:47 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/18/2022 14:02:47 - INFO - __main__ - ['neutral']
06/18/2022 14:02:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 14:02:47 - INFO - __main__ - Tokenizing Output ...
06/18/2022 14:02:47 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 14:02:47 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 14:02:47 - INFO - __main__ - Printing 3 examples
06/18/2022 14:02:47 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/18/2022 14:02:47 - INFO - __main__ - ['neutral']
06/18/2022 14:02:47 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/18/2022 14:02:47 - INFO - __main__ - ['neutral']
06/18/2022 14:02:47 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/18/2022 14:02:47 - INFO - __main__ - ['neutral']
06/18/2022 14:02:47 - INFO - __main__ - Tokenizing Input ...
06/18/2022 14:02:47 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.1297207367795603 on epoch=999
06/18/2022 14:02:47 - INFO - __main__ - save last model!
06/18/2022 14:02:47 - INFO - __main__ - Tokenizing Output ...
06/18/2022 14:02:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 14:02:47 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 14:02:47 - INFO - __main__ - Printing 3 examples
06/18/2022 14:02:47 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 14:02:47 - INFO - __main__ - ['contradiction']
06/18/2022 14:02:47 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 14:02:47 - INFO - __main__ - ['entailment']
06/18/2022 14:02:47 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 14:02:47 - INFO - __main__ - ['contradiction']
06/18/2022 14:02:47 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 14:02:47 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 14:02:47 - INFO - __main__ - Tokenizing Output ...
06/18/2022 14:02:49 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 14:03:06 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 14:03:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 14:03:07 - INFO - __main__ - Starting training!
06/18/2022 14:03:18 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_42_0.4_8_predictions.txt
06/18/2022 14:03:18 - INFO - __main__ - Classification-F1 on test data: 0.0559
06/18/2022 14:03:18 - INFO - __main__ - prefix=anli_16_42, lr=0.4, bsz=8, dev_performance=0.3666666666666667, test_performance=0.0558567437531219
06/18/2022 14:03:18 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.3, bsz=8 ...
06/18/2022 14:03:19 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 14:03:19 - INFO - __main__ - Printing 3 examples
06/18/2022 14:03:19 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/18/2022 14:03:19 - INFO - __main__ - ['neutral']
06/18/2022 14:03:19 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/18/2022 14:03:19 - INFO - __main__ - ['neutral']
06/18/2022 14:03:19 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/18/2022 14:03:19 - INFO - __main__ - ['neutral']
06/18/2022 14:03:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 14:03:19 - INFO - __main__ - Tokenizing Output ...
06/18/2022 14:03:19 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 14:03:19 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 14:03:19 - INFO - __main__ - Printing 3 examples
06/18/2022 14:03:19 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/18/2022 14:03:19 - INFO - __main__ - ['neutral']
06/18/2022 14:03:19 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/18/2022 14:03:19 - INFO - __main__ - ['neutral']
06/18/2022 14:03:19 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/18/2022 14:03:19 - INFO - __main__ - ['neutral']
06/18/2022 14:03:19 - INFO - __main__ - Tokenizing Input ...
06/18/2022 14:03:19 - INFO - __main__ - Tokenizing Output ...
06/18/2022 14:03:19 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 14:03:35 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 14:03:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 14:03:36 - INFO - __main__ - Starting training!
06/18/2022 14:03:39 - INFO - __main__ - Step 10 Global step 10 Train loss 0.84 on epoch=3
06/18/2022 14:03:42 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=6
06/18/2022 14:03:44 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=9
06/18/2022 14:03:47 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=13
06/18/2022 14:03:49 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=16
06/18/2022 14:03:51 - INFO - __main__ - Global step 50 Train loss 0.58 Classification-F1 0.1983273596176822 on epoch=16
06/18/2022 14:03:51 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1983273596176822 on epoch=16, global_step=50
06/18/2022 14:03:53 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=19
06/18/2022 14:03:56 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=23
06/18/2022 14:03:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=26
06/18/2022 14:04:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=29
06/18/2022 14:04:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=33
06/18/2022 14:04:05 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=33
06/18/2022 14:04:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=36
06/18/2022 14:04:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=39
06/18/2022 14:04:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=43
06/18/2022 14:04:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=46
06/18/2022 14:04:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=49
06/18/2022 14:04:19 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.31788041776943 on epoch=49
06/18/2022 14:04:19 - INFO - __main__ - Saving model with best Classification-F1: 0.1983273596176822 -> 0.31788041776943 on epoch=49, global_step=150
06/18/2022 14:04:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
06/18/2022 14:04:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=56
06/18/2022 14:04:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=59
06/18/2022 14:04:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=63
06/18/2022 14:04:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=66
06/18/2022 14:04:33 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.25182863113897597 on epoch=66
06/18/2022 14:04:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=69
06/18/2022 14:04:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=73
06/18/2022 14:04:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.55 on epoch=76
06/18/2022 14:04:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
06/18/2022 14:04:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
06/18/2022 14:04:47 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.14942528735632185 on epoch=83
06/18/2022 14:04:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=86
06/18/2022 14:04:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=89
06/18/2022 14:04:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
06/18/2022 14:04:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=96
06/18/2022 14:05:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=99
06/18/2022 14:05:01 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.2127329192546584 on epoch=99
06/18/2022 14:05:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=103
06/18/2022 14:05:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
06/18/2022 14:05:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=109
06/18/2022 14:05:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=113
06/18/2022 14:05:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=116
06/18/2022 14:05:16 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.18275862068965518 on epoch=116
06/18/2022 14:05:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=119
06/18/2022 14:05:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=123
06/18/2022 14:05:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=126
06/18/2022 14:05:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=129
06/18/2022 14:05:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=133
06/18/2022 14:05:30 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.22434875066454016 on epoch=133
06/18/2022 14:05:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=136
06/18/2022 14:05:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=139
06/18/2022 14:05:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=143
06/18/2022 14:05:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=146
06/18/2022 14:05:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=149
06/18/2022 14:05:44 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.19696969696969693 on epoch=149
06/18/2022 14:05:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=153
06/18/2022 14:05:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=156
06/18/2022 14:05:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=159
06/18/2022 14:05:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=163
06/18/2022 14:05:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=166
06/18/2022 14:05:58 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.2276679841897233 on epoch=166
06/18/2022 14:06:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=169
06/18/2022 14:06:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.31 on epoch=173
06/18/2022 14:06:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=176
06/18/2022 14:06:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=179
06/18/2022 14:06:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=183
06/18/2022 14:06:12 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.28055904961565337 on epoch=183
06/18/2022 14:06:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.34 on epoch=186
06/18/2022 14:06:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.33 on epoch=189
06/18/2022 14:06:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.30 on epoch=193
06/18/2022 14:06:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=196
06/18/2022 14:06:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=199
06/18/2022 14:06:27 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.3062678062678063 on epoch=199
06/18/2022 14:06:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=203
06/18/2022 14:06:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=206
06/18/2022 14:06:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.30 on epoch=209
06/18/2022 14:06:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=213
06/18/2022 14:06:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=216
06/18/2022 14:06:41 - INFO - __main__ - Global step 650 Train loss 0.31 Classification-F1 0.34352144341045565 on epoch=216
06/18/2022 14:06:41 - INFO - __main__ - Saving model with best Classification-F1: 0.31788041776943 -> 0.34352144341045565 on epoch=216, global_step=650
06/18/2022 14:06:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=219
06/18/2022 14:06:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=223
06/18/2022 14:06:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.29 on epoch=226
06/18/2022 14:06:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=229
06/18/2022 14:06:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.28 on epoch=233
06/18/2022 14:06:55 - INFO - __main__ - Global step 700 Train loss 0.27 Classification-F1 0.3304152637485971 on epoch=233
06/18/2022 14:06:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.32 on epoch=236
06/18/2022 14:07:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=239
06/18/2022 14:07:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.31 on epoch=243
06/18/2022 14:07:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=246
06/18/2022 14:07:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=249
06/18/2022 14:07:09 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.35641998734977864 on epoch=249
06/18/2022 14:07:09 - INFO - __main__ - Saving model with best Classification-F1: 0.34352144341045565 -> 0.35641998734977864 on epoch=249, global_step=750
06/18/2022 14:07:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=253
06/18/2022 14:07:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=256
06/18/2022 14:07:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=259
06/18/2022 14:07:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=263
06/18/2022 14:07:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=266
06/18/2022 14:07:24 - INFO - __main__ - Global step 800 Train loss 0.23 Classification-F1 0.3532275132275133 on epoch=266
06/18/2022 14:07:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=269
06/18/2022 14:07:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=273
06/18/2022 14:07:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=276
06/18/2022 14:07:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=279
06/18/2022 14:07:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=283
06/18/2022 14:07:38 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.34472934472934474 on epoch=283
06/18/2022 14:07:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=286
06/18/2022 14:07:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=289
06/18/2022 14:07:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=293
06/18/2022 14:07:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=296
06/18/2022 14:07:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=299
06/18/2022 14:07:52 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.3297238173366147 on epoch=299
06/18/2022 14:07:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=303
06/18/2022 14:07:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=306
06/18/2022 14:08:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=309
06/18/2022 14:08:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=313
06/18/2022 14:08:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=316
06/18/2022 14:08:06 - INFO - __main__ - Global step 950 Train loss 0.17 Classification-F1 0.3583522297808012 on epoch=316
06/18/2022 14:08:06 - INFO - __main__ - Saving model with best Classification-F1: 0.35641998734977864 -> 0.3583522297808012 on epoch=316, global_step=950
06/18/2022 14:08:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=319
06/18/2022 14:08:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=323
06/18/2022 14:08:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=326
06/18/2022 14:08:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=329
06/18/2022 14:08:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=333
06/18/2022 14:08:21 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.2885117156216901 on epoch=333
06/18/2022 14:08:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.16 on epoch=336
06/18/2022 14:08:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.14 on epoch=339
06/18/2022 14:08:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=343
06/18/2022 14:08:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.14 on epoch=346
06/18/2022 14:08:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=349
06/18/2022 14:08:35 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.304927536231884 on epoch=349
06/18/2022 14:08:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.15 on epoch=353
06/18/2022 14:08:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.17 on epoch=356
06/18/2022 14:08:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=359
06/18/2022 14:08:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=363
06/18/2022 14:08:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=366
06/18/2022 14:08:49 - INFO - __main__ - Global step 1100 Train loss 0.14 Classification-F1 0.32830527497194156 on epoch=366
06/18/2022 14:08:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=369
06/18/2022 14:08:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=373
06/18/2022 14:08:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.12 on epoch=376
06/18/2022 14:08:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=379
06/18/2022 14:09:02 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=383
06/18/2022 14:09:03 - INFO - __main__ - Global step 1150 Train loss 0.13 Classification-F1 0.2628985507246377 on epoch=383
06/18/2022 14:09:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=386
06/18/2022 14:09:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=389
06/18/2022 14:09:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=393
06/18/2022 14:09:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=396
06/18/2022 14:09:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=399
06/18/2022 14:09:18 - INFO - __main__ - Global step 1200 Train loss 0.12 Classification-F1 0.26312997347480105 on epoch=399
06/18/2022 14:09:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=403
06/18/2022 14:09:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=406
06/18/2022 14:09:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=409
06/18/2022 14:09:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=413
06/18/2022 14:09:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=416
06/18/2022 14:09:32 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.26550062840385424 on epoch=416
06/18/2022 14:09:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=419
06/18/2022 14:09:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=423
06/18/2022 14:09:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=426
06/18/2022 14:09:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=429
06/18/2022 14:09:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=433
06/18/2022 14:09:46 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.1704680977936792 on epoch=433
06/18/2022 14:09:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=436
06/18/2022 14:09:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=439
06/18/2022 14:09:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=443
06/18/2022 14:09:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=446
06/18/2022 14:09:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=449
06/18/2022 14:10:01 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.20256171735241502 on epoch=449
06/18/2022 14:10:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=453
06/18/2022 14:10:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=456
06/18/2022 14:10:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=459
06/18/2022 14:10:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=463
06/18/2022 14:10:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=466
06/18/2022 14:10:15 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.15910761878503812 on epoch=466
06/18/2022 14:10:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=469
06/18/2022 14:10:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=473
06/18/2022 14:10:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=476
06/18/2022 14:10:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=479
06/18/2022 14:10:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=483
06/18/2022 14:10:29 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.12454212454212453 on epoch=483
06/18/2022 14:10:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=486
06/18/2022 14:10:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=489
06/18/2022 14:10:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=493
06/18/2022 14:10:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=496
06/18/2022 14:10:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=499
06/18/2022 14:10:43 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.10062092379165548 on epoch=499
06/18/2022 14:10:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=503
06/18/2022 14:10:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=506
06/18/2022 14:10:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=509
06/18/2022 14:10:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=513
06/18/2022 14:10:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=516
06/18/2022 14:10:58 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.1418060200668896 on epoch=516
06/18/2022 14:11:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=519
06/18/2022 14:11:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=523
06/18/2022 14:11:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=526
06/18/2022 14:11:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=529
06/18/2022 14:11:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=533
06/18/2022 14:11:12 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.12052952349877058 on epoch=533
06/18/2022 14:11:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=536
06/18/2022 14:11:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=539
06/18/2022 14:11:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=543
06/18/2022 14:11:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=546
06/18/2022 14:11:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=549
06/18/2022 14:11:26 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.1030896686159844 on epoch=549
06/18/2022 14:11:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=553
06/18/2022 14:11:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=556
06/18/2022 14:11:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=559
06/18/2022 14:11:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=563
06/18/2022 14:11:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=566
06/18/2022 14:11:40 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.1163200639062708 on epoch=566
06/18/2022 14:11:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
06/18/2022 14:11:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
06/18/2022 14:11:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=576
06/18/2022 14:11:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=579
06/18/2022 14:11:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
06/18/2022 14:11:55 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.14727118175394036 on epoch=583
06/18/2022 14:11:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
06/18/2022 14:12:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=589
06/18/2022 14:12:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=593
06/18/2022 14:12:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=596
06/18/2022 14:12:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=599
06/18/2022 14:12:09 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.1312891986062718 on epoch=599
06/18/2022 14:12:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=603
06/18/2022 14:12:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=606
06/18/2022 14:12:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
06/18/2022 14:12:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=613
06/18/2022 14:12:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
06/18/2022 14:12:23 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.15842279060669864 on epoch=616
06/18/2022 14:12:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=619
06/18/2022 14:12:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=623
06/18/2022 14:12:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=626
06/18/2022 14:12:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
06/18/2022 14:12:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
06/18/2022 14:12:37 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.131458477110651 on epoch=633
06/18/2022 14:12:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
06/18/2022 14:12:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=639
06/18/2022 14:12:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=643
06/18/2022 14:12:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=646
06/18/2022 14:12:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=649
06/18/2022 14:12:51 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.11739295187571049 on epoch=649
06/18/2022 14:12:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=653
06/18/2022 14:12:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=656
06/18/2022 14:12:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
06/18/2022 14:13:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/18/2022 14:13:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=666
06/18/2022 14:13:06 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.1231319038336582 on epoch=666
06/18/2022 14:13:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=669
06/18/2022 14:13:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=673
06/18/2022 14:13:14 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
06/18/2022 14:13:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
06/18/2022 14:13:19 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=683
06/18/2022 14:13:20 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.11834990406418978 on epoch=683
06/18/2022 14:13:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=686
06/18/2022 14:13:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=689
06/18/2022 14:13:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=693
06/18/2022 14:13:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=696
06/18/2022 14:13:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=699
06/18/2022 14:13:34 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.11913483342054769 on epoch=699
06/18/2022 14:13:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=703
06/18/2022 14:13:40 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
06/18/2022 14:13:42 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
06/18/2022 14:13:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
06/18/2022 14:13:47 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=716
06/18/2022 14:13:49 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.17768537768537768 on epoch=716
06/18/2022 14:13:51 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
06/18/2022 14:13:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
06/18/2022 14:13:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=726
06/18/2022 14:13:59 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
06/18/2022 14:14:01 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=733
06/18/2022 14:14:03 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.10456349206349207 on epoch=733
06/18/2022 14:14:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
06/18/2022 14:14:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=739
06/18/2022 14:14:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=743
06/18/2022 14:14:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/18/2022 14:14:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
06/18/2022 14:14:17 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.07515444015444014 on epoch=749
06/18/2022 14:14:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
06/18/2022 14:14:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=756
06/18/2022 14:14:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=759
06/18/2022 14:14:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
06/18/2022 14:14:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=766
06/18/2022 14:14:31 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.07923280423280424 on epoch=766
06/18/2022 14:14:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=769
06/18/2022 14:14:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=773
06/18/2022 14:14:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=776
06/18/2022 14:14:42 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/18/2022 14:14:44 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/18/2022 14:14:46 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.11215538847117794 on epoch=783
06/18/2022 14:14:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=786
06/18/2022 14:14:51 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=789
06/18/2022 14:14:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=793
06/18/2022 14:14:56 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=796
06/18/2022 14:14:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/18/2022 14:15:00 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.11830315034899076 on epoch=799
06/18/2022 14:15:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=803
06/18/2022 14:15:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
06/18/2022 14:15:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/18/2022 14:15:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
06/18/2022 14:15:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=816
06/18/2022 14:15:14 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.1532624557014801 on epoch=816
06/18/2022 14:15:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
06/18/2022 14:15:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
06/18/2022 14:15:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/18/2022 14:15:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/18/2022 14:15:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
06/18/2022 14:15:28 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.17492063492063492 on epoch=833
06/18/2022 14:15:31 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=836
06/18/2022 14:15:34 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
06/18/2022 14:15:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
06/18/2022 14:15:39 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
06/18/2022 14:15:41 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=849
06/18/2022 14:15:43 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.12661641233069806 on epoch=849
06/18/2022 14:15:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=853
06/18/2022 14:15:48 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
06/18/2022 14:15:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=859
06/18/2022 14:15:53 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
06/18/2022 14:15:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
06/18/2022 14:15:57 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.10554163185742131 on epoch=866
06/18/2022 14:16:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=869
06/18/2022 14:16:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/18/2022 14:16:05 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=876
06/18/2022 14:16:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=879
06/18/2022 14:16:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=883
06/18/2022 14:16:11 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.10444022770398481 on epoch=883
06/18/2022 14:16:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
06/18/2022 14:16:16 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=889
06/18/2022 14:16:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=893
06/18/2022 14:16:21 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=896
06/18/2022 14:16:24 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=899
06/18/2022 14:16:25 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.17338217338217338 on epoch=899
06/18/2022 14:16:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
06/18/2022 14:16:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/18/2022 14:16:33 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
06/18/2022 14:16:36 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
06/18/2022 14:16:38 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/18/2022 14:16:40 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.11566104702750667 on epoch=916
06/18/2022 14:16:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
06/18/2022 14:16:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=923
06/18/2022 14:16:47 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=926
06/18/2022 14:16:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=929
06/18/2022 14:16:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=933
06/18/2022 14:16:54 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.1281281281281281 on epoch=933
06/18/2022 14:16:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
06/18/2022 14:16:59 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
06/18/2022 14:17:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
06/18/2022 14:17:04 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
06/18/2022 14:17:07 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/18/2022 14:17:08 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.11199752628324057 on epoch=949
06/18/2022 14:17:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/18/2022 14:17:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=956
06/18/2022 14:17:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
06/18/2022 14:17:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/18/2022 14:17:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
06/18/2022 14:17:22 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.17207572684246117 on epoch=966
06/18/2022 14:17:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/18/2022 14:17:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=973
06/18/2022 14:17:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=976
06/18/2022 14:17:33 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=979
06/18/2022 14:17:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=983
06/18/2022 14:17:37 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.14566544566544565 on epoch=983
06/18/2022 14:17:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
06/18/2022 14:17:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
06/18/2022 14:17:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
06/18/2022 14:17:47 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
06/18/2022 14:17:50 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/18/2022 14:17:51 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 14:17:51 - INFO - __main__ - Printing 3 examples
06/18/2022 14:17:51 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/18/2022 14:17:51 - INFO - __main__ - ['neutral']
06/18/2022 14:17:51 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/18/2022 14:17:51 - INFO - __main__ - ['neutral']
06/18/2022 14:17:51 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/18/2022 14:17:51 - INFO - __main__ - ['neutral']
06/18/2022 14:17:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/18/2022 14:17:51 - INFO - __main__ - Tokenizing Output ...
06/18/2022 14:17:51 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.1710417022244979 on epoch=999
06/18/2022 14:17:51 - INFO - __main__ - save last model!
06/18/2022 14:17:51 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 14:17:51 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 14:17:51 - INFO - __main__ - Printing 3 examples
06/18/2022 14:17:51 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/18/2022 14:17:51 - INFO - __main__ - ['neutral']
06/18/2022 14:17:51 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/18/2022 14:17:51 - INFO - __main__ - ['neutral']
06/18/2022 14:17:51 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/18/2022 14:17:51 - INFO - __main__ - ['neutral']
06/18/2022 14:17:51 - INFO - __main__ - Tokenizing Input ...
06/18/2022 14:17:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/18/2022 14:17:51 - INFO - __main__ - Start tokenizing ... 1000 instances
06/18/2022 14:17:51 - INFO - __main__ - Printing 3 examples
06/18/2022 14:17:51 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/18/2022 14:17:51 - INFO - __main__ - ['contradiction']
06/18/2022 14:17:51 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/18/2022 14:17:51 - INFO - __main__ - ['entailment']
06/18/2022 14:17:51 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/18/2022 14:17:51 - INFO - __main__ - ['contradiction']
06/18/2022 14:17:51 - INFO - __main__ - Tokenizing Output ...
06/18/2022 14:17:51 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 14:17:51 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 14:17:52 - INFO - __main__ - Tokenizing Output ...
06/18/2022 14:17:53 - INFO - __main__ - Loaded 1000 examples from test data
06/18/2022 14:18:07 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 14:18:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 14:18:07 - INFO - __main__ - Starting training!
06/18/2022 14:18:23 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-10-up64shot/singletask-anli/anli_16_42_0.3_8_predictions.txt
06/18/2022 14:18:23 - INFO - __main__ - Classification-F1 on test data: 0.0498
06/18/2022 14:18:23 - INFO - __main__ - prefix=anli_16_42, lr=0.3, bsz=8, dev_performance=0.3583522297808012, test_performance=0.04982963174193319
06/18/2022 14:18:23 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.2, bsz=8 ...
06/18/2022 14:18:24 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 14:18:24 - INFO - __main__ - Printing 3 examples
06/18/2022 14:18:24 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/18/2022 14:18:24 - INFO - __main__ - ['neutral']
06/18/2022 14:18:24 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/18/2022 14:18:24 - INFO - __main__ - ['neutral']
06/18/2022 14:18:24 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/18/2022 14:18:24 - INFO - __main__ - ['neutral']
06/18/2022 14:18:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/18/2022 14:18:24 - INFO - __main__ - Tokenizing Output ...
06/18/2022 14:18:24 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
06/18/2022 14:18:24 - INFO - __main__ - Start tokenizing ... 48 instances
06/18/2022 14:18:24 - INFO - __main__ - Printing 3 examples
06/18/2022 14:18:24 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/18/2022 14:18:24 - INFO - __main__ - ['neutral']
06/18/2022 14:18:24 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/18/2022 14:18:24 - INFO - __main__ - ['neutral']
06/18/2022 14:18:24 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/18/2022 14:18:24 - INFO - __main__ - ['neutral']
06/18/2022 14:18:24 - INFO - __main__ - Tokenizing Input ...
06/18/2022 14:18:24 - INFO - __main__ - Tokenizing Output ...
06/18/2022 14:18:24 - INFO - __main__ - Loaded 48 examples from dev data
06/18/2022 14:18:39 - INFO - __main__ - load prompt embedding from ckpt
06/18/2022 14:18:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/18/2022 14:18:40 - INFO - __main__ - Starting training!
06/18/2022 14:18:43 - INFO - __main__ - Step 10 Global step 10 Train loss 0.85 on epoch=3
06/18/2022 14:18:46 - INFO - __main__ - Step 20 Global step 20 Train loss 0.62 on epoch=6
06/18/2022 14:18:48 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=9
06/18/2022 14:18:51 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=13
06/18/2022 14:18:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=16
06/18/2022 14:18:55 - INFO - __main__ - Global step 50 Train loss 0.60 Classification-F1 0.1983273596176822 on epoch=16
06/18/2022 14:18:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1983273596176822 on epoch=16, global_step=50
06/18/2022 14:18:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.52 on epoch=19
06/18/2022 14:19:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=23
06/18/2022 14:19:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.53 on epoch=26
06/18/2022 14:19:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=29
06/18/2022 14:19:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=33
06/18/2022 14:19:09 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.2085278555866791 on epoch=33
06/18/2022 14:19:09 - INFO - __main__ - Saving model with best Classification-F1: 0.1983273596176822 -> 0.2085278555866791 on epoch=33, global_step=100
06/18/2022 14:19:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=36
06/18/2022 14:19:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=39
06/18/2022 14:19:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=43
06/18/2022 14:19:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=46
06/18/2022 14:19:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
06/18/2022 14:19:23 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.2085278555866791 on epoch=49
06/18/2022 14:19:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
06/18/2022 14:19:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=56
06/18/2022 14:19:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=59
06/18/2022 14:19:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=63
06/18/2022 14:19:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=66
06/18/2022 14:19:37 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=66
06/18/2022 14:19:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
06/18/2022 14:19:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=73
06/18/2022 14:19:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=76
06/18/2022 14:19:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=79
06/18/2022 14:19:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=83
06/18/2022 14:19:51 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=83
06/18/2022 14:19:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=86
06/18/2022 14:19:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=89
06/18/2022 14:19:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=93
06/18/2022 14:20:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=96
06/18/2022 14:20:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=99
06/18/2022 14:20:05 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=99
06/18/2022 14:20:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=103
06/18/2022 14:20:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=106
06/18/2022 14:20:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=109
06/18/2022 14:20:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=113
06/18/2022 14:20:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
06/18/2022 14:20:19 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.2085278555866791 on epoch=116
06/18/2022 14:20:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=119
06/18/2022 14:20:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=123
06/18/2022 14:20:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=126
06/18/2022 14:20:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=129
06/18/2022 14:20:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=133
06/18/2022 14:20:33 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.1693121693121693 on epoch=133
06/18/2022 14:20:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=136
06/18/2022 14:20:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=139
06/18/2022 14:20:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=143
06/18/2022 14:20:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=146
06/18/2022 14:20:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=149
06/18/2022 14:20:47 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.1983273596176822 on epoch=149
06/18/2022 14:20:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=153
06/18/2022 14:20:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=156
06/18/2022 14:20:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=159
06/18/2022 14:20:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=163
06/18/2022 14:21:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=166
06/18/2022 14:21:01 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.1983273596176822 on epoch=166
