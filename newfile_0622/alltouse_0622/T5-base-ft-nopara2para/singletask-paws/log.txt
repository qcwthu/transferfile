05/15/2022 18:55:57 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=False, bsz_list=[4], cache_dir='/data/qin/cache/', checkpoint='None', cuda='4', dataset='nlp_forest_single', debug=False, dev_file='data', do_lowercase=False, do_predict=True, do_train=True, eval_period=50, freeze_embeds=False, gradient_accumulation_steps=2, identifier='T5-large-ft-nopara2para', learning_rate=0.5, learning_rate_list=[0.5], lm_adapted_path='/data/qin/lm_adapted_t5model/torch_ckpt/large/pytorch_model.bin', local_rank=0, log_step=10, max_grad_norm=1.0, max_input_length=512, max_output_length=128, model='google/t5-v1_1-large', num_beams=4, num_train_epochs=1000.0, output_dir='models/T5-large-ft-nopara2para/singletask-paws', predict_batch_size=16, predict_checkpoint='best-model.pt', prefix='', prompt_number=100, quiet=False, seed=42, task_dir='data/paws/', task_name='paws', test_file='data', total_steps=3000, train_batch_size=4, train_file='data', wait_step=10000000000, warmup_steps=50, weight_decay=1e-05)
05/15/2022 18:55:57 - INFO - __main__ - models/T5-large-ft-nopara2para/singletask-paws
06/24/2022 18:55:57 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/24/2022 18:55:57 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/24/2022 18:55:57 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-paws
06/24/2022 18:55:57 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-paws
06/24/2022 18:55:57 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/24/2022 18:55:57 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/24/2022 18:55:57 - INFO - __main__ - args.device: cuda:0
06/24/2022 18:55:57 - INFO - __main__ - args.device: cuda:1
06/24/2022 18:55:57 - INFO - __main__ - Using 2 gpus
06/24/2022 18:55:57 - INFO - __main__ - Using 2 gpus
06/24/2022 18:55:57 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/24/2022 18:55:57 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/24/2022 18:56:02 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.0005, bsz=8 ...
06/24/2022 18:56:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:56:03 - INFO - __main__ - Printing 3 examples
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 18:56:03 - INFO - __main__ - Printing 3 examples
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:56:03 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 18:56:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:56:03 - INFO - __main__ - Printing 3 examples
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 18:56:03 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 18:56:03 - INFO - __main__ - Printing 3 examples
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:56:03 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:56:03 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:56:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:56:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:56:08 - INFO - __main__ - Starting training!
06/24/2022 18:56:08 - INFO - __main__ - Starting training!
06/24/2022 18:56:10 - INFO - __main__ - Step 10 Global step 10 Train loss 18.488073 on epoch=4
06/24/2022 18:56:13 - INFO - __main__ - Step 20 Global step 20 Train loss 12.240129 on epoch=9
06/24/2022 18:56:15 - INFO - __main__ - Step 30 Global step 30 Train loss 8.048197 on epoch=14
06/24/2022 18:56:17 - INFO - __main__ - Step 40 Global step 40 Train loss 4.204375 on epoch=19
06/24/2022 18:56:20 - INFO - __main__ - Step 50 Global step 50 Train loss 3.076987 on epoch=24
06/24/2022 18:56:20 - INFO - __main__ - Global step 50 Train loss 9.211553 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 18:56:23 - INFO - __main__ - Step 60 Global step 60 Train loss 2.424563 on epoch=29
06/24/2022 18:56:25 - INFO - __main__ - Step 70 Global step 70 Train loss 2.539328 on epoch=34
06/24/2022 18:56:28 - INFO - __main__ - Step 80 Global step 80 Train loss 2.181306 on epoch=39
06/24/2022 18:56:30 - INFO - __main__ - Step 90 Global step 90 Train loss 1.503955 on epoch=44
06/24/2022 18:56:33 - INFO - __main__ - Step 100 Global step 100 Train loss 1.443496 on epoch=49
06/24/2022 18:56:33 - INFO - __main__ - Global step 100 Train loss 2.018530 Classification-F1 0.5076923076923077 on epoch=49
06/24/2022 18:56:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.764060 on epoch=54
06/24/2022 18:56:39 - INFO - __main__ - Step 120 Global step 120 Train loss 1.175244 on epoch=59
06/24/2022 18:56:41 - INFO - __main__ - Step 130 Global step 130 Train loss 1.134496 on epoch=64
06/24/2022 18:56:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.967894 on epoch=69
06/24/2022 18:56:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.937112 on epoch=74
06/24/2022 18:56:46 - INFO - __main__ - Global step 150 Train loss 0.995761 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 18:56:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.683671 on epoch=79
06/24/2022 18:56:51 - INFO - __main__ - Step 170 Global step 170 Train loss 1.046668 on epoch=84
06/24/2022 18:56:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.694055 on epoch=89
06/24/2022 18:56:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.959592 on epoch=94
06/24/2022 18:56:58 - INFO - __main__ - Step 200 Global step 200 Train loss 1.015592 on epoch=99
06/24/2022 18:56:59 - INFO - __main__ - Global step 200 Train loss 0.879916 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 18:57:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.796328 on epoch=104
06/24/2022 18:57:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.649964 on epoch=109
06/24/2022 18:57:06 - INFO - __main__ - Step 230 Global step 230 Train loss 1.199510 on epoch=114
06/24/2022 18:57:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.743072 on epoch=119
06/24/2022 18:57:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.760412 on epoch=124
06/24/2022 18:57:11 - INFO - __main__ - Global step 250 Train loss 0.829857 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 18:57:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.787524 on epoch=129
06/24/2022 18:57:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.996781 on epoch=134
06/24/2022 18:57:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.816902 on epoch=139
06/24/2022 18:57:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.859983 on epoch=144
06/24/2022 18:57:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.997016 on epoch=149
06/24/2022 18:57:23 - INFO - __main__ - Global step 300 Train loss 0.891641 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 18:57:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.538004 on epoch=154
06/24/2022 18:57:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.887252 on epoch=159
06/24/2022 18:57:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.970309 on epoch=164
06/24/2022 18:57:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.807468 on epoch=169
06/24/2022 18:57:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.657008 on epoch=174
06/24/2022 18:57:36 - INFO - __main__ - Global step 350 Train loss 0.772008 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 18:57:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.715020 on epoch=179
06/24/2022 18:57:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.490913 on epoch=184
06/24/2022 18:57:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.634690 on epoch=189
06/24/2022 18:57:46 - INFO - __main__ - Step 390 Global step 390 Train loss 1.061828 on epoch=194
06/24/2022 18:57:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.756035 on epoch=199
06/24/2022 18:57:48 - INFO - __main__ - Global step 400 Train loss 0.731697 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 18:57:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.570059 on epoch=204
06/24/2022 18:57:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.738593 on epoch=209
06/24/2022 18:57:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.639659 on epoch=214
06/24/2022 18:57:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.758286 on epoch=219
06/24/2022 18:58:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.646089 on epoch=224
06/24/2022 18:58:01 - INFO - __main__ - Global step 450 Train loss 0.670537 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 18:58:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.730290 on epoch=229
06/24/2022 18:58:06 - INFO - __main__ - Step 470 Global step 470 Train loss 1.032561 on epoch=234
06/24/2022 18:58:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.770202 on epoch=239
06/24/2022 18:58:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.702856 on epoch=244
06/24/2022 18:58:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.788700 on epoch=249
06/24/2022 18:58:13 - INFO - __main__ - Global step 500 Train loss 0.804922 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 18:58:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.630741 on epoch=254
06/24/2022 18:58:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.750160 on epoch=259
06/24/2022 18:58:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.757372 on epoch=264
06/24/2022 18:58:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.498567 on epoch=269
06/24/2022 18:58:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.420639 on epoch=274
06/24/2022 18:58:26 - INFO - __main__ - Global step 550 Train loss 0.611496 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 18:58:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.623642 on epoch=279
06/24/2022 18:58:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.559853 on epoch=284
06/24/2022 18:58:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.708903 on epoch=289
06/24/2022 18:58:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.595385 on epoch=294
06/24/2022 18:58:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.508255 on epoch=299
06/24/2022 18:58:38 - INFO - __main__ - Global step 600 Train loss 0.599208 Classification-F1 0.3333333333333333 on epoch=299
06/24/2022 18:58:38 - INFO - __main__ - save last model!
06/24/2022 18:58:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:58:39 - INFO - __main__ - Printing 3 examples
06/24/2022 18:58:39 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 18:58:39 - INFO - __main__ - ['1']
06/24/2022 18:58:39 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 18:58:39 - INFO - __main__ - ['1']
06/24/2022 18:58:39 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 18:58:39 - INFO - __main__ - ['1']
06/24/2022 18:58:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:58:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:58:39 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 18:58:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:58:39 - INFO - __main__ - Printing 3 examples
06/24/2022 18:58:39 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 18:58:39 - INFO - __main__ - ['1']
06/24/2022 18:58:39 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 18:58:39 - INFO - __main__ - ['1']
06/24/2022 18:58:39 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 18:58:39 - INFO - __main__ - ['1']
06/24/2022 18:58:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:58:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:58:39 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:58:41 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:58:41 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 18:58:41 - INFO - __main__ - Printing 3 examples
06/24/2022 18:58:41 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 18:58:41 - INFO - __main__ - ['0']
06/24/2022 18:58:41 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 18:58:41 - INFO - __main__ - ['1']
06/24/2022 18:58:41 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 18:58:41 - INFO - __main__ - ['1']
06/24/2022 18:58:41 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:58:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:58:43 - INFO - __main__ - Starting training!
06/24/2022 18:58:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:58:53 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 18:59:51 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_100_0.0005_8_predictions.txt
06/24/2022 18:59:51 - INFO - __main__ - Classification-F1 on test data: 0.4623
06/24/2022 18:59:51 - INFO - __main__ - prefix=paws_16_100, lr=0.0005, bsz=8, dev_performance=0.5076923076923077, test_performance=0.46229017965944685
06/24/2022 18:59:51 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.0003, bsz=8 ...
06/24/2022 18:59:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:59:52 - INFO - __main__ - Printing 3 examples
06/24/2022 18:59:52 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 18:59:52 - INFO - __main__ - ['1']
06/24/2022 18:59:52 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 18:59:52 - INFO - __main__ - ['1']
06/24/2022 18:59:52 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 18:59:52 - INFO - __main__ - ['1']
06/24/2022 18:59:52 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:59:52 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:59:52 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 18:59:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:59:52 - INFO - __main__ - Printing 3 examples
06/24/2022 18:59:52 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 18:59:52 - INFO - __main__ - ['1']
06/24/2022 18:59:52 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 18:59:52 - INFO - __main__ - ['1']
06/24/2022 18:59:52 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 18:59:52 - INFO - __main__ - ['1']
06/24/2022 18:59:52 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:59:52 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:59:52 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:59:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:59:56 - INFO - __main__ - Starting training!
06/24/2022 18:59:58 - INFO - __main__ - Step 10 Global step 10 Train loss 19.564877 on epoch=4
06/24/2022 19:00:01 - INFO - __main__ - Step 20 Global step 20 Train loss 14.314105 on epoch=9
06/24/2022 19:00:03 - INFO - __main__ - Step 30 Global step 30 Train loss 9.880041 on epoch=14
06/24/2022 19:00:06 - INFO - __main__ - Step 40 Global step 40 Train loss 7.545162 on epoch=19
06/24/2022 19:00:08 - INFO - __main__ - Step 50 Global step 50 Train loss 5.229483 on epoch=24
06/24/2022 19:00:09 - INFO - __main__ - Global step 50 Train loss 11.306735 Classification-F1 0.2074074074074074 on epoch=24
06/24/2022 19:00:11 - INFO - __main__ - Step 60 Global step 60 Train loss 3.636333 on epoch=29
06/24/2022 19:00:14 - INFO - __main__ - Step 70 Global step 70 Train loss 4.564093 on epoch=34
06/24/2022 19:00:17 - INFO - __main__ - Step 80 Global step 80 Train loss 3.496155 on epoch=39
06/24/2022 19:00:19 - INFO - __main__ - Step 90 Global step 90 Train loss 2.810601 on epoch=44
06/24/2022 19:00:22 - INFO - __main__ - Step 100 Global step 100 Train loss 2.644682 on epoch=49
06/24/2022 19:00:22 - INFO - __main__ - Global step 100 Train loss 3.430373 Classification-F1 0.3992490613266583 on epoch=49
06/24/2022 19:00:25 - INFO - __main__ - Step 110 Global step 110 Train loss 2.479713 on epoch=54
06/24/2022 19:00:28 - INFO - __main__ - Step 120 Global step 120 Train loss 2.144805 on epoch=59
06/24/2022 19:00:30 - INFO - __main__ - Step 130 Global step 130 Train loss 1.581690 on epoch=64
06/24/2022 19:00:33 - INFO - __main__ - Step 140 Global step 140 Train loss 1.670161 on epoch=69
06/24/2022 19:00:36 - INFO - __main__ - Step 150 Global step 150 Train loss 1.034819 on epoch=74
06/24/2022 19:00:36 - INFO - __main__ - Global step 150 Train loss 1.782238 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:00:38 - INFO - __main__ - Step 160 Global step 160 Train loss 1.173926 on epoch=79
06/24/2022 19:00:41 - INFO - __main__ - Step 170 Global step 170 Train loss 1.475846 on epoch=84
06/24/2022 19:00:44 - INFO - __main__ - Step 180 Global step 180 Train loss 1.243093 on epoch=89
06/24/2022 19:00:46 - INFO - __main__ - Step 190 Global step 190 Train loss 1.359110 on epoch=94
06/24/2022 19:00:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.834385 on epoch=99
06/24/2022 19:00:49 - INFO - __main__ - Global step 200 Train loss 1.217272 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:00:52 - INFO - __main__ - Step 210 Global step 210 Train loss 1.498053 on epoch=104
06/24/2022 19:00:54 - INFO - __main__ - Step 220 Global step 220 Train loss 1.514706 on epoch=109
06/24/2022 19:00:57 - INFO - __main__ - Step 230 Global step 230 Train loss 1.306730 on epoch=114
06/24/2022 19:00:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.629577 on epoch=119
06/24/2022 19:01:02 - INFO - __main__ - Step 250 Global step 250 Train loss 1.185808 on epoch=124
06/24/2022 19:01:02 - INFO - __main__ - Global step 250 Train loss 1.226975 Classification-F1 0.46843853820598 on epoch=124
06/24/2022 19:01:05 - INFO - __main__ - Step 260 Global step 260 Train loss 1.022391 on epoch=129
06/24/2022 19:01:08 - INFO - __main__ - Step 270 Global step 270 Train loss 1.302691 on epoch=134
06/24/2022 19:01:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.673943 on epoch=139
06/24/2022 19:01:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.851326 on epoch=144
06/24/2022 19:01:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.947689 on epoch=149
06/24/2022 19:01:16 - INFO - __main__ - Global step 300 Train loss 0.959608 Classification-F1 0.3816425120772947 on epoch=149
06/24/2022 19:01:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.949186 on epoch=154
06/24/2022 19:01:21 - INFO - __main__ - Step 320 Global step 320 Train loss 1.163329 on epoch=159
06/24/2022 19:01:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.928209 on epoch=164
06/24/2022 19:01:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.790151 on epoch=169
06/24/2022 19:01:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.944816 on epoch=174
06/24/2022 19:01:29 - INFO - __main__ - Global step 350 Train loss 0.955138 Classification-F1 0.3454545454545454 on epoch=174
06/24/2022 19:01:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.681252 on epoch=179
06/24/2022 19:01:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.788602 on epoch=184
06/24/2022 19:01:36 - INFO - __main__ - Step 380 Global step 380 Train loss 1.006303 on epoch=189
06/24/2022 19:01:39 - INFO - __main__ - Step 390 Global step 390 Train loss 1.079082 on epoch=194
06/24/2022 19:01:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.660370 on epoch=199
06/24/2022 19:01:42 - INFO - __main__ - Global step 400 Train loss 0.843122 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 19:01:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.823107 on epoch=204
06/24/2022 19:01:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.771528 on epoch=209
06/24/2022 19:01:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.539242 on epoch=214
06/24/2022 19:01:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.940730 on epoch=219
06/24/2022 19:01:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.841753 on epoch=224
06/24/2022 19:01:55 - INFO - __main__ - Global step 450 Train loss 0.783272 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 19:01:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.842726 on epoch=229
06/24/2022 19:02:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.822570 on epoch=234
06/24/2022 19:02:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.749368 on epoch=239
06/24/2022 19:02:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.784871 on epoch=244
06/24/2022 19:02:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.622654 on epoch=249
06/24/2022 19:02:08 - INFO - __main__ - Global step 500 Train loss 0.764438 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 19:02:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.868877 on epoch=254
06/24/2022 19:02:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.701646 on epoch=259
06/24/2022 19:02:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.561440 on epoch=264
06/24/2022 19:02:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.653917 on epoch=269
06/24/2022 19:02:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.702932 on epoch=274
06/24/2022 19:02:21 - INFO - __main__ - Global step 550 Train loss 0.697763 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 19:02:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.634953 on epoch=279
06/24/2022 19:02:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.784902 on epoch=284
06/24/2022 19:02:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.486673 on epoch=289
06/24/2022 19:02:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.511099 on epoch=294
06/24/2022 19:02:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.787423 on epoch=299
06/24/2022 19:02:34 - INFO - __main__ - Global step 600 Train loss 0.641010 Classification-F1 0.4554554554554554 on epoch=299
06/24/2022 19:02:34 - INFO - __main__ - save last model!
06/24/2022 19:02:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:02:35 - INFO - __main__ - Printing 3 examples
06/24/2022 19:02:35 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 19:02:35 - INFO - __main__ - ['1']
06/24/2022 19:02:35 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 19:02:35 - INFO - __main__ - ['1']
06/24/2022 19:02:35 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 19:02:35 - INFO - __main__ - ['1']
06/24/2022 19:02:35 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:02:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:02:35 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:02:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:02:35 - INFO - __main__ - Printing 3 examples
06/24/2022 19:02:35 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 19:02:35 - INFO - __main__ - ['1']
06/24/2022 19:02:35 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 19:02:35 - INFO - __main__ - ['1']
06/24/2022 19:02:35 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 19:02:35 - INFO - __main__ - ['1']
06/24/2022 19:02:35 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:02:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:02:35 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:02:37 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:02:38 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:02:38 - INFO - __main__ - Printing 3 examples
06/24/2022 19:02:38 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:02:38 - INFO - __main__ - ['0']
06/24/2022 19:02:38 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:02:38 - INFO - __main__ - ['1']
06/24/2022 19:02:38 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:02:38 - INFO - __main__ - ['1']
06/24/2022 19:02:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:02:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:02:39 - INFO - __main__ - Starting training!
06/24/2022 19:02:42 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:02:49 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:03:49 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_100_0.0003_8_predictions.txt
06/24/2022 19:03:49 - INFO - __main__ - Classification-F1 on test data: 0.4501
06/24/2022 19:03:49 - INFO - __main__ - prefix=paws_16_100, lr=0.0003, bsz=8, dev_performance=0.46843853820598, test_performance=0.4500660469126676
06/24/2022 19:03:49 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.0002, bsz=8 ...
06/24/2022 19:03:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:03:50 - INFO - __main__ - Printing 3 examples
06/24/2022 19:03:50 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 19:03:50 - INFO - __main__ - ['1']
06/24/2022 19:03:50 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 19:03:50 - INFO - __main__ - ['1']
06/24/2022 19:03:50 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 19:03:50 - INFO - __main__ - ['1']
06/24/2022 19:03:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:03:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:03:50 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:03:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:03:50 - INFO - __main__ - Printing 3 examples
06/24/2022 19:03:50 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 19:03:50 - INFO - __main__ - ['1']
06/24/2022 19:03:50 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 19:03:50 - INFO - __main__ - ['1']
06/24/2022 19:03:50 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 19:03:50 - INFO - __main__ - ['1']
06/24/2022 19:03:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:03:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:03:50 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:03:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:03:54 - INFO - __main__ - Starting training!
06/24/2022 19:03:56 - INFO - __main__ - Step 10 Global step 10 Train loss 18.980835 on epoch=4
06/24/2022 19:03:58 - INFO - __main__ - Step 20 Global step 20 Train loss 15.102415 on epoch=9
06/24/2022 19:04:00 - INFO - __main__ - Step 30 Global step 30 Train loss 11.595715 on epoch=14
06/24/2022 19:04:03 - INFO - __main__ - Step 40 Global step 40 Train loss 8.434122 on epoch=19
06/24/2022 19:04:05 - INFO - __main__ - Step 50 Global step 50 Train loss 7.040746 on epoch=24
06/24/2022 19:04:06 - INFO - __main__ - Global step 50 Train loss 12.230765 Classification-F1 0.08440647711334948 on epoch=24
06/24/2022 19:04:09 - INFO - __main__ - Step 60 Global step 60 Train loss 5.368169 on epoch=29
06/24/2022 19:04:12 - INFO - __main__ - Step 70 Global step 70 Train loss 4.601288 on epoch=34
06/24/2022 19:04:14 - INFO - __main__ - Step 80 Global step 80 Train loss 3.595553 on epoch=39
06/24/2022 19:04:16 - INFO - __main__ - Step 90 Global step 90 Train loss 3.001110 on epoch=44
06/24/2022 19:04:19 - INFO - __main__ - Step 100 Global step 100 Train loss 2.110503 on epoch=49
06/24/2022 19:04:19 - INFO - __main__ - Global step 100 Train loss 3.735324 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:04:22 - INFO - __main__ - Step 110 Global step 110 Train loss 2.834332 on epoch=54
06/24/2022 19:04:25 - INFO - __main__ - Step 120 Global step 120 Train loss 1.803958 on epoch=59
06/24/2022 19:04:27 - INFO - __main__ - Step 130 Global step 130 Train loss 1.780046 on epoch=64
06/24/2022 19:04:29 - INFO - __main__ - Step 140 Global step 140 Train loss 1.349424 on epoch=69
06/24/2022 19:04:32 - INFO - __main__ - Step 150 Global step 150 Train loss 2.147232 on epoch=74
06/24/2022 19:04:32 - INFO - __main__ - Global step 150 Train loss 1.982998 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:04:35 - INFO - __main__ - Step 160 Global step 160 Train loss 2.074554 on epoch=79
06/24/2022 19:04:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.225931 on epoch=84
06/24/2022 19:04:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.903302 on epoch=89
06/24/2022 19:04:42 - INFO - __main__ - Step 190 Global step 190 Train loss 1.666656 on epoch=94
06/24/2022 19:04:44 - INFO - __main__ - Step 200 Global step 200 Train loss 1.225490 on epoch=99
06/24/2022 19:04:45 - INFO - __main__ - Global step 200 Train loss 1.419187 Classification-F1 0.4554554554554554 on epoch=99
06/24/2022 19:04:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.748977 on epoch=104
06/24/2022 19:04:50 - INFO - __main__ - Step 220 Global step 220 Train loss 1.080450 on epoch=109
06/24/2022 19:04:52 - INFO - __main__ - Step 230 Global step 230 Train loss 1.004850 on epoch=114
06/24/2022 19:04:55 - INFO - __main__ - Step 240 Global step 240 Train loss 1.097284 on epoch=119
06/24/2022 19:04:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.877564 on epoch=124
06/24/2022 19:04:58 - INFO - __main__ - Global step 250 Train loss 0.961825 Classification-F1 0.3454545454545454 on epoch=124
06/24/2022 19:05:00 - INFO - __main__ - Step 260 Global step 260 Train loss 1.373724 on epoch=129
06/24/2022 19:05:02 - INFO - __main__ - Step 270 Global step 270 Train loss 1.160308 on epoch=134
06/24/2022 19:05:05 - INFO - __main__ - Step 280 Global step 280 Train loss 1.026951 on epoch=139
06/24/2022 19:05:07 - INFO - __main__ - Step 290 Global step 290 Train loss 1.042904 on epoch=144
06/24/2022 19:05:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.964290 on epoch=149
06/24/2022 19:05:10 - INFO - __main__ - Global step 300 Train loss 1.113635 Classification-F1 0.28888888888888886 on epoch=149
06/24/2022 19:05:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.918783 on epoch=154
06/24/2022 19:05:15 - INFO - __main__ - Step 320 Global step 320 Train loss 1.008956 on epoch=159
06/24/2022 19:05:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.696177 on epoch=164
06/24/2022 19:05:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.753930 on epoch=169
06/24/2022 19:05:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.878734 on epoch=174
06/24/2022 19:05:22 - INFO - __main__ - Global step 350 Train loss 0.851316 Classification-F1 0.43529411764705883 on epoch=174
06/24/2022 19:05:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.959418 on epoch=179
06/24/2022 19:05:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.710231 on epoch=184
06/24/2022 19:05:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.425614 on epoch=189
06/24/2022 19:05:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.769840 on epoch=194
06/24/2022 19:05:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.474217 on epoch=199
06/24/2022 19:05:35 - INFO - __main__ - Global step 400 Train loss 0.667864 Classification-F1 0.4682306940371457 on epoch=199
06/24/2022 19:05:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.465213 on epoch=204
06/24/2022 19:05:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.476833 on epoch=209
06/24/2022 19:05:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.512757 on epoch=214
06/24/2022 19:05:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.363752 on epoch=219
06/24/2022 19:05:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.221288 on epoch=224
06/24/2022 19:05:48 - INFO - __main__ - Global step 450 Train loss 0.407969 Classification-F1 0.4420512820512821 on epoch=224
06/24/2022 19:05:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.270928 on epoch=229
06/24/2022 19:05:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.516473 on epoch=234
06/24/2022 19:05:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.394848 on epoch=239
06/24/2022 19:05:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.181207 on epoch=244
06/24/2022 19:06:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.319979 on epoch=249
06/24/2022 19:06:00 - INFO - __main__ - Global step 500 Train loss 0.336687 Classification-F1 0.4420512820512821 on epoch=249
06/24/2022 19:06:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.372477 on epoch=254
06/24/2022 19:06:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.086943 on epoch=259
06/24/2022 19:06:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.205025 on epoch=264
06/24/2022 19:06:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.152850 on epoch=269
06/24/2022 19:06:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.168420 on epoch=274
06/24/2022 19:06:13 - INFO - __main__ - Global step 550 Train loss 0.197143 Classification-F1 0.3816425120772947 on epoch=274
06/24/2022 19:06:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.181192 on epoch=279
06/24/2022 19:06:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.405202 on epoch=284
06/24/2022 19:06:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.045189 on epoch=289
06/24/2022 19:06:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.099061 on epoch=294
06/24/2022 19:06:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.112723 on epoch=299
06/24/2022 19:06:25 - INFO - __main__ - Global step 600 Train loss 0.168673 Classification-F1 0.41700404858299595 on epoch=299
06/24/2022 19:06:25 - INFO - __main__ - save last model!
06/24/2022 19:06:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:06:26 - INFO - __main__ - Printing 3 examples
06/24/2022 19:06:26 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 19:06:26 - INFO - __main__ - ['1']
06/24/2022 19:06:26 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 19:06:26 - INFO - __main__ - ['1']
06/24/2022 19:06:26 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 19:06:26 - INFO - __main__ - ['1']
06/24/2022 19:06:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:06:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:06:26 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:06:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:06:26 - INFO - __main__ - Printing 3 examples
06/24/2022 19:06:26 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 19:06:26 - INFO - __main__ - ['1']
06/24/2022 19:06:26 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 19:06:26 - INFO - __main__ - ['1']
06/24/2022 19:06:26 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 19:06:26 - INFO - __main__ - ['1']
06/24/2022 19:06:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:06:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:06:26 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:06:28 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:06:28 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:06:28 - INFO - __main__ - Printing 3 examples
06/24/2022 19:06:28 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:06:28 - INFO - __main__ - ['0']
06/24/2022 19:06:28 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:06:28 - INFO - __main__ - ['1']
06/24/2022 19:06:28 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:06:28 - INFO - __main__ - ['1']
06/24/2022 19:06:28 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:06:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:06:31 - INFO - __main__ - Starting training!
06/24/2022 19:06:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:06:40 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:07:39 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_100_0.0002_8_predictions.txt
06/24/2022 19:07:39 - INFO - __main__ - Classification-F1 on test data: 0.4975
06/24/2022 19:07:39 - INFO - __main__ - prefix=paws_16_100, lr=0.0002, bsz=8, dev_performance=0.4682306940371457, test_performance=0.4974788327502479
06/24/2022 19:07:39 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.0001, bsz=8 ...
06/24/2022 19:07:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:07:40 - INFO - __main__ - Printing 3 examples
06/24/2022 19:07:40 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 19:07:40 - INFO - __main__ - ['1']
06/24/2022 19:07:40 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 19:07:40 - INFO - __main__ - ['1']
06/24/2022 19:07:40 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 19:07:40 - INFO - __main__ - ['1']
06/24/2022 19:07:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:07:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:07:40 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:07:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:07:40 - INFO - __main__ - Printing 3 examples
06/24/2022 19:07:40 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 19:07:40 - INFO - __main__ - ['1']
06/24/2022 19:07:40 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 19:07:40 - INFO - __main__ - ['1']
06/24/2022 19:07:40 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 19:07:40 - INFO - __main__ - ['1']
06/24/2022 19:07:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:07:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:07:40 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:07:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:07:44 - INFO - __main__ - Starting training!
06/24/2022 19:07:46 - INFO - __main__ - Step 10 Global step 10 Train loss 18.157490 on epoch=4
06/24/2022 19:07:48 - INFO - __main__ - Step 20 Global step 20 Train loss 16.850668 on epoch=9
06/24/2022 19:07:51 - INFO - __main__ - Step 30 Global step 30 Train loss 13.300146 on epoch=14
06/24/2022 19:07:53 - INFO - __main__ - Step 40 Global step 40 Train loss 12.095674 on epoch=19
06/24/2022 19:07:56 - INFO - __main__ - Step 50 Global step 50 Train loss 10.740697 on epoch=24
06/24/2022 19:07:58 - INFO - __main__ - Global step 50 Train loss 14.228934 Classification-F1 0.006896551724137932 on epoch=24
06/24/2022 19:08:00 - INFO - __main__ - Step 60 Global step 60 Train loss 8.936110 on epoch=29
06/24/2022 19:08:03 - INFO - __main__ - Step 70 Global step 70 Train loss 7.996220 on epoch=34
06/24/2022 19:08:05 - INFO - __main__ - Step 80 Global step 80 Train loss 7.652712 on epoch=39
06/24/2022 19:08:08 - INFO - __main__ - Step 90 Global step 90 Train loss 8.114249 on epoch=44
06/24/2022 19:08:10 - INFO - __main__ - Step 100 Global step 100 Train loss 6.861159 on epoch=49
06/24/2022 19:08:11 - INFO - __main__ - Global step 100 Train loss 7.912091 Classification-F1 0.10077519379844961 on epoch=49
06/24/2022 19:08:14 - INFO - __main__ - Step 110 Global step 110 Train loss 6.685088 on epoch=54
06/24/2022 19:08:16 - INFO - __main__ - Step 120 Global step 120 Train loss 6.298478 on epoch=59
06/24/2022 19:08:19 - INFO - __main__ - Step 130 Global step 130 Train loss 5.211701 on epoch=64
06/24/2022 19:08:21 - INFO - __main__ - Step 140 Global step 140 Train loss 4.320987 on epoch=69
06/24/2022 19:08:23 - INFO - __main__ - Step 150 Global step 150 Train loss 4.430605 on epoch=74
06/24/2022 19:08:24 - INFO - __main__ - Global step 150 Train loss 5.389372 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:08:27 - INFO - __main__ - Step 160 Global step 160 Train loss 3.346350 on epoch=79
06/24/2022 19:08:29 - INFO - __main__ - Step 170 Global step 170 Train loss 3.140928 on epoch=84
06/24/2022 19:08:31 - INFO - __main__ - Step 180 Global step 180 Train loss 3.390441 on epoch=89
06/24/2022 19:08:34 - INFO - __main__ - Step 190 Global step 190 Train loss 3.170716 on epoch=94
06/24/2022 19:08:36 - INFO - __main__ - Step 200 Global step 200 Train loss 2.604563 on epoch=99
06/24/2022 19:08:37 - INFO - __main__ - Global step 200 Train loss 3.130600 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:08:39 - INFO - __main__ - Step 210 Global step 210 Train loss 2.664571 on epoch=104
06/24/2022 19:08:41 - INFO - __main__ - Step 220 Global step 220 Train loss 1.834392 on epoch=109
06/24/2022 19:08:44 - INFO - __main__ - Step 230 Global step 230 Train loss 2.289062 on epoch=114
06/24/2022 19:08:46 - INFO - __main__ - Step 240 Global step 240 Train loss 1.880731 on epoch=119
06/24/2022 19:08:49 - INFO - __main__ - Step 250 Global step 250 Train loss 1.504866 on epoch=124
06/24/2022 19:08:49 - INFO - __main__ - Global step 250 Train loss 2.034724 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:08:52 - INFO - __main__ - Step 260 Global step 260 Train loss 2.902151 on epoch=129
06/24/2022 19:08:54 - INFO - __main__ - Step 270 Global step 270 Train loss 2.743124 on epoch=134
06/24/2022 19:08:56 - INFO - __main__ - Step 280 Global step 280 Train loss 1.949452 on epoch=139
06/24/2022 19:08:59 - INFO - __main__ - Step 290 Global step 290 Train loss 2.160208 on epoch=144
06/24/2022 19:09:01 - INFO - __main__ - Step 300 Global step 300 Train loss 2.126773 on epoch=149
06/24/2022 19:09:02 - INFO - __main__ - Global step 300 Train loss 2.376342 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 19:09:04 - INFO - __main__ - Step 310 Global step 310 Train loss 2.177492 on epoch=154
06/24/2022 19:09:07 - INFO - __main__ - Step 320 Global step 320 Train loss 1.319015 on epoch=159
06/24/2022 19:09:09 - INFO - __main__ - Step 330 Global step 330 Train loss 1.321043 on epoch=164
06/24/2022 19:09:11 - INFO - __main__ - Step 340 Global step 340 Train loss 1.499443 on epoch=169
06/24/2022 19:09:14 - INFO - __main__ - Step 350 Global step 350 Train loss 1.378266 on epoch=174
06/24/2022 19:09:14 - INFO - __main__ - Global step 350 Train loss 1.539052 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 19:09:17 - INFO - __main__ - Step 360 Global step 360 Train loss 1.529958 on epoch=179
06/24/2022 19:09:19 - INFO - __main__ - Step 370 Global step 370 Train loss 1.710218 on epoch=184
06/24/2022 19:09:21 - INFO - __main__ - Step 380 Global step 380 Train loss 1.526869 on epoch=189
06/24/2022 19:09:24 - INFO - __main__ - Step 390 Global step 390 Train loss 1.277701 on epoch=194
06/24/2022 19:09:26 - INFO - __main__ - Step 400 Global step 400 Train loss 1.052454 on epoch=199
06/24/2022 19:09:27 - INFO - __main__ - Global step 400 Train loss 1.419440 Classification-F1 0.4458874458874459 on epoch=199
06/24/2022 19:09:30 - INFO - __main__ - Step 410 Global step 410 Train loss 1.237007 on epoch=204
06/24/2022 19:09:32 - INFO - __main__ - Step 420 Global step 420 Train loss 1.341453 on epoch=209
06/24/2022 19:09:34 - INFO - __main__ - Step 430 Global step 430 Train loss 1.557869 on epoch=214
06/24/2022 19:09:37 - INFO - __main__ - Step 440 Global step 440 Train loss 1.065002 on epoch=219
06/24/2022 19:09:39 - INFO - __main__ - Step 450 Global step 450 Train loss 1.395208 on epoch=224
06/24/2022 19:09:40 - INFO - __main__ - Global step 450 Train loss 1.319308 Classification-F1 0.4920634920634921 on epoch=224
06/24/2022 19:09:42 - INFO - __main__ - Step 460 Global step 460 Train loss 1.080009 on epoch=229
06/24/2022 19:09:45 - INFO - __main__ - Step 470 Global step 470 Train loss 1.163825 on epoch=234
06/24/2022 19:09:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.857817 on epoch=239
06/24/2022 19:09:50 - INFO - __main__ - Step 490 Global step 490 Train loss 1.176219 on epoch=244
06/24/2022 19:09:52 - INFO - __main__ - Step 500 Global step 500 Train loss 1.108552 on epoch=249
06/24/2022 19:09:53 - INFO - __main__ - Global step 500 Train loss 1.077284 Classification-F1 0.4231177094379639 on epoch=249
06/24/2022 19:09:55 - INFO - __main__ - Step 510 Global step 510 Train loss 1.002236 on epoch=254
06/24/2022 19:09:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.987150 on epoch=259
06/24/2022 19:10:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.931881 on epoch=264
06/24/2022 19:10:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.754783 on epoch=269
06/24/2022 19:10:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.560630 on epoch=274
06/24/2022 19:10:05 - INFO - __main__ - Global step 550 Train loss 0.847336 Classification-F1 0.4181818181818182 on epoch=274
06/24/2022 19:10:08 - INFO - __main__ - Step 560 Global step 560 Train loss 1.015641 on epoch=279
06/24/2022 19:10:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.739580 on epoch=284
06/24/2022 19:10:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.786541 on epoch=289
06/24/2022 19:10:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.991137 on epoch=294
06/24/2022 19:10:17 - INFO - __main__ - Step 600 Global step 600 Train loss 1.004584 on epoch=299
06/24/2022 19:10:18 - INFO - __main__ - Global step 600 Train loss 0.907497 Classification-F1 0.3191489361702127 on epoch=299
06/24/2022 19:10:18 - INFO - __main__ - save last model!
06/24/2022 19:10:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:10:18 - INFO - __main__ - Printing 3 examples
06/24/2022 19:10:18 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:10:18 - INFO - __main__ - ['1']
06/24/2022 19:10:18 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:10:18 - INFO - __main__ - ['1']
06/24/2022 19:10:18 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:10:18 - INFO - __main__ - ['1']
06/24/2022 19:10:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:10:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:10:18 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:10:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:10:18 - INFO - __main__ - Printing 3 examples
06/24/2022 19:10:18 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:10:18 - INFO - __main__ - ['1']
06/24/2022 19:10:18 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:10:18 - INFO - __main__ - ['1']
06/24/2022 19:10:18 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:10:18 - INFO - __main__ - ['1']
06/24/2022 19:10:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:10:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:10:19 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:10:20 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:10:20 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:10:20 - INFO - __main__ - Printing 3 examples
06/24/2022 19:10:20 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:10:20 - INFO - __main__ - ['0']
06/24/2022 19:10:20 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:10:20 - INFO - __main__ - ['1']
06/24/2022 19:10:20 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:10:20 - INFO - __main__ - ['1']
06/24/2022 19:10:20 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:10:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:10:22 - INFO - __main__ - Starting training!
06/24/2022 19:10:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:10:32 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:11:31 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_100_0.0001_8_predictions.txt
06/24/2022 19:11:31 - INFO - __main__ - Classification-F1 on test data: 0.5202
06/24/2022 19:11:31 - INFO - __main__ - prefix=paws_16_100, lr=0.0001, bsz=8, dev_performance=0.4920634920634921, test_performance=0.5201521714015664
06/24/2022 19:11:31 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.0005, bsz=8 ...
06/24/2022 19:11:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:11:32 - INFO - __main__ - Printing 3 examples
06/24/2022 19:11:32 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:11:32 - INFO - __main__ - ['1']
06/24/2022 19:11:32 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:11:32 - INFO - __main__ - ['1']
06/24/2022 19:11:32 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:11:32 - INFO - __main__ - ['1']
06/24/2022 19:11:32 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:11:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:11:32 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:11:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:11:32 - INFO - __main__ - Printing 3 examples
06/24/2022 19:11:32 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:11:32 - INFO - __main__ - ['1']
06/24/2022 19:11:32 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:11:32 - INFO - __main__ - ['1']
06/24/2022 19:11:32 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:11:32 - INFO - __main__ - ['1']
06/24/2022 19:11:32 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:11:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:11:32 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:11:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:11:35 - INFO - __main__ - Starting training!
06/24/2022 19:11:37 - INFO - __main__ - Step 10 Global step 10 Train loss 18.760315 on epoch=4
06/24/2022 19:11:40 - INFO - __main__ - Step 20 Global step 20 Train loss 12.115448 on epoch=9
06/24/2022 19:11:42 - INFO - __main__ - Step 30 Global step 30 Train loss 8.715551 on epoch=14
06/24/2022 19:11:44 - INFO - __main__ - Step 40 Global step 40 Train loss 4.820919 on epoch=19
06/24/2022 19:11:47 - INFO - __main__ - Step 50 Global step 50 Train loss 3.682077 on epoch=24
06/24/2022 19:11:47 - INFO - __main__ - Global step 50 Train loss 9.618863 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 19:11:50 - INFO - __main__ - Step 60 Global step 60 Train loss 3.287867 on epoch=29
06/24/2022 19:11:52 - INFO - __main__ - Step 70 Global step 70 Train loss 2.385054 on epoch=34
06/24/2022 19:11:55 - INFO - __main__ - Step 80 Global step 80 Train loss 2.082311 on epoch=39
06/24/2022 19:11:57 - INFO - __main__ - Step 90 Global step 90 Train loss 1.386418 on epoch=44
06/24/2022 19:11:59 - INFO - __main__ - Step 100 Global step 100 Train loss 1.347488 on epoch=49
06/24/2022 19:12:00 - INFO - __main__ - Global step 100 Train loss 2.097827 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:12:02 - INFO - __main__ - Step 110 Global step 110 Train loss 1.132566 on epoch=54
06/24/2022 19:12:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.908656 on epoch=59
06/24/2022 19:12:07 - INFO - __main__ - Step 130 Global step 130 Train loss 1.371471 on epoch=64
06/24/2022 19:12:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.816266 on epoch=69
06/24/2022 19:12:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.858946 on epoch=74
06/24/2022 19:12:12 - INFO - __main__ - Global step 150 Train loss 1.017581 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:12:15 - INFO - __main__ - Step 160 Global step 160 Train loss 1.042486 on epoch=79
06/24/2022 19:12:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.715730 on epoch=84
06/24/2022 19:12:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.849375 on epoch=89
06/24/2022 19:12:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.751550 on epoch=94
06/24/2022 19:12:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.938317 on epoch=99
06/24/2022 19:12:25 - INFO - __main__ - Global step 200 Train loss 0.859492 Classification-F1 0.5134502923976608 on epoch=99
06/24/2022 19:12:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.783956 on epoch=104
06/24/2022 19:12:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.505614 on epoch=109
06/24/2022 19:12:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.511568 on epoch=114
06/24/2022 19:12:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.578023 on epoch=119
06/24/2022 19:12:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.433735 on epoch=124
06/24/2022 19:12:37 - INFO - __main__ - Global step 250 Train loss 0.562579 Classification-F1 0.3992490613266583 on epoch=124
06/24/2022 19:12:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.454726 on epoch=129
06/24/2022 19:12:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.456554 on epoch=134
06/24/2022 19:12:45 - INFO - __main__ - Step 280 Global step 280 Train loss 5.177397 on epoch=139
06/24/2022 19:12:47 - INFO - __main__ - Step 290 Global step 290 Train loss 1.995462 on epoch=144
06/24/2022 19:12:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.148491 on epoch=149
06/24/2022 19:12:50 - INFO - __main__ - Global step 300 Train loss 1.846526 Classification-F1 0.6875 on epoch=149
06/24/2022 19:12:53 - INFO - __main__ - Step 310 Global step 310 Train loss 1.237569 on epoch=154
06/24/2022 19:12:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.706731 on epoch=159
06/24/2022 19:12:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.806047 on epoch=164
06/24/2022 19:13:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.744628 on epoch=169
06/24/2022 19:13:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.654146 on epoch=174
06/24/2022 19:13:03 - INFO - __main__ - Global step 350 Train loss 0.829824 Classification-F1 0.25581395348837205 on epoch=174
06/24/2022 19:13:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.648310 on epoch=179
06/24/2022 19:13:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.711762 on epoch=184
06/24/2022 19:13:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.625237 on epoch=189
06/24/2022 19:13:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.565655 on epoch=194
06/24/2022 19:13:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.528193 on epoch=199
06/24/2022 19:13:15 - INFO - __main__ - Global step 400 Train loss 0.615831 Classification-F1 0.5555555555555556 on epoch=199
06/24/2022 19:13:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.634955 on epoch=204
06/24/2022 19:13:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.531699 on epoch=209
06/24/2022 19:13:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.697413 on epoch=214
06/24/2022 19:13:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.459813 on epoch=219
06/24/2022 19:13:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.495675 on epoch=224
06/24/2022 19:13:27 - INFO - __main__ - Global step 450 Train loss 0.563911 Classification-F1 0.5625 on epoch=224
06/24/2022 19:13:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.488927 on epoch=229
06/24/2022 19:13:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.656352 on epoch=234
06/24/2022 19:13:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.428672 on epoch=239
06/24/2022 19:13:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.337406 on epoch=244
06/24/2022 19:13:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.413742 on epoch=249
06/24/2022 19:13:40 - INFO - __main__ - Global step 500 Train loss 0.465020 Classification-F1 0.46843853820598 on epoch=249
06/24/2022 19:13:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.320089 on epoch=254
06/24/2022 19:13:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.451703 on epoch=259
06/24/2022 19:13:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.495477 on epoch=264
06/24/2022 19:13:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.249954 on epoch=269
06/24/2022 19:13:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.245073 on epoch=274
06/24/2022 19:13:52 - INFO - __main__ - Global step 550 Train loss 0.352459 Classification-F1 0.6113360323886641 on epoch=274
06/24/2022 19:13:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.292085 on epoch=279
06/24/2022 19:13:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.291854 on epoch=284
06/24/2022 19:13:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.232937 on epoch=289
06/24/2022 19:14:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.233480 on epoch=294
06/24/2022 19:14:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.243884 on epoch=299
06/24/2022 19:14:05 - INFO - __main__ - Global step 600 Train loss 0.258848 Classification-F1 0.5835835835835835 on epoch=299
06/24/2022 19:14:05 - INFO - __main__ - save last model!
06/24/2022 19:14:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:14:05 - INFO - __main__ - Printing 3 examples
06/24/2022 19:14:05 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:14:05 - INFO - __main__ - ['1']
06/24/2022 19:14:05 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:14:05 - INFO - __main__ - ['1']
06/24/2022 19:14:05 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:14:05 - INFO - __main__ - ['1']
06/24/2022 19:14:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:14:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:14:05 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:14:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:14:05 - INFO - __main__ - Printing 3 examples
06/24/2022 19:14:05 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:14:05 - INFO - __main__ - ['1']
06/24/2022 19:14:05 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:14:05 - INFO - __main__ - ['1']
06/24/2022 19:14:05 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:14:05 - INFO - __main__ - ['1']
06/24/2022 19:14:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:14:06 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:14:06 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:14:07 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:14:07 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:14:07 - INFO - __main__ - Printing 3 examples
06/24/2022 19:14:07 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:14:07 - INFO - __main__ - ['0']
06/24/2022 19:14:07 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:14:07 - INFO - __main__ - ['1']
06/24/2022 19:14:07 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:14:07 - INFO - __main__ - ['1']
06/24/2022 19:14:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:14:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:14:09 - INFO - __main__ - Starting training!
06/24/2022 19:14:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:14:19 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:15:19 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_13_0.0005_8_predictions.txt
06/24/2022 19:15:19 - INFO - __main__ - Classification-F1 on test data: 0.0903
06/24/2022 19:15:19 - INFO - __main__ - prefix=paws_16_13, lr=0.0005, bsz=8, dev_performance=0.6875, test_performance=0.09033191360243632
06/24/2022 19:15:19 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.0003, bsz=8 ...
06/24/2022 19:15:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:15:20 - INFO - __main__ - Printing 3 examples
06/24/2022 19:15:20 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:15:20 - INFO - __main__ - ['1']
06/24/2022 19:15:20 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:15:20 - INFO - __main__ - ['1']
06/24/2022 19:15:20 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:15:20 - INFO - __main__ - ['1']
06/24/2022 19:15:20 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:15:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:15:20 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:15:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:15:20 - INFO - __main__ - Printing 3 examples
06/24/2022 19:15:20 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:15:20 - INFO - __main__ - ['1']
06/24/2022 19:15:20 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:15:20 - INFO - __main__ - ['1']
06/24/2022 19:15:20 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:15:20 - INFO - __main__ - ['1']
06/24/2022 19:15:20 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:15:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:15:20 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:15:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:15:24 - INFO - __main__ - Starting training!
06/24/2022 19:15:26 - INFO - __main__ - Step 10 Global step 10 Train loss 18.989279 on epoch=4
06/24/2022 19:15:29 - INFO - __main__ - Step 20 Global step 20 Train loss 15.593657 on epoch=9
06/24/2022 19:15:31 - INFO - __main__ - Step 30 Global step 30 Train loss 9.811989 on epoch=14
06/24/2022 19:15:34 - INFO - __main__ - Step 40 Global step 40 Train loss 5.878953 on epoch=19
06/24/2022 19:15:36 - INFO - __main__ - Step 50 Global step 50 Train loss 5.563643 on epoch=24
06/24/2022 19:15:36 - INFO - __main__ - Global step 50 Train loss 11.167505 Classification-F1 0.21658986175115205 on epoch=24
06/24/2022 19:15:39 - INFO - __main__ - Step 60 Global step 60 Train loss 4.619464 on epoch=29
06/24/2022 19:15:42 - INFO - __main__ - Step 70 Global step 70 Train loss 3.961968 on epoch=34
06/24/2022 19:15:44 - INFO - __main__ - Step 80 Global step 80 Train loss 2.474783 on epoch=39
06/24/2022 19:15:47 - INFO - __main__ - Step 90 Global step 90 Train loss 2.873657 on epoch=44
06/24/2022 19:15:49 - INFO - __main__ - Step 100 Global step 100 Train loss 1.715405 on epoch=49
06/24/2022 19:15:49 - INFO - __main__ - Global step 100 Train loss 3.129055 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:15:52 - INFO - __main__ - Step 110 Global step 110 Train loss 2.100524 on epoch=54
06/24/2022 19:15:55 - INFO - __main__ - Step 120 Global step 120 Train loss 1.505673 on epoch=59
06/24/2022 19:15:57 - INFO - __main__ - Step 130 Global step 130 Train loss 1.843402 on epoch=64
06/24/2022 19:16:00 - INFO - __main__ - Step 140 Global step 140 Train loss 1.573747 on epoch=69
06/24/2022 19:16:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.983626 on epoch=74
06/24/2022 19:16:02 - INFO - __main__ - Global step 150 Train loss 1.601395 Classification-F1 0.3073593073593074 on epoch=74
06/24/2022 19:16:05 - INFO - __main__ - Step 160 Global step 160 Train loss 1.368447 on epoch=79
06/24/2022 19:16:07 - INFO - __main__ - Step 170 Global step 170 Train loss 1.396451 on epoch=84
06/24/2022 19:16:10 - INFO - __main__ - Step 180 Global step 180 Train loss 1.220609 on epoch=89
06/24/2022 19:16:12 - INFO - __main__ - Step 190 Global step 190 Train loss 1.258261 on epoch=94
06/24/2022 19:16:15 - INFO - __main__ - Step 200 Global step 200 Train loss 1.128282 on epoch=99
06/24/2022 19:16:15 - INFO - __main__ - Global step 200 Train loss 1.274410 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:16:18 - INFO - __main__ - Step 210 Global step 210 Train loss 1.009148 on epoch=104
06/24/2022 19:16:20 - INFO - __main__ - Step 220 Global step 220 Train loss 1.113517 on epoch=109
06/24/2022 19:16:23 - INFO - __main__ - Step 230 Global step 230 Train loss 1.051020 on epoch=114
06/24/2022 19:16:25 - INFO - __main__ - Step 240 Global step 240 Train loss 1.005140 on epoch=119
06/24/2022 19:16:28 - INFO - __main__ - Step 250 Global step 250 Train loss 1.160240 on epoch=124
06/24/2022 19:16:28 - INFO - __main__ - Global step 250 Train loss 1.067813 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:16:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.363951 on epoch=129
06/24/2022 19:16:33 - INFO - __main__ - Step 270 Global step 270 Train loss 1.121725 on epoch=134
06/24/2022 19:16:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.734535 on epoch=139
06/24/2022 19:16:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.782341 on epoch=144
06/24/2022 19:16:40 - INFO - __main__ - Step 300 Global step 300 Train loss 1.258429 on epoch=149
06/24/2022 19:16:41 - INFO - __main__ - Global step 300 Train loss 1.052196 Classification-F1 0.4181818181818182 on epoch=149
06/24/2022 19:16:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.840894 on epoch=154
06/24/2022 19:16:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.832283 on epoch=159
06/24/2022 19:16:49 - INFO - __main__ - Step 330 Global step 330 Train loss 1.145375 on epoch=164
06/24/2022 19:16:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.825061 on epoch=169
06/24/2022 19:16:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.797064 on epoch=174
06/24/2022 19:16:54 - INFO - __main__ - Global step 350 Train loss 0.888136 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 19:16:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.802375 on epoch=179
06/24/2022 19:16:59 - INFO - __main__ - Step 370 Global step 370 Train loss 1.160099 on epoch=184
06/24/2022 19:17:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.816765 on epoch=189
06/24/2022 19:17:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.739395 on epoch=194
06/24/2022 19:17:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.724374 on epoch=199
06/24/2022 19:17:07 - INFO - __main__ - Global step 400 Train loss 0.848602 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 19:17:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.369251 on epoch=204
06/24/2022 19:17:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.983281 on epoch=209
06/24/2022 19:17:14 - INFO - __main__ - Step 430 Global step 430 Train loss 1.000612 on epoch=214
06/24/2022 19:17:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.931226 on epoch=219
06/24/2022 19:17:19 - INFO - __main__ - Step 450 Global step 450 Train loss 1.133998 on epoch=224
06/24/2022 19:17:20 - INFO - __main__ - Global step 450 Train loss 0.883674 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 19:17:22 - INFO - __main__ - Step 460 Global step 460 Train loss 1.066952 on epoch=229
06/24/2022 19:17:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.859396 on epoch=234
06/24/2022 19:17:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.626426 on epoch=239
06/24/2022 19:17:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.733350 on epoch=244
06/24/2022 19:17:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.740199 on epoch=249
06/24/2022 19:17:32 - INFO - __main__ - Global step 500 Train loss 0.805265 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 19:17:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.624112 on epoch=254
06/24/2022 19:17:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.957918 on epoch=259
06/24/2022 19:17:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.544566 on epoch=264
06/24/2022 19:17:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.698383 on epoch=269
06/24/2022 19:17:45 - INFO - __main__ - Step 550 Global step 550 Train loss 1.025905 on epoch=274
06/24/2022 19:17:45 - INFO - __main__ - Global step 550 Train loss 0.770177 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 19:17:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.676705 on epoch=279
06/24/2022 19:17:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.844380 on epoch=284
06/24/2022 19:17:53 - INFO - __main__ - Step 580 Global step 580 Train loss 1.167803 on epoch=289
06/24/2022 19:17:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.611662 on epoch=294
06/24/2022 19:17:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.862417 on epoch=299
06/24/2022 19:17:58 - INFO - __main__ - Global step 600 Train loss 0.832593 Classification-F1 0.3333333333333333 on epoch=299
06/24/2022 19:17:58 - INFO - __main__ - save last model!
06/24/2022 19:17:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:17:59 - INFO - __main__ - Printing 3 examples
06/24/2022 19:17:59 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:17:59 - INFO - __main__ - ['1']
06/24/2022 19:17:59 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:17:59 - INFO - __main__ - ['1']
06/24/2022 19:17:59 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:17:59 - INFO - __main__ - ['1']
06/24/2022 19:17:59 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:17:59 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:17:59 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:17:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:17:59 - INFO - __main__ - Printing 3 examples
06/24/2022 19:17:59 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:17:59 - INFO - __main__ - ['1']
06/24/2022 19:17:59 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:17:59 - INFO - __main__ - ['1']
06/24/2022 19:17:59 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:17:59 - INFO - __main__ - ['1']
06/24/2022 19:17:59 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:17:59 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:17:59 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:18:00 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:18:01 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:18:01 - INFO - __main__ - Printing 3 examples
06/24/2022 19:18:01 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:18:01 - INFO - __main__ - ['0']
06/24/2022 19:18:01 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:18:01 - INFO - __main__ - ['1']
06/24/2022 19:18:01 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:18:01 - INFO - __main__ - ['1']
06/24/2022 19:18:01 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:18:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:18:03 - INFO - __main__ - Starting training!
06/24/2022 19:18:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:18:13 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:19:11 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_13_0.0003_8_predictions.txt
06/24/2022 19:19:11 - INFO - __main__ - Classification-F1 on test data: 0.4041
06/24/2022 19:19:11 - INFO - __main__ - prefix=paws_16_13, lr=0.0003, bsz=8, dev_performance=0.4181818181818182, test_performance=0.4040976231912496
06/24/2022 19:19:11 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.0002, bsz=8 ...
06/24/2022 19:19:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:19:12 - INFO - __main__ - Printing 3 examples
06/24/2022 19:19:12 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:19:12 - INFO - __main__ - ['1']
06/24/2022 19:19:12 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:19:12 - INFO - __main__ - ['1']
06/24/2022 19:19:12 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:19:12 - INFO - __main__ - ['1']
06/24/2022 19:19:12 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:19:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:19:12 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:19:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:19:12 - INFO - __main__ - Printing 3 examples
06/24/2022 19:19:12 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:19:12 - INFO - __main__ - ['1']
06/24/2022 19:19:12 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:19:12 - INFO - __main__ - ['1']
06/24/2022 19:19:12 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:19:12 - INFO - __main__ - ['1']
06/24/2022 19:19:12 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:19:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:19:13 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:19:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:19:17 - INFO - __main__ - Starting training!
06/24/2022 19:19:19 - INFO - __main__ - Step 10 Global step 10 Train loss 18.273840 on epoch=4
06/24/2022 19:19:21 - INFO - __main__ - Step 20 Global step 20 Train loss 16.040924 on epoch=9
06/24/2022 19:19:24 - INFO - __main__ - Step 30 Global step 30 Train loss 12.143332 on epoch=14
06/24/2022 19:19:26 - INFO - __main__ - Step 40 Global step 40 Train loss 9.210495 on epoch=19
06/24/2022 19:19:29 - INFO - __main__ - Step 50 Global step 50 Train loss 5.968013 on epoch=24
06/24/2022 19:19:29 - INFO - __main__ - Global step 50 Train loss 12.327322 Classification-F1 0.16880341880341881 on epoch=24
06/24/2022 19:19:32 - INFO - __main__ - Step 60 Global step 60 Train loss 4.674319 on epoch=29
06/24/2022 19:19:34 - INFO - __main__ - Step 70 Global step 70 Train loss 3.472973 on epoch=34
06/24/2022 19:19:37 - INFO - __main__ - Step 80 Global step 80 Train loss 3.442551 on epoch=39
06/24/2022 19:19:39 - INFO - __main__ - Step 90 Global step 90 Train loss 3.214423 on epoch=44
06/24/2022 19:19:42 - INFO - __main__ - Step 100 Global step 100 Train loss 2.372362 on epoch=49
06/24/2022 19:19:42 - INFO - __main__ - Global step 100 Train loss 3.435326 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:19:45 - INFO - __main__ - Step 110 Global step 110 Train loss 2.117404 on epoch=54
06/24/2022 19:19:48 - INFO - __main__ - Step 120 Global step 120 Train loss 2.197869 on epoch=59
06/24/2022 19:19:50 - INFO - __main__ - Step 130 Global step 130 Train loss 1.757640 on epoch=64
06/24/2022 19:19:53 - INFO - __main__ - Step 140 Global step 140 Train loss 1.856669 on epoch=69
06/24/2022 19:19:55 - INFO - __main__ - Step 150 Global step 150 Train loss 1.692746 on epoch=74
06/24/2022 19:19:56 - INFO - __main__ - Global step 150 Train loss 1.924466 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:19:58 - INFO - __main__ - Step 160 Global step 160 Train loss 1.381038 on epoch=79
06/24/2022 19:20:01 - INFO - __main__ - Step 170 Global step 170 Train loss 1.336419 on epoch=84
06/24/2022 19:20:03 - INFO - __main__ - Step 180 Global step 180 Train loss 1.370529 on epoch=89
06/24/2022 19:20:06 - INFO - __main__ - Step 190 Global step 190 Train loss 1.232659 on epoch=94
06/24/2022 19:20:09 - INFO - __main__ - Step 200 Global step 200 Train loss 1.112050 on epoch=99
06/24/2022 19:20:09 - INFO - __main__ - Global step 200 Train loss 1.286539 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:20:11 - INFO - __main__ - Step 210 Global step 210 Train loss 1.131809 on epoch=104
06/24/2022 19:20:14 - INFO - __main__ - Step 220 Global step 220 Train loss 1.727566 on epoch=109
06/24/2022 19:20:16 - INFO - __main__ - Step 230 Global step 230 Train loss 1.188752 on epoch=114
06/24/2022 19:20:19 - INFO - __main__ - Step 240 Global step 240 Train loss 1.006001 on epoch=119
06/24/2022 19:20:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.803380 on epoch=124
06/24/2022 19:20:22 - INFO - __main__ - Global step 250 Train loss 1.171502 Classification-F1 0.5307917888563051 on epoch=124
06/24/2022 19:20:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.879805 on epoch=129
06/24/2022 19:20:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.663818 on epoch=134
06/24/2022 19:20:30 - INFO - __main__ - Step 280 Global step 280 Train loss 1.069009 on epoch=139
06/24/2022 19:20:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.868480 on epoch=144
06/24/2022 19:20:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.842965 on epoch=149
06/24/2022 19:20:35 - INFO - __main__ - Global step 300 Train loss 0.864815 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 19:20:38 - INFO - __main__ - Step 310 Global step 310 Train loss 1.047143 on epoch=154
06/24/2022 19:20:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.928325 on epoch=159
06/24/2022 19:20:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.836877 on epoch=164
06/24/2022 19:20:45 - INFO - __main__ - Step 340 Global step 340 Train loss 1.186984 on epoch=169
06/24/2022 19:20:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.831804 on epoch=174
06/24/2022 19:20:48 - INFO - __main__ - Global step 350 Train loss 0.966227 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 19:20:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.758772 on epoch=179
06/24/2022 19:20:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.774523 on epoch=184
06/24/2022 19:20:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.843773 on epoch=189
06/24/2022 19:20:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.635545 on epoch=194
06/24/2022 19:21:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.886021 on epoch=199
06/24/2022 19:21:01 - INFO - __main__ - Global step 400 Train loss 0.779727 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 19:21:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.688743 on epoch=204
06/24/2022 19:21:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.820245 on epoch=209
06/24/2022 19:21:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.480946 on epoch=214
06/24/2022 19:21:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.878393 on epoch=219
06/24/2022 19:21:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.677200 on epoch=224
06/24/2022 19:21:14 - INFO - __main__ - Global step 450 Train loss 0.709106 Classification-F1 0.2727272727272727 on epoch=224
06/24/2022 19:21:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.716182 on epoch=229
06/24/2022 19:21:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.719746 on epoch=234
06/24/2022 19:21:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.896613 on epoch=239
06/24/2022 19:21:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.609989 on epoch=244
06/24/2022 19:21:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.673100 on epoch=249
06/24/2022 19:21:27 - INFO - __main__ - Global step 500 Train loss 0.723126 Classification-F1 0.5195195195195195 on epoch=249
06/24/2022 19:21:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.573552 on epoch=254
06/24/2022 19:21:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.838834 on epoch=259
06/24/2022 19:21:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.489635 on epoch=264
06/24/2022 19:21:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.258673 on epoch=269
06/24/2022 19:21:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.441423 on epoch=274
06/24/2022 19:21:40 - INFO - __main__ - Global step 550 Train loss 0.520423 Classification-F1 0.5933528836754642 on epoch=274
06/24/2022 19:21:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.466849 on epoch=279
06/24/2022 19:21:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.410597 on epoch=284
06/24/2022 19:21:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.665573 on epoch=289
06/24/2022 19:21:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.277529 on epoch=294
06/24/2022 19:21:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.312874 on epoch=299
06/24/2022 19:21:54 - INFO - __main__ - Global step 600 Train loss 0.426684 Classification-F1 0.46843853820598 on epoch=299
06/24/2022 19:21:54 - INFO - __main__ - save last model!
06/24/2022 19:21:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:21:54 - INFO - __main__ - Printing 3 examples
06/24/2022 19:21:54 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:21:54 - INFO - __main__ - ['1']
06/24/2022 19:21:54 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:21:54 - INFO - __main__ - ['1']
06/24/2022 19:21:54 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:21:54 - INFO - __main__ - ['1']
06/24/2022 19:21:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:21:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:21:54 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:21:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:21:54 - INFO - __main__ - Printing 3 examples
06/24/2022 19:21:54 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:21:54 - INFO - __main__ - ['1']
06/24/2022 19:21:54 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:21:54 - INFO - __main__ - ['1']
06/24/2022 19:21:54 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:21:54 - INFO - __main__ - ['1']
06/24/2022 19:21:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:21:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:21:54 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:21:56 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:21:56 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:21:56 - INFO - __main__ - Printing 3 examples
06/24/2022 19:21:56 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:21:56 - INFO - __main__ - ['0']
06/24/2022 19:21:56 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:21:56 - INFO - __main__ - ['1']
06/24/2022 19:21:56 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:21:56 - INFO - __main__ - ['1']
06/24/2022 19:21:56 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:21:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:21:58 - INFO - __main__ - Starting training!
06/24/2022 19:22:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:22:08 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:23:07 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_13_0.0002_8_predictions.txt
06/24/2022 19:23:07 - INFO - __main__ - Classification-F1 on test data: 0.4806
06/24/2022 19:23:07 - INFO - __main__ - prefix=paws_16_13, lr=0.0002, bsz=8, dev_performance=0.5933528836754642, test_performance=0.48062281959358544
06/24/2022 19:23:07 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.0001, bsz=8 ...
06/24/2022 19:23:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:23:08 - INFO - __main__ - Printing 3 examples
06/24/2022 19:23:08 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:23:08 - INFO - __main__ - ['1']
06/24/2022 19:23:08 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:23:08 - INFO - __main__ - ['1']
06/24/2022 19:23:08 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:23:08 - INFO - __main__ - ['1']
06/24/2022 19:23:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:23:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:23:08 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:23:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:23:08 - INFO - __main__ - Printing 3 examples
06/24/2022 19:23:08 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:23:08 - INFO - __main__ - ['1']
06/24/2022 19:23:08 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:23:08 - INFO - __main__ - ['1']
06/24/2022 19:23:08 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:23:08 - INFO - __main__ - ['1']
06/24/2022 19:23:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:23:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:23:08 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:23:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:23:11 - INFO - __main__ - Starting training!
06/24/2022 19:23:13 - INFO - __main__ - Step 10 Global step 10 Train loss 18.636131 on epoch=4
06/24/2022 19:23:16 - INFO - __main__ - Step 20 Global step 20 Train loss 16.268793 on epoch=9
06/24/2022 19:23:18 - INFO - __main__ - Step 30 Global step 30 Train loss 12.236886 on epoch=14
06/24/2022 19:23:20 - INFO - __main__ - Step 40 Global step 40 Train loss 11.255623 on epoch=19
06/24/2022 19:23:23 - INFO - __main__ - Step 50 Global step 50 Train loss 8.993055 on epoch=24
06/24/2022 19:23:25 - INFO - __main__ - Global step 50 Train loss 13.478098 Classification-F1 0.006791171477079796 on epoch=24
06/24/2022 19:23:28 - INFO - __main__ - Step 60 Global step 60 Train loss 9.676052 on epoch=29
06/24/2022 19:23:30 - INFO - __main__ - Step 70 Global step 70 Train loss 7.999013 on epoch=34
06/24/2022 19:23:32 - INFO - __main__ - Step 80 Global step 80 Train loss 6.637070 on epoch=39
06/24/2022 19:23:35 - INFO - __main__ - Step 90 Global step 90 Train loss 5.825637 on epoch=44
06/24/2022 19:23:37 - INFO - __main__ - Step 100 Global step 100 Train loss 5.261481 on epoch=49
06/24/2022 19:23:37 - INFO - __main__ - Global step 100 Train loss 7.079851 Classification-F1 0.22695035460992907 on epoch=49
06/24/2022 19:23:41 - INFO - __main__ - Step 110 Global step 110 Train loss 5.420087 on epoch=54
06/24/2022 19:23:43 - INFO - __main__ - Step 120 Global step 120 Train loss 3.888200 on epoch=59
06/24/2022 19:23:45 - INFO - __main__ - Step 130 Global step 130 Train loss 4.414083 on epoch=64
06/24/2022 19:23:48 - INFO - __main__ - Step 140 Global step 140 Train loss 3.250174 on epoch=69
06/24/2022 19:23:50 - INFO - __main__ - Step 150 Global step 150 Train loss 3.179623 on epoch=74
06/24/2022 19:23:51 - INFO - __main__ - Global step 150 Train loss 4.030434 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:23:53 - INFO - __main__ - Step 160 Global step 160 Train loss 3.728525 on epoch=79
06/24/2022 19:23:56 - INFO - __main__ - Step 170 Global step 170 Train loss 3.692624 on epoch=84
06/24/2022 19:23:58 - INFO - __main__ - Step 180 Global step 180 Train loss 2.515836 on epoch=89
06/24/2022 19:24:01 - INFO - __main__ - Step 190 Global step 190 Train loss 2.121382 on epoch=94
06/24/2022 19:24:03 - INFO - __main__ - Step 200 Global step 200 Train loss 1.789983 on epoch=99
06/24/2022 19:24:03 - INFO - __main__ - Global step 200 Train loss 2.769670 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:24:06 - INFO - __main__ - Step 210 Global step 210 Train loss 2.262825 on epoch=104
06/24/2022 19:24:08 - INFO - __main__ - Step 220 Global step 220 Train loss 2.051809 on epoch=109
06/24/2022 19:24:11 - INFO - __main__ - Step 230 Global step 230 Train loss 2.353955 on epoch=114
06/24/2022 19:24:13 - INFO - __main__ - Step 240 Global step 240 Train loss 2.745810 on epoch=119
06/24/2022 19:24:16 - INFO - __main__ - Step 250 Global step 250 Train loss 2.467023 on epoch=124
06/24/2022 19:24:16 - INFO - __main__ - Global step 250 Train loss 2.376285 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:24:18 - INFO - __main__ - Step 260 Global step 260 Train loss 1.458655 on epoch=129
06/24/2022 19:24:21 - INFO - __main__ - Step 270 Global step 270 Train loss 1.492821 on epoch=134
06/24/2022 19:24:23 - INFO - __main__ - Step 280 Global step 280 Train loss 1.493459 on epoch=139
06/24/2022 19:24:26 - INFO - __main__ - Step 290 Global step 290 Train loss 1.995163 on epoch=144
06/24/2022 19:24:28 - INFO - __main__ - Step 300 Global step 300 Train loss 1.289229 on epoch=149
06/24/2022 19:24:28 - INFO - __main__ - Global step 300 Train loss 1.545865 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 19:24:31 - INFO - __main__ - Step 310 Global step 310 Train loss 1.316971 on epoch=154
06/24/2022 19:24:33 - INFO - __main__ - Step 320 Global step 320 Train loss 1.281198 on epoch=159
06/24/2022 19:24:36 - INFO - __main__ - Step 330 Global step 330 Train loss 1.247386 on epoch=164
06/24/2022 19:24:38 - INFO - __main__ - Step 340 Global step 340 Train loss 1.453127 on epoch=169
06/24/2022 19:24:40 - INFO - __main__ - Step 350 Global step 350 Train loss 1.191969 on epoch=174
06/24/2022 19:24:41 - INFO - __main__ - Global step 350 Train loss 1.298130 Classification-F1 0.3816425120772947 on epoch=174
06/24/2022 19:24:44 - INFO - __main__ - Step 360 Global step 360 Train loss 1.664808 on epoch=179
06/24/2022 19:24:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.895225 on epoch=184
06/24/2022 19:24:49 - INFO - __main__ - Step 380 Global step 380 Train loss 1.078724 on epoch=189
06/24/2022 19:24:51 - INFO - __main__ - Step 390 Global step 390 Train loss 1.371546 on epoch=194
06/24/2022 19:24:54 - INFO - __main__ - Step 400 Global step 400 Train loss 1.299597 on epoch=199
06/24/2022 19:24:54 - INFO - __main__ - Global step 400 Train loss 1.261980 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 19:24:56 - INFO - __main__ - Step 410 Global step 410 Train loss 1.081830 on epoch=204
06/24/2022 19:24:59 - INFO - __main__ - Step 420 Global step 420 Train loss 1.022288 on epoch=209
06/24/2022 19:25:01 - INFO - __main__ - Step 430 Global step 430 Train loss 1.157412 on epoch=214
06/24/2022 19:25:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.971906 on epoch=219
06/24/2022 19:25:06 - INFO - __main__ - Step 450 Global step 450 Train loss 1.163168 on epoch=224
06/24/2022 19:25:06 - INFO - __main__ - Global step 450 Train loss 1.079321 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 19:25:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.843273 on epoch=229
06/24/2022 19:25:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.932024 on epoch=234
06/24/2022 19:25:13 - INFO - __main__ - Step 480 Global step 480 Train loss 1.117956 on epoch=239
06/24/2022 19:25:16 - INFO - __main__ - Step 490 Global step 490 Train loss 1.030971 on epoch=244
06/24/2022 19:25:18 - INFO - __main__ - Step 500 Global step 500 Train loss 1.013367 on epoch=249
06/24/2022 19:25:18 - INFO - __main__ - Global step 500 Train loss 0.987518 Classification-F1 0.539313399778516 on epoch=249
06/24/2022 19:25:21 - INFO - __main__ - Step 510 Global step 510 Train loss 1.069781 on epoch=254
06/24/2022 19:25:24 - INFO - __main__ - Step 520 Global step 520 Train loss 1.440531 on epoch=259
06/24/2022 19:25:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.940743 on epoch=264
06/24/2022 19:25:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.704118 on epoch=269
06/24/2022 19:25:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.723825 on epoch=274
06/24/2022 19:25:31 - INFO - __main__ - Global step 550 Train loss 0.975800 Classification-F1 0.46843853820598 on epoch=274
06/24/2022 19:25:34 - INFO - __main__ - Step 560 Global step 560 Train loss 1.204557 on epoch=279
06/24/2022 19:25:36 - INFO - __main__ - Step 570 Global step 570 Train loss 1.206268 on epoch=284
06/24/2022 19:25:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.954299 on epoch=289
06/24/2022 19:25:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.682955 on epoch=294
06/24/2022 19:25:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.763562 on epoch=299
06/24/2022 19:25:44 - INFO - __main__ - Global step 600 Train loss 0.962328 Classification-F1 0.5901477832512315 on epoch=299
06/24/2022 19:25:44 - INFO - __main__ - save last model!
06/24/2022 19:25:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:25:44 - INFO - __main__ - Printing 3 examples
06/24/2022 19:25:44 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:25:44 - INFO - __main__ - ['1']
06/24/2022 19:25:44 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:25:44 - INFO - __main__ - ['1']
06/24/2022 19:25:44 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:25:44 - INFO - __main__ - ['1']
06/24/2022 19:25:44 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:25:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:25:45 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:25:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:25:45 - INFO - __main__ - Printing 3 examples
06/24/2022 19:25:45 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:25:45 - INFO - __main__ - ['1']
06/24/2022 19:25:45 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:25:45 - INFO - __main__ - ['1']
06/24/2022 19:25:45 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:25:45 - INFO - __main__ - ['1']
06/24/2022 19:25:45 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:25:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:25:45 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:25:47 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:25:47 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:25:47 - INFO - __main__ - Printing 3 examples
06/24/2022 19:25:47 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:25:47 - INFO - __main__ - ['0']
06/24/2022 19:25:47 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:25:47 - INFO - __main__ - ['1']
06/24/2022 19:25:47 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:25:47 - INFO - __main__ - ['1']
06/24/2022 19:25:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:25:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:25:48 - INFO - __main__ - Starting training!
06/24/2022 19:25:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:25:59 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:26:57 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_13_0.0001_8_predictions.txt
06/24/2022 19:26:57 - INFO - __main__ - Classification-F1 on test data: 0.4997
06/24/2022 19:26:57 - INFO - __main__ - prefix=paws_16_13, lr=0.0001, bsz=8, dev_performance=0.5901477832512315, test_performance=0.499737493437336
06/24/2022 19:26:57 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.0005, bsz=8 ...
06/24/2022 19:26:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:26:58 - INFO - __main__ - Printing 3 examples
06/24/2022 19:26:58 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:26:58 - INFO - __main__ - ['1']
06/24/2022 19:26:58 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:26:58 - INFO - __main__ - ['1']
06/24/2022 19:26:58 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:26:58 - INFO - __main__ - ['1']
06/24/2022 19:26:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:26:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:26:58 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:26:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:26:58 - INFO - __main__ - Printing 3 examples
06/24/2022 19:26:58 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:26:58 - INFO - __main__ - ['1']
06/24/2022 19:26:58 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:26:58 - INFO - __main__ - ['1']
06/24/2022 19:26:58 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:26:58 - INFO - __main__ - ['1']
06/24/2022 19:26:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:26:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:26:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:27:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:27:02 - INFO - __main__ - Starting training!
06/24/2022 19:27:05 - INFO - __main__ - Step 10 Global step 10 Train loss 17.274071 on epoch=4
06/24/2022 19:27:07 - INFO - __main__ - Step 20 Global step 20 Train loss 13.891212 on epoch=9
06/24/2022 19:27:09 - INFO - __main__ - Step 30 Global step 30 Train loss 7.150117 on epoch=14
06/24/2022 19:27:12 - INFO - __main__ - Step 40 Global step 40 Train loss 3.365751 on epoch=19
06/24/2022 19:27:14 - INFO - __main__ - Step 50 Global step 50 Train loss 3.062154 on epoch=24
06/24/2022 19:27:15 - INFO - __main__ - Global step 50 Train loss 8.948662 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 19:27:17 - INFO - __main__ - Step 60 Global step 60 Train loss 2.588110 on epoch=29
06/24/2022 19:27:20 - INFO - __main__ - Step 70 Global step 70 Train loss 1.328197 on epoch=34
06/24/2022 19:27:22 - INFO - __main__ - Step 80 Global step 80 Train loss 1.152042 on epoch=39
06/24/2022 19:27:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.965765 on epoch=44
06/24/2022 19:27:27 - INFO - __main__ - Step 100 Global step 100 Train loss 1.025919 on epoch=49
06/24/2022 19:27:28 - INFO - __main__ - Global step 100 Train loss 1.412007 Classification-F1 0.5076923076923077 on epoch=49
06/24/2022 19:27:31 - INFO - __main__ - Step 110 Global step 110 Train loss 1.143474 on epoch=54
06/24/2022 19:27:33 - INFO - __main__ - Step 120 Global step 120 Train loss 1.003026 on epoch=59
06/24/2022 19:27:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.865615 on epoch=64
06/24/2022 19:27:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.885773 on epoch=69
06/24/2022 19:27:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.695284 on epoch=74
06/24/2022 19:27:41 - INFO - __main__ - Global step 150 Train loss 0.918634 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:27:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.837942 on epoch=79
06/24/2022 19:27:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.731949 on epoch=84
06/24/2022 19:27:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.450592 on epoch=89
06/24/2022 19:27:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.477154 on epoch=94
06/24/2022 19:27:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.507266 on epoch=99
06/24/2022 19:27:54 - INFO - __main__ - Global step 200 Train loss 0.600981 Classification-F1 0.5307917888563051 on epoch=99
06/24/2022 19:27:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.847844 on epoch=104
06/24/2022 19:28:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.956674 on epoch=109
06/24/2022 19:28:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.754661 on epoch=114
06/24/2022 19:28:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.366173 on epoch=119
06/24/2022 19:28:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.339032 on epoch=124
06/24/2022 19:28:08 - INFO - __main__ - Global step 250 Train loss 0.652877 Classification-F1 0.4385964912280702 on epoch=124
06/24/2022 19:28:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.455700 on epoch=129
06/24/2022 19:28:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.337170 on epoch=134
06/24/2022 19:28:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.456443 on epoch=139
06/24/2022 19:28:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.462760 on epoch=144
06/24/2022 19:28:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.394782 on epoch=149
06/24/2022 19:28:20 - INFO - __main__ - Global step 300 Train loss 0.421371 Classification-F1 0.49090909090909085 on epoch=149
06/24/2022 19:28:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.182850 on epoch=154
06/24/2022 19:28:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.229538 on epoch=159
06/24/2022 19:28:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.190890 on epoch=164
06/24/2022 19:28:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.189502 on epoch=169
06/24/2022 19:28:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.163849 on epoch=174
06/24/2022 19:28:33 - INFO - __main__ - Global step 350 Train loss 0.191326 Classification-F1 0.5333333333333333 on epoch=174
06/24/2022 19:28:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.145782 on epoch=179
06/24/2022 19:28:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.066831 on epoch=184
06/24/2022 19:28:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.110449 on epoch=189
06/24/2022 19:28:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.114463 on epoch=194
06/24/2022 19:28:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.050190 on epoch=199
06/24/2022 19:28:47 - INFO - __main__ - Global step 400 Train loss 0.097543 Classification-F1 0.4589371980676329 on epoch=199
06/24/2022 19:28:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.062793 on epoch=204
06/24/2022 19:28:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.049545 on epoch=209
06/24/2022 19:28:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.068164 on epoch=214
06/24/2022 19:28:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.006516 on epoch=219
06/24/2022 19:28:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.038918 on epoch=224
06/24/2022 19:29:00 - INFO - __main__ - Global step 450 Train loss 0.045187 Classification-F1 0.5076923076923077 on epoch=224
06/24/2022 19:29:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.025903 on epoch=229
06/24/2022 19:29:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.005597 on epoch=234
06/24/2022 19:29:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004013 on epoch=239
06/24/2022 19:29:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.041036 on epoch=244
06/24/2022 19:29:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.016925 on epoch=249
06/24/2022 19:29:13 - INFO - __main__ - Global step 500 Train loss 0.018695 Classification-F1 0.4181818181818182 on epoch=249
06/24/2022 19:29:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.003082 on epoch=254
06/24/2022 19:29:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001200 on epoch=259
06/24/2022 19:29:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001539 on epoch=264
06/24/2022 19:29:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000848 on epoch=269
06/24/2022 19:29:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002296 on epoch=274
06/24/2022 19:29:25 - INFO - __main__ - Global step 550 Train loss 0.001793 Classification-F1 0.4909862142099682 on epoch=274
06/24/2022 19:29:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000330 on epoch=279
06/24/2022 19:29:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003348 on epoch=284
06/24/2022 19:29:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.007378 on epoch=289
06/24/2022 19:29:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.068133 on epoch=294
06/24/2022 19:29:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003413 on epoch=299
06/24/2022 19:29:38 - INFO - __main__ - Global step 600 Train loss 0.016520 Classification-F1 0.4181818181818182 on epoch=299
06/24/2022 19:29:38 - INFO - __main__ - save last model!
06/24/2022 19:29:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:29:39 - INFO - __main__ - Printing 3 examples
06/24/2022 19:29:39 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:29:39 - INFO - __main__ - ['1']
06/24/2022 19:29:39 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:29:39 - INFO - __main__ - ['1']
06/24/2022 19:29:39 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:29:39 - INFO - __main__ - ['1']
06/24/2022 19:29:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:29:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:29:39 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:29:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:29:39 - INFO - __main__ - Printing 3 examples
06/24/2022 19:29:39 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:29:39 - INFO - __main__ - ['1']
06/24/2022 19:29:39 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:29:39 - INFO - __main__ - ['1']
06/24/2022 19:29:39 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:29:39 - INFO - __main__ - ['1']
06/24/2022 19:29:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:29:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:29:39 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:29:41 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:29:41 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:29:41 - INFO - __main__ - Printing 3 examples
06/24/2022 19:29:41 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:29:41 - INFO - __main__ - ['0']
06/24/2022 19:29:41 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:29:41 - INFO - __main__ - ['1']
06/24/2022 19:29:41 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:29:41 - INFO - __main__ - ['1']
06/24/2022 19:29:41 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:29:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:29:43 - INFO - __main__ - Starting training!
06/24/2022 19:29:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:29:53 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:30:53 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_21_0.0005_8_predictions.txt
06/24/2022 19:30:53 - INFO - __main__ - Classification-F1 on test data: 0.4685
06/24/2022 19:30:53 - INFO - __main__ - prefix=paws_16_21, lr=0.0005, bsz=8, dev_performance=0.5333333333333333, test_performance=0.46851376745822937
06/24/2022 19:30:53 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.0003, bsz=8 ...
06/24/2022 19:30:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:30:54 - INFO - __main__ - Printing 3 examples
06/24/2022 19:30:54 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:30:54 - INFO - __main__ - ['1']
06/24/2022 19:30:54 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:30:54 - INFO - __main__ - ['1']
06/24/2022 19:30:54 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:30:54 - INFO - __main__ - ['1']
06/24/2022 19:30:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:30:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:30:54 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:30:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:30:54 - INFO - __main__ - Printing 3 examples
06/24/2022 19:30:54 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:30:54 - INFO - __main__ - ['1']
06/24/2022 19:30:54 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:30:54 - INFO - __main__ - ['1']
06/24/2022 19:30:54 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:30:54 - INFO - __main__ - ['1']
06/24/2022 19:30:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:30:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:30:54 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:30:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:30:59 - INFO - __main__ - Starting training!
06/24/2022 19:31:01 - INFO - __main__ - Step 10 Global step 10 Train loss 18.257887 on epoch=4
06/24/2022 19:31:03 - INFO - __main__ - Step 20 Global step 20 Train loss 15.065611 on epoch=9
06/24/2022 19:31:06 - INFO - __main__ - Step 30 Global step 30 Train loss 8.665182 on epoch=14
06/24/2022 19:31:08 - INFO - __main__ - Step 40 Global step 40 Train loss 6.343405 on epoch=19
06/24/2022 19:31:11 - INFO - __main__ - Step 50 Global step 50 Train loss 4.519876 on epoch=24
06/24/2022 19:31:11 - INFO - __main__ - Global step 50 Train loss 10.570392 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 19:31:14 - INFO - __main__ - Step 60 Global step 60 Train loss 4.178642 on epoch=29
06/24/2022 19:31:16 - INFO - __main__ - Step 70 Global step 70 Train loss 3.215014 on epoch=34
06/24/2022 19:31:19 - INFO - __main__ - Step 80 Global step 80 Train loss 2.809184 on epoch=39
06/24/2022 19:31:21 - INFO - __main__ - Step 90 Global step 90 Train loss 2.724012 on epoch=44
06/24/2022 19:31:24 - INFO - __main__ - Step 100 Global step 100 Train loss 2.341506 on epoch=49
06/24/2022 19:31:24 - INFO - __main__ - Global step 100 Train loss 3.053672 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:31:27 - INFO - __main__ - Step 110 Global step 110 Train loss 1.596172 on epoch=54
06/24/2022 19:31:29 - INFO - __main__ - Step 120 Global step 120 Train loss 1.792162 on epoch=59
06/24/2022 19:31:32 - INFO - __main__ - Step 130 Global step 130 Train loss 1.329898 on epoch=64
06/24/2022 19:31:34 - INFO - __main__ - Step 140 Global step 140 Train loss 1.616809 on epoch=69
06/24/2022 19:31:37 - INFO - __main__ - Step 150 Global step 150 Train loss 1.020216 on epoch=74
06/24/2022 19:31:37 - INFO - __main__ - Global step 150 Train loss 1.471052 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:31:39 - INFO - __main__ - Step 160 Global step 160 Train loss 1.013776 on epoch=79
06/24/2022 19:31:42 - INFO - __main__ - Step 170 Global step 170 Train loss 1.230460 on epoch=84
06/24/2022 19:31:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.812347 on epoch=89
06/24/2022 19:31:47 - INFO - __main__ - Step 190 Global step 190 Train loss 1.048815 on epoch=94
06/24/2022 19:31:49 - INFO - __main__ - Step 200 Global step 200 Train loss 1.034082 on epoch=99
06/24/2022 19:31:50 - INFO - __main__ - Global step 200 Train loss 1.027896 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:31:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.676410 on epoch=104
06/24/2022 19:31:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.656824 on epoch=109
06/24/2022 19:31:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.723811 on epoch=114
06/24/2022 19:32:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.963222 on epoch=119
06/24/2022 19:32:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.983047 on epoch=124
06/24/2022 19:32:03 - INFO - __main__ - Global step 250 Train loss 0.800663 Classification-F1 0.4385964912280702 on epoch=124
06/24/2022 19:32:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.830445 on epoch=129
06/24/2022 19:32:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.860552 on epoch=134
06/24/2022 19:32:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.796031 on epoch=139
06/24/2022 19:32:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.864819 on epoch=144
06/24/2022 19:32:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.691149 on epoch=149
06/24/2022 19:32:16 - INFO - __main__ - Global step 300 Train loss 0.808599 Classification-F1 0.3191489361702127 on epoch=149
06/24/2022 19:32:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.594157 on epoch=154
06/24/2022 19:32:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.644697 on epoch=159
06/24/2022 19:32:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.457326 on epoch=164
06/24/2022 19:32:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.692143 on epoch=169
06/24/2022 19:32:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.775566 on epoch=174
06/24/2022 19:32:29 - INFO - __main__ - Global step 350 Train loss 0.632778 Classification-F1 0.4589371980676329 on epoch=174
06/24/2022 19:32:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.592881 on epoch=179
06/24/2022 19:32:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.775918 on epoch=184
06/24/2022 19:32:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.371163 on epoch=189
06/24/2022 19:32:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.578915 on epoch=194
06/24/2022 19:32:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.429777 on epoch=199
06/24/2022 19:32:43 - INFO - __main__ - Global step 400 Train loss 0.549731 Classification-F1 0.4385964912280702 on epoch=199
06/24/2022 19:32:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.427920 on epoch=204
06/24/2022 19:32:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.731084 on epoch=209
06/24/2022 19:32:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.500697 on epoch=214
06/24/2022 19:32:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.232082 on epoch=219
06/24/2022 19:32:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.134614 on epoch=224
06/24/2022 19:32:55 - INFO - __main__ - Global step 450 Train loss 0.405280 Classification-F1 0.46843853820598 on epoch=224
06/24/2022 19:32:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.112991 on epoch=229
06/24/2022 19:33:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.186540 on epoch=234
06/24/2022 19:33:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.167116 on epoch=239
06/24/2022 19:33:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.059800 on epoch=244
06/24/2022 19:33:09 - INFO - __main__ - Step 500 Global step 500 Train loss 1.046728 on epoch=249
06/24/2022 19:33:09 - INFO - __main__ - Global step 500 Train loss 0.314635 Classification-F1 0.3992490613266583 on epoch=249
06/24/2022 19:33:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.289367 on epoch=254
06/24/2022 19:33:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.216632 on epoch=259
06/24/2022 19:33:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.165672 on epoch=264
06/24/2022 19:33:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.287084 on epoch=269
06/24/2022 19:33:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.182542 on epoch=274
06/24/2022 19:33:22 - INFO - __main__ - Global step 550 Train loss 0.228259 Classification-F1 0.46843853820598 on epoch=274
06/24/2022 19:33:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.203007 on epoch=279
06/24/2022 19:33:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.206908 on epoch=284
06/24/2022 19:33:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.137098 on epoch=289
06/24/2022 19:33:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.395856 on epoch=294
06/24/2022 19:33:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.158605 on epoch=299
06/24/2022 19:33:34 - INFO - __main__ - Global step 600 Train loss 0.220295 Classification-F1 0.5151515151515151 on epoch=299
06/24/2022 19:33:35 - INFO - __main__ - save last model!
06/24/2022 19:33:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:33:35 - INFO - __main__ - Printing 3 examples
06/24/2022 19:33:35 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:33:35 - INFO - __main__ - ['1']
06/24/2022 19:33:35 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:33:35 - INFO - __main__ - ['1']
06/24/2022 19:33:35 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:33:35 - INFO - __main__ - ['1']
06/24/2022 19:33:35 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:33:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:33:35 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:33:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:33:35 - INFO - __main__ - Printing 3 examples
06/24/2022 19:33:35 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:33:35 - INFO - __main__ - ['1']
06/24/2022 19:33:35 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:33:35 - INFO - __main__ - ['1']
06/24/2022 19:33:35 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:33:35 - INFO - __main__ - ['1']
06/24/2022 19:33:35 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:33:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:33:35 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:33:37 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:33:38 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:33:38 - INFO - __main__ - Printing 3 examples
06/24/2022 19:33:38 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:33:38 - INFO - __main__ - ['0']
06/24/2022 19:33:38 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:33:38 - INFO - __main__ - ['1']
06/24/2022 19:33:38 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:33:38 - INFO - __main__ - ['1']
06/24/2022 19:33:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:33:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:33:39 - INFO - __main__ - Starting training!
06/24/2022 19:33:42 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:33:49 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:34:49 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_21_0.0003_8_predictions.txt
06/24/2022 19:34:49 - INFO - __main__ - Classification-F1 on test data: 0.4710
06/24/2022 19:34:50 - INFO - __main__ - prefix=paws_16_21, lr=0.0003, bsz=8, dev_performance=0.5151515151515151, test_performance=0.4709934995299648
06/24/2022 19:34:50 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.0002, bsz=8 ...
06/24/2022 19:34:51 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:34:51 - INFO - __main__ - Printing 3 examples
06/24/2022 19:34:51 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:34:51 - INFO - __main__ - ['1']
06/24/2022 19:34:51 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:34:51 - INFO - __main__ - ['1']
06/24/2022 19:34:51 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:34:51 - INFO - __main__ - ['1']
06/24/2022 19:34:51 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:34:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:34:51 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:34:51 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:34:51 - INFO - __main__ - Printing 3 examples
06/24/2022 19:34:51 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:34:51 - INFO - __main__ - ['1']
06/24/2022 19:34:51 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:34:51 - INFO - __main__ - ['1']
06/24/2022 19:34:51 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:34:51 - INFO - __main__ - ['1']
06/24/2022 19:34:51 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:34:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:34:51 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:34:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:34:55 - INFO - __main__ - Starting training!
06/24/2022 19:34:57 - INFO - __main__ - Step 10 Global step 10 Train loss 18.620905 on epoch=4
06/24/2022 19:34:59 - INFO - __main__ - Step 20 Global step 20 Train loss 15.388863 on epoch=9
06/24/2022 19:35:01 - INFO - __main__ - Step 30 Global step 30 Train loss 11.354353 on epoch=14
06/24/2022 19:35:04 - INFO - __main__ - Step 40 Global step 40 Train loss 8.287121 on epoch=19
06/24/2022 19:35:06 - INFO - __main__ - Step 50 Global step 50 Train loss 7.021180 on epoch=24
06/24/2022 19:35:06 - INFO - __main__ - Global step 50 Train loss 12.134485 Classification-F1 0.14473684210526316 on epoch=24
06/24/2022 19:35:09 - INFO - __main__ - Step 60 Global step 60 Train loss 6.042306 on epoch=29
06/24/2022 19:35:12 - INFO - __main__ - Step 70 Global step 70 Train loss 4.367318 on epoch=34
06/24/2022 19:35:14 - INFO - __main__ - Step 80 Global step 80 Train loss 4.167357 on epoch=39
06/24/2022 19:35:17 - INFO - __main__ - Step 90 Global step 90 Train loss 3.071975 on epoch=44
06/24/2022 19:35:19 - INFO - __main__ - Step 100 Global step 100 Train loss 3.188432 on epoch=49
06/24/2022 19:35:19 - INFO - __main__ - Global step 100 Train loss 4.167478 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:35:22 - INFO - __main__ - Step 110 Global step 110 Train loss 3.193263 on epoch=54
06/24/2022 19:35:24 - INFO - __main__ - Step 120 Global step 120 Train loss 2.685688 on epoch=59
06/24/2022 19:35:27 - INFO - __main__ - Step 130 Global step 130 Train loss 2.502991 on epoch=64
06/24/2022 19:35:29 - INFO - __main__ - Step 140 Global step 140 Train loss 2.817993 on epoch=69
06/24/2022 19:35:32 - INFO - __main__ - Step 150 Global step 150 Train loss 1.915219 on epoch=74
06/24/2022 19:35:32 - INFO - __main__ - Global step 150 Train loss 2.623031 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:35:34 - INFO - __main__ - Step 160 Global step 160 Train loss 1.651995 on epoch=79
06/24/2022 19:35:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.776486 on epoch=84
06/24/2022 19:35:39 - INFO - __main__ - Step 180 Global step 180 Train loss 1.548394 on epoch=89
06/24/2022 19:35:42 - INFO - __main__ - Step 190 Global step 190 Train loss 1.355190 on epoch=94
06/24/2022 19:35:44 - INFO - __main__ - Step 200 Global step 200 Train loss 1.595068 on epoch=99
06/24/2022 19:35:44 - INFO - __main__ - Global step 200 Train loss 1.585427 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:35:47 - INFO - __main__ - Step 210 Global step 210 Train loss 1.497940 on epoch=104
06/24/2022 19:35:49 - INFO - __main__ - Step 220 Global step 220 Train loss 1.179557 on epoch=109
06/24/2022 19:35:52 - INFO - __main__ - Step 230 Global step 230 Train loss 1.084847 on epoch=114
06/24/2022 19:35:54 - INFO - __main__ - Step 240 Global step 240 Train loss 1.366999 on epoch=119
06/24/2022 19:35:57 - INFO - __main__ - Step 250 Global step 250 Train loss 1.620858 on epoch=124
06/24/2022 19:35:57 - INFO - __main__ - Global step 250 Train loss 1.350040 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:35:59 - INFO - __main__ - Step 260 Global step 260 Train loss 1.144199 on epoch=129
06/24/2022 19:36:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.837045 on epoch=134
06/24/2022 19:36:04 - INFO - __main__ - Step 280 Global step 280 Train loss 1.231829 on epoch=139
06/24/2022 19:36:06 - INFO - __main__ - Step 290 Global step 290 Train loss 1.025651 on epoch=144
06/24/2022 19:36:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.754085 on epoch=149
06/24/2022 19:36:09 - INFO - __main__ - Global step 300 Train loss 0.998562 Classification-F1 0.3191489361702127 on epoch=149
06/24/2022 19:36:12 - INFO - __main__ - Step 310 Global step 310 Train loss 1.056789 on epoch=154
06/24/2022 19:36:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.693726 on epoch=159
06/24/2022 19:36:16 - INFO - __main__ - Step 330 Global step 330 Train loss 1.089097 on epoch=164
06/24/2022 19:36:19 - INFO - __main__ - Step 340 Global step 340 Train loss 1.279687 on epoch=169
06/24/2022 19:36:21 - INFO - __main__ - Step 350 Global step 350 Train loss 1.306084 on epoch=174
06/24/2022 19:36:22 - INFO - __main__ - Global step 350 Train loss 1.085077 Classification-F1 0.41700404858299595 on epoch=174
06/24/2022 19:36:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.738485 on epoch=179
06/24/2022 19:36:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.746167 on epoch=184
06/24/2022 19:36:29 - INFO - __main__ - Step 380 Global step 380 Train loss 1.103486 on epoch=189
06/24/2022 19:36:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.677232 on epoch=194
06/24/2022 19:36:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.764150 on epoch=199
06/24/2022 19:36:34 - INFO - __main__ - Global step 400 Train loss 0.805904 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 19:36:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.969161 on epoch=204
06/24/2022 19:36:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.986785 on epoch=209
06/24/2022 19:36:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.733802 on epoch=214
06/24/2022 19:36:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.627604 on epoch=219
06/24/2022 19:36:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.592623 on epoch=224
06/24/2022 19:36:47 - INFO - __main__ - Global step 450 Train loss 0.781995 Classification-F1 0.28744939271255066 on epoch=224
06/24/2022 19:36:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.791320 on epoch=229
06/24/2022 19:36:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.627774 on epoch=234
06/24/2022 19:36:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.645169 on epoch=239
06/24/2022 19:36:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.605172 on epoch=244
06/24/2022 19:36:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.783778 on epoch=249
06/24/2022 19:36:59 - INFO - __main__ - Global step 500 Train loss 0.690643 Classification-F1 0.46843853820598 on epoch=249
06/24/2022 19:37:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.517364 on epoch=254
06/24/2022 19:37:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.471180 on epoch=259
06/24/2022 19:37:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.710557 on epoch=264
06/24/2022 19:37:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.557061 on epoch=269
06/24/2022 19:37:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.398903 on epoch=274
06/24/2022 19:37:12 - INFO - __main__ - Global step 550 Train loss 0.531013 Classification-F1 0.5270935960591133 on epoch=274
06/24/2022 19:37:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.864571 on epoch=279
06/24/2022 19:37:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.794682 on epoch=284
06/24/2022 19:37:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.528410 on epoch=289
06/24/2022 19:37:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.439317 on epoch=294
06/24/2022 19:37:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.515220 on epoch=299
06/24/2022 19:37:25 - INFO - __main__ - Global step 600 Train loss 0.628440 Classification-F1 0.33793103448275863 on epoch=299
06/24/2022 19:37:25 - INFO - __main__ - save last model!
06/24/2022 19:37:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:37:26 - INFO - __main__ - Printing 3 examples
06/24/2022 19:37:26 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:37:26 - INFO - __main__ - ['1']
06/24/2022 19:37:26 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:37:26 - INFO - __main__ - ['1']
06/24/2022 19:37:26 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:37:26 - INFO - __main__ - ['1']
06/24/2022 19:37:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:37:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:37:26 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:37:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:37:26 - INFO - __main__ - Printing 3 examples
06/24/2022 19:37:26 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:37:26 - INFO - __main__ - ['1']
06/24/2022 19:37:26 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:37:26 - INFO - __main__ - ['1']
06/24/2022 19:37:26 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:37:26 - INFO - __main__ - ['1']
06/24/2022 19:37:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:37:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:37:26 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:37:28 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:37:28 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:37:28 - INFO - __main__ - Printing 3 examples
06/24/2022 19:37:28 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:37:28 - INFO - __main__ - ['0']
06/24/2022 19:37:28 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:37:28 - INFO - __main__ - ['1']
06/24/2022 19:37:28 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:37:28 - INFO - __main__ - ['1']
06/24/2022 19:37:28 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:37:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:37:30 - INFO - __main__ - Starting training!
06/24/2022 19:37:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:37:40 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:38:39 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_21_0.0002_8_predictions.txt
06/24/2022 19:38:39 - INFO - __main__ - Classification-F1 on test data: 0.4797
06/24/2022 19:38:39 - INFO - __main__ - prefix=paws_16_21, lr=0.0002, bsz=8, dev_performance=0.5270935960591133, test_performance=0.4797438873306056
06/24/2022 19:38:39 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.0001, bsz=8 ...
06/24/2022 19:38:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:38:40 - INFO - __main__ - Printing 3 examples
06/24/2022 19:38:40 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:38:40 - INFO - __main__ - ['1']
06/24/2022 19:38:40 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:38:40 - INFO - __main__ - ['1']
06/24/2022 19:38:40 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:38:40 - INFO - __main__ - ['1']
06/24/2022 19:38:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:38:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:38:40 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:38:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:38:40 - INFO - __main__ - Printing 3 examples
06/24/2022 19:38:40 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:38:40 - INFO - __main__ - ['1']
06/24/2022 19:38:40 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:38:40 - INFO - __main__ - ['1']
06/24/2022 19:38:40 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:38:40 - INFO - __main__ - ['1']
06/24/2022 19:38:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:38:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:38:40 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:38:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:38:44 - INFO - __main__ - Starting training!
06/24/2022 19:38:46 - INFO - __main__ - Step 10 Global step 10 Train loss 18.692295 on epoch=4
06/24/2022 19:38:49 - INFO - __main__ - Step 20 Global step 20 Train loss 17.613937 on epoch=9
06/24/2022 19:38:51 - INFO - __main__ - Step 30 Global step 30 Train loss 13.956186 on epoch=14
06/24/2022 19:38:54 - INFO - __main__ - Step 40 Global step 40 Train loss 10.920215 on epoch=19
06/24/2022 19:38:56 - INFO - __main__ - Step 50 Global step 50 Train loss 10.642973 on epoch=24
06/24/2022 19:38:58 - INFO - __main__ - Global step 50 Train loss 14.365120 Classification-F1 0.016 on epoch=24
06/24/2022 19:39:00 - INFO - __main__ - Step 60 Global step 60 Train loss 8.959156 on epoch=29
06/24/2022 19:39:03 - INFO - __main__ - Step 70 Global step 70 Train loss 9.069086 on epoch=34
06/24/2022 19:39:05 - INFO - __main__ - Step 80 Global step 80 Train loss 7.757186 on epoch=39
06/24/2022 19:39:08 - INFO - __main__ - Step 90 Global step 90 Train loss 7.111947 on epoch=44
06/24/2022 19:39:11 - INFO - __main__ - Step 100 Global step 100 Train loss 5.805356 on epoch=49
06/24/2022 19:39:11 - INFO - __main__ - Global step 100 Train loss 7.740546 Classification-F1 0.10483351235230934 on epoch=49
06/24/2022 19:39:14 - INFO - __main__ - Step 110 Global step 110 Train loss 6.124685 on epoch=54
06/24/2022 19:39:17 - INFO - __main__ - Step 120 Global step 120 Train loss 4.523122 on epoch=59
06/24/2022 19:39:19 - INFO - __main__ - Step 130 Global step 130 Train loss 4.307140 on epoch=64
06/24/2022 19:39:22 - INFO - __main__ - Step 140 Global step 140 Train loss 4.911710 on epoch=69
06/24/2022 19:39:24 - INFO - __main__ - Step 150 Global step 150 Train loss 4.108825 on epoch=74
06/24/2022 19:39:25 - INFO - __main__ - Global step 150 Train loss 4.795096 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:39:28 - INFO - __main__ - Step 160 Global step 160 Train loss 4.292697 on epoch=79
06/24/2022 19:39:30 - INFO - __main__ - Step 170 Global step 170 Train loss 3.957476 on epoch=84
06/24/2022 19:39:33 - INFO - __main__ - Step 180 Global step 180 Train loss 3.585941 on epoch=89
06/24/2022 19:39:35 - INFO - __main__ - Step 190 Global step 190 Train loss 3.197808 on epoch=94
06/24/2022 19:39:38 - INFO - __main__ - Step 200 Global step 200 Train loss 3.032729 on epoch=99
06/24/2022 19:39:38 - INFO - __main__ - Global step 200 Train loss 3.613330 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:39:41 - INFO - __main__ - Step 210 Global step 210 Train loss 2.381713 on epoch=104
06/24/2022 19:39:43 - INFO - __main__ - Step 220 Global step 220 Train loss 1.834528 on epoch=109
06/24/2022 19:39:46 - INFO - __main__ - Step 230 Global step 230 Train loss 3.314250 on epoch=114
06/24/2022 19:39:48 - INFO - __main__ - Step 240 Global step 240 Train loss 2.644478 on epoch=119
06/24/2022 19:39:51 - INFO - __main__ - Step 250 Global step 250 Train loss 1.998127 on epoch=124
06/24/2022 19:39:51 - INFO - __main__ - Global step 250 Train loss 2.434619 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:39:54 - INFO - __main__ - Step 260 Global step 260 Train loss 2.074090 on epoch=129
06/24/2022 19:39:57 - INFO - __main__ - Step 270 Global step 270 Train loss 2.367315 on epoch=134
06/24/2022 19:39:59 - INFO - __main__ - Step 280 Global step 280 Train loss 1.677291 on epoch=139
06/24/2022 19:40:02 - INFO - __main__ - Step 290 Global step 290 Train loss 2.105669 on epoch=144
06/24/2022 19:40:04 - INFO - __main__ - Step 300 Global step 300 Train loss 1.643373 on epoch=149
06/24/2022 19:40:05 - INFO - __main__ - Global step 300 Train loss 1.973548 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 19:40:07 - INFO - __main__ - Step 310 Global step 310 Train loss 1.448235 on epoch=154
06/24/2022 19:40:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.956200 on epoch=159
06/24/2022 19:40:13 - INFO - __main__ - Step 330 Global step 330 Train loss 1.807747 on epoch=164
06/24/2022 19:40:15 - INFO - __main__ - Step 340 Global step 340 Train loss 1.287284 on epoch=169
06/24/2022 19:40:18 - INFO - __main__ - Step 350 Global step 350 Train loss 1.854182 on epoch=174
06/24/2022 19:40:18 - INFO - __main__ - Global step 350 Train loss 1.470730 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 19:40:21 - INFO - __main__ - Step 360 Global step 360 Train loss 1.528572 on epoch=179
06/24/2022 19:40:23 - INFO - __main__ - Step 370 Global step 370 Train loss 1.758467 on epoch=184
06/24/2022 19:40:26 - INFO - __main__ - Step 380 Global step 380 Train loss 1.566288 on epoch=189
06/24/2022 19:40:28 - INFO - __main__ - Step 390 Global step 390 Train loss 1.439713 on epoch=194
06/24/2022 19:40:31 - INFO - __main__ - Step 400 Global step 400 Train loss 1.423964 on epoch=199
06/24/2022 19:40:31 - INFO - __main__ - Global step 400 Train loss 1.543401 Classification-F1 0.3816425120772947 on epoch=199
06/24/2022 19:40:34 - INFO - __main__ - Step 410 Global step 410 Train loss 1.410772 on epoch=204
06/24/2022 19:40:37 - INFO - __main__ - Step 420 Global step 420 Train loss 1.073613 on epoch=209
06/24/2022 19:40:39 - INFO - __main__ - Step 430 Global step 430 Train loss 1.177377 on epoch=214
06/24/2022 19:40:42 - INFO - __main__ - Step 440 Global step 440 Train loss 1.739299 on epoch=219
06/24/2022 19:40:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.988757 on epoch=224
06/24/2022 19:40:45 - INFO - __main__ - Global step 450 Train loss 1.277964 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 19:40:47 - INFO - __main__ - Step 460 Global step 460 Train loss 1.366991 on epoch=229
06/24/2022 19:40:50 - INFO - __main__ - Step 470 Global step 470 Train loss 1.064601 on epoch=234
06/24/2022 19:40:52 - INFO - __main__ - Step 480 Global step 480 Train loss 1.231667 on epoch=239
06/24/2022 19:40:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.971896 on epoch=244
06/24/2022 19:40:57 - INFO - __main__ - Step 500 Global step 500 Train loss 1.451910 on epoch=249
06/24/2022 19:40:58 - INFO - __main__ - Global step 500 Train loss 1.217413 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 19:41:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.904204 on epoch=254
06/24/2022 19:41:03 - INFO - __main__ - Step 520 Global step 520 Train loss 1.475807 on epoch=259
06/24/2022 19:41:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.739236 on epoch=264
06/24/2022 19:41:08 - INFO - __main__ - Step 540 Global step 540 Train loss 1.002265 on epoch=269
06/24/2022 19:41:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.975372 on epoch=274
06/24/2022 19:41:11 - INFO - __main__ - Global step 550 Train loss 1.019377 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 19:41:13 - INFO - __main__ - Step 560 Global step 560 Train loss 1.164303 on epoch=279
06/24/2022 19:41:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.607590 on epoch=284
06/24/2022 19:41:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.988737 on epoch=289
06/24/2022 19:41:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.847035 on epoch=294
06/24/2022 19:41:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.996812 on epoch=299
06/24/2022 19:41:24 - INFO - __main__ - Global step 600 Train loss 0.920895 Classification-F1 0.3333333333333333 on epoch=299
06/24/2022 19:41:24 - INFO - __main__ - save last model!
06/24/2022 19:41:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:41:24 - INFO - __main__ - Printing 3 examples
06/24/2022 19:41:24 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:41:24 - INFO - __main__ - ['1']
06/24/2022 19:41:24 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:41:24 - INFO - __main__ - ['1']
06/24/2022 19:41:24 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:41:24 - INFO - __main__ - ['1']
06/24/2022 19:41:24 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:41:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:41:25 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:41:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:41:25 - INFO - __main__ - Printing 3 examples
06/24/2022 19:41:25 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:41:25 - INFO - __main__ - ['1']
06/24/2022 19:41:25 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:41:25 - INFO - __main__ - ['1']
06/24/2022 19:41:25 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:41:25 - INFO - __main__ - ['1']
06/24/2022 19:41:25 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:41:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:41:25 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:41:26 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:41:27 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:41:27 - INFO - __main__ - Printing 3 examples
06/24/2022 19:41:27 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:41:27 - INFO - __main__ - ['0']
06/24/2022 19:41:27 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:41:27 - INFO - __main__ - ['1']
06/24/2022 19:41:27 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:41:27 - INFO - __main__ - ['1']
06/24/2022 19:41:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:41:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:41:29 - INFO - __main__ - Starting training!
06/24/2022 19:41:31 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:41:38 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:42:36 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_21_0.0001_8_predictions.txt
06/24/2022 19:42:36 - INFO - __main__ - Classification-F1 on test data: 0.3440
06/24/2022 19:42:37 - INFO - __main__ - prefix=paws_16_21, lr=0.0001, bsz=8, dev_performance=0.3816425120772947, test_performance=0.34403874656089367
06/24/2022 19:42:37 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.0005, bsz=8 ...
06/24/2022 19:42:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:42:38 - INFO - __main__ - Printing 3 examples
06/24/2022 19:42:38 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:42:38 - INFO - __main__ - ['1']
06/24/2022 19:42:38 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:42:38 - INFO - __main__ - ['1']
06/24/2022 19:42:38 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:42:38 - INFO - __main__ - ['1']
06/24/2022 19:42:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:42:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:42:38 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:42:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:42:38 - INFO - __main__ - Printing 3 examples
06/24/2022 19:42:38 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:42:38 - INFO - __main__ - ['1']
06/24/2022 19:42:38 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:42:38 - INFO - __main__ - ['1']
06/24/2022 19:42:38 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:42:38 - INFO - __main__ - ['1']
06/24/2022 19:42:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:42:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:42:38 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:42:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:42:41 - INFO - __main__ - Starting training!
06/24/2022 19:42:44 - INFO - __main__ - Step 10 Global step 10 Train loss 18.415697 on epoch=4
06/24/2022 19:42:46 - INFO - __main__ - Step 20 Global step 20 Train loss 12.560564 on epoch=9
06/24/2022 19:42:48 - INFO - __main__ - Step 30 Global step 30 Train loss 8.240090 on epoch=14
06/24/2022 19:42:51 - INFO - __main__ - Step 40 Global step 40 Train loss 6.562280 on epoch=19
06/24/2022 19:42:53 - INFO - __main__ - Step 50 Global step 50 Train loss 4.344825 on epoch=24
06/24/2022 19:42:54 - INFO - __main__ - Global step 50 Train loss 10.024692 Classification-F1 0.3191489361702127 on epoch=24
06/24/2022 19:42:56 - INFO - __main__ - Step 60 Global step 60 Train loss 2.394375 on epoch=29
06/24/2022 19:42:59 - INFO - __main__ - Step 70 Global step 70 Train loss 2.610148 on epoch=34
06/24/2022 19:43:02 - INFO - __main__ - Step 80 Global step 80 Train loss 2.104713 on epoch=39
06/24/2022 19:43:04 - INFO - __main__ - Step 90 Global step 90 Train loss 2.002684 on epoch=44
06/24/2022 19:43:07 - INFO - __main__ - Step 100 Global step 100 Train loss 1.921515 on epoch=49
06/24/2022 19:43:07 - INFO - __main__ - Global step 100 Train loss 2.206687 Classification-F1 0.3191489361702127 on epoch=49
06/24/2022 19:43:09 - INFO - __main__ - Step 110 Global step 110 Train loss 1.945712 on epoch=54
06/24/2022 19:43:12 - INFO - __main__ - Step 120 Global step 120 Train loss 1.616817 on epoch=59
06/24/2022 19:43:14 - INFO - __main__ - Step 130 Global step 130 Train loss 1.389851 on epoch=64
06/24/2022 19:43:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.907677 on epoch=69
06/24/2022 19:43:20 - INFO - __main__ - Step 150 Global step 150 Train loss 1.937530 on epoch=74
06/24/2022 19:43:20 - INFO - __main__ - Global step 150 Train loss 1.559518 Classification-F1 0.4385964912280702 on epoch=74
06/24/2022 19:43:23 - INFO - __main__ - Step 160 Global step 160 Train loss 1.491960 on epoch=79
06/24/2022 19:43:26 - INFO - __main__ - Step 170 Global step 170 Train loss 1.197883 on epoch=84
06/24/2022 19:43:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.986143 on epoch=89
06/24/2022 19:43:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.730025 on epoch=94
06/24/2022 19:43:33 - INFO - __main__ - Step 200 Global step 200 Train loss 1.078399 on epoch=99
06/24/2022 19:43:33 - INFO - __main__ - Global step 200 Train loss 1.096882 Classification-F1 0.5733333333333335 on epoch=99
06/24/2022 19:43:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.727629 on epoch=104
06/24/2022 19:43:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.554098 on epoch=109
06/24/2022 19:43:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.715226 on epoch=114
06/24/2022 19:43:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.842000 on epoch=119
06/24/2022 19:43:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.799902 on epoch=124
06/24/2022 19:43:47 - INFO - __main__ - Global step 250 Train loss 0.727771 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:43:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.658164 on epoch=129
06/24/2022 19:43:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.868900 on epoch=134
06/24/2022 19:43:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.767971 on epoch=139
06/24/2022 19:43:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.845734 on epoch=144
06/24/2022 19:44:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.947716 on epoch=149
06/24/2022 19:44:00 - INFO - __main__ - Global step 300 Train loss 0.817697 Classification-F1 0.3043478260869565 on epoch=149
06/24/2022 19:44:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.703301 on epoch=154
06/24/2022 19:44:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.897266 on epoch=159
06/24/2022 19:44:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.698976 on epoch=164
06/24/2022 19:44:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.432472 on epoch=169
06/24/2022 19:44:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.812874 on epoch=174
06/24/2022 19:44:13 - INFO - __main__ - Global step 350 Train loss 0.708978 Classification-F1 0.39999999999999997 on epoch=174
06/24/2022 19:44:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.639367 on epoch=179
06/24/2022 19:44:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.531936 on epoch=184
06/24/2022 19:44:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.686203 on epoch=189
06/24/2022 19:44:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.937829 on epoch=194
06/24/2022 19:44:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.706576 on epoch=199
06/24/2022 19:44:26 - INFO - __main__ - Global step 400 Train loss 0.700382 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 19:44:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.594252 on epoch=204
06/24/2022 19:44:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.667805 on epoch=209
06/24/2022 19:44:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.754670 on epoch=214
06/24/2022 19:44:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.560917 on epoch=219
06/24/2022 19:44:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.411428 on epoch=224
06/24/2022 19:44:39 - INFO - __main__ - Global step 450 Train loss 0.597814 Classification-F1 0.4666666666666667 on epoch=224
06/24/2022 19:44:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.631295 on epoch=229
06/24/2022 19:44:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.409600 on epoch=234
06/24/2022 19:44:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.588607 on epoch=239
06/24/2022 19:44:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.438521 on epoch=244
06/24/2022 19:44:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.437250 on epoch=249
06/24/2022 19:44:52 - INFO - __main__ - Global step 500 Train loss 0.501055 Classification-F1 0.4817813765182186 on epoch=249
06/24/2022 19:44:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.564293 on epoch=254
06/24/2022 19:44:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.428594 on epoch=259
06/24/2022 19:45:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.501571 on epoch=264
06/24/2022 19:45:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.441739 on epoch=269
06/24/2022 19:45:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.434799 on epoch=274
06/24/2022 19:45:05 - INFO - __main__ - Global step 550 Train loss 0.474199 Classification-F1 0.39999999999999997 on epoch=274
06/24/2022 19:45:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.374310 on epoch=279
06/24/2022 19:45:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.349503 on epoch=284
06/24/2022 19:45:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.423204 on epoch=289
06/24/2022 19:45:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.387028 on epoch=294
06/24/2022 19:45:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.300222 on epoch=299
06/24/2022 19:45:18 - INFO - __main__ - Global step 600 Train loss 0.366853 Classification-F1 0.4682306940371457 on epoch=299
06/24/2022 19:45:18 - INFO - __main__ - save last model!
06/24/2022 19:45:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:45:19 - INFO - __main__ - Printing 3 examples
06/24/2022 19:45:19 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:45:19 - INFO - __main__ - ['1']
06/24/2022 19:45:19 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:45:19 - INFO - __main__ - ['1']
06/24/2022 19:45:19 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:45:19 - INFO - __main__ - ['1']
06/24/2022 19:45:19 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:45:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:45:19 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:45:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:45:19 - INFO - __main__ - Printing 3 examples
06/24/2022 19:45:19 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:45:19 - INFO - __main__ - ['1']
06/24/2022 19:45:19 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:45:19 - INFO - __main__ - ['1']
06/24/2022 19:45:19 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:45:19 - INFO - __main__ - ['1']
06/24/2022 19:45:19 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:45:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:45:20 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:45:21 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:45:21 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:45:21 - INFO - __main__ - Printing 3 examples
06/24/2022 19:45:21 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:45:21 - INFO - __main__ - ['0']
06/24/2022 19:45:21 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:45:21 - INFO - __main__ - ['1']
06/24/2022 19:45:21 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:45:21 - INFO - __main__ - ['1']
06/24/2022 19:45:21 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:45:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:45:24 - INFO - __main__ - Starting training!
06/24/2022 19:45:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:45:33 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:46:32 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_42_0.0005_8_predictions.txt
06/24/2022 19:46:32 - INFO - __main__ - Classification-F1 on test data: 0.4748
06/24/2022 19:46:32 - INFO - __main__ - prefix=paws_16_42, lr=0.0005, bsz=8, dev_performance=0.5733333333333335, test_performance=0.47480251691372055
06/24/2022 19:46:32 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.0003, bsz=8 ...
06/24/2022 19:46:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:46:33 - INFO - __main__ - Printing 3 examples
06/24/2022 19:46:33 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:46:33 - INFO - __main__ - ['1']
06/24/2022 19:46:33 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:46:33 - INFO - __main__ - ['1']
06/24/2022 19:46:33 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:46:33 - INFO - __main__ - ['1']
06/24/2022 19:46:33 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:46:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:46:33 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:46:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:46:33 - INFO - __main__ - Printing 3 examples
06/24/2022 19:46:33 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:46:33 - INFO - __main__ - ['1']
06/24/2022 19:46:33 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:46:33 - INFO - __main__ - ['1']
06/24/2022 19:46:33 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:46:33 - INFO - __main__ - ['1']
06/24/2022 19:46:33 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:46:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:46:33 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:46:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:46:37 - INFO - __main__ - Starting training!
06/24/2022 19:46:39 - INFO - __main__ - Step 10 Global step 10 Train loss 18.146641 on epoch=4
06/24/2022 19:46:41 - INFO - __main__ - Step 20 Global step 20 Train loss 17.455408 on epoch=9
06/24/2022 19:46:43 - INFO - __main__ - Step 30 Global step 30 Train loss 10.627536 on epoch=14
06/24/2022 19:46:46 - INFO - __main__ - Step 40 Global step 40 Train loss 7.425112 on epoch=19
06/24/2022 19:46:48 - INFO - __main__ - Step 50 Global step 50 Train loss 4.683543 on epoch=24
06/24/2022 19:46:48 - INFO - __main__ - Global step 50 Train loss 11.667647 Classification-F1 0.14695121951219514 on epoch=24
06/24/2022 19:46:51 - INFO - __main__ - Step 60 Global step 60 Train loss 3.192115 on epoch=29
06/24/2022 19:46:53 - INFO - __main__ - Step 70 Global step 70 Train loss 3.082392 on epoch=34
06/24/2022 19:46:56 - INFO - __main__ - Step 80 Global step 80 Train loss 2.919779 on epoch=39
06/24/2022 19:46:58 - INFO - __main__ - Step 90 Global step 90 Train loss 2.222760 on epoch=44
06/24/2022 19:47:01 - INFO - __main__ - Step 100 Global step 100 Train loss 1.996280 on epoch=49
06/24/2022 19:47:01 - INFO - __main__ - Global step 100 Train loss 2.682665 Classification-F1 0.3191489361702127 on epoch=49
06/24/2022 19:47:04 - INFO - __main__ - Step 110 Global step 110 Train loss 1.307583 on epoch=54
06/24/2022 19:47:07 - INFO - __main__ - Step 120 Global step 120 Train loss 1.796091 on epoch=59
06/24/2022 19:47:09 - INFO - __main__ - Step 130 Global step 130 Train loss 1.526505 on epoch=64
06/24/2022 19:47:11 - INFO - __main__ - Step 140 Global step 140 Train loss 1.177732 on epoch=69
06/24/2022 19:47:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.759973 on epoch=74
06/24/2022 19:47:14 - INFO - __main__ - Global step 150 Train loss 1.313577 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:47:17 - INFO - __main__ - Step 160 Global step 160 Train loss 1.399680 on epoch=79
06/24/2022 19:47:19 - INFO - __main__ - Step 170 Global step 170 Train loss 1.140395 on epoch=84
06/24/2022 19:47:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.787084 on epoch=89
06/24/2022 19:47:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.793913 on epoch=94
06/24/2022 19:47:27 - INFO - __main__ - Step 200 Global step 200 Train loss 1.236342 on epoch=99
06/24/2022 19:47:27 - INFO - __main__ - Global step 200 Train loss 1.071483 Classification-F1 0.3125 on epoch=99
06/24/2022 19:47:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.785909 on epoch=104
06/24/2022 19:47:32 - INFO - __main__ - Step 220 Global step 220 Train loss 1.114986 on epoch=109
06/24/2022 19:47:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.645546 on epoch=114
06/24/2022 19:47:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.894590 on epoch=119
06/24/2022 19:47:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.793587 on epoch=124
06/24/2022 19:47:40 - INFO - __main__ - Global step 250 Train loss 0.846924 Classification-F1 0.3764102564102564 on epoch=124
06/24/2022 19:47:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.460832 on epoch=129
06/24/2022 19:47:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.501075 on epoch=134
06/24/2022 19:47:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.597297 on epoch=139
06/24/2022 19:47:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.432567 on epoch=144
06/24/2022 19:47:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.323675 on epoch=149
06/24/2022 19:47:53 - INFO - __main__ - Global step 300 Train loss 0.463089 Classification-F1 0.4817813765182186 on epoch=149
06/24/2022 19:47:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.391158 on epoch=154
06/24/2022 19:47:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.342239 on epoch=159
06/24/2022 19:48:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.348064 on epoch=164
06/24/2022 19:48:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.176082 on epoch=169
06/24/2022 19:48:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.196226 on epoch=174
06/24/2022 19:48:05 - INFO - __main__ - Global step 350 Train loss 0.290754 Classification-F1 0.4554554554554554 on epoch=174
06/24/2022 19:48:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.248122 on epoch=179
06/24/2022 19:48:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.214113 on epoch=184
06/24/2022 19:48:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.090292 on epoch=189
06/24/2022 19:48:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.054683 on epoch=194
06/24/2022 19:48:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.102190 on epoch=199
06/24/2022 19:48:18 - INFO - __main__ - Global step 400 Train loss 0.141880 Classification-F1 0.4554554554554554 on epoch=199
06/24/2022 19:48:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.112474 on epoch=204
06/24/2022 19:48:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.194737 on epoch=209
06/24/2022 19:48:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.089241 on epoch=214
06/24/2022 19:48:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.014979 on epoch=219
06/24/2022 19:48:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.013099 on epoch=224
06/24/2022 19:48:30 - INFO - __main__ - Global step 450 Train loss 0.084906 Classification-F1 0.4817813765182186 on epoch=224
06/24/2022 19:48:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.205569 on epoch=229
06/24/2022 19:48:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.031146 on epoch=234
06/24/2022 19:48:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.024180 on epoch=239
06/24/2022 19:48:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.022113 on epoch=244
06/24/2022 19:48:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.112653 on epoch=249
06/24/2022 19:48:43 - INFO - __main__ - Global step 500 Train loss 0.079132 Classification-F1 0.40566959921798634 on epoch=249
06/24/2022 19:48:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.113925 on epoch=254
06/24/2022 19:48:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000163 on epoch=259
06/24/2022 19:48:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.121624 on epoch=264
06/24/2022 19:48:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.089117 on epoch=269
06/24/2022 19:48:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.256208 on epoch=274
06/24/2022 19:48:55 - INFO - __main__ - Global step 550 Train loss 0.116207 Classification-F1 0.4980392156862745 on epoch=274
06/24/2022 19:48:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.023881 on epoch=279
06/24/2022 19:49:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.043260 on epoch=284
06/24/2022 19:49:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.005349 on epoch=289
06/24/2022 19:49:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.051746 on epoch=294
06/24/2022 19:49:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.032114 on epoch=299
06/24/2022 19:49:08 - INFO - __main__ - Global step 600 Train loss 0.031270 Classification-F1 0.4009852216748768 on epoch=299
06/24/2022 19:49:08 - INFO - __main__ - save last model!
06/24/2022 19:49:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:49:09 - INFO - __main__ - Printing 3 examples
06/24/2022 19:49:09 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:49:09 - INFO - __main__ - ['1']
06/24/2022 19:49:09 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:49:09 - INFO - __main__ - ['1']
06/24/2022 19:49:09 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:49:09 - INFO - __main__ - ['1']
06/24/2022 19:49:09 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:49:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:49:09 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:49:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:49:09 - INFO - __main__ - Printing 3 examples
06/24/2022 19:49:09 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:49:09 - INFO - __main__ - ['1']
06/24/2022 19:49:09 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:49:09 - INFO - __main__ - ['1']
06/24/2022 19:49:09 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:49:09 - INFO - __main__ - ['1']
06/24/2022 19:49:09 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:49:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:49:09 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:49:11 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:49:11 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:49:11 - INFO - __main__ - Printing 3 examples
06/24/2022 19:49:11 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:49:11 - INFO - __main__ - ['0']
06/24/2022 19:49:11 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:49:11 - INFO - __main__ - ['1']
06/24/2022 19:49:11 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:49:11 - INFO - __main__ - ['1']
06/24/2022 19:49:11 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:49:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:49:13 - INFO - __main__ - Starting training!
06/24/2022 19:49:15 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:49:23 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:50:22 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_42_0.0003_8_predictions.txt
06/24/2022 19:50:23 - INFO - __main__ - Classification-F1 on test data: 0.5130
06/24/2022 19:50:23 - INFO - __main__ - prefix=paws_16_42, lr=0.0003, bsz=8, dev_performance=0.4980392156862745, test_performance=0.5129683411450097
06/24/2022 19:50:23 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.0002, bsz=8 ...
06/24/2022 19:50:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:50:24 - INFO - __main__ - Printing 3 examples
06/24/2022 19:50:24 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:50:24 - INFO - __main__ - ['1']
06/24/2022 19:50:24 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:50:24 - INFO - __main__ - ['1']
06/24/2022 19:50:24 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:50:24 - INFO - __main__ - ['1']
06/24/2022 19:50:24 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:50:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:50:24 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:50:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:50:24 - INFO - __main__ - Printing 3 examples
06/24/2022 19:50:24 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:50:24 - INFO - __main__ - ['1']
06/24/2022 19:50:24 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:50:24 - INFO - __main__ - ['1']
06/24/2022 19:50:24 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:50:24 - INFO - __main__ - ['1']
06/24/2022 19:50:24 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:50:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:50:24 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:50:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:50:28 - INFO - __main__ - Starting training!
06/24/2022 19:50:30 - INFO - __main__ - Step 10 Global step 10 Train loss 17.851278 on epoch=4
06/24/2022 19:50:33 - INFO - __main__ - Step 20 Global step 20 Train loss 15.504946 on epoch=9
06/24/2022 19:50:35 - INFO - __main__ - Step 30 Global step 30 Train loss 11.111494 on epoch=14
06/24/2022 19:50:38 - INFO - __main__ - Step 40 Global step 40 Train loss 8.841539 on epoch=19
06/24/2022 19:50:40 - INFO - __main__ - Step 50 Global step 50 Train loss 7.882529 on epoch=24
06/24/2022 19:50:41 - INFO - __main__ - Global step 50 Train loss 12.238357 Classification-F1 0.08125 on epoch=24
06/24/2022 19:50:43 - INFO - __main__ - Step 60 Global step 60 Train loss 6.318281 on epoch=29
06/24/2022 19:50:46 - INFO - __main__ - Step 70 Global step 70 Train loss 6.618601 on epoch=34
06/24/2022 19:50:48 - INFO - __main__ - Step 80 Global step 80 Train loss 3.784927 on epoch=39
06/24/2022 19:50:51 - INFO - __main__ - Step 90 Global step 90 Train loss 3.061241 on epoch=44
06/24/2022 19:50:53 - INFO - __main__ - Step 100 Global step 100 Train loss 2.039476 on epoch=49
06/24/2022 19:50:54 - INFO - __main__ - Global step 100 Train loss 4.364505 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:50:57 - INFO - __main__ - Step 110 Global step 110 Train loss 2.844899 on epoch=54
06/24/2022 19:50:59 - INFO - __main__ - Step 120 Global step 120 Train loss 2.104368 on epoch=59
06/24/2022 19:51:02 - INFO - __main__ - Step 130 Global step 130 Train loss 2.287822 on epoch=64
06/24/2022 19:51:04 - INFO - __main__ - Step 140 Global step 140 Train loss 2.253255 on epoch=69
06/24/2022 19:51:07 - INFO - __main__ - Step 150 Global step 150 Train loss 1.880979 on epoch=74
06/24/2022 19:51:07 - INFO - __main__ - Global step 150 Train loss 2.274265 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:51:09 - INFO - __main__ - Step 160 Global step 160 Train loss 2.432750 on epoch=79
06/24/2022 19:51:12 - INFO - __main__ - Step 170 Global step 170 Train loss 1.427901 on epoch=84
06/24/2022 19:51:14 - INFO - __main__ - Step 180 Global step 180 Train loss 1.950796 on epoch=89
06/24/2022 19:51:17 - INFO - __main__ - Step 190 Global step 190 Train loss 1.460444 on epoch=94
06/24/2022 19:51:20 - INFO - __main__ - Step 200 Global step 200 Train loss 1.171338 on epoch=99
06/24/2022 19:51:20 - INFO - __main__ - Global step 200 Train loss 1.688646 Classification-F1 0.5307917888563051 on epoch=99
06/24/2022 19:51:23 - INFO - __main__ - Step 210 Global step 210 Train loss 1.096462 on epoch=104
06/24/2022 19:51:25 - INFO - __main__ - Step 220 Global step 220 Train loss 1.612038 on epoch=109
06/24/2022 19:51:28 - INFO - __main__ - Step 230 Global step 230 Train loss 1.053031 on epoch=114
06/24/2022 19:51:30 - INFO - __main__ - Step 240 Global step 240 Train loss 1.246426 on epoch=119
06/24/2022 19:51:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.782562 on epoch=124
06/24/2022 19:51:33 - INFO - __main__ - Global step 250 Train loss 1.158104 Classification-F1 0.3816425120772947 on epoch=124
06/24/2022 19:51:35 - INFO - __main__ - Step 260 Global step 260 Train loss 1.030382 on epoch=129
06/24/2022 19:51:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.954537 on epoch=134
06/24/2022 19:51:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.818372 on epoch=139
06/24/2022 19:51:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.962917 on epoch=144
06/24/2022 19:51:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.762413 on epoch=149
06/24/2022 19:51:46 - INFO - __main__ - Global step 300 Train loss 0.905724 Classification-F1 0.4285714285714286 on epoch=149
06/24/2022 19:51:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.899073 on epoch=154
06/24/2022 19:51:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.708256 on epoch=159
06/24/2022 19:51:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.734953 on epoch=164
06/24/2022 19:51:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.870285 on epoch=169
06/24/2022 19:51:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.523443 on epoch=174
06/24/2022 19:51:59 - INFO - __main__ - Global step 350 Train loss 0.747202 Classification-F1 0.4458874458874459 on epoch=174
06/24/2022 19:52:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.675477 on epoch=179
06/24/2022 19:52:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.611644 on epoch=184
06/24/2022 19:52:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.668930 on epoch=189
06/24/2022 19:52:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.401790 on epoch=194
06/24/2022 19:52:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.473429 on epoch=199
06/24/2022 19:52:12 - INFO - __main__ - Global step 400 Train loss 0.566254 Classification-F1 0.4554554554554554 on epoch=199
06/24/2022 19:52:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.380026 on epoch=204
06/24/2022 19:52:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.314176 on epoch=209
06/24/2022 19:52:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.365109 on epoch=214
06/24/2022 19:52:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.211138 on epoch=219
06/24/2022 19:52:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.251382 on epoch=224
06/24/2022 19:52:24 - INFO - __main__ - Global step 450 Train loss 0.304366 Classification-F1 0.4554554554554554 on epoch=224
06/24/2022 19:52:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.258097 on epoch=229
06/24/2022 19:52:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.189710 on epoch=234
06/24/2022 19:52:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.109103 on epoch=239
06/24/2022 19:52:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.228926 on epoch=244
06/24/2022 19:52:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.111600 on epoch=249
06/24/2022 19:52:37 - INFO - __main__ - Global step 500 Train loss 0.179487 Classification-F1 0.39139139139139134 on epoch=249
06/24/2022 19:52:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.272710 on epoch=254
06/24/2022 19:52:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.096440 on epoch=259
06/24/2022 19:52:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.183630 on epoch=264
06/24/2022 19:52:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.089816 on epoch=269
06/24/2022 19:52:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.018733 on epoch=274
06/24/2022 19:52:50 - INFO - __main__ - Global step 550 Train loss 0.132266 Classification-F1 0.43529411764705883 on epoch=274
06/24/2022 19:52:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.072256 on epoch=279
06/24/2022 19:52:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.030310 on epoch=284
06/24/2022 19:52:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.043136 on epoch=289
06/24/2022 19:53:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.146801 on epoch=294
06/24/2022 19:53:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.084742 on epoch=299
06/24/2022 19:53:03 - INFO - __main__ - Global step 600 Train loss 0.075449 Classification-F1 0.4009852216748768 on epoch=299
06/24/2022 19:53:03 - INFO - __main__ - save last model!
06/24/2022 19:53:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:53:04 - INFO - __main__ - Printing 3 examples
06/24/2022 19:53:04 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:53:04 - INFO - __main__ - ['1']
06/24/2022 19:53:04 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:53:04 - INFO - __main__ - ['1']
06/24/2022 19:53:04 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:53:04 - INFO - __main__ - ['1']
06/24/2022 19:53:04 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:53:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:53:04 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:53:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:53:04 - INFO - __main__ - Printing 3 examples
06/24/2022 19:53:04 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:53:04 - INFO - __main__ - ['1']
06/24/2022 19:53:04 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:53:04 - INFO - __main__ - ['1']
06/24/2022 19:53:04 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:53:04 - INFO - __main__ - ['1']
06/24/2022 19:53:04 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:53:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:53:04 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:53:06 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:53:06 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:53:06 - INFO - __main__ - Printing 3 examples
06/24/2022 19:53:06 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:53:06 - INFO - __main__ - ['0']
06/24/2022 19:53:06 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:53:06 - INFO - __main__ - ['1']
06/24/2022 19:53:06 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:53:06 - INFO - __main__ - ['1']
06/24/2022 19:53:06 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:53:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:53:07 - INFO - __main__ - Starting training!
06/24/2022 19:53:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:53:18 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:54:17 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_42_0.0002_8_predictions.txt
06/24/2022 19:54:17 - INFO - __main__ - Classification-F1 on test data: 0.5129
06/24/2022 19:54:17 - INFO - __main__ - prefix=paws_16_42, lr=0.0002, bsz=8, dev_performance=0.5307917888563051, test_performance=0.5129098200691231
06/24/2022 19:54:17 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.0001, bsz=8 ...
06/24/2022 19:54:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:54:18 - INFO - __main__ - Printing 3 examples
06/24/2022 19:54:18 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:54:18 - INFO - __main__ - ['1']
06/24/2022 19:54:18 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:54:18 - INFO - __main__ - ['1']
06/24/2022 19:54:18 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:54:18 - INFO - __main__ - ['1']
06/24/2022 19:54:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:54:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:54:18 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:54:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:54:18 - INFO - __main__ - Printing 3 examples
06/24/2022 19:54:18 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:54:18 - INFO - __main__ - ['1']
06/24/2022 19:54:18 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:54:18 - INFO - __main__ - ['1']
06/24/2022 19:54:18 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:54:18 - INFO - __main__ - ['1']
06/24/2022 19:54:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:54:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:54:18 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:54:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:54:22 - INFO - __main__ - Starting training!
06/24/2022 19:54:24 - INFO - __main__ - Step 10 Global step 10 Train loss 18.557276 on epoch=4
06/24/2022 19:54:26 - INFO - __main__ - Step 20 Global step 20 Train loss 18.294590 on epoch=9
06/24/2022 19:54:28 - INFO - __main__ - Step 30 Global step 30 Train loss 13.522710 on epoch=14
06/24/2022 19:54:31 - INFO - __main__ - Step 40 Global step 40 Train loss 10.957727 on epoch=19
06/24/2022 19:54:33 - INFO - __main__ - Step 50 Global step 50 Train loss 8.933477 on epoch=24
06/24/2022 19:54:35 - INFO - __main__ - Global step 50 Train loss 14.053156 Classification-F1 0.02888888888888889 on epoch=24
06/24/2022 19:54:38 - INFO - __main__ - Step 60 Global step 60 Train loss 8.450672 on epoch=29
06/24/2022 19:54:40 - INFO - __main__ - Step 70 Global step 70 Train loss 7.614158 on epoch=34
06/24/2022 19:54:43 - INFO - __main__ - Step 80 Global step 80 Train loss 7.283175 on epoch=39
06/24/2022 19:54:45 - INFO - __main__ - Step 90 Global step 90 Train loss 6.686679 on epoch=44
06/24/2022 19:54:47 - INFO - __main__ - Step 100 Global step 100 Train loss 6.988896 on epoch=49
06/24/2022 19:54:48 - INFO - __main__ - Global step 100 Train loss 7.404716 Classification-F1 0.042539682539682544 on epoch=49
06/24/2022 19:54:51 - INFO - __main__ - Step 110 Global step 110 Train loss 6.046941 on epoch=54
06/24/2022 19:54:53 - INFO - __main__ - Step 120 Global step 120 Train loss 5.188281 on epoch=59
06/24/2022 19:54:56 - INFO - __main__ - Step 130 Global step 130 Train loss 5.343556 on epoch=64
06/24/2022 19:54:58 - INFO - __main__ - Step 140 Global step 140 Train loss 4.781535 on epoch=69
06/24/2022 19:55:00 - INFO - __main__ - Step 150 Global step 150 Train loss 4.159587 on epoch=74
06/24/2022 19:55:01 - INFO - __main__ - Global step 150 Train loss 5.103980 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:55:03 - INFO - __main__ - Step 160 Global step 160 Train loss 3.429219 on epoch=79
06/24/2022 19:55:06 - INFO - __main__ - Step 170 Global step 170 Train loss 2.979606 on epoch=84
06/24/2022 19:55:08 - INFO - __main__ - Step 180 Global step 180 Train loss 3.223781 on epoch=89
06/24/2022 19:55:11 - INFO - __main__ - Step 190 Global step 190 Train loss 2.902179 on epoch=94
06/24/2022 19:55:13 - INFO - __main__ - Step 200 Global step 200 Train loss 2.432151 on epoch=99
06/24/2022 19:55:14 - INFO - __main__ - Global step 200 Train loss 2.993387 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:55:16 - INFO - __main__ - Step 210 Global step 210 Train loss 2.171896 on epoch=104
06/24/2022 19:55:18 - INFO - __main__ - Step 220 Global step 220 Train loss 2.757163 on epoch=109
06/24/2022 19:55:21 - INFO - __main__ - Step 230 Global step 230 Train loss 1.465657 on epoch=114
06/24/2022 19:55:23 - INFO - __main__ - Step 240 Global step 240 Train loss 1.855921 on epoch=119
06/24/2022 19:55:26 - INFO - __main__ - Step 250 Global step 250 Train loss 1.904064 on epoch=124
06/24/2022 19:55:26 - INFO - __main__ - Global step 250 Train loss 2.030940 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:55:28 - INFO - __main__ - Step 260 Global step 260 Train loss 1.724907 on epoch=129
06/24/2022 19:55:31 - INFO - __main__ - Step 270 Global step 270 Train loss 2.288544 on epoch=134
06/24/2022 19:55:33 - INFO - __main__ - Step 280 Global step 280 Train loss 2.282082 on epoch=139
06/24/2022 19:55:36 - INFO - __main__ - Step 290 Global step 290 Train loss 1.392456 on epoch=144
06/24/2022 19:55:38 - INFO - __main__ - Step 300 Global step 300 Train loss 1.558609 on epoch=149
06/24/2022 19:55:38 - INFO - __main__ - Global step 300 Train loss 1.849319 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 19:55:41 - INFO - __main__ - Step 310 Global step 310 Train loss 1.835732 on epoch=154
06/24/2022 19:55:43 - INFO - __main__ - Step 320 Global step 320 Train loss 1.277493 on epoch=159
06/24/2022 19:55:46 - INFO - __main__ - Step 330 Global step 330 Train loss 1.348766 on epoch=164
06/24/2022 19:55:48 - INFO - __main__ - Step 340 Global step 340 Train loss 1.288175 on epoch=169
06/24/2022 19:55:51 - INFO - __main__ - Step 350 Global step 350 Train loss 1.812210 on epoch=174
06/24/2022 19:55:51 - INFO - __main__ - Global step 350 Train loss 1.512475 Classification-F1 0.3191489361702127 on epoch=174
06/24/2022 19:55:53 - INFO - __main__ - Step 360 Global step 360 Train loss 1.598080 on epoch=179
06/24/2022 19:55:56 - INFO - __main__ - Step 370 Global step 370 Train loss 1.654667 on epoch=184
06/24/2022 19:55:58 - INFO - __main__ - Step 380 Global step 380 Train loss 1.192274 on epoch=189
06/24/2022 19:56:01 - INFO - __main__ - Step 390 Global step 390 Train loss 1.187700 on epoch=194
06/24/2022 19:56:03 - INFO - __main__ - Step 400 Global step 400 Train loss 1.374914 on epoch=199
06/24/2022 19:56:03 - INFO - __main__ - Global step 400 Train loss 1.401527 Classification-F1 0.46843853820598 on epoch=199
06/24/2022 19:56:06 - INFO - __main__ - Step 410 Global step 410 Train loss 1.329824 on epoch=204
06/24/2022 19:56:09 - INFO - __main__ - Step 420 Global step 420 Train loss 1.346727 on epoch=209
06/24/2022 19:56:11 - INFO - __main__ - Step 430 Global step 430 Train loss 1.261116 on epoch=214
06/24/2022 19:56:14 - INFO - __main__ - Step 440 Global step 440 Train loss 1.399519 on epoch=219
06/24/2022 19:56:16 - INFO - __main__ - Step 450 Global step 450 Train loss 1.017519 on epoch=224
06/24/2022 19:56:16 - INFO - __main__ - Global step 450 Train loss 1.270941 Classification-F1 0.5 on epoch=224
06/24/2022 19:56:19 - INFO - __main__ - Step 460 Global step 460 Train loss 1.043504 on epoch=229
06/24/2022 19:56:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.827864 on epoch=234
06/24/2022 19:56:24 - INFO - __main__ - Step 480 Global step 480 Train loss 1.380969 on epoch=239
06/24/2022 19:56:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.861887 on epoch=244
06/24/2022 19:56:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.892295 on epoch=249
06/24/2022 19:56:29 - INFO - __main__ - Global step 500 Train loss 1.001304 Classification-F1 0.5333333333333333 on epoch=249
06/24/2022 19:56:32 - INFO - __main__ - Step 510 Global step 510 Train loss 1.088401 on epoch=254
06/24/2022 19:56:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.881608 on epoch=259
06/24/2022 19:56:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.784313 on epoch=264
06/24/2022 19:56:40 - INFO - __main__ - Step 540 Global step 540 Train loss 1.229812 on epoch=269
06/24/2022 19:56:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.888308 on epoch=274
06/24/2022 19:56:43 - INFO - __main__ - Global step 550 Train loss 0.974488 Classification-F1 0.3191489361702127 on epoch=274
06/24/2022 19:56:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.917416 on epoch=279
06/24/2022 19:56:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.956340 on epoch=284
06/24/2022 19:56:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.603555 on epoch=289
06/24/2022 19:56:53 - INFO - __main__ - Step 590 Global step 590 Train loss 1.068368 on epoch=294
06/24/2022 19:56:55 - INFO - __main__ - Step 600 Global step 600 Train loss 1.037902 on epoch=299
06/24/2022 19:56:56 - INFO - __main__ - Global step 600 Train loss 0.916716 Classification-F1 0.3191489361702127 on epoch=299
06/24/2022 19:56:56 - INFO - __main__ - save last model!
06/24/2022 19:56:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:56:56 - INFO - __main__ - Printing 3 examples
06/24/2022 19:56:56 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 19:56:56 - INFO - __main__ - ['0']
06/24/2022 19:56:56 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 19:56:56 - INFO - __main__ - ['0']
06/24/2022 19:56:56 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 19:56:56 - INFO - __main__ - ['0']
06/24/2022 19:56:56 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:56:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:56:57 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:56:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:56:57 - INFO - __main__ - Printing 3 examples
06/24/2022 19:56:57 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 19:56:57 - INFO - __main__ - ['0']
06/24/2022 19:56:57 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 19:56:57 - INFO - __main__ - ['0']
06/24/2022 19:56:57 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 19:56:57 - INFO - __main__ - ['0']
06/24/2022 19:56:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:56:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:56:57 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:56:58 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:56:59 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:56:59 - INFO - __main__ - Printing 3 examples
06/24/2022 19:56:59 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:56:59 - INFO - __main__ - ['0']
06/24/2022 19:56:59 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:56:59 - INFO - __main__ - ['1']
06/24/2022 19:56:59 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:56:59 - INFO - __main__ - ['1']
06/24/2022 19:56:59 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:57:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:57:00 - INFO - __main__ - Starting training!
06/24/2022 19:57:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:57:10 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:58:11 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_42_0.0001_8_predictions.txt
06/24/2022 19:58:11 - INFO - __main__ - Classification-F1 on test data: 0.4823
06/24/2022 19:58:11 - INFO - __main__ - prefix=paws_16_42, lr=0.0001, bsz=8, dev_performance=0.5333333333333333, test_performance=0.48225621076699576
06/24/2022 19:58:11 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.0005, bsz=8 ...
06/24/2022 19:58:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:58:12 - INFO - __main__ - Printing 3 examples
06/24/2022 19:58:12 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 19:58:12 - INFO - __main__ - ['0']
06/24/2022 19:58:12 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 19:58:12 - INFO - __main__ - ['0']
06/24/2022 19:58:12 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 19:58:12 - INFO - __main__ - ['0']
06/24/2022 19:58:12 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:58:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:58:12 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 19:58:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:58:12 - INFO - __main__ - Printing 3 examples
06/24/2022 19:58:12 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 19:58:12 - INFO - __main__ - ['0']
06/24/2022 19:58:12 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 19:58:12 - INFO - __main__ - ['0']
06/24/2022 19:58:12 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 19:58:12 - INFO - __main__ - ['0']
06/24/2022 19:58:12 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:58:13 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:58:13 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:58:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:58:17 - INFO - __main__ - Starting training!
06/24/2022 19:58:19 - INFO - __main__ - Step 10 Global step 10 Train loss 17.859043 on epoch=4
06/24/2022 19:58:21 - INFO - __main__ - Step 20 Global step 20 Train loss 15.006889 on epoch=9
06/24/2022 19:58:24 - INFO - __main__ - Step 30 Global step 30 Train loss 7.998172 on epoch=14
06/24/2022 19:58:26 - INFO - __main__ - Step 40 Global step 40 Train loss 4.753346 on epoch=19
06/24/2022 19:58:29 - INFO - __main__ - Step 50 Global step 50 Train loss 3.231876 on epoch=24
06/24/2022 19:58:29 - INFO - __main__ - Global step 50 Train loss 9.769865 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 19:58:32 - INFO - __main__ - Step 60 Global step 60 Train loss 3.102000 on epoch=29
06/24/2022 19:58:34 - INFO - __main__ - Step 70 Global step 70 Train loss 2.131850 on epoch=34
06/24/2022 19:58:37 - INFO - __main__ - Step 80 Global step 80 Train loss 2.029642 on epoch=39
06/24/2022 19:58:39 - INFO - __main__ - Step 90 Global step 90 Train loss 1.796295 on epoch=44
06/24/2022 19:58:42 - INFO - __main__ - Step 100 Global step 100 Train loss 1.786660 on epoch=49
06/24/2022 19:58:42 - INFO - __main__ - Global step 100 Train loss 2.169289 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:58:45 - INFO - __main__ - Step 110 Global step 110 Train loss 1.242304 on epoch=54
06/24/2022 19:58:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.814367 on epoch=59
06/24/2022 19:58:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.961770 on epoch=64
06/24/2022 19:58:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.817905 on epoch=69
06/24/2022 19:58:55 - INFO - __main__ - Step 150 Global step 150 Train loss 1.221562 on epoch=74
06/24/2022 19:58:55 - INFO - __main__ - Global step 150 Train loss 1.011582 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:58:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.989435 on epoch=79
06/24/2022 19:59:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.923997 on epoch=84
06/24/2022 19:59:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.810751 on epoch=89
06/24/2022 19:59:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.817802 on epoch=94
06/24/2022 19:59:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.626176 on epoch=99
06/24/2022 19:59:08 - INFO - __main__ - Global step 200 Train loss 0.833632 Classification-F1 0.3992490613266583 on epoch=99
06/24/2022 19:59:11 - INFO - __main__ - Step 210 Global step 210 Train loss 1.051739 on epoch=104
06/24/2022 19:59:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.973524 on epoch=109
06/24/2022 19:59:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.729989 on epoch=114
06/24/2022 19:59:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.915619 on epoch=119
06/24/2022 19:59:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.638589 on epoch=124
06/24/2022 19:59:22 - INFO - __main__ - Global step 250 Train loss 0.861892 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:59:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.773546 on epoch=129
06/24/2022 19:59:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.969468 on epoch=134
06/24/2022 19:59:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.892037 on epoch=139
06/24/2022 19:59:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.821866 on epoch=144
06/24/2022 19:59:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.586067 on epoch=149
06/24/2022 19:59:35 - INFO - __main__ - Global step 300 Train loss 0.808597 Classification-F1 0.3191489361702127 on epoch=149
06/24/2022 19:59:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.737805 on epoch=154
06/24/2022 19:59:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.704131 on epoch=159
06/24/2022 19:59:42 - INFO - __main__ - Step 330 Global step 330 Train loss 1.006131 on epoch=164
06/24/2022 19:59:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.842118 on epoch=169
06/24/2022 19:59:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.615074 on epoch=174
06/24/2022 19:59:48 - INFO - __main__ - Global step 350 Train loss 0.781052 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 19:59:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.693873 on epoch=179
06/24/2022 19:59:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.572551 on epoch=184
06/24/2022 19:59:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.731572 on epoch=189
06/24/2022 19:59:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.644168 on epoch=194
06/24/2022 20:00:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.586765 on epoch=199
06/24/2022 20:00:01 - INFO - __main__ - Global step 400 Train loss 0.645786 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 20:00:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.637972 on epoch=204
06/24/2022 20:00:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.584678 on epoch=209
06/24/2022 20:00:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.616338 on epoch=214
06/24/2022 20:00:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.556887 on epoch=219
06/24/2022 20:00:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.576462 on epoch=224
06/24/2022 20:00:14 - INFO - __main__ - Global step 450 Train loss 0.594468 Classification-F1 0.5270935960591133 on epoch=224
06/24/2022 20:00:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.659956 on epoch=229
06/24/2022 20:00:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.359030 on epoch=234
06/24/2022 20:00:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.447731 on epoch=239
06/24/2022 20:00:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.399155 on epoch=244
06/24/2022 20:00:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.437029 on epoch=249
06/24/2022 20:00:27 - INFO - __main__ - Global step 500 Train loss 0.460580 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 20:00:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.365010 on epoch=254
06/24/2022 20:00:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.409087 on epoch=259
06/24/2022 20:00:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.369446 on epoch=264
06/24/2022 20:00:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.400745 on epoch=269
06/24/2022 20:00:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.414147 on epoch=274
06/24/2022 20:00:40 - INFO - __main__ - Global step 550 Train loss 0.391687 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 20:00:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.451596 on epoch=279
06/24/2022 20:00:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.394467 on epoch=284
06/24/2022 20:00:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.351680 on epoch=289
06/24/2022 20:00:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.330711 on epoch=294
06/24/2022 20:00:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.309988 on epoch=299
06/24/2022 20:00:54 - INFO - __main__ - Global step 600 Train loss 0.367688 Classification-F1 0.3333333333333333 on epoch=299
06/24/2022 20:00:54 - INFO - __main__ - save last model!
06/24/2022 20:00:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:00:54 - INFO - __main__ - Printing 3 examples
06/24/2022 20:00:54 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 20:00:54 - INFO - __main__ - ['0']
06/24/2022 20:00:54 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 20:00:54 - INFO - __main__ - ['0']
06/24/2022 20:00:54 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 20:00:54 - INFO - __main__ - ['0']
06/24/2022 20:00:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:00:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:00:54 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 20:00:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:00:54 - INFO - __main__ - Printing 3 examples
06/24/2022 20:00:54 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 20:00:54 - INFO - __main__ - ['0']
06/24/2022 20:00:54 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 20:00:54 - INFO - __main__ - ['0']
06/24/2022 20:00:54 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 20:00:54 - INFO - __main__ - ['0']
06/24/2022 20:00:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:00:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:00:54 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 20:00:57 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 20:00:57 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 20:00:57 - INFO - __main__ - Printing 3 examples
06/24/2022 20:00:57 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 20:00:57 - INFO - __main__ - ['0']
06/24/2022 20:00:57 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 20:00:57 - INFO - __main__ - ['1']
06/24/2022 20:00:57 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 20:00:57 - INFO - __main__ - ['1']
06/24/2022 20:00:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:00:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 20:00:58 - INFO - __main__ - Starting training!
06/24/2022 20:01:01 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:01:09 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 20:02:07 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_87_0.0005_8_predictions.txt
06/24/2022 20:02:07 - INFO - __main__ - Classification-F1 on test data: 0.4960
06/24/2022 20:02:07 - INFO - __main__ - prefix=paws_16_87, lr=0.0005, bsz=8, dev_performance=0.5270935960591133, test_performance=0.4960101252214757
06/24/2022 20:02:07 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.0003, bsz=8 ...
06/24/2022 20:02:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:02:08 - INFO - __main__ - Printing 3 examples
06/24/2022 20:02:08 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 20:02:08 - INFO - __main__ - ['0']
06/24/2022 20:02:08 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 20:02:08 - INFO - __main__ - ['0']
06/24/2022 20:02:08 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 20:02:08 - INFO - __main__ - ['0']
06/24/2022 20:02:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:02:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:02:08 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 20:02:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:02:08 - INFO - __main__ - Printing 3 examples
06/24/2022 20:02:08 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 20:02:08 - INFO - __main__ - ['0']
06/24/2022 20:02:08 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 20:02:08 - INFO - __main__ - ['0']
06/24/2022 20:02:08 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 20:02:08 - INFO - __main__ - ['0']
06/24/2022 20:02:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:02:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:02:08 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 20:02:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 20:02:12 - INFO - __main__ - Starting training!
06/24/2022 20:02:14 - INFO - __main__ - Step 10 Global step 10 Train loss 18.225842 on epoch=4
06/24/2022 20:02:17 - INFO - __main__ - Step 20 Global step 20 Train loss 13.344610 on epoch=9
06/24/2022 20:02:19 - INFO - __main__ - Step 30 Global step 30 Train loss 9.280280 on epoch=14
06/24/2022 20:02:22 - INFO - __main__ - Step 40 Global step 40 Train loss 6.520228 on epoch=19
06/24/2022 20:02:24 - INFO - __main__ - Step 50 Global step 50 Train loss 6.311319 on epoch=24
06/24/2022 20:02:24 - INFO - __main__ - Global step 50 Train loss 10.736455 Classification-F1 0.3992490613266583 on epoch=24
06/24/2022 20:02:27 - INFO - __main__ - Step 60 Global step 60 Train loss 3.786195 on epoch=29
06/24/2022 20:02:30 - INFO - __main__ - Step 70 Global step 70 Train loss 3.719945 on epoch=34
06/24/2022 20:02:32 - INFO - __main__ - Step 80 Global step 80 Train loss 3.598518 on epoch=39
06/24/2022 20:02:35 - INFO - __main__ - Step 90 Global step 90 Train loss 1.741688 on epoch=44
06/24/2022 20:02:38 - INFO - __main__ - Step 100 Global step 100 Train loss 2.130688 on epoch=49
06/24/2022 20:02:38 - INFO - __main__ - Global step 100 Train loss 2.995407 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 20:02:41 - INFO - __main__ - Step 110 Global step 110 Train loss 1.496953 on epoch=54
06/24/2022 20:02:43 - INFO - __main__ - Step 120 Global step 120 Train loss 2.168131 on epoch=59
06/24/2022 20:02:46 - INFO - __main__ - Step 130 Global step 130 Train loss 1.757914 on epoch=64
06/24/2022 20:02:49 - INFO - __main__ - Step 140 Global step 140 Train loss 2.018715 on epoch=69
06/24/2022 20:02:51 - INFO - __main__ - Step 150 Global step 150 Train loss 1.710997 on epoch=74
06/24/2022 20:02:51 - INFO - __main__ - Global step 150 Train loss 1.830542 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 20:02:54 - INFO - __main__ - Step 160 Global step 160 Train loss 1.228947 on epoch=79
06/24/2022 20:02:56 - INFO - __main__ - Step 170 Global step 170 Train loss 1.407626 on epoch=84
06/24/2022 20:02:59 - INFO - __main__ - Step 180 Global step 180 Train loss 1.207727 on epoch=89
06/24/2022 20:03:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.824724 on epoch=94
06/24/2022 20:03:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.887260 on epoch=99
06/24/2022 20:03:04 - INFO - __main__ - Global step 200 Train loss 1.111257 Classification-F1 0.3043478260869565 on epoch=99
06/24/2022 20:03:07 - INFO - __main__ - Step 210 Global step 210 Train loss 1.367212 on epoch=104
06/24/2022 20:03:09 - INFO - __main__ - Step 220 Global step 220 Train loss 1.227192 on epoch=109
06/24/2022 20:03:12 - INFO - __main__ - Step 230 Global step 230 Train loss 1.160066 on epoch=114
06/24/2022 20:03:14 - INFO - __main__ - Step 240 Global step 240 Train loss 1.125609 on epoch=119
06/24/2022 20:03:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.862723 on epoch=124
06/24/2022 20:03:17 - INFO - __main__ - Global step 250 Train loss 1.148561 Classification-F1 0.4920634920634921 on epoch=124
06/24/2022 20:03:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.764751 on epoch=129
06/24/2022 20:03:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.922346 on epoch=134
06/24/2022 20:03:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.950643 on epoch=139
06/24/2022 20:03:28 - INFO - __main__ - Step 290 Global step 290 Train loss 1.140175 on epoch=144
06/24/2022 20:03:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.918683 on epoch=149
06/24/2022 20:03:31 - INFO - __main__ - Global step 300 Train loss 0.939319 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 20:03:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.972561 on epoch=154
06/24/2022 20:03:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.951689 on epoch=159
06/24/2022 20:03:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.831334 on epoch=164
06/24/2022 20:03:41 - INFO - __main__ - Step 340 Global step 340 Train loss 1.005059 on epoch=169
06/24/2022 20:03:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.799221 on epoch=174
06/24/2022 20:03:44 - INFO - __main__ - Global step 350 Train loss 0.911973 Classification-F1 0.4920634920634921 on epoch=174
06/24/2022 20:03:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.882938 on epoch=179
06/24/2022 20:03:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.959942 on epoch=184
06/24/2022 20:03:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.473711 on epoch=189
06/24/2022 20:03:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.709475 on epoch=194
06/24/2022 20:03:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.786066 on epoch=199
06/24/2022 20:03:57 - INFO - __main__ - Global step 400 Train loss 0.762427 Classification-F1 0.3992490613266583 on epoch=199
06/24/2022 20:03:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.844013 on epoch=204
06/24/2022 20:04:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.628994 on epoch=209
06/24/2022 20:04:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.607219 on epoch=214
06/24/2022 20:04:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.704033 on epoch=219
06/24/2022 20:04:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.437128 on epoch=224
06/24/2022 20:04:10 - INFO - __main__ - Global step 450 Train loss 0.644277 Classification-F1 0.34310850439882695 on epoch=224
06/24/2022 20:04:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.699322 on epoch=229
06/24/2022 20:04:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.616526 on epoch=234
06/24/2022 20:04:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.428785 on epoch=239
06/24/2022 20:04:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.540092 on epoch=244
06/24/2022 20:04:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.534123 on epoch=249
06/24/2022 20:04:22 - INFO - __main__ - Global step 500 Train loss 0.563770 Classification-F1 0.4980392156862745 on epoch=249
06/24/2022 20:04:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.518935 on epoch=254
06/24/2022 20:04:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.637745 on epoch=259
06/24/2022 20:04:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.488528 on epoch=264
06/24/2022 20:04:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.433396 on epoch=269
06/24/2022 20:04:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.406900 on epoch=274
06/24/2022 20:04:36 - INFO - __main__ - Global step 550 Train loss 0.497101 Classification-F1 0.37254901960784315 on epoch=274
06/24/2022 20:04:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.422179 on epoch=279
06/24/2022 20:04:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.291620 on epoch=284
06/24/2022 20:04:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.211562 on epoch=289
06/24/2022 20:04:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.548309 on epoch=294
06/24/2022 20:04:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.422108 on epoch=299
06/24/2022 20:04:49 - INFO - __main__ - Global step 600 Train loss 0.379156 Classification-F1 0.4554554554554554 on epoch=299
06/24/2022 20:04:49 - INFO - __main__ - save last model!
06/24/2022 20:04:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:04:50 - INFO - __main__ - Printing 3 examples
06/24/2022 20:04:50 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 20:04:50 - INFO - __main__ - ['0']
06/24/2022 20:04:50 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 20:04:50 - INFO - __main__ - ['0']
06/24/2022 20:04:50 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 20:04:50 - INFO - __main__ - ['0']
06/24/2022 20:04:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:04:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:04:50 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 20:04:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:04:50 - INFO - __main__ - Printing 3 examples
06/24/2022 20:04:50 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 20:04:50 - INFO - __main__ - ['0']
06/24/2022 20:04:50 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 20:04:50 - INFO - __main__ - ['0']
06/24/2022 20:04:50 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 20:04:50 - INFO - __main__ - ['0']
06/24/2022 20:04:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:04:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:04:50 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 20:04:52 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 20:04:52 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 20:04:52 - INFO - __main__ - Printing 3 examples
06/24/2022 20:04:52 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 20:04:52 - INFO - __main__ - ['0']
06/24/2022 20:04:52 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 20:04:52 - INFO - __main__ - ['1']
06/24/2022 20:04:52 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 20:04:52 - INFO - __main__ - ['1']
06/24/2022 20:04:52 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:04:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 20:04:54 - INFO - __main__ - Starting training!
06/24/2022 20:04:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:05:04 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 20:06:03 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_87_0.0003_8_predictions.txt
06/24/2022 20:06:03 - INFO - __main__ - Classification-F1 on test data: 0.5090
06/24/2022 20:06:03 - INFO - __main__ - prefix=paws_16_87, lr=0.0003, bsz=8, dev_performance=0.4980392156862745, test_performance=0.508974710935986
06/24/2022 20:06:03 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.0002, bsz=8 ...
06/24/2022 20:06:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:06:04 - INFO - __main__ - Printing 3 examples
06/24/2022 20:06:04 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 20:06:04 - INFO - __main__ - ['0']
06/24/2022 20:06:04 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 20:06:04 - INFO - __main__ - ['0']
06/24/2022 20:06:04 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 20:06:04 - INFO - __main__ - ['0']
06/24/2022 20:06:04 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:06:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:06:04 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 20:06:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:06:04 - INFO - __main__ - Printing 3 examples
06/24/2022 20:06:04 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 20:06:04 - INFO - __main__ - ['0']
06/24/2022 20:06:04 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 20:06:04 - INFO - __main__ - ['0']
06/24/2022 20:06:04 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 20:06:04 - INFO - __main__ - ['0']
06/24/2022 20:06:04 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:06:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:06:04 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 20:06:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 20:06:08 - INFO - __main__ - Starting training!
06/24/2022 20:06:10 - INFO - __main__ - Step 10 Global step 10 Train loss 18.272266 on epoch=4
06/24/2022 20:06:12 - INFO - __main__ - Step 20 Global step 20 Train loss 15.873416 on epoch=9
06/24/2022 20:06:14 - INFO - __main__ - Step 30 Global step 30 Train loss 12.299232 on epoch=14
06/24/2022 20:06:17 - INFO - __main__ - Step 40 Global step 40 Train loss 9.629784 on epoch=19
06/24/2022 20:06:19 - INFO - __main__ - Step 50 Global step 50 Train loss 7.503863 on epoch=24
06/24/2022 20:06:20 - INFO - __main__ - Global step 50 Train loss 12.715712 Classification-F1 0.12903225806451615 on epoch=24
06/24/2022 20:06:23 - INFO - __main__ - Step 60 Global step 60 Train loss 6.649984 on epoch=29
06/24/2022 20:06:25 - INFO - __main__ - Step 70 Global step 70 Train loss 5.499284 on epoch=34
06/24/2022 20:06:28 - INFO - __main__ - Step 80 Global step 80 Train loss 4.948716 on epoch=39
06/24/2022 20:06:30 - INFO - __main__ - Step 90 Global step 90 Train loss 3.758120 on epoch=44
06/24/2022 20:06:33 - INFO - __main__ - Step 100 Global step 100 Train loss 3.719437 on epoch=49
06/24/2022 20:06:33 - INFO - __main__ - Global step 100 Train loss 4.915108 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 20:06:36 - INFO - __main__ - Step 110 Global step 110 Train loss 2.443645 on epoch=54
06/24/2022 20:06:39 - INFO - __main__ - Step 120 Global step 120 Train loss 2.979368 on epoch=59
06/24/2022 20:06:41 - INFO - __main__ - Step 130 Global step 130 Train loss 2.847932 on epoch=64
06/24/2022 20:06:44 - INFO - __main__ - Step 140 Global step 140 Train loss 2.053156 on epoch=69
06/24/2022 20:06:46 - INFO - __main__ - Step 150 Global step 150 Train loss 1.530029 on epoch=74
06/24/2022 20:06:47 - INFO - __main__ - Global step 150 Train loss 2.370826 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 20:06:49 - INFO - __main__ - Step 160 Global step 160 Train loss 2.485966 on epoch=79
06/24/2022 20:06:52 - INFO - __main__ - Step 170 Global step 170 Train loss 2.394474 on epoch=84
06/24/2022 20:06:54 - INFO - __main__ - Step 180 Global step 180 Train loss 2.121197 on epoch=89
06/24/2022 20:06:57 - INFO - __main__ - Step 190 Global step 190 Train loss 1.760669 on epoch=94
06/24/2022 20:06:59 - INFO - __main__ - Step 200 Global step 200 Train loss 1.457062 on epoch=99
06/24/2022 20:06:59 - INFO - __main__ - Global step 200 Train loss 2.043874 Classification-F1 0.3191489361702127 on epoch=99
06/24/2022 20:07:02 - INFO - __main__ - Step 210 Global step 210 Train loss 1.277126 on epoch=104
06/24/2022 20:07:04 - INFO - __main__ - Step 220 Global step 220 Train loss 1.533648 on epoch=109
06/24/2022 20:07:07 - INFO - __main__ - Step 230 Global step 230 Train loss 1.254734 on epoch=114
06/24/2022 20:07:09 - INFO - __main__ - Step 240 Global step 240 Train loss 1.287117 on epoch=119
06/24/2022 20:07:12 - INFO - __main__ - Step 250 Global step 250 Train loss 1.488904 on epoch=124
06/24/2022 20:07:12 - INFO - __main__ - Global step 250 Train loss 1.368306 Classification-F1 0.5307917888563051 on epoch=124
06/24/2022 20:07:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.969119 on epoch=129
06/24/2022 20:07:18 - INFO - __main__ - Step 270 Global step 270 Train loss 1.470936 on epoch=134
06/24/2022 20:07:20 - INFO - __main__ - Step 280 Global step 280 Train loss 1.118436 on epoch=139
06/24/2022 20:07:23 - INFO - __main__ - Step 290 Global step 290 Train loss 1.289152 on epoch=144
06/24/2022 20:07:25 - INFO - __main__ - Step 300 Global step 300 Train loss 1.359090 on epoch=149
06/24/2022 20:07:25 - INFO - __main__ - Global step 300 Train loss 1.241347 Classification-F1 0.3454545454545454 on epoch=149
06/24/2022 20:07:28 - INFO - __main__ - Step 310 Global step 310 Train loss 1.135232 on epoch=154
06/24/2022 20:07:30 - INFO - __main__ - Step 320 Global step 320 Train loss 1.072848 on epoch=159
06/24/2022 20:07:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.876339 on epoch=164
06/24/2022 20:07:35 - INFO - __main__ - Step 340 Global step 340 Train loss 1.029675 on epoch=169
06/24/2022 20:07:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.958574 on epoch=174
06/24/2022 20:07:38 - INFO - __main__ - Global step 350 Train loss 1.014534 Classification-F1 0.4458874458874459 on epoch=174
06/24/2022 20:07:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.940025 on epoch=179
06/24/2022 20:07:43 - INFO - __main__ - Step 370 Global step 370 Train loss 1.077780 on epoch=184
06/24/2022 20:07:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.966345 on epoch=189
06/24/2022 20:07:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.773342 on epoch=194
06/24/2022 20:07:51 - INFO - __main__ - Step 400 Global step 400 Train loss 1.001989 on epoch=199
06/24/2022 20:07:51 - INFO - __main__ - Global step 400 Train loss 0.951896 Classification-F1 0.3992490613266583 on epoch=199
06/24/2022 20:07:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.647254 on epoch=204
06/24/2022 20:07:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.974119 on epoch=209
06/24/2022 20:07:58 - INFO - __main__ - Step 430 Global step 430 Train loss 1.089622 on epoch=214
06/24/2022 20:08:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.692288 on epoch=219
06/24/2022 20:08:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.870436 on epoch=224
06/24/2022 20:08:03 - INFO - __main__ - Global step 450 Train loss 0.854744 Classification-F1 0.5151515151515151 on epoch=224
06/24/2022 20:08:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.930491 on epoch=229
06/24/2022 20:08:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.982710 on epoch=234
06/24/2022 20:08:11 - INFO - __main__ - Step 480 Global step 480 Train loss 1.022246 on epoch=239
06/24/2022 20:08:13 - INFO - __main__ - Step 490 Global step 490 Train loss 1.034307 on epoch=244
06/24/2022 20:08:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.949914 on epoch=249
06/24/2022 20:08:16 - INFO - __main__ - Global step 500 Train loss 0.983934 Classification-F1 0.4458874458874459 on epoch=249
06/24/2022 20:08:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.681453 on epoch=254
06/24/2022 20:08:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.677282 on epoch=259
06/24/2022 20:08:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.976046 on epoch=264
06/24/2022 20:08:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.743706 on epoch=269
06/24/2022 20:08:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.721542 on epoch=274
06/24/2022 20:08:29 - INFO - __main__ - Global step 550 Train loss 0.760006 Classification-F1 0.3043478260869565 on epoch=274
06/24/2022 20:08:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.714352 on epoch=279
06/24/2022 20:08:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.956482 on epoch=284
06/24/2022 20:08:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.914236 on epoch=289
06/24/2022 20:08:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.730023 on epoch=294
06/24/2022 20:08:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.827095 on epoch=299
06/24/2022 20:08:42 - INFO - __main__ - Global step 600 Train loss 0.828438 Classification-F1 0.36374269005847953 on epoch=299
06/24/2022 20:08:42 - INFO - __main__ - save last model!
06/24/2022 20:08:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:08:43 - INFO - __main__ - Printing 3 examples
06/24/2022 20:08:43 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 20:08:43 - INFO - __main__ - ['0']
06/24/2022 20:08:43 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 20:08:43 - INFO - __main__ - ['0']
06/24/2022 20:08:43 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 20:08:43 - INFO - __main__ - ['0']
06/24/2022 20:08:43 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:08:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:08:43 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 20:08:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:08:43 - INFO - __main__ - Printing 3 examples
06/24/2022 20:08:43 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 20:08:43 - INFO - __main__ - ['0']
06/24/2022 20:08:43 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 20:08:43 - INFO - __main__ - ['0']
06/24/2022 20:08:43 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 20:08:43 - INFO - __main__ - ['0']
06/24/2022 20:08:43 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:08:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:08:43 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 20:08:45 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 20:08:45 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 20:08:45 - INFO - __main__ - Printing 3 examples
06/24/2022 20:08:45 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 20:08:45 - INFO - __main__ - ['0']
06/24/2022 20:08:45 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 20:08:45 - INFO - __main__ - ['1']
06/24/2022 20:08:45 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 20:08:45 - INFO - __main__ - ['1']
06/24/2022 20:08:45 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:08:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 20:08:47 - INFO - __main__ - Starting training!
06/24/2022 20:08:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:08:57 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 20:09:57 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_87_0.0002_8_predictions.txt
06/24/2022 20:09:57 - INFO - __main__ - Classification-F1 on test data: 0.4902
06/24/2022 20:09:57 - INFO - __main__ - prefix=paws_16_87, lr=0.0002, bsz=8, dev_performance=0.5307917888563051, test_performance=0.49024843888584413
06/24/2022 20:09:57 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.0001, bsz=8 ...
06/24/2022 20:09:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:09:58 - INFO - __main__ - Printing 3 examples
06/24/2022 20:09:58 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 20:09:58 - INFO - __main__ - ['0']
06/24/2022 20:09:58 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 20:09:58 - INFO - __main__ - ['0']
06/24/2022 20:09:58 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 20:09:58 - INFO - __main__ - ['0']
06/24/2022 20:09:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:09:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:09:58 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 20:09:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:09:58 - INFO - __main__ - Printing 3 examples
06/24/2022 20:09:58 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 20:09:58 - INFO - __main__ - ['0']
06/24/2022 20:09:58 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 20:09:58 - INFO - __main__ - ['0']
06/24/2022 20:09:58 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 20:09:58 - INFO - __main__ - ['0']
06/24/2022 20:09:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:09:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:09:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 20:10:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 20:10:02 - INFO - __main__ - Starting training!
06/24/2022 20:10:04 - INFO - __main__ - Step 10 Global step 10 Train loss 18.306149 on epoch=4
06/24/2022 20:10:07 - INFO - __main__ - Step 20 Global step 20 Train loss 17.737345 on epoch=9
06/24/2022 20:10:09 - INFO - __main__ - Step 30 Global step 30 Train loss 13.588193 on epoch=14
06/24/2022 20:10:11 - INFO - __main__ - Step 40 Global step 40 Train loss 12.880322 on epoch=19
06/24/2022 20:10:14 - INFO - __main__ - Step 50 Global step 50 Train loss 11.720693 on epoch=24
06/24/2022 20:10:16 - INFO - __main__ - Global step 50 Train loss 14.846539 Classification-F1 0.0035842293906810036 on epoch=24
06/24/2022 20:10:19 - INFO - __main__ - Step 60 Global step 60 Train loss 10.600663 on epoch=29
06/24/2022 20:10:21 - INFO - __main__ - Step 70 Global step 70 Train loss 10.135930 on epoch=34
06/24/2022 20:10:24 - INFO - __main__ - Step 80 Global step 80 Train loss 8.409504 on epoch=39
06/24/2022 20:10:26 - INFO - __main__ - Step 90 Global step 90 Train loss 8.314714 on epoch=44
06/24/2022 20:10:29 - INFO - __main__ - Step 100 Global step 100 Train loss 7.789214 on epoch=49
06/24/2022 20:10:29 - INFO - __main__ - Global step 100 Train loss 9.050005 Classification-F1 0.22621082621082617 on epoch=49
06/24/2022 20:10:32 - INFO - __main__ - Step 110 Global step 110 Train loss 7.426347 on epoch=54
06/24/2022 20:10:35 - INFO - __main__ - Step 120 Global step 120 Train loss 7.053820 on epoch=59
06/24/2022 20:10:37 - INFO - __main__ - Step 130 Global step 130 Train loss 6.932936 on epoch=64
06/24/2022 20:10:40 - INFO - __main__ - Step 140 Global step 140 Train loss 5.963532 on epoch=69
06/24/2022 20:10:42 - INFO - __main__ - Step 150 Global step 150 Train loss 4.702482 on epoch=74
06/24/2022 20:10:42 - INFO - __main__ - Global step 150 Train loss 6.415824 Classification-F1 0.33086419753086416 on epoch=74
06/24/2022 20:10:45 - INFO - __main__ - Step 160 Global step 160 Train loss 5.050243 on epoch=79
06/24/2022 20:10:48 - INFO - __main__ - Step 170 Global step 170 Train loss 5.136830 on epoch=84
06/24/2022 20:10:50 - INFO - __main__ - Step 180 Global step 180 Train loss 4.403351 on epoch=89
06/24/2022 20:10:53 - INFO - __main__ - Step 190 Global step 190 Train loss 4.580024 on epoch=94
06/24/2022 20:10:55 - INFO - __main__ - Step 200 Global step 200 Train loss 3.403796 on epoch=99
06/24/2022 20:10:56 - INFO - __main__ - Global step 200 Train loss 4.514849 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 20:10:59 - INFO - __main__ - Step 210 Global step 210 Train loss 2.830732 on epoch=104
06/24/2022 20:11:01 - INFO - __main__ - Step 220 Global step 220 Train loss 3.097867 on epoch=109
06/24/2022 20:11:04 - INFO - __main__ - Step 230 Global step 230 Train loss 3.519270 on epoch=114
06/24/2022 20:11:06 - INFO - __main__ - Step 240 Global step 240 Train loss 2.716503 on epoch=119
06/24/2022 20:11:09 - INFO - __main__ - Step 250 Global step 250 Train loss 2.987255 on epoch=124
06/24/2022 20:11:09 - INFO - __main__ - Global step 250 Train loss 3.030325 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 20:11:11 - INFO - __main__ - Step 260 Global step 260 Train loss 2.051555 on epoch=129
06/24/2022 20:11:14 - INFO - __main__ - Step 270 Global step 270 Train loss 2.565394 on epoch=134
06/24/2022 20:11:17 - INFO - __main__ - Step 280 Global step 280 Train loss 2.017922 on epoch=139
06/24/2022 20:11:19 - INFO - __main__ - Step 290 Global step 290 Train loss 2.356881 on epoch=144
06/24/2022 20:11:22 - INFO - __main__ - Step 300 Global step 300 Train loss 2.688838 on epoch=149
06/24/2022 20:11:22 - INFO - __main__ - Global step 300 Train loss 2.336118 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 20:11:24 - INFO - __main__ - Step 310 Global step 310 Train loss 2.871899 on epoch=154
06/24/2022 20:11:27 - INFO - __main__ - Step 320 Global step 320 Train loss 2.263367 on epoch=159
06/24/2022 20:11:29 - INFO - __main__ - Step 330 Global step 330 Train loss 2.359102 on epoch=164
06/24/2022 20:11:32 - INFO - __main__ - Step 340 Global step 340 Train loss 1.909289 on epoch=169
06/24/2022 20:11:34 - INFO - __main__ - Step 350 Global step 350 Train loss 1.955052 on epoch=174
06/24/2022 20:11:34 - INFO - __main__ - Global step 350 Train loss 2.271742 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 20:11:37 - INFO - __main__ - Step 360 Global step 360 Train loss 2.413523 on epoch=179
06/24/2022 20:11:39 - INFO - __main__ - Step 370 Global step 370 Train loss 1.963431 on epoch=184
06/24/2022 20:11:42 - INFO - __main__ - Step 380 Global step 380 Train loss 1.403795 on epoch=189
06/24/2022 20:11:44 - INFO - __main__ - Step 390 Global step 390 Train loss 1.771543 on epoch=194
06/24/2022 20:11:47 - INFO - __main__ - Step 400 Global step 400 Train loss 1.273161 on epoch=199
06/24/2022 20:11:47 - INFO - __main__ - Global step 400 Train loss 1.765090 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 20:11:50 - INFO - __main__ - Step 410 Global step 410 Train loss 1.727518 on epoch=204
06/24/2022 20:11:52 - INFO - __main__ - Step 420 Global step 420 Train loss 1.359388 on epoch=209
06/24/2022 20:11:55 - INFO - __main__ - Step 430 Global step 430 Train loss 1.855217 on epoch=214
06/24/2022 20:11:57 - INFO - __main__ - Step 440 Global step 440 Train loss 1.188078 on epoch=219
06/24/2022 20:12:00 - INFO - __main__ - Step 450 Global step 450 Train loss 1.814466 on epoch=224
06/24/2022 20:12:00 - INFO - __main__ - Global step 450 Train loss 1.588933 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 20:12:03 - INFO - __main__ - Step 460 Global step 460 Train loss 1.531227 on epoch=229
06/24/2022 20:12:05 - INFO - __main__ - Step 470 Global step 470 Train loss 1.783478 on epoch=234
06/24/2022 20:12:08 - INFO - __main__ - Step 480 Global step 480 Train loss 1.074816 on epoch=239
06/24/2022 20:12:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.900765 on epoch=244
06/24/2022 20:12:13 - INFO - __main__ - Step 500 Global step 500 Train loss 1.783712 on epoch=249
06/24/2022 20:12:13 - INFO - __main__ - Global step 500 Train loss 1.414799 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 20:12:15 - INFO - __main__ - Step 510 Global step 510 Train loss 1.005900 on epoch=254
06/24/2022 20:12:18 - INFO - __main__ - Step 520 Global step 520 Train loss 1.252774 on epoch=259
06/24/2022 20:12:21 - INFO - __main__ - Step 530 Global step 530 Train loss 1.314011 on epoch=264
06/24/2022 20:12:23 - INFO - __main__ - Step 540 Global step 540 Train loss 1.187949 on epoch=269
06/24/2022 20:12:26 - INFO - __main__ - Step 550 Global step 550 Train loss 1.057229 on epoch=274
06/24/2022 20:12:26 - INFO - __main__ - Global step 550 Train loss 1.163573 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 20:12:28 - INFO - __main__ - Step 560 Global step 560 Train loss 1.342266 on epoch=279
06/24/2022 20:12:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.995254 on epoch=284
06/24/2022 20:12:33 - INFO - __main__ - Step 580 Global step 580 Train loss 1.034240 on epoch=289
06/24/2022 20:12:36 - INFO - __main__ - Step 590 Global step 590 Train loss 1.318752 on epoch=294
06/24/2022 20:12:38 - INFO - __main__ - Step 600 Global step 600 Train loss 1.700005 on epoch=299
06/24/2022 20:12:39 - INFO - __main__ - Global step 600 Train loss 1.278103 Classification-F1 0.3333333333333333 on epoch=299
06/24/2022 20:12:39 - INFO - __main__ - save last model!
06/24/2022 20:12:41 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 20:12:41 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 20:12:41 - INFO - __main__ - Printing 3 examples
06/24/2022 20:12:41 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 20:12:41 - INFO - __main__ - ['0']
06/24/2022 20:12:41 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 20:12:41 - INFO - __main__ - ['1']
06/24/2022 20:12:41 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 20:12:41 - INFO - __main__ - ['1']
06/24/2022 20:12:41 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:12:46 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:12:53 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 20:13:47 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_87_0.0001_8_predictions.txt
06/24/2022 20:13:47 - INFO - __main__ - Classification-F1 on test data: 0.2107
06/24/2022 20:13:48 - INFO - __main__ - prefix=paws_16_87, lr=0.0001, bsz=8, dev_performance=0.3333333333333333, test_performance=0.21071556614054676
