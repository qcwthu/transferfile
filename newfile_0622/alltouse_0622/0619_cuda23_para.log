nohup: ignoring input
t5base para fomaml upstream
06/19/2022 14:26:32 - INFO - __main__ - Namespace(train_dir='data', predict_dir='data', identifier='base', output_dir='models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base', do_train=True, do_predict=False, inner_bsz=4, inner_lr=3e-05, checkpoint=None, do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=64, num_beams=4, append_another_bos=False, train_batch_size=1, predict_batch_size=1, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=2, num_train_epochs=120.0, warmup_steps=360, total_steps=5000, wait_step=10000000000, verbose=False, eval_period=10, prefix='', debug=False, seed=42, custom_tasks_splits='./dataloader/custom_tasks_splits/train_nonparaphrase_classification_test_paraphrase.json', cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=-1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2')
06/19/2022 14:26:32 - INFO - __main__ - models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base
06/19/2022 14:26:32 - INFO - __main__ - args.device: cuda
06/19/2022 14:26:32 - INFO - __main__ - Using 1 gpus
06/19/2022 14:26:33 - INFO - __main__ - Training on the following tasks: ['ade_corpus_v2-classification', 'ag_news', 'amazon_polarity', 'anli', 'circa', 'climate_fever', 'dbpedia_14', 'discovery', 'emo', 'emotion', 'ethos-directed_vs_generalized', 'ethos-disability', 'ethos-gender', 'ethos-national_origin', 'ethos-race', 'ethos-religion', 'ethos-sexual_orientation', 'financial_phrasebank', 'glue-cola', 'glue-mnli', 'glue-qnli', 'glue-rte', 'glue-sst2', 'glue-wnli', 'google_wellformed_query', 'hate_speech18', 'hate_speech_offensive', 'hatexplain', 'health_fact', 'imdb', 'kilt_fever', 'liar', 'onestop_english', 'poem_sentiment', 'rotten_tomatoes', 'scicite', 'scitail', 'sick', 'sms_spam', 'superglue-cb', 'superglue-rte', 'superglue-wic', 'superglue-wsc', 'tab_fact', 'trec', 'trec-finegrained', 'tweet_eval-emoji', 'tweet_eval-emotion', 'tweet_eval-hate', 'tweet_eval-irony', 'tweet_eval-offensive', 'tweet_eval-sentiment', 'tweet_eval-stance_abortion', 'tweet_eval-stance_atheism', 'tweet_eval-stance_climate', 'tweet_eval-stance_feminist', 'tweet_eval-stance_hillary', 'wiki_qa', 'yahoo_answers_topics', 'yelp_polarity']
06/19/2022 14:26:36 - INFO - __main__ - Start tokenizing ... 300 instances
06/19/2022 14:26:36 - INFO - __main__ - Printing 3 examples
06/19/2022 14:26:36 - INFO - __main__ -  [ade_corpus_v2-classification] The treatment of FMF attacks in patients who cannot use colchicine is an important problem.
06/19/2022 14:26:36 - INFO - __main__ -  Not Related
06/19/2022 14:26:36 - INFO - __main__ -  [ade_corpus_v2-classification] The origin of NIS is outlined briefly and some fundamental clinical and experimental facts are presented, all of which stress the importance of the acute blockade of postsynaptic DA-ergic receptors.
06/19/2022 14:26:36 - INFO - __main__ -  Not Related
06/19/2022 14:26:36 - INFO - __main__ -  [ade_corpus_v2-classification] His AFP was initially 9828 microg/L and rapidly dropped to 5597 microg/L in ten days after oral sorafenib treatment.
06/19/2022 14:26:36 - INFO - __main__ -  Not Related
06/19/2022 14:26:36 - INFO - __main__ - Tokenizing Train Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 14:26:53 - INFO - __main__ - Tokenizing Train Output ...
06/19/2022 14:27:03 - INFO - __main__ - Tokenizing Dev Input ...
06/19/2022 14:27:20 - INFO - __main__ - Tokenizing Dev Output ...
06/19/2022 14:28:09 - INFO - __main__ - Loaded 300 examples from train data
06/19/2022 14:28:15 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 14:28:19 - INFO - __main__ - Starting training!
use RandomSampler
initialize from c4
[(1219, 784566), (205, 3175490), (3178, 338316), (2767, 377111), (2421, 410967), (1481, 640748), (924, 843157), (5916, 171503), (566, 946306), (5534, 158824), (4682, 222111), (360, 2496612), (333, 2670525), (958, 890939), (2537, 417981), (2569, 396692), (5644, 185115), (6480, 156027), (326, 2947559), (6313, 166539), (2123, 459189), (3488, 171592), (4602, 223532), (1752, 415773), (3177, 209086), (6847, 159087), (3213, 334916), (68, 12242894), (1729, 565725), (3341, 221693), (3798, 276026), (3041, 335480), (986, 581568), (2389, 425302), (3832, 279541), (1023, 843875), (725, 898187), (4260, 246836), (1139, 870475), (4205, 261446)]
Epoch 0:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 0:   0%|          | 1/300 [00:00<04:27,  1.12it/s]Epoch 0:   1%|          | 2/300 [00:01<03:04,  1.62it/s]Epoch 0:   1%|          | 3/300 [00:01<02:26,  2.03it/s]Epoch 0:   1%|▏         | 4/300 [00:02<02:09,  2.29it/s]Epoch 0:   2%|▏         | 5/300 [00:02<02:08,  2.30it/s]Epoch 0:   2%|▏         | 6/300 [00:02<02:10,  2.26it/s]Epoch 0:   2%|▏         | 7/300 [00:03<02:12,  2.22it/s]Epoch 0:   3%|▎         | 8/300 [00:03<02:06,  2.31it/s]Epoch 0:   3%|▎         | 9/300 [00:04<02:03,  2.36it/s]Epoch 0:   3%|▎         | 10/300 [00:04<02:04,  2.32it/s]Epoch 0:   4%|▎         | 11/300 [00:04<01:56,  2.48it/s]Epoch 0:   4%|▍         | 12/300 [00:05<01:55,  2.49it/s]Epoch 0:   4%|▍         | 13/300 [00:05<01:53,  2.53it/s]Epoch 0:   5%|▍         | 14/300 [00:06<01:59,  2.39it/s]Epoch 0:   5%|▌         | 15/300 [00:06<01:53,  2.52it/s]Epoch 0:   5%|▌         | 16/300 [00:06<01:53,  2.51it/s]Epoch 0:   6%|▌         | 17/300 [00:07<01:53,  2.49it/s]Epoch 0:   6%|▌         | 18/300 [00:07<01:51,  2.52it/s]Epoch 0:   6%|▋         | 19/300 [00:08<01:55,  2.44it/s]06/19/2022 14:28:28 - INFO - __main__ - global step: 10; train loss: 7.578407287597656; dev loss: 7.827520847320557
Epoch 0:   7%|▋         | 20/300 [00:08<01:49,  2.56it/s]Epoch 0:   7%|▋         | 21/300 [00:08<01:45,  2.65it/s]Epoch 0:   7%|▋         | 22/300 [00:09<01:42,  2.72it/s]Epoch 0:   8%|▊         | 23/300 [00:09<01:51,  2.48it/s]Epoch 0:   8%|▊         | 24/300 [00:10<01:47,  2.57it/s]Epoch 0:   8%|▊         | 25/300 [00:10<01:42,  2.68it/s]Epoch 0:   9%|▊         | 26/300 [00:10<01:41,  2.70it/s]Epoch 0:   9%|▉         | 27/300 [00:11<01:50,  2.46it/s]Epoch 0:   9%|▉         | 28/300 [00:11<01:46,  2.56it/s]Epoch 0:  10%|▉         | 29/300 [00:11<01:41,  2.67it/s]Epoch 0:  10%|█         | 30/300 [00:12<01:37,  2.76it/s]Epoch 0:  10%|█         | 31/300 [00:12<01:47,  2.50it/s]Epoch 0:  11%|█         | 32/300 [00:13<01:48,  2.48it/s]Epoch 0:  11%|█         | 33/300 [00:13<01:43,  2.59it/s]Epoch 0:  11%|█▏        | 34/300 [00:13<01:39,  2.68it/s]Epoch 0:  12%|█▏        | 35/300 [00:14<01:46,  2.49it/s]Epoch 0:  12%|█▏        | 36/300 [00:14<01:45,  2.50it/s]Epoch 0:  12%|█▏        | 37/300 [00:15<01:41,  2.60it/s]Epoch 0:  13%|█▎        | 38/300 [00:15<01:37,  2.69it/s]Epoch 0:  13%|█▎        | 39/300 [00:15<01:41,  2.56it/s]06/19/2022 14:28:35 - INFO - __main__ - global step: 20; train loss: 7.986338138580322; dev loss: 7.748887538909912
Epoch 0:  13%|█▎        | 40/300 [00:16<01:37,  2.66it/s]Epoch 0:  14%|█▎        | 41/300 [00:16<01:35,  2.72it/s]Epoch 0:  14%|█▍        | 42/300 [00:16<01:32,  2.78it/s]Epoch 0:  14%|█▍        | 43/300 [00:17<01:43,  2.49it/s]Epoch 0:  15%|█▍        | 44/300 [00:17<01:41,  2.52it/s]Epoch 0:  15%|█▌        | 45/300 [00:18<01:42,  2.50it/s]Epoch 0:  15%|█▌        | 46/300 [00:18<01:42,  2.47it/s]Epoch 0:  16%|█▌        | 47/300 [00:18<01:38,  2.58it/s]Epoch 0:  16%|█▌        | 48/300 [00:19<01:41,  2.47it/s]Epoch 0:  16%|█▋        | 49/300 [00:19<01:36,  2.61it/s]Epoch 0:  17%|█▋        | 50/300 [00:20<01:37,  2.57it/s]Epoch 0:  17%|█▋        | 51/300 [00:20<01:33,  2.65it/s]Epoch 0:  17%|█▋        | 52/300 [00:20<01:41,  2.44it/s]Epoch 0:  18%|█▊        | 53/300 [00:21<01:36,  2.56it/s]Epoch 0:  18%|█▊        | 54/300 [00:21<01:32,  2.65it/s]Epoch 0:  18%|█▊        | 55/300 [00:22<01:33,  2.61it/s]Epoch 0:  19%|█▊        | 56/300 [00:22<01:42,  2.38it/s]Epoch 0:  19%|█▉        | 57/300 [00:22<01:38,  2.46it/s]Epoch 0:  19%|█▉        | 58/300 [00:23<01:34,  2.56it/s]Epoch 0:  20%|█▉        | 59/300 [00:23<01:30,  2.66it/s]06/19/2022 14:28:43 - INFO - __main__ - global step: 30; train loss: 8.103416442871094; dev loss: 8.087159156799316
Epoch 0:  20%|██        | 60/300 [00:24<01:34,  2.53it/s]Epoch 0:  20%|██        | 61/300 [00:24<01:30,  2.63it/s]Epoch 0:  21%|██        | 62/300 [00:24<01:30,  2.64it/s]Epoch 0:  21%|██        | 63/300 [00:25<01:26,  2.73it/s]Epoch 0:  21%|██▏       | 64/300 [00:25<01:33,  2.52it/s]Epoch 0:  22%|██▏       | 65/300 [00:25<01:28,  2.64it/s]Epoch 0:  22%|██▏       | 66/300 [00:26<01:26,  2.72it/s]Epoch 0:  22%|██▏       | 67/300 [00:26<01:23,  2.78it/s]Epoch 0:  23%|██▎       | 68/300 [00:27<01:29,  2.61it/s]Epoch 0:  23%|██▎       | 69/300 [00:27<01:29,  2.57it/s]Epoch 0:  23%|██▎       | 70/300 [00:27<01:26,  2.67it/s]Epoch 0:  24%|██▎       | 71/300 [00:28<01:27,  2.62it/s]Epoch 0:  24%|██▍       | 72/300 [00:28<01:27,  2.60it/s]Epoch 0:  24%|██▍       | 73/300 [00:29<01:33,  2.43it/s]Epoch 0:  25%|██▍       | 74/300 [00:29<01:29,  2.53it/s]Epoch 0:  25%|██▌       | 75/300 [00:29<01:26,  2.61it/s]Epoch 0:  25%|██▌       | 76/300 [00:30<01:24,  2.66it/s]Epoch 0:  26%|██▌       | 77/300 [00:30<01:29,  2.50it/s]Epoch 0:  26%|██▌       | 78/300 [00:30<01:25,  2.59it/s]Epoch 0:  26%|██▋       | 79/300 [00:31<01:24,  2.60it/s]06/19/2022 14:28:51 - INFO - __main__ - global step: 40; train loss: 7.49144983291626; dev loss: 7.386211395263672
Epoch 0:  27%|██▋       | 80/300 [00:31<01:23,  2.63it/s]Epoch 0:  27%|██▋       | 81/300 [00:32<01:28,  2.48it/s]Epoch 0:  27%|██▋       | 82/300 [00:32<01:25,  2.56it/s]Epoch 0:  28%|██▊       | 83/300 [00:32<01:22,  2.63it/s]Epoch 0:  28%|██▊       | 84/300 [00:33<01:20,  2.69it/s]Epoch 0:  28%|██▊       | 85/300 [00:33<01:27,  2.46it/s]Epoch 0:  29%|██▊       | 86/300 [00:34<01:24,  2.54it/s]Epoch 0:  29%|██▉       | 87/300 [00:34<01:21,  2.63it/s]Epoch 0:  29%|██▉       | 88/300 [00:34<01:19,  2.67it/s]Epoch 0:  30%|██▉       | 89/300 [00:35<01:24,  2.50it/s]Epoch 0:  30%|███       | 90/300 [00:35<01:21,  2.58it/s]Epoch 0:  30%|███       | 91/300 [00:35<01:18,  2.67it/s]Epoch 0:  31%|███       | 92/300 [00:36<01:17,  2.70it/s]Epoch 0:  31%|███       | 93/300 [00:36<01:22,  2.50it/s]Epoch 0:  31%|███▏      | 94/300 [00:37<01:19,  2.59it/s]Epoch 0:  32%|███▏      | 95/300 [00:37<01:18,  2.61it/s]Epoch 0:  32%|███▏      | 96/300 [00:37<01:16,  2.67it/s]Epoch 0:  32%|███▏      | 97/300 [00:38<01:21,  2.49it/s]Epoch 0:  33%|███▎      | 98/300 [00:38<01:18,  2.57it/s]Epoch 0:  33%|███▎      | 99/300 [00:39<01:16,  2.64it/s]06/19/2022 14:28:59 - INFO - __main__ - global step: 50; train loss: 8.139128684997559; dev loss: 7.984269618988037
Epoch 0:  33%|███▎      | 100/300 [00:39<01:14,  2.69it/s]Epoch 0:  34%|███▎      | 101/300 [00:39<01:13,  2.71it/s]Epoch 0:  34%|███▍      | 102/300 [00:40<01:19,  2.48it/s]Epoch 0:  34%|███▍      | 103/300 [00:40<01:17,  2.56it/s]Epoch 0:  35%|███▍      | 104/300 [00:40<01:14,  2.63it/s]Epoch 0:  35%|███▌      | 105/300 [00:41<01:12,  2.68it/s]Epoch 0:  35%|███▌      | 106/300 [00:41<01:18,  2.46it/s]Epoch 0:  36%|███▌      | 107/300 [00:42<01:17,  2.50it/s]Epoch 0:  36%|███▌      | 108/300 [00:42<01:14,  2.57it/s]Epoch 0:  36%|███▋      | 109/300 [00:42<01:12,  2.63it/s]Epoch 0:  37%|███▋      | 110/300 [00:43<01:16,  2.48it/s]Epoch 0:  37%|███▋      | 111/300 [00:43<01:13,  2.58it/s]Epoch 0:  37%|███▋      | 112/300 [00:44<01:10,  2.66it/s]Epoch 0:  38%|███▊      | 113/300 [00:44<01:08,  2.72it/s]Epoch 0:  38%|███▊      | 114/300 [00:44<01:14,  2.51it/s]Epoch 0:  38%|███▊      | 115/300 [00:45<01:11,  2.58it/s]Epoch 0:  39%|███▊      | 116/300 [00:45<01:09,  2.65it/s]Epoch 0:  39%|███▉      | 117/300 [00:45<01:07,  2.71it/s]Epoch 0:  39%|███▉      | 118/300 [00:46<01:12,  2.53it/s]Epoch 0:  40%|███▉      | 119/300 [00:46<01:09,  2.60it/s]06/19/2022 14:29:06 - INFO - __main__ - global step: 60; train loss: 7.868981838226318; dev loss: 7.956567287445068
Epoch 0:  40%|████      | 120/300 [00:47<01:07,  2.66it/s]Epoch 0:  40%|████      | 121/300 [00:47<01:06,  2.71it/s]Epoch 0:  41%|████      | 122/300 [00:47<01:11,  2.49it/s]Epoch 0:  41%|████      | 123/300 [00:48<01:08,  2.59it/s]Epoch 0:  41%|████▏     | 124/300 [00:48<01:06,  2.66it/s]Epoch 0:  42%|████▏     | 125/300 [00:49<01:04,  2.70it/s]Epoch 0:  42%|████▏     | 126/300 [00:49<01:03,  2.73it/s]Epoch 0:  42%|████▏     | 127/300 [00:49<01:08,  2.52it/s]Epoch 0:  43%|████▎     | 128/300 [00:50<01:06,  2.58it/s]Epoch 0:  43%|████▎     | 129/300 [00:50<01:04,  2.64it/s]Epoch 0:  43%|████▎     | 130/300 [00:50<01:03,  2.68it/s]Epoch 0:  44%|████▎     | 131/300 [00:51<01:07,  2.51it/s]Epoch 0:  44%|████▍     | 132/300 [00:51<01:04,  2.61it/s]Epoch 0:  44%|████▍     | 133/300 [00:52<01:02,  2.65it/s]Epoch 0:  45%|████▍     | 134/300 [00:52<01:01,  2.68it/s]Epoch 0:  45%|████▌     | 135/300 [00:52<01:06,  2.50it/s]Epoch 0:  45%|████▌     | 136/300 [00:53<01:03,  2.59it/s]Epoch 0:  46%|████▌     | 137/300 [00:53<01:01,  2.65it/s]Epoch 0:  46%|████▌     | 138/300 [00:53<00:59,  2.70it/s]Epoch 0:  46%|████▋     | 139/300 [00:54<01:03,  2.52it/s]06/19/2022 14:29:14 - INFO - __main__ - global step: 70; train loss: 8.52284049987793; dev loss: 8.41170597076416
Epoch 0:  47%|████▋     | 140/300 [00:54<01:01,  2.59it/s]Epoch 0:  47%|████▋     | 141/300 [00:55<01:00,  2.65it/s]Epoch 0:  47%|████▋     | 142/300 [00:55<00:59,  2.66it/s]Epoch 0:  48%|████▊     | 143/300 [00:56<01:05,  2.38it/s]Epoch 0:  48%|████▊     | 144/300 [00:56<01:02,  2.51it/s]Epoch 0:  48%|████▊     | 145/300 [00:56<00:59,  2.59it/s]Epoch 0:  49%|████▊     | 146/300 [00:57<00:58,  2.65it/s]Epoch 0:  49%|████▉     | 147/300 [00:57<01:01,  2.48it/s]Epoch 0:  49%|████▉     | 148/300 [00:57<00:59,  2.57it/s]Epoch 0:  50%|████▉     | 149/300 [00:58<00:56,  2.65it/s]Epoch 0:  50%|█████     | 150/300 [00:58<00:55,  2.70it/s]Epoch 0:  50%|█████     | 151/300 [00:59<00:59,  2.52it/s]Epoch 0:  51%|█████     | 152/300 [00:59<00:56,  2.61it/s]Epoch 0:  51%|█████     | 153/300 [00:59<00:54,  2.67it/s]Epoch 0:  51%|█████▏    | 154/300 [01:00<00:53,  2.72it/s]Epoch 0:  52%|█████▏    | 155/300 [01:00<00:52,  2.75it/s]Epoch 0:  52%|█████▏    | 156/300 [01:00<00:57,  2.51it/s]Epoch 0:  52%|█████▏    | 157/300 [01:01<00:55,  2.57it/s]Epoch 0:  53%|█████▎    | 158/300 [01:01<00:54,  2.62it/s]Epoch 0:  53%|█████▎    | 159/300 [01:02<00:53,  2.66it/s]06/19/2022 14:29:22 - INFO - __main__ - global step: 80; train loss: 7.6127214431762695; dev loss: 7.49758768081665
Epoch 0:  53%|█████▎    | 160/300 [01:02<00:56,  2.49it/s]Epoch 0:  54%|█████▎    | 161/300 [01:02<00:54,  2.57it/s]Epoch 0:  54%|█████▍    | 162/300 [01:03<00:52,  2.65it/s]Epoch 0:  54%|█████▍    | 163/300 [01:03<00:51,  2.68it/s]Epoch 0:  55%|█████▍    | 164/300 [01:04<00:54,  2.50it/s]Epoch 0:  55%|█████▌    | 165/300 [01:04<00:52,  2.59it/s]Epoch 0:  55%|█████▌    | 166/300 [01:04<00:51,  2.61it/s]Epoch 0:  56%|█████▌    | 167/300 [01:05<00:49,  2.67it/s]Epoch 0:  56%|█████▌    | 168/300 [01:05<00:53,  2.48it/s]Epoch 0:  56%|█████▋    | 169/300 [01:06<00:51,  2.53it/s]Epoch 0:  57%|█████▋    | 170/300 [01:06<00:49,  2.61it/s]Epoch 0:  57%|█████▋    | 171/300 [01:06<00:48,  2.67it/s]Epoch 0:  57%|█████▋    | 172/300 [01:07<00:51,  2.50it/s]Epoch 0:  58%|█████▊    | 173/300 [01:07<00:49,  2.56it/s]Epoch 0:  58%|█████▊    | 174/300 [01:07<00:48,  2.62it/s]Epoch 0:  58%|█████▊    | 175/300 [01:08<00:47,  2.65it/s]Epoch 0:  59%|█████▊    | 176/300 [01:08<00:50,  2.45it/s]Epoch 0:  59%|█████▉    | 177/300 [01:09<00:48,  2.55it/s]Epoch 0:  59%|█████▉    | 178/300 [01:09<00:46,  2.62it/s]Epoch 0:  60%|█████▉    | 179/300 [01:09<00:45,  2.68it/s]06/19/2022 14:29:29 - INFO - __main__ - global step: 90; train loss: 8.09733772277832; dev loss: 8.095796585083008
Epoch 0:  60%|██████    | 180/300 [01:10<00:44,  2.72it/s]Epoch 0:  60%|██████    | 181/300 [01:10<00:47,  2.50it/s]Epoch 0:  61%|██████    | 182/300 [01:11<00:45,  2.59it/s]Epoch 0:  61%|██████    | 183/300 [01:11<00:43,  2.67it/s]Epoch 0:  61%|██████▏   | 184/300 [01:11<00:42,  2.72it/s]Epoch 0:  62%|██████▏   | 185/300 [01:12<00:45,  2.53it/s]Epoch 0:  62%|██████▏   | 186/300 [01:12<00:44,  2.59it/s]Epoch 0:  62%|██████▏   | 187/300 [01:12<00:42,  2.65it/s]Epoch 0:  63%|██████▎   | 188/300 [01:13<00:41,  2.69it/s]Epoch 0:  63%|██████▎   | 189/300 [01:13<00:45,  2.45it/s]Epoch 0:  63%|██████▎   | 190/300 [01:14<00:43,  2.53it/s]Epoch 0:  64%|██████▎   | 191/300 [01:14<00:41,  2.61it/s]Epoch 0:  64%|██████▍   | 192/300 [01:14<00:40,  2.64it/s]Epoch 0:  64%|██████▍   | 193/300 [01:15<00:43,  2.48it/s]Epoch 0:  65%|██████▍   | 194/300 [01:15<00:41,  2.56it/s]Epoch 0:  65%|██████▌   | 195/300 [01:16<00:39,  2.64it/s]Epoch 0:  65%|██████▌   | 196/300 [01:16<00:38,  2.68it/s]Epoch 0:  66%|██████▌   | 197/300 [01:16<00:41,  2.48it/s]Epoch 0:  66%|██████▌   | 198/300 [01:17<00:40,  2.54it/s]Epoch 0:  66%|██████▋   | 199/300 [01:17<00:38,  2.62it/s]06/19/2022 14:29:37 - INFO - __main__ - global step: 100; train loss: 7.738088130950928; dev loss: 7.117182731628418
Epoch 0:  67%|██████▋   | 200/300 [01:17<00:37,  2.67it/s]Epoch 0:  67%|██████▋   | 201/300 [01:18<00:39,  2.49it/s]Epoch 0:  67%|██████▋   | 202/300 [01:18<00:38,  2.58it/s]Epoch 0:  68%|██████▊   | 203/300 [01:19<00:36,  2.65it/s]Epoch 0:  68%|██████▊   | 204/300 [01:19<00:35,  2.70it/s]Epoch 0:  68%|██████▊   | 205/300 [01:19<00:38,  2.48it/s]Epoch 0:  69%|██████▊   | 206/300 [01:20<00:36,  2.57it/s]Epoch 0:  69%|██████▉   | 207/300 [01:20<00:35,  2.62it/s]Epoch 0:  69%|██████▉   | 208/300 [01:21<00:34,  2.68it/s]Epoch 0:  70%|██████▉   | 209/300 [01:21<00:33,  2.73it/s]Epoch 0:  70%|███████   | 210/300 [01:21<00:35,  2.52it/s]Epoch 0:  70%|███████   | 211/300 [01:22<00:34,  2.59it/s]Epoch 0:  71%|███████   | 212/300 [01:22<00:33,  2.64it/s]Epoch 0:  71%|███████   | 213/300 [01:22<00:32,  2.69it/s]Epoch 0:  71%|███████▏  | 214/300 [01:23<00:34,  2.51it/s]Epoch 0:  72%|███████▏  | 215/300 [01:23<00:32,  2.58it/s]Epoch 0:  72%|███████▏  | 216/300 [01:24<00:31,  2.63it/s]Epoch 0:  72%|███████▏  | 217/300 [01:24<00:31,  2.66it/s]Epoch 0:  73%|███████▎  | 218/300 [01:24<00:33,  2.44it/s]Epoch 0:  73%|███████▎  | 219/300 [01:25<00:32,  2.52it/s]06/19/2022 14:29:45 - INFO - __main__ - global step: 110; train loss: 7.680413722991943; dev loss: 8.123779296875
Epoch 0:  73%|███████▎  | 220/300 [01:25<00:31,  2.58it/s]Epoch 0:  74%|███████▎  | 221/300 [01:26<00:30,  2.62it/s]Epoch 0:  74%|███████▍  | 222/300 [01:26<00:33,  2.33it/s]Epoch 0:  74%|███████▍  | 223/300 [01:26<00:32,  2.39it/s]Epoch 0:  75%|███████▍  | 224/300 [01:27<00:30,  2.53it/s]Epoch 0:  75%|███████▌  | 225/300 [01:27<00:29,  2.52it/s]Epoch 0:  75%|███████▌  | 226/300 [01:28<00:30,  2.42it/s]Epoch 0:  76%|███████▌  | 227/300 [01:28<00:28,  2.55it/s]Epoch 0:  76%|███████▌  | 228/300 [01:28<00:27,  2.59it/s]Epoch 0:  76%|███████▋  | 229/300 [01:29<00:26,  2.66it/s]Epoch 0:  77%|███████▋  | 230/300 [01:29<00:27,  2.53it/s]Epoch 0:  77%|███████▋  | 231/300 [01:30<00:27,  2.52it/s]Epoch 0:  77%|███████▋  | 232/300 [01:30<00:26,  2.61it/s]Epoch 0:  78%|███████▊  | 233/300 [01:30<00:24,  2.68it/s]Epoch 0:  78%|███████▊  | 234/300 [01:31<00:24,  2.75it/s]Epoch 0:  78%|███████▊  | 235/300 [01:31<00:25,  2.59it/s]Epoch 0:  79%|███████▊  | 236/300 [01:31<00:24,  2.60it/s]Epoch 0:  79%|███████▉  | 237/300 [01:32<00:23,  2.64it/s]Epoch 0:  79%|███████▉  | 238/300 [01:32<00:23,  2.69it/s]Epoch 0:  80%|███████▉  | 239/300 [01:33<00:23,  2.56it/s]06/19/2022 14:29:53 - INFO - __main__ - global step: 120; train loss: 7.200382232666016; dev loss: 7.347708225250244
Epoch 0:  80%|████████  | 240/300 [01:33<00:23,  2.54it/s]Epoch 0:  80%|████████  | 241/300 [01:33<00:22,  2.61it/s]Epoch 0:  81%|████████  | 242/300 [01:34<00:22,  2.58it/s]Epoch 0:  81%|████████  | 243/300 [01:34<00:23,  2.45it/s]Epoch 0:  81%|████████▏ | 244/300 [01:35<00:21,  2.58it/s]Epoch 0:  82%|████████▏ | 245/300 [01:35<00:21,  2.61it/s]Epoch 0:  82%|████████▏ | 246/300 [01:35<00:20,  2.70it/s]Epoch 0:  82%|████████▏ | 247/300 [01:36<00:20,  2.56it/s]Epoch 0:  83%|████████▎ | 248/300 [01:36<00:19,  2.64it/s]Epoch 0:  83%|████████▎ | 249/300 [01:36<00:18,  2.72it/s]Epoch 0:  83%|████████▎ | 250/300 [01:37<00:18,  2.72it/s]Epoch 0:  84%|████████▎ | 251/300 [01:37<00:19,  2.55it/s]Epoch 0:  84%|████████▍ | 252/300 [01:38<00:18,  2.65it/s]Epoch 0:  84%|████████▍ | 253/300 [01:38<00:18,  2.60it/s]Epoch 0:  85%|████████▍ | 254/300 [01:38<00:17,  2.64it/s]Epoch 0:  85%|████████▌ | 255/300 [01:39<00:18,  2.48it/s]Epoch 0:  85%|████████▌ | 256/300 [01:39<00:17,  2.45it/s]Epoch 0:  86%|████████▌ | 257/300 [01:40<00:16,  2.56it/s]Epoch 0:  86%|████████▌ | 258/300 [01:40<00:15,  2.66it/s]Epoch 0:  86%|████████▋ | 259/300 [01:40<00:16,  2.48it/s]06/19/2022 14:30:00 - INFO - __main__ - global step: 130; train loss: 7.469138145446777; dev loss: 7.4183855056762695
Epoch 0:  87%|████████▋ | 260/300 [01:41<00:16,  2.48it/s]Epoch 0:  87%|████████▋ | 261/300 [01:41<00:15,  2.46it/s]Epoch 0:  87%|████████▋ | 262/300 [01:42<00:15,  2.46it/s]Epoch 0:  88%|████████▊ | 263/300 [01:42<00:14,  2.52it/s]Epoch 0:  88%|████████▊ | 264/300 [01:42<00:15,  2.33it/s]Epoch 0:  88%|████████▊ | 265/300 [01:43<00:14,  2.38it/s]Epoch 0:  89%|████████▊ | 266/300 [01:43<00:13,  2.52it/s]Epoch 0:  89%|████████▉ | 267/300 [01:44<00:12,  2.64it/s]Epoch 0:  89%|████████▉ | 268/300 [01:44<00:12,  2.54it/s]Epoch 0:  90%|████████▉ | 269/300 [01:44<00:11,  2.60it/s]Epoch 0:  90%|█████████ | 270/300 [01:45<00:11,  2.68it/s]Epoch 0:  90%|█████████ | 271/300 [01:45<00:10,  2.67it/s]Epoch 0:  91%|█████████ | 272/300 [01:45<00:11,  2.52it/s]Epoch 0:  91%|█████████ | 273/300 [01:46<00:10,  2.63it/s]Epoch 0:  91%|█████████▏| 274/300 [01:46<00:09,  2.72it/s]Epoch 0:  92%|█████████▏| 275/300 [01:47<00:09,  2.66it/s]Epoch 0:  92%|█████████▏| 276/300 [01:47<00:09,  2.47it/s]Epoch 0:  92%|█████████▏| 277/300 [01:47<00:08,  2.57it/s]Epoch 0:  93%|█████████▎| 278/300 [01:48<00:08,  2.68it/s]Epoch 0:  93%|█████████▎| 279/300 [01:48<00:07,  2.70it/s]06/19/2022 14:30:08 - INFO - __main__ - global step: 140; train loss: 8.03734016418457; dev loss: 8.124882698059082
Epoch 0:  93%|█████████▎| 280/300 [01:49<00:07,  2.55it/s]Epoch 0:  94%|█████████▎| 281/300 [01:49<00:07,  2.54it/s]Epoch 0:  94%|█████████▍| 282/300 [01:49<00:07,  2.55it/s]Epoch 0:  94%|█████████▍| 283/300 [01:50<00:06,  2.60it/s]Epoch 0:  95%|█████████▍| 284/300 [01:50<00:06,  2.49it/s]Epoch 0:  95%|█████████▌| 285/300 [01:50<00:05,  2.56it/s]Epoch 0:  95%|█████████▌| 286/300 [01:51<00:05,  2.53it/s]Epoch 0:  96%|█████████▌| 287/300 [01:51<00:04,  2.61it/s]Epoch 0:  96%|█████████▌| 288/300 [01:52<00:04,  2.62it/s]Epoch 0:  96%|█████████▋| 289/300 [01:52<00:04,  2.45it/s]Epoch 0:  97%|█████████▋| 290/300 [01:52<00:03,  2.51it/s]Epoch 0:  97%|█████████▋| 291/300 [01:53<00:03,  2.62it/s]Epoch 0:  97%|█████████▋| 292/300 [01:53<00:02,  2.71it/s]Epoch 0:  98%|█████████▊| 293/300 [01:54<00:02,  2.57it/s]Epoch 0:  98%|█████████▊| 294/300 [01:54<00:02,  2.53it/s]Epoch 0:  98%|█████████▊| 295/300 [01:54<00:01,  2.62it/s]Epoch 0:  99%|█████████▊| 296/300 [01:55<00:01,  2.69it/s]Epoch 0:  99%|█████████▉| 297/300 [01:55<00:01,  2.56it/s]Epoch 0:  99%|█████████▉| 298/300 [01:56<00:00,  2.60it/s]Epoch 0: 100%|█████████▉| 299/300 [01:56<00:00,  2.67it/s]06/19/2022 14:30:16 - INFO - __main__ - global step: 150; train loss: 7.650745391845703; dev loss: 7.588461399078369
Epoch 0: 100%|██████████| 300/300 [01:56<00:00,  2.73it/s]Epoch 0: 100%|██████████| 300/300 [01:56<00:00,  2.57it/s]
Epoch 1:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 1:   0%|          | 1/300 [00:00<02:10,  2.29it/s]Epoch 1:   1%|          | 2/300 [00:00<01:53,  2.62it/s]Epoch 1:   1%|          | 3/300 [00:01<01:56,  2.56it/s]Epoch 1:   1%|▏         | 4/300 [00:01<01:51,  2.65it/s]Epoch 1:   2%|▏         | 5/300 [00:02<02:03,  2.38it/s]Epoch 1:   2%|▏         | 6/300 [00:02<01:56,  2.52it/s]Epoch 1:   2%|▏         | 7/300 [00:02<01:53,  2.57it/s]Epoch 1:   3%|▎         | 8/300 [00:03<01:49,  2.68it/s]Epoch 1:   3%|▎         | 9/300 [00:03<01:58,  2.46it/s]Epoch 1:   3%|▎         | 10/300 [00:03<01:51,  2.59it/s]Epoch 1:   4%|▎         | 11/300 [00:04<01:52,  2.56it/s]Epoch 1:   4%|▍         | 12/300 [00:04<01:54,  2.52it/s]Epoch 1:   4%|▍         | 13/300 [00:05<01:56,  2.47it/s]Epoch 1:   5%|▍         | 14/300 [00:05<01:59,  2.39it/s]Epoch 1:   5%|▌         | 15/300 [00:05<01:52,  2.54it/s]Epoch 1:   5%|▌         | 16/300 [00:06<01:50,  2.57it/s]Epoch 1:   6%|▌         | 17/300 [00:06<01:46,  2.66it/s]Epoch 1:   6%|▌         | 18/300 [00:07<01:52,  2.52it/s]Epoch 1:   6%|▋         | 19/300 [00:07<01:47,  2.62it/s]06/19/2022 14:30:24 - INFO - __main__ - global step: 160; train loss: 8.013177871704102; dev loss: 8.049175262451172
Epoch 1:   7%|▋         | 20/300 [00:07<01:43,  2.71it/s]Epoch 1:   7%|▋         | 21/300 [00:08<01:40,  2.78it/s]Epoch 1:   7%|▋         | 22/300 [00:08<01:46,  2.62it/s]Epoch 1:   8%|▊         | 23/300 [00:08<01:43,  2.69it/s]Epoch 1:   8%|▊         | 24/300 [00:09<01:40,  2.76it/s]Epoch 1:   8%|▊         | 25/300 [00:09<01:37,  2.81it/s]Epoch 1:   9%|▊         | 26/300 [00:10<01:43,  2.63it/s]Epoch 1:   9%|▉         | 27/300 [00:10<01:40,  2.72it/s]Epoch 1:   9%|▉         | 28/300 [00:10<01:37,  2.78it/s]Epoch 1:  10%|▉         | 29/300 [00:11<01:36,  2.82it/s]Epoch 1:  10%|█         | 30/300 [00:11<01:42,  2.64it/s]Epoch 1:  10%|█         | 31/300 [00:11<01:41,  2.64it/s]Epoch 1:  11%|█         | 32/300 [00:12<01:38,  2.72it/s]Epoch 1:  11%|█         | 33/300 [00:12<01:38,  2.70it/s]Epoch 1:  11%|█▏        | 34/300 [00:13<01:46,  2.50it/s]Epoch 1:  12%|█▏        | 35/300 [00:13<01:41,  2.60it/s]Epoch 1:  12%|█▏        | 36/300 [00:13<01:40,  2.62it/s]Epoch 1:  12%|█▏        | 37/300 [00:14<01:36,  2.71it/s]Epoch 1:  13%|█▎        | 38/300 [00:14<01:41,  2.57it/s]Epoch 1:  13%|█▎        | 39/300 [00:14<01:37,  2.67it/s]06/19/2022 14:30:31 - INFO - __main__ - global step: 170; train loss: 7.815533638000488; dev loss: 8.025541305541992
Epoch 1:  13%|█▎        | 40/300 [00:15<01:35,  2.73it/s]Epoch 1:  14%|█▎        | 41/300 [00:15<01:32,  2.79it/s]Epoch 1:  14%|█▍        | 42/300 [00:15<01:31,  2.81it/s]Epoch 1:  14%|█▍        | 43/300 [00:16<01:37,  2.63it/s]Epoch 1:  15%|█▍        | 44/300 [00:16<01:35,  2.68it/s]Epoch 1:  15%|█▌        | 45/300 [00:17<01:32,  2.75it/s]Epoch 1:  15%|█▌        | 46/300 [00:17<01:31,  2.79it/s]Epoch 1:  16%|█▌        | 47/300 [00:17<01:36,  2.62it/s]Epoch 1:  16%|█▌        | 48/300 [00:18<01:37,  2.58it/s]Epoch 1:  16%|█▋        | 49/300 [00:18<01:36,  2.59it/s]Epoch 1:  17%|█▋        | 50/300 [00:19<01:36,  2.59it/s]Epoch 1:  17%|█▋        | 51/300 [00:19<01:42,  2.43it/s]Epoch 1:  17%|█▋        | 52/300 [00:19<01:37,  2.56it/s]Epoch 1:  18%|█▊        | 53/300 [00:20<01:37,  2.53it/s]Epoch 1:  18%|█▊        | 54/300 [00:20<01:35,  2.57it/s]Epoch 1:  18%|█▊        | 55/300 [00:21<01:41,  2.41it/s]Epoch 1:  19%|█▊        | 56/300 [00:21<01:36,  2.53it/s]Epoch 1:  19%|█▉        | 57/300 [00:21<01:33,  2.60it/s]Epoch 1:  19%|█▉        | 58/300 [00:22<01:30,  2.68it/s]Epoch 1:  20%|█▉        | 59/300 [00:22<01:36,  2.49it/s]06/19/2022 14:30:39 - INFO - __main__ - global step: 180; train loss: 8.167181968688965; dev loss: 8.299748420715332
Epoch 1:  20%|██        | 60/300 [00:22<01:33,  2.56it/s]Epoch 1:  20%|██        | 61/300 [00:23<01:30,  2.65it/s]Epoch 1:  21%|██        | 62/300 [00:23<01:27,  2.73it/s]Epoch 1:  21%|██        | 63/300 [00:24<01:36,  2.46it/s]Epoch 1:  21%|██▏       | 64/300 [00:24<01:32,  2.55it/s]Epoch 1:  22%|██▏       | 65/300 [00:24<01:28,  2.65it/s]Epoch 1:  22%|██▏       | 66/300 [00:25<01:26,  2.72it/s]Epoch 1:  22%|██▏       | 67/300 [00:25<01:25,  2.72it/s]Epoch 1:  23%|██▎       | 68/300 [00:26<01:30,  2.56it/s]Epoch 1:  23%|██▎       | 69/300 [00:26<01:28,  2.60it/s]Epoch 1:  23%|██▎       | 70/300 [00:26<01:28,  2.59it/s]Epoch 1:  24%|██▎       | 71/300 [00:27<01:25,  2.68it/s]Epoch 1:  24%|██▍       | 72/300 [00:27<01:34,  2.42it/s]Epoch 1:  24%|██▍       | 73/300 [00:27<01:29,  2.52it/s]Epoch 1:  25%|██▍       | 74/300 [00:28<01:30,  2.51it/s]Epoch 1:  25%|██▌       | 75/300 [00:28<01:26,  2.59it/s]Epoch 1:  25%|██▌       | 76/300 [00:29<01:29,  2.50it/s]Epoch 1:  26%|██▌       | 77/300 [00:29<01:29,  2.48it/s]Epoch 1:  26%|██▌       | 78/300 [00:30<01:30,  2.46it/s]Epoch 1:  26%|██▋       | 79/300 [00:30<01:28,  2.50it/s]06/19/2022 14:30:47 - INFO - __main__ - global step: 190; train loss: 7.305569648742676; dev loss: 7.795590877532959
Epoch 1:  27%|██▋       | 80/300 [00:30<01:32,  2.39it/s]Epoch 1:  27%|██▋       | 81/300 [00:31<01:27,  2.50it/s]Epoch 1:  27%|██▋       | 82/300 [00:31<01:26,  2.51it/s]Epoch 1:  28%|██▊       | 83/300 [00:32<01:27,  2.49it/s]Epoch 1:  28%|██▊       | 84/300 [00:32<01:33,  2.32it/s]Epoch 1:  28%|██▊       | 85/300 [00:32<01:31,  2.35it/s]Epoch 1:  29%|██▊       | 86/300 [00:33<01:30,  2.38it/s]Epoch 1:  29%|██▉       | 87/300 [00:33<01:25,  2.50it/s]Epoch 1:  29%|██▉       | 88/300 [00:34<01:27,  2.41it/s]Epoch 1:  30%|██▉       | 89/300 [00:34<01:26,  2.44it/s]Epoch 1:  30%|███       | 90/300 [00:34<01:22,  2.55it/s]Epoch 1:  30%|███       | 91/300 [00:35<01:22,  2.54it/s]Epoch 1:  31%|███       | 92/300 [00:35<01:25,  2.44it/s]Epoch 1:  31%|███       | 93/300 [00:36<01:24,  2.46it/s]Epoch 1:  31%|███▏      | 94/300 [00:36<01:23,  2.45it/s]Epoch 1:  32%|███▏      | 95/300 [00:36<01:19,  2.56it/s]Epoch 1:  32%|███▏      | 96/300 [00:37<01:20,  2.54it/s]Epoch 1:  32%|███▏      | 97/300 [00:37<01:25,  2.38it/s]Epoch 1:  33%|███▎      | 98/300 [00:38<01:20,  2.52it/s]Epoch 1:  33%|███▎      | 99/300 [00:38<01:19,  2.52it/s]06/19/2022 14:30:55 - INFO - __main__ - global step: 200; train loss: 7.607867240905762; dev loss: 7.471767425537109
Epoch 1:  33%|███▎      | 100/300 [00:38<01:16,  2.62it/s]Epoch 1:  34%|███▎      | 101/300 [00:39<01:18,  2.52it/s]Epoch 1:  34%|███▍      | 102/300 [00:39<01:16,  2.57it/s]Epoch 1:  34%|███▍      | 103/300 [00:40<01:13,  2.67it/s]Epoch 1:  35%|███▍      | 104/300 [00:40<01:12,  2.69it/s]Epoch 1:  35%|███▌      | 105/300 [00:40<01:18,  2.48it/s]Epoch 1:  35%|███▌      | 106/300 [00:41<01:14,  2.59it/s]Epoch 1:  36%|███▌      | 107/300 [00:41<01:11,  2.69it/s]Epoch 1:  36%|███▌      | 108/300 [00:41<01:09,  2.76it/s]Epoch 1:  36%|███▋      | 109/300 [00:42<01:16,  2.48it/s]Epoch 1:  37%|███▋      | 110/300 [00:42<01:13,  2.58it/s]Epoch 1:  37%|███▋      | 111/300 [00:43<01:12,  2.61it/s]Epoch 1:  37%|███▋      | 112/300 [00:43<01:10,  2.68it/s]Epoch 1:  38%|███▊      | 113/300 [00:43<01:13,  2.55it/s]Epoch 1:  38%|███▊      | 114/300 [00:44<01:13,  2.54it/s]Epoch 1:  38%|███▊      | 115/300 [00:44<01:10,  2.62it/s]Epoch 1:  39%|███▊      | 116/300 [00:44<01:09,  2.65it/s]Epoch 1:  39%|███▉      | 117/300 [00:45<01:14,  2.46it/s]Epoch 1:  39%|███▉      | 118/300 [00:45<01:11,  2.56it/s]Epoch 1:  40%|███▉      | 119/300 [00:46<01:10,  2.55it/s]06/19/2022 14:31:02 - INFO - __main__ - global step: 210; train loss: 7.299381256103516; dev loss: 7.443155765533447
Epoch 1:  40%|████      | 120/300 [00:46<01:10,  2.56it/s]Epoch 1:  40%|████      | 121/300 [00:47<01:10,  2.54it/s]Epoch 1:  41%|████      | 122/300 [00:47<01:12,  2.45it/s]Epoch 1:  41%|████      | 123/300 [00:47<01:08,  2.59it/s]Epoch 1:  41%|████▏     | 124/300 [00:48<01:05,  2.68it/s]Epoch 1:  42%|████▏     | 125/300 [00:48<01:03,  2.75it/s]Epoch 1:  42%|████▏     | 126/300 [00:48<01:10,  2.47it/s]Epoch 1:  42%|████▏     | 127/300 [00:49<01:09,  2.48it/s]Epoch 1:  43%|████▎     | 128/300 [00:49<01:06,  2.57it/s]Epoch 1:  43%|████▎     | 129/300 [00:50<01:05,  2.62it/s]Epoch 1:  43%|████▎     | 130/300 [00:50<01:07,  2.53it/s]Epoch 1:  44%|████▎     | 131/300 [00:50<01:04,  2.63it/s]Epoch 1:  44%|████▍     | 132/300 [00:51<01:02,  2.70it/s]Epoch 1:  44%|████▍     | 133/300 [00:51<01:00,  2.76it/s]Epoch 1:  45%|████▍     | 134/300 [00:52<01:04,  2.57it/s]Epoch 1:  45%|████▌     | 135/300 [00:52<01:04,  2.54it/s]Epoch 1:  45%|████▌     | 136/300 [00:52<01:02,  2.63it/s]Epoch 1:  46%|████▌     | 137/300 [00:53<01:00,  2.71it/s]Epoch 1:  46%|████▌     | 138/300 [00:53<01:03,  2.57it/s]Epoch 1:  46%|████▋     | 139/300 [00:53<01:00,  2.65it/s]06/19/2022 14:31:10 - INFO - __main__ - global step: 220; train loss: 7.65295934677124; dev loss: 7.432228088378906
Epoch 1:  47%|████▋     | 140/300 [00:54<00:58,  2.73it/s]Epoch 1:  47%|████▋     | 141/300 [00:54<01:00,  2.63it/s]Epoch 1:  47%|████▋     | 142/300 [00:55<01:03,  2.50it/s]Epoch 1:  48%|████▊     | 143/300 [00:55<01:00,  2.61it/s]Epoch 1:  48%|████▊     | 144/300 [00:55<00:57,  2.69it/s]Epoch 1:  48%|████▊     | 145/300 [00:56<00:56,  2.76it/s]Epoch 1:  49%|████▊     | 146/300 [00:56<00:59,  2.59it/s]Epoch 1:  49%|████▉     | 147/300 [00:56<00:57,  2.68it/s]Epoch 1:  49%|████▉     | 148/300 [00:57<00:55,  2.74it/s]Epoch 1:  50%|████▉     | 149/300 [00:57<00:56,  2.66it/s]Epoch 1:  50%|█████     | 150/300 [00:58<00:56,  2.64it/s]Epoch 1:  50%|█████     | 151/300 [00:58<01:02,  2.40it/s]Epoch 1:  51%|█████     | 152/300 [00:58<01:01,  2.41it/s]Epoch 1:  51%|█████     | 153/300 [00:59<00:58,  2.53it/s]Epoch 1:  51%|█████▏    | 154/300 [00:59<00:55,  2.63it/s]Epoch 1:  52%|█████▏    | 155/300 [01:00<00:57,  2.53it/s]Epoch 1:  52%|█████▏    | 156/300 [01:00<00:54,  2.64it/s]Epoch 1:  52%|█████▏    | 157/300 [01:00<00:52,  2.72it/s]Epoch 1:  53%|█████▎    | 158/300 [01:01<00:50,  2.79it/s]Epoch 1:  53%|█████▎    | 159/300 [01:01<00:53,  2.63it/s]06/19/2022 14:31:18 - INFO - __main__ - global step: 230; train loss: 7.416999816894531; dev loss: 7.757455348968506
Epoch 1:  53%|█████▎    | 160/300 [01:01<00:51,  2.71it/s]Epoch 1:  54%|█████▎    | 161/300 [01:02<00:50,  2.77it/s]Epoch 1:  54%|█████▍    | 162/300 [01:02<00:49,  2.81it/s]Epoch 1:  54%|█████▍    | 163/300 [01:03<00:53,  2.58it/s]Epoch 1:  55%|█████▍    | 164/300 [01:03<00:50,  2.68it/s]Epoch 1:  55%|█████▌    | 165/300 [01:03<00:49,  2.75it/s]Epoch 1:  55%|█████▌    | 166/300 [01:04<00:47,  2.80it/s]Epoch 1:  56%|█████▌    | 167/300 [01:04<00:50,  2.63it/s]Epoch 1:  56%|█████▌    | 168/300 [01:04<00:51,  2.59it/s]Epoch 1:  56%|█████▋    | 169/300 [01:05<00:51,  2.56it/s]Epoch 1:  57%|█████▋    | 170/300 [01:05<00:51,  2.53it/s]Epoch 1:  57%|█████▋    | 171/300 [01:06<00:55,  2.35it/s]Epoch 1:  57%|█████▋    | 172/300 [01:06<00:51,  2.48it/s]Epoch 1:  58%|█████▊    | 173/300 [01:06<00:48,  2.60it/s]Epoch 1:  58%|█████▊    | 174/300 [01:07<00:46,  2.68it/s]Epoch 1:  58%|█████▊    | 175/300 [01:07<00:45,  2.74it/s]Epoch 1:  59%|█████▊    | 176/300 [01:08<00:49,  2.52it/s]Epoch 1:  59%|█████▉    | 177/300 [01:08<00:46,  2.64it/s]Epoch 1:  59%|█████▉    | 178/300 [01:08<00:44,  2.71it/s]Epoch 1:  60%|█████▉    | 179/300 [01:09<00:43,  2.76it/s]06/19/2022 14:31:25 - INFO - __main__ - global step: 240; train loss: 7.707505702972412; dev loss: 7.753231048583984
Epoch 1:  60%|██████    | 180/300 [01:09<00:46,  2.60it/s]Epoch 1:  60%|██████    | 181/300 [01:09<00:44,  2.69it/s]Epoch 1:  61%|██████    | 182/300 [01:10<00:42,  2.76it/s]Epoch 1:  61%|██████    | 183/300 [01:10<00:41,  2.81it/s]Epoch 1:  61%|██████▏   | 184/300 [01:10<00:44,  2.63it/s]Epoch 1:  62%|██████▏   | 185/300 [01:11<00:42,  2.70it/s]Epoch 1:  62%|██████▏   | 186/300 [01:11<00:42,  2.69it/s]Epoch 1:  62%|██████▏   | 187/300 [01:12<00:41,  2.75it/s]Epoch 1:  63%|██████▎   | 188/300 [01:12<00:43,  2.59it/s]Epoch 1:  63%|██████▎   | 189/300 [01:12<00:43,  2.57it/s]Epoch 1:  63%|██████▎   | 190/300 [01:13<00:43,  2.53it/s]Epoch 1:  64%|██████▎   | 191/300 [01:13<00:41,  2.61it/s]Epoch 1:  64%|██████▍   | 192/300 [01:14<00:44,  2.41it/s]Epoch 1:  64%|██████▍   | 193/300 [01:14<00:42,  2.54it/s]Epoch 1:  65%|██████▍   | 194/300 [01:14<00:41,  2.54it/s]Epoch 1:  65%|██████▌   | 195/300 [01:15<00:39,  2.63it/s]Epoch 1:  65%|██████▌   | 196/300 [01:15<00:42,  2.47it/s]Epoch 1:  66%|██████▌   | 197/300 [01:16<00:41,  2.47it/s]Epoch 1:  66%|██████▌   | 198/300 [01:16<00:40,  2.51it/s]Epoch 1:  66%|██████▋   | 199/300 [01:16<00:39,  2.58it/s]06/19/2022 14:31:33 - INFO - __main__ - global step: 250; train loss: 8.05528450012207; dev loss: 8.237037658691406
Epoch 1:  67%|██████▋   | 200/300 [01:17<00:40,  2.46it/s]Epoch 1:  67%|██████▋   | 201/300 [01:17<00:39,  2.54it/s]Epoch 1:  67%|██████▋   | 202/300 [01:18<00:37,  2.58it/s]Epoch 1:  68%|██████▊   | 203/300 [01:18<00:36,  2.65it/s]Epoch 1:  68%|██████▊   | 204/300 [01:18<00:37,  2.59it/s]Epoch 1:  68%|██████▊   | 205/300 [01:19<00:39,  2.38it/s]Epoch 1:  69%|██████▊   | 206/300 [01:19<00:37,  2.50it/s]Epoch 1:  69%|██████▉   | 207/300 [01:19<00:35,  2.62it/s]Epoch 1:  69%|██████▉   | 208/300 [01:20<00:33,  2.71it/s]Epoch 1:  70%|██████▉   | 209/300 [01:20<00:35,  2.57it/s]Epoch 1:  70%|███████   | 210/300 [01:21<00:33,  2.68it/s]Epoch 1:  70%|███████   | 211/300 [01:21<00:33,  2.63it/s]Epoch 1:  71%|███████   | 212/300 [01:21<00:32,  2.69it/s]Epoch 1:  71%|███████   | 213/300 [01:22<00:33,  2.57it/s]Epoch 1:  71%|███████▏  | 214/300 [01:22<00:32,  2.67it/s]Epoch 1:  72%|███████▏  | 215/300 [01:22<00:31,  2.72it/s]Epoch 1:  72%|███████▏  | 216/300 [01:23<00:31,  2.64it/s]Epoch 1:  72%|███████▏  | 217/300 [01:23<00:33,  2.50it/s]Epoch 1:  73%|███████▎  | 218/300 [01:24<00:31,  2.60it/s]Epoch 1:  73%|███████▎  | 219/300 [01:24<00:31,  2.58it/s]06/19/2022 14:31:41 - INFO - __main__ - global step: 260; train loss: 8.297712326049805; dev loss: 8.338946342468262
Epoch 1:  73%|███████▎  | 220/300 [01:24<00:30,  2.66it/s]Epoch 1:  74%|███████▎  | 221/300 [01:25<00:30,  2.55it/s]Epoch 1:  74%|███████▍  | 222/300 [01:25<00:29,  2.66it/s]Epoch 1:  74%|███████▍  | 223/300 [01:26<00:29,  2.62it/s]Epoch 1:  75%|███████▍  | 224/300 [01:26<00:29,  2.57it/s]Epoch 1:  75%|███████▌  | 225/300 [01:26<00:30,  2.46it/s]Epoch 1:  75%|███████▌  | 226/300 [01:27<00:29,  2.47it/s]Epoch 1:  76%|███████▌  | 227/300 [01:27<00:28,  2.56it/s]Epoch 1:  76%|███████▌  | 228/300 [01:28<00:27,  2.66it/s]Epoch 1:  76%|███████▋  | 229/300 [01:28<00:27,  2.61it/s]Epoch 1:  77%|███████▋  | 230/300 [01:28<00:29,  2.37it/s]Epoch 1:  77%|███████▋  | 231/300 [01:29<00:27,  2.50it/s]Epoch 1:  77%|███████▋  | 232/300 [01:29<00:25,  2.62it/s]Epoch 1:  78%|███████▊  | 233/300 [01:29<00:24,  2.71it/s]Epoch 1:  78%|███████▊  | 234/300 [01:30<00:25,  2.58it/s]Epoch 1:  78%|███████▊  | 235/300 [01:30<00:24,  2.68it/s]Epoch 1:  79%|███████▊  | 236/300 [01:31<00:23,  2.76it/s]Epoch 1:  79%|███████▉  | 237/300 [01:31<00:22,  2.74it/s]Epoch 1:  79%|███████▉  | 238/300 [01:31<00:24,  2.57it/s]Epoch 1:  80%|███████▉  | 239/300 [01:32<00:22,  2.68it/s]06/19/2022 14:31:48 - INFO - __main__ - global step: 270; train loss: 8.023836135864258; dev loss: 7.8808417320251465
Epoch 1:  80%|████████  | 240/300 [01:32<00:21,  2.75it/s]Epoch 1:  80%|████████  | 241/300 [01:32<00:21,  2.68it/s]Epoch 1:  81%|████████  | 242/300 [01:33<00:23,  2.49it/s]Epoch 1:  81%|████████  | 243/300 [01:33<00:22,  2.52it/s]Epoch 1:  81%|████████▏ | 244/300 [01:34<00:21,  2.61it/s]Epoch 1:  82%|████████▏ | 245/300 [01:34<00:21,  2.58it/s]Epoch 1:  82%|████████▏ | 246/300 [01:34<00:21,  2.47it/s]Epoch 1:  82%|████████▏ | 247/300 [01:35<00:20,  2.59it/s]Epoch 1:  83%|████████▎ | 248/300 [01:35<00:19,  2.68it/s]Epoch 1:  83%|████████▎ | 249/300 [01:36<00:18,  2.75it/s]Epoch 1:  83%|████████▎ | 250/300 [01:36<00:19,  2.52it/s]Epoch 1:  84%|████████▎ | 251/300 [01:36<00:18,  2.64it/s]Epoch 1:  84%|████████▍ | 252/300 [01:37<00:17,  2.72it/s]Epoch 1:  84%|████████▍ | 253/300 [01:37<00:16,  2.79it/s]Epoch 1:  85%|████████▍ | 254/300 [01:37<00:17,  2.60it/s]Epoch 1:  85%|████████▌ | 255/300 [01:38<00:16,  2.69it/s]Epoch 1:  85%|████████▌ | 256/300 [01:38<00:16,  2.75it/s]Epoch 1:  86%|████████▌ | 257/300 [01:39<00:16,  2.67it/s]Epoch 1:  86%|████████▌ | 258/300 [01:39<00:15,  2.72it/s]Epoch 1:  86%|████████▋ | 259/300 [01:39<00:16,  2.52it/s]06/19/2022 14:31:56 - INFO - __main__ - global step: 280; train loss: 8.332959175109863; dev loss: 7.896489143371582
Epoch 1:  87%|████████▋ | 260/300 [01:40<00:15,  2.61it/s]Epoch 1:  87%|████████▋ | 261/300 [01:40<00:14,  2.71it/s]Epoch 1:  87%|████████▋ | 262/300 [01:40<00:13,  2.77it/s]Epoch 1:  88%|████████▊ | 263/300 [01:41<00:14,  2.49it/s]Epoch 1:  88%|████████▊ | 264/300 [01:41<00:13,  2.59it/s]Epoch 1:  88%|████████▊ | 265/300 [01:42<00:13,  2.69it/s]Epoch 1:  89%|████████▊ | 266/300 [01:42<00:12,  2.77it/s]Epoch 1:  89%|████████▉ | 267/300 [01:42<00:12,  2.61it/s]Epoch 1:  89%|████████▉ | 268/300 [01:43<00:11,  2.70it/s]Epoch 1:  90%|████████▉ | 269/300 [01:43<00:11,  2.77it/s]Epoch 1:  90%|█████████ | 270/300 [01:43<00:10,  2.83it/s]Epoch 1:  90%|█████████ | 271/300 [01:44<00:10,  2.65it/s]Epoch 1:  91%|█████████ | 272/300 [01:44<00:10,  2.60it/s]Epoch 1:  91%|█████████ | 273/300 [01:45<00:10,  2.65it/s]Epoch 1:  91%|█████████▏| 274/300 [01:45<00:09,  2.61it/s]Epoch 1:  92%|█████████▏| 275/300 [01:45<00:10,  2.49it/s]Epoch 1:  92%|█████████▏| 276/300 [01:46<00:09,  2.56it/s]Epoch 1:  92%|█████████▏| 277/300 [01:46<00:08,  2.64it/s]Epoch 1:  93%|█████████▎| 278/300 [01:46<00:08,  2.71it/s]Epoch 1:  93%|█████████▎| 279/300 [01:47<00:08,  2.58it/s]06/19/2022 14:32:04 - INFO - __main__ - global step: 290; train loss: 8.084030151367188; dev loss: 8.18777847290039
Epoch 1:  93%|█████████▎| 280/300 [01:47<00:07,  2.63it/s]Epoch 1:  94%|█████████▎| 281/300 [01:48<00:07,  2.70it/s]Epoch 1:  94%|█████████▍| 282/300 [01:48<00:06,  2.76it/s]Epoch 1:  94%|█████████▍| 283/300 [01:48<00:06,  2.81it/s]Epoch 1:  95%|█████████▍| 284/300 [01:49<00:06,  2.61it/s]Epoch 1:  95%|█████████▌| 285/300 [01:49<00:05,  2.70it/s]Epoch 1:  95%|█████████▌| 286/300 [01:49<00:05,  2.76it/s]Epoch 1:  96%|█████████▌| 287/300 [01:50<00:04,  2.81it/s]Epoch 1:  96%|█████████▌| 288/300 [01:50<00:04,  2.53it/s]Epoch 1:  96%|█████████▋| 289/300 [01:51<00:04,  2.56it/s]Epoch 1:  97%|█████████▋| 290/300 [01:51<00:03,  2.56it/s]Epoch 1:  97%|█████████▋| 291/300 [01:51<00:03,  2.64it/s]Epoch 1:  97%|█████████▋| 292/300 [01:52<00:03,  2.44it/s]Epoch 1:  98%|█████████▊| 293/300 [01:52<00:02,  2.44it/s]Epoch 1:  98%|█████████▊| 294/300 [01:53<00:02,  2.50it/s]Epoch 1:  98%|█████████▊| 295/300 [01:53<00:01,  2.60it/s]Epoch 1:  99%|█████████▊| 296/300 [01:53<00:01,  2.50it/s]Epoch 1:  99%|█████████▉| 297/300 [01:54<00:01,  2.50it/s]Epoch 1:  99%|█████████▉| 298/300 [01:54<00:00,  2.59it/s]Epoch 1: 100%|█████████▉| 299/300 [01:55<00:00,  2.69it/s]06/19/2022 14:32:11 - INFO - __main__ - global step: 300; train loss: 7.676220893859863; dev loss: 7.777108192443848
Epoch 1: 100%|██████████| 300/300 [01:55<00:00,  2.46it/s]Epoch 1: 100%|██████████| 300/300 [01:55<00:00,  2.60it/s]
Epoch 2:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 2:   0%|          | 1/300 [00:00<01:44,  2.87it/s]Epoch 2:   1%|          | 2/300 [00:00<01:48,  2.74it/s]Epoch 2:   1%|          | 3/300 [00:01<01:44,  2.83it/s]Epoch 2:   1%|▏         | 4/300 [00:01<01:54,  2.58it/s]Epoch 2:   2%|▏         | 5/300 [00:01<01:49,  2.71it/s]Epoch 2:   2%|▏         | 6/300 [00:02<01:51,  2.64it/s]Epoch 2:   2%|▏         | 7/300 [00:02<01:47,  2.73it/s]Epoch 2:   3%|▎         | 8/300 [00:03<01:59,  2.44it/s]Epoch 2:   3%|▎         | 9/300 [00:03<01:53,  2.56it/s]Epoch 2:   3%|▎         | 10/300 [00:03<01:51,  2.59it/s]Epoch 2:   4%|▎         | 11/300 [00:04<01:50,  2.62it/s]Epoch 2:   4%|▍         | 12/300 [00:04<01:51,  2.58it/s]Epoch 2:   4%|▍         | 13/300 [00:05<01:56,  2.47it/s]Epoch 2:   5%|▍         | 14/300 [00:05<01:50,  2.59it/s]Epoch 2:   5%|▌         | 15/300 [00:05<01:48,  2.63it/s]Epoch 2:   5%|▌         | 16/300 [00:06<01:45,  2.69it/s]Epoch 2:   6%|▌         | 17/300 [00:06<01:50,  2.57it/s]Epoch 2:   6%|▌         | 18/300 [00:06<01:50,  2.56it/s]Epoch 2:   6%|▋         | 19/300 [00:07<01:49,  2.58it/s]06/19/2022 14:32:19 - INFO - __main__ - global step: 310; train loss: 7.686821937561035; dev loss: 7.734449863433838
Epoch 2:   7%|▋         | 20/300 [00:07<01:44,  2.68it/s]Epoch 2:   7%|▋         | 21/300 [00:08<01:48,  2.57it/s]Epoch 2:   7%|▋         | 22/300 [00:08<01:48,  2.55it/s]Epoch 2:   8%|▊         | 23/300 [00:08<01:49,  2.53it/s]Epoch 2:   8%|▊         | 24/300 [00:09<01:47,  2.57it/s]Epoch 2:   8%|▊         | 25/300 [00:09<01:51,  2.47it/s]Epoch 2:   9%|▊         | 26/300 [00:10<01:45,  2.60it/s]Epoch 2:   9%|▉         | 27/300 [00:10<01:41,  2.70it/s]Epoch 2:   9%|▉         | 28/300 [00:10<01:38,  2.76it/s]Epoch 2:  10%|▉         | 29/300 [00:11<01:43,  2.62it/s]Epoch 2:  10%|█         | 30/300 [00:11<01:41,  2.66it/s]Epoch 2:  10%|█         | 31/300 [00:11<01:38,  2.74it/s]Epoch 2:  11%|█         | 32/300 [00:12<01:38,  2.72it/s]Epoch 2:  11%|█         | 33/300 [00:12<01:46,  2.51it/s]Epoch 2:  11%|█▏        | 34/300 [00:13<01:41,  2.62it/s]Epoch 2:  12%|█▏        | 35/300 [00:13<01:37,  2.71it/s]Epoch 2:  12%|█▏        | 36/300 [00:13<01:39,  2.65it/s]Epoch 2:  12%|█▏        | 37/300 [00:14<01:41,  2.60it/s]Epoch 2:  13%|█▎        | 38/300 [00:14<01:45,  2.49it/s]Epoch 2:  13%|█▎        | 39/300 [00:14<01:43,  2.51it/s]06/19/2022 14:32:27 - INFO - __main__ - global step: 320; train loss: 7.568299770355225; dev loss: 7.659651279449463
Epoch 2:  13%|█▎        | 40/300 [00:15<01:44,  2.50it/s]Epoch 2:  14%|█▎        | 41/300 [00:15<01:39,  2.59it/s]Epoch 2:  14%|█▍        | 42/300 [00:16<01:44,  2.46it/s]Epoch 2:  14%|█▍        | 43/300 [00:16<01:40,  2.57it/s]Epoch 2:  15%|█▍        | 44/300 [00:16<01:35,  2.67it/s]Epoch 2:  15%|█▌        | 45/300 [00:17<01:33,  2.73it/s]Epoch 2:  15%|█▌        | 46/300 [00:17<01:38,  2.59it/s]Epoch 2:  16%|█▌        | 47/300 [00:18<01:34,  2.68it/s]Epoch 2:  16%|█▌        | 48/300 [00:18<01:31,  2.75it/s]Epoch 2:  16%|█▋        | 49/300 [00:18<01:33,  2.67it/s]Epoch 2:  17%|█▋        | 50/300 [00:19<01:43,  2.42it/s]Epoch 2:  17%|█▋        | 51/300 [00:19<01:39,  2.51it/s]Epoch 2:  17%|█▋        | 52/300 [00:19<01:34,  2.61it/s]Epoch 2:  18%|█▊        | 53/300 [00:20<01:31,  2.69it/s]Epoch 2:  18%|█▊        | 54/300 [00:20<01:36,  2.56it/s]Epoch 2:  18%|█▊        | 55/300 [00:21<01:31,  2.67it/s]Epoch 2:  19%|█▊        | 56/300 [00:21<01:29,  2.73it/s]Epoch 2:  19%|█▉        | 57/300 [00:21<01:28,  2.74it/s]Epoch 2:  19%|█▉        | 58/300 [00:22<01:34,  2.57it/s]Epoch 2:  20%|█▉        | 59/300 [00:22<01:30,  2.68it/s]06/19/2022 14:32:34 - INFO - __main__ - global step: 330; train loss: 7.750636100769043; dev loss: 7.752638339996338
Epoch 2:  20%|██        | 60/300 [00:22<01:27,  2.75it/s]Epoch 2:  20%|██        | 61/300 [00:23<01:29,  2.68it/s]Epoch 2:  21%|██        | 62/300 [00:23<01:38,  2.41it/s]Epoch 2:  21%|██        | 63/300 [00:24<01:35,  2.48it/s]Epoch 2:  21%|██▏       | 64/300 [00:24<01:35,  2.48it/s]Epoch 2:  22%|██▏       | 65/300 [00:24<01:31,  2.57it/s]Epoch 2:  22%|██▏       | 66/300 [00:25<01:27,  2.67it/s]Epoch 2:  22%|██▏       | 67/300 [00:25<01:31,  2.54it/s]Epoch 2:  23%|██▎       | 68/300 [00:26<01:28,  2.64it/s]Epoch 2:  23%|██▎       | 69/300 [00:26<01:25,  2.70it/s]Epoch 2:  23%|██▎       | 70/300 [00:26<01:27,  2.63it/s]Epoch 2:  24%|██▎       | 71/300 [00:27<01:35,  2.40it/s]Epoch 2:  24%|██▍       | 72/300 [00:27<01:30,  2.52it/s]Epoch 2:  24%|██▍       | 73/300 [00:28<01:27,  2.58it/s]Epoch 2:  25%|██▍       | 74/300 [00:28<01:29,  2.52it/s]Epoch 2:  25%|██▌       | 75/300 [00:28<01:32,  2.44it/s]Epoch 2:  25%|██▌       | 76/300 [00:29<01:28,  2.52it/s]Epoch 2:  26%|██▌       | 77/300 [00:29<01:25,  2.62it/s]Epoch 2:  26%|██▌       | 78/300 [00:30<01:26,  2.57it/s]Epoch 2:  26%|██▋       | 79/300 [00:30<01:34,  2.33it/s]06/19/2022 14:32:42 - INFO - __main__ - global step: 340; train loss: 7.953305721282959; dev loss: 8.100175857543945
Epoch 2:  27%|██▋       | 80/300 [00:30<01:29,  2.46it/s]Epoch 2:  27%|██▋       | 81/300 [00:31<01:24,  2.58it/s]Epoch 2:  27%|██▋       | 82/300 [00:31<01:21,  2.68it/s]Epoch 2:  28%|██▊       | 83/300 [00:32<01:28,  2.44it/s]Epoch 2:  28%|██▊       | 84/300 [00:32<01:26,  2.50it/s]Epoch 2:  28%|██▊       | 85/300 [00:32<01:22,  2.59it/s]Epoch 2:  29%|██▊       | 86/300 [00:33<01:20,  2.67it/s]Epoch 2:  29%|██▉       | 87/300 [00:33<01:27,  2.44it/s]Epoch 2:  29%|██▉       | 88/300 [00:33<01:23,  2.54it/s]Epoch 2:  30%|██▉       | 89/300 [00:34<01:19,  2.65it/s]Epoch 2:  30%|███       | 90/300 [00:34<01:16,  2.73it/s]Epoch 2:  30%|███       | 91/300 [00:35<01:14,  2.80it/s]Epoch 2:  31%|███       | 92/300 [00:35<01:21,  2.55it/s]Epoch 2:  31%|███       | 93/300 [00:35<01:18,  2.65it/s]Epoch 2:  31%|███▏      | 94/300 [00:36<01:18,  2.62it/s]Epoch 2:  32%|███▏      | 95/300 [00:36<01:18,  2.61it/s]Epoch 2:  32%|███▏      | 96/300 [00:37<01:21,  2.50it/s]Epoch 2:  32%|███▏      | 97/300 [00:37<01:19,  2.56it/s]Epoch 2:  33%|███▎      | 98/300 [00:37<01:16,  2.64it/s]Epoch 2:  33%|███▎      | 99/300 [00:38<01:15,  2.66it/s]06/19/2022 14:32:50 - INFO - __main__ - global step: 350; train loss: 7.5490264892578125; dev loss: 7.580382347106934
Epoch 2:  33%|███▎      | 100/300 [00:38<01:19,  2.50it/s]Epoch 2:  34%|███▎      | 101/300 [00:38<01:16,  2.62it/s]Epoch 2:  34%|███▍      | 102/300 [00:39<01:13,  2.70it/s]Epoch 2:  34%|███▍      | 103/300 [00:39<01:11,  2.77it/s]Epoch 2:  35%|███▍      | 104/300 [00:40<01:15,  2.60it/s]Epoch 2:  35%|███▌      | 105/300 [00:40<01:15,  2.58it/s]Epoch 2:  35%|███▌      | 106/300 [00:40<01:16,  2.55it/s]Epoch 2:  36%|███▌      | 107/300 [00:41<01:13,  2.63it/s]Epoch 2:  36%|███▌      | 108/300 [00:41<01:15,  2.54it/s]Epoch 2:  36%|███▋      | 109/300 [00:41<01:11,  2.66it/s]Epoch 2:  37%|███▋      | 110/300 [00:42<01:12,  2.62it/s]Epoch 2:  37%|███▋      | 111/300 [00:42<01:10,  2.67it/s]Epoch 2:  37%|███▋      | 112/300 [00:43<01:14,  2.53it/s]Epoch 2:  38%|███▊      | 113/300 [00:43<01:10,  2.64it/s]Epoch 2:  38%|███▊      | 114/300 [00:43<01:10,  2.65it/s]Epoch 2:  38%|███▊      | 115/300 [00:44<01:07,  2.73it/s]Epoch 2:  39%|███▊      | 116/300 [00:44<01:12,  2.53it/s]Epoch 2:  39%|███▉      | 117/300 [00:45<01:10,  2.60it/s]Epoch 2:  39%|███▉      | 118/300 [00:45<01:07,  2.69it/s]Epoch 2:  40%|███▉      | 119/300 [00:45<01:08,  2.64it/s]06/19/2022 14:32:57 - INFO - __main__ - global step: 360; train loss: 8.26763916015625; dev loss: 8.304710388183594
Epoch 2:  40%|████      | 120/300 [00:46<01:06,  2.70it/s]Epoch 2:  40%|████      | 121/300 [00:46<01:12,  2.45it/s]Epoch 2:  41%|████      | 122/300 [00:46<01:09,  2.56it/s]Epoch 2:  41%|████      | 123/300 [00:47<01:06,  2.66it/s]Epoch 2:  41%|████▏     | 124/300 [00:47<01:07,  2.62it/s]Epoch 2:  42%|████▏     | 125/300 [00:48<01:11,  2.45it/s]Epoch 2:  42%|████▏     | 126/300 [00:48<01:08,  2.53it/s]Epoch 2:  42%|████▏     | 127/300 [00:48<01:05,  2.63it/s]Epoch 2:  43%|████▎     | 128/300 [00:49<01:04,  2.66it/s]Epoch 2:  43%|████▎     | 129/300 [00:49<01:08,  2.48it/s]Epoch 2:  43%|████▎     | 130/300 [00:50<01:05,  2.59it/s]Epoch 2:  44%|████▎     | 131/300 [00:50<01:03,  2.68it/s]Epoch 2:  44%|████▍     | 132/300 [00:50<01:04,  2.60it/s]Epoch 2:  44%|████▍     | 133/300 [00:51<01:09,  2.41it/s]Epoch 2:  45%|████▍     | 134/300 [00:51<01:04,  2.56it/s]Epoch 2:  45%|████▌     | 135/300 [00:51<01:02,  2.66it/s]Epoch 2:  45%|████▌     | 136/300 [00:52<01:01,  2.66it/s]Epoch 2:  46%|████▌     | 137/300 [00:52<01:03,  2.55it/s]Epoch 2:  46%|████▌     | 138/300 [00:53<01:00,  2.66it/s]Epoch 2:  46%|████▋     | 139/300 [00:53<00:58,  2.73it/s]06/19/2022 14:33:05 - INFO - __main__ - global step: 370; train loss: 7.7574310302734375; dev loss: 7.687833309173584
Epoch 2:  47%|████▋     | 140/300 [00:53<00:58,  2.72it/s]Epoch 2:  47%|████▋     | 141/300 [00:54<01:02,  2.56it/s]Epoch 2:  47%|████▋     | 142/300 [00:54<01:02,  2.51it/s]Epoch 2:  48%|████▊     | 143/300 [00:55<00:59,  2.63it/s]Epoch 2:  48%|████▊     | 144/300 [00:55<00:59,  2.60it/s]Epoch 2:  48%|████▊     | 145/300 [00:55<00:58,  2.67it/s]Epoch 2:  49%|████▊     | 146/300 [00:56<01:03,  2.44it/s]Epoch 2:  49%|████▉     | 147/300 [00:56<00:59,  2.55it/s]Epoch 2:  49%|████▉     | 148/300 [00:56<00:57,  2.66it/s]Epoch 2:  50%|████▉     | 149/300 [00:57<00:55,  2.74it/s]Epoch 2:  50%|█████     | 150/300 [00:57<00:57,  2.60it/s]Epoch 2:  50%|█████     | 151/300 [00:58<00:55,  2.68it/s]Epoch 2:  51%|█████     | 152/300 [00:58<00:56,  2.63it/s]Epoch 2:  51%|█████     | 153/300 [00:58<00:54,  2.70it/s]Epoch 2:  51%|█████▏    | 154/300 [00:59<00:59,  2.47it/s]Epoch 2:  52%|█████▏    | 155/300 [00:59<00:59,  2.45it/s]Epoch 2:  52%|█████▏    | 156/300 [01:00<00:56,  2.55it/s]Epoch 2:  52%|█████▏    | 157/300 [01:00<00:53,  2.65it/s]Epoch 2:  53%|█████▎    | 158/300 [01:00<00:56,  2.53it/s]Epoch 2:  53%|█████▎    | 159/300 [01:01<00:53,  2.63it/s]06/19/2022 14:33:13 - INFO - __main__ - global step: 380; train loss: 8.324688911437988; dev loss: 8.478044509887695
Epoch 2:  53%|█████▎    | 160/300 [01:01<00:53,  2.60it/s]Epoch 2:  54%|█████▎    | 161/300 [01:01<00:52,  2.67it/s]Epoch 2:  54%|█████▍    | 162/300 [01:02<00:56,  2.45it/s]Epoch 2:  54%|█████▍    | 163/300 [01:02<00:53,  2.56it/s]Epoch 2:  55%|█████▍    | 164/300 [01:03<00:50,  2.67it/s]Epoch 2:  55%|█████▌    | 165/300 [01:03<00:50,  2.69it/s]Epoch 2:  55%|█████▌    | 166/300 [01:03<00:54,  2.48it/s]Epoch 2:  56%|█████▌    | 167/300 [01:04<00:53,  2.49it/s]Epoch 2:  56%|█████▌    | 168/300 [01:04<00:51,  2.59it/s]Epoch 2:  56%|█████▋    | 169/300 [01:05<00:51,  2.56it/s]Epoch 2:  57%|█████▋    | 170/300 [01:05<00:53,  2.45it/s]Epoch 2:  57%|█████▋    | 171/300 [01:05<00:50,  2.58it/s]Epoch 2:  57%|█████▋    | 172/300 [01:06<00:47,  2.68it/s]Epoch 2:  58%|█████▊    | 173/300 [01:06<00:45,  2.76it/s]Epoch 2:  58%|█████▊    | 174/300 [01:06<00:45,  2.74it/s]Epoch 2:  58%|█████▊    | 175/300 [01:07<00:48,  2.57it/s]Epoch 2:  59%|█████▊    | 176/300 [01:07<00:46,  2.66it/s]Epoch 2:  59%|█████▉    | 177/300 [01:08<00:45,  2.73it/s]Epoch 2:  59%|█████▉    | 178/300 [01:08<00:43,  2.78it/s]Epoch 2:  60%|█████▉    | 179/300 [01:08<00:46,  2.62it/s]06/19/2022 14:33:21 - INFO - __main__ - global step: 390; train loss: 8.426855087280273; dev loss: 8.329863548278809
Epoch 2:  60%|██████    | 180/300 [01:09<00:46,  2.58it/s]Epoch 2:  60%|██████    | 181/300 [01:09<00:44,  2.66it/s]Epoch 2:  61%|██████    | 182/300 [01:09<00:43,  2.73it/s]Epoch 2:  61%|██████    | 183/300 [01:10<00:45,  2.59it/s]Epoch 2:  61%|██████▏   | 184/300 [01:10<00:43,  2.67it/s]Epoch 2:  62%|██████▏   | 185/300 [01:11<00:44,  2.61it/s]Epoch 2:  62%|██████▏   | 186/300 [01:11<00:43,  2.60it/s]Epoch 2:  62%|██████▏   | 187/300 [01:11<00:44,  2.52it/s]Epoch 2:  63%|██████▎   | 188/300 [01:12<00:43,  2.60it/s]Epoch 2:  63%|██████▎   | 189/300 [01:12<00:43,  2.58it/s]Epoch 2:  63%|██████▎   | 190/300 [01:13<00:43,  2.55it/s]Epoch 2:  64%|██████▎   | 191/300 [01:13<00:45,  2.41it/s]Epoch 2:  64%|██████▍   | 192/300 [01:13<00:43,  2.46it/s]Epoch 2:  64%|██████▍   | 193/300 [01:14<00:42,  2.54it/s]Epoch 2:  65%|██████▍   | 194/300 [01:14<00:40,  2.64it/s]Epoch 2:  65%|██████▌   | 195/300 [01:15<00:41,  2.55it/s]Epoch 2:  65%|██████▌   | 196/300 [01:15<00:39,  2.65it/s]Epoch 2:  66%|██████▌   | 197/300 [01:15<00:37,  2.74it/s]Epoch 2:  66%|██████▌   | 198/300 [01:16<00:36,  2.81it/s]Epoch 2:  66%|██████▋   | 199/300 [01:16<00:35,  2.86it/s]06/19/2022 14:33:28 - INFO - __main__ - global step: 400; train loss: 7.523958683013916; dev loss: 7.472075462341309
Epoch 2:  67%|██████▋   | 200/300 [01:16<00:39,  2.56it/s]Epoch 2:  67%|██████▋   | 201/300 [01:17<00:37,  2.64it/s]Epoch 2:  67%|██████▋   | 202/300 [01:17<00:37,  2.61it/s]Epoch 2:  68%|██████▊   | 203/300 [01:18<00:36,  2.67it/s]Epoch 2:  68%|██████▊   | 204/300 [01:18<00:39,  2.45it/s]Epoch 2:  68%|██████▊   | 205/300 [01:18<00:37,  2.53it/s]Epoch 2:  69%|██████▊   | 206/300 [01:19<00:37,  2.52it/s]Epoch 2:  69%|██████▉   | 207/300 [01:19<00:36,  2.55it/s]Epoch 2:  69%|██████▉   | 208/300 [01:20<00:38,  2.38it/s]Epoch 2:  70%|██████▉   | 209/300 [01:20<00:37,  2.45it/s]Epoch 2:  70%|███████   | 210/300 [01:20<00:35,  2.57it/s]Epoch 2:  70%|███████   | 211/300 [01:21<00:33,  2.68it/s]Epoch 2:  71%|███████   | 212/300 [01:21<00:35,  2.46it/s]Epoch 2:  71%|███████   | 213/300 [01:22<00:34,  2.52it/s]Epoch 2:  71%|███████▏  | 214/300 [01:22<00:34,  2.50it/s]Epoch 2:  72%|███████▏  | 215/300 [01:22<00:34,  2.48it/s]Epoch 2:  72%|███████▏  | 216/300 [01:23<00:34,  2.40it/s]Epoch 2:  72%|███████▏  | 217/300 [01:23<00:32,  2.53it/s]Epoch 2:  73%|███████▎  | 218/300 [01:24<00:32,  2.51it/s]Epoch 2:  73%|███████▎  | 219/300 [01:24<00:31,  2.54it/s]06/19/2022 14:33:36 - INFO - __main__ - global step: 410; train loss: 7.829822540283203; dev loss: 7.760565757751465
Epoch 2:  73%|███████▎  | 220/300 [01:24<00:32,  2.47it/s]Epoch 2:  74%|███████▎  | 221/300 [01:25<00:30,  2.59it/s]Epoch 2:  74%|███████▍  | 222/300 [01:25<00:28,  2.70it/s]Epoch 2:  74%|███████▍  | 223/300 [01:25<00:29,  2.62it/s]Epoch 2:  75%|███████▍  | 224/300 [01:26<00:31,  2.44it/s]Epoch 2:  75%|███████▌  | 225/300 [01:26<00:29,  2.57it/s]Epoch 2:  75%|███████▌  | 226/300 [01:27<00:27,  2.67it/s]Epoch 2:  76%|███████▌  | 227/300 [01:27<00:26,  2.75it/s]Epoch 2:  76%|███████▌  | 228/300 [01:27<00:27,  2.64it/s]Epoch 2:  76%|███████▋  | 229/300 [01:28<00:28,  2.52it/s]Epoch 2:  77%|███████▋  | 230/300 [01:28<00:26,  2.63it/s]Epoch 2:  77%|███████▋  | 231/300 [01:29<00:25,  2.72it/s]Epoch 2:  77%|███████▋  | 232/300 [01:29<00:25,  2.66it/s]Epoch 2:  78%|███████▊  | 233/300 [01:29<00:26,  2.52it/s]Epoch 2:  78%|███████▊  | 234/300 [01:30<00:25,  2.64it/s]Epoch 2:  78%|███████▊  | 235/300 [01:30<00:23,  2.72it/s]Epoch 2:  79%|███████▊  | 236/300 [01:30<00:22,  2.78it/s]Epoch 2:  79%|███████▉  | 237/300 [01:31<00:23,  2.64it/s]Epoch 2:  79%|███████▉  | 238/300 [01:31<00:22,  2.74it/s]Epoch 2:  80%|███████▉  | 239/300 [01:31<00:21,  2.81it/s]06/19/2022 14:33:44 - INFO - __main__ - global step: 420; train loss: 8.073687553405762; dev loss: 7.931249141693115
Epoch 2:  80%|████████  | 240/300 [01:32<00:21,  2.77it/s]Epoch 2:  80%|████████  | 241/300 [01:32<00:22,  2.63it/s]Epoch 2:  81%|████████  | 242/300 [01:33<00:21,  2.72it/s]Epoch 2:  81%|████████  | 243/300 [01:33<00:21,  2.65it/s]Epoch 2:  81%|████████▏ | 244/300 [01:33<00:21,  2.59it/s]Epoch 2:  82%|████████▏ | 245/300 [01:34<00:22,  2.47it/s]Epoch 2:  82%|████████▏ | 246/300 [01:34<00:20,  2.60it/s]Epoch 2:  82%|████████▏ | 247/300 [01:35<00:19,  2.70it/s]Epoch 2:  83%|████████▎ | 248/300 [01:35<00:18,  2.76it/s]Epoch 2:  83%|████████▎ | 249/300 [01:35<00:19,  2.61it/s]Epoch 2:  83%|████████▎ | 250/300 [01:36<00:18,  2.69it/s]Epoch 2:  84%|████████▎ | 251/300 [01:36<00:18,  2.69it/s]Epoch 2:  84%|████████▍ | 252/300 [01:36<00:17,  2.74it/s]Epoch 2:  84%|████████▍ | 253/300 [01:37<00:17,  2.68it/s]Epoch 2:  85%|████████▍ | 254/300 [01:37<00:18,  2.54it/s]Epoch 2:  85%|████████▌ | 255/300 [01:38<00:16,  2.65it/s]Epoch 2:  85%|████████▌ | 256/300 [01:38<00:16,  2.60it/s]Epoch 2:  86%|████████▌ | 257/300 [01:38<00:16,  2.67it/s]Epoch 2:  86%|████████▌ | 258/300 [01:39<00:16,  2.52it/s]Epoch 2:  86%|████████▋ | 259/300 [01:39<00:15,  2.56it/s]06/19/2022 14:33:51 - INFO - __main__ - global step: 430; train loss: 7.439253330230713; dev loss: 7.5321478843688965
Epoch 2:  87%|████████▋ | 260/300 [01:39<00:15,  2.65it/s]Epoch 2:  87%|████████▋ | 261/300 [01:40<00:14,  2.67it/s]Epoch 2:  87%|████████▋ | 262/300 [01:40<00:14,  2.55it/s]Epoch 2:  88%|████████▊ | 263/300 [01:41<00:13,  2.67it/s]Epoch 2:  88%|████████▊ | 264/300 [01:41<00:13,  2.60it/s]Epoch 2:  88%|████████▊ | 265/300 [01:41<00:13,  2.61it/s]Epoch 2:  89%|████████▊ | 266/300 [01:42<00:13,  2.45it/s]Epoch 2:  89%|████████▉ | 267/300 [01:42<00:13,  2.47it/s]Epoch 2:  89%|████████▉ | 268/300 [01:43<00:12,  2.60it/s]Epoch 2:  90%|████████▉ | 269/300 [01:43<00:11,  2.66it/s]Epoch 2:  90%|█████████ | 270/300 [01:43<00:11,  2.51it/s]Epoch 2:  90%|█████████ | 271/300 [01:44<00:11,  2.49it/s]Epoch 2:  91%|█████████ | 272/300 [01:44<00:11,  2.53it/s]Epoch 2:  91%|█████████ | 273/300 [01:45<00:10,  2.60it/s]Epoch 2:  91%|█████████▏| 274/300 [01:45<00:10,  2.50it/s]Epoch 2:  92%|█████████▏| 275/300 [01:45<00:09,  2.63it/s]Epoch 2:  92%|█████████▏| 276/300 [01:46<00:09,  2.63it/s]Epoch 2:  92%|█████████▏| 277/300 [01:46<00:08,  2.68it/s]Epoch 2:  93%|█████████▎| 278/300 [01:47<00:08,  2.54it/s]Epoch 2:  93%|█████████▎| 279/300 [01:47<00:08,  2.61it/s]06/19/2022 14:33:59 - INFO - __main__ - global step: 440; train loss: 8.064769744873047; dev loss: 7.984548091888428
Epoch 2:  93%|█████████▎| 280/300 [01:47<00:07,  2.65it/s]Epoch 2:  94%|█████████▎| 281/300 [01:48<00:07,  2.64it/s]Epoch 2:  94%|█████████▍| 282/300 [01:48<00:06,  2.62it/s]Epoch 2:  94%|█████████▍| 283/300 [01:48<00:06,  2.44it/s]Epoch 2:  95%|█████████▍| 284/300 [01:49<00:06,  2.42it/s]Epoch 2:  95%|█████████▌| 285/300 [01:49<00:06,  2.47it/s]Epoch 2:  95%|█████████▌| 286/300 [01:50<00:05,  2.50it/s]Epoch 2:  96%|█████████▌| 287/300 [01:50<00:05,  2.37it/s]Epoch 2:  96%|█████████▌| 288/300 [01:50<00:04,  2.53it/s]Epoch 2:  96%|█████████▋| 289/300 [01:51<00:04,  2.49it/s]Epoch 2:  97%|█████████▋| 290/300 [01:51<00:04,  2.44it/s]Epoch 2:  97%|█████████▋| 291/300 [01:52<00:03,  2.39it/s]Epoch 2:  97%|█████████▋| 292/300 [01:52<00:03,  2.41it/s]Epoch 2:  98%|█████████▊| 293/300 [01:53<00:02,  2.42it/s]Epoch 2:  98%|█████████▊| 294/300 [01:53<00:02,  2.41it/s]Epoch 2:  98%|█████████▊| 295/300 [01:53<00:02,  2.34it/s]Epoch 2:  99%|█████████▊| 296/300 [01:54<00:01,  2.49it/s]Epoch 2:  99%|█████████▉| 297/300 [01:54<00:01,  2.61it/s]Epoch 2:  99%|█████████▉| 298/300 [01:54<00:00,  2.70it/s]Epoch 2: 100%|█████████▉| 299/300 [01:55<00:00,  2.57it/s]06/19/2022 14:34:07 - INFO - __main__ - global step: 450; train loss: 7.673885345458984; dev loss: 7.424298286437988
Epoch 2: 100%|██████████| 300/300 [01:55<00:00,  2.60it/s]Epoch 2: 100%|██████████| 300/300 [01:55<00:00,  2.59it/s]
Epoch 3:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 3:   0%|          | 1/300 [00:00<02:00,  2.48it/s]Epoch 3:   1%|          | 2/300 [00:00<01:58,  2.52it/s]Epoch 3:   1%|          | 3/300 [00:01<02:11,  2.25it/s]Epoch 3:   1%|▏         | 4/300 [00:01<01:59,  2.47it/s]Epoch 3:   2%|▏         | 5/300 [00:01<01:52,  2.62it/s]Epoch 3:   2%|▏         | 6/300 [00:02<01:55,  2.54it/s]Epoch 3:   2%|▏         | 7/300 [00:02<01:55,  2.53it/s]Epoch 3:   3%|▎         | 8/300 [00:03<02:03,  2.37it/s]Epoch 3:   3%|▎         | 9/300 [00:03<02:01,  2.40it/s]Epoch 3:   3%|▎         | 10/300 [00:04<01:58,  2.44it/s]Epoch 3:   4%|▎         | 11/300 [00:04<01:52,  2.57it/s]Epoch 3:   4%|▍         | 12/300 [00:04<02:03,  2.33it/s]Epoch 3:   4%|▍         | 13/300 [00:05<01:54,  2.50it/s]Epoch 3:   5%|▍         | 14/300 [00:05<01:51,  2.58it/s]Epoch 3:   5%|▌         | 15/300 [00:06<01:49,  2.61it/s]Epoch 3:   5%|▌         | 16/300 [00:06<02:00,  2.35it/s]Epoch 3:   6%|▌         | 17/300 [00:06<01:57,  2.41it/s]Epoch 3:   6%|▌         | 18/300 [00:07<01:58,  2.38it/s]Epoch 3:   6%|▋         | 19/300 [00:07<01:52,  2.49it/s]06/19/2022 14:34:15 - INFO - __main__ - global step: 460; train loss: 8.107635498046875; dev loss: 8.039756774902344
Epoch 3:   7%|▋         | 20/300 [00:08<01:55,  2.43it/s]Epoch 3:   7%|▋         | 21/300 [00:08<01:51,  2.51it/s]Epoch 3:   7%|▋         | 22/300 [00:08<01:48,  2.57it/s]Epoch 3:   8%|▊         | 23/300 [00:09<01:48,  2.55it/s]Epoch 3:   8%|▊         | 24/300 [00:09<01:56,  2.37it/s]Epoch 3:   8%|▊         | 25/300 [00:10<01:52,  2.43it/s]Epoch 3:   9%|▊         | 26/300 [00:10<01:54,  2.38it/s]Epoch 3:   9%|▉         | 27/300 [00:11<01:53,  2.40it/s]Epoch 3:   9%|▉         | 28/300 [00:11<02:02,  2.22it/s]Epoch 3:  10%|▉         | 29/300 [00:11<01:58,  2.29it/s]Epoch 3:  10%|█         | 30/300 [00:12<01:49,  2.46it/s]Epoch 3:  10%|█         | 31/300 [00:12<01:46,  2.52it/s]Epoch 3:  11%|█         | 32/300 [00:13<01:54,  2.34it/s]Epoch 3:  11%|█         | 33/300 [00:13<01:53,  2.35it/s]Epoch 3:  11%|█▏        | 34/300 [00:13<01:51,  2.38it/s]Epoch 3:  12%|█▏        | 35/300 [00:14<01:44,  2.53it/s]Epoch 3:  12%|█▏        | 36/300 [00:14<01:39,  2.65it/s]Epoch 3:  12%|█▏        | 37/300 [00:15<01:43,  2.55it/s]Epoch 3:  13%|█▎        | 38/300 [00:15<01:43,  2.53it/s]Epoch 3:  13%|█▎        | 39/300 [00:15<01:39,  2.62it/s]06/19/2022 14:34:23 - INFO - __main__ - global step: 470; train loss: 7.623083591461182; dev loss: 7.706582069396973
Epoch 3:  13%|█▎        | 40/300 [00:16<01:35,  2.72it/s]Epoch 3:  14%|█▎        | 41/300 [00:16<01:40,  2.59it/s]Epoch 3:  14%|█▍        | 42/300 [00:16<01:40,  2.58it/s]Epoch 3:  14%|█▍        | 43/300 [00:17<01:38,  2.61it/s]Epoch 3:  15%|█▍        | 44/300 [00:17<01:36,  2.66it/s]Epoch 3:  15%|█▌        | 45/300 [00:18<01:39,  2.56it/s]Epoch 3:  15%|█▌        | 46/300 [00:18<01:35,  2.66it/s]Epoch 3:  16%|█▌        | 47/300 [00:18<01:32,  2.75it/s]Epoch 3:  16%|█▌        | 48/300 [00:19<01:29,  2.81it/s]Epoch 3:  16%|█▋        | 49/300 [00:19<01:34,  2.65it/s]Epoch 3:  17%|█▋        | 50/300 [00:19<01:35,  2.62it/s]Epoch 3:  17%|█▋        | 51/300 [00:20<01:32,  2.70it/s]Epoch 3:  17%|█▋        | 52/300 [00:20<01:29,  2.76it/s]Epoch 3:  18%|█▊        | 53/300 [00:21<01:34,  2.61it/s]Epoch 3:  18%|█▊        | 54/300 [00:21<01:33,  2.63it/s]Epoch 3:  18%|█▊        | 55/300 [00:21<01:33,  2.61it/s]Epoch 3:  19%|█▊        | 56/300 [00:22<01:31,  2.68it/s]Epoch 3:  19%|█▉        | 57/300 [00:22<01:34,  2.56it/s]Epoch 3:  19%|█▉        | 58/300 [00:22<01:30,  2.67it/s]Epoch 3:  20%|█▉        | 59/300 [00:23<01:30,  2.68it/s]06/19/2022 14:34:31 - INFO - __main__ - global step: 480; train loss: 7.917567253112793; dev loss: 7.800662040710449
Epoch 3:  20%|██        | 60/300 [00:23<01:27,  2.74it/s]Epoch 3:  20%|██        | 61/300 [00:24<01:24,  2.82it/s]Epoch 3:  21%|██        | 62/300 [00:24<01:34,  2.52it/s]Epoch 3:  21%|██        | 63/300 [00:24<01:32,  2.55it/s]Epoch 3:  21%|██▏       | 64/300 [00:25<01:32,  2.54it/s]Epoch 3:  22%|██▏       | 65/300 [00:25<01:29,  2.63it/s]Epoch 3:  22%|██▏       | 66/300 [00:26<01:32,  2.53it/s]Epoch 3:  22%|██▏       | 67/300 [00:26<01:32,  2.53it/s]Epoch 3:  23%|██▎       | 68/300 [00:26<01:32,  2.51it/s]Epoch 3:  23%|██▎       | 69/300 [00:27<01:28,  2.61it/s]Epoch 3:  23%|██▎       | 70/300 [00:27<01:34,  2.43it/s]Epoch 3:  24%|██▎       | 71/300 [00:28<01:34,  2.43it/s]Epoch 3:  24%|██▍       | 72/300 [00:28<01:30,  2.53it/s]Epoch 3:  24%|██▍       | 73/300 [00:28<01:29,  2.53it/s]Epoch 3:  25%|██▍       | 74/300 [00:29<01:36,  2.35it/s]Epoch 3:  25%|██▌       | 75/300 [00:29<01:34,  2.39it/s]Epoch 3:  25%|██▌       | 76/300 [00:30<01:29,  2.52it/s]Epoch 3:  26%|██▌       | 77/300 [00:30<01:24,  2.63it/s]Epoch 3:  26%|██▌       | 78/300 [00:30<01:31,  2.42it/s]Epoch 3:  26%|██▋       | 79/300 [00:31<01:29,  2.48it/s]06/19/2022 14:34:39 - INFO - __main__ - global step: 490; train loss: 7.405051231384277; dev loss: 7.371319770812988
Epoch 3:  27%|██▋       | 80/300 [00:31<01:24,  2.60it/s]Epoch 3:  27%|██▋       | 81/300 [00:32<01:25,  2.57it/s]Epoch 3:  27%|██▋       | 82/300 [00:32<01:32,  2.37it/s]Epoch 3:  28%|██▊       | 83/300 [00:32<01:28,  2.44it/s]Epoch 3:  28%|██▊       | 84/300 [00:33<01:26,  2.51it/s]Epoch 3:  28%|██▊       | 85/300 [00:33<01:22,  2.61it/s]Epoch 3:  29%|██▊       | 86/300 [00:34<01:25,  2.50it/s]Epoch 3:  29%|██▉       | 87/300 [00:34<01:21,  2.61it/s]Epoch 3:  29%|██▉       | 88/300 [00:34<01:18,  2.70it/s]Epoch 3:  30%|██▉       | 89/300 [00:35<01:18,  2.70it/s]Epoch 3:  30%|███       | 90/300 [00:35<01:16,  2.76it/s]Epoch 3:  30%|███       | 91/300 [00:35<01:20,  2.60it/s]Epoch 3:  31%|███       | 92/300 [00:36<01:17,  2.69it/s]Epoch 3:  31%|███       | 93/300 [00:36<01:18,  2.64it/s]Epoch 3:  31%|███▏      | 94/300 [00:37<01:16,  2.70it/s]Epoch 3:  32%|███▏      | 95/300 [00:37<01:19,  2.58it/s]Epoch 3:  32%|███▏      | 96/300 [00:37<01:20,  2.53it/s]Epoch 3:  32%|███▏      | 97/300 [00:38<01:17,  2.62it/s]Epoch 3:  33%|███▎      | 98/300 [00:38<01:16,  2.63it/s]Epoch 3:  33%|███▎      | 99/300 [00:39<01:20,  2.51it/s]06/19/2022 14:34:46 - INFO - __main__ - global step: 500; train loss: 7.837388515472412; dev loss: 7.839684963226318
Epoch 3:  33%|███▎      | 100/300 [00:39<01:16,  2.62it/s]Epoch 3:  34%|███▎      | 101/300 [00:39<01:13,  2.71it/s]Epoch 3:  34%|███▍      | 102/300 [00:40<01:11,  2.77it/s]Epoch 3:  34%|███▍      | 103/300 [00:40<01:18,  2.50it/s]Epoch 3:  35%|███▍      | 104/300 [00:40<01:15,  2.59it/s]Epoch 3:  35%|███▌      | 105/300 [00:41<01:12,  2.68it/s]Epoch 3:  35%|███▌      | 106/300 [00:41<01:10,  2.75it/s]Epoch 3:  36%|███▌      | 107/300 [00:42<01:14,  2.60it/s]Epoch 3:  36%|███▌      | 108/300 [00:42<01:13,  2.62it/s]Epoch 3:  36%|███▋      | 109/300 [00:42<01:10,  2.69it/s]Epoch 3:  37%|███▋      | 110/300 [00:43<01:08,  2.77it/s]Epoch 3:  37%|███▋      | 111/300 [00:43<01:15,  2.50it/s]Epoch 3:  37%|███▋      | 112/300 [00:43<01:12,  2.59it/s]Epoch 3:  38%|███▊      | 113/300 [00:44<01:09,  2.68it/s]Epoch 3:  38%|███▊      | 114/300 [00:44<01:07,  2.74it/s]Epoch 3:  38%|███▊      | 115/300 [00:44<01:05,  2.81it/s]Epoch 3:  39%|███▊      | 116/300 [00:45<01:09,  2.63it/s]Epoch 3:  39%|███▉      | 117/300 [00:45<01:07,  2.73it/s]Epoch 3:  39%|███▉      | 118/300 [00:46<01:08,  2.66it/s]Epoch 3:  40%|███▉      | 119/300 [00:46<01:06,  2.72it/s]06/19/2022 14:34:54 - INFO - __main__ - global step: 510; train loss: 8.077409744262695; dev loss: 8.047234535217285
Epoch 3:  40%|████      | 120/300 [00:46<01:09,  2.57it/s]Epoch 3:  40%|████      | 121/300 [00:47<01:06,  2.69it/s]Epoch 3:  41%|████      | 122/300 [00:47<01:06,  2.68it/s]Epoch 3:  41%|████      | 123/300 [00:47<01:03,  2.77it/s]Epoch 3:  41%|████▏     | 124/300 [00:48<01:07,  2.61it/s]Epoch 3:  42%|████▏     | 125/300 [00:48<01:08,  2.57it/s]Epoch 3:  42%|████▏     | 126/300 [00:49<01:08,  2.54it/s]Epoch 3:  42%|████▏     | 127/300 [00:49<01:08,  2.51it/s]Epoch 3:  43%|████▎     | 128/300 [00:50<01:10,  2.43it/s]Epoch 3:  43%|████▎     | 129/300 [00:50<01:06,  2.57it/s]Epoch 3:  43%|████▎     | 130/300 [00:50<01:03,  2.67it/s]Epoch 3:  44%|████▎     | 131/300 [00:51<01:01,  2.75it/s]Epoch 3:  44%|████▍     | 132/300 [00:51<01:07,  2.48it/s]Epoch 3:  44%|████▍     | 133/300 [00:51<01:07,  2.48it/s]Epoch 3:  45%|████▍     | 134/300 [00:52<01:04,  2.58it/s]Epoch 3:  45%|████▌     | 135/300 [00:52<01:04,  2.57it/s]Epoch 3:  45%|████▌     | 136/300 [00:53<01:06,  2.46it/s]Epoch 3:  46%|████▌     | 137/300 [00:53<01:05,  2.47it/s]Epoch 3:  46%|████▌     | 138/300 [00:53<01:02,  2.58it/s]Epoch 3:  46%|████▋     | 139/300 [00:54<01:00,  2.67it/s]06/19/2022 14:35:02 - INFO - __main__ - global step: 520; train loss: 8.115938186645508; dev loss: 8.292113304138184
Epoch 3:  47%|████▋     | 140/300 [00:54<01:03,  2.54it/s]Epoch 3:  47%|████▋     | 141/300 [00:55<01:00,  2.64it/s]Epoch 3:  47%|████▋     | 142/300 [00:55<00:59,  2.64it/s]Epoch 3:  48%|████▊     | 143/300 [00:55<00:57,  2.73it/s]Epoch 3:  48%|████▊     | 144/300 [00:56<00:58,  2.66it/s]Epoch 3:  48%|████▊     | 145/300 [00:56<01:03,  2.42it/s]Epoch 3:  49%|████▊     | 146/300 [00:56<01:00,  2.54it/s]Epoch 3:  49%|████▉     | 147/300 [00:57<00:57,  2.64it/s]Epoch 3:  49%|████▉     | 148/300 [00:57<00:57,  2.66it/s]Epoch 3:  50%|████▉     | 149/300 [00:58<01:00,  2.48it/s]Epoch 3:  50%|█████     | 150/300 [00:58<01:00,  2.47it/s]Epoch 3:  50%|█████     | 151/300 [00:58<00:57,  2.59it/s]Epoch 3:  51%|█████     | 152/300 [00:59<00:55,  2.69it/s]Epoch 3:  51%|█████     | 153/300 [00:59<00:57,  2.57it/s]Epoch 3:  51%|█████▏    | 154/300 [01:00<00:54,  2.67it/s]Epoch 3:  52%|█████▏    | 155/300 [01:00<00:52,  2.76it/s]Epoch 3:  52%|█████▏    | 156/300 [01:00<00:51,  2.80it/s]Epoch 3:  52%|█████▏    | 157/300 [01:01<00:56,  2.52it/s]Epoch 3:  53%|█████▎    | 158/300 [01:01<00:54,  2.62it/s]Epoch 3:  53%|█████▎    | 159/300 [01:01<00:52,  2.68it/s]06/19/2022 14:35:09 - INFO - __main__ - global step: 530; train loss: 7.583927154541016; dev loss: 7.753125190734863
Epoch 3:  53%|█████▎    | 160/300 [01:02<00:54,  2.59it/s]Epoch 3:  54%|█████▎    | 161/300 [01:02<00:56,  2.45it/s]Epoch 3:  54%|█████▍    | 162/300 [01:03<00:53,  2.56it/s]Epoch 3:  54%|█████▍    | 163/300 [01:03<00:51,  2.65it/s]Epoch 3:  55%|█████▍    | 164/300 [01:03<00:49,  2.72it/s]Epoch 3:  55%|█████▌    | 165/300 [01:04<00:52,  2.57it/s]Epoch 3:  55%|█████▌    | 166/300 [01:04<00:50,  2.67it/s]Epoch 3:  56%|█████▌    | 167/300 [01:04<00:48,  2.74it/s]Epoch 3:  56%|█████▌    | 168/300 [01:05<00:47,  2.78it/s]Epoch 3:  56%|█████▋    | 169/300 [01:05<00:47,  2.77it/s]Epoch 3:  57%|█████▋    | 170/300 [01:06<00:52,  2.48it/s]Epoch 3:  57%|█████▋    | 171/300 [01:06<00:51,  2.52it/s]Epoch 3:  57%|█████▋    | 172/300 [01:06<00:49,  2.61it/s]Epoch 3:  58%|█████▊    | 173/300 [01:07<00:49,  2.58it/s]Epoch 3:  58%|█████▊    | 174/300 [01:07<00:51,  2.45it/s]Epoch 3:  58%|█████▊    | 175/300 [01:08<00:48,  2.58it/s]Epoch 3:  59%|█████▊    | 176/300 [01:08<00:46,  2.64it/s]Epoch 3:  59%|█████▉    | 177/300 [01:08<00:45,  2.71it/s]Epoch 3:  59%|█████▉    | 178/300 [01:09<00:49,  2.46it/s]Epoch 3:  60%|█████▉    | 179/300 [01:09<00:49,  2.46it/s]06/19/2022 14:35:17 - INFO - __main__ - global step: 540; train loss: 7.203165531158447; dev loss: 7.513696193695068
Epoch 3:  60%|██████    | 180/300 [01:10<00:48,  2.50it/s]Epoch 3:  60%|██████    | 181/300 [01:10<00:47,  2.50it/s]Epoch 3:  61%|██████    | 182/300 [01:10<00:48,  2.41it/s]Epoch 3:  61%|██████    | 183/300 [01:11<00:47,  2.49it/s]Epoch 3:  61%|██████▏   | 184/300 [01:11<00:45,  2.53it/s]Epoch 3:  62%|██████▏   | 185/300 [01:11<00:43,  2.64it/s]Epoch 3:  62%|██████▏   | 186/300 [01:12<00:45,  2.52it/s]Epoch 3:  62%|██████▏   | 187/300 [01:12<00:44,  2.52it/s]Epoch 3:  63%|██████▎   | 188/300 [01:13<00:44,  2.50it/s]Epoch 3:  63%|██████▎   | 189/300 [01:13<00:42,  2.59it/s]Epoch 3:  63%|██████▎   | 190/300 [01:14<00:44,  2.49it/s]Epoch 3:  64%|██████▎   | 191/300 [01:14<00:42,  2.55it/s]Epoch 3:  64%|██████▍   | 192/300 [01:14<00:41,  2.63it/s]Epoch 3:  64%|██████▍   | 193/300 [01:15<00:39,  2.71it/s]Epoch 3:  65%|██████▍   | 194/300 [01:15<00:41,  2.55it/s]Epoch 3:  65%|██████▌   | 195/300 [01:15<00:39,  2.65it/s]Epoch 3:  65%|██████▌   | 196/300 [01:16<00:40,  2.57it/s]Epoch 3:  66%|██████▌   | 197/300 [01:16<00:40,  2.57it/s]Epoch 3:  66%|██████▌   | 198/300 [01:17<00:38,  2.68it/s]Epoch 3:  66%|██████▋   | 199/300 [01:17<00:40,  2.52it/s]06/19/2022 14:35:25 - INFO - __main__ - global step: 550; train loss: 8.16588306427002; dev loss: 8.283329010009766
Epoch 3:  67%|██████▋   | 200/300 [01:17<00:38,  2.63it/s]Epoch 3:  67%|██████▋   | 201/300 [01:18<00:36,  2.70it/s]Epoch 3:  67%|██████▋   | 202/300 [01:18<00:37,  2.59it/s]Epoch 3:  68%|██████▊   | 203/300 [01:19<00:38,  2.49it/s]Epoch 3:  68%|██████▊   | 204/300 [01:19<00:36,  2.60it/s]Epoch 3:  68%|██████▊   | 205/300 [01:19<00:35,  2.69it/s]Epoch 3:  69%|██████▊   | 206/300 [01:20<00:33,  2.77it/s]Epoch 3:  69%|██████▉   | 207/300 [01:20<00:36,  2.57it/s]Epoch 3:  69%|██████▉   | 208/300 [01:20<00:35,  2.59it/s]Epoch 3:  70%|██████▉   | 209/300 [01:21<00:34,  2.61it/s]Epoch 3:  70%|███████   | 210/300 [01:21<00:33,  2.69it/s]Epoch 3:  70%|███████   | 211/300 [01:22<00:35,  2.50it/s]Epoch 3:  71%|███████   | 212/300 [01:22<00:34,  2.53it/s]Epoch 3:  71%|███████   | 213/300 [01:22<00:34,  2.50it/s]Epoch 3:  71%|███████▏  | 214/300 [01:23<00:33,  2.59it/s]Epoch 3:  72%|███████▏  | 215/300 [01:23<00:34,  2.50it/s]Epoch 3:  72%|███████▏  | 216/300 [01:24<00:33,  2.49it/s]Epoch 3:  72%|███████▏  | 217/300 [01:24<00:32,  2.58it/s]Epoch 3:  73%|███████▎  | 218/300 [01:24<00:31,  2.63it/s]Epoch 3:  73%|███████▎  | 219/300 [01:25<00:32,  2.50it/s]06/19/2022 14:35:33 - INFO - __main__ - global step: 560; train loss: 8.156426429748535; dev loss: 8.289961814880371
Epoch 3:  73%|███████▎  | 220/300 [01:25<00:30,  2.61it/s]Epoch 3:  74%|███████▎  | 221/300 [01:25<00:29,  2.69it/s]Epoch 3:  74%|███████▍  | 222/300 [01:26<00:29,  2.63it/s]Epoch 3:  74%|███████▍  | 223/300 [01:26<00:28,  2.67it/s]Epoch 3:  75%|███████▍  | 224/300 [01:27<00:31,  2.43it/s]Epoch 3:  75%|███████▌  | 225/300 [01:27<00:30,  2.44it/s]Epoch 3:  75%|███████▌  | 226/300 [01:27<00:29,  2.54it/s]Epoch 3:  76%|███████▌  | 227/300 [01:28<00:28,  2.53it/s]Epoch 3:  76%|███████▌  | 228/300 [01:28<00:29,  2.41it/s]Epoch 3:  76%|███████▋  | 229/300 [01:29<00:27,  2.54it/s]Epoch 3:  77%|███████▋  | 230/300 [01:29<00:26,  2.64it/s]Epoch 3:  77%|███████▋  | 231/300 [01:29<00:26,  2.59it/s]Epoch 3:  77%|███████▋  | 232/300 [01:30<00:27,  2.47it/s]Epoch 3:  78%|███████▊  | 233/300 [01:30<00:25,  2.59it/s]Epoch 3:  78%|███████▊  | 234/300 [01:31<00:25,  2.62it/s]Epoch 3:  78%|███████▊  | 235/300 [01:31<00:25,  2.56it/s]Epoch 3:  79%|███████▊  | 236/300 [01:31<00:27,  2.36it/s]Epoch 3:  79%|███████▉  | 237/300 [01:32<00:25,  2.50it/s]Epoch 3:  79%|███████▉  | 238/300 [01:32<00:23,  2.60it/s]Epoch 3:  80%|███████▉  | 239/300 [01:33<00:23,  2.55it/s]06/19/2022 14:35:41 - INFO - __main__ - global step: 570; train loss: 8.432723999023438; dev loss: 8.429286003112793
Epoch 3:  80%|████████  | 240/300 [01:33<00:24,  2.44it/s]Epoch 3:  80%|████████  | 241/300 [01:33<00:23,  2.50it/s]Epoch 3:  81%|████████  | 242/300 [01:34<00:22,  2.61it/s]Epoch 3:  81%|████████  | 243/300 [01:34<00:21,  2.64it/s]Epoch 3:  81%|████████▏ | 244/300 [01:35<00:23,  2.40it/s]Epoch 3:  82%|████████▏ | 245/300 [01:35<00:22,  2.42it/s]Epoch 3:  82%|████████▏ | 246/300 [01:35<00:22,  2.43it/s]Epoch 3:  82%|████████▏ | 247/300 [01:36<00:20,  2.54it/s]Epoch 3:  83%|████████▎ | 248/300 [01:36<00:21,  2.47it/s]Epoch 3:  83%|████████▎ | 249/300 [01:37<00:20,  2.48it/s]Epoch 3:  83%|████████▎ | 250/300 [01:37<00:19,  2.51it/s]Epoch 3:  84%|████████▎ | 251/300 [01:37<00:18,  2.61it/s]Epoch 3:  84%|████████▍ | 252/300 [01:38<00:17,  2.68it/s]Epoch 3:  84%|████████▍ | 253/300 [01:38<00:18,  2.54it/s]Epoch 3:  85%|████████▍ | 254/300 [01:39<00:18,  2.52it/s]Epoch 3:  85%|████████▌ | 255/300 [01:39<00:17,  2.61it/s]Epoch 3:  85%|████████▌ | 256/300 [01:39<00:16,  2.69it/s]Epoch 3:  86%|████████▌ | 257/300 [01:40<00:17,  2.45it/s]Epoch 3:  86%|████████▌ | 258/300 [01:40<00:16,  2.55it/s]Epoch 3:  86%|████████▋ | 259/300 [01:40<00:15,  2.65it/s]06/19/2022 14:35:48 - INFO - __main__ - global step: 580; train loss: 7.879148960113525; dev loss: 7.893214225769043
Epoch 3:  87%|████████▋ | 260/300 [01:41<00:14,  2.71it/s]Epoch 3:  87%|████████▋ | 261/300 [01:41<00:15,  2.57it/s]Epoch 3:  87%|████████▋ | 262/300 [01:42<00:14,  2.59it/s]Epoch 3:  88%|████████▊ | 263/300 [01:42<00:14,  2.60it/s]Epoch 3:  88%|████████▊ | 264/300 [01:42<00:13,  2.61it/s]Epoch 3:  88%|████████▊ | 265/300 [01:43<00:14,  2.44it/s]Epoch 3:  89%|████████▊ | 266/300 [01:43<00:13,  2.57it/s]Epoch 3:  89%|████████▉ | 267/300 [01:44<00:12,  2.54it/s]Epoch 3:  89%|████████▉ | 268/300 [01:44<00:12,  2.62it/s]Epoch 3:  90%|████████▉ | 269/300 [01:44<00:12,  2.45it/s]Epoch 3:  90%|█████████ | 270/300 [01:45<00:12,  2.46it/s]Epoch 3:  90%|█████████ | 271/300 [01:45<00:11,  2.56it/s]Epoch 3:  91%|█████████ | 272/300 [01:45<00:10,  2.66it/s]Epoch 3:  91%|█████████ | 273/300 [01:46<00:10,  2.52it/s]Epoch 3:  91%|█████████▏| 274/300 [01:46<00:09,  2.63it/s]Epoch 3:  92%|█████████▏| 275/300 [01:47<00:09,  2.71it/s]Epoch 3:  92%|█████████▏| 276/300 [01:47<00:08,  2.71it/s]Epoch 3:  92%|█████████▏| 277/300 [01:47<00:08,  2.61it/s]Epoch 3:  93%|█████████▎| 278/300 [01:48<00:09,  2.40it/s]Epoch 3:  93%|█████████▎| 279/300 [01:48<00:08,  2.52it/s]06/19/2022 14:35:56 - INFO - __main__ - global step: 590; train loss: 7.7476067543029785; dev loss: 7.856533050537109
Epoch 3:  93%|█████████▎| 280/300 [01:49<00:07,  2.62it/s]Epoch 3:  94%|█████████▎| 281/300 [01:49<00:07,  2.66it/s]Epoch 3:  94%|█████████▍| 282/300 [01:49<00:07,  2.52it/s]Epoch 3:  94%|█████████▍| 283/300 [01:50<00:06,  2.51it/s]Epoch 3:  95%|█████████▍| 284/300 [01:50<00:06,  2.53it/s]Epoch 3:  95%|█████████▌| 285/300 [01:51<00:05,  2.63it/s]Epoch 3:  95%|█████████▌| 286/300 [01:51<00:05,  2.52it/s]Epoch 3:  96%|█████████▌| 287/300 [01:51<00:04,  2.63it/s]Epoch 3:  96%|█████████▌| 288/300 [01:52<00:04,  2.71it/s]Epoch 3:  96%|█████████▋| 289/300 [01:52<00:04,  2.65it/s]Epoch 3:  97%|█████████▋| 290/300 [01:53<00:04,  2.45it/s]Epoch 3:  97%|█████████▋| 291/300 [01:53<00:03,  2.57it/s]Epoch 3:  97%|█████████▋| 292/300 [01:53<00:02,  2.67it/s]Epoch 3:  98%|█████████▊| 293/300 [01:54<00:02,  2.74it/s]Epoch 3:  98%|█████████▊| 294/300 [01:54<00:02,  2.57it/s]Epoch 3:  98%|█████████▊| 295/300 [01:54<00:01,  2.64it/s]Epoch 3:  99%|█████████▊| 296/300 [01:55<00:01,  2.66it/s]Epoch 3:  99%|█████████▉| 297/300 [01:55<00:01,  2.72it/s]Epoch 3:  99%|█████████▉| 298/300 [01:56<00:00,  2.58it/s]Epoch 3: 100%|█████████▉| 299/300 [01:56<00:00,  2.68it/s]06/19/2022 14:36:04 - INFO - __main__ - global step: 600; train loss: 7.74526834487915; dev loss: 7.7574286460876465
Epoch 3: 100%|██████████| 300/300 [01:56<00:00,  2.73it/s]Epoch 3: 100%|██████████| 300/300 [01:56<00:00,  2.57it/s]
Epoch 4:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 4:   0%|          | 1/300 [00:00<01:40,  2.97it/s]Epoch 4:   1%|          | 2/300 [00:00<01:57,  2.53it/s]Epoch 4:   1%|          | 3/300 [00:01<01:49,  2.70it/s]Epoch 4:   1%|▏         | 4/300 [00:01<01:46,  2.78it/s]Epoch 4:   2%|▏         | 5/300 [00:01<01:44,  2.82it/s]Epoch 4:   2%|▏         | 6/300 [00:02<01:43,  2.85it/s]Epoch 4:   2%|▏         | 7/300 [00:02<01:50,  2.64it/s]Epoch 4:   3%|▎         | 8/300 [00:02<01:49,  2.67it/s]Epoch 4:   3%|▎         | 9/300 [00:03<01:51,  2.60it/s]Epoch 4:   3%|▎         | 10/300 [00:03<01:48,  2.67it/s]Epoch 4:   4%|▎         | 11/300 [00:04<01:57,  2.46it/s]Epoch 4:   4%|▍         | 12/300 [00:04<01:57,  2.45it/s]Epoch 4:   4%|▍         | 13/300 [00:04<01:52,  2.56it/s]Epoch 4:   5%|▍         | 14/300 [00:05<01:47,  2.65it/s]Epoch 4:   5%|▌         | 15/300 [00:05<01:58,  2.40it/s]Epoch 4:   5%|▌         | 16/300 [00:06<01:55,  2.46it/s]Epoch 4:   6%|▌         | 17/300 [00:06<01:54,  2.48it/s]Epoch 4:   6%|▌         | 18/300 [00:06<01:49,  2.57it/s]Epoch 4:   6%|▋         | 19/300 [00:07<01:57,  2.38it/s]06/19/2022 14:36:12 - INFO - __main__ - global step: 610; train loss: 7.561917304992676; dev loss: 7.441098690032959
Epoch 4:   7%|▋         | 20/300 [00:07<01:51,  2.50it/s]Epoch 4:   7%|▋         | 21/300 [00:08<01:46,  2.63it/s]Epoch 4:   7%|▋         | 22/300 [00:08<01:42,  2.72it/s]Epoch 4:   8%|▊         | 23/300 [00:08<01:49,  2.54it/s]Epoch 4:   8%|▊         | 24/300 [00:09<01:48,  2.54it/s]Epoch 4:   8%|▊         | 25/300 [00:09<01:44,  2.63it/s]Epoch 4:   9%|▊         | 26/300 [00:10<01:45,  2.60it/s]Epoch 4:   9%|▉         | 27/300 [00:10<01:49,  2.48it/s]Epoch 4:   9%|▉         | 28/300 [00:10<01:44,  2.60it/s]Epoch 4:  10%|▉         | 29/300 [00:11<01:40,  2.69it/s]Epoch 4:  10%|█         | 30/300 [00:11<01:37,  2.76it/s]Epoch 4:  10%|█         | 31/300 [00:11<01:36,  2.80it/s]Epoch 4:  11%|█         | 32/300 [00:12<01:45,  2.55it/s]Epoch 4:  11%|█         | 33/300 [00:12<01:42,  2.62it/s]Epoch 4:  11%|█▏        | 34/300 [00:13<01:39,  2.67it/s]Epoch 4:  12%|█▏        | 35/300 [00:13<01:36,  2.73it/s]Epoch 4:  12%|█▏        | 36/300 [00:13<01:45,  2.49it/s]Epoch 4:  12%|█▏        | 37/300 [00:14<01:44,  2.53it/s]Epoch 4:  13%|█▎        | 38/300 [00:14<01:38,  2.65it/s]Epoch 4:  13%|█▎        | 39/300 [00:15<01:40,  2.60it/s]06/19/2022 14:36:19 - INFO - __main__ - global step: 620; train loss: 8.228824615478516; dev loss: 8.045331954956055
Epoch 4:  13%|█▎        | 40/300 [00:15<01:44,  2.50it/s]Epoch 4:  14%|█▎        | 41/300 [00:15<01:39,  2.61it/s]Epoch 4:  14%|█▍        | 42/300 [00:16<01:36,  2.68it/s]Epoch 4:  14%|█▍        | 43/300 [00:16<01:33,  2.76it/s]Epoch 4:  15%|█▍        | 44/300 [00:16<01:38,  2.61it/s]Epoch 4:  15%|█▌        | 45/300 [00:17<01:37,  2.62it/s]Epoch 4:  15%|█▌        | 46/300 [00:17<01:33,  2.71it/s]Epoch 4:  16%|█▌        | 47/300 [00:17<01:31,  2.77it/s]Epoch 4:  16%|█▌        | 48/300 [00:18<01:36,  2.61it/s]Epoch 4:  16%|█▋        | 49/300 [00:18<01:33,  2.68it/s]Epoch 4:  17%|█▋        | 50/300 [00:19<01:34,  2.64it/s]Epoch 4:  17%|█▋        | 51/300 [00:19<01:32,  2.70it/s]Epoch 4:  17%|█▋        | 52/300 [00:19<01:37,  2.55it/s]Epoch 4:  18%|█▊        | 53/300 [00:20<01:34,  2.60it/s]Epoch 4:  18%|█▊        | 54/300 [00:20<01:32,  2.67it/s]Epoch 4:  18%|█▊        | 55/300 [00:20<01:28,  2.76it/s]Epoch 4:  19%|█▊        | 56/300 [00:21<01:33,  2.61it/s]Epoch 4:  19%|█▉        | 57/300 [00:21<01:29,  2.71it/s]Epoch 4:  19%|█▉        | 58/300 [00:22<01:27,  2.77it/s]Epoch 4:  20%|█▉        | 59/300 [00:22<01:25,  2.81it/s]06/19/2022 14:36:27 - INFO - __main__ - global step: 630; train loss: 7.625404357910156; dev loss: 7.581915378570557
Epoch 4:  20%|██        | 60/300 [00:22<01:28,  2.71it/s]Epoch 4:  20%|██        | 61/300 [00:23<01:33,  2.56it/s]Epoch 4:  21%|██        | 62/300 [00:23<01:29,  2.66it/s]Epoch 4:  21%|██        | 63/300 [00:23<01:26,  2.73it/s]Epoch 4:  21%|██▏       | 64/300 [00:24<01:24,  2.79it/s]Epoch 4:  22%|██▏       | 65/300 [00:24<01:33,  2.52it/s]Epoch 4:  22%|██▏       | 66/300 [00:25<01:33,  2.50it/s]Epoch 4:  22%|██▏       | 67/300 [00:25<01:33,  2.50it/s]Epoch 4:  23%|██▎       | 68/300 [00:25<01:29,  2.60it/s]Epoch 4:  23%|██▎       | 69/300 [00:26<01:31,  2.52it/s]Epoch 4:  23%|██▎       | 70/300 [00:26<01:27,  2.62it/s]Epoch 4:  24%|██▎       | 71/300 [00:27<01:28,  2.59it/s]Epoch 4:  24%|██▍       | 72/300 [00:27<01:25,  2.65it/s]Epoch 4:  24%|██▍       | 73/300 [00:27<01:31,  2.48it/s]Epoch 4:  25%|██▍       | 74/300 [00:28<01:28,  2.55it/s]Epoch 4:  25%|██▌       | 75/300 [00:28<01:25,  2.64it/s]Epoch 4:  25%|██▌       | 76/300 [00:29<01:24,  2.65it/s]Epoch 4:  26%|██▌       | 77/300 [00:29<01:27,  2.55it/s]Epoch 4:  26%|██▌       | 78/300 [00:29<01:23,  2.66it/s]Epoch 4:  26%|██▋       | 79/300 [00:30<01:20,  2.73it/s]06/19/2022 14:36:34 - INFO - __main__ - global step: 640; train loss: 7.678281307220459; dev loss: 7.577439308166504
Epoch 4:  27%|██▋       | 80/300 [00:30<01:18,  2.79it/s]Epoch 4:  27%|██▋       | 81/300 [00:30<01:23,  2.63it/s]Epoch 4:  27%|██▋       | 82/300 [00:31<01:23,  2.61it/s]Epoch 4:  28%|██▊       | 83/300 [00:31<01:24,  2.57it/s]Epoch 4:  28%|██▊       | 84/300 [00:32<01:21,  2.66it/s]Epoch 4:  28%|██▊       | 85/300 [00:32<01:18,  2.72it/s]Epoch 4:  29%|██▊       | 86/300 [00:32<01:22,  2.58it/s]Epoch 4:  29%|██▉       | 87/300 [00:33<01:19,  2.68it/s]Epoch 4:  29%|██▉       | 88/300 [00:33<01:19,  2.67it/s]Epoch 4:  30%|██▉       | 89/300 [00:33<01:18,  2.69it/s]Epoch 4:  30%|███       | 90/300 [00:34<01:21,  2.56it/s]Epoch 4:  30%|███       | 91/300 [00:34<01:20,  2.60it/s]Epoch 4:  31%|███       | 92/300 [00:35<01:18,  2.64it/s]Epoch 4:  31%|███       | 93/300 [00:35<01:16,  2.71it/s]Epoch 4:  31%|███▏      | 94/300 [00:35<01:22,  2.51it/s]Epoch 4:  32%|███▏      | 95/300 [00:36<01:17,  2.63it/s]Epoch 4:  32%|███▏      | 96/300 [00:36<01:15,  2.71it/s]Epoch 4:  32%|███▏      | 97/300 [00:36<01:12,  2.79it/s]Epoch 4:  33%|███▎      | 98/300 [00:37<01:17,  2.60it/s]Epoch 4:  33%|███▎      | 99/300 [00:37<01:15,  2.67it/s]06/19/2022 14:36:42 - INFO - __main__ - global step: 650; train loss: 7.842406272888184; dev loss: 7.9049072265625
Epoch 4:  33%|███▎      | 100/300 [00:38<01:17,  2.57it/s]Epoch 4:  34%|███▎      | 101/300 [00:38<01:14,  2.69it/s]Epoch 4:  34%|███▍      | 102/300 [00:38<01:20,  2.46it/s]Epoch 4:  34%|███▍      | 103/300 [00:39<01:16,  2.57it/s]Epoch 4:  35%|███▍      | 104/300 [00:39<01:13,  2.67it/s]Epoch 4:  35%|███▌      | 105/300 [00:39<01:10,  2.76it/s]Epoch 4:  35%|███▌      | 106/300 [00:40<01:17,  2.50it/s]Epoch 4:  36%|███▌      | 107/300 [00:40<01:18,  2.47it/s]Epoch 4:  36%|███▌      | 108/300 [00:41<01:17,  2.48it/s]Epoch 4:  36%|███▋      | 109/300 [00:41<01:13,  2.58it/s]Epoch 4:  37%|███▋      | 110/300 [00:42<01:19,  2.40it/s]Epoch 4:  37%|███▋      | 111/300 [00:42<01:15,  2.51it/s]Epoch 4:  37%|███▋      | 112/300 [00:42<01:11,  2.61it/s]Epoch 4:  38%|███▊      | 113/300 [00:43<01:09,  2.70it/s]Epoch 4:  38%|███▊      | 114/300 [00:43<01:07,  2.77it/s]Epoch 4:  38%|███▊      | 115/300 [00:43<01:10,  2.61it/s]Epoch 4:  39%|███▊      | 116/300 [00:44<01:07,  2.72it/s]Epoch 4:  39%|███▉      | 117/300 [00:44<01:05,  2.78it/s]Epoch 4:  39%|███▉      | 118/300 [00:44<01:04,  2.82it/s]Epoch 4:  40%|███▉      | 119/300 [00:45<01:08,  2.65it/s]06/19/2022 14:36:50 - INFO - __main__ - global step: 660; train loss: 8.127589225769043; dev loss: 7.99484395980835
Epoch 4:  40%|████      | 120/300 [00:45<01:05,  2.73it/s]Epoch 4:  40%|████      | 121/300 [00:46<01:05,  2.72it/s]Epoch 4:  41%|████      | 122/300 [00:46<01:04,  2.78it/s]Epoch 4:  41%|████      | 123/300 [00:46<01:09,  2.53it/s]Epoch 4:  41%|████▏     | 124/300 [00:47<01:07,  2.60it/s]Epoch 4:  42%|████▏     | 125/300 [00:47<01:08,  2.54it/s]Epoch 4:  42%|████▏     | 126/300 [00:48<01:06,  2.61it/s]Epoch 4:  42%|████▏     | 127/300 [00:48<01:11,  2.44it/s]Epoch 4:  43%|████▎     | 128/300 [00:48<01:09,  2.47it/s]Epoch 4:  43%|████▎     | 129/300 [00:49<01:09,  2.46it/s]Epoch 4:  43%|████▎     | 130/300 [00:49<01:06,  2.56it/s]Epoch 4:  44%|████▎     | 131/300 [00:50<01:08,  2.46it/s]Epoch 4:  44%|████▍     | 132/300 [00:50<01:07,  2.47it/s]Epoch 4:  44%|████▍     | 133/300 [00:50<01:05,  2.56it/s]Epoch 4:  45%|████▍     | 134/300 [00:51<01:04,  2.59it/s]Epoch 4:  45%|████▌     | 135/300 [00:51<01:07,  2.44it/s]Epoch 4:  45%|████▌     | 136/300 [00:52<01:05,  2.52it/s]Epoch 4:  46%|████▌     | 137/300 [00:52<01:05,  2.51it/s]Epoch 4:  46%|████▌     | 138/300 [00:52<01:08,  2.37it/s]Epoch 4:  46%|████▋     | 139/300 [00:53<01:04,  2.48it/s]06/19/2022 14:36:58 - INFO - __main__ - global step: 670; train loss: 8.039661407470703; dev loss: 7.995343208312988
Epoch 4:  47%|████▋     | 140/300 [00:53<01:07,  2.36it/s]Epoch 4:  47%|████▋     | 141/300 [00:54<01:05,  2.42it/s]Epoch 4:  47%|████▋     | 142/300 [00:54<01:04,  2.47it/s]Epoch 4:  48%|████▊     | 143/300 [00:54<01:01,  2.56it/s]Epoch 4:  48%|████▊     | 144/300 [00:55<01:06,  2.36it/s]Epoch 4:  48%|████▊     | 145/300 [00:55<01:02,  2.46it/s]Epoch 4:  49%|████▊     | 146/300 [00:56<01:00,  2.54it/s]Epoch 4:  49%|████▉     | 147/300 [00:56<01:01,  2.50it/s]Epoch 4:  49%|████▉     | 148/300 [00:57<01:04,  2.37it/s]Epoch 4:  50%|████▉     | 149/300 [00:57<01:04,  2.36it/s]Epoch 4:  50%|█████     | 150/300 [00:57<01:00,  2.46it/s]Epoch 4:  50%|█████     | 151/300 [00:58<00:58,  2.55it/s]Epoch 4:  51%|█████     | 152/300 [00:58<01:01,  2.41it/s]Epoch 4:  51%|█████     | 153/300 [00:59<01:01,  2.41it/s]Epoch 4:  51%|█████▏    | 154/300 [00:59<01:00,  2.40it/s]Epoch 4:  52%|█████▏    | 155/300 [00:59<00:57,  2.52it/s]Epoch 4:  52%|█████▏    | 156/300 [01:00<01:01,  2.34it/s]Epoch 4:  52%|█████▏    | 157/300 [01:00<00:57,  2.49it/s]Epoch 4:  53%|█████▎    | 158/300 [01:01<00:54,  2.61it/s]Epoch 4:  53%|█████▎    | 159/300 [01:01<00:52,  2.71it/s]06/19/2022 14:37:06 - INFO - __main__ - global step: 680; train loss: 8.523404121398926; dev loss: 8.569134712219238
Epoch 4:  53%|█████▎    | 160/300 [01:01<00:54,  2.56it/s]Epoch 4:  54%|█████▎    | 161/300 [01:02<00:52,  2.66it/s]Epoch 4:  54%|█████▍    | 162/300 [01:02<00:50,  2.76it/s]Epoch 4:  54%|█████▍    | 163/300 [01:02<00:51,  2.65it/s]Epoch 4:  55%|█████▍    | 164/300 [01:03<00:55,  2.46it/s]Epoch 4:  55%|█████▌    | 165/300 [01:03<00:51,  2.60it/s]Epoch 4:  55%|█████▌    | 166/300 [01:04<00:50,  2.63it/s]Epoch 4:  56%|█████▌    | 167/300 [01:04<00:49,  2.66it/s]Epoch 4:  56%|█████▌    | 168/300 [01:04<00:47,  2.75it/s]Epoch 4:  56%|█████▋    | 169/300 [01:05<00:56,  2.32it/s]Epoch 4:  57%|█████▋    | 170/300 [01:05<01:01,  2.13it/s]Epoch 4:  57%|█████▋    | 171/300 [01:06<00:56,  2.28it/s]Epoch 4:  57%|█████▋    | 172/300 [01:06<00:58,  2.19it/s]Epoch 4:  58%|█████▊    | 173/300 [01:07<01:02,  2.04it/s]Epoch 4:  58%|█████▊    | 174/300 [01:07<00:59,  2.12it/s]Epoch 4:  58%|█████▊    | 175/300 [01:08<00:56,  2.23it/s]Epoch 4:  59%|█████▊    | 176/300 [01:08<00:52,  2.37it/s]Epoch 4:  59%|█████▉    | 177/300 [01:08<00:51,  2.38it/s]Epoch 4:  59%|█████▉    | 178/300 [01:09<00:48,  2.50it/s]Epoch 4:  60%|█████▉    | 179/300 [01:09<00:45,  2.65it/s]06/19/2022 14:37:14 - INFO - __main__ - global step: 690; train loss: 7.972927093505859; dev loss: 8.24262523651123
Epoch 4:  60%|██████    | 180/300 [01:09<00:43,  2.77it/s]Epoch 4:  60%|██████    | 181/300 [01:10<00:46,  2.58it/s]Epoch 4:  61%|██████    | 182/300 [01:10<00:50,  2.32it/s]Epoch 4:  61%|██████    | 183/300 [01:11<00:48,  2.43it/s]Epoch 4:  61%|██████▏   | 184/300 [01:11<00:47,  2.43it/s]Epoch 4:  62%|██████▏   | 185/300 [01:12<00:51,  2.23it/s]Epoch 4:  62%|██████▏   | 186/300 [01:12<00:50,  2.25it/s]Epoch 4:  62%|██████▏   | 187/300 [01:13<00:50,  2.22it/s]Epoch 4:  63%|██████▎   | 188/300 [01:13<00:49,  2.27it/s]Epoch 4:  63%|██████▎   | 189/300 [01:14<00:50,  2.18it/s]Epoch 4:  63%|██████▎   | 190/300 [01:14<00:49,  2.24it/s]Epoch 4:  64%|██████▎   | 191/300 [01:14<00:49,  2.22it/s]Epoch 4:  64%|██████▍   | 192/300 [01:15<00:48,  2.22it/s]Epoch 4:  64%|██████▍   | 193/300 [01:15<00:48,  2.19it/s]Epoch 4:  65%|██████▍   | 194/300 [01:16<00:48,  2.18it/s]Epoch 4:  65%|██████▌   | 195/300 [01:16<00:47,  2.23it/s]Epoch 4:  65%|██████▌   | 196/300 [01:17<00:43,  2.40it/s]Epoch 4:  66%|██████▌   | 197/300 [01:17<00:42,  2.40it/s]Epoch 4:  66%|██████▌   | 198/300 [01:18<00:45,  2.26it/s]Epoch 4:  66%|██████▋   | 199/300 [01:18<00:41,  2.43it/s]06/19/2022 14:37:22 - INFO - __main__ - global step: 700; train loss: 7.788443088531494; dev loss: 7.784582614898682
Epoch 4:  67%|██████▋   | 200/300 [01:18<00:39,  2.56it/s]Epoch 4:  67%|██████▋   | 201/300 [01:19<00:37,  2.64it/s]Epoch 4:  67%|██████▋   | 202/300 [01:19<00:39,  2.50it/s]Epoch 4:  68%|██████▊   | 203/300 [01:19<00:40,  2.42it/s]Epoch 4:  68%|██████▊   | 204/300 [01:20<00:40,  2.36it/s]Epoch 4:  68%|██████▊   | 205/300 [01:20<00:41,  2.29it/s]Epoch 4:  69%|██████▊   | 206/300 [01:21<00:44,  2.10it/s]Epoch 4:  69%|██████▉   | 207/300 [01:21<00:43,  2.13it/s]Epoch 4:  69%|██████▉   | 208/300 [01:22<00:42,  2.15it/s]Epoch 4:  70%|██████▉   | 209/300 [01:22<00:42,  2.15it/s]Epoch 4:  70%|███████   | 210/300 [01:23<00:43,  2.05it/s]Epoch 4:  70%|███████   | 211/300 [01:23<00:42,  2.09it/s]Epoch 4:  71%|███████   | 212/300 [01:24<00:41,  2.13it/s]Epoch 4:  71%|███████   | 213/300 [01:24<00:40,  2.14it/s]Epoch 4:  71%|███████▏  | 214/300 [01:25<00:39,  2.15it/s]Epoch 4:  72%|███████▏  | 215/300 [01:25<00:37,  2.28it/s]Epoch 4:  72%|███████▏  | 216/300 [01:25<00:34,  2.43it/s]Epoch 4:  72%|███████▏  | 217/300 [01:26<00:34,  2.42it/s]Epoch 4:  73%|███████▎  | 218/300 [01:26<00:38,  2.16it/s]Epoch 4:  73%|███████▎  | 219/300 [01:27<00:37,  2.17it/s]06/19/2022 14:37:32 - INFO - __main__ - global step: 710; train loss: 7.567774295806885; dev loss: 7.826577663421631
Epoch 4:  73%|███████▎  | 220/300 [01:27<00:36,  2.18it/s]Epoch 4:  74%|███████▎  | 221/300 [01:28<00:35,  2.20it/s]Epoch 4:  74%|███████▍  | 222/300 [01:28<00:35,  2.17it/s]Epoch 4:  74%|███████▍  | 223/300 [01:29<00:35,  2.14it/s]Epoch 4:  75%|███████▍  | 224/300 [01:29<00:36,  2.09it/s]Epoch 4:  75%|███████▌  | 225/300 [01:30<00:35,  2.10it/s]Epoch 4:  75%|███████▌  | 226/300 [01:30<00:34,  2.15it/s]Epoch 4:  76%|███████▌  | 227/300 [01:31<00:38,  1.88it/s]Epoch 4:  76%|███████▌  | 228/300 [01:31<00:36,  1.96it/s]Epoch 4:  76%|███████▋  | 229/300 [01:32<00:34,  2.04it/s]Epoch 4:  77%|███████▋  | 230/300 [01:32<00:31,  2.25it/s]Epoch 4:  77%|███████▋  | 231/300 [01:32<00:30,  2.23it/s]Epoch 4:  77%|███████▋  | 232/300 [01:33<00:28,  2.39it/s]Epoch 4:  78%|███████▊  | 233/300 [01:33<00:27,  2.48it/s]Epoch 4:  78%|███████▊  | 234/300 [01:34<00:27,  2.36it/s]Epoch 4:  78%|███████▊  | 235/300 [01:34<00:30,  2.14it/s]Epoch 4:  79%|███████▊  | 236/300 [01:35<00:29,  2.18it/s]Epoch 4:  79%|███████▉  | 237/300 [01:35<00:27,  2.31it/s]Epoch 4:  79%|███████▉  | 238/300 [01:35<00:26,  2.34it/s]Epoch 4:  80%|███████▉  | 239/300 [01:36<00:26,  2.26it/s]06/19/2022 14:37:41 - INFO - __main__ - global step: 720; train loss: 8.433286666870117; dev loss: 8.087199211120605
Epoch 4:  80%|████████  | 240/300 [01:36<00:25,  2.35it/s]Epoch 4:  80%|████████  | 241/300 [01:37<00:24,  2.41it/s]Epoch 4:  81%|████████  | 242/300 [01:37<00:23,  2.44it/s]Epoch 4:  81%|████████  | 243/300 [01:38<00:25,  2.26it/s]Epoch 4:  81%|████████▏ | 244/300 [01:38<00:26,  2.14it/s]Epoch 4:  82%|████████▏ | 245/300 [01:39<00:23,  2.30it/s]Epoch 4:  82%|████████▏ | 246/300 [01:39<00:23,  2.34it/s]Epoch 4:  82%|████████▏ | 247/300 [01:39<00:23,  2.26it/s]Epoch 4:  83%|████████▎ | 248/300 [01:40<00:25,  2.03it/s]Epoch 4:  83%|████████▎ | 249/300 [01:40<00:24,  2.12it/s]Epoch 4:  83%|████████▎ | 250/300 [01:41<00:21,  2.29it/s]Epoch 4:  84%|████████▎ | 251/300 [01:41<00:20,  2.42it/s]Epoch 4:  84%|████████▍ | 252/300 [01:42<00:20,  2.33it/s]Epoch 4:  84%|████████▍ | 253/300 [01:42<00:19,  2.46it/s]Epoch 4:  85%|████████▍ | 254/300 [01:42<00:18,  2.49it/s]Epoch 4:  85%|████████▌ | 255/300 [01:43<00:18,  2.48it/s]Epoch 4:  85%|████████▌ | 256/300 [01:43<00:19,  2.30it/s]Epoch 4:  86%|████████▌ | 257/300 [01:44<00:18,  2.30it/s]Epoch 4:  86%|████████▌ | 258/300 [01:44<00:18,  2.24it/s]Epoch 4:  86%|████████▋ | 259/300 [01:45<00:19,  2.15it/s]06/19/2022 14:37:50 - INFO - __main__ - global step: 730; train loss: 7.7617621421813965; dev loss: 7.735413551330566
Epoch 4:  87%|████████▋ | 260/300 [01:45<00:19,  2.09it/s]Epoch 4:  87%|████████▋ | 261/300 [01:46<00:18,  2.14it/s]Epoch 4:  87%|████████▋ | 262/300 [01:46<00:18,  2.09it/s]Epoch 4:  88%|████████▊ | 263/300 [01:47<00:16,  2.20it/s]Epoch 4:  88%|████████▊ | 264/300 [01:47<00:17,  2.01it/s]Epoch 4:  88%|████████▊ | 265/300 [01:48<00:17,  2.00it/s]Epoch 4:  89%|████████▊ | 266/300 [01:48<00:15,  2.19it/s]Epoch 4:  89%|████████▉ | 267/300 [01:48<00:14,  2.33it/s]Epoch 4:  89%|████████▉ | 268/300 [01:49<00:14,  2.26it/s]Epoch 4:  90%|████████▉ | 269/300 [01:49<00:13,  2.36it/s]Epoch 4:  90%|█████████ | 270/300 [01:50<00:12,  2.47it/s]Epoch 4:  90%|█████████ | 271/300 [01:50<00:11,  2.53it/s]Epoch 4:  91%|█████████ | 272/300 [01:50<00:11,  2.40it/s]Epoch 4:  91%|█████████ | 273/300 [01:51<00:10,  2.46it/s]Epoch 4:  91%|█████████▏| 274/300 [01:51<00:10,  2.51it/s]Epoch 4:  92%|█████████▏| 275/300 [01:52<00:10,  2.31it/s]Epoch 4:  92%|█████████▏| 276/300 [01:52<00:09,  2.41it/s]Epoch 4:  92%|█████████▏| 277/300 [01:53<00:11,  2.08it/s]Epoch 4:  93%|█████████▎| 278/300 [01:53<00:09,  2.21it/s]Epoch 4:  93%|█████████▎| 279/300 [01:53<00:09,  2.31it/s]06/19/2022 14:37:58 - INFO - __main__ - global step: 740; train loss: 7.9995574951171875; dev loss: 8.1997652053833
Epoch 4:  93%|█████████▎| 280/300 [01:54<00:09,  2.12it/s]Epoch 4:  94%|█████████▎| 281/300 [01:55<00:09,  2.03it/s]Epoch 4:  94%|█████████▍| 282/300 [01:55<00:08,  2.08it/s]Epoch 4:  94%|█████████▍| 283/300 [01:56<00:08,  2.12it/s]Epoch 4:  95%|█████████▍| 284/300 [01:56<00:07,  2.07it/s]Epoch 4:  95%|█████████▌| 285/300 [01:57<00:07,  2.03it/s]Epoch 4:  95%|█████████▌| 286/300 [01:57<00:06,  2.18it/s]Epoch 4:  96%|█████████▌| 287/300 [01:57<00:05,  2.27it/s]Epoch 4:  96%|█████████▌| 288/300 [01:58<00:05,  2.39it/s]Epoch 4:  96%|█████████▋| 289/300 [01:58<00:04,  2.20it/s]Epoch 4:  97%|█████████▋| 290/300 [01:59<00:04,  2.34it/s]Epoch 4:  97%|█████████▋| 291/300 [01:59<00:03,  2.46it/s]Epoch 4:  97%|█████████▋| 292/300 [01:59<00:03,  2.48it/s]Epoch 4:  98%|█████████▊| 293/300 [02:00<00:03,  2.23it/s]Epoch 4:  98%|█████████▊| 294/300 [02:00<00:02,  2.14it/s]Epoch 4:  98%|█████████▊| 295/300 [02:01<00:02,  2.15it/s]Epoch 4:  99%|█████████▊| 296/300 [02:01<00:01,  2.14it/s]Epoch 4:  99%|█████████▉| 297/300 [02:02<00:01,  1.99it/s]Epoch 4:  99%|█████████▉| 298/300 [02:02<00:00,  2.02it/s]Epoch 4: 100%|█████████▉| 299/300 [02:03<00:00,  2.06it/s]06/19/2022 14:38:08 - INFO - __main__ - global step: 750; train loss: 8.089941024780273; dev loss: 8.242169380187988
Epoch 4: 100%|██████████| 300/300 [02:03<00:00,  2.07it/s]Epoch 4: 100%|██████████| 300/300 [02:03<00:00,  2.42it/s]
Epoch 5:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 5:   0%|          | 1/300 [00:00<01:43,  2.88it/s]Epoch 5:   1%|          | 2/300 [00:00<02:07,  2.34it/s]Epoch 5:   1%|          | 3/300 [00:01<01:58,  2.51it/s]Epoch 5:   1%|▏         | 4/300 [00:01<01:52,  2.63it/s]Epoch 5:   2%|▏         | 5/300 [00:01<01:49,  2.68it/s]Epoch 5:   2%|▏         | 6/300 [00:02<02:03,  2.38it/s]Epoch 5:   2%|▏         | 7/300 [00:02<01:57,  2.50it/s]Epoch 5:   3%|▎         | 8/300 [00:03<02:01,  2.41it/s]Epoch 5:   3%|▎         | 9/300 [00:03<02:13,  2.17it/s]Epoch 5:   3%|▎         | 10/300 [00:04<02:27,  1.96it/s]Epoch 5:   4%|▎         | 11/300 [00:04<02:26,  1.97it/s]Epoch 5:   4%|▍         | 12/300 [00:05<02:19,  2.06it/s]Epoch 5:   4%|▍         | 13/300 [00:05<02:21,  2.03it/s]Epoch 5:   5%|▍         | 14/300 [00:06<02:31,  1.89it/s]Epoch 5:   5%|▌         | 15/300 [00:06<02:24,  1.97it/s]Epoch 5:   5%|▌         | 16/300 [00:07<02:18,  2.05it/s]Epoch 5:   6%|▌         | 17/300 [00:07<02:19,  2.03it/s]Epoch 5:   6%|▌         | 18/300 [00:08<02:31,  1.86it/s]Epoch 5:   6%|▋         | 19/300 [00:09<02:27,  1.91it/s]06/19/2022 14:38:17 - INFO - __main__ - global step: 760; train loss: 7.588549613952637; dev loss: 7.527655124664307
Epoch 5:   7%|▋         | 20/300 [00:09<02:20,  2.00it/s]Epoch 5:   7%|▋         | 21/300 [00:09<02:13,  2.10it/s]Epoch 5:   7%|▋         | 22/300 [00:10<02:16,  2.04it/s]Epoch 5:   8%|▊         | 23/300 [00:10<02:09,  2.14it/s]Epoch 5:   8%|▊         | 24/300 [00:11<02:10,  2.12it/s]Epoch 5:   8%|▊         | 25/300 [00:11<02:19,  1.98it/s]Epoch 5:   9%|▊         | 26/300 [00:12<02:30,  1.82it/s]Epoch 5:   9%|▉         | 27/300 [00:12<02:17,  1.99it/s]Epoch 5:   9%|▉         | 28/300 [00:13<02:13,  2.04it/s]Epoch 5:  10%|▉         | 29/300 [00:13<02:12,  2.04it/s]Epoch 5:  10%|█         | 30/300 [00:14<02:10,  2.08it/s]Epoch 5:  10%|█         | 31/300 [00:14<02:24,  1.87it/s]Epoch 5:  11%|█         | 32/300 [00:15<02:17,  1.94it/s]Epoch 5:  11%|█         | 33/300 [00:15<02:19,  1.92it/s]Epoch 5:  11%|█▏        | 34/300 [00:16<02:08,  2.08it/s]Epoch 5:  12%|█▏        | 35/300 [00:16<02:12,  2.00it/s]Epoch 5:  12%|█▏        | 36/300 [00:17<02:05,  2.10it/s]Epoch 5:  12%|█▏        | 37/300 [00:17<02:04,  2.11it/s]Epoch 5:  13%|█▎        | 38/300 [00:18<02:08,  2.04it/s]Epoch 5:  13%|█▎        | 39/300 [00:18<02:06,  2.06it/s]06/19/2022 14:38:27 - INFO - __main__ - global step: 770; train loss: 7.779818534851074; dev loss: 7.7659912109375
Epoch 5:  13%|█▎        | 40/300 [00:19<01:57,  2.22it/s]Epoch 5:  14%|█▎        | 41/300 [00:19<01:51,  2.32it/s]Epoch 5:  14%|█▍        | 42/300 [00:19<01:45,  2.44it/s]Epoch 5:  14%|█▍        | 43/300 [00:20<01:50,  2.33it/s]Epoch 5:  15%|█▍        | 44/300 [00:20<01:44,  2.45it/s]Epoch 5:  15%|█▌        | 45/300 [00:21<01:40,  2.55it/s]Epoch 5:  15%|█▌        | 46/300 [00:21<01:37,  2.59it/s]Epoch 5:  16%|█▌        | 47/300 [00:21<01:43,  2.44it/s]Epoch 5:  16%|█▌        | 48/300 [00:22<01:39,  2.52it/s]Epoch 5:  16%|█▋        | 49/300 [00:22<01:37,  2.58it/s]Epoch 5:  17%|█▋        | 50/300 [00:23<01:35,  2.61it/s]Epoch 5:  17%|█▋        | 51/300 [00:23<01:47,  2.32it/s]Epoch 5:  17%|█▋        | 52/300 [00:24<01:45,  2.35it/s]Epoch 5:  18%|█▊        | 53/300 [00:24<01:48,  2.27it/s]Epoch 5:  18%|█▊        | 54/300 [00:24<01:48,  2.26it/s]Epoch 5:  18%|█▊        | 55/300 [00:25<01:41,  2.41it/s]Epoch 5:  19%|█▊        | 56/300 [00:25<01:47,  2.28it/s]Epoch 5:  19%|█▉        | 57/300 [00:26<01:41,  2.40it/s]Epoch 5:  19%|█▉        | 58/300 [00:26<01:36,  2.50it/s]Epoch 5:  20%|█▉        | 59/300 [00:26<01:35,  2.52it/s]06/19/2022 14:38:35 - INFO - __main__ - global step: 780; train loss: 8.259783744812012; dev loss: 8.620302200317383
Epoch 5:  20%|██        | 60/300 [00:27<01:47,  2.22it/s]Epoch 5:  20%|██        | 61/300 [00:27<01:42,  2.33it/s]Epoch 5:  21%|██        | 62/300 [00:28<01:39,  2.40it/s]Epoch 5:  21%|██        | 63/300 [00:28<01:34,  2.50it/s]Epoch 5:  21%|██▏       | 64/300 [00:29<01:42,  2.31it/s]Epoch 5:  22%|██▏       | 65/300 [00:29<01:41,  2.31it/s]Epoch 5:  22%|██▏       | 66/300 [00:29<01:36,  2.43it/s]Epoch 5:  22%|██▏       | 67/300 [00:30<01:35,  2.44it/s]Epoch 5:  23%|██▎       | 68/300 [00:30<01:42,  2.27it/s]Epoch 5:  23%|██▎       | 69/300 [00:31<01:37,  2.37it/s]Epoch 5:  23%|██▎       | 70/300 [00:31<01:33,  2.45it/s]Epoch 5:  24%|██▎       | 71/300 [00:32<01:38,  2.33it/s]Epoch 5:  24%|██▍       | 72/300 [00:32<01:44,  2.19it/s]Epoch 5:  24%|██▍       | 73/300 [00:32<01:40,  2.26it/s]Epoch 5:  25%|██▍       | 74/300 [00:33<01:42,  2.21it/s]Epoch 5:  25%|██▌       | 75/300 [00:33<01:39,  2.25it/s]Epoch 5:  25%|██▌       | 76/300 [00:34<01:50,  2.03it/s]Epoch 5:  26%|██▌       | 77/300 [00:34<01:48,  2.06it/s]Epoch 5:  26%|██▌       | 78/300 [00:35<01:45,  2.10it/s]Epoch 5:  26%|██▋       | 79/300 [00:35<01:44,  2.11it/s]06/19/2022 14:38:44 - INFO - __main__ - global step: 790; train loss: 8.100862503051758; dev loss: 7.998152256011963
Epoch 5:  27%|██▋       | 80/300 [00:36<01:53,  1.94it/s]Epoch 5:  27%|██▋       | 81/300 [00:36<01:49,  2.00it/s]Epoch 5:  27%|██▋       | 82/300 [00:37<01:44,  2.08it/s]Epoch 5:  28%|██▊       | 83/300 [00:37<01:36,  2.25it/s]Epoch 5:  28%|██▊       | 84/300 [00:38<01:30,  2.40it/s]Epoch 5:  28%|██▊       | 85/300 [00:38<01:33,  2.31it/s]Epoch 5:  29%|██▊       | 86/300 [00:38<01:29,  2.39it/s]Epoch 5:  29%|██▉       | 87/300 [00:39<01:27,  2.44it/s]Epoch 5:  29%|██▉       | 88/300 [00:39<01:25,  2.48it/s]Epoch 5:  30%|██▉       | 89/300 [00:40<01:29,  2.36it/s]Epoch 5:  30%|███       | 90/300 [00:40<01:25,  2.46it/s]Epoch 5:  30%|███       | 91/300 [00:40<01:21,  2.55it/s]Epoch 5:  31%|███       | 92/300 [00:41<01:22,  2.51it/s]Epoch 5:  31%|███       | 93/300 [00:41<01:26,  2.40it/s]Epoch 5:  31%|███▏      | 94/300 [00:42<01:23,  2.48it/s]Epoch 5:  32%|███▏      | 95/300 [00:42<01:20,  2.54it/s]Epoch 5:  32%|███▏      | 96/300 [00:42<01:19,  2.58it/s]Epoch 5:  32%|███▏      | 97/300 [00:43<01:24,  2.41it/s]Epoch 5:  33%|███▎      | 98/300 [00:43<01:20,  2.52it/s]Epoch 5:  33%|███▎      | 99/300 [00:44<01:23,  2.40it/s]06/19/2022 14:38:52 - INFO - __main__ - global step: 800; train loss: 7.5393266677856445; dev loss: 7.670403957366943
Epoch 5:  33%|███▎      | 100/300 [00:44<01:31,  2.19it/s]Epoch 5:  34%|███▎      | 101/300 [00:45<01:43,  1.92it/s]Epoch 5:  34%|███▍      | 102/300 [00:45<01:42,  1.94it/s]Epoch 5:  34%|███▍      | 103/300 [00:46<01:35,  2.06it/s]Epoch 5:  35%|███▍      | 104/300 [00:46<01:28,  2.22it/s]Epoch 5:  35%|███▌      | 105/300 [00:47<01:29,  2.18it/s]Epoch 5:  35%|███▌      | 106/300 [00:47<01:28,  2.20it/s]Epoch 5:  36%|███▌      | 107/300 [00:48<01:27,  2.20it/s]Epoch 5:  36%|███▌      | 108/300 [00:48<01:29,  2.13it/s]Epoch 5:  36%|███▋      | 109/300 [00:49<01:30,  2.11it/s]Epoch 5:  37%|███▋      | 110/300 [00:49<01:40,  1.90it/s]Epoch 5:  37%|███▋      | 111/300 [00:50<01:38,  1.92it/s]Epoch 5:  37%|███▋      | 112/300 [00:50<01:38,  1.91it/s]Epoch 5:  38%|███▊      | 113/300 [00:51<01:34,  1.98it/s]Epoch 5:  38%|███▊      | 114/300 [00:51<01:37,  1.90it/s]Epoch 5:  38%|███▊      | 115/300 [00:52<01:35,  1.95it/s]Epoch 5:  39%|███▊      | 116/300 [00:52<01:31,  2.00it/s]Epoch 5:  39%|███▉      | 117/300 [00:53<01:28,  2.06it/s]Epoch 5:  39%|███▉      | 118/300 [00:53<01:33,  1.94it/s]Epoch 5:  40%|███▉      | 119/300 [00:54<01:30,  2.00it/s]06/19/2022 14:39:02 - INFO - __main__ - global step: 810; train loss: 8.076876640319824; dev loss: 7.848074436187744
Epoch 5:  40%|████      | 120/300 [00:54<01:27,  2.05it/s]Epoch 5:  40%|████      | 121/300 [00:55<01:23,  2.13it/s]Epoch 5:  41%|████      | 122/300 [00:55<01:24,  2.12it/s]Epoch 5:  41%|████      | 123/300 [00:56<01:19,  2.22it/s]Epoch 5:  41%|████▏     | 124/300 [00:56<01:14,  2.38it/s]Epoch 5:  42%|████▏     | 125/300 [00:56<01:09,  2.51it/s]Epoch 5:  42%|████▏     | 126/300 [00:57<01:19,  2.20it/s]Epoch 5:  42%|████▏     | 127/300 [00:57<01:16,  2.28it/s]Epoch 5:  43%|████▎     | 128/300 [00:58<01:16,  2.24it/s]Epoch 5:  43%|████▎     | 129/300 [00:58<01:16,  2.25it/s]Epoch 5:  43%|████▎     | 130/300 [00:59<01:22,  2.07it/s]Epoch 5:  44%|████▎     | 131/300 [00:59<01:15,  2.25it/s]Epoch 5:  44%|████▍     | 132/300 [00:59<01:10,  2.39it/s]Epoch 5:  44%|████▍     | 133/300 [01:00<01:08,  2.44it/s]Epoch 5:  45%|████▍     | 134/300 [01:00<01:13,  2.27it/s]Epoch 5:  45%|████▌     | 135/300 [01:01<01:14,  2.21it/s]Epoch 5:  45%|████▌     | 136/300 [01:01<01:17,  2.12it/s]Epoch 5:  46%|████▌     | 137/300 [01:02<01:21,  2.00it/s]Epoch 5:  46%|████▌     | 138/300 [01:02<01:20,  2.02it/s]Epoch 5:  46%|████▋     | 139/300 [01:03<01:28,  1.81it/s]06/19/2022 14:39:12 - INFO - __main__ - global step: 820; train loss: 7.841548919677734; dev loss: 8.298831939697266
Epoch 5:  47%|████▋     | 140/300 [01:04<01:27,  1.84it/s]Epoch 5:  47%|████▋     | 141/300 [01:04<01:21,  1.95it/s]Epoch 5:  47%|████▋     | 142/300 [01:04<01:14,  2.11it/s]Epoch 5:  48%|████▊     | 143/300 [01:05<01:16,  2.05it/s]Epoch 5:  48%|████▊     | 144/300 [01:05<01:10,  2.21it/s]Epoch 5:  48%|████▊     | 145/300 [01:06<01:06,  2.32it/s]Epoch 5:  49%|████▊     | 146/300 [01:06<01:08,  2.25it/s]Epoch 5:  49%|████▉     | 147/300 [01:07<01:19,  1.93it/s]Epoch 5:  49%|████▉     | 148/300 [01:07<01:15,  2.03it/s]Epoch 5:  50%|████▉     | 149/300 [01:08<01:10,  2.13it/s]Epoch 5:  50%|█████     | 150/300 [01:08<01:05,  2.29it/s]Epoch 5:  50%|█████     | 151/300 [01:09<01:06,  2.23it/s]Epoch 5:  51%|█████     | 152/300 [01:09<01:02,  2.38it/s]Epoch 5:  51%|█████     | 153/300 [01:09<01:00,  2.43it/s]Epoch 5:  51%|█████▏    | 154/300 [01:10<01:02,  2.34it/s]Epoch 5:  52%|█████▏    | 155/300 [01:10<01:07,  2.15it/s]Epoch 5:  52%|█████▏    | 156/300 [01:11<01:07,  2.13it/s]Epoch 5:  52%|█████▏    | 157/300 [01:11<01:03,  2.25it/s]Epoch 5:  53%|█████▎    | 158/300 [01:12<01:03,  2.23it/s]Epoch 5:  53%|█████▎    | 159/300 [01:12<01:04,  2.18it/s]06/19/2022 14:39:21 - INFO - __main__ - global step: 830; train loss: 7.618116855621338; dev loss: 7.716672420501709
Epoch 5:  53%|█████▎    | 160/300 [01:12<01:00,  2.32it/s]Epoch 5:  54%|█████▎    | 161/300 [01:13<00:57,  2.40it/s]Epoch 5:  54%|█████▍    | 162/300 [01:13<01:00,  2.28it/s]Epoch 5:  54%|█████▍    | 163/300 [01:14<01:02,  2.19it/s]Epoch 5:  55%|█████▍    | 164/300 [01:14<01:10,  1.92it/s]Epoch 5:  55%|█████▌    | 165/300 [01:15<01:08,  1.97it/s]Epoch 5:  55%|█████▌    | 166/300 [01:16<01:08,  1.95it/s]Epoch 5:  56%|█████▌    | 167/300 [01:16<01:09,  1.91it/s]Epoch 5:  56%|█████▌    | 168/300 [01:17<01:12,  1.82it/s]Epoch 5:  56%|█████▋    | 169/300 [01:17<01:08,  1.90it/s]Epoch 5:  57%|█████▋    | 170/300 [01:18<01:09,  1.88it/s]Epoch 5:  57%|█████▋    | 171/300 [01:18<01:05,  1.97it/s]Epoch 5:  57%|█████▋    | 172/300 [01:19<01:08,  1.87it/s]Epoch 5:  58%|█████▊    | 173/300 [01:19<01:05,  1.94it/s]Epoch 5:  58%|█████▊    | 174/300 [01:20<01:02,  2.00it/s]Epoch 5:  58%|█████▊    | 175/300 [01:20<01:02,  2.01it/s]Epoch 5:  59%|█████▊    | 176/300 [01:21<01:02,  1.99it/s]Epoch 5:  59%|█████▉    | 177/300 [01:21<00:57,  2.14it/s]Epoch 5:  59%|█████▉    | 178/300 [01:21<00:54,  2.24it/s]Epoch 5:  60%|█████▉    | 179/300 [01:22<00:52,  2.30it/s]06/19/2022 14:39:31 - INFO - __main__ - global step: 840; train loss: 7.63986873626709; dev loss: 7.920223236083984
Epoch 5:  60%|██████    | 180/300 [01:22<00:55,  2.15it/s]Epoch 5:  60%|██████    | 181/300 [01:23<00:55,  2.16it/s]Epoch 5:  61%|██████    | 182/300 [01:23<00:53,  2.22it/s]Epoch 5:  61%|██████    | 183/300 [01:24<00:54,  2.13it/s]Epoch 5:  61%|██████▏   | 184/300 [01:24<00:55,  2.08it/s]Epoch 5:  62%|██████▏   | 185/300 [01:25<00:52,  2.21it/s]Epoch 5:  62%|██████▏   | 186/300 [01:25<00:49,  2.32it/s]Epoch 5:  62%|██████▏   | 187/300 [01:25<00:47,  2.36it/s]Epoch 5:  63%|██████▎   | 188/300 [01:26<00:53,  2.08it/s]Epoch 5:  63%|██████▎   | 189/300 [01:26<00:51,  2.17it/s]Epoch 5:  63%|██████▎   | 190/300 [01:27<00:49,  2.20it/s]Epoch 5:  64%|██████▎   | 191/300 [01:27<00:50,  2.15it/s]Epoch 5:  64%|██████▍   | 192/300 [01:28<00:52,  2.04it/s]Epoch 5:  64%|██████▍   | 193/300 [01:29<00:56,  1.90it/s]Epoch 5:  65%|██████▍   | 194/300 [01:29<00:53,  1.99it/s]Epoch 5:  65%|██████▌   | 195/300 [01:29<00:51,  2.03it/s]Epoch 5:  65%|██████▌   | 196/300 [01:30<00:48,  2.14it/s]Epoch 5:  66%|██████▌   | 197/300 [01:30<00:50,  2.04it/s]Epoch 5:  66%|██████▌   | 198/300 [01:31<00:48,  2.11it/s]Epoch 5:  66%|██████▋   | 199/300 [01:31<00:47,  2.13it/s]06/19/2022 14:39:40 - INFO - __main__ - global step: 850; train loss: 8.008347511291504; dev loss: 7.828642845153809
Epoch 5:  67%|██████▋   | 200/300 [01:32<00:47,  2.08it/s]Epoch 5:  67%|██████▋   | 201/300 [01:32<00:52,  1.88it/s]Epoch 5:  67%|██████▋   | 202/300 [01:33<00:51,  1.91it/s]Epoch 5:  68%|██████▊   | 203/300 [01:33<00:47,  2.04it/s]Epoch 5:  68%|██████▊   | 204/300 [01:34<00:46,  2.07it/s]Epoch 5:  68%|██████▊   | 205/300 [01:35<00:51,  1.85it/s]Epoch 5:  69%|██████▊   | 206/300 [01:35<00:49,  1.91it/s]Epoch 5:  69%|██████▉   | 207/300 [01:35<00:46,  2.00it/s]Epoch 5:  69%|██████▉   | 208/300 [01:36<00:45,  2.04it/s]Epoch 5:  70%|██████▉   | 209/300 [01:37<00:47,  1.90it/s]Epoch 5:  70%|███████   | 210/300 [01:37<00:45,  1.97it/s]Epoch 5:  70%|███████   | 211/300 [01:37<00:43,  2.04it/s]Epoch 5:  71%|███████   | 212/300 [01:38<00:42,  2.05it/s]Epoch 5:  71%|███████   | 213/300 [01:39<00:45,  1.91it/s]Epoch 5:  71%|███████▏  | 214/300 [01:39<00:43,  1.97it/s]Epoch 5:  72%|███████▏  | 215/300 [01:39<00:39,  2.14it/s]Epoch 5:  72%|███████▏  | 216/300 [01:40<00:36,  2.30it/s]Epoch 5:  72%|███████▏  | 217/300 [01:40<00:34,  2.44it/s]Epoch 5:  73%|███████▎  | 218/300 [01:41<00:37,  2.20it/s]Epoch 5:  73%|███████▎  | 219/300 [01:41<00:34,  2.34it/s]06/19/2022 14:39:50 - INFO - __main__ - global step: 860; train loss: 7.917320251464844; dev loss: 7.791919708251953
Epoch 5:  73%|███████▎  | 220/300 [01:41<00:32,  2.46it/s]Epoch 5:  74%|███████▎  | 221/300 [01:42<00:30,  2.55it/s]Epoch 5:  74%|███████▍  | 222/300 [01:42<00:32,  2.41it/s]Epoch 5:  74%|███████▍  | 223/300 [01:43<00:30,  2.50it/s]Epoch 5:  75%|███████▍  | 224/300 [01:43<00:30,  2.50it/s]Epoch 5:  75%|███████▌  | 225/300 [01:43<00:29,  2.58it/s]Epoch 5:  75%|███████▌  | 226/300 [01:44<00:30,  2.41it/s]Epoch 5:  76%|███████▌  | 227/300 [01:44<00:31,  2.35it/s]Epoch 5:  76%|███████▌  | 228/300 [01:45<00:32,  2.23it/s]Epoch 5:  76%|███████▋  | 229/300 [01:45<00:32,  2.22it/s]Epoch 5:  77%|███████▋  | 230/300 [01:46<00:35,  2.00it/s]Epoch 5:  77%|███████▋  | 231/300 [01:46<00:33,  2.08it/s]Epoch 5:  77%|███████▋  | 232/300 [01:47<00:34,  1.99it/s]Epoch 5:  78%|███████▊  | 233/300 [01:47<00:33,  2.03it/s]Epoch 5:  78%|███████▊  | 234/300 [01:48<00:35,  1.86it/s]Epoch 5:  78%|███████▊  | 235/300 [01:48<00:32,  2.01it/s]Epoch 5:  79%|███████▊  | 236/300 [01:49<00:31,  2.02it/s]Epoch 5:  79%|███████▉  | 237/300 [01:49<00:29,  2.14it/s]Epoch 5:  79%|███████▉  | 238/300 [01:50<00:29,  2.10it/s]Epoch 5:  80%|███████▉  | 239/300 [01:50<00:26,  2.26it/s]06/19/2022 14:39:59 - INFO - __main__ - global step: 870; train loss: 7.9659295082092285; dev loss: 7.985220432281494
Epoch 5:  80%|████████  | 240/300 [01:50<00:24,  2.40it/s]Epoch 5:  80%|████████  | 241/300 [01:51<00:23,  2.47it/s]Epoch 5:  81%|████████  | 242/300 [01:51<00:24,  2.36it/s]Epoch 5:  81%|████████  | 243/300 [01:52<00:23,  2.40it/s]Epoch 5:  81%|████████▏ | 244/300 [01:52<00:22,  2.50it/s]Epoch 5:  82%|████████▏ | 245/300 [01:52<00:21,  2.59it/s]Epoch 5:  82%|████████▏ | 246/300 [01:53<00:21,  2.48it/s]Epoch 5:  82%|████████▏ | 247/300 [01:53<00:24,  2.18it/s]Epoch 5:  83%|████████▎ | 248/300 [01:54<00:22,  2.26it/s]Epoch 5:  83%|████████▎ | 249/300 [01:54<00:21,  2.33it/s]Epoch 5:  83%|████████▎ | 250/300 [01:55<00:21,  2.29it/s]Epoch 5:  84%|████████▎ | 251/300 [01:55<00:23,  2.13it/s]Epoch 5:  84%|████████▍ | 252/300 [01:56<00:21,  2.28it/s]Epoch 5:  84%|████████▍ | 253/300 [01:56<00:19,  2.40it/s]Epoch 5:  85%|████████▍ | 254/300 [01:56<00:18,  2.48it/s]Epoch 5:  85%|████████▌ | 255/300 [01:57<00:19,  2.35it/s]Epoch 5:  85%|████████▌ | 256/300 [01:57<00:18,  2.32it/s]Epoch 5:  86%|████████▌ | 257/300 [01:58<00:19,  2.17it/s]Epoch 5:  86%|████████▌ | 258/300 [01:58<00:19,  2.16it/s]Epoch 5:  86%|████████▋ | 259/300 [01:59<00:20,  2.00it/s]06/19/2022 14:40:07 - INFO - __main__ - global step: 880; train loss: 8.279026985168457; dev loss: 8.081466674804688
Epoch 5:  87%|████████▋ | 260/300 [01:59<00:18,  2.16it/s]Epoch 5:  87%|████████▋ | 261/300 [02:00<00:17,  2.29it/s]Epoch 5:  87%|████████▋ | 262/300 [02:00<00:15,  2.41it/s]Epoch 5:  88%|████████▊ | 263/300 [02:00<00:16,  2.29it/s]Epoch 5:  88%|████████▊ | 264/300 [02:01<00:15,  2.28it/s]Epoch 5:  88%|████████▊ | 265/300 [02:01<00:15,  2.29it/s]Epoch 5:  89%|████████▊ | 266/300 [02:02<00:14,  2.36it/s]Epoch 5:  89%|████████▉ | 267/300 [02:02<00:15,  2.17it/s]Epoch 5:  89%|████████▉ | 268/300 [02:03<00:14,  2.26it/s]Epoch 5:  90%|████████▉ | 269/300 [02:03<00:13,  2.28it/s]Epoch 5:  90%|█████████ | 270/300 [02:04<00:13,  2.29it/s]Epoch 5:  90%|█████████ | 271/300 [02:04<00:12,  2.28it/s]Epoch 5:  91%|█████████ | 272/300 [02:05<00:12,  2.16it/s]Epoch 5:  91%|█████████ | 273/300 [02:05<00:11,  2.29it/s]Epoch 5:  91%|█████████▏| 274/300 [02:05<00:10,  2.39it/s]Epoch 5:  92%|█████████▏| 275/300 [02:06<00:10,  2.49it/s]Epoch 5:  92%|█████████▏| 276/300 [02:06<00:10,  2.36it/s]Epoch 5:  92%|█████████▏| 277/300 [02:07<00:10,  2.29it/s]Epoch 5:  93%|█████████▎| 278/300 [02:07<00:09,  2.23it/s]Epoch 5:  93%|█████████▎| 279/300 [02:08<00:09,  2.13it/s]06/19/2022 14:40:16 - INFO - __main__ - global step: 890; train loss: 7.946223258972168; dev loss: 8.062973976135254
Epoch 5:  93%|█████████▎| 280/300 [02:08<00:10,  1.97it/s]Epoch 5:  94%|█████████▎| 281/300 [02:09<00:09,  2.01it/s]Epoch 5:  94%|█████████▍| 282/300 [02:09<00:09,  1.90it/s]Epoch 5:  94%|█████████▍| 283/300 [02:10<00:08,  1.98it/s]Epoch 5:  95%|█████████▍| 284/300 [02:10<00:08,  1.98it/s]Epoch 5:  95%|█████████▌| 285/300 [02:11<00:07,  2.09it/s]Epoch 5:  95%|█████████▌| 286/300 [02:11<00:06,  2.25it/s]Epoch 5:  96%|█████████▌| 287/300 [02:11<00:05,  2.34it/s]Epoch 5:  96%|█████████▌| 288/300 [02:12<00:05,  2.23it/s]Epoch 5:  96%|█████████▋| 289/300 [02:12<00:04,  2.27it/s]Epoch 5:  97%|█████████▋| 290/300 [02:13<00:04,  2.24it/s]Epoch 5:  97%|█████████▋| 291/300 [02:13<00:04,  2.17it/s]Epoch 5:  97%|█████████▋| 292/300 [02:14<00:04,  1.97it/s]Epoch 5:  98%|█████████▊| 293/300 [02:14<00:03,  2.11it/s]Epoch 5:  98%|█████████▊| 294/300 [02:15<00:02,  2.11it/s]Epoch 5:  98%|█████████▊| 295/300 [02:15<00:02,  2.14it/s]Epoch 5:  99%|█████████▊| 296/300 [02:16<00:02,  1.99it/s]Epoch 5:  99%|█████████▉| 297/300 [02:16<00:01,  2.16it/s]Epoch 5:  99%|█████████▉| 298/300 [02:16<00:00,  2.32it/s]Epoch 5: 100%|█████████▉| 299/300 [02:17<00:00,  2.41it/s]06/19/2022 14:40:25 - INFO - __main__ - global step: 900; train loss: 8.051909446716309; dev loss: 8.048162460327148
Epoch 5: 100%|██████████| 300/300 [02:17<00:00,  2.50it/s]Epoch 5: 100%|██████████| 300/300 [02:17<00:00,  2.18it/s]
Epoch 6:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 6:   0%|          | 1/300 [00:00<02:22,  2.10it/s]Epoch 6:   1%|          | 2/300 [00:00<02:07,  2.33it/s]Epoch 6:   1%|          | 3/300 [00:01<02:04,  2.38it/s]Epoch 6:   1%|▏         | 4/300 [00:01<02:06,  2.34it/s]Epoch 6:   2%|▏         | 5/300 [00:02<02:11,  2.25it/s]Epoch 6:   2%|▏         | 6/300 [00:02<02:18,  2.12it/s]Epoch 6:   2%|▏         | 7/300 [00:03<02:25,  2.02it/s]Epoch 6:   3%|▎         | 8/300 [00:03<02:25,  2.01it/s]Epoch 6:   3%|▎         | 9/300 [00:04<02:39,  1.82it/s]Epoch 6:   3%|▎         | 10/300 [00:04<02:33,  1.89it/s]Epoch 6:   4%|▎         | 11/300 [00:05<02:27,  1.96it/s]Epoch 6:   4%|▍         | 12/300 [00:05<02:21,  2.03it/s]Epoch 6:   4%|▍         | 13/300 [00:06<02:31,  1.89it/s]Epoch 6:   5%|▍         | 14/300 [00:06<02:25,  1.97it/s]Epoch 6:   5%|▌         | 15/300 [00:07<02:34,  1.85it/s]Epoch 6:   5%|▌         | 16/300 [00:07<02:19,  2.03it/s]Epoch 6:   6%|▌         | 17/300 [00:08<02:28,  1.90it/s]Epoch 6:   6%|▌         | 18/300 [00:08<02:20,  2.00it/s]Epoch 6:   6%|▋         | 19/300 [00:09<02:12,  2.12it/s]06/19/2022 14:40:35 - INFO - __main__ - global step: 910; train loss: 7.913226127624512; dev loss: 7.979990482330322
Epoch 6:   7%|▋         | 20/300 [00:09<02:09,  2.16it/s]Epoch 6:   7%|▋         | 21/300 [00:10<02:13,  2.09it/s]Epoch 6:   7%|▋         | 22/300 [00:10<02:03,  2.25it/s]Epoch 6:   8%|▊         | 23/300 [00:11<02:00,  2.29it/s]Epoch 6:   8%|▊         | 24/300 [00:11<01:55,  2.40it/s]Epoch 6:   8%|▊         | 25/300 [00:11<01:48,  2.53it/s]Epoch 6:   9%|▊         | 26/300 [00:12<01:52,  2.43it/s]Epoch 6:   9%|▉         | 27/300 [00:12<01:48,  2.53it/s]Epoch 6:   9%|▉         | 28/300 [00:13<01:57,  2.31it/s]Epoch 6:  10%|▉         | 29/300 [00:13<01:56,  2.33it/s]Epoch 6:  10%|█         | 30/300 [00:14<02:10,  2.06it/s]Epoch 6:  10%|█         | 31/300 [00:14<02:08,  2.09it/s]Epoch 6:  11%|█         | 32/300 [00:15<02:08,  2.08it/s]Epoch 6:  11%|█         | 33/300 [00:15<02:06,  2.12it/s]Epoch 6:  11%|█▏        | 34/300 [00:16<02:11,  2.02it/s]Epoch 6:  12%|█▏        | 35/300 [00:16<01:59,  2.22it/s]Epoch 6:  12%|█▏        | 36/300 [00:16<01:50,  2.39it/s]Epoch 6:  12%|█▏        | 37/300 [00:17<01:43,  2.53it/s]Epoch 6:  13%|█▎        | 38/300 [00:17<01:50,  2.37it/s]Epoch 6:  13%|█▎        | 39/300 [00:18<01:49,  2.39it/s]06/19/2022 14:40:44 - INFO - __main__ - global step: 920; train loss: 7.972041130065918; dev loss: 7.9925079345703125
Epoch 6:  13%|█▎        | 40/300 [00:18<01:42,  2.53it/s]Epoch 6:  14%|█▎        | 41/300 [00:18<01:43,  2.49it/s]Epoch 6:  14%|█▍        | 42/300 [00:19<01:47,  2.40it/s]Epoch 6:  14%|█▍        | 43/300 [00:19<01:53,  2.27it/s]Epoch 6:  15%|█▍        | 44/300 [00:20<01:49,  2.34it/s]Epoch 6:  15%|█▌        | 45/300 [00:20<01:50,  2.31it/s]Epoch 6:  15%|█▌        | 46/300 [00:21<01:59,  2.12it/s]Epoch 6:  16%|█▌        | 47/300 [00:21<01:58,  2.14it/s]Epoch 6:  16%|█▌        | 48/300 [00:22<01:52,  2.24it/s]Epoch 6:  16%|█▋        | 49/300 [00:22<01:48,  2.32it/s]Epoch 6:  17%|█▋        | 50/300 [00:22<01:58,  2.11it/s]Epoch 6:  17%|█▋        | 51/300 [00:23<01:50,  2.25it/s]Epoch 6:  17%|█▋        | 52/300 [00:23<01:47,  2.31it/s]Epoch 6:  18%|█▊        | 53/300 [00:24<01:45,  2.33it/s]Epoch 6:  18%|█▊        | 54/300 [00:24<01:43,  2.38it/s]Epoch 6:  18%|█▊        | 55/300 [00:25<01:48,  2.26it/s]Epoch 6:  19%|█▊        | 56/300 [00:25<01:42,  2.39it/s]Epoch 6:  19%|█▉        | 57/300 [00:25<01:37,  2.49it/s]Epoch 6:  19%|█▉        | 58/300 [00:26<01:38,  2.45it/s]Epoch 6:  20%|█▉        | 59/300 [00:26<01:44,  2.31it/s]06/19/2022 14:40:53 - INFO - __main__ - global step: 930; train loss: 7.418889045715332; dev loss: 7.535776615142822
Epoch 6:  20%|██        | 60/300 [00:27<01:48,  2.21it/s]Epoch 6:  20%|██        | 61/300 [00:27<01:44,  2.29it/s]Epoch 6:  21%|██        | 62/300 [00:28<01:41,  2.34it/s]Epoch 6:  21%|██        | 63/300 [00:28<01:46,  2.22it/s]Epoch 6:  21%|██▏       | 64/300 [00:29<01:49,  2.15it/s]Epoch 6:  22%|██▏       | 65/300 [00:29<01:48,  2.17it/s]Epoch 6:  22%|██▏       | 66/300 [00:29<01:42,  2.29it/s]Epoch 6:  22%|██▏       | 67/300 [00:30<01:43,  2.25it/s]Epoch 6:  23%|██▎       | 68/300 [00:30<01:36,  2.41it/s]Epoch 6:  23%|██▎       | 69/300 [00:31<01:37,  2.36it/s]Epoch 6:  23%|██▎       | 70/300 [00:31<01:32,  2.49it/s]Epoch 6:  24%|██▎       | 71/300 [00:31<01:35,  2.41it/s]Epoch 6:  24%|██▍       | 72/300 [00:32<01:29,  2.54it/s]Epoch 6:  24%|██▍       | 73/300 [00:32<01:26,  2.63it/s]Epoch 6:  25%|██▍       | 74/300 [00:32<01:23,  2.69it/s]Epoch 6:  25%|██▌       | 75/300 [00:33<01:28,  2.54it/s]Epoch 6:  25%|██▌       | 76/300 [00:33<01:30,  2.49it/s]Epoch 6:  26%|██▌       | 77/300 [00:34<01:31,  2.43it/s]Epoch 6:  26%|██▌       | 78/300 [00:34<01:27,  2.54it/s]Epoch 6:  26%|██▋       | 79/300 [00:34<01:24,  2.63it/s]06/19/2022 14:41:01 - INFO - __main__ - global step: 940; train loss: 8.062373161315918; dev loss: 8.01648235321045
Epoch 6:  27%|██▋       | 80/300 [00:35<01:28,  2.49it/s]Epoch 6:  27%|██▋       | 81/300 [00:35<01:24,  2.60it/s]Epoch 6:  27%|██▋       | 82/300 [00:36<01:21,  2.69it/s]Epoch 6:  28%|██▊       | 83/300 [00:36<01:23,  2.59it/s]Epoch 6:  28%|██▊       | 84/300 [00:36<01:27,  2.47it/s]Epoch 6:  28%|██▊       | 85/300 [00:37<01:23,  2.58it/s]Epoch 6:  29%|██▊       | 86/300 [00:37<01:24,  2.52it/s]Epoch 6:  29%|██▉       | 87/300 [00:38<01:21,  2.61it/s]Epoch 6:  29%|██▉       | 88/300 [00:38<01:33,  2.26it/s]Epoch 6:  30%|██▉       | 89/300 [00:39<01:37,  2.15it/s]Epoch 6:  30%|███       | 90/300 [00:39<01:40,  2.10it/s]Epoch 6:  30%|███       | 91/300 [00:40<01:36,  2.16it/s]Epoch 6:  31%|███       | 92/300 [00:40<01:35,  2.18it/s]Epoch 6:  31%|███       | 93/300 [00:40<01:30,  2.30it/s]Epoch 6:  31%|███▏      | 94/300 [00:41<01:29,  2.31it/s]Epoch 6:  32%|███▏      | 95/300 [00:41<01:27,  2.35it/s]Epoch 6:  32%|███▏      | 96/300 [00:42<01:36,  2.12it/s]Epoch 6:  32%|███▏      | 97/300 [00:42<01:36,  2.10it/s]Epoch 6:  33%|███▎      | 98/300 [00:43<01:36,  2.10it/s]Epoch 6:  33%|███▎      | 99/300 [00:43<01:29,  2.25it/s]06/19/2022 14:41:10 - INFO - __main__ - global step: 950; train loss: 7.673378944396973; dev loss: 7.700774192810059
Epoch 6:  33%|███▎      | 100/300 [00:44<01:33,  2.13it/s]Epoch 6:  34%|███▎      | 101/300 [00:44<01:28,  2.25it/s]Epoch 6:  34%|███▍      | 102/300 [00:44<01:24,  2.33it/s]Epoch 6:  34%|███▍      | 103/300 [00:45<01:20,  2.44it/s]Epoch 6:  35%|███▍      | 104/300 [00:45<01:23,  2.36it/s]Epoch 6:  35%|███▌      | 105/300 [00:46<01:23,  2.34it/s]Epoch 6:  35%|███▌      | 106/300 [00:46<01:23,  2.33it/s]Epoch 6:  36%|███▌      | 107/300 [00:47<01:20,  2.41it/s]Epoch 6:  36%|███▌      | 108/300 [00:47<01:19,  2.41it/s]Epoch 6:  36%|███▋      | 109/300 [00:47<01:23,  2.30it/s]Epoch 6:  37%|███▋      | 110/300 [00:48<01:24,  2.24it/s]Epoch 6:  37%|███▋      | 111/300 [00:48<01:26,  2.19it/s]Epoch 6:  37%|███▋      | 112/300 [00:49<01:22,  2.27it/s]Epoch 6:  38%|███▊      | 113/300 [00:49<01:33,  2.01it/s]Epoch 6:  38%|███▊      | 114/300 [00:50<01:30,  2.06it/s]Epoch 6:  38%|███▊      | 115/300 [00:50<01:29,  2.07it/s]Epoch 6:  39%|███▊      | 116/300 [00:51<01:27,  2.10it/s]Epoch 6:  39%|███▉      | 117/300 [00:51<01:31,  2.00it/s]Epoch 6:  39%|███▉      | 118/300 [00:52<01:27,  2.09it/s]Epoch 6:  40%|███▉      | 119/300 [00:52<01:20,  2.24it/s]06/19/2022 14:41:18 - INFO - __main__ - global step: 960; train loss: 8.258231163024902; dev loss: 8.332831382751465
Epoch 6:  40%|████      | 120/300 [00:53<01:18,  2.30it/s]Epoch 6:  40%|████      | 121/300 [00:53<01:23,  2.13it/s]Epoch 6:  41%|████      | 122/300 [00:54<01:18,  2.27it/s]Epoch 6:  41%|████      | 123/300 [00:54<01:17,  2.29it/s]Epoch 6:  41%|████▏     | 124/300 [00:54<01:13,  2.40it/s]Epoch 6:  42%|████▏     | 125/300 [00:55<01:18,  2.23it/s]Epoch 6:  42%|████▏     | 126/300 [00:55<01:21,  2.15it/s]Epoch 6:  42%|████▏     | 127/300 [00:56<01:16,  2.27it/s]Epoch 6:  43%|████▎     | 128/300 [00:56<01:21,  2.12it/s]Epoch 6:  43%|████▎     | 129/300 [00:57<01:21,  2.09it/s]Epoch 6:  43%|████▎     | 130/300 [00:57<01:16,  2.23it/s]Epoch 6:  44%|████▎     | 131/300 [00:58<01:13,  2.29it/s]Epoch 6:  44%|████▍     | 132/300 [00:58<01:20,  2.08it/s]Epoch 6:  44%|████▍     | 133/300 [00:58<01:13,  2.26it/s]Epoch 6:  45%|████▍     | 134/300 [00:59<01:13,  2.25it/s]Epoch 6:  45%|████▌     | 135/300 [00:59<01:08,  2.42it/s]Epoch 6:  45%|████▌     | 136/300 [01:00<01:03,  2.57it/s]Epoch 6:  46%|████▌     | 137/300 [01:00<01:05,  2.48it/s]Epoch 6:  46%|████▌     | 138/300 [01:01<01:08,  2.37it/s]Epoch 6:  46%|████▋     | 139/300 [01:01<01:04,  2.50it/s]06/19/2022 14:41:27 - INFO - __main__ - global step: 970; train loss: 7.96804666519165; dev loss: 7.739260196685791
Epoch 6:  47%|████▋     | 140/300 [01:01<01:00,  2.64it/s]Epoch 6:  47%|████▋     | 141/300 [01:02<01:01,  2.60it/s]Epoch 6:  47%|████▋     | 142/300 [01:02<01:10,  2.23it/s]Epoch 6:  48%|████▊     | 143/300 [01:03<01:08,  2.28it/s]Epoch 6:  48%|████▊     | 144/300 [01:03<01:07,  2.29it/s]Epoch 6:  48%|████▊     | 145/300 [01:03<01:06,  2.33it/s]Epoch 6:  49%|████▊     | 146/300 [01:04<01:12,  2.13it/s]Epoch 6:  49%|████▉     | 147/300 [01:05<01:12,  2.10it/s]Epoch 6:  49%|████▉     | 148/300 [01:05<01:11,  2.14it/s]Epoch 6:  50%|████▉     | 149/300 [01:05<01:09,  2.17it/s]Epoch 6:  50%|█████     | 150/300 [01:06<01:14,  2.02it/s]Epoch 6:  50%|█████     | 151/300 [01:06<01:11,  2.07it/s]Epoch 6:  51%|█████     | 152/300 [01:07<01:10,  2.09it/s]Epoch 6:  51%|█████     | 153/300 [01:07<01:05,  2.24it/s]Epoch 6:  51%|█████▏    | 154/300 [01:08<01:05,  2.21it/s]Epoch 6:  52%|█████▏    | 155/300 [01:08<01:00,  2.40it/s]Epoch 6:  52%|█████▏    | 156/300 [01:08<00:57,  2.51it/s]Epoch 6:  52%|█████▏    | 157/300 [01:09<00:56,  2.55it/s]Epoch 6:  53%|█████▎    | 158/300 [01:09<00:58,  2.41it/s]Epoch 6:  53%|█████▎    | 159/300 [01:10<00:56,  2.49it/s]06/19/2022 14:41:36 - INFO - __main__ - global step: 980; train loss: 8.02910041809082; dev loss: 7.9496965408325195
Epoch 6:  53%|█████▎    | 160/300 [01:10<00:53,  2.61it/s]Epoch 6:  54%|█████▎    | 161/300 [01:10<00:51,  2.71it/s]Epoch 6:  54%|█████▍    | 162/300 [01:11<00:51,  2.70it/s]Epoch 6:  54%|█████▍    | 163/300 [01:11<00:54,  2.50it/s]Epoch 6:  55%|█████▍    | 164/300 [01:12<00:52,  2.57it/s]Epoch 6:  55%|█████▌    | 165/300 [01:12<00:53,  2.52it/s]Epoch 6:  55%|█████▌    | 166/300 [01:12<00:54,  2.46it/s]Epoch 6:  56%|█████▌    | 167/300 [01:13<00:59,  2.24it/s]Epoch 6:  56%|█████▌    | 168/300 [01:13<01:00,  2.19it/s]Epoch 6:  56%|█████▋    | 169/300 [01:14<01:00,  2.16it/s]Epoch 6:  57%|█████▋    | 170/300 [01:14<00:59,  2.19it/s]Epoch 6:  57%|█████▋    | 171/300 [01:15<01:03,  2.03it/s]Epoch 6:  57%|█████▋    | 172/300 [01:15<01:00,  2.11it/s]Epoch 6:  58%|█████▊    | 173/300 [01:16<00:59,  2.15it/s]Epoch 6:  58%|█████▊    | 174/300 [01:16<00:57,  2.17it/s]Epoch 6:  58%|█████▊    | 175/300 [01:17<01:01,  2.02it/s]Epoch 6:  59%|█████▊    | 176/300 [01:17<00:58,  2.11it/s]Epoch 6:  59%|█████▉    | 177/300 [01:18<00:57,  2.12it/s]Epoch 6:  59%|█████▉    | 178/300 [01:18<00:56,  2.15it/s]Epoch 6:  60%|█████▉    | 179/300 [01:19<01:03,  1.92it/s]06/19/2022 14:41:45 - INFO - __main__ - global step: 990; train loss: 8.204717636108398; dev loss: 8.135674476623535
Epoch 6:  60%|██████    | 180/300 [01:19<00:59,  2.01it/s]Epoch 6:  60%|██████    | 181/300 [01:20<00:55,  2.14it/s]Epoch 6:  61%|██████    | 182/300 [01:20<00:51,  2.27it/s]Epoch 6:  61%|██████    | 183/300 [01:21<00:57,  2.05it/s]Epoch 6:  61%|██████▏   | 184/300 [01:21<00:54,  2.14it/s]Epoch 6:  62%|██████▏   | 185/300 [01:21<00:51,  2.25it/s]Epoch 6:  62%|██████▏   | 186/300 [01:22<00:46,  2.44it/s]Epoch 6:  62%|██████▏   | 187/300 [01:22<00:43,  2.58it/s]Epoch 6:  63%|██████▎   | 188/300 [01:22<00:44,  2.49it/s]Epoch 6:  63%|██████▎   | 189/300 [01:23<00:46,  2.39it/s]Epoch 6:  63%|██████▎   | 190/300 [01:23<00:47,  2.30it/s]Epoch 6:  64%|██████▎   | 191/300 [01:24<00:48,  2.27it/s]Epoch 6:  64%|██████▍   | 192/300 [01:24<00:53,  2.03it/s]Epoch 6:  64%|██████▍   | 193/300 [01:25<00:48,  2.21it/s]Epoch 6:  65%|██████▍   | 194/300 [01:25<00:45,  2.34it/s]Epoch 6:  65%|██████▌   | 195/300 [01:26<00:45,  2.31it/s]Epoch 6:  65%|██████▌   | 196/300 [01:26<00:47,  2.20it/s]Epoch 6:  66%|██████▌   | 197/300 [01:27<00:42,  2.40it/s]Epoch 6:  66%|██████▌   | 198/300 [01:27<00:40,  2.55it/s]Epoch 6:  66%|██████▋   | 199/300 [01:27<00:37,  2.67it/s]06/19/2022 14:41:53 - INFO - __main__ - global step: 1000; train loss: 7.9463210105896; dev loss: 8.185704231262207
Epoch 6:  67%|██████▋   | 200/300 [01:28<00:39,  2.56it/s]Epoch 6:  67%|██████▋   | 201/300 [01:28<00:36,  2.68it/s]Epoch 6:  67%|██████▋   | 202/300 [01:28<00:35,  2.77it/s]Epoch 6:  68%|██████▊   | 203/300 [01:29<00:34,  2.84it/s]Epoch 6:  68%|██████▊   | 204/300 [01:29<00:36,  2.66it/s]Epoch 6:  68%|██████▊   | 205/300 [01:29<00:34,  2.77it/s]Epoch 6:  69%|██████▊   | 206/300 [01:30<00:33,  2.83it/s]Epoch 6:  69%|██████▉   | 207/300 [01:30<00:32,  2.88it/s]Epoch 6:  69%|██████▉   | 208/300 [01:30<00:34,  2.69it/s]Epoch 6:  70%|██████▉   | 209/300 [01:31<00:32,  2.77it/s]Epoch 6:  70%|███████   | 210/300 [01:31<00:36,  2.45it/s]Epoch 6:  70%|███████   | 211/300 [01:32<00:38,  2.31it/s]Epoch 6:  71%|███████   | 212/300 [01:32<00:42,  2.09it/s]Epoch 6:  71%|███████   | 213/300 [01:33<00:40,  2.15it/s]Epoch 6:  71%|███████▏  | 214/300 [01:33<00:39,  2.18it/s]Epoch 6:  72%|███████▏  | 215/300 [01:34<00:38,  2.21it/s]Epoch 6:  72%|███████▏  | 216/300 [01:34<00:37,  2.23it/s]Epoch 6:  72%|███████▏  | 217/300 [01:35<00:39,  2.13it/s]Epoch 6:  73%|███████▎  | 218/300 [01:35<00:37,  2.17it/s]Epoch 6:  73%|███████▎  | 219/300 [01:36<00:38,  2.12it/s]06/19/2022 14:42:02 - INFO - __main__ - global step: 1010; train loss: 7.494891166687012; dev loss: 7.519022464752197
Epoch 6:  73%|███████▎  | 220/300 [01:36<00:37,  2.16it/s]Epoch 6:  74%|███████▎  | 221/300 [01:37<00:38,  2.06it/s]Epoch 6:  74%|███████▍  | 222/300 [01:37<00:37,  2.05it/s]Epoch 6:  74%|███████▍  | 223/300 [01:37<00:36,  2.13it/s]Epoch 6:  75%|███████▍  | 224/300 [01:38<00:35,  2.16it/s]Epoch 6:  75%|███████▌  | 225/300 [01:38<00:36,  2.07it/s]Epoch 6:  75%|███████▌  | 226/300 [01:39<00:32,  2.26it/s]Epoch 6:  76%|███████▌  | 227/300 [01:39<00:30,  2.37it/s]Epoch 6:  76%|███████▌  | 228/300 [01:40<00:28,  2.53it/s]Epoch 6:  76%|███████▋  | 229/300 [01:40<00:28,  2.47it/s]Epoch 6:  77%|███████▋  | 230/300 [01:40<00:26,  2.61it/s]Epoch 6:  77%|███████▋  | 231/300 [01:41<00:26,  2.62it/s]Epoch 6:  77%|███████▋  | 232/300 [01:41<00:26,  2.56it/s]Epoch 6:  78%|███████▊  | 233/300 [01:42<00:28,  2.35it/s]Epoch 6:  78%|███████▊  | 234/300 [01:42<00:26,  2.50it/s]Epoch 6:  78%|███████▊  | 235/300 [01:42<00:24,  2.64it/s]Epoch 6:  79%|███████▊  | 236/300 [01:43<00:23,  2.75it/s]Epoch 6:  79%|███████▉  | 237/300 [01:43<00:24,  2.62it/s]Epoch 6:  79%|███████▉  | 238/300 [01:43<00:22,  2.71it/s]Epoch 6:  80%|███████▉  | 239/300 [01:44<00:21,  2.78it/s]06/19/2022 14:42:10 - INFO - __main__ - global step: 1020; train loss: 7.838653564453125; dev loss: 7.725840091705322
Epoch 6:  80%|████████  | 240/300 [01:44<00:22,  2.66it/s]Epoch 6:  80%|████████  | 241/300 [01:45<00:22,  2.58it/s]Epoch 6:  81%|████████  | 242/300 [01:45<00:23,  2.47it/s]Epoch 6:  81%|████████  | 243/300 [01:45<00:21,  2.62it/s]Epoch 6:  81%|████████▏ | 244/300 [01:46<00:20,  2.73it/s]Epoch 6:  82%|████████▏ | 245/300 [01:46<00:19,  2.80it/s]Epoch 6:  82%|████████▏ | 246/300 [01:46<00:20,  2.66it/s]Epoch 6:  82%|████████▏ | 247/300 [01:47<00:19,  2.67it/s]Epoch 6:  83%|████████▎ | 248/300 [01:47<00:19,  2.70it/s]Epoch 6:  83%|████████▎ | 249/300 [01:47<00:18,  2.77it/s]Epoch 6:  83%|████████▎ | 250/300 [01:48<00:19,  2.63it/s]Epoch 6:  84%|████████▎ | 251/300 [01:48<00:18,  2.62it/s]Epoch 6:  84%|████████▍ | 252/300 [01:49<00:17,  2.72it/s]Epoch 6:  84%|████████▍ | 253/300 [01:49<00:16,  2.82it/s]Epoch 6:  85%|████████▍ | 254/300 [01:49<00:17,  2.60it/s]Epoch 6:  85%|████████▌ | 255/300 [01:50<00:16,  2.72it/s]Epoch 6:  85%|████████▌ | 256/300 [01:50<00:16,  2.69it/s]Epoch 6:  86%|████████▌ | 257/300 [01:50<00:15,  2.77it/s]Epoch 6:  86%|████████▌ | 258/300 [01:51<00:15,  2.64it/s]Epoch 6:  86%|████████▋ | 259/300 [01:51<00:15,  2.62it/s]06/19/2022 14:42:17 - INFO - __main__ - global step: 1030; train loss: 7.7029571533203125; dev loss: 7.593096733093262
Epoch 6:  87%|████████▋ | 260/300 [01:52<00:15,  2.64it/s]Epoch 6:  87%|████████▋ | 261/300 [01:52<00:14,  2.67it/s]Epoch 6:  87%|████████▋ | 262/300 [01:52<00:15,  2.48it/s]Epoch 6:  88%|████████▊ | 263/300 [01:53<00:14,  2.54it/s]Epoch 6:  88%|████████▊ | 264/300 [01:53<00:13,  2.61it/s]Epoch 6:  88%|████████▊ | 265/300 [01:54<00:13,  2.64it/s]Epoch 6:  89%|████████▊ | 266/300 [01:54<00:13,  2.48it/s]Epoch 6:  89%|████████▉ | 267/300 [01:54<00:12,  2.55it/s]Epoch 6:  89%|████████▉ | 268/300 [01:55<00:12,  2.53it/s]Epoch 6:  90%|████████▉ | 269/300 [01:55<00:11,  2.64it/s]Epoch 6:  90%|█████████ | 270/300 [01:55<00:11,  2.66it/s]Epoch 6:  90%|█████████ | 271/300 [01:56<00:11,  2.46it/s]Epoch 6:  91%|█████████ | 272/300 [01:56<00:11,  2.51it/s]Epoch 6:  91%|█████████ | 273/300 [01:57<00:10,  2.64it/s]Epoch 6:  91%|█████████▏| 274/300 [01:57<00:09,  2.74it/s]Epoch 6:  92%|█████████▏| 275/300 [01:57<00:10,  2.46it/s]Epoch 6:  92%|█████████▏| 276/300 [01:58<00:09,  2.59it/s]Epoch 6:  92%|█████████▏| 277/300 [01:58<00:09,  2.53it/s]Epoch 6:  93%|█████████▎| 278/300 [01:59<00:08,  2.49it/s]Epoch 6:  93%|█████████▎| 279/300 [01:59<00:09,  2.24it/s]06/19/2022 14:42:25 - INFO - __main__ - global step: 1040; train loss: 7.9145050048828125; dev loss: 7.911360740661621
Epoch 6:  93%|█████████▎| 280/300 [02:00<00:08,  2.29it/s]Epoch 6:  94%|█████████▎| 281/300 [02:00<00:08,  2.35it/s]Epoch 6:  94%|█████████▍| 282/300 [02:00<00:07,  2.36it/s]Epoch 6:  94%|█████████▍| 283/300 [02:01<00:07,  2.26it/s]Epoch 6:  95%|█████████▍| 284/300 [02:01<00:06,  2.43it/s]Epoch 6:  95%|█████████▌| 285/300 [02:02<00:05,  2.50it/s]Epoch 6:  95%|█████████▌| 286/300 [02:02<00:05,  2.49it/s]Epoch 6:  96%|█████████▌| 287/300 [02:02<00:05,  2.43it/s]Epoch 6:  96%|█████████▌| 288/300 [02:03<00:04,  2.41it/s]Epoch 6:  96%|█████████▋| 289/300 [02:03<00:04,  2.41it/s]Epoch 6:  97%|█████████▋| 290/300 [02:04<00:04,  2.42it/s]Epoch 6:  97%|█████████▋| 291/300 [02:04<00:03,  2.37it/s]Epoch 6:  97%|█████████▋| 292/300 [02:05<00:03,  2.55it/s]Epoch 6:  98%|█████████▊| 293/300 [02:05<00:02,  2.66it/s]Epoch 6:  98%|█████████▊| 294/300 [02:05<00:02,  2.76it/s]Epoch 6:  98%|█████████▊| 295/300 [02:05<00:01,  2.84it/s]Epoch 6:  99%|█████████▊| 296/300 [02:06<00:01,  2.67it/s]Epoch 6:  99%|█████████▉| 297/300 [02:06<00:01,  2.61it/s]Epoch 6:  99%|█████████▉| 298/300 [02:07<00:00,  2.55it/s]Epoch 6: 100%|█████████▉| 299/300 [02:07<00:00,  2.50it/s]06/19/2022 14:42:33 - INFO - __main__ - global step: 1050; train loss: 7.7502593994140625; dev loss: 7.707648277282715
Epoch 6: 100%|██████████| 300/300 [02:08<00:00,  2.46it/s]Epoch 6: 100%|██████████| 300/300 [02:08<00:00,  2.34it/s]
Epoch 7:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 7:   0%|          | 1/300 [00:00<01:37,  3.06it/s]Epoch 7:   1%|          | 2/300 [00:00<01:36,  3.09it/s]Epoch 7:   1%|          | 3/300 [00:01<01:47,  2.76it/s]Epoch 7:   1%|▏         | 4/300 [00:01<02:00,  2.46it/s]Epoch 7:   2%|▏         | 5/300 [00:01<02:00,  2.44it/s]Epoch 7:   2%|▏         | 6/300 [00:02<02:06,  2.33it/s]Epoch 7:   2%|▏         | 7/300 [00:02<02:06,  2.31it/s]Epoch 7:   3%|▎         | 8/300 [00:03<02:18,  2.11it/s]Epoch 7:   3%|▎         | 9/300 [00:03<02:14,  2.17it/s]Epoch 7:   3%|▎         | 10/300 [00:04<02:11,  2.20it/s]Epoch 7:   4%|▎         | 11/300 [00:04<02:14,  2.15it/s]Epoch 7:   4%|▍         | 12/300 [00:05<02:33,  1.88it/s]Epoch 7:   4%|▍         | 13/300 [00:05<02:27,  1.94it/s]Epoch 7:   5%|▍         | 14/300 [00:06<02:23,  1.99it/s]Epoch 7:   5%|▌         | 15/300 [00:06<02:19,  2.04it/s]Epoch 7:   5%|▌         | 16/300 [00:07<02:30,  1.89it/s]Epoch 7:   6%|▌         | 17/300 [00:07<02:26,  1.93it/s]Epoch 7:   6%|▌         | 18/300 [00:08<02:22,  1.98it/s]Epoch 7:   6%|▋         | 19/300 [00:08<02:18,  2.03it/s]06/19/2022 14:42:43 - INFO - __main__ - global step: 1060; train loss: 7.450247764587402; dev loss: 7.585717678070068
Epoch 7:   7%|▋         | 20/300 [00:09<02:39,  1.75it/s]Epoch 7:   7%|▋         | 21/300 [00:10<02:40,  1.74it/s]Epoch 7:   7%|▋         | 22/300 [00:10<02:32,  1.82it/s]Epoch 7:   8%|▊         | 23/300 [00:11<02:17,  2.01it/s]Epoch 7:   8%|▊         | 24/300 [00:11<02:11,  2.09it/s]Epoch 7:   8%|▊         | 25/300 [00:12<02:23,  1.91it/s]Epoch 7:   9%|▊         | 26/300 [00:12<02:13,  2.05it/s]Epoch 7:   9%|▉         | 27/300 [00:13<02:13,  2.04it/s]Epoch 7:   9%|▉         | 28/300 [00:13<02:09,  2.09it/s]Epoch 7:  10%|▉         | 29/300 [00:14<02:13,  2.03it/s]Epoch 7:  10%|█         | 30/300 [00:14<02:08,  2.09it/s]Epoch 7:  10%|█         | 31/300 [00:14<02:05,  2.15it/s]Epoch 7:  11%|█         | 32/300 [00:15<02:02,  2.18it/s]Epoch 7:  11%|█         | 33/300 [00:15<02:07,  2.09it/s]Epoch 7:  11%|█▏        | 34/300 [00:16<02:04,  2.13it/s]Epoch 7:  12%|█▏        | 35/300 [00:16<02:08,  2.06it/s]Epoch 7:  12%|█▏        | 36/300 [00:17<02:09,  2.05it/s]Epoch 7:  12%|█▏        | 37/300 [00:17<02:11,  2.00it/s]Epoch 7:  13%|█▎        | 38/300 [00:18<02:01,  2.15it/s]Epoch 7:  13%|█▎        | 39/300 [00:18<01:51,  2.34it/s]06/19/2022 14:42:52 - INFO - __main__ - global step: 1070; train loss: 7.728785514831543; dev loss: 7.388773441314697
Epoch 7:  13%|█▎        | 40/300 [00:19<01:47,  2.42it/s]Epoch 7:  14%|█▎        | 41/300 [00:19<01:48,  2.39it/s]Epoch 7:  14%|█▍        | 42/300 [00:19<01:44,  2.47it/s]Epoch 7:  14%|█▍        | 43/300 [00:20<01:38,  2.60it/s]Epoch 7:  15%|█▍        | 44/300 [00:20<01:34,  2.71it/s]Epoch 7:  15%|█▌        | 45/300 [00:20<01:39,  2.57it/s]Epoch 7:  15%|█▌        | 46/300 [00:21<01:47,  2.35it/s]Epoch 7:  16%|█▌        | 47/300 [00:21<01:46,  2.37it/s]Epoch 7:  16%|█▌        | 48/300 [00:22<01:48,  2.31it/s]Epoch 7:  16%|█▋        | 49/300 [00:22<01:52,  2.23it/s]Epoch 7:  17%|█▋        | 50/300 [00:23<01:53,  2.20it/s]Epoch 7:  17%|█▋        | 51/300 [00:23<01:44,  2.37it/s]Epoch 7:  17%|█▋        | 52/300 [00:23<01:39,  2.50it/s]Epoch 7:  18%|█▊        | 53/300 [00:24<01:35,  2.59it/s]Epoch 7:  18%|█▊        | 54/300 [00:24<01:41,  2.42it/s]Epoch 7:  18%|█▊        | 55/300 [00:25<01:38,  2.48it/s]Epoch 7:  19%|█▊        | 56/300 [00:25<01:34,  2.57it/s]Epoch 7:  19%|█▉        | 57/300 [00:25<01:31,  2.66it/s]Epoch 7:  19%|█▉        | 58/300 [00:26<01:36,  2.52it/s]Epoch 7:  20%|█▉        | 59/300 [00:26<01:33,  2.58it/s]06/19/2022 14:43:00 - INFO - __main__ - global step: 1080; train loss: 8.055290222167969; dev loss: 8.285414695739746
Epoch 7:  20%|██        | 60/300 [00:27<01:31,  2.62it/s]Epoch 7:  20%|██        | 61/300 [00:27<01:29,  2.66it/s]Epoch 7:  21%|██        | 62/300 [00:27<01:36,  2.46it/s]Epoch 7:  21%|██        | 63/300 [00:28<01:31,  2.58it/s]Epoch 7:  21%|██▏       | 64/300 [00:28<01:33,  2.52it/s]Epoch 7:  22%|██▏       | 65/300 [00:29<01:37,  2.42it/s]Epoch 7:  22%|██▏       | 66/300 [00:29<01:48,  2.15it/s]Epoch 7:  22%|██▏       | 67/300 [00:30<01:52,  2.08it/s]Epoch 7:  23%|██▎       | 68/300 [00:30<01:43,  2.25it/s]Epoch 7:  23%|██▎       | 69/300 [00:30<01:36,  2.38it/s]Epoch 7:  23%|██▎       | 70/300 [00:31<01:41,  2.27it/s]Epoch 7:  24%|██▎       | 71/300 [00:31<01:36,  2.38it/s]Epoch 7:  24%|██▍       | 72/300 [00:32<01:31,  2.50it/s]Epoch 7:  24%|██▍       | 73/300 [00:32<01:34,  2.40it/s]Epoch 7:  25%|██▍       | 74/300 [00:33<01:49,  2.06it/s]Epoch 7:  25%|██▌       | 75/300 [00:33<01:46,  2.12it/s]Epoch 7:  25%|██▌       | 76/300 [00:34<01:42,  2.19it/s]Epoch 7:  26%|██▌       | 77/300 [00:34<01:37,  2.29it/s]Epoch 7:  26%|██▌       | 78/300 [00:34<01:30,  2.44it/s]Epoch 7:  26%|██▋       | 79/300 [00:35<01:35,  2.31it/s]06/19/2022 14:43:09 - INFO - __main__ - global step: 1090; train loss: 7.9825439453125; dev loss: 7.4923858642578125
Epoch 7:  27%|██▋       | 80/300 [00:35<01:29,  2.45it/s]Epoch 7:  27%|██▋       | 81/300 [00:36<01:25,  2.56it/s]Epoch 7:  27%|██▋       | 82/300 [00:36<01:22,  2.64it/s]Epoch 7:  28%|██▊       | 83/300 [00:36<01:26,  2.51it/s]Epoch 7:  28%|██▊       | 84/300 [00:37<01:22,  2.61it/s]Epoch 7:  28%|██▊       | 85/300 [00:37<01:22,  2.59it/s]Epoch 7:  29%|██▊       | 86/300 [00:37<01:20,  2.65it/s]Epoch 7:  29%|██▉       | 87/300 [00:38<01:27,  2.45it/s]Epoch 7:  29%|██▉       | 88/300 [00:38<01:23,  2.52it/s]Epoch 7:  30%|██▉       | 89/300 [00:39<01:27,  2.41it/s]Epoch 7:  30%|███       | 90/300 [00:39<01:32,  2.27it/s]Epoch 7:  30%|███       | 91/300 [00:40<01:40,  2.08it/s]Epoch 7:  31%|███       | 92/300 [00:40<01:35,  2.18it/s]Epoch 7:  31%|███       | 93/300 [00:41<01:29,  2.32it/s]Epoch 7:  31%|███▏      | 94/300 [00:41<01:25,  2.40it/s]Epoch 7:  32%|███▏      | 95/300 [00:41<01:29,  2.28it/s]Epoch 7:  32%|███▏      | 96/300 [00:42<01:26,  2.37it/s]Epoch 7:  32%|███▏      | 97/300 [00:42<01:27,  2.32it/s]Epoch 7:  33%|███▎      | 98/300 [00:43<01:31,  2.21it/s]Epoch 7:  33%|███▎      | 99/300 [00:43<01:39,  2.02it/s]06/19/2022 14:43:18 - INFO - __main__ - global step: 1100; train loss: 8.206727027893066; dev loss: 8.21760082244873
Epoch 7:  33%|███▎      | 100/300 [00:44<01:32,  2.17it/s]Epoch 7:  34%|███▎      | 101/300 [00:44<01:26,  2.30it/s]Epoch 7:  34%|███▍      | 102/300 [00:45<01:25,  2.31it/s]Epoch 7:  34%|███▍      | 103/300 [00:45<01:23,  2.36it/s]Epoch 7:  35%|███▍      | 104/300 [00:45<01:28,  2.22it/s]Epoch 7:  35%|███▌      | 105/300 [00:46<01:22,  2.35it/s]Epoch 7:  35%|███▌      | 106/300 [00:46<01:21,  2.39it/s]Epoch 7:  36%|███▌      | 107/300 [00:47<01:18,  2.45it/s]Epoch 7:  36%|███▌      | 108/300 [00:47<01:24,  2.27it/s]Epoch 7:  36%|███▋      | 109/300 [00:48<01:22,  2.32it/s]Epoch 7:  37%|███▋      | 110/300 [00:48<01:22,  2.31it/s]Epoch 7:  37%|███▋      | 111/300 [00:48<01:16,  2.47it/s]Epoch 7:  37%|███▋      | 112/300 [00:49<01:18,  2.40it/s]Epoch 7:  38%|███▊      | 113/300 [00:49<01:14,  2.52it/s]Epoch 7:  38%|███▊      | 114/300 [00:50<01:14,  2.50it/s]Epoch 7:  38%|███▊      | 115/300 [00:50<01:13,  2.51it/s]Epoch 7:  39%|███▊      | 116/300 [00:51<01:23,  2.21it/s]Epoch 7:  39%|███▉      | 117/300 [00:51<01:18,  2.32it/s]Epoch 7:  39%|███▉      | 118/300 [00:51<01:20,  2.26it/s]Epoch 7:  40%|███▉      | 119/300 [00:52<01:19,  2.27it/s]06/19/2022 14:43:26 - INFO - __main__ - global step: 1110; train loss: 7.830108642578125; dev loss: 7.654432773590088
Epoch 7:  40%|████      | 120/300 [00:52<01:24,  2.13it/s]Epoch 7:  40%|████      | 121/300 [00:53<01:17,  2.30it/s]Epoch 7:  41%|████      | 122/300 [00:53<01:12,  2.45it/s]Epoch 7:  41%|████      | 123/300 [00:53<01:08,  2.57it/s]Epoch 7:  41%|████▏     | 124/300 [00:54<01:11,  2.48it/s]Epoch 7:  42%|████▏     | 125/300 [00:54<01:11,  2.44it/s]Epoch 7:  42%|████▏     | 126/300 [00:55<01:12,  2.39it/s]Epoch 7:  42%|████▏     | 127/300 [00:55<01:11,  2.43it/s]Epoch 7:  43%|████▎     | 128/300 [00:56<01:23,  2.05it/s]Epoch 7:  43%|████▎     | 129/300 [00:56<01:20,  2.13it/s]Epoch 7:  43%|████▎     | 130/300 [00:57<01:17,  2.20it/s]Epoch 7:  44%|████▎     | 131/300 [00:57<01:14,  2.28it/s]Epoch 7:  44%|████▍     | 132/300 [00:57<01:12,  2.33it/s]Epoch 7:  44%|████▍     | 133/300 [00:58<01:16,  2.17it/s]Epoch 7:  45%|████▍     | 134/300 [00:58<01:16,  2.17it/s]Epoch 7:  45%|████▌     | 135/300 [00:59<01:15,  2.18it/s]Epoch 7:  45%|████▌     | 136/300 [00:59<01:15,  2.19it/s]Epoch 7:  46%|████▌     | 137/300 [01:00<01:20,  2.01it/s]Epoch 7:  46%|████▌     | 138/300 [01:00<01:18,  2.06it/s]Epoch 7:  46%|████▋     | 139/300 [01:01<01:20,  2.00it/s]06/19/2022 14:43:35 - INFO - __main__ - global step: 1120; train loss: 7.981871604919434; dev loss: 8.034341812133789
Epoch 7:  47%|████▋     | 140/300 [01:01<01:18,  2.03it/s]Epoch 7:  47%|████▋     | 141/300 [01:02<01:17,  2.06it/s]Epoch 7:  47%|████▋     | 142/300 [01:02<01:10,  2.23it/s]Epoch 7:  48%|████▊     | 143/300 [01:03<01:06,  2.36it/s]Epoch 7:  48%|████▊     | 144/300 [01:03<01:03,  2.47it/s]Epoch 7:  48%|████▊     | 145/300 [01:03<01:04,  2.42it/s]Epoch 7:  49%|████▊     | 146/300 [01:04<00:59,  2.57it/s]Epoch 7:  49%|████▉     | 147/300 [01:04<00:57,  2.68it/s]Epoch 7:  49%|████▉     | 148/300 [01:04<00:54,  2.78it/s]Epoch 7:  50%|████▉     | 149/300 [01:05<00:59,  2.54it/s]Epoch 7:  50%|█████     | 150/300 [01:05<00:57,  2.60it/s]Epoch 7:  50%|█████     | 151/300 [01:06<00:59,  2.51it/s]Epoch 7:  51%|█████     | 152/300 [01:06<00:58,  2.53it/s]Epoch 7:  51%|█████     | 153/300 [01:06<01:03,  2.32it/s]Epoch 7:  51%|█████▏    | 154/300 [01:07<00:59,  2.46it/s]Epoch 7:  52%|█████▏    | 155/300 [01:07<00:59,  2.45it/s]Epoch 7:  52%|█████▏    | 156/300 [01:08<00:55,  2.57it/s]Epoch 7:  52%|█████▏    | 157/300 [01:08<00:54,  2.61it/s]Epoch 7:  53%|█████▎    | 158/300 [01:08<00:58,  2.45it/s]Epoch 7:  53%|█████▎    | 159/300 [01:09<00:56,  2.50it/s]06/19/2022 14:43:43 - INFO - __main__ - global step: 1130; train loss: 7.858724117279053; dev loss: 7.93158483505249
Epoch 7:  53%|█████▎    | 160/300 [01:09<00:53,  2.63it/s]Epoch 7:  54%|█████▎    | 161/300 [01:09<00:50,  2.73it/s]Epoch 7:  54%|█████▍    | 162/300 [01:10<00:53,  2.56it/s]Epoch 7:  54%|█████▍    | 163/300 [01:10<00:57,  2.39it/s]Epoch 7:  55%|█████▍    | 164/300 [01:11<00:59,  2.29it/s]Epoch 7:  55%|█████▌    | 165/300 [01:11<01:01,  2.18it/s]Epoch 7:  55%|█████▌    | 166/300 [01:12<01:06,  2.01it/s]Epoch 7:  56%|█████▌    | 167/300 [01:12<01:06,  2.01it/s]Epoch 7:  56%|█████▌    | 168/300 [01:13<01:04,  2.04it/s]Epoch 7:  56%|█████▋    | 169/300 [01:13<00:58,  2.23it/s]Epoch 7:  57%|█████▋    | 170/300 [01:14<00:59,  2.17it/s]Epoch 7:  57%|█████▋    | 171/300 [01:14<00:59,  2.18it/s]Epoch 7:  57%|█████▋    | 172/300 [01:15<00:58,  2.19it/s]Epoch 7:  58%|█████▊    | 173/300 [01:15<00:59,  2.14it/s]Epoch 7:  58%|█████▊    | 174/300 [01:16<00:59,  2.10it/s]Epoch 7:  58%|█████▊    | 175/300 [01:16<00:57,  2.16it/s]Epoch 7:  59%|█████▊    | 176/300 [01:17<00:55,  2.24it/s]Epoch 7:  59%|█████▉    | 177/300 [01:17<00:57,  2.13it/s]Epoch 7:  59%|█████▉    | 178/300 [01:18<01:03,  1.93it/s]Epoch 7:  60%|█████▉    | 179/300 [01:18<00:58,  2.07it/s]06/19/2022 14:43:52 - INFO - __main__ - global step: 1140; train loss: 7.5209245681762695; dev loss: 7.567992210388184
Epoch 7:  60%|██████    | 180/300 [01:18<00:53,  2.23it/s]Epoch 7:  60%|██████    | 181/300 [01:19<00:52,  2.29it/s]Epoch 7:  61%|██████    | 182/300 [01:19<00:53,  2.19it/s]Epoch 7:  61%|██████    | 183/300 [01:20<00:51,  2.28it/s]Epoch 7:  61%|██████▏   | 184/300 [01:20<00:49,  2.36it/s]Epoch 7:  62%|██████▏   | 185/300 [01:21<00:47,  2.40it/s]Epoch 7:  62%|██████▏   | 186/300 [01:21<00:48,  2.35it/s]Epoch 7:  62%|██████▏   | 187/300 [01:22<00:51,  2.20it/s]Epoch 7:  63%|██████▎   | 188/300 [01:22<00:50,  2.22it/s]Epoch 7:  63%|██████▎   | 189/300 [01:22<00:50,  2.22it/s]Epoch 7:  63%|██████▎   | 190/300 [01:23<00:51,  2.13it/s]Epoch 7:  64%|██████▎   | 191/300 [01:23<00:51,  2.13it/s]Epoch 7:  64%|██████▍   | 192/300 [01:24<00:47,  2.29it/s]Epoch 7:  64%|██████▍   | 193/300 [01:24<00:46,  2.30it/s]Epoch 7:  65%|██████▍   | 194/300 [01:25<00:44,  2.38it/s]Epoch 7:  65%|██████▌   | 195/300 [01:25<00:46,  2.26it/s]Epoch 7:  65%|██████▌   | 196/300 [01:25<00:44,  2.34it/s]Epoch 7:  66%|██████▌   | 197/300 [01:26<00:42,  2.45it/s]Epoch 7:  66%|██████▌   | 198/300 [01:26<00:42,  2.40it/s]Epoch 7:  66%|██████▋   | 199/300 [01:27<00:46,  2.19it/s]06/19/2022 14:44:01 - INFO - __main__ - global step: 1150; train loss: 7.724886894226074; dev loss: 7.749111175537109
Epoch 7:  67%|██████▋   | 200/300 [01:27<00:42,  2.33it/s]Epoch 7:  67%|██████▋   | 201/300 [01:28<00:40,  2.46it/s]Epoch 7:  67%|██████▋   | 202/300 [01:28<00:38,  2.53it/s]Epoch 7:  68%|██████▊   | 203/300 [01:28<00:40,  2.40it/s]Epoch 7:  68%|██████▊   | 204/300 [01:29<00:38,  2.50it/s]Epoch 7:  68%|██████▊   | 205/300 [01:29<00:37,  2.51it/s]Epoch 7:  69%|██████▊   | 206/300 [01:30<00:41,  2.29it/s]Epoch 7:  69%|██████▉   | 207/300 [01:30<00:44,  2.08it/s]Epoch 7:  69%|██████▉   | 208/300 [01:31<00:43,  2.14it/s]Epoch 7:  70%|██████▉   | 209/300 [01:31<00:41,  2.20it/s]Epoch 7:  70%|███████   | 210/300 [01:31<00:39,  2.30it/s]Epoch 7:  70%|███████   | 211/300 [01:32<00:38,  2.31it/s]Epoch 7:  71%|███████   | 212/300 [01:32<00:40,  2.18it/s]Epoch 7:  71%|███████   | 213/300 [01:33<00:37,  2.30it/s]Epoch 7:  71%|███████▏  | 214/300 [01:33<00:36,  2.36it/s]Epoch 7:  72%|███████▏  | 215/300 [01:34<00:36,  2.35it/s]Epoch 7:  72%|███████▏  | 216/300 [01:34<00:40,  2.09it/s]Epoch 7:  72%|███████▏  | 217/300 [01:35<00:37,  2.20it/s]Epoch 7:  73%|███████▎  | 218/300 [01:35<00:35,  2.30it/s]Epoch 7:  73%|███████▎  | 219/300 [01:35<00:34,  2.33it/s]06/19/2022 14:44:10 - INFO - __main__ - global step: 1160; train loss: 8.294590950012207; dev loss: 8.099782943725586
Epoch 7:  73%|███████▎  | 220/300 [01:36<00:37,  2.13it/s]Epoch 7:  74%|███████▎  | 221/300 [01:36<00:34,  2.28it/s]Epoch 7:  74%|███████▍  | 222/300 [01:37<00:33,  2.34it/s]Epoch 7:  74%|███████▍  | 223/300 [01:37<00:32,  2.39it/s]Epoch 7:  75%|███████▍  | 224/300 [01:38<00:34,  2.20it/s]Epoch 7:  75%|███████▌  | 225/300 [01:38<00:33,  2.23it/s]Epoch 7:  75%|███████▌  | 226/300 [01:39<00:31,  2.32it/s]Epoch 7:  76%|███████▌  | 227/300 [01:39<00:30,  2.38it/s]Epoch 7:  76%|███████▌  | 228/300 [01:40<00:33,  2.15it/s]Epoch 7:  76%|███████▋  | 229/300 [01:40<00:33,  2.14it/s]Epoch 7:  77%|███████▋  | 230/300 [01:40<00:33,  2.11it/s]Epoch 7:  77%|███████▋  | 231/300 [01:41<00:32,  2.10it/s]Epoch 7:  77%|███████▋  | 232/300 [01:42<00:36,  1.86it/s]Epoch 7:  78%|███████▊  | 233/300 [01:42<00:35,  1.88it/s]Epoch 7:  78%|███████▊  | 234/300 [01:43<00:35,  1.87it/s]Epoch 7:  78%|███████▊  | 235/300 [01:43<00:34,  1.88it/s]Epoch 7:  79%|███████▊  | 236/300 [01:44<00:34,  1.83it/s]Epoch 7:  79%|███████▉  | 237/300 [01:44<00:31,  2.01it/s]Epoch 7:  79%|███████▉  | 238/300 [01:45<00:29,  2.14it/s]Epoch 7:  80%|███████▉  | 239/300 [01:45<00:27,  2.24it/s]06/19/2022 14:44:19 - INFO - __main__ - global step: 1170; train loss: 8.082036972045898; dev loss: 7.715838432312012
Epoch 7:  80%|████████  | 240/300 [01:45<00:26,  2.30it/s]Epoch 7:  80%|████████  | 241/300 [01:46<00:27,  2.18it/s]Epoch 7:  81%|████████  | 242/300 [01:46<00:25,  2.28it/s]Epoch 7:  81%|████████  | 243/300 [01:47<00:23,  2.39it/s]Epoch 7:  81%|████████▏ | 244/300 [01:47<00:23,  2.41it/s]Epoch 7:  82%|████████▏ | 245/300 [01:48<00:24,  2.27it/s]Epoch 7:  82%|████████▏ | 246/300 [01:48<00:23,  2.34it/s]Epoch 7:  82%|████████▏ | 247/300 [01:48<00:21,  2.43it/s]Epoch 7:  83%|████████▎ | 248/300 [01:49<00:21,  2.41it/s]Epoch 7:  83%|████████▎ | 249/300 [01:49<00:23,  2.21it/s]Epoch 7:  83%|████████▎ | 250/300 [01:50<00:21,  2.31it/s]Epoch 7:  84%|████████▎ | 251/300 [01:50<00:21,  2.30it/s]Epoch 7:  84%|████████▍ | 252/300 [01:51<00:20,  2.34it/s]Epoch 7:  84%|████████▍ | 253/300 [01:51<00:21,  2.17it/s]Epoch 7:  85%|████████▍ | 254/300 [01:51<00:20,  2.23it/s]Epoch 7:  85%|████████▌ | 255/300 [01:52<00:19,  2.31it/s]Epoch 7:  85%|████████▌ | 256/300 [01:52<00:18,  2.32it/s]Epoch 7:  86%|████████▌ | 257/300 [01:53<00:20,  2.06it/s]Epoch 7:  86%|████████▌ | 258/300 [01:53<00:19,  2.18it/s]Epoch 7:  86%|████████▋ | 259/300 [01:54<00:18,  2.23it/s]06/19/2022 14:44:28 - INFO - __main__ - global step: 1180; train loss: 7.659806728363037; dev loss: 7.6015801429748535
Epoch 7:  87%|████████▋ | 260/300 [01:54<00:18,  2.15it/s]Epoch 7:  87%|████████▋ | 261/300 [01:55<00:19,  1.99it/s]Epoch 7:  87%|████████▋ | 262/300 [01:55<00:17,  2.13it/s]Epoch 7:  88%|████████▊ | 263/300 [01:56<00:16,  2.25it/s]Epoch 7:  88%|████████▊ | 264/300 [01:56<00:16,  2.15it/s]Epoch 7:  88%|████████▊ | 265/300 [01:57<00:16,  2.11it/s]Epoch 7:  89%|████████▊ | 266/300 [01:57<00:16,  2.05it/s]Epoch 7:  89%|████████▉ | 267/300 [01:58<00:15,  2.12it/s]Epoch 7:  89%|████████▉ | 268/300 [01:58<00:14,  2.28it/s]Epoch 7:  90%|████████▉ | 269/300 [01:58<00:13,  2.38it/s]Epoch 7:  90%|█████████ | 270/300 [01:59<00:13,  2.18it/s]Epoch 7:  90%|█████████ | 271/300 [01:59<00:12,  2.26it/s]Epoch 7:  91%|█████████ | 272/300 [02:00<00:12,  2.22it/s]Epoch 7:  91%|█████████ | 273/300 [02:00<00:11,  2.37it/s]Epoch 7:  91%|█████████▏| 274/300 [02:01<00:11,  2.29it/s]Epoch 7:  92%|█████████▏| 275/300 [02:01<00:10,  2.31it/s]Epoch 7:  92%|█████████▏| 276/300 [02:01<00:10,  2.28it/s]Epoch 7:  92%|█████████▏| 277/300 [02:02<00:10,  2.28it/s]Epoch 7:  93%|█████████▎| 278/300 [02:03<00:10,  2.00it/s]Epoch 7:  93%|█████████▎| 279/300 [02:03<00:10,  1.99it/s]06/19/2022 14:44:37 - INFO - __main__ - global step: 1190; train loss: 8.140295028686523; dev loss: 8.115925788879395
Epoch 7:  93%|█████████▎| 280/300 [02:04<00:10,  1.99it/s]Epoch 7:  94%|█████████▎| 281/300 [02:04<00:09,  2.11it/s]Epoch 7:  94%|█████████▍| 282/300 [02:04<00:08,  2.11it/s]Epoch 7:  94%|█████████▍| 283/300 [02:05<00:07,  2.26it/s]Epoch 7:  95%|█████████▍| 284/300 [02:05<00:06,  2.40it/s]Epoch 7:  95%|█████████▌| 285/300 [02:06<00:06,  2.31it/s]Epoch 7:  95%|█████████▌| 286/300 [02:06<00:06,  2.06it/s]Epoch 7:  96%|█████████▌| 287/300 [02:07<00:05,  2.18it/s]Epoch 7:  96%|█████████▌| 288/300 [02:07<00:05,  2.29it/s]Epoch 7:  96%|█████████▋| 289/300 [02:07<00:04,  2.34it/s]Epoch 7:  97%|█████████▋| 290/300 [02:08<00:04,  2.22it/s]Epoch 7:  97%|█████████▋| 291/300 [02:08<00:03,  2.29it/s]Epoch 7:  97%|█████████▋| 292/300 [02:09<00:03,  2.33it/s]Epoch 7:  98%|█████████▊| 293/300 [02:09<00:03,  2.27it/s]Epoch 7:  98%|█████████▊| 294/300 [02:10<00:02,  2.22it/s]Epoch 7:  98%|█████████▊| 295/300 [02:10<00:02,  2.00it/s]Epoch 7:  99%|█████████▊| 296/300 [02:11<00:02,  1.98it/s]Epoch 7:  99%|█████████▉| 297/300 [02:11<00:01,  2.13it/s]Epoch 7:  99%|█████████▉| 298/300 [02:12<00:00,  2.26it/s]Epoch 7: 100%|█████████▉| 299/300 [02:12<00:00,  2.09it/s]06/19/2022 14:44:47 - INFO - __main__ - global step: 1200; train loss: 7.3068647384643555; dev loss: 7.223646640777588
Epoch 7: 100%|██████████| 300/300 [02:13<00:00,  2.12it/s]Epoch 7: 100%|██████████| 300/300 [02:13<00:00,  2.25it/s]
Epoch 8:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 8:   0%|          | 1/300 [00:00<02:22,  2.10it/s]Epoch 8:   1%|          | 2/300 [00:00<02:19,  2.14it/s]Epoch 8:   1%|          | 3/300 [00:01<02:37,  1.89it/s]Epoch 8:   1%|▏         | 4/300 [00:01<02:27,  2.00it/s]Epoch 8:   2%|▏         | 5/300 [00:02<02:31,  1.95it/s]Epoch 8:   2%|▏         | 6/300 [00:02<02:24,  2.04it/s]Epoch 8:   2%|▏         | 7/300 [00:03<02:36,  1.87it/s]Epoch 8:   3%|▎         | 8/300 [00:04<02:28,  1.96it/s]Epoch 8:   3%|▎         | 9/300 [00:04<02:21,  2.06it/s]Epoch 8:   3%|▎         | 10/300 [00:04<02:15,  2.13it/s]Epoch 8:   4%|▎         | 11/300 [00:05<02:19,  2.07it/s]Epoch 8:   4%|▍         | 12/300 [00:05<02:10,  2.21it/s]Epoch 8:   4%|▍         | 13/300 [00:06<02:05,  2.28it/s]Epoch 8:   5%|▍         | 14/300 [00:06<02:01,  2.35it/s]Epoch 8:   5%|▌         | 15/300 [00:07<02:06,  2.25it/s]Epoch 8:   5%|▌         | 16/300 [00:07<02:02,  2.32it/s]Epoch 8:   6%|▌         | 17/300 [00:07<01:55,  2.45it/s]Epoch 8:   6%|▌         | 18/300 [00:08<01:55,  2.43it/s]Epoch 8:   6%|▋         | 19/300 [00:08<01:51,  2.53it/s]06/19/2022 14:44:56 - INFO - __main__ - global step: 1210; train loss: 7.925207614898682; dev loss: 8.124931335449219
Epoch 8:   7%|▋         | 20/300 [00:09<01:59,  2.33it/s]Epoch 8:   7%|▋         | 21/300 [00:09<01:54,  2.43it/s]Epoch 8:   7%|▋         | 22/300 [00:09<01:50,  2.53it/s]Epoch 8:   8%|▊         | 23/300 [00:10<01:46,  2.60it/s]Epoch 8:   8%|▊         | 24/300 [00:10<01:52,  2.45it/s]Epoch 8:   8%|▊         | 25/300 [00:11<01:55,  2.38it/s]Epoch 8:   9%|▊         | 26/300 [00:11<01:54,  2.39it/s]Epoch 8:   9%|▉         | 27/300 [00:12<02:01,  2.25it/s]Epoch 8:   9%|▉         | 28/300 [00:12<02:14,  2.03it/s]Epoch 8:  10%|▉         | 29/300 [00:13<02:20,  1.93it/s]Epoch 8:  10%|█         | 30/300 [00:13<02:17,  1.97it/s]Epoch 8:  10%|█         | 31/300 [00:14<02:14,  2.01it/s]Epoch 8:  11%|█         | 32/300 [00:14<02:20,  1.91it/s]Epoch 8:  11%|█         | 33/300 [00:15<02:09,  2.06it/s]Epoch 8:  11%|█▏        | 34/300 [00:15<01:58,  2.24it/s]Epoch 8:  12%|█▏        | 35/300 [00:15<01:51,  2.38it/s]Epoch 8:  12%|█▏        | 36/300 [00:16<02:05,  2.11it/s]Epoch 8:  12%|█▏        | 37/300 [00:16<01:58,  2.23it/s]Epoch 8:  13%|█▎        | 38/300 [00:17<02:02,  2.13it/s]Epoch 8:  13%|█▎        | 39/300 [00:17<02:00,  2.17it/s]06/19/2022 14:45:05 - INFO - __main__ - global step: 1220; train loss: 7.831233024597168; dev loss: 8.135026931762695
Epoch 8:  13%|█▎        | 40/300 [00:18<02:01,  2.15it/s]Epoch 8:  14%|█▎        | 41/300 [00:18<01:52,  2.31it/s]Epoch 8:  14%|█▍        | 42/300 [00:19<01:46,  2.43it/s]Epoch 8:  14%|█▍        | 43/300 [00:19<01:41,  2.54it/s]Epoch 8:  15%|█▍        | 44/300 [00:20<01:56,  2.19it/s]Epoch 8:  15%|█▌        | 45/300 [00:20<01:59,  2.13it/s]Epoch 8:  15%|█▌        | 46/300 [00:21<02:07,  1.99it/s]Epoch 8:  16%|█▌        | 47/300 [00:21<02:03,  2.06it/s]Epoch 8:  16%|█▌        | 48/300 [00:21<01:53,  2.23it/s]Epoch 8:  16%|█▋        | 49/300 [00:22<02:00,  2.09it/s]Epoch 8:  17%|█▋        | 50/300 [00:22<01:54,  2.19it/s]Epoch 8:  17%|█▋        | 51/300 [00:23<01:52,  2.22it/s]Epoch 8:  17%|█▋        | 52/300 [00:23<01:51,  2.23it/s]Epoch 8:  18%|█▊        | 53/300 [00:24<01:52,  2.20it/s]Epoch 8:  18%|█▊        | 54/300 [00:24<01:50,  2.23it/s]Epoch 8:  18%|█▊        | 55/300 [00:24<01:43,  2.37it/s]Epoch 8:  19%|█▊        | 56/300 [00:25<01:41,  2.41it/s]Epoch 8:  19%|█▉        | 57/300 [00:26<01:57,  2.07it/s]Epoch 8:  19%|█▉        | 58/300 [00:26<01:56,  2.08it/s]Epoch 8:  20%|█▉        | 59/300 [00:27<02:01,  1.98it/s]06/19/2022 14:45:14 - INFO - __main__ - global step: 1230; train loss: 7.792308807373047; dev loss: 7.892570495605469
Epoch 8:  20%|██        | 60/300 [00:27<01:58,  2.03it/s]Epoch 8:  20%|██        | 61/300 [00:28<02:05,  1.90it/s]Epoch 8:  21%|██        | 62/300 [00:28<01:59,  1.98it/s]Epoch 8:  21%|██        | 63/300 [00:29<01:53,  2.09it/s]Epoch 8:  21%|██▏       | 64/300 [00:29<01:44,  2.25it/s]Epoch 8:  22%|██▏       | 65/300 [00:29<01:47,  2.18it/s]Epoch 8:  22%|██▏       | 66/300 [00:30<01:41,  2.30it/s]Epoch 8:  22%|██▏       | 67/300 [00:30<01:43,  2.25it/s]Epoch 8:  23%|██▎       | 68/300 [00:31<01:43,  2.24it/s]Epoch 8:  23%|██▎       | 69/300 [00:31<01:48,  2.13it/s]Epoch 8:  23%|██▎       | 70/300 [00:32<01:47,  2.14it/s]Epoch 8:  24%|██▎       | 71/300 [00:32<01:51,  2.06it/s]Epoch 8:  24%|██▍       | 72/300 [00:33<01:48,  2.10it/s]Epoch 8:  24%|██▍       | 73/300 [00:33<01:41,  2.23it/s]Epoch 8:  25%|██▍       | 74/300 [00:34<01:55,  1.96it/s]Epoch 8:  25%|██▌       | 75/300 [00:34<01:53,  1.99it/s]Epoch 8:  25%|██▌       | 76/300 [00:35<01:50,  2.04it/s]Epoch 8:  26%|██▌       | 77/300 [00:35<01:53,  1.97it/s]Epoch 8:  26%|██▌       | 78/300 [00:36<01:50,  2.01it/s]Epoch 8:  26%|██▋       | 79/300 [00:36<01:40,  2.20it/s]06/19/2022 14:45:23 - INFO - __main__ - global step: 1240; train loss: 7.632171630859375; dev loss: 7.628617763519287
Epoch 8:  27%|██▋       | 80/300 [00:36<01:33,  2.35it/s]Epoch 8:  27%|██▋       | 81/300 [00:37<01:28,  2.46it/s]Epoch 8:  27%|██▋       | 82/300 [00:37<01:33,  2.32it/s]Epoch 8:  28%|██▊       | 83/300 [00:38<01:35,  2.26it/s]Epoch 8:  28%|██▊       | 84/300 [00:38<01:37,  2.23it/s]Epoch 8:  28%|██▊       | 85/300 [00:39<01:37,  2.21it/s]Epoch 8:  29%|██▊       | 86/300 [00:39<01:46,  2.01it/s]Epoch 8:  29%|██▉       | 87/300 [00:40<01:44,  2.04it/s]Epoch 8:  29%|██▉       | 88/300 [00:40<01:46,  2.00it/s]Epoch 8:  30%|██▉       | 89/300 [00:41<01:37,  2.16it/s]Epoch 8:  30%|███       | 90/300 [00:41<01:40,  2.09it/s]Epoch 8:  30%|███       | 91/300 [00:41<01:32,  2.27it/s]Epoch 8:  31%|███       | 92/300 [00:42<01:29,  2.32it/s]Epoch 8:  31%|███       | 93/300 [00:42<01:26,  2.38it/s]Epoch 8:  31%|███▏      | 94/300 [00:43<01:31,  2.25it/s]Epoch 8:  32%|███▏      | 95/300 [00:43<01:28,  2.33it/s]Epoch 8:  32%|███▏      | 96/300 [00:44<01:25,  2.39it/s]Epoch 8:  32%|███▏      | 97/300 [00:44<01:27,  2.32it/s]Epoch 8:  33%|███▎      | 98/300 [00:45<01:35,  2.12it/s]Epoch 8:  33%|███▎      | 99/300 [00:45<01:34,  2.13it/s]06/19/2022 14:45:33 - INFO - __main__ - global step: 1250; train loss: 7.76956033706665; dev loss: 7.906622409820557
Epoch 8:  33%|███▎      | 100/300 [00:45<01:34,  2.12it/s]Epoch 8:  34%|███▎      | 101/300 [00:46<01:33,  2.12it/s]Epoch 8:  34%|███▍      | 102/300 [00:46<01:33,  2.12it/s]Epoch 8:  34%|███▍      | 103/300 [00:47<01:38,  2.00it/s]Epoch 8:  35%|███▍      | 104/300 [00:47<01:36,  2.03it/s]Epoch 8:  35%|███▌      | 105/300 [00:48<01:34,  2.07it/s]Epoch 8:  35%|███▌      | 106/300 [00:48<01:29,  2.16it/s]Epoch 8:  36%|███▌      | 107/300 [00:49<01:36,  2.01it/s]Epoch 8:  36%|███▌      | 108/300 [00:49<01:33,  2.05it/s]Epoch 8:  36%|███▋      | 109/300 [00:50<01:26,  2.20it/s]Epoch 8:  37%|███▋      | 110/300 [00:50<01:26,  2.19it/s]Epoch 8:  37%|███▋      | 111/300 [00:51<01:34,  2.00it/s]Epoch 8:  37%|███▋      | 112/300 [00:51<01:31,  2.05it/s]Epoch 8:  38%|███▊      | 113/300 [00:52<01:24,  2.22it/s]Epoch 8:  38%|███▊      | 114/300 [00:52<01:22,  2.25it/s]Epoch 8:  38%|███▊      | 115/300 [00:53<01:23,  2.21it/s]Epoch 8:  39%|███▊      | 116/300 [00:53<01:22,  2.23it/s]Epoch 8:  39%|███▉      | 117/300 [00:53<01:22,  2.21it/s]Epoch 8:  39%|███▉      | 118/300 [00:54<01:23,  2.19it/s]Epoch 8:  40%|███▉      | 119/300 [00:55<01:30,  2.00it/s]06/19/2022 14:45:42 - INFO - __main__ - global step: 1260; train loss: 7.7395172119140625; dev loss: 7.770373344421387
Epoch 8:  40%|████      | 120/300 [00:55<01:24,  2.12it/s]Epoch 8:  40%|████      | 121/300 [00:55<01:18,  2.29it/s]Epoch 8:  41%|████      | 122/300 [00:56<01:13,  2.43it/s]Epoch 8:  41%|████      | 123/300 [00:56<01:20,  2.19it/s]Epoch 8:  41%|████▏     | 124/300 [00:57<01:15,  2.32it/s]Epoch 8:  42%|████▏     | 125/300 [00:57<01:13,  2.37it/s]Epoch 8:  42%|████▏     | 126/300 [00:57<01:10,  2.45it/s]Epoch 8:  42%|████▏     | 127/300 [00:58<01:11,  2.41it/s]Epoch 8:  43%|████▎     | 128/300 [00:58<01:14,  2.29it/s]Epoch 8:  43%|████▎     | 129/300 [00:59<01:12,  2.35it/s]Epoch 8:  43%|████▎     | 130/300 [00:59<01:10,  2.40it/s]Epoch 8:  44%|████▎     | 131/300 [00:59<01:08,  2.46it/s]Epoch 8:  44%|████▍     | 132/300 [01:00<01:13,  2.29it/s]Epoch 8:  44%|████▍     | 133/300 [01:00<01:11,  2.34it/s]Epoch 8:  45%|████▍     | 134/300 [01:01<01:11,  2.34it/s]Epoch 8:  45%|████▌     | 135/300 [01:01<01:10,  2.33it/s]Epoch 8:  45%|████▌     | 136/300 [01:02<01:12,  2.28it/s]Epoch 8:  46%|████▌     | 137/300 [01:02<01:11,  2.29it/s]Epoch 8:  46%|████▌     | 138/300 [01:03<01:10,  2.29it/s]Epoch 8:  46%|████▋     | 139/300 [01:03<01:10,  2.29it/s]06/19/2022 14:45:51 - INFO - __main__ - global step: 1270; train loss: 7.894798278808594; dev loss: 7.950660705566406
Epoch 8:  47%|████▋     | 140/300 [01:04<01:15,  2.13it/s]Epoch 8:  47%|████▋     | 141/300 [01:04<01:09,  2.28it/s]Epoch 8:  47%|████▋     | 142/300 [01:04<01:09,  2.29it/s]Epoch 8:  48%|████▊     | 143/300 [01:05<01:06,  2.35it/s]Epoch 8:  48%|████▊     | 144/300 [01:05<01:11,  2.18it/s]Epoch 8:  48%|████▊     | 145/300 [01:06<01:07,  2.29it/s]Epoch 8:  49%|████▊     | 146/300 [01:06<01:10,  2.19it/s]Epoch 8:  49%|████▉     | 147/300 [01:07<01:10,  2.18it/s]Epoch 8:  49%|████▉     | 148/300 [01:07<01:17,  1.97it/s]Epoch 8:  50%|████▉     | 149/300 [01:08<01:14,  2.04it/s]Epoch 8:  50%|█████     | 150/300 [01:08<01:12,  2.06it/s]Epoch 8:  50%|█████     | 151/300 [01:09<01:11,  2.08it/s]Epoch 8:  51%|█████     | 152/300 [01:09<01:16,  1.94it/s]Epoch 8:  51%|█████     | 153/300 [01:10<01:08,  2.13it/s]Epoch 8:  51%|█████▏    | 154/300 [01:10<01:03,  2.29it/s]Epoch 8:  52%|█████▏    | 155/300 [01:10<00:59,  2.43it/s]Epoch 8:  52%|█████▏    | 156/300 [01:11<00:56,  2.53it/s]Epoch 8:  52%|█████▏    | 157/300 [01:11<01:04,  2.22it/s]Epoch 8:  53%|█████▎    | 158/300 [01:12<01:04,  2.19it/s]Epoch 8:  53%|█████▎    | 159/300 [01:12<01:04,  2.17it/s]06/19/2022 14:46:00 - INFO - __main__ - global step: 1280; train loss: 7.734442710876465; dev loss: 7.92507791519165
Epoch 8:  53%|█████▎    | 160/300 [01:13<01:04,  2.18it/s]Epoch 8:  54%|█████▎    | 161/300 [01:13<01:04,  2.17it/s]Epoch 8:  54%|█████▍    | 162/300 [01:13<00:59,  2.31it/s]Epoch 8:  54%|█████▍    | 163/300 [01:14<00:56,  2.43it/s]Epoch 8:  55%|█████▍    | 164/300 [01:14<00:54,  2.49it/s]Epoch 8:  55%|█████▌    | 165/300 [01:15<01:02,  2.16it/s]Epoch 8:  55%|█████▌    | 166/300 [01:15<01:02,  2.16it/s]Epoch 8:  56%|█████▌    | 167/300 [01:16<01:01,  2.16it/s]Epoch 8:  56%|█████▌    | 168/300 [01:16<01:00,  2.19it/s]Epoch 8:  56%|█████▋    | 169/300 [01:17<01:00,  2.17it/s]Epoch 8:  57%|█████▋    | 170/300 [01:17<00:54,  2.37it/s]Epoch 8:  57%|█████▋    | 171/300 [01:17<00:51,  2.50it/s]Epoch 8:  57%|█████▋    | 172/300 [01:18<00:49,  2.60it/s]Epoch 8:  58%|█████▊    | 173/300 [01:18<00:53,  2.39it/s]Epoch 8:  58%|█████▊    | 174/300 [01:19<00:54,  2.33it/s]Epoch 8:  58%|█████▊    | 175/300 [01:19<00:56,  2.22it/s]Epoch 8:  59%|█████▊    | 176/300 [01:20<00:56,  2.19it/s]Epoch 8:  59%|█████▉    | 177/300 [01:20<00:55,  2.22it/s]Epoch 8:  59%|█████▉    | 178/300 [01:20<00:50,  2.42it/s]Epoch 8:  60%|█████▉    | 179/300 [01:21<00:47,  2.56it/s]06/19/2022 14:46:08 - INFO - __main__ - global step: 1290; train loss: 7.358819484710693; dev loss: 7.450841426849365
Epoch 8:  60%|██████    | 180/300 [01:21<00:45,  2.61it/s]Epoch 8:  60%|██████    | 181/300 [01:22<00:49,  2.41it/s]Epoch 8:  61%|██████    | 182/300 [01:22<01:01,  1.92it/s]Epoch 8:  61%|██████    | 183/300 [01:23<01:04,  1.83it/s]Epoch 8:  61%|██████▏   | 184/300 [01:23<01:01,  1.88it/s]Epoch 8:  62%|██████▏   | 185/300 [01:24<01:00,  1.90it/s]Epoch 8:  62%|██████▏   | 186/300 [01:25<01:04,  1.78it/s]Epoch 8:  62%|██████▏   | 187/300 [01:25<00:57,  1.96it/s]Epoch 8:  63%|██████▎   | 188/300 [01:25<00:54,  2.07it/s]Epoch 8:  63%|██████▎   | 189/300 [01:26<00:53,  2.07it/s]Epoch 8:  63%|██████▎   | 190/300 [01:27<01:00,  1.81it/s]Epoch 8:  64%|██████▎   | 191/300 [01:27<00:57,  1.90it/s]Epoch 8:  64%|██████▍   | 192/300 [01:27<00:51,  2.10it/s]Epoch 8:  64%|██████▍   | 193/300 [01:28<00:47,  2.24it/s]Epoch 8:  65%|██████▍   | 194/300 [01:28<00:48,  2.21it/s]Epoch 8:  65%|██████▌   | 195/300 [01:29<00:46,  2.25it/s]Epoch 8:  65%|██████▌   | 196/300 [01:29<00:43,  2.39it/s]Epoch 8:  66%|██████▌   | 197/300 [01:29<00:41,  2.50it/s]Epoch 8:  66%|██████▌   | 198/300 [01:30<00:42,  2.38it/s]Epoch 8:  66%|██████▋   | 199/300 [01:30<00:40,  2.50it/s]06/19/2022 14:46:18 - INFO - __main__ - global step: 1300; train loss: 8.381205558776855; dev loss: 8.217775344848633
Epoch 8:  67%|██████▋   | 200/300 [01:31<00:42,  2.37it/s]Epoch 8:  67%|██████▋   | 201/300 [01:31<00:42,  2.31it/s]Epoch 8:  67%|██████▋   | 202/300 [01:32<00:50,  1.96it/s]Epoch 8:  68%|██████▊   | 203/300 [01:32<00:45,  2.15it/s]Epoch 8:  68%|██████▊   | 204/300 [01:33<00:41,  2.31it/s]Epoch 8:  68%|██████▊   | 205/300 [01:33<00:39,  2.40it/s]Epoch 8:  69%|██████▊   | 206/300 [01:34<00:44,  2.12it/s]Epoch 8:  69%|██████▉   | 207/300 [01:34<00:43,  2.15it/s]Epoch 8:  69%|██████▉   | 208/300 [01:35<00:43,  2.11it/s]Epoch 8:  70%|██████▉   | 209/300 [01:35<00:43,  2.10it/s]Epoch 8:  70%|███████   | 210/300 [01:35<00:42,  2.13it/s]Epoch 8:  70%|███████   | 211/300 [01:36<00:43,  2.02it/s]Epoch 8:  71%|███████   | 212/300 [01:36<00:43,  2.03it/s]Epoch 8:  71%|███████   | 213/300 [01:37<00:41,  2.08it/s]Epoch 8:  71%|███████▏  | 214/300 [01:37<00:40,  2.11it/s]Epoch 8:  72%|███████▏  | 215/300 [01:38<00:41,  2.06it/s]Epoch 8:  72%|███████▏  | 216/300 [01:38<00:38,  2.19it/s]Epoch 8:  72%|███████▏  | 217/300 [01:39<00:36,  2.29it/s]Epoch 8:  73%|███████▎  | 218/300 [01:39<00:34,  2.39it/s]Epoch 8:  73%|███████▎  | 219/300 [01:40<00:35,  2.28it/s]06/19/2022 14:46:27 - INFO - __main__ - global step: 1310; train loss: 7.809762001037598; dev loss: 7.704705715179443
Epoch 8:  73%|███████▎  | 220/300 [01:40<00:33,  2.35it/s]Epoch 8:  74%|███████▎  | 221/300 [01:40<00:34,  2.30it/s]Epoch 8:  74%|███████▍  | 222/300 [01:41<00:33,  2.34it/s]Epoch 8:  74%|███████▍  | 223/300 [01:41<00:35,  2.14it/s]Epoch 8:  75%|███████▍  | 224/300 [01:42<00:35,  2.16it/s]Epoch 8:  75%|███████▌  | 225/300 [01:42<00:32,  2.32it/s]Epoch 8:  75%|███████▌  | 226/300 [01:43<00:30,  2.43it/s]Epoch 8:  76%|███████▌  | 227/300 [01:43<00:32,  2.26it/s]Epoch 8:  76%|███████▌  | 228/300 [01:43<00:30,  2.34it/s]Epoch 8:  76%|███████▋  | 229/300 [01:44<00:28,  2.45it/s]Epoch 8:  77%|███████▋  | 230/300 [01:44<00:28,  2.44it/s]Epoch 8:  77%|███████▋  | 231/300 [01:45<00:30,  2.28it/s]Epoch 8:  77%|███████▋  | 232/300 [01:45<00:29,  2.34it/s]Epoch 8:  78%|███████▊  | 233/300 [01:46<00:27,  2.41it/s]Epoch 8:  78%|███████▊  | 234/300 [01:46<00:28,  2.34it/s]Epoch 8:  78%|███████▊  | 235/300 [01:46<00:27,  2.39it/s]Epoch 8:  79%|███████▊  | 236/300 [01:47<00:30,  2.07it/s]Epoch 8:  79%|███████▉  | 237/300 [01:47<00:28,  2.19it/s]Epoch 8:  79%|███████▉  | 238/300 [01:48<00:26,  2.32it/s]Epoch 8:  80%|███████▉  | 239/300 [01:48<00:24,  2.45it/s]06/19/2022 14:46:36 - INFO - __main__ - global step: 1320; train loss: 7.820225715637207; dev loss: 7.432347297668457
Epoch 8:  80%|████████  | 240/300 [01:49<00:25,  2.35it/s]Epoch 8:  80%|████████  | 241/300 [01:49<00:23,  2.46it/s]Epoch 8:  81%|████████  | 242/300 [01:49<00:23,  2.52it/s]Epoch 8:  81%|████████  | 243/300 [01:50<00:24,  2.32it/s]Epoch 8:  81%|████████▏ | 244/300 [01:50<00:25,  2.17it/s]Epoch 8:  82%|████████▏ | 245/300 [01:51<00:24,  2.20it/s]Epoch 8:  82%|████████▏ | 246/300 [01:51<00:23,  2.33it/s]Epoch 8:  82%|████████▏ | 247/300 [01:52<00:21,  2.50it/s]Epoch 8:  83%|████████▎ | 248/300 [01:52<00:21,  2.45it/s]Epoch 8:  83%|████████▎ | 249/300 [01:52<00:20,  2.45it/s]Epoch 8:  83%|████████▎ | 250/300 [01:53<00:20,  2.41it/s]Epoch 8:  84%|████████▎ | 251/300 [01:53<00:20,  2.40it/s]Epoch 8:  84%|████████▍ | 252/300 [01:54<00:20,  2.31it/s]Epoch 8:  84%|████████▍ | 253/300 [01:54<00:19,  2.47it/s]Epoch 8:  85%|████████▍ | 254/300 [01:54<00:18,  2.53it/s]Epoch 8:  85%|████████▌ | 255/300 [01:55<00:16,  2.65it/s]Epoch 8:  85%|████████▌ | 256/300 [01:55<00:17,  2.47it/s]Epoch 8:  86%|████████▌ | 257/300 [01:56<00:16,  2.61it/s]Epoch 8:  86%|████████▌ | 258/300 [01:56<00:15,  2.71it/s]Epoch 8:  86%|████████▋ | 259/300 [01:56<00:16,  2.53it/s]06/19/2022 14:46:44 - INFO - __main__ - global step: 1330; train loss: 7.510318756103516; dev loss: 7.489683628082275
Epoch 8:  87%|████████▋ | 260/300 [01:57<00:16,  2.36it/s]Epoch 8:  87%|████████▋ | 261/300 [01:57<00:17,  2.19it/s]Epoch 8:  87%|████████▋ | 262/300 [01:58<00:16,  2.24it/s]Epoch 8:  88%|████████▊ | 263/300 [01:58<00:15,  2.37it/s]Epoch 8:  88%|████████▊ | 264/300 [01:58<00:14,  2.53it/s]Epoch 8:  88%|████████▊ | 265/300 [01:59<00:14,  2.34it/s]Epoch 8:  89%|████████▊ | 266/300 [01:59<00:13,  2.45it/s]Epoch 8:  89%|████████▉ | 267/300 [02:00<00:13,  2.41it/s]Epoch 8:  89%|████████▉ | 268/300 [02:00<00:13,  2.35it/s]Epoch 8:  90%|████████▉ | 269/300 [02:01<00:14,  2.14it/s]Epoch 8:  90%|█████████ | 270/300 [02:01<00:13,  2.16it/s]Epoch 8:  90%|█████████ | 271/300 [02:02<00:13,  2.12it/s]Epoch 8:  91%|█████████ | 272/300 [02:02<00:13,  2.14it/s]Epoch 8:  91%|█████████ | 273/300 [02:03<00:13,  2.01it/s]Epoch 8:  91%|█████████▏| 274/300 [02:03<00:12,  2.09it/s]Epoch 8:  92%|█████████▏| 275/300 [02:04<00:11,  2.25it/s]Epoch 8:  92%|█████████▏| 276/300 [02:04<00:10,  2.37it/s]Epoch 8:  92%|█████████▏| 277/300 [02:04<00:10,  2.28it/s]Epoch 8:  93%|█████████▎| 278/300 [02:05<00:09,  2.38it/s]Epoch 8:  93%|█████████▎| 279/300 [02:05<00:08,  2.51it/s]06/19/2022 14:46:53 - INFO - __main__ - global step: 1340; train loss: 7.638467311859131; dev loss: 7.834560394287109
Epoch 8:  93%|█████████▎| 280/300 [02:06<00:08,  2.28it/s]Epoch 8:  94%|█████████▎| 281/300 [02:06<00:08,  2.16it/s]Epoch 8:  94%|█████████▍| 282/300 [02:07<00:08,  2.24it/s]Epoch 8:  94%|█████████▍| 283/300 [02:07<00:07,  2.26it/s]Epoch 8:  95%|█████████▍| 284/300 [02:07<00:06,  2.29it/s]Epoch 8:  95%|█████████▌| 285/300 [02:08<00:06,  2.18it/s]Epoch 8:  95%|█████████▌| 286/300 [02:08<00:06,  2.26it/s]Epoch 8:  96%|█████████▌| 287/300 [02:09<00:05,  2.39it/s]Epoch 8:  96%|█████████▌| 288/300 [02:09<00:05,  2.39it/s]Epoch 8:  96%|█████████▋| 289/300 [02:10<00:04,  2.33it/s]Epoch 8:  97%|█████████▋| 290/300 [02:10<00:04,  2.07it/s]Epoch 8:  97%|█████████▋| 291/300 [02:11<00:04,  2.16it/s]Epoch 8:  97%|█████████▋| 292/300 [02:11<00:03,  2.36it/s]Epoch 8:  98%|█████████▊| 293/300 [02:11<00:02,  2.53it/s]Epoch 8:  98%|█████████▊| 294/300 [02:12<00:02,  2.47it/s]Epoch 8:  98%|█████████▊| 295/300 [02:12<00:01,  2.62it/s]Epoch 8:  99%|█████████▊| 296/300 [02:12<00:01,  2.74it/s]Epoch 8:  99%|█████████▉| 297/300 [02:13<00:01,  2.82it/s]Epoch 8:  99%|█████████▉| 298/300 [02:13<00:00,  2.50it/s]Epoch 8: 100%|█████████▉| 299/300 [02:14<00:00,  2.63it/s]06/19/2022 14:47:01 - INFO - __main__ - global step: 1350; train loss: 8.05257511138916; dev loss: 8.199666976928711
Epoch 8: 100%|██████████| 300/300 [02:14<00:00,  2.36it/s]Epoch 8: 100%|██████████| 300/300 [02:14<00:00,  2.23it/s]
Epoch 9:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 9:   0%|          | 1/300 [00:00<02:56,  1.70it/s]Epoch 9:   1%|          | 2/300 [00:01<02:38,  1.88it/s]Epoch 9:   1%|          | 3/300 [00:01<02:17,  2.16it/s]Epoch 9:   1%|▏         | 4/300 [00:01<02:05,  2.36it/s]Epoch 9:   2%|▏         | 5/300 [00:02<01:54,  2.58it/s]Epoch 9:   2%|▏         | 6/300 [00:02<01:58,  2.48it/s]Epoch 9:   2%|▏         | 7/300 [00:02<01:59,  2.46it/s]Epoch 9:   3%|▎         | 8/300 [00:03<01:55,  2.52it/s]Epoch 9:   3%|▎         | 9/300 [00:03<01:59,  2.43it/s]Epoch 9:   3%|▎         | 10/300 [00:04<02:23,  2.02it/s]Epoch 9:   4%|▎         | 11/300 [00:04<02:18,  2.08it/s]Epoch 9:   4%|▍         | 12/300 [00:05<02:22,  2.01it/s]Epoch 9:   4%|▍         | 13/300 [00:06<02:27,  1.95it/s]Epoch 9:   5%|▍         | 14/300 [00:06<02:23,  1.99it/s]Epoch 9:   5%|▌         | 15/300 [00:06<02:21,  2.01it/s]Epoch 9:   5%|▌         | 16/300 [00:07<02:15,  2.10it/s]Epoch 9:   6%|▌         | 17/300 [00:07<02:11,  2.14it/s]Epoch 9:   6%|▌         | 18/300 [00:08<02:11,  2.14it/s]Epoch 9:   6%|▋         | 19/300 [00:08<02:25,  1.93it/s]06/19/2022 14:47:11 - INFO - __main__ - global step: 1360; train loss: 7.5966668128967285; dev loss: 7.718218803405762
Epoch 9:   7%|▋         | 20/300 [00:09<02:21,  1.97it/s]Epoch 9:   7%|▋         | 21/300 [00:09<02:11,  2.12it/s]Epoch 9:   7%|▋         | 22/300 [00:10<01:59,  2.32it/s]Epoch 9:   8%|▊         | 23/300 [00:10<01:59,  2.33it/s]Epoch 9:   8%|▊         | 24/300 [00:10<01:50,  2.49it/s]Epoch 9:   8%|▊         | 25/300 [00:11<01:45,  2.60it/s]Epoch 9:   9%|▊         | 26/300 [00:11<01:41,  2.71it/s]Epoch 9:   9%|▉         | 27/300 [00:12<01:45,  2.58it/s]Epoch 9:   9%|▉         | 28/300 [00:12<01:42,  2.67it/s]Epoch 9:  10%|▉         | 29/300 [00:12<01:39,  2.73it/s]Epoch 9:  10%|█         | 30/300 [00:13<01:45,  2.56it/s]Epoch 9:  10%|█         | 31/300 [00:13<01:50,  2.42it/s]Epoch 9:  11%|█         | 32/300 [00:14<01:46,  2.52it/s]Epoch 9:  11%|█         | 33/300 [00:14<01:54,  2.34it/s]Epoch 9:  11%|█▏        | 34/300 [00:14<01:55,  2.30it/s]Epoch 9:  12%|█▏        | 35/300 [00:15<02:11,  2.01it/s]Epoch 9:  12%|█▏        | 36/300 [00:16<02:04,  2.12it/s]Epoch 9:  12%|█▏        | 37/300 [00:16<01:58,  2.23it/s]Epoch 9:  13%|█▎        | 38/300 [00:16<01:56,  2.24it/s]Epoch 9:  13%|█▎        | 39/300 [00:17<02:02,  2.14it/s]06/19/2022 14:47:19 - INFO - __main__ - global step: 1370; train loss: 7.355137825012207; dev loss: 7.388619422912598
Epoch 9:  13%|█▎        | 40/300 [00:17<01:57,  2.21it/s]Epoch 9:  14%|█▎        | 41/300 [00:18<01:50,  2.35it/s]Epoch 9:  14%|█▍        | 42/300 [00:18<01:49,  2.36it/s]Epoch 9:  14%|█▍        | 43/300 [00:19<01:56,  2.20it/s]Epoch 9:  15%|█▍        | 44/300 [00:19<02:08,  1.99it/s]Epoch 9:  15%|█▌        | 45/300 [00:20<02:05,  2.04it/s]Epoch 9:  15%|█▌        | 46/300 [00:20<02:03,  2.05it/s]Epoch 9:  16%|█▌        | 47/300 [00:21<02:02,  2.06it/s]Epoch 9:  16%|█▌        | 48/300 [00:21<02:00,  2.09it/s]Epoch 9:  16%|█▋        | 49/300 [00:21<01:53,  2.21it/s]Epoch 9:  17%|█▋        | 50/300 [00:22<01:48,  2.31it/s]Epoch 9:  17%|█▋        | 51/300 [00:22<01:50,  2.25it/s]Epoch 9:  17%|█▋        | 52/300 [00:23<02:01,  2.04it/s]Epoch 9:  18%|█▊        | 53/300 [00:23<02:03,  2.00it/s]Epoch 9:  18%|█▊        | 54/300 [00:24<01:58,  2.07it/s]Epoch 9:  18%|█▊        | 55/300 [00:24<01:56,  2.10it/s]Epoch 9:  19%|█▊        | 56/300 [00:25<02:04,  1.95it/s]Epoch 9:  19%|█▉        | 57/300 [00:25<01:59,  2.03it/s]Epoch 9:  19%|█▉        | 58/300 [00:26<01:49,  2.20it/s]Epoch 9:  20%|█▉        | 59/300 [00:26<01:42,  2.34it/s]06/19/2022 14:47:28 - INFO - __main__ - global step: 1380; train loss: 8.052343368530273; dev loss: 7.75017786026001
Epoch 9:  20%|██        | 60/300 [00:27<01:48,  2.21it/s]Epoch 9:  20%|██        | 61/300 [00:27<01:40,  2.37it/s]Epoch 9:  21%|██        | 62/300 [00:27<01:35,  2.49it/s]Epoch 9:  21%|██        | 63/300 [00:28<01:32,  2.57it/s]Epoch 9:  21%|██▏       | 64/300 [00:28<01:37,  2.42it/s]Epoch 9:  22%|██▏       | 65/300 [00:29<01:33,  2.53it/s]Epoch 9:  22%|██▏       | 66/300 [00:29<01:35,  2.45it/s]Epoch 9:  22%|██▏       | 67/300 [00:29<01:39,  2.35it/s]Epoch 9:  23%|██▎       | 68/300 [00:30<01:51,  2.08it/s]Epoch 9:  23%|██▎       | 69/300 [00:31<01:49,  2.12it/s]Epoch 9:  23%|██▎       | 70/300 [00:31<01:42,  2.24it/s]Epoch 9:  24%|██▎       | 71/300 [00:31<01:36,  2.38it/s]Epoch 9:  24%|██▍       | 72/300 [00:32<01:31,  2.50it/s]Epoch 9:  24%|██▍       | 73/300 [00:32<01:35,  2.37it/s]Epoch 9:  25%|██▍       | 74/300 [00:32<01:30,  2.49it/s]Epoch 9:  25%|██▌       | 75/300 [00:33<01:27,  2.58it/s]Epoch 9:  25%|██▌       | 76/300 [00:33<01:24,  2.64it/s]Epoch 9:  26%|██▌       | 77/300 [00:34<01:32,  2.40it/s]Epoch 9:  26%|██▌       | 78/300 [00:34<01:30,  2.45it/s]Epoch 9:  26%|██▋       | 79/300 [00:34<01:26,  2.55it/s]06/19/2022 14:47:36 - INFO - __main__ - global step: 1390; train loss: 7.745635986328125; dev loss: 7.788496971130371
Epoch 9:  27%|██▋       | 80/300 [00:35<01:23,  2.63it/s]Epoch 9:  27%|██▋       | 81/300 [00:35<01:28,  2.48it/s]Epoch 9:  27%|██▋       | 82/300 [00:36<01:24,  2.58it/s]Epoch 9:  28%|██▊       | 83/300 [00:36<01:21,  2.65it/s]Epoch 9:  28%|██▊       | 84/300 [00:36<01:20,  2.67it/s]Epoch 9:  28%|██▊       | 85/300 [00:37<01:26,  2.49it/s]Epoch 9:  29%|██▊       | 86/300 [00:37<01:22,  2.58it/s]Epoch 9:  29%|██▉       | 87/300 [00:37<01:22,  2.58it/s]Epoch 9:  29%|██▉       | 88/300 [00:38<01:19,  2.65it/s]Epoch 9:  30%|██▉       | 89/300 [00:38<01:25,  2.48it/s]Epoch 9:  30%|███       | 90/300 [00:39<01:26,  2.44it/s]Epoch 9:  30%|███       | 91/300 [00:39<01:37,  2.15it/s]Epoch 9:  31%|███       | 92/300 [00:40<01:41,  2.05it/s]Epoch 9:  31%|███       | 93/300 [00:40<01:47,  1.93it/s]Epoch 9:  31%|███▏      | 94/300 [00:41<01:43,  1.98it/s]Epoch 9:  32%|███▏      | 95/300 [00:41<01:46,  1.92it/s]Epoch 9:  32%|███▏      | 96/300 [00:42<01:38,  2.08it/s]Epoch 9:  32%|███▏      | 97/300 [00:42<01:36,  2.10it/s]Epoch 9:  33%|███▎      | 98/300 [00:43<01:37,  2.07it/s]Epoch 9:  33%|███▎      | 99/300 [00:43<01:31,  2.20it/s]06/19/2022 14:47:45 - INFO - __main__ - global step: 1400; train loss: 7.729438781738281; dev loss: 7.744356632232666
Epoch 9:  33%|███▎      | 100/300 [00:44<01:25,  2.33it/s]Epoch 9:  34%|███▎      | 101/300 [00:44<01:27,  2.27it/s]Epoch 9:  34%|███▍      | 102/300 [00:45<01:40,  1.97it/s]Epoch 9:  34%|███▍      | 103/300 [00:45<01:35,  2.06it/s]Epoch 9:  35%|███▍      | 104/300 [00:46<01:34,  2.08it/s]Epoch 9:  35%|███▌      | 105/300 [00:46<01:32,  2.11it/s]Epoch 9:  35%|███▌      | 106/300 [00:47<01:33,  2.08it/s]Epoch 9:  36%|███▌      | 107/300 [00:47<01:30,  2.14it/s]Epoch 9:  36%|███▌      | 108/300 [00:47<01:23,  2.30it/s]Epoch 9:  36%|███▋      | 109/300 [00:48<01:20,  2.38it/s]Epoch 9:  37%|███▋      | 110/300 [00:48<01:25,  2.22it/s]Epoch 9:  37%|███▋      | 111/300 [00:49<01:19,  2.37it/s]Epoch 9:  37%|███▋      | 112/300 [00:49<01:15,  2.49it/s]Epoch 9:  38%|███▊      | 113/300 [00:49<01:18,  2.39it/s]Epoch 9:  38%|███▊      | 114/300 [00:50<01:33,  1.99it/s]Epoch 9:  38%|███▊      | 115/300 [00:51<01:30,  2.05it/s]Epoch 9:  39%|███▊      | 116/300 [00:51<01:28,  2.08it/s]Epoch 9:  39%|███▉      | 117/300 [00:52<01:26,  2.11it/s]Epoch 9:  39%|███▉      | 118/300 [00:52<01:34,  1.93it/s]Epoch 9:  40%|███▉      | 119/300 [00:53<01:31,  1.99it/s]06/19/2022 14:47:55 - INFO - __main__ - global step: 1410; train loss: 7.499000549316406; dev loss: 7.43383264541626
Epoch 9:  40%|████      | 120/300 [00:53<01:24,  2.14it/s]Epoch 9:  40%|████      | 121/300 [00:53<01:18,  2.29it/s]Epoch 9:  41%|████      | 122/300 [00:54<01:21,  2.18it/s]Epoch 9:  41%|████      | 123/300 [00:54<01:19,  2.22it/s]Epoch 9:  41%|████▏     | 124/300 [00:55<01:20,  2.20it/s]Epoch 9:  42%|████▏     | 125/300 [00:55<01:16,  2.28it/s]Epoch 9:  42%|████▏     | 126/300 [00:56<01:14,  2.34it/s]Epoch 9:  42%|████▏     | 127/300 [00:56<01:21,  2.12it/s]Epoch 9:  43%|████▎     | 128/300 [00:57<01:21,  2.10it/s]Epoch 9:  43%|████▎     | 129/300 [00:57<01:20,  2.12it/s]Epoch 9:  43%|████▎     | 130/300 [00:58<01:21,  2.10it/s]Epoch 9:  44%|████▎     | 131/300 [00:58<01:25,  1.97it/s]Epoch 9:  44%|████▍     | 132/300 [00:59<01:23,  2.02it/s]Epoch 9:  44%|████▍     | 133/300 [00:59<01:21,  2.06it/s]Epoch 9:  45%|████▍     | 134/300 [01:00<01:19,  2.08it/s]Epoch 9:  45%|████▌     | 135/300 [01:00<01:18,  2.09it/s]Epoch 9:  45%|████▌     | 136/300 [01:00<01:13,  2.22it/s]Epoch 9:  46%|████▌     | 137/300 [01:01<01:11,  2.30it/s]Epoch 9:  46%|████▌     | 138/300 [01:01<01:08,  2.37it/s]Epoch 9:  46%|████▋     | 139/300 [01:02<01:09,  2.31it/s]06/19/2022 14:48:04 - INFO - __main__ - global step: 1420; train loss: 7.632662773132324; dev loss: 7.835616111755371
Epoch 9:  47%|████▋     | 140/300 [01:02<01:05,  2.45it/s]Epoch 9:  47%|████▋     | 141/300 [01:02<01:02,  2.55it/s]Epoch 9:  47%|████▋     | 142/300 [01:03<01:02,  2.54it/s]Epoch 9:  48%|████▊     | 143/300 [01:03<01:06,  2.35it/s]Epoch 9:  48%|████▊     | 144/300 [01:04<01:11,  2.18it/s]Epoch 9:  48%|████▊     | 145/300 [01:04<01:11,  2.18it/s]Epoch 9:  49%|████▊     | 146/300 [01:05<01:10,  2.19it/s]Epoch 9:  49%|████▉     | 147/300 [01:05<01:18,  1.95it/s]Epoch 9:  49%|████▉     | 148/300 [01:06<01:18,  1.94it/s]Epoch 9:  50%|████▉     | 149/300 [01:06<01:15,  2.01it/s]Epoch 9:  50%|█████     | 150/300 [01:07<01:12,  2.07it/s]Epoch 9:  50%|█████     | 151/300 [01:07<01:11,  2.08it/s]Epoch 9:  51%|█████     | 152/300 [01:08<01:18,  1.89it/s]Epoch 9:  51%|█████     | 153/300 [01:08<01:14,  1.96it/s]Epoch 9:  51%|█████▏    | 154/300 [01:09<01:10,  2.07it/s]Epoch 9:  52%|█████▏    | 155/300 [01:09<01:09,  2.08it/s]Epoch 9:  52%|█████▏    | 156/300 [01:10<01:14,  1.93it/s]Epoch 9:  52%|█████▏    | 157/300 [01:10<01:12,  1.98it/s]Epoch 9:  53%|█████▎    | 158/300 [01:11<01:05,  2.16it/s]Epoch 9:  53%|█████▎    | 159/300 [01:11<01:00,  2.32it/s]06/19/2022 14:48:13 - INFO - __main__ - global step: 1430; train loss: 7.84680700302124; dev loss: 7.955880165100098
Epoch 9:  53%|█████▎    | 160/300 [01:12<01:03,  2.21it/s]Epoch 9:  54%|█████▎    | 161/300 [01:12<01:00,  2.30it/s]Epoch 9:  54%|█████▍    | 162/300 [01:12<01:00,  2.27it/s]Epoch 9:  54%|█████▍    | 163/300 [01:13<01:01,  2.21it/s]Epoch 9:  55%|█████▍    | 164/300 [01:14<01:08,  1.98it/s]Epoch 9:  55%|█████▌    | 165/300 [01:14<01:05,  2.05it/s]Epoch 9:  55%|█████▌    | 166/300 [01:14<01:04,  2.08it/s]Epoch 9:  56%|█████▌    | 167/300 [01:15<01:03,  2.09it/s]Epoch 9:  56%|█████▌    | 168/300 [01:16<01:09,  1.90it/s]Epoch 9:  56%|█████▋    | 169/300 [01:16<01:06,  1.98it/s]Epoch 9:  57%|█████▋    | 170/300 [01:17<01:07,  1.93it/s]Epoch 9:  57%|█████▋    | 171/300 [01:17<01:07,  1.92it/s]Epoch 9:  57%|█████▋    | 172/300 [01:18<01:09,  1.84it/s]Epoch 9:  58%|█████▊    | 173/300 [01:18<01:06,  1.92it/s]Epoch 9:  58%|█████▊    | 174/300 [01:19<01:03,  1.98it/s]Epoch 9:  58%|█████▊    | 175/300 [01:19<01:01,  2.02it/s]Epoch 9:  59%|█████▊    | 176/300 [01:20<01:05,  1.89it/s]Epoch 9:  59%|█████▉    | 177/300 [01:20<01:06,  1.84it/s]Epoch 9:  59%|█████▉    | 178/300 [01:21<01:03,  1.92it/s]Epoch 9:  60%|█████▉    | 179/300 [01:21<01:01,  1.96it/s]06/19/2022 14:48:23 - INFO - __main__ - global step: 1440; train loss: 8.386993408203125; dev loss: 8.22082233428955
Epoch 9:  60%|██████    | 180/300 [01:22<00:56,  2.11it/s]Epoch 9:  60%|██████    | 181/300 [01:22<00:59,  1.99it/s]Epoch 9:  61%|██████    | 182/300 [01:23<00:59,  1.99it/s]Epoch 9:  61%|██████    | 183/300 [01:23<00:55,  2.09it/s]Epoch 9:  61%|██████▏   | 184/300 [01:23<00:51,  2.25it/s]Epoch 9:  62%|██████▏   | 185/300 [01:24<00:51,  2.23it/s]Epoch 9:  62%|██████▏   | 186/300 [01:24<00:48,  2.36it/s]Epoch 9:  62%|██████▏   | 187/300 [01:25<00:45,  2.48it/s]Epoch 9:  63%|██████▎   | 188/300 [01:25<00:47,  2.36it/s]Epoch 9:  63%|██████▎   | 189/300 [01:26<00:52,  2.13it/s]Epoch 9:  63%|██████▎   | 190/300 [01:26<00:48,  2.26it/s]Epoch 9:  64%|██████▎   | 191/300 [01:26<00:46,  2.36it/s]Epoch 9:  64%|██████▍   | 192/300 [01:27<00:43,  2.47it/s]Epoch 9:  64%|██████▍   | 193/300 [01:27<00:45,  2.37it/s]Epoch 9:  65%|██████▍   | 194/300 [01:28<00:42,  2.49it/s]Epoch 9:  65%|██████▌   | 195/300 [01:28<00:40,  2.58it/s]Epoch 9:  65%|██████▌   | 196/300 [01:28<00:40,  2.58it/s]Epoch 9:  66%|██████▌   | 197/300 [01:29<00:43,  2.36it/s]Epoch 9:  66%|██████▌   | 198/300 [01:29<00:44,  2.31it/s]Epoch 9:  66%|██████▋   | 199/300 [01:30<00:45,  2.24it/s]06/19/2022 14:48:32 - INFO - __main__ - global step: 1450; train loss: 7.824471473693848; dev loss: 7.355677127838135
Epoch 9:  67%|██████▋   | 200/300 [01:30<00:43,  2.32it/s]Epoch 9:  67%|██████▋   | 201/300 [01:31<00:44,  2.24it/s]Epoch 9:  67%|██████▋   | 202/300 [01:31<00:41,  2.37it/s]Epoch 9:  68%|██████▊   | 203/300 [01:31<00:41,  2.36it/s]Epoch 9:  68%|██████▊   | 204/300 [01:32<00:41,  2.29it/s]Epoch 9:  68%|██████▊   | 205/300 [01:32<00:43,  2.18it/s]Epoch 9:  69%|██████▊   | 206/300 [01:33<00:48,  1.93it/s]Epoch 9:  69%|██████▉   | 207/300 [01:34<00:48,  1.94it/s]Epoch 9:  69%|██████▉   | 208/300 [01:34<00:48,  1.91it/s]Epoch 9:  70%|██████▉   | 209/300 [01:35<00:47,  1.90it/s]Epoch 9:  70%|███████   | 210/300 [01:35<00:48,  1.87it/s]Epoch 9:  70%|███████   | 211/300 [01:36<00:44,  2.02it/s]Epoch 9:  71%|███████   | 212/300 [01:36<00:41,  2.10it/s]Epoch 9:  71%|███████   | 213/300 [01:37<00:41,  2.12it/s]Epoch 9:  71%|███████▏  | 214/300 [01:37<00:46,  1.87it/s]Epoch 9:  72%|███████▏  | 215/300 [01:38<00:43,  1.96it/s]Epoch 9:  72%|███████▏  | 216/300 [01:38<00:41,  2.03it/s]Epoch 9:  72%|███████▏  | 217/300 [01:39<00:39,  2.11it/s]Epoch 9:  73%|███████▎  | 218/300 [01:39<00:42,  1.94it/s]Epoch 9:  73%|███████▎  | 219/300 [01:40<00:42,  1.89it/s]06/19/2022 14:48:42 - INFO - __main__ - global step: 1460; train loss: 8.020278930664062; dev loss: 7.992057800292969
Epoch 9:  73%|███████▎  | 220/300 [01:40<00:40,  1.98it/s]Epoch 9:  74%|███████▎  | 221/300 [01:41<00:37,  2.11it/s]Epoch 9:  74%|███████▍  | 222/300 [01:41<00:38,  2.02it/s]Epoch 9:  74%|███████▍  | 223/300 [01:42<00:36,  2.13it/s]Epoch 9:  75%|███████▍  | 224/300 [01:42<00:35,  2.14it/s]Epoch 9:  75%|███████▌  | 225/300 [01:43<00:35,  2.12it/s]Epoch 9:  75%|███████▌  | 226/300 [01:43<00:37,  1.95it/s]Epoch 9:  76%|███████▌  | 227/300 [01:44<00:35,  2.08it/s]Epoch 9:  76%|███████▌  | 228/300 [01:44<00:33,  2.15it/s]Epoch 9:  76%|███████▋  | 229/300 [01:44<00:31,  2.23it/s]Epoch 9:  77%|███████▋  | 230/300 [01:45<00:32,  2.12it/s]Epoch 9:  77%|███████▋  | 231/300 [01:45<00:31,  2.21it/s]Epoch 9:  77%|███████▋  | 232/300 [01:46<00:30,  2.21it/s]Epoch 9:  78%|███████▊  | 233/300 [01:46<00:31,  2.14it/s]Epoch 9:  78%|███████▊  | 234/300 [01:47<00:31,  2.07it/s]Epoch 9:  78%|███████▊  | 235/300 [01:47<00:33,  1.93it/s]Epoch 9:  79%|███████▊  | 236/300 [01:48<00:31,  2.02it/s]Epoch 9:  79%|███████▉  | 237/300 [01:48<00:30,  2.05it/s]Epoch 9:  79%|███████▉  | 238/300 [01:49<00:30,  2.03it/s]Epoch 9:  80%|███████▉  | 239/300 [01:49<00:31,  1.92it/s]06/19/2022 14:48:51 - INFO - __main__ - global step: 1470; train loss: 7.924719333648682; dev loss: 7.975470542907715
Epoch 9:  80%|████████  | 240/300 [01:50<00:29,  2.06it/s]Epoch 9:  80%|████████  | 241/300 [01:50<00:27,  2.15it/s]Epoch 9:  81%|████████  | 242/300 [01:51<00:25,  2.27it/s]Epoch 9:  81%|████████  | 243/300 [01:51<00:25,  2.23it/s]Epoch 9:  81%|████████▏ | 244/300 [01:51<00:23,  2.38it/s]Epoch 9:  82%|████████▏ | 245/300 [01:52<00:24,  2.26it/s]Epoch 9:  82%|████████▏ | 246/300 [01:52<00:24,  2.21it/s]Epoch 9:  82%|████████▏ | 247/300 [01:53<00:27,  1.94it/s]Epoch 9:  83%|████████▎ | 248/300 [01:53<00:25,  2.00it/s]Epoch 9:  83%|████████▎ | 249/300 [01:54<00:24,  2.06it/s]Epoch 9:  83%|████████▎ | 250/300 [01:54<00:23,  2.09it/s]Epoch 9:  84%|████████▎ | 251/300 [01:55<00:25,  1.94it/s]Epoch 9:  84%|████████▍ | 252/300 [01:55<00:23,  2.06it/s]Epoch 9:  84%|████████▍ | 253/300 [01:56<00:21,  2.18it/s]Epoch 9:  85%|████████▍ | 254/300 [01:56<00:20,  2.20it/s]Epoch 9:  85%|████████▌ | 255/300 [01:57<00:21,  2.06it/s]Epoch 9:  85%|████████▌ | 256/300 [01:57<00:19,  2.24it/s]Epoch 9:  86%|████████▌ | 257/300 [01:58<00:18,  2.38it/s]Epoch 9:  86%|████████▌ | 258/300 [01:58<00:16,  2.49it/s]Epoch 9:  86%|████████▋ | 259/300 [01:58<00:16,  2.50it/s]06/19/2022 14:49:00 - INFO - __main__ - global step: 1480; train loss: 7.4525299072265625; dev loss: 7.5322747230529785
Epoch 9:  87%|████████▋ | 260/300 [01:59<00:17,  2.33it/s]Epoch 9:  87%|████████▋ | 261/300 [01:59<00:16,  2.43it/s]Epoch 9:  87%|████████▋ | 262/300 [02:00<00:15,  2.50it/s]Epoch 9:  88%|████████▊ | 263/300 [02:00<00:15,  2.36it/s]Epoch 9:  88%|████████▊ | 264/300 [02:01<00:17,  2.05it/s]Epoch 9:  88%|████████▊ | 265/300 [02:01<00:16,  2.07it/s]Epoch 9:  89%|████████▊ | 266/300 [02:02<00:16,  2.12it/s]Epoch 9:  89%|████████▉ | 267/300 [02:02<00:14,  2.24it/s]Epoch 9:  89%|████████▉ | 268/300 [02:02<00:14,  2.15it/s]Epoch 9:  90%|████████▉ | 269/300 [02:03<00:13,  2.25it/s]Epoch 9:  90%|█████████ | 270/300 [02:03<00:12,  2.35it/s]Epoch 9:  90%|█████████ | 271/300 [02:04<00:12,  2.30it/s]Epoch 9:  91%|█████████ | 272/300 [02:04<00:12,  2.17it/s]Epoch 9:  91%|█████████ | 273/300 [02:05<00:12,  2.16it/s]Epoch 9:  91%|█████████▏| 274/300 [02:05<00:11,  2.20it/s]Epoch 9:  92%|█████████▏| 275/300 [02:05<00:10,  2.35it/s]Epoch 9:  92%|█████████▏| 276/300 [02:06<00:11,  2.11it/s]Epoch 9:  92%|█████████▏| 277/300 [02:06<00:10,  2.23it/s]Epoch 9:  93%|█████████▎| 278/300 [02:07<00:09,  2.24it/s]Epoch 9:  93%|█████████▎| 279/300 [02:07<00:09,  2.27it/s]06/19/2022 14:49:09 - INFO - __main__ - global step: 1490; train loss: 7.774694919586182; dev loss: 7.764929294586182
Epoch 9:  93%|█████████▎| 280/300 [02:08<00:09,  2.18it/s]Epoch 9:  94%|█████████▎| 281/300 [02:08<00:08,  2.13it/s]Epoch 9:  94%|█████████▍| 282/300 [02:09<00:08,  2.13it/s]Epoch 9:  94%|█████████▍| 283/300 [02:09<00:07,  2.25it/s]Epoch 9:  95%|█████████▍| 284/300 [02:10<00:07,  2.00it/s]Epoch 9:  95%|█████████▌| 285/300 [02:10<00:07,  2.13it/s]Epoch 9:  95%|█████████▌| 286/300 [02:11<00:06,  2.18it/s]Epoch 9:  96%|█████████▌| 287/300 [02:11<00:05,  2.21it/s]Epoch 9:  96%|█████████▌| 288/300 [02:11<00:05,  2.30it/s]Epoch 9:  96%|█████████▋| 289/300 [02:12<00:04,  2.22it/s]Epoch 9:  97%|█████████▋| 290/300 [02:12<00:04,  2.36it/s]Epoch 9:  97%|█████████▋| 291/300 [02:13<00:03,  2.34it/s]Epoch 9:  97%|█████████▋| 292/300 [02:13<00:03,  2.44it/s]Epoch 9:  98%|█████████▊| 293/300 [02:14<00:03,  2.11it/s]Epoch 9:  98%|█████████▊| 294/300 [02:14<00:02,  2.08it/s]Epoch 9:  98%|█████████▊| 295/300 [02:15<00:02,  2.08it/s]Epoch 9:  99%|█████████▊| 296/300 [02:15<00:01,  2.14it/s]Epoch 9:  99%|█████████▉| 297/300 [02:16<00:01,  2.11it/s]Epoch 9:  99%|█████████▉| 298/300 [02:16<00:00,  2.27it/s]Epoch 9: 100%|█████████▉| 299/300 [02:16<00:00,  2.37it/s]06/19/2022 14:49:18 - INFO - __main__ - global step: 1500; train loss: 8.268811225891113; dev loss: 8.046041488647461
Epoch 9: 100%|██████████| 300/300 [02:17<00:00,  2.28it/s]Epoch 9: 100%|██████████| 300/300 [02:17<00:00,  2.18it/s]
Epoch 10:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 10:   0%|          | 1/300 [00:00<03:13,  1.55it/s]Epoch 10:   1%|          | 2/300 [00:01<02:46,  1.79it/s]Epoch 10:   1%|          | 3/300 [00:01<02:22,  2.09it/s]Epoch 10:   1%|▏         | 4/300 [00:01<02:09,  2.28it/s]Epoch 10:   2%|▏         | 5/300 [00:02<02:22,  2.07it/s]Epoch 10:   2%|▏         | 6/300 [00:02<02:13,  2.21it/s]Epoch 10:   2%|▏         | 7/300 [00:03<02:19,  2.10it/s]Epoch 10:   3%|▎         | 8/300 [00:03<02:16,  2.14it/s]Epoch 10:   3%|▎         | 9/300 [00:04<02:44,  1.77it/s]Epoch 10:   3%|▎         | 10/300 [00:05<02:31,  1.92it/s]Epoch 10:   4%|▎         | 11/300 [00:05<02:19,  2.07it/s]Epoch 10:   4%|▍         | 12/300 [00:05<02:11,  2.18it/s]Epoch 10:   4%|▍         | 13/300 [00:06<02:09,  2.22it/s]Epoch 10:   5%|▍         | 14/300 [00:06<02:13,  2.14it/s]Epoch 10:   5%|▌         | 15/300 [00:07<02:03,  2.30it/s]Epoch 10:   5%|▌         | 16/300 [00:07<01:57,  2.42it/s]Epoch 10:   6%|▌         | 17/300 [00:07<01:52,  2.52it/s]Epoch 10:   6%|▌         | 18/300 [00:08<01:57,  2.40it/s]Epoch 10:   6%|▋         | 19/300 [00:08<01:51,  2.52it/s]06/19/2022 14:49:27 - INFO - __main__ - global step: 1510; train loss: 7.981558322906494; dev loss: 7.847470760345459
Epoch 10:   7%|▋         | 20/300 [00:09<01:51,  2.52it/s]Epoch 10:   7%|▋         | 21/300 [00:09<01:48,  2.57it/s]Epoch 10:   7%|▋         | 22/300 [00:09<01:55,  2.40it/s]Epoch 10:   8%|▊         | 23/300 [00:10<01:51,  2.50it/s]Epoch 10:   8%|▊         | 24/300 [00:10<01:47,  2.57it/s]Epoch 10:   8%|▊         | 25/300 [00:11<01:45,  2.60it/s]Epoch 10:   9%|▊         | 26/300 [00:11<01:55,  2.37it/s]Epoch 10:   9%|▉         | 27/300 [00:11<01:52,  2.44it/s]Epoch 10:   9%|▉         | 28/300 [00:12<01:49,  2.49it/s]Epoch 10:  10%|▉         | 29/300 [00:12<01:47,  2.53it/s]Epoch 10:  10%|█         | 30/300 [00:13<01:56,  2.31it/s]Epoch 10:  10%|█         | 31/300 [00:13<02:04,  2.16it/s]Epoch 10:  11%|█         | 32/300 [00:14<02:01,  2.20it/s]Epoch 10:  11%|█         | 33/300 [00:14<01:55,  2.31it/s]Epoch 10:  11%|█▏        | 34/300 [00:15<02:06,  2.10it/s]Epoch 10:  12%|█▏        | 35/300 [00:15<01:57,  2.26it/s]Epoch 10:  12%|█▏        | 36/300 [00:15<01:49,  2.40it/s]Epoch 10:  12%|█▏        | 37/300 [00:16<01:52,  2.34it/s]Epoch 10:  13%|█▎        | 38/300 [00:16<02:07,  2.06it/s]Epoch 10:  13%|█▎        | 39/300 [00:17<02:07,  2.05it/s]06/19/2022 14:49:36 - INFO - __main__ - global step: 1520; train loss: 7.973366737365723; dev loss: 7.8492865562438965
Epoch 10:  13%|█▎        | 40/300 [00:17<02:02,  2.12it/s]Epoch 10:  14%|█▎        | 41/300 [00:18<01:58,  2.18it/s]Epoch 10:  14%|█▍        | 42/300 [00:18<01:55,  2.23it/s]Epoch 10:  14%|█▍        | 43/300 [00:19<01:57,  2.18it/s]Epoch 10:  15%|█▍        | 44/300 [00:19<01:50,  2.33it/s]Epoch 10:  15%|█▌        | 45/300 [00:19<01:44,  2.45it/s]Epoch 10:  15%|█▌        | 46/300 [00:20<01:43,  2.45it/s]Epoch 10:  16%|█▌        | 47/300 [00:20<01:49,  2.32it/s]Epoch 10:  16%|█▌        | 48/300 [00:21<01:47,  2.35it/s]Epoch 10:  16%|█▋        | 49/300 [00:21<01:42,  2.45it/s]Epoch 10:  17%|█▋        | 50/300 [00:21<01:41,  2.47it/s]Epoch 10:  17%|█▋        | 51/300 [00:22<01:47,  2.31it/s]Epoch 10:  17%|█▋        | 52/300 [00:22<01:44,  2.37it/s]Epoch 10:  18%|█▊        | 53/300 [00:23<01:40,  2.46it/s]Epoch 10:  18%|█▊        | 54/300 [00:23<01:40,  2.45it/s]Epoch 10:  18%|█▊        | 55/300 [00:24<01:45,  2.33it/s]Epoch 10:  19%|█▊        | 56/300 [00:24<01:39,  2.45it/s]Epoch 10:  19%|█▉        | 57/300 [00:24<01:37,  2.49it/s]Epoch 10:  19%|█▉        | 58/300 [00:25<01:46,  2.28it/s]Epoch 10:  20%|█▉        | 59/300 [00:25<01:49,  2.19it/s]06/19/2022 14:49:45 - INFO - __main__ - global step: 1530; train loss: 7.5223870277404785; dev loss: 7.409358978271484
Epoch 10:  20%|██        | 60/300 [00:26<01:42,  2.35it/s]Epoch 10:  20%|██        | 61/300 [00:26<01:36,  2.48it/s]Epoch 10:  21%|██        | 62/300 [00:26<01:32,  2.56it/s]Epoch 10:  21%|██        | 63/300 [00:27<01:38,  2.40it/s]Epoch 10:  21%|██▏       | 64/300 [00:27<01:37,  2.43it/s]Epoch 10:  22%|██▏       | 65/300 [00:28<01:32,  2.54it/s]Epoch 10:  22%|██▏       | 66/300 [00:28<01:32,  2.53it/s]Epoch 10:  22%|██▏       | 67/300 [00:28<01:29,  2.60it/s]Epoch 10:  23%|██▎       | 68/300 [00:29<01:40,  2.31it/s]Epoch 10:  23%|██▎       | 69/300 [00:29<01:35,  2.42it/s]Epoch 10:  23%|██▎       | 70/300 [00:30<01:30,  2.53it/s]Epoch 10:  24%|██▎       | 71/300 [00:30<01:29,  2.55it/s]Epoch 10:  24%|██▍       | 72/300 [00:31<01:36,  2.36it/s]Epoch 10:  24%|██▍       | 73/300 [00:31<01:33,  2.43it/s]Epoch 10:  25%|██▍       | 74/300 [00:31<01:30,  2.50it/s]Epoch 10:  25%|██▌       | 75/300 [00:32<01:28,  2.55it/s]Epoch 10:  25%|██▌       | 76/300 [00:32<01:32,  2.43it/s]Epoch 10:  26%|██▌       | 77/300 [00:33<01:30,  2.47it/s]Epoch 10:  26%|██▌       | 78/300 [00:33<01:27,  2.54it/s]Epoch 10:  26%|██▋       | 79/300 [00:33<01:25,  2.58it/s]06/19/2022 14:49:53 - INFO - __main__ - global step: 1540; train loss: 8.060606002807617; dev loss: 8.186026573181152
Epoch 10:  27%|██▋       | 80/300 [00:34<01:40,  2.18it/s]Epoch 10:  27%|██▋       | 81/300 [00:34<01:41,  2.17it/s]Epoch 10:  27%|██▋       | 82/300 [00:35<01:40,  2.18it/s]Epoch 10:  28%|██▊       | 83/300 [00:35<01:34,  2.30it/s]Epoch 10:  28%|██▊       | 84/300 [00:36<01:35,  2.25it/s]Epoch 10:  28%|██▊       | 85/300 [00:36<01:30,  2.38it/s]Epoch 10:  29%|██▊       | 86/300 [00:36<01:27,  2.46it/s]Epoch 10:  29%|██▉       | 87/300 [00:37<01:23,  2.55it/s]Epoch 10:  29%|██▉       | 88/300 [00:37<01:29,  2.37it/s]Epoch 10:  30%|██▉       | 89/300 [00:38<01:25,  2.47it/s]Epoch 10:  30%|███       | 90/300 [00:38<01:28,  2.38it/s]Epoch 10:  30%|███       | 91/300 [00:39<01:31,  2.28it/s]Epoch 10:  31%|███       | 92/300 [00:39<01:45,  1.97it/s]Epoch 10:  31%|███       | 93/300 [00:40<01:37,  2.13it/s]Epoch 10:  31%|███▏      | 94/300 [00:40<01:31,  2.24it/s]Epoch 10:  32%|███▏      | 95/300 [00:40<01:29,  2.29it/s]Epoch 10:  32%|███▏      | 96/300 [00:41<01:28,  2.32it/s]Epoch 10:  32%|███▏      | 97/300 [00:41<01:33,  2.18it/s]Epoch 10:  33%|███▎      | 98/300 [00:42<01:35,  2.11it/s]Epoch 10:  33%|███▎      | 99/300 [00:42<01:33,  2.15it/s]06/19/2022 14:50:02 - INFO - __main__ - global step: 1550; train loss: 7.893481254577637; dev loss: 7.762560844421387
Epoch 10:  33%|███▎      | 100/300 [00:43<01:31,  2.20it/s]Epoch 10:  34%|███▎      | 101/300 [00:43<01:39,  1.99it/s]Epoch 10:  34%|███▍      | 102/300 [00:44<01:36,  2.05it/s]Epoch 10:  34%|███▍      | 103/300 [00:44<01:35,  2.07it/s]Epoch 10:  35%|███▍      | 104/300 [00:45<01:38,  2.00it/s]Epoch 10:  35%|███▌      | 105/300 [00:45<01:44,  1.87it/s]Epoch 10:  35%|███▌      | 106/300 [00:46<01:41,  1.91it/s]Epoch 10:  36%|███▌      | 107/300 [00:46<01:38,  1.96it/s]Epoch 10:  36%|███▌      | 108/300 [00:47<01:29,  2.15it/s]Epoch 10:  36%|███▋      | 109/300 [00:47<01:39,  1.93it/s]Epoch 10:  37%|███▋      | 110/300 [00:48<01:37,  1.95it/s]Epoch 10:  37%|███▋      | 111/300 [00:48<01:34,  2.01it/s]Epoch 10:  37%|███▋      | 112/300 [00:49<01:32,  2.03it/s]Epoch 10:  38%|███▊      | 113/300 [00:49<01:38,  1.90it/s]Epoch 10:  38%|███▊      | 114/300 [00:50<01:33,  2.00it/s]Epoch 10:  38%|███▊      | 115/300 [00:50<01:30,  2.04it/s]Epoch 10:  39%|███▊      | 116/300 [00:51<01:25,  2.16it/s]Epoch 10:  39%|███▉      | 117/300 [00:51<01:26,  2.11it/s]Epoch 10:  39%|███▉      | 118/300 [00:52<01:21,  2.23it/s]Epoch 10:  40%|███▉      | 119/300 [00:52<01:17,  2.32it/s]06/19/2022 14:50:11 - INFO - __main__ - global step: 1560; train loss: 8.18083381652832; dev loss: 8.496053695678711
Epoch 10:  40%|████      | 120/300 [00:52<01:14,  2.40it/s]Epoch 10:  40%|████      | 121/300 [00:53<01:13,  2.44it/s]Epoch 10:  41%|████      | 122/300 [00:53<01:20,  2.21it/s]Epoch 10:  41%|████      | 123/300 [00:54<01:21,  2.17it/s]Epoch 10:  41%|████▏     | 124/300 [00:54<01:21,  2.16it/s]Epoch 10:  42%|████▏     | 125/300 [00:55<01:21,  2.14it/s]Epoch 10:  42%|████▏     | 126/300 [00:55<01:27,  1.98it/s]Epoch 10:  42%|████▏     | 127/300 [00:56<01:20,  2.16it/s]Epoch 10:  43%|████▎     | 128/300 [00:56<01:13,  2.33it/s]Epoch 10:  43%|████▎     | 129/300 [00:56<01:09,  2.45it/s]Epoch 10:  43%|████▎     | 130/300 [00:57<01:11,  2.37it/s]Epoch 10:  44%|████▎     | 131/300 [00:57<01:13,  2.30it/s]Epoch 10:  44%|████▍     | 132/300 [00:58<01:15,  2.23it/s]Epoch 10:  44%|████▍     | 133/300 [00:58<01:16,  2.19it/s]Epoch 10:  45%|████▍     | 134/300 [00:59<01:22,  2.01it/s]Epoch 10:  45%|████▌     | 135/300 [00:59<01:15,  2.18it/s]Epoch 10:  45%|████▌     | 136/300 [01:00<01:10,  2.33it/s]Epoch 10:  46%|████▌     | 137/300 [01:00<01:06,  2.46it/s]Epoch 10:  46%|████▌     | 138/300 [01:01<01:08,  2.35it/s]Epoch 10:  46%|████▋     | 139/300 [01:01<01:09,  2.32it/s]06/19/2022 14:50:20 - INFO - __main__ - global step: 1570; train loss: 7.099869728088379; dev loss: 7.506497383117676
Epoch 10:  47%|████▋     | 140/300 [01:01<01:13,  2.17it/s]Epoch 10:  47%|████▋     | 141/300 [01:02<01:15,  2.10it/s]Epoch 10:  47%|████▋     | 142/300 [01:03<01:20,  1.95it/s]Epoch 10:  48%|████▊     | 143/300 [01:03<01:13,  2.15it/s]Epoch 10:  48%|████▊     | 144/300 [01:03<01:07,  2.31it/s]Epoch 10:  48%|████▊     | 145/300 [01:04<01:05,  2.36it/s]Epoch 10:  49%|████▊     | 146/300 [01:04<01:09,  2.21it/s]Epoch 10:  49%|████▉     | 147/300 [01:05<01:04,  2.39it/s]Epoch 10:  49%|████▉     | 148/300 [01:05<01:02,  2.41it/s]Epoch 10:  50%|████▉     | 149/300 [01:05<01:00,  2.51it/s]Epoch 10:  50%|█████     | 150/300 [01:06<00:57,  2.62it/s]Epoch 10:  50%|█████     | 151/300 [01:06<00:59,  2.49it/s]Epoch 10:  51%|█████     | 152/300 [01:06<00:56,  2.61it/s]Epoch 10:  51%|█████     | 153/300 [01:07<00:55,  2.67it/s]Epoch 10:  51%|█████▏    | 154/300 [01:07<00:53,  2.74it/s]Epoch 10:  52%|█████▏    | 155/300 [01:08<00:56,  2.58it/s]Epoch 10:  52%|█████▏    | 156/300 [01:08<00:58,  2.45it/s]Epoch 10:  52%|█████▏    | 157/300 [01:09<01:00,  2.38it/s]Epoch 10:  53%|█████▎    | 158/300 [01:09<01:02,  2.28it/s]Epoch 10:  53%|█████▎    | 159/300 [01:10<01:06,  2.11it/s]06/19/2022 14:50:29 - INFO - __main__ - global step: 1580; train loss: 7.785961151123047; dev loss: 7.7560715675354
Epoch 10:  53%|█████▎    | 160/300 [01:10<01:01,  2.29it/s]Epoch 10:  54%|█████▎    | 161/300 [01:10<00:56,  2.45it/s]Epoch 10:  54%|█████▍    | 162/300 [01:11<00:53,  2.57it/s]Epoch 10:  54%|█████▍    | 163/300 [01:11<00:55,  2.45it/s]Epoch 10:  55%|█████▍    | 164/300 [01:11<00:52,  2.58it/s]Epoch 10:  55%|█████▌    | 165/300 [01:12<00:50,  2.67it/s]Epoch 10:  55%|█████▌    | 166/300 [01:12<00:48,  2.75it/s]Epoch 10:  56%|█████▌    | 167/300 [01:12<00:50,  2.61it/s]Epoch 10:  56%|█████▌    | 168/300 [01:13<00:52,  2.52it/s]Epoch 10:  56%|█████▋    | 169/300 [01:14<01:00,  2.18it/s]Epoch 10:  57%|█████▋    | 170/300 [01:14<01:00,  2.15it/s]Epoch 10:  57%|█████▋    | 171/300 [01:15<01:05,  1.97it/s]Epoch 10:  57%|█████▋    | 172/300 [01:15<01:04,  1.99it/s]Epoch 10:  58%|█████▊    | 173/300 [01:16<01:04,  1.98it/s]Epoch 10:  58%|█████▊    | 174/300 [01:16<01:02,  2.02it/s]Epoch 10:  58%|█████▊    | 175/300 [01:17<00:59,  2.09it/s]Epoch 10:  59%|█████▊    | 176/300 [01:17<00:58,  2.10it/s]Epoch 10:  59%|█████▉    | 177/300 [01:17<00:53,  2.28it/s]Epoch 10:  59%|█████▉    | 178/300 [01:18<00:50,  2.41it/s]Epoch 10:  60%|█████▉    | 179/300 [01:18<00:48,  2.51it/s]06/19/2022 14:50:37 - INFO - __main__ - global step: 1590; train loss: 7.282464027404785; dev loss: 7.262993812561035
Epoch 10:  60%|██████    | 180/300 [01:19<00:50,  2.38it/s]Epoch 10:  60%|██████    | 181/300 [01:19<00:49,  2.40it/s]Epoch 10:  61%|██████    | 182/300 [01:19<00:47,  2.49it/s]Epoch 10:  61%|██████    | 183/300 [01:20<00:45,  2.57it/s]Epoch 10:  61%|██████▏   | 184/300 [01:20<00:51,  2.23it/s]Epoch 10:  62%|██████▏   | 185/300 [01:21<00:48,  2.36it/s]Epoch 10:  62%|██████▏   | 186/300 [01:21<00:49,  2.32it/s]Epoch 10:  62%|██████▏   | 187/300 [01:21<00:47,  2.38it/s]Epoch 10:  63%|██████▎   | 188/300 [01:22<00:49,  2.24it/s]Epoch 10:  63%|██████▎   | 189/300 [01:22<00:48,  2.31it/s]Epoch 10:  63%|██████▎   | 190/300 [01:23<00:46,  2.36it/s]Epoch 10:  64%|██████▎   | 191/300 [01:23<00:45,  2.39it/s]Epoch 10:  64%|██████▍   | 192/300 [01:24<00:50,  2.14it/s]Epoch 10:  64%|██████▍   | 193/300 [01:24<00:48,  2.21it/s]Epoch 10:  65%|██████▍   | 194/300 [01:25<00:52,  2.03it/s]Epoch 10:  65%|██████▌   | 195/300 [01:25<00:50,  2.08it/s]Epoch 10:  65%|██████▌   | 196/300 [01:26<00:52,  1.98it/s]Epoch 10:  66%|██████▌   | 197/300 [01:26<00:50,  2.03it/s]Epoch 10:  66%|██████▌   | 198/300 [01:27<00:50,  2.02it/s]Epoch 10:  66%|██████▋   | 199/300 [01:27<00:46,  2.17it/s]06/19/2022 14:50:47 - INFO - __main__ - global step: 1600; train loss: 8.088549613952637; dev loss: 7.934889316558838
Epoch 10:  67%|██████▋   | 200/300 [01:28<00:47,  2.12it/s]Epoch 10:  67%|██████▋   | 201/300 [01:28<00:47,  2.09it/s]Epoch 10:  67%|██████▋   | 202/300 [01:29<00:46,  2.10it/s]Epoch 10:  68%|██████▊   | 203/300 [01:29<00:45,  2.13it/s]Epoch 10:  68%|██████▊   | 204/300 [01:29<00:41,  2.30it/s]Epoch 10:  68%|██████▊   | 205/300 [01:30<00:44,  2.13it/s]Epoch 10:  69%|██████▊   | 206/300 [01:30<00:40,  2.31it/s]Epoch 10:  69%|██████▉   | 207/300 [01:31<00:37,  2.46it/s]Epoch 10:  69%|██████▉   | 208/300 [01:31<00:36,  2.50it/s]Epoch 10:  70%|██████▉   | 209/300 [01:32<00:39,  2.29it/s]Epoch 10:  70%|███████   | 210/300 [01:32<00:37,  2.39it/s]Epoch 10:  70%|███████   | 211/300 [01:32<00:36,  2.45it/s]Epoch 10:  71%|███████   | 212/300 [01:33<00:34,  2.57it/s]Epoch 10:  71%|███████   | 213/300 [01:33<00:34,  2.49it/s]Epoch 10:  71%|███████▏  | 214/300 [01:33<00:33,  2.59it/s]Epoch 10:  72%|███████▏  | 215/300 [01:34<00:36,  2.34it/s]Epoch 10:  72%|███████▏  | 216/300 [01:34<00:38,  2.17it/s]Epoch 10:  72%|███████▏  | 217/300 [01:35<00:42,  1.94it/s]Epoch 10:  73%|███████▎  | 218/300 [01:36<00:38,  2.13it/s]Epoch 10:  73%|███████▎  | 219/300 [01:36<00:36,  2.21it/s]06/19/2022 14:50:55 - INFO - __main__ - global step: 1610; train loss: 7.613319396972656; dev loss: 7.062714576721191
Epoch 10:  73%|███████▎  | 220/300 [01:36<00:36,  2.17it/s]Epoch 10:  74%|███████▎  | 221/300 [01:37<00:37,  2.11it/s]Epoch 10:  74%|███████▍  | 222/300 [01:37<00:36,  2.15it/s]Epoch 10:  74%|███████▍  | 223/300 [01:38<00:36,  2.14it/s]Epoch 10:  75%|███████▍  | 224/300 [01:38<00:35,  2.14it/s]Epoch 10:  75%|███████▌  | 225/300 [01:39<00:37,  1.99it/s]Epoch 10:  75%|███████▌  | 226/300 [01:39<00:36,  2.05it/s]Epoch 10:  76%|███████▌  | 227/300 [01:40<00:32,  2.22it/s]Epoch 10:  76%|███████▌  | 228/300 [01:40<00:31,  2.30it/s]Epoch 10:  76%|███████▋  | 229/300 [01:40<00:29,  2.44it/s]Epoch 10:  77%|███████▋  | 230/300 [01:41<00:31,  2.23it/s]Epoch 10:  77%|███████▋  | 231/300 [01:41<00:30,  2.26it/s]Epoch 10:  77%|███████▋  | 232/300 [01:42<00:28,  2.38it/s]Epoch 10:  78%|███████▊  | 233/300 [01:42<00:27,  2.44it/s]Epoch 10:  78%|███████▊  | 234/300 [01:43<00:28,  2.34it/s]Epoch 10:  78%|███████▊  | 235/300 [01:43<00:26,  2.48it/s]Epoch 10:  79%|███████▊  | 236/300 [01:43<00:25,  2.51it/s]Epoch 10:  79%|███████▉  | 237/300 [01:44<00:25,  2.45it/s]Epoch 10:  79%|███████▉  | 238/300 [01:44<00:26,  2.36it/s]Epoch 10:  80%|███████▉  | 239/300 [01:45<00:25,  2.35it/s]06/19/2022 14:51:04 - INFO - __main__ - global step: 1620; train loss: 7.4316887855529785; dev loss: 7.720785617828369
Epoch 10:  80%|████████  | 240/300 [01:45<00:24,  2.47it/s]Epoch 10:  80%|████████  | 241/300 [01:45<00:24,  2.43it/s]Epoch 10:  81%|████████  | 242/300 [01:46<00:26,  2.22it/s]Epoch 10:  81%|████████  | 243/300 [01:46<00:24,  2.28it/s]Epoch 10:  81%|████████▏ | 244/300 [01:47<00:23,  2.38it/s]Epoch 10:  82%|████████▏ | 245/300 [01:47<00:23,  2.35it/s]Epoch 10:  82%|████████▏ | 246/300 [01:48<00:25,  2.10it/s]Epoch 10:  82%|████████▏ | 247/300 [01:48<00:24,  2.13it/s]Epoch 10:  83%|████████▎ | 248/300 [01:49<00:22,  2.29it/s]Epoch 10:  83%|████████▎ | 249/300 [01:49<00:23,  2.18it/s]Epoch 10:  83%|████████▎ | 250/300 [01:50<00:23,  2.09it/s]Epoch 10:  84%|████████▎ | 251/300 [01:50<00:22,  2.19it/s]Epoch 10:  84%|████████▍ | 252/300 [01:51<00:22,  2.18it/s]Epoch 10:  84%|████████▍ | 253/300 [01:51<00:21,  2.18it/s]Epoch 10:  85%|████████▍ | 254/300 [01:52<00:21,  2.12it/s]Epoch 10:  85%|████████▌ | 255/300 [01:52<00:19,  2.26it/s]Epoch 10:  85%|████████▌ | 256/300 [01:52<00:18,  2.37it/s]Epoch 10:  86%|████████▌ | 257/300 [01:53<00:17,  2.41it/s]Epoch 10:  86%|████████▌ | 258/300 [01:53<00:17,  2.35it/s]Epoch 10:  86%|████████▋ | 259/300 [01:54<00:19,  2.08it/s]06/19/2022 14:51:13 - INFO - __main__ - global step: 1630; train loss: 8.152036666870117; dev loss: 7.635157108306885
Epoch 10:  87%|████████▋ | 260/300 [01:54<00:18,  2.11it/s]Epoch 10:  87%|████████▋ | 261/300 [01:55<00:18,  2.16it/s]Epoch 10:  87%|████████▋ | 262/300 [01:55<00:16,  2.31it/s]Epoch 10:  88%|████████▊ | 263/300 [01:55<00:16,  2.26it/s]Epoch 10:  88%|████████▊ | 264/300 [01:56<00:15,  2.39it/s]Epoch 10:  88%|████████▊ | 265/300 [01:56<00:14,  2.47it/s]Epoch 10:  89%|████████▊ | 266/300 [01:57<00:13,  2.54it/s]Epoch 10:  89%|████████▉ | 267/300 [01:57<00:13,  2.38it/s]Epoch 10:  89%|████████▉ | 268/300 [01:57<00:12,  2.49it/s]Epoch 10:  90%|████████▉ | 269/300 [01:58<00:12,  2.52it/s]Epoch 10:  90%|█████████ | 270/300 [01:58<00:11,  2.54it/s]Epoch 10:  90%|█████████ | 271/300 [01:59<00:13,  2.16it/s]Epoch 10:  91%|█████████ | 272/300 [01:59<00:12,  2.27it/s]Epoch 10:  91%|█████████ | 273/300 [02:00<00:11,  2.37it/s]Epoch 10:  91%|█████████▏| 274/300 [02:00<00:10,  2.44it/s]Epoch 10:  92%|█████████▏| 275/300 [02:00<00:10,  2.28it/s]Epoch 10:  92%|█████████▏| 276/300 [02:01<00:10,  2.21it/s]Epoch 10:  92%|█████████▏| 277/300 [02:01<00:10,  2.18it/s]Epoch 10:  93%|█████████▎| 278/300 [02:02<00:09,  2.22it/s]Epoch 10:  93%|█████████▎| 279/300 [02:02<00:09,  2.19it/s]06/19/2022 14:51:22 - INFO - __main__ - global step: 1640; train loss: 8.516912460327148; dev loss: 8.232146263122559
Epoch 10:  93%|█████████▎| 280/300 [02:03<00:08,  2.29it/s]Epoch 10:  94%|█████████▎| 281/300 [02:03<00:08,  2.37it/s]Epoch 10:  94%|█████████▍| 282/300 [02:03<00:07,  2.37it/s]Epoch 10:  94%|█████████▍| 283/300 [02:04<00:07,  2.27it/s]Epoch 10:  95%|█████████▍| 284/300 [02:05<00:07,  2.04it/s]Epoch 10:  95%|█████████▌| 285/300 [02:05<00:07,  2.03it/s]Epoch 10:  95%|█████████▌| 286/300 [02:05<00:06,  2.14it/s]Epoch 10:  96%|█████████▌| 287/300 [02:06<00:05,  2.24it/s]Epoch 10:  96%|█████████▌| 288/300 [02:06<00:05,  2.22it/s]Epoch 10:  96%|█████████▋| 289/300 [02:07<00:04,  2.38it/s]Epoch 10:  97%|█████████▋| 290/300 [02:07<00:04,  2.36it/s]Epoch 10:  97%|█████████▋| 291/300 [02:08<00:03,  2.38it/s]Epoch 10:  97%|█████████▋| 292/300 [02:08<00:03,  2.10it/s]Epoch 10:  98%|█████████▊| 293/300 [02:09<00:03,  2.15it/s]Epoch 10:  98%|█████████▊| 294/300 [02:09<00:02,  2.23it/s]Epoch 10:  98%|█████████▊| 295/300 [02:09<00:02,  2.31it/s]Epoch 10:  99%|█████████▊| 296/300 [02:10<00:01,  2.20it/s]Epoch 10:  99%|█████████▉| 297/300 [02:10<00:01,  2.31it/s]Epoch 10:  99%|█████████▉| 298/300 [02:11<00:00,  2.25it/s]Epoch 10: 100%|█████████▉| 299/300 [02:11<00:00,  2.21it/s]06/19/2022 14:51:31 - INFO - __main__ - global step: 1650; train loss: 7.732283115386963; dev loss: 7.473886966705322
Epoch 10: 100%|██████████| 300/300 [02:12<00:00,  2.17it/s]Epoch 10: 100%|██████████| 300/300 [02:12<00:00,  2.27it/s]
Epoch 11:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 11:   0%|          | 1/300 [00:00<02:29,  2.00it/s]Epoch 11:   1%|          | 2/300 [00:00<02:18,  2.15it/s]Epoch 11:   1%|          | 3/300 [00:01<02:22,  2.09it/s]Epoch 11:   1%|▏         | 4/300 [00:02<02:33,  1.93it/s]Epoch 11:   2%|▏         | 5/300 [00:02<02:29,  1.97it/s]Epoch 11:   2%|▏         | 6/300 [00:02<02:22,  2.07it/s]Epoch 11:   2%|▏         | 7/300 [00:03<02:19,  2.10it/s]Epoch 11:   3%|▎         | 8/300 [00:04<02:30,  1.93it/s]Epoch 11:   3%|▎         | 9/300 [00:04<02:25,  2.00it/s]Epoch 11:   3%|▎         | 10/300 [00:05<02:35,  1.87it/s]Epoch 11:   4%|▎         | 11/300 [00:05<02:22,  2.02it/s]Epoch 11:   4%|▍         | 12/300 [00:05<02:16,  2.11it/s]Epoch 11:   4%|▍         | 13/300 [00:06<02:12,  2.16it/s]Epoch 11:   5%|▍         | 14/300 [00:06<02:05,  2.28it/s]Epoch 11:   5%|▌         | 15/300 [00:07<01:58,  2.40it/s]Epoch 11:   5%|▌         | 16/300 [00:07<01:51,  2.54it/s]Epoch 11:   6%|▌         | 17/300 [00:07<02:04,  2.27it/s]Epoch 11:   6%|▌         | 18/300 [00:08<02:00,  2.34it/s]Epoch 11:   6%|▋         | 19/300 [00:08<02:01,  2.30it/s]06/19/2022 14:51:40 - INFO - __main__ - global step: 1660; train loss: 7.813374996185303; dev loss: 7.78423547744751
Epoch 11:   7%|▋         | 20/300 [00:09<02:02,  2.28it/s]Epoch 11:   7%|▋         | 21/300 [00:09<02:02,  2.28it/s]Epoch 11:   7%|▋         | 22/300 [00:10<01:56,  2.39it/s]Epoch 11:   8%|▊         | 23/300 [00:10<01:51,  2.48it/s]Epoch 11:   8%|▊         | 24/300 [00:10<01:46,  2.58it/s]Epoch 11:   8%|▊         | 25/300 [00:11<01:51,  2.46it/s]Epoch 11:   9%|▊         | 26/300 [00:11<01:54,  2.40it/s]Epoch 11:   9%|▉         | 27/300 [00:12<01:54,  2.38it/s]Epoch 11:   9%|▉         | 28/300 [00:12<01:54,  2.38it/s]Epoch 11:  10%|▉         | 29/300 [00:13<01:58,  2.28it/s]Epoch 11:  10%|█         | 30/300 [00:13<01:52,  2.40it/s]Epoch 11:  10%|█         | 31/300 [00:13<01:48,  2.47it/s]Epoch 11:  11%|█         | 32/300 [00:14<01:51,  2.41it/s]Epoch 11:  11%|█         | 33/300 [00:14<01:54,  2.33it/s]Epoch 11:  11%|█▏        | 34/300 [00:15<01:54,  2.32it/s]Epoch 11:  12%|█▏        | 35/300 [00:15<01:56,  2.27it/s]Epoch 11:  12%|█▏        | 36/300 [00:15<01:51,  2.36it/s]Epoch 11:  12%|█▏        | 37/300 [00:16<01:47,  2.44it/s]Epoch 11:  13%|█▎        | 38/300 [00:16<01:50,  2.38it/s]Epoch 11:  13%|█▎        | 39/300 [00:17<01:43,  2.53it/s]06/19/2022 14:51:48 - INFO - __main__ - global step: 1670; train loss: 7.9805803298950195; dev loss: 8.072229385375977
Epoch 11:  13%|█▎        | 40/300 [00:17<01:38,  2.65it/s]Epoch 11:  14%|█▎        | 41/300 [00:17<01:34,  2.73it/s]Epoch 11:  14%|█▍        | 42/300 [00:18<01:41,  2.53it/s]Epoch 11:  14%|█▍        | 43/300 [00:18<01:39,  2.57it/s]Epoch 11:  15%|█▍        | 44/300 [00:19<01:41,  2.51it/s]Epoch 11:  15%|█▌        | 45/300 [00:19<01:49,  2.33it/s]Epoch 11:  15%|█▌        | 46/300 [00:20<01:54,  2.21it/s]Epoch 11:  16%|█▌        | 47/300 [00:20<01:57,  2.15it/s]Epoch 11:  16%|█▌        | 48/300 [00:20<01:56,  2.17it/s]Epoch 11:  16%|█▋        | 49/300 [00:21<01:55,  2.17it/s]Epoch 11:  17%|█▋        | 50/300 [00:21<02:01,  2.07it/s]Epoch 11:  17%|█▋        | 51/300 [00:22<01:56,  2.13it/s]Epoch 11:  17%|█▋        | 52/300 [00:22<01:47,  2.31it/s]Epoch 11:  18%|█▊        | 53/300 [00:23<01:42,  2.41it/s]Epoch 11:  18%|█▊        | 54/300 [00:23<01:44,  2.36it/s]Epoch 11:  18%|█▊        | 55/300 [00:24<01:43,  2.37it/s]Epoch 11:  19%|█▊        | 56/300 [00:24<01:43,  2.36it/s]Epoch 11:  19%|█▉        | 57/300 [00:24<01:37,  2.50it/s]Epoch 11:  19%|█▉        | 58/300 [00:25<01:45,  2.30it/s]Epoch 11:  20%|█▉        | 59/300 [00:25<01:44,  2.31it/s]06/19/2022 14:51:57 - INFO - __main__ - global step: 1680; train loss: 7.712815761566162; dev loss: 7.663374423980713
Epoch 11:  20%|██        | 60/300 [00:26<01:39,  2.40it/s]Epoch 11:  20%|██        | 61/300 [00:26<01:47,  2.22it/s]Epoch 11:  21%|██        | 62/300 [00:27<01:56,  2.05it/s]Epoch 11:  21%|██        | 63/300 [00:27<02:00,  1.97it/s]Epoch 11:  21%|██▏       | 64/300 [00:28<01:52,  2.09it/s]Epoch 11:  22%|██▏       | 65/300 [00:28<01:50,  2.12it/s]Epoch 11:  22%|██▏       | 66/300 [00:29<01:48,  2.15it/s]Epoch 11:  22%|██▏       | 67/300 [00:29<01:49,  2.13it/s]Epoch 11:  23%|██▎       | 68/300 [00:29<01:46,  2.18it/s]Epoch 11:  23%|██▎       | 69/300 [00:30<01:41,  2.29it/s]Epoch 11:  23%|██▎       | 70/300 [00:30<01:38,  2.34it/s]Epoch 11:  24%|██▎       | 71/300 [00:31<01:41,  2.26it/s]Epoch 11:  24%|██▍       | 72/300 [00:31<01:40,  2.27it/s]Epoch 11:  24%|██▍       | 73/300 [00:32<01:36,  2.36it/s]Epoch 11:  25%|██▍       | 74/300 [00:32<01:31,  2.46it/s]Epoch 11:  25%|██▌       | 75/300 [00:33<01:44,  2.15it/s]Epoch 11:  25%|██▌       | 76/300 [00:33<01:45,  2.12it/s]Epoch 11:  26%|██▌       | 77/300 [00:34<01:45,  2.10it/s]Epoch 11:  26%|██▌       | 78/300 [00:34<01:45,  2.10it/s]Epoch 11:  26%|██▋       | 79/300 [00:35<01:53,  1.94it/s]06/19/2022 14:52:06 - INFO - __main__ - global step: 1690; train loss: 7.7237043380737305; dev loss: 7.8241400718688965
Epoch 11:  27%|██▋       | 80/300 [00:35<01:48,  2.04it/s]Epoch 11:  27%|██▋       | 81/300 [00:35<01:44,  2.10it/s]Epoch 11:  27%|██▋       | 82/300 [00:36<01:42,  2.12it/s]Epoch 11:  28%|██▊       | 83/300 [00:36<01:44,  2.08it/s]Epoch 11:  28%|██▊       | 84/300 [00:37<01:38,  2.19it/s]Epoch 11:  28%|██▊       | 85/300 [00:37<01:31,  2.34it/s]Epoch 11:  29%|██▊       | 86/300 [00:38<01:26,  2.47it/s]Epoch 11:  29%|██▉       | 87/300 [00:38<01:30,  2.35it/s]Epoch 11:  29%|██▉       | 88/300 [00:38<01:25,  2.47it/s]Epoch 11:  30%|██▉       | 89/300 [00:39<01:22,  2.57it/s]Epoch 11:  30%|███       | 90/300 [00:39<01:28,  2.36it/s]Epoch 11:  30%|███       | 91/300 [00:40<01:36,  2.16it/s]Epoch 11:  31%|███       | 92/300 [00:40<01:43,  2.00it/s]Epoch 11:  31%|███       | 93/300 [00:41<01:36,  2.14it/s]Epoch 11:  31%|███▏      | 94/300 [00:41<01:30,  2.27it/s]Epoch 11:  32%|███▏      | 95/300 [00:42<01:26,  2.38it/s]Epoch 11:  32%|███▏      | 96/300 [00:42<01:33,  2.19it/s]Epoch 11:  32%|███▏      | 97/300 [00:42<01:30,  2.23it/s]Epoch 11:  33%|███▎      | 98/300 [00:43<01:27,  2.30it/s]Epoch 11:  33%|███▎      | 99/300 [00:43<01:26,  2.33it/s]06/19/2022 14:52:15 - INFO - __main__ - global step: 1700; train loss: 7.884930610656738; dev loss: 7.73650598526001
Epoch 11:  33%|███▎      | 100/300 [00:44<01:33,  2.13it/s]Epoch 11:  34%|███▎      | 101/300 [00:44<01:30,  2.19it/s]Epoch 11:  34%|███▍      | 102/300 [00:45<01:29,  2.22it/s]Epoch 11:  34%|███▍      | 103/300 [00:45<01:30,  2.19it/s]Epoch 11:  35%|███▍      | 104/300 [00:46<01:36,  2.04it/s]Epoch 11:  35%|███▌      | 105/300 [00:46<01:30,  2.16it/s]Epoch 11:  35%|███▌      | 106/300 [00:47<01:32,  2.10it/s]Epoch 11:  36%|███▌      | 107/300 [00:47<01:31,  2.11it/s]Epoch 11:  36%|███▌      | 108/300 [00:48<01:37,  1.97it/s]Epoch 11:  36%|███▋      | 109/300 [00:48<01:31,  2.08it/s]Epoch 11:  37%|███▋      | 110/300 [00:49<01:31,  2.08it/s]Epoch 11:  37%|███▋      | 111/300 [00:49<01:30,  2.09it/s]Epoch 11:  37%|███▋      | 112/300 [00:50<01:33,  2.02it/s]Epoch 11:  38%|███▊      | 113/300 [00:50<01:31,  2.04it/s]Epoch 11:  38%|███▊      | 114/300 [00:51<01:30,  2.06it/s]Epoch 11:  38%|███▊      | 115/300 [00:51<01:34,  1.96it/s]Epoch 11:  39%|███▊      | 116/300 [00:52<01:39,  1.86it/s]Epoch 11:  39%|███▉      | 117/300 [00:52<01:34,  1.94it/s]Epoch 11:  39%|███▉      | 118/300 [00:53<01:31,  1.98it/s]Epoch 11:  40%|███▉      | 119/300 [00:53<01:29,  2.02it/s]06/19/2022 14:52:25 - INFO - __main__ - global step: 1710; train loss: 7.631725311279297; dev loss: 7.790865898132324
Epoch 11:  40%|████      | 120/300 [00:54<01:26,  2.09it/s]Epoch 11:  40%|████      | 121/300 [00:54<01:25,  2.09it/s]Epoch 11:  41%|████      | 122/300 [00:54<01:18,  2.28it/s]Epoch 11:  41%|████      | 123/300 [00:55<01:19,  2.21it/s]Epoch 11:  41%|████▏     | 124/300 [00:55<01:19,  2.23it/s]Epoch 11:  42%|████▏     | 125/300 [00:56<01:23,  2.10it/s]Epoch 11:  42%|████▏     | 126/300 [00:56<01:17,  2.25it/s]Epoch 11:  42%|████▏     | 127/300 [00:57<01:16,  2.26it/s]Epoch 11:  43%|████▎     | 128/300 [00:57<01:19,  2.16it/s]Epoch 11:  43%|████▎     | 129/300 [00:58<01:30,  1.88it/s]Epoch 11:  43%|████▎     | 130/300 [00:58<01:30,  1.88it/s]Epoch 11:  44%|████▎     | 131/300 [00:59<01:29,  1.88it/s]Epoch 11:  44%|████▍     | 132/300 [00:59<01:25,  1.97it/s]Epoch 11:  44%|████▍     | 133/300 [01:00<01:25,  1.95it/s]Epoch 11:  45%|████▍     | 134/300 [01:00<01:21,  2.03it/s]Epoch 11:  45%|████▌     | 135/300 [01:01<01:19,  2.08it/s]Epoch 11:  45%|████▌     | 136/300 [01:01<01:15,  2.16it/s]Epoch 11:  46%|████▌     | 137/300 [01:02<01:15,  2.15it/s]Epoch 11:  46%|████▌     | 138/300 [01:02<01:09,  2.31it/s]Epoch 11:  46%|████▋     | 139/300 [01:02<01:05,  2.45it/s]06/19/2022 14:52:34 - INFO - __main__ - global step: 1720; train loss: 7.664255619049072; dev loss: 7.662392616271973
Epoch 11:  47%|████▋     | 140/300 [01:03<01:02,  2.56it/s]Epoch 11:  47%|████▋     | 141/300 [01:03<01:05,  2.42it/s]Epoch 11:  47%|████▋     | 142/300 [01:04<01:02,  2.52it/s]Epoch 11:  48%|████▊     | 143/300 [01:04<01:01,  2.54it/s]Epoch 11:  48%|████▊     | 144/300 [01:04<00:59,  2.62it/s]Epoch 11:  48%|████▊     | 145/300 [01:05<00:57,  2.67it/s]Epoch 11:  49%|████▊     | 146/300 [01:05<01:05,  2.36it/s]Epoch 11:  49%|████▉     | 147/300 [01:06<01:03,  2.40it/s]Epoch 11:  49%|████▉     | 148/300 [01:06<01:02,  2.42it/s]Epoch 11:  50%|████▉     | 149/300 [01:06<01:00,  2.48it/s]Epoch 11:  50%|█████     | 150/300 [01:07<01:03,  2.37it/s]Epoch 11:  50%|█████     | 151/300 [01:07<00:59,  2.49it/s]Epoch 11:  51%|█████     | 152/300 [01:08<00:57,  2.58it/s]Epoch 11:  51%|█████     | 153/300 [01:08<00:55,  2.63it/s]Epoch 11:  51%|█████▏    | 154/300 [01:08<00:59,  2.47it/s]Epoch 11:  52%|█████▏    | 155/300 [01:09<00:57,  2.53it/s]Epoch 11:  52%|█████▏    | 156/300 [01:09<00:56,  2.54it/s]Epoch 11:  52%|█████▏    | 157/300 [01:10<00:54,  2.62it/s]Epoch 11:  53%|█████▎    | 158/300 [01:10<00:57,  2.46it/s]Epoch 11:  53%|█████▎    | 159/300 [01:10<00:58,  2.43it/s]06/19/2022 14:52:42 - INFO - __main__ - global step: 1730; train loss: 7.768306732177734; dev loss: 7.723532199859619
Epoch 11:  53%|█████▎    | 160/300 [01:11<01:02,  2.26it/s]Epoch 11:  54%|█████▎    | 161/300 [01:11<01:04,  2.17it/s]Epoch 11:  54%|█████▍    | 162/300 [01:12<01:05,  2.12it/s]Epoch 11:  54%|█████▍    | 163/300 [01:12<01:02,  2.20it/s]Epoch 11:  55%|█████▍    | 164/300 [01:13<01:05,  2.09it/s]Epoch 11:  55%|█████▌    | 165/300 [01:13<01:03,  2.13it/s]Epoch 11:  55%|█████▌    | 166/300 [01:14<01:11,  1.87it/s]Epoch 11:  56%|█████▌    | 167/300 [01:15<01:08,  1.95it/s]Epoch 11:  56%|█████▌    | 168/300 [01:15<01:06,  2.00it/s]Epoch 11:  56%|█████▋    | 169/300 [01:15<01:05,  2.00it/s]Epoch 11:  57%|█████▋    | 170/300 [01:16<01:08,  1.89it/s]Epoch 11:  57%|█████▋    | 171/300 [01:16<01:01,  2.09it/s]Epoch 11:  57%|█████▋    | 172/300 [01:17<00:56,  2.26it/s]Epoch 11:  58%|█████▊    | 173/300 [01:17<00:52,  2.41it/s]Epoch 11:  58%|█████▊    | 174/300 [01:18<00:50,  2.49it/s]Epoch 11:  58%|█████▊    | 175/300 [01:18<01:00,  2.06it/s]Epoch 11:  59%|█████▊    | 176/300 [01:19<00:56,  2.19it/s]Epoch 11:  59%|█████▉    | 177/300 [01:19<00:56,  2.18it/s]Epoch 11:  59%|█████▉    | 178/300 [01:19<00:54,  2.23it/s]Epoch 11:  60%|█████▉    | 179/300 [01:20<00:59,  2.04it/s]06/19/2022 14:52:52 - INFO - __main__ - global step: 1740; train loss: 7.987967014312744; dev loss: 8.005413055419922
Epoch 11:  60%|██████    | 180/300 [01:21<00:58,  2.07it/s]Epoch 11:  60%|██████    | 181/300 [01:21<00:56,  2.10it/s]Epoch 11:  61%|██████    | 182/300 [01:22<00:59,  2.00it/s]Epoch 11:  61%|██████    | 183/300 [01:22<00:59,  1.97it/s]Epoch 11:  61%|██████▏   | 184/300 [01:23<00:57,  2.03it/s]Epoch 11:  62%|██████▏   | 185/300 [01:23<00:55,  2.07it/s]Epoch 11:  62%|██████▏   | 186/300 [01:24<00:56,  2.02it/s]Epoch 11:  62%|██████▏   | 187/300 [01:24<00:59,  1.89it/s]Epoch 11:  63%|██████▎   | 188/300 [01:25<00:58,  1.92it/s]Epoch 11:  63%|██████▎   | 189/300 [01:25<00:54,  2.02it/s]Epoch 11:  63%|██████▎   | 190/300 [01:26<00:53,  2.06it/s]Epoch 11:  64%|██████▎   | 191/300 [01:26<00:56,  1.92it/s]Epoch 11:  64%|██████▍   | 192/300 [01:27<00:54,  2.00it/s]Epoch 11:  64%|██████▍   | 193/300 [01:27<00:52,  2.03it/s]Epoch 11:  65%|██████▍   | 194/300 [01:28<00:51,  2.05it/s]Epoch 11:  65%|██████▌   | 195/300 [01:28<00:54,  1.91it/s]Epoch 11:  65%|██████▌   | 196/300 [01:29<00:54,  1.92it/s]Epoch 11:  66%|██████▌   | 197/300 [01:29<00:48,  2.12it/s]Epoch 11:  66%|██████▌   | 198/300 [01:29<00:45,  2.24it/s]Epoch 11:  66%|██████▋   | 199/300 [01:30<00:43,  2.34it/s]06/19/2022 14:53:01 - INFO - __main__ - global step: 1750; train loss: 7.3966474533081055; dev loss: 7.542012691497803
Epoch 11:  67%|██████▋   | 200/300 [01:30<00:44,  2.25it/s]Epoch 11:  67%|██████▋   | 201/300 [01:31<00:41,  2.38it/s]Epoch 11:  67%|██████▋   | 202/300 [01:31<00:40,  2.45it/s]Epoch 11:  68%|██████▊   | 203/300 [01:31<00:38,  2.53it/s]Epoch 11:  68%|██████▊   | 204/300 [01:32<00:43,  2.22it/s]Epoch 11:  68%|██████▊   | 205/300 [01:32<00:43,  2.21it/s]Epoch 11:  69%|██████▊   | 206/300 [01:33<00:42,  2.20it/s]Epoch 11:  69%|██████▉   | 207/300 [01:33<00:42,  2.19it/s]Epoch 11:  69%|██████▉   | 208/300 [01:34<00:44,  2.05it/s]Epoch 11:  70%|██████▉   | 209/300 [01:34<00:43,  2.08it/s]Epoch 11:  70%|███████   | 210/300 [01:35<00:44,  2.02it/s]Epoch 11:  70%|███████   | 211/300 [01:35<00:43,  2.06it/s]Epoch 11:  71%|███████   | 212/300 [01:36<00:43,  2.02it/s]Epoch 11:  71%|███████   | 213/300 [01:36<00:39,  2.18it/s]Epoch 11:  71%|███████▏  | 214/300 [01:37<00:38,  2.21it/s]Epoch 11:  72%|███████▏  | 215/300 [01:37<00:37,  2.25it/s]Epoch 11:  72%|███████▏  | 216/300 [01:38<00:39,  2.13it/s]Epoch 11:  72%|███████▏  | 217/300 [01:38<00:38,  2.16it/s]Epoch 11:  73%|███████▎  | 218/300 [01:39<00:37,  2.21it/s]Epoch 11:  73%|███████▎  | 219/300 [01:39<00:36,  2.24it/s]06/19/2022 14:53:11 - INFO - __main__ - global step: 1760; train loss: 7.748533725738525; dev loss: 7.808099269866943
Epoch 11:  73%|███████▎  | 220/300 [01:40<00:38,  2.08it/s]Epoch 11:  74%|███████▎  | 221/300 [01:40<00:37,  2.13it/s]Epoch 11:  74%|███████▍  | 222/300 [01:40<00:35,  2.20it/s]Epoch 11:  74%|███████▍  | 223/300 [01:41<00:34,  2.22it/s]Epoch 11:  75%|███████▍  | 224/300 [01:41<00:34,  2.17it/s]Epoch 11:  75%|███████▌  | 225/300 [01:42<00:32,  2.33it/s]Epoch 11:  75%|███████▌  | 226/300 [01:42<00:30,  2.46it/s]Epoch 11:  76%|███████▌  | 227/300 [01:42<00:28,  2.53it/s]Epoch 11:  76%|███████▌  | 228/300 [01:43<00:28,  2.54it/s]Epoch 11:  76%|███████▋  | 229/300 [01:43<00:29,  2.41it/s]Epoch 11:  77%|███████▋  | 230/300 [01:44<00:27,  2.51it/s]Epoch 11:  77%|███████▋  | 231/300 [01:44<00:28,  2.46it/s]Epoch 11:  77%|███████▋  | 232/300 [01:44<00:28,  2.40it/s]Epoch 11:  78%|███████▊  | 233/300 [01:45<00:29,  2.25it/s]Epoch 11:  78%|███████▊  | 234/300 [01:45<00:27,  2.39it/s]Epoch 11:  78%|███████▊  | 235/300 [01:46<00:26,  2.48it/s]Epoch 11:  79%|███████▊  | 236/300 [01:46<00:26,  2.41it/s]Epoch 11:  79%|███████▉  | 237/300 [01:47<00:29,  2.12it/s]Epoch 11:  79%|███████▉  | 238/300 [01:47<00:29,  2.14it/s]Epoch 11:  80%|███████▉  | 239/300 [01:48<00:27,  2.18it/s]06/19/2022 14:53:19 - INFO - __main__ - global step: 1770; train loss: 7.608422756195068; dev loss: 8.05164623260498
Epoch 11:  80%|████████  | 240/300 [01:48<00:25,  2.34it/s]Epoch 11:  80%|████████  | 241/300 [01:48<00:25,  2.28it/s]Epoch 11:  81%|████████  | 242/300 [01:49<00:24,  2.35it/s]Epoch 11:  81%|████████  | 243/300 [01:49<00:23,  2.44it/s]Epoch 11:  81%|████████▏ | 244/300 [01:50<00:22,  2.44it/s]Epoch 11:  82%|████████▏ | 245/300 [01:50<00:25,  2.15it/s]Epoch 11:  82%|████████▏ | 246/300 [01:51<00:25,  2.09it/s]Epoch 11:  82%|████████▏ | 247/300 [01:51<00:23,  2.26it/s]Epoch 11:  83%|████████▎ | 248/300 [01:51<00:21,  2.38it/s]Epoch 11:  83%|████████▎ | 249/300 [01:52<00:22,  2.30it/s]Epoch 11:  83%|████████▎ | 250/300 [01:52<00:21,  2.35it/s]Epoch 11:  84%|████████▎ | 251/300 [01:53<00:20,  2.44it/s]Epoch 11:  84%|████████▍ | 252/300 [01:53<00:19,  2.46it/s]Epoch 11:  84%|████████▍ | 253/300 [01:54<00:19,  2.46it/s]Epoch 11:  85%|████████▍ | 254/300 [01:54<00:19,  2.35it/s]Epoch 11:  85%|████████▌ | 255/300 [01:54<00:19,  2.27it/s]Epoch 11:  85%|████████▌ | 256/300 [01:55<00:19,  2.21it/s]Epoch 11:  86%|████████▌ | 257/300 [01:55<00:19,  2.19it/s]Epoch 11:  86%|████████▌ | 258/300 [01:56<00:22,  1.90it/s]Epoch 11:  86%|████████▋ | 259/300 [01:57<00:20,  2.00it/s]06/19/2022 14:53:28 - INFO - __main__ - global step: 1780; train loss: 7.730515956878662; dev loss: 7.604918003082275
Epoch 11:  87%|████████▋ | 260/300 [01:57<00:19,  2.05it/s]Epoch 11:  87%|████████▋ | 261/300 [01:57<00:18,  2.09it/s]Epoch 11:  87%|████████▋ | 262/300 [01:58<00:19,  1.94it/s]Epoch 11:  88%|████████▊ | 263/300 [01:58<00:17,  2.15it/s]Epoch 11:  88%|████████▊ | 264/300 [01:59<00:15,  2.33it/s]Epoch 11:  88%|████████▊ | 265/300 [01:59<00:14,  2.49it/s]Epoch 11:  89%|████████▊ | 266/300 [02:00<00:14,  2.43it/s]Epoch 11:  89%|████████▉ | 267/300 [02:00<00:12,  2.55it/s]Epoch 11:  89%|████████▉ | 268/300 [02:00<00:12,  2.64it/s]Epoch 11:  90%|████████▉ | 269/300 [02:01<00:11,  2.70it/s]Epoch 11:  90%|█████████ | 270/300 [02:01<00:12,  2.34it/s]Epoch 11:  90%|█████████ | 271/300 [02:02<00:12,  2.31it/s]Epoch 11:  91%|█████████ | 272/300 [02:02<00:12,  2.23it/s]Epoch 11:  91%|█████████ | 273/300 [02:03<00:12,  2.22it/s]Epoch 11:  91%|█████████▏| 274/300 [02:03<00:12,  2.04it/s]Epoch 11:  92%|█████████▏| 275/300 [02:04<00:11,  2.13it/s]Epoch 11:  92%|█████████▏| 276/300 [02:04<00:10,  2.27it/s]Epoch 11:  92%|█████████▏| 277/300 [02:04<00:09,  2.43it/s]Epoch 11:  93%|█████████▎| 278/300 [02:05<00:09,  2.34it/s]Epoch 11:  93%|█████████▎| 279/300 [02:05<00:08,  2.37it/s]06/19/2022 14:53:37 - INFO - __main__ - global step: 1790; train loss: 8.178349494934082; dev loss: 8.483026504516602
Epoch 11:  93%|█████████▎| 280/300 [02:06<00:08,  2.29it/s]Epoch 11:  94%|█████████▎| 281/300 [02:06<00:08,  2.25it/s]Epoch 11:  94%|█████████▍| 282/300 [02:07<00:08,  2.17it/s]Epoch 11:  94%|█████████▍| 283/300 [02:07<00:08,  1.93it/s]Epoch 11:  95%|█████████▍| 284/300 [02:08<00:08,  1.99it/s]Epoch 11:  95%|█████████▌| 285/300 [02:08<00:07,  2.07it/s]Epoch 11:  95%|█████████▌| 286/300 [02:08<00:06,  2.17it/s]Epoch 11:  96%|█████████▌| 287/300 [02:09<00:05,  2.22it/s]Epoch 11:  96%|█████████▌| 288/300 [02:09<00:05,  2.36it/s]Epoch 11:  96%|█████████▋| 289/300 [02:10<00:04,  2.52it/s]Epoch 11:  97%|█████████▋| 290/300 [02:10<00:03,  2.60it/s]Epoch 11:  97%|█████████▋| 291/300 [02:10<00:03,  2.38it/s]Epoch 11:  97%|█████████▋| 292/300 [02:11<00:03,  2.35it/s]Epoch 11:  98%|█████████▊| 293/300 [02:11<00:03,  2.33it/s]Epoch 11:  98%|█████████▊| 294/300 [02:12<00:02,  2.32it/s]Epoch 11:  98%|█████████▊| 295/300 [02:12<00:02,  2.18it/s]Epoch 11:  99%|█████████▊| 296/300 [02:13<00:01,  2.11it/s]Epoch 11:  99%|█████████▉| 297/300 [02:13<00:01,  2.25it/s]Epoch 11:  99%|█████████▉| 298/300 [02:14<00:00,  2.26it/s]Epoch 11: 100%|█████████▉| 299/300 [02:14<00:00,  2.09it/s]06/19/2022 14:53:46 - INFO - __main__ - global step: 1800; train loss: 7.1333513259887695; dev loss: 7.113182067871094
Epoch 11: 100%|██████████| 300/300 [02:15<00:00,  2.23it/s]Epoch 11: 100%|██████████| 300/300 [02:15<00:00,  2.22it/s]
Epoch 12:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 12:   0%|          | 1/300 [00:00<02:06,  2.36it/s]Epoch 12:   1%|          | 2/300 [00:00<02:04,  2.39it/s]Epoch 12:   1%|          | 3/300 [00:01<02:06,  2.35it/s]Epoch 12:   1%|▏         | 4/300 [00:01<02:07,  2.33it/s]Epoch 12:   2%|▏         | 5/300 [00:02<01:59,  2.46it/s]Epoch 12:   2%|▏         | 6/300 [00:02<02:04,  2.37it/s]Epoch 12:   2%|▏         | 7/300 [00:02<02:06,  2.31it/s]Epoch 12:   3%|▎         | 8/300 [00:03<02:15,  2.15it/s]Epoch 12:   3%|▎         | 9/300 [00:03<02:07,  2.29it/s]Epoch 12:   3%|▎         | 10/300 [00:04<01:59,  2.43it/s]Epoch 12:   4%|▎         | 11/300 [00:04<01:54,  2.53it/s]Epoch 12:   4%|▍         | 12/300 [00:05<02:00,  2.39it/s]Epoch 12:   4%|▍         | 13/300 [00:05<01:57,  2.45it/s]Epoch 12:   5%|▍         | 14/300 [00:05<01:56,  2.45it/s]Epoch 12:   5%|▌         | 15/300 [00:06<02:05,  2.27it/s]Epoch 12:   5%|▌         | 16/300 [00:06<02:16,  2.08it/s]Epoch 12:   6%|▌         | 17/300 [00:07<02:08,  2.20it/s]Epoch 12:   6%|▌         | 18/300 [00:07<01:57,  2.40it/s]Epoch 12:   6%|▋         | 19/300 [00:08<01:54,  2.44it/s]06/19/2022 14:53:54 - INFO - __main__ - global step: 1810; train loss: 7.446996212005615; dev loss: 7.514573574066162
Epoch 12:   7%|▋         | 20/300 [00:08<01:56,  2.40it/s]Epoch 12:   7%|▋         | 21/300 [00:08<01:48,  2.57it/s]Epoch 12:   7%|▋         | 22/300 [00:09<01:56,  2.39it/s]Epoch 12:   8%|▊         | 23/300 [00:09<01:56,  2.38it/s]Epoch 12:   8%|▊         | 24/300 [00:10<02:08,  2.15it/s]Epoch 12:   8%|▊         | 25/300 [00:10<02:10,  2.11it/s]Epoch 12:   9%|▊         | 26/300 [00:11<02:02,  2.23it/s]Epoch 12:   9%|▉         | 27/300 [00:11<01:59,  2.28it/s]Epoch 12:   9%|▉         | 28/300 [00:12<02:03,  2.20it/s]Epoch 12:  10%|▉         | 29/300 [00:12<01:57,  2.31it/s]Epoch 12:  10%|█         | 30/300 [00:12<01:52,  2.39it/s]Epoch 12:  10%|█         | 31/300 [00:13<01:54,  2.36it/s]Epoch 12:  11%|█         | 32/300 [00:13<02:07,  2.10it/s]Epoch 12:  11%|█         | 33/300 [00:14<02:04,  2.15it/s]Epoch 12:  11%|█▏        | 34/300 [00:14<01:54,  2.33it/s]Epoch 12:  12%|█▏        | 35/300 [00:15<01:48,  2.43it/s]Epoch 12:  12%|█▏        | 36/300 [00:15<01:44,  2.52it/s]Epoch 12:  12%|█▏        | 37/300 [00:15<01:51,  2.37it/s]Epoch 12:  13%|█▎        | 38/300 [00:16<01:46,  2.46it/s]Epoch 12:  13%|█▎        | 39/300 [00:16<01:50,  2.36it/s]06/19/2022 14:54:03 - INFO - __main__ - global step: 1820; train loss: 7.868124485015869; dev loss: 7.7767438888549805
Epoch 12:  13%|█▎        | 40/300 [00:17<02:00,  2.15it/s]Epoch 12:  14%|█▎        | 41/300 [00:17<02:03,  2.10it/s]Epoch 12:  14%|█▍        | 42/300 [00:18<01:55,  2.23it/s]Epoch 12:  14%|█▍        | 43/300 [00:18<01:53,  2.26it/s]Epoch 12:  15%|█▍        | 44/300 [00:19<01:52,  2.28it/s]Epoch 12:  15%|█▌        | 45/300 [00:19<02:03,  2.06it/s]Epoch 12:  15%|█▌        | 46/300 [00:19<01:52,  2.26it/s]Epoch 12:  16%|█▌        | 47/300 [00:20<01:44,  2.43it/s]Epoch 12:  16%|█▌        | 48/300 [00:20<01:38,  2.55it/s]Epoch 12:  16%|█▋        | 49/300 [00:21<01:42,  2.45it/s]Epoch 12:  17%|█▋        | 50/300 [00:21<01:37,  2.57it/s]Epoch 12:  17%|█▋        | 51/300 [00:21<01:37,  2.57it/s]Epoch 12:  17%|█▋        | 52/300 [00:22<01:38,  2.51it/s]Epoch 12:  18%|█▊        | 53/300 [00:22<01:42,  2.41it/s]Epoch 12:  18%|█▊        | 54/300 [00:23<01:40,  2.45it/s]Epoch 12:  18%|█▊        | 55/300 [00:23<01:35,  2.57it/s]Epoch 12:  19%|█▊        | 56/300 [00:23<01:31,  2.66it/s]Epoch 12:  19%|█▉        | 57/300 [00:24<01:36,  2.52it/s]Epoch 12:  19%|█▉        | 58/300 [00:24<01:31,  2.64it/s]Epoch 12:  20%|█▉        | 59/300 [00:25<01:34,  2.56it/s]06/19/2022 14:54:11 - INFO - __main__ - global step: 1830; train loss: 7.541257381439209; dev loss: 7.224801063537598
Epoch 12:  20%|██        | 60/300 [00:25<01:30,  2.64it/s]Epoch 12:  20%|██        | 61/300 [00:25<01:27,  2.72it/s]Epoch 12:  21%|██        | 62/300 [00:26<01:34,  2.53it/s]Epoch 12:  21%|██        | 63/300 [00:26<01:40,  2.37it/s]Epoch 12:  21%|██▏       | 64/300 [00:27<01:48,  2.18it/s]Epoch 12:  22%|██▏       | 65/300 [00:27<01:47,  2.18it/s]Epoch 12:  22%|██▏       | 66/300 [00:28<01:56,  2.01it/s]Epoch 12:  22%|██▏       | 67/300 [00:28<01:56,  2.00it/s]Epoch 12:  23%|██▎       | 68/300 [00:29<01:54,  2.03it/s]Epoch 12:  23%|██▎       | 69/300 [00:29<01:46,  2.18it/s]Epoch 12:  23%|██▎       | 70/300 [00:30<01:46,  2.15it/s]Epoch 12:  24%|██▎       | 71/300 [00:30<01:40,  2.28it/s]Epoch 12:  24%|██▍       | 72/300 [00:30<01:34,  2.41it/s]Epoch 12:  24%|██▍       | 73/300 [00:31<01:29,  2.54it/s]Epoch 12:  25%|██▍       | 74/300 [00:31<01:32,  2.45it/s]Epoch 12:  25%|██▌       | 75/300 [00:31<01:27,  2.56it/s]Epoch 12:  25%|██▌       | 76/300 [00:32<01:27,  2.55it/s]Epoch 12:  26%|██▌       | 77/300 [00:32<01:26,  2.56it/s]Epoch 12:  26%|██▌       | 78/300 [00:33<01:44,  2.13it/s]Epoch 12:  26%|██▋       | 79/300 [00:33<01:39,  2.22it/s]06/19/2022 14:54:20 - INFO - __main__ - global step: 1840; train loss: 7.173532009124756; dev loss: 7.126763343811035
Epoch 12:  27%|██▋       | 80/300 [00:34<01:45,  2.09it/s]Epoch 12:  27%|██▋       | 81/300 [00:34<01:42,  2.14it/s]Epoch 12:  27%|██▋       | 82/300 [00:35<01:44,  2.09it/s]Epoch 12:  28%|██▊       | 83/300 [00:35<01:42,  2.12it/s]Epoch 12:  28%|██▊       | 84/300 [00:36<01:40,  2.15it/s]Epoch 12:  28%|██▊       | 85/300 [00:36<01:40,  2.13it/s]Epoch 12:  29%|██▊       | 86/300 [00:37<01:47,  1.99it/s]Epoch 12:  29%|██▉       | 87/300 [00:37<01:42,  2.07it/s]Epoch 12:  29%|██▉       | 88/300 [00:38<01:37,  2.17it/s]Epoch 12:  30%|██▉       | 89/300 [00:38<01:34,  2.23it/s]Epoch 12:  30%|███       | 90/300 [00:38<01:32,  2.28it/s]Epoch 12:  30%|███       | 91/300 [00:39<01:36,  2.16it/s]Epoch 12:  31%|███       | 92/300 [00:39<01:33,  2.23it/s]Epoch 12:  31%|███       | 93/300 [00:40<01:28,  2.33it/s]Epoch 12:  31%|███▏      | 94/300 [00:40<01:25,  2.40it/s]Epoch 12:  32%|███▏      | 95/300 [00:41<01:30,  2.27it/s]Epoch 12:  32%|███▏      | 96/300 [00:41<01:30,  2.25it/s]Epoch 12:  32%|███▏      | 97/300 [00:41<01:23,  2.42it/s]Epoch 12:  33%|███▎      | 98/300 [00:42<01:19,  2.54it/s]Epoch 12:  33%|███▎      | 99/300 [00:42<01:21,  2.47it/s]06/19/2022 14:54:29 - INFO - __main__ - global step: 1850; train loss: 7.957715034484863; dev loss: 7.595399379730225
Epoch 12:  33%|███▎      | 100/300 [00:43<01:16,  2.62it/s]Epoch 12:  34%|███▎      | 101/300 [00:43<01:13,  2.73it/s]Epoch 12:  34%|███▍      | 102/300 [00:43<01:14,  2.66it/s]Epoch 12:  34%|███▍      | 103/300 [00:44<01:17,  2.54it/s]Epoch 12:  35%|███▍      | 104/300 [00:44<01:15,  2.59it/s]Epoch 12:  35%|███▌      | 105/300 [00:44<01:11,  2.71it/s]Epoch 12:  35%|███▌      | 106/300 [00:45<01:09,  2.80it/s]Epoch 12:  36%|███▌      | 107/300 [00:45<01:17,  2.49it/s]Epoch 12:  36%|███▌      | 108/300 [00:46<01:20,  2.37it/s]Epoch 12:  36%|███▋      | 109/300 [00:46<01:17,  2.45it/s]Epoch 12:  37%|███▋      | 110/300 [00:47<01:19,  2.38it/s]Epoch 12:  37%|███▋      | 111/300 [00:47<01:28,  2.13it/s]Epoch 12:  37%|███▋      | 112/300 [00:48<01:25,  2.20it/s]Epoch 12:  38%|███▊      | 113/300 [00:48<01:24,  2.22it/s]Epoch 12:  38%|███▊      | 114/300 [00:48<01:23,  2.24it/s]Epoch 12:  38%|███▊      | 115/300 [00:49<01:21,  2.26it/s]Epoch 12:  39%|███▊      | 116/300 [00:49<01:31,  2.02it/s]Epoch 12:  39%|███▉      | 117/300 [00:50<01:22,  2.23it/s]Epoch 12:  39%|███▉      | 118/300 [00:50<01:15,  2.41it/s]Epoch 12:  40%|███▉      | 119/300 [00:50<01:10,  2.55it/s]06/19/2022 14:54:37 - INFO - __main__ - global step: 1860; train loss: 8.224291801452637; dev loss: 8.121064186096191
Epoch 12:  40%|████      | 120/300 [00:51<01:12,  2.47it/s]Epoch 12:  40%|████      | 121/300 [00:51<01:08,  2.62it/s]Epoch 12:  41%|████      | 122/300 [00:52<01:05,  2.71it/s]Epoch 12:  41%|████      | 123/300 [00:52<01:03,  2.78it/s]Epoch 12:  41%|████▏     | 124/300 [00:52<01:07,  2.60it/s]Epoch 12:  42%|████▏     | 125/300 [00:53<01:04,  2.72it/s]Epoch 12:  42%|████▏     | 126/300 [00:53<01:01,  2.81it/s]Epoch 12:  42%|████▏     | 127/300 [00:53<01:00,  2.87it/s]Epoch 12:  43%|████▎     | 128/300 [00:54<01:04,  2.68it/s]Epoch 12:  43%|████▎     | 129/300 [00:54<01:07,  2.55it/s]Epoch 12:  43%|████▎     | 130/300 [00:55<01:04,  2.64it/s]Epoch 12:  44%|████▎     | 131/300 [00:55<01:06,  2.56it/s]Epoch 12:  44%|████▍     | 132/300 [00:56<01:16,  2.21it/s]Epoch 12:  44%|████▍     | 133/300 [00:56<01:10,  2.35it/s]Epoch 12:  45%|████▍     | 134/300 [00:56<01:08,  2.43it/s]Epoch 12:  45%|████▌     | 135/300 [00:57<01:09,  2.36it/s]Epoch 12:  45%|████▌     | 136/300 [00:57<01:15,  2.17it/s]Epoch 12:  46%|████▌     | 137/300 [00:58<01:08,  2.37it/s]Epoch 12:  46%|████▌     | 138/300 [00:58<01:04,  2.50it/s]Epoch 12:  46%|████▋     | 139/300 [00:58<01:02,  2.58it/s]06/19/2022 14:54:45 - INFO - __main__ - global step: 1870; train loss: 7.384446620941162; dev loss: 7.381675720214844
Epoch 12:  47%|████▋     | 140/300 [00:59<01:05,  2.43it/s]Epoch 12:  47%|████▋     | 141/300 [00:59<01:04,  2.47it/s]Epoch 12:  47%|████▋     | 142/300 [01:00<01:05,  2.40it/s]Epoch 12:  48%|████▊     | 143/300 [01:00<01:07,  2.33it/s]Epoch 12:  48%|████▊     | 144/300 [01:01<01:08,  2.27it/s]Epoch 12:  48%|████▊     | 145/300 [01:01<01:13,  2.10it/s]Epoch 12:  49%|████▊     | 146/300 [01:02<01:10,  2.19it/s]Epoch 12:  49%|████▉     | 147/300 [01:02<01:09,  2.20it/s]Epoch 12:  49%|████▉     | 148/300 [01:02<01:10,  2.14it/s]Epoch 12:  50%|████▉     | 149/300 [01:03<01:12,  2.08it/s]Epoch 12:  50%|█████     | 150/300 [01:03<01:10,  2.14it/s]Epoch 12:  50%|█████     | 151/300 [01:04<01:08,  2.19it/s]Epoch 12:  51%|█████     | 152/300 [01:04<01:10,  2.11it/s]Epoch 12:  51%|█████     | 153/300 [01:05<01:12,  2.03it/s]Epoch 12:  51%|█████▏    | 154/300 [01:05<01:14,  1.97it/s]Epoch 12:  52%|█████▏    | 155/300 [01:06<01:10,  2.06it/s]Epoch 12:  52%|█████▏    | 156/300 [01:06<01:09,  2.08it/s]Epoch 12:  52%|█████▏    | 157/300 [01:07<01:12,  1.98it/s]Epoch 12:  53%|█████▎    | 158/300 [01:07<01:09,  2.06it/s]Epoch 12:  53%|█████▎    | 159/300 [01:08<01:10,  2.01it/s]06/19/2022 14:54:55 - INFO - __main__ - global step: 1880; train loss: 8.091826438903809; dev loss: 8.345195770263672
Epoch 12:  53%|█████▎    | 160/300 [01:08<01:08,  2.03it/s]Epoch 12:  54%|█████▎    | 161/300 [01:09<01:09,  2.01it/s]Epoch 12:  54%|█████▍    | 162/300 [01:09<01:05,  2.09it/s]Epoch 12:  54%|█████▍    | 163/300 [01:10<01:03,  2.16it/s]Epoch 12:  55%|█████▍    | 164/300 [01:10<01:06,  2.05it/s]Epoch 12:  55%|█████▌    | 165/300 [01:11<01:08,  1.96it/s]Epoch 12:  55%|█████▌    | 166/300 [01:11<01:07,  1.99it/s]Epoch 12:  56%|█████▌    | 167/300 [01:12<01:03,  2.09it/s]Epoch 12:  56%|█████▌    | 168/300 [01:12<01:00,  2.18it/s]Epoch 12:  56%|█████▋    | 169/300 [01:13<01:02,  2.09it/s]Epoch 12:  57%|█████▋    | 170/300 [01:13<01:01,  2.10it/s]Epoch 12:  57%|█████▋    | 171/300 [01:14<01:02,  2.07it/s]Epoch 12:  57%|█████▋    | 172/300 [01:14<00:57,  2.23it/s]Epoch 12:  58%|█████▊    | 173/300 [01:14<00:52,  2.41it/s]Epoch 12:  58%|█████▊    | 174/300 [01:15<00:53,  2.37it/s]Epoch 12:  58%|█████▊    | 175/300 [01:15<00:51,  2.43it/s]Epoch 12:  59%|█████▊    | 176/300 [01:16<00:51,  2.40it/s]Epoch 12:  59%|█████▉    | 177/300 [01:16<00:48,  2.52it/s]Epoch 12:  59%|█████▉    | 178/300 [01:16<00:50,  2.42it/s]Epoch 12:  60%|█████▉    | 179/300 [01:17<00:47,  2.54it/s]06/19/2022 14:55:03 - INFO - __main__ - global step: 1890; train loss: 7.656088829040527; dev loss: 7.795645236968994
Epoch 12:  60%|██████    | 180/300 [01:17<00:45,  2.65it/s]Epoch 12:  60%|██████    | 181/300 [01:17<00:44,  2.64it/s]Epoch 12:  61%|██████    | 182/300 [01:18<00:48,  2.42it/s]Epoch 12:  61%|██████    | 183/300 [01:18<00:47,  2.45it/s]Epoch 12:  61%|██████▏   | 184/300 [01:19<00:46,  2.48it/s]Epoch 12:  62%|██████▏   | 185/300 [01:19<00:49,  2.34it/s]Epoch 12:  62%|██████▏   | 186/300 [01:20<00:53,  2.14it/s]Epoch 12:  62%|██████▏   | 187/300 [01:20<00:51,  2.19it/s]Epoch 12:  63%|██████▎   | 188/300 [01:21<00:48,  2.29it/s]Epoch 12:  63%|██████▎   | 189/300 [01:21<00:45,  2.45it/s]Epoch 12:  63%|██████▎   | 190/300 [01:21<00:45,  2.39it/s]Epoch 12:  64%|██████▎   | 191/300 [01:22<00:45,  2.41it/s]Epoch 12:  64%|██████▍   | 192/300 [01:22<00:42,  2.54it/s]Epoch 12:  64%|██████▍   | 193/300 [01:22<00:39,  2.69it/s]Epoch 12:  65%|██████▍   | 194/300 [01:23<00:41,  2.58it/s]Epoch 12:  65%|██████▌   | 195/300 [01:23<00:41,  2.55it/s]Epoch 12:  65%|██████▌   | 196/300 [01:24<00:38,  2.67it/s]Epoch 12:  66%|██████▌   | 197/300 [01:24<00:36,  2.79it/s]Epoch 12:  66%|██████▌   | 198/300 [01:24<00:35,  2.87it/s]Epoch 12:  66%|██████▋   | 199/300 [01:25<00:39,  2.54it/s]06/19/2022 14:55:11 - INFO - __main__ - global step: 1900; train loss: 7.760599613189697; dev loss: 7.76092004776001
Epoch 12:  67%|██████▋   | 200/300 [01:25<00:41,  2.42it/s]Epoch 12:  67%|██████▋   | 201/300 [01:26<00:44,  2.20it/s]Epoch 12:  67%|██████▋   | 202/300 [01:26<00:42,  2.30it/s]Epoch 12:  68%|██████▊   | 203/300 [01:27<00:46,  2.09it/s]Epoch 12:  68%|██████▊   | 204/300 [01:27<00:44,  2.15it/s]Epoch 12:  68%|██████▊   | 205/300 [01:28<00:43,  2.19it/s]Epoch 12:  69%|██████▊   | 206/300 [01:28<00:42,  2.20it/s]Epoch 12:  69%|██████▉   | 207/300 [01:29<00:46,  2.01it/s]Epoch 12:  69%|██████▉   | 208/300 [01:29<00:46,  1.98it/s]Epoch 12:  70%|██████▉   | 209/300 [01:30<00:43,  2.08it/s]Epoch 12:  70%|███████   | 210/300 [01:30<00:42,  2.11it/s]Epoch 12:  70%|███████   | 211/300 [01:31<00:45,  1.97it/s]Epoch 12:  71%|███████   | 212/300 [01:31<00:42,  2.09it/s]Epoch 12:  71%|███████   | 213/300 [01:31<00:38,  2.28it/s]Epoch 12:  71%|███████▏  | 214/300 [01:32<00:36,  2.36it/s]Epoch 12:  72%|███████▏  | 215/300 [01:32<00:37,  2.25it/s]Epoch 12:  72%|███████▏  | 216/300 [01:33<00:36,  2.29it/s]Epoch 12:  72%|███████▏  | 217/300 [01:33<00:33,  2.48it/s]Epoch 12:  73%|███████▎  | 218/300 [01:33<00:31,  2.64it/s]Epoch 12:  73%|███████▎  | 219/300 [01:34<00:31,  2.53it/s]06/19/2022 14:55:20 - INFO - __main__ - global step: 1910; train loss: 7.614438056945801; dev loss: 7.861773490905762
Epoch 12:  73%|███████▎  | 220/300 [01:34<00:30,  2.64it/s]Epoch 12:  74%|███████▎  | 221/300 [01:34<00:28,  2.74it/s]Epoch 12:  74%|███████▍  | 222/300 [01:35<00:27,  2.79it/s]Epoch 12:  74%|███████▍  | 223/300 [01:35<00:27,  2.84it/s]Epoch 12:  75%|███████▍  | 224/300 [01:36<00:29,  2.56it/s]Epoch 12:  75%|███████▌  | 225/300 [01:36<00:30,  2.49it/s]Epoch 12:  75%|███████▌  | 226/300 [01:36<00:28,  2.58it/s]Epoch 12:  76%|███████▌  | 227/300 [01:37<00:27,  2.62it/s]Epoch 12:  76%|███████▌  | 228/300 [01:37<00:30,  2.39it/s]Epoch 12:  76%|███████▋  | 229/300 [01:38<00:28,  2.47it/s]Epoch 12:  77%|███████▋  | 230/300 [01:38<00:27,  2.52it/s]Epoch 12:  77%|███████▋  | 231/300 [01:38<00:25,  2.65it/s]Epoch 12:  77%|███████▋  | 232/300 [01:39<00:27,  2.45it/s]Epoch 12:  78%|███████▊  | 233/300 [01:39<00:26,  2.57it/s]Epoch 12:  78%|███████▊  | 234/300 [01:40<00:25,  2.62it/s]Epoch 12:  78%|███████▊  | 235/300 [01:40<00:23,  2.72it/s]Epoch 12:  79%|███████▊  | 236/300 [01:40<00:25,  2.48it/s]Epoch 12:  79%|███████▉  | 237/300 [01:41<00:26,  2.34it/s]Epoch 12:  79%|███████▉  | 238/300 [01:41<00:26,  2.31it/s]Epoch 12:  80%|███████▉  | 239/300 [01:42<00:26,  2.29it/s]06/19/2022 14:55:29 - INFO - __main__ - global step: 1920; train loss: 7.534024238586426; dev loss: 7.397230625152588
Epoch 12:  80%|████████  | 240/300 [01:42<00:28,  2.11it/s]Epoch 12:  80%|████████  | 241/300 [01:43<00:29,  2.02it/s]Epoch 12:  81%|████████  | 242/300 [01:43<00:28,  2.00it/s]Epoch 12:  81%|████████  | 243/300 [01:44<00:26,  2.11it/s]Epoch 12:  81%|████████▏ | 244/300 [01:44<00:27,  2.05it/s]Epoch 12:  82%|████████▏ | 245/300 [01:45<00:26,  2.06it/s]Epoch 12:  82%|████████▏ | 246/300 [01:45<00:24,  2.22it/s]Epoch 12:  82%|████████▏ | 247/300 [01:46<00:24,  2.19it/s]Epoch 12:  83%|████████▎ | 248/300 [01:46<00:24,  2.17it/s]Epoch 12:  83%|████████▎ | 249/300 [01:47<00:23,  2.18it/s]Epoch 12:  83%|████████▎ | 250/300 [01:47<00:21,  2.32it/s]Epoch 12:  84%|████████▎ | 251/300 [01:47<00:22,  2.20it/s]Epoch 12:  84%|████████▍ | 252/300 [01:48<00:20,  2.35it/s]Epoch 12:  84%|████████▍ | 253/300 [01:48<00:21,  2.24it/s]Epoch 12:  85%|████████▍ | 254/300 [01:49<00:19,  2.36it/s]Epoch 12:  85%|████████▌ | 255/300 [01:49<00:17,  2.51it/s]Epoch 12:  85%|████████▌ | 256/300 [01:49<00:16,  2.65it/s]Epoch 12:  86%|████████▌ | 257/300 [01:50<00:17,  2.50it/s]Epoch 12:  86%|████████▌ | 258/300 [01:50<00:16,  2.58it/s]Epoch 12:  86%|████████▋ | 259/300 [01:51<00:16,  2.55it/s]06/19/2022 14:55:37 - INFO - __main__ - global step: 1930; train loss: 7.42230749130249; dev loss: 7.602972507476807
Epoch 12:  87%|████████▋ | 260/300 [01:51<00:15,  2.65it/s]Epoch 12:  87%|████████▋ | 261/300 [01:51<00:15,  2.51it/s]Epoch 12:  87%|████████▋ | 262/300 [01:52<00:14,  2.61it/s]Epoch 12:  88%|████████▊ | 263/300 [01:52<00:14,  2.59it/s]Epoch 12:  88%|████████▊ | 264/300 [01:53<00:15,  2.33it/s]Epoch 12:  88%|████████▊ | 265/300 [01:53<00:16,  2.09it/s]Epoch 12:  89%|████████▊ | 266/300 [01:54<00:15,  2.23it/s]Epoch 12:  89%|████████▉ | 267/300 [01:54<00:13,  2.42it/s]Epoch 12:  89%|████████▉ | 268/300 [01:54<00:12,  2.59it/s]Epoch 12:  90%|████████▉ | 269/300 [01:55<00:12,  2.53it/s]Epoch 12:  90%|█████████ | 270/300 [01:55<00:11,  2.66it/s]Epoch 12:  90%|█████████ | 271/300 [01:55<00:11,  2.47it/s]Epoch 12:  91%|█████████ | 272/300 [01:56<00:10,  2.55it/s]Epoch 12:  91%|█████████ | 273/300 [01:56<00:11,  2.42it/s]Epoch 12:  91%|█████████▏| 274/300 [01:57<00:10,  2.51it/s]Epoch 12:  92%|█████████▏| 275/300 [01:57<00:09,  2.58it/s]Epoch 12:  92%|█████████▏| 276/300 [01:57<00:09,  2.63it/s]Epoch 12:  92%|█████████▏| 277/300 [01:58<00:08,  2.67it/s]Epoch 12:  93%|█████████▎| 278/300 [01:58<00:08,  2.49it/s]Epoch 12:  93%|█████████▎| 279/300 [01:59<00:08,  2.56it/s]06/19/2022 14:55:45 - INFO - __main__ - global step: 1940; train loss: 7.582827091217041; dev loss: 7.825219631195068
Epoch 12:  93%|█████████▎| 280/300 [01:59<00:07,  2.62it/s]Epoch 12:  94%|█████████▎| 281/300 [01:59<00:07,  2.64it/s]Epoch 12:  94%|█████████▍| 282/300 [02:00<00:07,  2.36it/s]Epoch 12:  94%|█████████▍| 283/300 [02:00<00:07,  2.31it/s]Epoch 12:  95%|█████████▍| 284/300 [02:01<00:06,  2.29it/s]Epoch 12:  95%|█████████▌| 285/300 [02:01<00:06,  2.30it/s]Epoch 12:  95%|█████████▌| 286/300 [02:02<00:06,  2.07it/s]Epoch 12:  96%|█████████▌| 287/300 [02:02<00:06,  2.16it/s]Epoch 12:  96%|█████████▌| 288/300 [02:03<00:05,  2.17it/s]Epoch 12:  96%|█████████▋| 289/300 [02:03<00:04,  2.29it/s]Epoch 12:  97%|█████████▋| 290/300 [02:03<00:04,  2.31it/s]Epoch 12:  97%|█████████▋| 291/300 [02:04<00:03,  2.47it/s]Epoch 12:  97%|█████████▋| 292/300 [02:04<00:03,  2.64it/s]Epoch 12:  98%|█████████▊| 293/300 [02:04<00:02,  2.76it/s]Epoch 12:  98%|█████████▊| 294/300 [02:05<00:02,  2.63it/s]Epoch 12:  98%|█████████▊| 295/300 [02:05<00:01,  2.68it/s]Epoch 12:  99%|█████████▊| 296/300 [02:06<00:01,  2.58it/s]Epoch 12:  99%|█████████▉| 297/300 [02:06<00:01,  2.53it/s]Epoch 12:  99%|█████████▉| 298/300 [02:06<00:00,  2.45it/s]Epoch 12: 100%|█████████▉| 299/300 [02:07<00:00,  2.61it/s]06/19/2022 14:55:53 - INFO - __main__ - global step: 1950; train loss: 7.466366767883301; dev loss: 7.447722434997559
Epoch 12: 100%|██████████| 300/300 [02:07<00:00,  2.64it/s]Epoch 12: 100%|██████████| 300/300 [02:07<00:00,  2.35it/s]
Epoch 13:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 13:   0%|          | 1/300 [00:00<01:48,  2.75it/s]Epoch 13:   1%|          | 2/300 [00:00<02:19,  2.13it/s]Epoch 13:   1%|          | 3/300 [00:01<02:01,  2.45it/s]Epoch 13:   1%|▏         | 4/300 [00:01<01:52,  2.62it/s]Epoch 13:   2%|▏         | 5/300 [00:02<02:02,  2.40it/s]Epoch 13:   2%|▏         | 6/300 [00:02<01:58,  2.49it/s]Epoch 13:   2%|▏         | 7/300 [00:03<02:18,  2.12it/s]Epoch 13:   3%|▎         | 8/300 [00:03<02:09,  2.25it/s]Epoch 13:   3%|▎         | 9/300 [00:03<02:13,  2.19it/s]Epoch 13:   3%|▎         | 10/300 [00:04<02:10,  2.23it/s]Epoch 13:   4%|▎         | 11/300 [00:04<02:16,  2.12it/s]Epoch 13:   4%|▍         | 12/300 [00:05<02:07,  2.26it/s]Epoch 13:   4%|▍         | 13/300 [00:05<01:57,  2.45it/s]Epoch 13:   5%|▍         | 14/300 [00:05<01:49,  2.61it/s]Epoch 13:   5%|▌         | 15/300 [00:06<02:01,  2.34it/s]Epoch 13:   5%|▌         | 16/300 [00:06<02:04,  2.28it/s]Epoch 13:   6%|▌         | 17/300 [00:07<02:04,  2.28it/s]Epoch 13:   6%|▌         | 18/300 [00:07<02:05,  2.25it/s]Epoch 13:   6%|▋         | 19/300 [00:08<02:07,  2.21it/s]06/19/2022 14:56:02 - INFO - __main__ - global step: 1960; train loss: 7.296545505523682; dev loss: 6.892422676086426
Epoch 13:   7%|▋         | 20/300 [00:08<02:01,  2.31it/s]Epoch 13:   7%|▋         | 21/300 [00:09<02:01,  2.30it/s]Epoch 13:   7%|▋         | 22/300 [00:09<02:04,  2.24it/s]Epoch 13:   8%|▊         | 23/300 [00:10<02:07,  2.18it/s]Epoch 13:   8%|▊         | 24/300 [00:10<01:57,  2.36it/s]Epoch 13:   8%|▊         | 25/300 [00:10<01:48,  2.53it/s]Epoch 13:   9%|▊         | 26/300 [00:11<01:42,  2.66it/s]Epoch 13:   9%|▉         | 27/300 [00:11<01:49,  2.49it/s]Epoch 13:   9%|▉         | 28/300 [00:11<01:44,  2.61it/s]Epoch 13:  10%|▉         | 29/300 [00:12<01:38,  2.74it/s]Epoch 13:  10%|█         | 30/300 [00:12<01:38,  2.73it/s]Epoch 13:  10%|█         | 31/300 [00:12<01:38,  2.74it/s]Epoch 13:  11%|█         | 32/300 [00:13<01:48,  2.47it/s]Epoch 13:  11%|█         | 33/300 [00:13<01:42,  2.62it/s]Epoch 13:  11%|█▏        | 34/300 [00:14<01:39,  2.67it/s]Epoch 13:  12%|█▏        | 35/300 [00:14<01:36,  2.75it/s]Epoch 13:  12%|█▏        | 36/300 [00:14<01:43,  2.55it/s]Epoch 13:  12%|█▏        | 37/300 [00:15<01:38,  2.67it/s]Epoch 13:  13%|█▎        | 38/300 [00:15<01:37,  2.70it/s]Epoch 13:  13%|█▎        | 39/300 [00:15<01:33,  2.79it/s]06/19/2022 14:56:10 - INFO - __main__ - global step: 1970; train loss: 7.807909965515137; dev loss: 7.9010820388793945
Epoch 13:  13%|█▎        | 40/300 [00:16<01:38,  2.64it/s]Epoch 13:  14%|█▎        | 41/300 [00:16<01:33,  2.76it/s]Epoch 13:  14%|█▍        | 42/300 [00:17<01:33,  2.76it/s]Epoch 13:  14%|█▍        | 43/300 [00:17<01:36,  2.66it/s]Epoch 13:  15%|█▍        | 44/300 [00:17<01:40,  2.56it/s]Epoch 13:  15%|█▌        | 45/300 [00:18<01:34,  2.69it/s]Epoch 13:  15%|█▌        | 46/300 [00:18<01:36,  2.62it/s]Epoch 13:  16%|█▌        | 47/300 [00:18<01:32,  2.72it/s]Epoch 13:  16%|█▌        | 48/300 [00:19<01:40,  2.51it/s]Epoch 13:  16%|█▋        | 49/300 [00:19<01:39,  2.52it/s]Epoch 13:  17%|█▋        | 50/300 [00:20<01:44,  2.40it/s]Epoch 13:  17%|█▋        | 51/300 [00:20<01:40,  2.48it/s]Epoch 13:  17%|█▋        | 52/300 [00:21<01:48,  2.29it/s]Epoch 13:  18%|█▊        | 53/300 [00:21<01:39,  2.48it/s]Epoch 13:  18%|█▊        | 54/300 [00:21<01:39,  2.48it/s]Epoch 13:  18%|█▊        | 55/300 [00:22<01:33,  2.61it/s]Epoch 13:  19%|█▊        | 56/300 [00:22<01:42,  2.39it/s]Epoch 13:  19%|█▉        | 57/300 [00:23<01:35,  2.54it/s]Epoch 13:  19%|█▉        | 58/300 [00:23<01:30,  2.67it/s]Epoch 13:  20%|█▉        | 59/300 [00:23<01:26,  2.78it/s]06/19/2022 14:56:17 - INFO - __main__ - global step: 1980; train loss: 7.990880489349365; dev loss: 8.046189308166504
Epoch 13:  20%|██        | 60/300 [00:24<01:23,  2.86it/s]Epoch 13:  20%|██        | 61/300 [00:24<01:29,  2.68it/s]Epoch 13:  21%|██        | 62/300 [00:24<01:25,  2.78it/s]Epoch 13:  21%|██        | 63/300 [00:25<01:22,  2.87it/s]Epoch 13:  21%|██▏       | 64/300 [00:25<01:20,  2.93it/s]Epoch 13:  22%|██▏       | 65/300 [00:25<01:29,  2.63it/s]Epoch 13:  22%|██▏       | 66/300 [00:26<01:27,  2.69it/s]Epoch 13:  22%|██▏       | 67/300 [00:26<01:26,  2.68it/s]Epoch 13:  23%|██▎       | 68/300 [00:27<01:26,  2.68it/s]Epoch 13:  23%|██▎       | 69/300 [00:27<01:37,  2.38it/s]Epoch 13:  23%|██▎       | 70/300 [00:27<01:33,  2.47it/s]Epoch 13:  24%|██▎       | 71/300 [00:28<01:35,  2.40it/s]Epoch 13:  24%|██▍       | 72/300 [00:28<01:40,  2.28it/s]Epoch 13:  24%|██▍       | 73/300 [00:29<01:41,  2.23it/s]Epoch 13:  25%|██▍       | 74/300 [00:29<01:34,  2.40it/s]Epoch 13:  25%|██▌       | 75/300 [00:30<01:34,  2.38it/s]Epoch 13:  25%|██▌       | 76/300 [00:30<01:35,  2.34it/s]Epoch 13:  26%|██▌       | 77/300 [00:31<01:48,  2.06it/s]Epoch 13:  26%|██▌       | 78/300 [00:31<01:39,  2.22it/s]Epoch 13:  26%|██▋       | 79/300 [00:31<01:33,  2.35it/s]06/19/2022 14:56:26 - INFO - __main__ - global step: 1990; train loss: 7.840311527252197; dev loss: 7.77267599105835
Epoch 13:  27%|██▋       | 80/300 [00:32<01:37,  2.26it/s]Epoch 13:  27%|██▋       | 81/300 [00:32<01:39,  2.20it/s]Epoch 13:  27%|██▋       | 82/300 [00:33<01:43,  2.10it/s]Epoch 13:  28%|██▊       | 83/300 [00:33<01:37,  2.22it/s]Epoch 13:  28%|██▊       | 84/300 [00:34<01:29,  2.41it/s]Epoch 13:  28%|██▊       | 85/300 [00:34<01:23,  2.56it/s]Epoch 13:  29%|██▊       | 86/300 [00:34<01:26,  2.46it/s]Epoch 13:  29%|██▉       | 87/300 [00:35<01:22,  2.57it/s]Epoch 13:  29%|██▉       | 88/300 [00:35<01:19,  2.68it/s]Epoch 13:  30%|██▉       | 89/300 [00:35<01:16,  2.74it/s]Epoch 13:  30%|███       | 90/300 [00:36<01:20,  2.60it/s]Epoch 13:  30%|███       | 91/300 [00:36<01:17,  2.71it/s]Epoch 13:  31%|███       | 92/300 [00:37<01:14,  2.78it/s]Epoch 13:  31%|███       | 93/300 [00:37<01:12,  2.85it/s]Epoch 13:  31%|███▏      | 94/300 [00:37<01:16,  2.68it/s]Epoch 13:  32%|███▏      | 95/300 [00:38<01:14,  2.76it/s]Epoch 13:  32%|███▏      | 96/300 [00:38<01:12,  2.81it/s]Epoch 13:  32%|███▏      | 97/300 [00:38<01:11,  2.86it/s]Epoch 13:  33%|███▎      | 98/300 [00:39<01:15,  2.66it/s]Epoch 13:  33%|███▎      | 99/300 [00:39<01:12,  2.76it/s]06/19/2022 14:56:33 - INFO - __main__ - global step: 2000; train loss: 7.791988372802734; dev loss: 7.575715065002441
Epoch 13:  33%|███▎      | 100/300 [00:39<01:10,  2.82it/s]Epoch 13:  34%|███▎      | 101/300 [00:40<01:09,  2.87it/s]Epoch 13:  34%|███▍      | 102/300 [00:40<01:13,  2.68it/s]Epoch 13:  34%|███▍      | 103/300 [00:40<01:10,  2.79it/s]Epoch 13:  35%|███▍      | 104/300 [00:41<01:08,  2.85it/s]Epoch 13:  35%|███▌      | 105/300 [00:41<01:08,  2.85it/s]Epoch 13:  35%|███▌      | 106/300 [00:42<01:13,  2.63it/s]Epoch 13:  36%|███▌      | 107/300 [00:42<01:10,  2.73it/s]Epoch 13:  36%|███▌      | 108/300 [00:42<01:08,  2.79it/s]Epoch 13:  36%|███▋      | 109/300 [00:43<01:06,  2.85it/s]Epoch 13:  37%|███▋      | 110/300 [00:43<01:15,  2.52it/s]Epoch 13:  37%|███▋      | 111/300 [00:44<01:19,  2.37it/s]Epoch 13:  37%|███▋      | 112/300 [00:44<01:23,  2.26it/s]Epoch 13:  38%|███▊      | 113/300 [00:45<01:23,  2.23it/s]Epoch 13:  38%|███▊      | 114/300 [00:45<01:27,  2.13it/s]Epoch 13:  38%|███▊      | 115/300 [00:45<01:24,  2.18it/s]Epoch 13:  39%|███▊      | 116/300 [00:46<01:21,  2.25it/s]Epoch 13:  39%|███▉      | 117/300 [00:46<01:15,  2.43it/s]Epoch 13:  39%|███▉      | 118/300 [00:47<01:10,  2.58it/s]Epoch 13:  40%|███▉      | 119/300 [00:47<01:17,  2.34it/s]06/19/2022 14:56:41 - INFO - __main__ - global step: 2010; train loss: 7.592983245849609; dev loss: 7.63912296295166
Epoch 13:  40%|████      | 120/300 [00:48<01:17,  2.33it/s]Epoch 13:  40%|████      | 121/300 [00:48<01:17,  2.31it/s]Epoch 13:  41%|████      | 122/300 [00:48<01:14,  2.40it/s]Epoch 13:  41%|████      | 123/300 [00:49<01:17,  2.28it/s]Epoch 13:  41%|████▏     | 124/300 [00:49<01:13,  2.39it/s]Epoch 13:  42%|████▏     | 125/300 [00:50<01:10,  2.47it/s]Epoch 13:  42%|████▏     | 126/300 [00:50<01:14,  2.34it/s]Epoch 13:  42%|████▏     | 127/300 [00:51<01:15,  2.28it/s]Epoch 13:  43%|████▎     | 128/300 [00:51<01:11,  2.39it/s]Epoch 13:  43%|████▎     | 129/300 [00:51<01:10,  2.43it/s]Epoch 13:  43%|████▎     | 130/300 [00:52<01:06,  2.54it/s]Epoch 13:  44%|████▎     | 131/300 [00:52<01:09,  2.43it/s]Epoch 13:  44%|████▍     | 132/300 [00:53<01:09,  2.42it/s]Epoch 13:  44%|████▍     | 133/300 [00:53<01:07,  2.47it/s]Epoch 13:  45%|████▍     | 134/300 [00:53<01:08,  2.43it/s]Epoch 13:  45%|████▌     | 135/300 [00:54<01:19,  2.09it/s]Epoch 13:  45%|████▌     | 136/300 [00:54<01:16,  2.15it/s]Epoch 13:  46%|████▌     | 137/300 [00:55<01:09,  2.35it/s]Epoch 13:  46%|████▌     | 138/300 [00:55<01:07,  2.41it/s]Epoch 13:  46%|████▋     | 139/300 [00:55<01:03,  2.54it/s]06/19/2022 14:56:50 - INFO - __main__ - global step: 2020; train loss: 7.389195442199707; dev loss: 7.484339714050293
Epoch 13:  47%|████▋     | 140/300 [00:56<01:08,  2.34it/s]Epoch 13:  47%|████▋     | 141/300 [00:56<01:04,  2.46it/s]Epoch 13:  47%|████▋     | 142/300 [00:57<01:02,  2.53it/s]Epoch 13:  48%|████▊     | 143/300 [00:57<01:02,  2.52it/s]Epoch 13:  48%|████▊     | 144/300 [00:58<01:05,  2.38it/s]Epoch 13:  48%|████▊     | 145/300 [00:58<01:02,  2.50it/s]Epoch 13:  49%|████▊     | 146/300 [00:58<01:00,  2.55it/s]Epoch 13:  49%|████▉     | 147/300 [00:59<00:58,  2.60it/s]Epoch 13:  49%|████▉     | 148/300 [00:59<01:02,  2.44it/s]Epoch 13:  50%|████▉     | 149/300 [00:59<00:59,  2.53it/s]Epoch 13:  50%|█████     | 150/300 [01:00<00:58,  2.59it/s]Epoch 13:  50%|█████     | 151/300 [01:00<00:57,  2.61it/s]Epoch 13:  51%|█████     | 152/300 [01:01<00:58,  2.53it/s]Epoch 13:  51%|█████     | 153/300 [01:01<00:57,  2.57it/s]Epoch 13:  51%|█████▏    | 154/300 [01:01<00:58,  2.50it/s]Epoch 13:  52%|█████▏    | 155/300 [01:02<00:55,  2.62it/s]Epoch 13:  52%|█████▏    | 156/300 [01:02<00:56,  2.53it/s]Epoch 13:  52%|█████▏    | 157/300 [01:03<00:56,  2.52it/s]Epoch 13:  53%|█████▎    | 158/300 [01:03<00:54,  2.63it/s]Epoch 13:  53%|█████▎    | 159/300 [01:03<00:53,  2.65it/s]06/19/2022 14:56:58 - INFO - __main__ - global step: 2030; train loss: 7.741175651550293; dev loss: 7.771239280700684
Epoch 13:  53%|█████▎    | 160/300 [01:04<00:58,  2.41it/s]Epoch 13:  54%|█████▎    | 161/300 [01:04<00:57,  2.42it/s]Epoch 13:  54%|█████▍    | 162/300 [01:05<00:57,  2.42it/s]Epoch 13:  54%|█████▍    | 163/300 [01:05<00:54,  2.49it/s]Epoch 13:  55%|█████▍    | 164/300 [01:06<01:03,  2.14it/s]Epoch 13:  55%|█████▌    | 165/300 [01:06<00:58,  2.29it/s]Epoch 13:  55%|█████▌    | 166/300 [01:06<00:55,  2.41it/s]Epoch 13:  56%|█████▌    | 167/300 [01:07<00:55,  2.41it/s]Epoch 13:  56%|█████▌    | 168/300 [01:07<00:53,  2.49it/s]Epoch 13:  56%|█████▋    | 169/300 [01:08<00:57,  2.29it/s]Epoch 13:  57%|█████▋    | 170/300 [01:08<00:52,  2.46it/s]Epoch 13:  57%|█████▋    | 171/300 [01:08<00:52,  2.46it/s]Epoch 13:  57%|█████▋    | 172/300 [01:09<00:53,  2.38it/s]Epoch 13:  58%|█████▊    | 173/300 [01:09<00:59,  2.13it/s]Epoch 13:  58%|█████▊    | 174/300 [01:10<00:54,  2.32it/s]Epoch 13:  58%|█████▊    | 175/300 [01:10<00:50,  2.46it/s]Epoch 13:  59%|█████▊    | 176/300 [01:11<00:47,  2.60it/s]Epoch 13:  59%|█████▉    | 177/300 [01:11<00:48,  2.52it/s]Epoch 13:  59%|█████▉    | 178/300 [01:11<00:46,  2.63it/s]Epoch 13:  60%|█████▉    | 179/300 [01:12<00:44,  2.73it/s]06/19/2022 14:57:06 - INFO - __main__ - global step: 2040; train loss: 7.687411308288574; dev loss: 7.601210117340088
Epoch 13:  60%|██████    | 180/300 [01:12<00:45,  2.63it/s]Epoch 13:  60%|██████    | 181/300 [01:13<00:49,  2.40it/s]Epoch 13:  61%|██████    | 182/300 [01:13<00:53,  2.20it/s]Epoch 13:  61%|██████    | 183/300 [01:13<00:50,  2.30it/s]Epoch 13:  61%|██████▏   | 184/300 [01:14<00:50,  2.29it/s]Epoch 13:  62%|██████▏   | 185/300 [01:14<00:53,  2.14it/s]Epoch 13:  62%|██████▏   | 186/300 [01:15<00:52,  2.17it/s]Epoch 13:  62%|██████▏   | 187/300 [01:15<00:51,  2.20it/s]Epoch 13:  63%|██████▎   | 188/300 [01:16<00:50,  2.22it/s]Epoch 13:  63%|██████▎   | 189/300 [01:16<00:50,  2.21it/s]Epoch 13:  63%|██████▎   | 190/300 [01:17<00:45,  2.40it/s]Epoch 13:  64%|██████▎   | 191/300 [01:17<00:42,  2.55it/s]Epoch 13:  64%|██████▍   | 192/300 [01:17<00:41,  2.63it/s]Epoch 13:  64%|██████▍   | 193/300 [01:18<00:38,  2.75it/s]Epoch 13:  65%|██████▍   | 194/300 [01:18<00:40,  2.62it/s]Epoch 13:  65%|██████▌   | 195/300 [01:18<00:38,  2.74it/s]Epoch 13:  65%|██████▌   | 196/300 [01:19<00:36,  2.83it/s]Epoch 13:  66%|██████▌   | 197/300 [01:19<00:35,  2.88it/s]Epoch 13:  66%|██████▌   | 198/300 [01:19<00:39,  2.56it/s]Epoch 13:  66%|██████▋   | 199/300 [01:20<00:38,  2.60it/s]06/19/2022 14:57:14 - INFO - __main__ - global step: 2050; train loss: 7.97683048248291; dev loss: 7.798774719238281
Epoch 13:  67%|██████▋   | 200/300 [01:20<00:42,  2.33it/s]Epoch 13:  67%|██████▋   | 201/300 [01:21<00:42,  2.34it/s]Epoch 13:  67%|██████▋   | 202/300 [01:21<00:47,  2.06it/s]Epoch 13:  68%|██████▊   | 203/300 [01:22<00:43,  2.22it/s]Epoch 13:  68%|██████▊   | 204/300 [01:22<00:44,  2.15it/s]Epoch 13:  68%|██████▊   | 205/300 [01:23<00:43,  2.20it/s]Epoch 13:  69%|██████▊   | 206/300 [01:23<00:42,  2.23it/s]Epoch 13:  69%|██████▉   | 207/300 [01:23<00:38,  2.43it/s]Epoch 13:  69%|██████▉   | 208/300 [01:24<00:35,  2.59it/s]Epoch 13:  70%|██████▉   | 209/300 [01:24<00:33,  2.69it/s]Epoch 13:  70%|███████   | 210/300 [01:25<00:38,  2.34it/s]Epoch 13:  70%|███████   | 211/300 [01:25<00:38,  2.33it/s]Epoch 13:  71%|███████   | 212/300 [01:26<00:39,  2.21it/s]Epoch 13:  71%|███████   | 213/300 [01:26<00:40,  2.17it/s]Epoch 13:  71%|███████▏  | 214/300 [01:27<00:40,  2.14it/s]Epoch 13:  72%|███████▏  | 215/300 [01:27<00:37,  2.29it/s]Epoch 13:  72%|███████▏  | 216/300 [01:27<00:34,  2.41it/s]Epoch 13:  72%|███████▏  | 217/300 [01:28<00:32,  2.54it/s]Epoch 13:  73%|███████▎  | 218/300 [01:28<00:33,  2.47it/s]Epoch 13:  73%|███████▎  | 219/300 [01:28<00:32,  2.47it/s]06/19/2022 14:57:23 - INFO - __main__ - global step: 2060; train loss: 7.551368713378906; dev loss: 7.6645708084106445
Epoch 13:  73%|███████▎  | 220/300 [01:29<00:30,  2.59it/s]Epoch 13:  74%|███████▎  | 221/300 [01:29<00:31,  2.48it/s]Epoch 13:  74%|███████▍  | 222/300 [01:30<00:34,  2.26it/s]Epoch 13:  74%|███████▍  | 223/300 [01:30<00:37,  2.07it/s]Epoch 13:  75%|███████▍  | 224/300 [01:31<00:39,  1.94it/s]Epoch 13:  75%|███████▌  | 225/300 [01:31<00:34,  2.14it/s]Epoch 13:  75%|███████▌  | 226/300 [01:32<00:32,  2.30it/s]Epoch 13:  76%|███████▌  | 227/300 [01:32<00:31,  2.29it/s]Epoch 13:  76%|███████▌  | 228/300 [01:33<00:30,  2.39it/s]Epoch 13:  76%|███████▋  | 229/300 [01:33<00:32,  2.21it/s]Epoch 13:  77%|███████▋  | 230/300 [01:33<00:29,  2.34it/s]Epoch 13:  77%|███████▋  | 231/300 [01:34<00:34,  2.01it/s]Epoch 13:  77%|███████▋  | 232/300 [01:34<00:32,  2.12it/s]Epoch 13:  78%|███████▊  | 233/300 [01:35<00:30,  2.23it/s]Epoch 13:  78%|███████▊  | 234/300 [01:35<00:27,  2.38it/s]Epoch 13:  78%|███████▊  | 235/300 [01:36<00:27,  2.35it/s]Epoch 13:  79%|███████▊  | 236/300 [01:36<00:25,  2.49it/s]Epoch 13:  79%|███████▉  | 237/300 [01:36<00:24,  2.60it/s]Epoch 13:  79%|███████▉  | 238/300 [01:37<00:23,  2.69it/s]Epoch 13:  80%|███████▉  | 239/300 [01:37<00:23,  2.55it/s]06/19/2022 14:57:31 - INFO - __main__ - global step: 2070; train loss: 7.489598274230957; dev loss: 7.393254280090332
Epoch 13:  80%|████████  | 240/300 [01:37<00:22,  2.64it/s]Epoch 13:  80%|████████  | 241/300 [01:38<00:21,  2.72it/s]Epoch 13:  81%|████████  | 242/300 [01:38<00:20,  2.77it/s]Epoch 13:  81%|████████  | 243/300 [01:39<00:22,  2.56it/s]Epoch 13:  81%|████████▏ | 244/300 [01:39<00:21,  2.64it/s]Epoch 13:  82%|████████▏ | 245/300 [01:39<00:20,  2.71it/s]Epoch 13:  82%|████████▏ | 246/300 [01:40<00:19,  2.77it/s]Epoch 13:  82%|████████▏ | 247/300 [01:40<00:19,  2.73it/s]Epoch 13:  83%|████████▎ | 248/300 [01:41<00:20,  2.51it/s]Epoch 13:  83%|████████▎ | 249/300 [01:41<00:21,  2.41it/s]Epoch 13:  83%|████████▎ | 250/300 [01:41<00:21,  2.36it/s]Epoch 13:  84%|████████▎ | 251/300 [01:42<00:20,  2.34it/s]Epoch 13:  84%|████████▍ | 252/300 [01:42<00:21,  2.23it/s]Epoch 13:  84%|████████▍ | 253/300 [01:43<00:20,  2.26it/s]Epoch 13:  85%|████████▍ | 254/300 [01:43<00:20,  2.23it/s]Epoch 13:  85%|████████▌ | 255/300 [01:44<00:20,  2.21it/s]Epoch 13:  85%|████████▌ | 256/300 [01:44<00:21,  2.04it/s]Epoch 13:  86%|████████▌ | 257/300 [01:45<00:21,  1.97it/s]Epoch 13:  86%|████████▌ | 258/300 [01:45<00:20,  2.02it/s]Epoch 13:  86%|████████▋ | 259/300 [01:46<00:20,  2.05it/s]06/19/2022 14:57:40 - INFO - __main__ - global step: 2080; train loss: 7.147435665130615; dev loss: 7.079527854919434
Epoch 13:  87%|████████▋ | 260/300 [01:46<00:20,  1.94it/s]Epoch 13:  87%|████████▋ | 261/300 [01:47<00:18,  2.10it/s]Epoch 13:  87%|████████▋ | 262/300 [01:47<00:17,  2.15it/s]Epoch 13:  88%|████████▊ | 263/300 [01:48<00:15,  2.33it/s]Epoch 13:  88%|████████▊ | 264/300 [01:48<00:15,  2.30it/s]Epoch 13:  88%|████████▊ | 265/300 [01:48<00:14,  2.45it/s]Epoch 13:  89%|████████▊ | 266/300 [01:49<00:13,  2.56it/s]Epoch 13:  89%|████████▉ | 267/300 [01:49<00:12,  2.61it/s]Epoch 13:  89%|████████▉ | 268/300 [01:50<00:13,  2.45it/s]Epoch 13:  90%|████████▉ | 269/300 [01:50<00:12,  2.57it/s]Epoch 13:  90%|█████████ | 270/300 [01:50<00:11,  2.58it/s]Epoch 13:  90%|█████████ | 271/300 [01:51<00:11,  2.60it/s]Epoch 13:  91%|█████████ | 272/300 [01:51<00:11,  2.40it/s]Epoch 13:  91%|█████████ | 273/300 [01:52<00:11,  2.31it/s]Epoch 13:  91%|█████████▏| 274/300 [01:52<00:11,  2.27it/s]Epoch 13:  92%|█████████▏| 275/300 [01:53<00:11,  2.19it/s]Epoch 13:  92%|█████████▏| 276/300 [01:53<00:10,  2.19it/s]Epoch 13:  92%|█████████▏| 277/300 [01:53<00:10,  2.12it/s]Epoch 13:  93%|█████████▎| 278/300 [01:54<00:09,  2.21it/s]Epoch 13:  93%|█████████▎| 279/300 [01:54<00:08,  2.40it/s]06/19/2022 14:57:48 - INFO - __main__ - global step: 2090; train loss: 7.843145847320557; dev loss: 7.524806022644043
Epoch 13:  93%|█████████▎| 280/300 [01:55<00:07,  2.55it/s]Epoch 13:  94%|█████████▎| 281/300 [01:55<00:07,  2.47it/s]Epoch 13:  94%|█████████▍| 282/300 [01:55<00:07,  2.46it/s]Epoch 13:  94%|█████████▍| 283/300 [01:56<00:06,  2.59it/s]Epoch 13:  95%|█████████▍| 284/300 [01:56<00:06,  2.55it/s]Epoch 13:  95%|█████████▌| 285/300 [01:57<00:06,  2.41it/s]Epoch 13:  95%|█████████▌| 286/300 [01:57<00:05,  2.55it/s]Epoch 13:  96%|█████████▌| 287/300 [01:57<00:04,  2.67it/s]Epoch 13:  96%|█████████▌| 288/300 [01:58<00:04,  2.77it/s]Epoch 13:  96%|█████████▋| 289/300 [01:58<00:04,  2.56it/s]Epoch 13:  97%|█████████▋| 290/300 [01:59<00:04,  2.40it/s]Epoch 13:  97%|█████████▋| 291/300 [01:59<00:03,  2.36it/s]Epoch 13:  97%|█████████▋| 292/300 [01:59<00:03,  2.33it/s]Epoch 13:  98%|█████████▊| 293/300 [02:00<00:03,  2.15it/s]Epoch 13:  98%|█████████▊| 294/300 [02:00<00:02,  2.18it/s]Epoch 13:  98%|█████████▊| 295/300 [02:01<00:02,  2.08it/s]Epoch 13:  99%|█████████▊| 296/300 [02:01<00:01,  2.09it/s]Epoch 13:  99%|█████████▉| 297/300 [02:02<00:01,  2.06it/s]Epoch 13:  99%|█████████▉| 298/300 [02:03<00:01,  1.93it/s]Epoch 13: 100%|█████████▉| 299/300 [02:03<00:00,  2.01it/s]06/19/2022 14:57:57 - INFO - __main__ - global step: 2100; train loss: 7.4684882164001465; dev loss: 7.609961032867432
Epoch 13: 100%|██████████| 300/300 [02:03<00:00,  2.09it/s]Epoch 13: 100%|██████████| 300/300 [02:03<00:00,  2.42it/s]
Epoch 14:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 14:   0%|          | 1/300 [00:00<02:07,  2.35it/s]Epoch 14:   1%|          | 2/300 [00:01<02:33,  1.94it/s]Epoch 14:   1%|          | 3/300 [00:01<02:43,  1.81it/s]Epoch 14:   1%|▏         | 4/300 [00:02<02:34,  1.91it/s]Epoch 14:   2%|▏         | 5/300 [00:02<02:18,  2.13it/s]Epoch 14:   2%|▏         | 6/300 [00:02<02:14,  2.19it/s]Epoch 14:   2%|▏         | 7/300 [00:03<02:01,  2.42it/s]Epoch 14:   3%|▎         | 8/300 [00:03<01:53,  2.57it/s]Epoch 14:   3%|▎         | 9/300 [00:03<01:56,  2.49it/s]Epoch 14:   3%|▎         | 10/300 [00:04<02:10,  2.22it/s]Epoch 14:   4%|▎         | 11/300 [00:05<02:12,  2.19it/s]Epoch 14:   4%|▍         | 12/300 [00:05<02:09,  2.23it/s]Epoch 14:   4%|▍         | 13/300 [00:05<01:58,  2.42it/s]Epoch 14:   5%|▍         | 14/300 [00:06<02:02,  2.34it/s]Epoch 14:   5%|▌         | 15/300 [00:06<01:55,  2.46it/s]Epoch 14:   5%|▌         | 16/300 [00:06<01:48,  2.61it/s]Epoch 14:   6%|▌         | 17/300 [00:07<01:53,  2.50it/s]Epoch 14:   6%|▌         | 18/300 [00:07<02:07,  2.21it/s]Epoch 14:   6%|▋         | 19/300 [00:08<02:05,  2.24it/s]06/19/2022 14:58:06 - INFO - __main__ - global step: 2110; train loss: 7.552053928375244; dev loss: 7.586220741271973
Epoch 14:   7%|▋         | 20/300 [00:08<02:02,  2.28it/s]Epoch 14:   7%|▋         | 21/300 [00:09<01:57,  2.38it/s]Epoch 14:   7%|▋         | 22/300 [00:09<02:01,  2.29it/s]Epoch 14:   8%|▊         | 23/300 [00:10<01:57,  2.35it/s]Epoch 14:   8%|▊         | 24/300 [00:10<01:52,  2.45it/s]Epoch 14:   8%|▊         | 25/300 [00:10<01:46,  2.58it/s]Epoch 14:   9%|▊         | 26/300 [00:11<01:54,  2.40it/s]Epoch 14:   9%|▉         | 27/300 [00:11<01:50,  2.46it/s]Epoch 14:   9%|▉         | 28/300 [00:11<01:45,  2.58it/s]Epoch 14:  10%|▉         | 29/300 [00:12<01:48,  2.50it/s]Epoch 14:  10%|█         | 30/300 [00:12<01:50,  2.43it/s]Epoch 14:  10%|█         | 31/300 [00:13<02:03,  2.18it/s]Epoch 14:  11%|█         | 32/300 [00:13<02:05,  2.13it/s]Epoch 14:  11%|█         | 33/300 [00:14<01:54,  2.33it/s]Epoch 14:  11%|█▏        | 34/300 [00:14<01:46,  2.49it/s]Epoch 14:  12%|█▏        | 35/300 [00:14<01:47,  2.45it/s]Epoch 14:  12%|█▏        | 36/300 [00:15<01:41,  2.59it/s]Epoch 14:  12%|█▏        | 37/300 [00:15<01:37,  2.70it/s]Epoch 14:  13%|█▎        | 38/300 [00:15<01:34,  2.78it/s]Epoch 14:  13%|█▎        | 39/300 [00:16<01:38,  2.64it/s]06/19/2022 14:58:14 - INFO - __main__ - global step: 2120; train loss: 7.442537784576416; dev loss: 7.371634006500244
Epoch 14:  13%|█▎        | 40/300 [00:16<01:34,  2.74it/s]Epoch 14:  14%|█▎        | 41/300 [00:17<01:33,  2.77it/s]Epoch 14:  14%|█▍        | 42/300 [00:17<01:39,  2.59it/s]Epoch 14:  14%|█▍        | 43/300 [00:18<02:00,  2.14it/s]Epoch 14:  15%|█▍        | 44/300 [00:18<01:58,  2.17it/s]Epoch 14:  15%|█▌        | 45/300 [00:19<01:55,  2.21it/s]Epoch 14:  15%|█▌        | 46/300 [00:19<01:55,  2.20it/s]Epoch 14:  16%|█▌        | 47/300 [00:20<02:03,  2.05it/s]Epoch 14:  16%|█▌        | 48/300 [00:20<02:01,  2.08it/s]Epoch 14:  16%|█▋        | 49/300 [00:20<01:50,  2.27it/s]Epoch 14:  17%|█▋        | 50/300 [00:21<01:41,  2.46it/s]Epoch 14:  17%|█▋        | 51/300 [00:21<01:42,  2.42it/s]Epoch 14:  17%|█▋        | 52/300 [00:21<01:36,  2.58it/s]Epoch 14:  18%|█▊        | 53/300 [00:22<01:36,  2.57it/s]Epoch 14:  18%|█▊        | 54/300 [00:22<01:36,  2.54it/s]Epoch 14:  18%|█▊        | 55/300 [00:23<01:33,  2.63it/s]Epoch 14:  19%|█▊        | 56/300 [00:23<01:36,  2.52it/s]Epoch 14:  19%|█▉        | 57/300 [00:23<01:31,  2.66it/s]Epoch 14:  19%|█▉        | 58/300 [00:24<01:30,  2.66it/s]Epoch 14:  20%|█▉        | 59/300 [00:24<01:35,  2.51it/s]06/19/2022 14:58:22 - INFO - __main__ - global step: 2130; train loss: 7.602746486663818; dev loss: 7.7023491859436035
Epoch 14:  20%|██        | 60/300 [00:25<01:38,  2.43it/s]Epoch 14:  20%|██        | 61/300 [00:25<01:36,  2.49it/s]Epoch 14:  21%|██        | 62/300 [00:25<01:33,  2.56it/s]Epoch 14:  21%|██        | 63/300 [00:26<01:32,  2.56it/s]Epoch 14:  21%|██▏       | 64/300 [00:26<01:48,  2.17it/s]Epoch 14:  22%|██▏       | 65/300 [00:27<01:41,  2.32it/s]Epoch 14:  22%|██▏       | 66/300 [00:27<01:39,  2.34it/s]Epoch 14:  22%|██▏       | 67/300 [00:28<01:40,  2.31it/s]Epoch 14:  23%|██▎       | 68/300 [00:28<01:47,  2.16it/s]Epoch 14:  23%|██▎       | 69/300 [00:29<01:46,  2.18it/s]Epoch 14:  23%|██▎       | 70/300 [00:29<01:43,  2.22it/s]Epoch 14:  24%|██▎       | 71/300 [00:29<01:39,  2.30it/s]Epoch 14:  24%|██▍       | 72/300 [00:30<01:47,  2.12it/s]Epoch 14:  24%|██▍       | 73/300 [00:30<01:44,  2.18it/s]Epoch 14:  25%|██▍       | 74/300 [00:31<01:39,  2.26it/s]Epoch 14:  25%|██▌       | 75/300 [00:31<01:35,  2.36it/s]Epoch 14:  25%|██▌       | 76/300 [00:32<01:38,  2.28it/s]Epoch 14:  26%|██▌       | 77/300 [00:32<01:34,  2.37it/s]Epoch 14:  26%|██▌       | 78/300 [00:32<01:31,  2.44it/s]Epoch 14:  26%|██▋       | 79/300 [00:33<01:29,  2.47it/s]06/19/2022 14:58:31 - INFO - __main__ - global step: 2140; train loss: 7.3809404373168945; dev loss: 7.071483612060547
Epoch 14:  27%|██▋       | 80/300 [00:33<01:41,  2.17it/s]Epoch 14:  27%|██▋       | 81/300 [00:34<01:41,  2.16it/s]Epoch 14:  27%|██▋       | 82/300 [00:34<01:39,  2.20it/s]Epoch 14:  28%|██▊       | 83/300 [00:35<01:33,  2.32it/s]Epoch 14:  28%|██▊       | 84/300 [00:35<01:33,  2.31it/s]Epoch 14:  28%|██▊       | 85/300 [00:36<01:36,  2.23it/s]Epoch 14:  29%|██▊       | 86/300 [00:36<01:31,  2.33it/s]Epoch 14:  29%|██▉       | 87/300 [00:36<01:28,  2.41it/s]Epoch 14:  29%|██▉       | 88/300 [00:37<01:24,  2.51it/s]Epoch 14:  30%|██▉       | 89/300 [00:37<01:26,  2.43it/s]Epoch 14:  30%|███       | 90/300 [00:38<01:25,  2.44it/s]Epoch 14:  30%|███       | 91/300 [00:38<01:22,  2.53it/s]Epoch 14:  31%|███       | 92/300 [00:38<01:19,  2.62it/s]Epoch 14:  31%|███       | 93/300 [00:39<01:22,  2.50it/s]Epoch 14:  31%|███▏      | 94/300 [00:39<01:23,  2.46it/s]Epoch 14:  32%|███▏      | 95/300 [00:40<01:26,  2.36it/s]Epoch 14:  32%|███▏      | 96/300 [00:40<01:28,  2.31it/s]Epoch 14:  32%|███▏      | 97/300 [00:41<01:36,  2.11it/s]Epoch 14:  33%|███▎      | 98/300 [00:41<01:32,  2.18it/s]Epoch 14:  33%|███▎      | 99/300 [00:41<01:25,  2.35it/s]06/19/2022 14:58:40 - INFO - __main__ - global step: 2150; train loss: 8.223021507263184; dev loss: 8.208298683166504
Epoch 14:  33%|███▎      | 100/300 [00:42<01:20,  2.49it/s]Epoch 14:  34%|███▎      | 101/300 [00:42<01:22,  2.42it/s]Epoch 14:  34%|███▍      | 102/300 [00:43<01:22,  2.39it/s]Epoch 14:  34%|███▍      | 103/300 [00:43<01:24,  2.33it/s]Epoch 14:  35%|███▍      | 104/300 [00:44<01:25,  2.29it/s]Epoch 14:  35%|███▌      | 105/300 [00:44<01:34,  2.05it/s]Epoch 14:  35%|███▌      | 106/300 [00:45<01:26,  2.24it/s]Epoch 14:  36%|███▌      | 107/300 [00:45<01:20,  2.40it/s]Epoch 14:  36%|███▌      | 108/300 [00:45<01:19,  2.42it/s]Epoch 14:  36%|███▋      | 109/300 [00:46<01:20,  2.38it/s]Epoch 14:  37%|███▋      | 110/300 [00:46<01:30,  2.09it/s]Epoch 14:  37%|███▋      | 111/300 [00:47<01:31,  2.06it/s]Epoch 14:  37%|███▋      | 112/300 [00:47<01:36,  1.95it/s]Epoch 14:  38%|███▊      | 113/300 [00:48<01:29,  2.08it/s]Epoch 14:  38%|███▊      | 114/300 [00:48<01:29,  2.07it/s]Epoch 14:  38%|███▊      | 115/300 [00:49<01:22,  2.25it/s]Epoch 14:  39%|███▊      | 116/300 [00:49<01:18,  2.36it/s]Epoch 14:  39%|███▉      | 117/300 [00:49<01:15,  2.43it/s]Epoch 14:  39%|███▉      | 118/300 [00:50<01:30,  2.02it/s]Epoch 14:  40%|███▉      | 119/300 [00:51<01:28,  2.06it/s]06/19/2022 14:58:49 - INFO - __main__ - global step: 2160; train loss: 7.538691520690918; dev loss: 7.667881965637207
Epoch 14:  40%|████      | 120/300 [00:51<01:27,  2.06it/s]Epoch 14:  40%|████      | 121/300 [00:52<01:26,  2.08it/s]Epoch 14:  41%|████      | 122/300 [00:52<01:32,  1.93it/s]Epoch 14:  41%|████      | 123/300 [00:53<01:28,  2.00it/s]Epoch 14:  41%|████▏     | 124/300 [00:53<01:25,  2.07it/s]Epoch 14:  42%|████▏     | 125/300 [00:53<01:17,  2.25it/s]Epoch 14:  42%|████▏     | 126/300 [00:54<01:18,  2.21it/s]Epoch 14:  42%|████▏     | 127/300 [00:54<01:16,  2.25it/s]Epoch 14:  43%|████▎     | 128/300 [00:55<01:13,  2.35it/s]Epoch 14:  43%|████▎     | 129/300 [00:55<01:12,  2.37it/s]Epoch 14:  43%|████▎     | 130/300 [00:56<01:17,  2.19it/s]Epoch 14:  44%|████▎     | 131/300 [00:56<01:13,  2.29it/s]Epoch 14:  44%|████▍     | 132/300 [00:56<01:12,  2.31it/s]Epoch 14:  44%|████▍     | 133/300 [00:57<01:10,  2.38it/s]Epoch 14:  45%|████▍     | 134/300 [00:57<01:14,  2.23it/s]Epoch 14:  45%|████▌     | 135/300 [00:58<01:13,  2.23it/s]Epoch 14:  45%|████▌     | 136/300 [00:58<01:13,  2.24it/s]Epoch 14:  46%|████▌     | 137/300 [00:59<01:08,  2.38it/s]Epoch 14:  46%|████▌     | 138/300 [00:59<01:06,  2.44it/s]Epoch 14:  46%|████▋     | 139/300 [01:00<01:15,  2.13it/s]06/19/2022 14:58:58 - INFO - __main__ - global step: 2170; train loss: 6.850930213928223; dev loss: 7.365293025970459
Epoch 14:  47%|████▋     | 140/300 [01:00<01:17,  2.06it/s]Epoch 14:  47%|████▋     | 141/300 [01:01<01:12,  2.20it/s]Epoch 14:  47%|████▋     | 142/300 [01:01<01:12,  2.18it/s]Epoch 14:  48%|████▊     | 143/300 [01:01<01:12,  2.17it/s]Epoch 14:  48%|████▊     | 144/300 [01:02<01:06,  2.33it/s]Epoch 14:  48%|████▊     | 145/300 [01:02<01:04,  2.40it/s]Epoch 14:  49%|████▊     | 146/300 [01:03<01:01,  2.50it/s]Epoch 14:  49%|████▉     | 147/300 [01:03<01:05,  2.34it/s]Epoch 14:  49%|████▉     | 148/300 [01:03<01:05,  2.34it/s]Epoch 14:  50%|████▉     | 149/300 [01:04<01:06,  2.27it/s]Epoch 14:  50%|█████     | 150/300 [01:04<01:07,  2.23it/s]Epoch 14:  50%|█████     | 151/300 [01:05<01:17,  1.93it/s]Epoch 14:  51%|█████     | 152/300 [01:06<01:15,  1.95it/s]Epoch 14:  51%|█████     | 153/300 [01:06<01:16,  1.92it/s]Epoch 14:  51%|█████▏    | 154/300 [01:07<01:14,  1.97it/s]Epoch 14:  52%|█████▏    | 155/300 [01:07<01:13,  1.96it/s]Epoch 14:  52%|█████▏    | 156/300 [01:08<01:08,  2.09it/s]Epoch 14:  52%|█████▏    | 157/300 [01:08<01:05,  2.17it/s]Epoch 14:  53%|█████▎    | 158/300 [01:08<01:04,  2.20it/s]Epoch 14:  53%|█████▎    | 159/300 [01:09<01:04,  2.17it/s]06/19/2022 14:59:07 - INFO - __main__ - global step: 2180; train loss: 7.099967002868652; dev loss: 7.1175971031188965
Epoch 14:  53%|█████▎    | 160/300 [01:09<01:00,  2.32it/s]Epoch 14:  54%|█████▎    | 161/300 [01:10<00:56,  2.46it/s]Epoch 14:  54%|█████▍    | 162/300 [01:10<00:55,  2.51it/s]Epoch 14:  54%|█████▍    | 163/300 [01:10<00:54,  2.51it/s]Epoch 14:  55%|█████▍    | 164/300 [01:11<01:00,  2.25it/s]Epoch 14:  55%|█████▌    | 165/300 [01:11<00:56,  2.39it/s]Epoch 14:  55%|█████▌    | 166/300 [01:12<00:57,  2.33it/s]Epoch 14:  56%|█████▌    | 167/300 [01:12<00:55,  2.39it/s]Epoch 14:  56%|█████▌    | 168/300 [01:13<01:06,  1.98it/s]Epoch 14:  56%|█████▋    | 169/300 [01:13<01:01,  2.12it/s]Epoch 14:  57%|█████▋    | 170/300 [01:14<01:00,  2.16it/s]Epoch 14:  57%|█████▋    | 171/300 [01:14<00:59,  2.15it/s]Epoch 14:  57%|█████▋    | 172/300 [01:15<01:07,  1.90it/s]Epoch 14:  58%|█████▊    | 173/300 [01:15<01:07,  1.89it/s]Epoch 14:  58%|█████▊    | 174/300 [01:16<01:01,  2.05it/s]Epoch 14:  58%|█████▊    | 175/300 [01:16<01:00,  2.06it/s]Epoch 14:  59%|█████▊    | 176/300 [01:17<01:06,  1.86it/s]Epoch 14:  59%|█████▉    | 177/300 [01:17<01:02,  1.98it/s]Epoch 14:  59%|█████▉    | 178/300 [01:18<00:59,  2.05it/s]Epoch 14:  60%|█████▉    | 179/300 [01:18<01:01,  1.98it/s]06/19/2022 14:59:17 - INFO - __main__ - global step: 2190; train loss: 7.075573921203613; dev loss: 7.195322513580322
Epoch 14:  60%|██████    | 180/300 [01:19<01:04,  1.85it/s]Epoch 14:  60%|██████    | 181/300 [01:19<00:59,  1.99it/s]Epoch 14:  61%|██████    | 182/300 [01:20<00:54,  2.17it/s]Epoch 14:  61%|██████    | 183/300 [01:20<00:51,  2.28it/s]Epoch 14:  61%|██████▏   | 184/300 [01:21<00:55,  2.09it/s]Epoch 14:  62%|██████▏   | 185/300 [01:21<00:54,  2.11it/s]Epoch 14:  62%|██████▏   | 186/300 [01:22<00:55,  2.05it/s]Epoch 14:  62%|██████▏   | 187/300 [01:22<00:56,  2.01it/s]Epoch 14:  63%|██████▎   | 188/300 [01:23<01:02,  1.78it/s]Epoch 14:  63%|██████▎   | 189/300 [01:23<00:57,  1.93it/s]Epoch 14:  63%|██████▎   | 190/300 [01:24<00:55,  1.97it/s]Epoch 14:  64%|██████▎   | 191/300 [01:24<00:56,  1.91it/s]Epoch 14:  64%|██████▍   | 192/300 [01:25<00:53,  2.04it/s]Epoch 14:  64%|██████▍   | 193/300 [01:25<00:53,  2.00it/s]Epoch 14:  65%|██████▍   | 194/300 [01:26<00:49,  2.14it/s]Epoch 14:  65%|██████▌   | 195/300 [01:26<00:47,  2.19it/s]Epoch 14:  65%|██████▌   | 196/300 [01:27<00:48,  2.13it/s]Epoch 14:  66%|██████▌   | 197/300 [01:27<00:49,  2.08it/s]Epoch 14:  66%|██████▌   | 198/300 [01:27<00:46,  2.21it/s]Epoch 14:  66%|██████▋   | 199/300 [01:28<00:43,  2.30it/s]06/19/2022 14:59:26 - INFO - __main__ - global step: 2200; train loss: 7.988574028015137; dev loss: 7.925485134124756
Epoch 14:  67%|██████▋   | 200/300 [01:28<00:41,  2.41it/s]Epoch 14:  67%|██████▋   | 201/300 [01:29<00:43,  2.27it/s]Epoch 14:  67%|██████▋   | 202/300 [01:29<00:40,  2.39it/s]Epoch 14:  68%|██████▊   | 203/300 [01:29<00:38,  2.50it/s]Epoch 14:  68%|██████▊   | 204/300 [01:30<00:40,  2.35it/s]Epoch 14:  68%|██████▊   | 205/300 [01:31<00:45,  2.09it/s]Epoch 14:  69%|██████▊   | 206/300 [01:31<00:46,  2.02it/s]Epoch 14:  69%|██████▉   | 207/300 [01:31<00:42,  2.17it/s]Epoch 14:  69%|██████▉   | 208/300 [01:32<00:39,  2.32it/s]Epoch 14:  70%|██████▉   | 209/300 [01:32<00:40,  2.27it/s]Epoch 14:  70%|███████   | 210/300 [01:33<00:37,  2.41it/s]Epoch 14:  70%|███████   | 211/300 [01:33<00:35,  2.47it/s]Epoch 14:  71%|███████   | 212/300 [01:33<00:34,  2.56it/s]Epoch 14:  71%|███████   | 213/300 [01:34<00:36,  2.40it/s]Epoch 14:  71%|███████▏  | 214/300 [01:34<00:34,  2.51it/s]Epoch 14:  72%|███████▏  | 215/300 [01:35<00:34,  2.45it/s]Epoch 14:  72%|███████▏  | 216/300 [01:35<00:34,  2.43it/s]Epoch 14:  72%|███████▏  | 217/300 [01:35<00:33,  2.45it/s]Epoch 14:  73%|███████▎  | 218/300 [01:36<00:35,  2.33it/s]Epoch 14:  73%|███████▎  | 219/300 [01:36<00:33,  2.44it/s]06/19/2022 14:59:34 - INFO - __main__ - global step: 2210; train loss: 7.353026390075684; dev loss: 7.4375410079956055
Epoch 14:  73%|███████▎  | 220/300 [01:37<00:31,  2.52it/s]Epoch 14:  74%|███████▎  | 221/300 [01:37<00:30,  2.59it/s]Epoch 14:  74%|███████▍  | 222/300 [01:37<00:31,  2.45it/s]Epoch 14:  74%|███████▍  | 223/300 [01:38<00:30,  2.55it/s]Epoch 14:  75%|███████▍  | 224/300 [01:38<00:31,  2.42it/s]Epoch 14:  75%|███████▌  | 225/300 [01:39<00:30,  2.46it/s]Epoch 14:  75%|███████▌  | 226/300 [01:39<00:32,  2.30it/s]Epoch 14:  76%|███████▌  | 227/300 [01:40<00:32,  2.24it/s]Epoch 14:  76%|███████▌  | 228/300 [01:40<00:30,  2.36it/s]Epoch 14:  76%|███████▋  | 229/300 [01:40<00:28,  2.48it/s]Epoch 14:  77%|███████▋  | 230/300 [01:41<00:30,  2.30it/s]Epoch 14:  77%|███████▋  | 231/300 [01:41<00:30,  2.30it/s]Epoch 14:  77%|███████▋  | 232/300 [01:42<00:27,  2.43it/s]Epoch 14:  78%|███████▊  | 233/300 [01:42<00:26,  2.52it/s]Epoch 14:  78%|███████▊  | 234/300 [01:43<00:27,  2.37it/s]Epoch 14:  78%|███████▊  | 235/300 [01:43<00:26,  2.49it/s]Epoch 14:  79%|███████▊  | 236/300 [01:43<00:24,  2.58it/s]Epoch 14:  79%|███████▉  | 237/300 [01:44<00:25,  2.48it/s]Epoch 14:  79%|███████▉  | 238/300 [01:44<00:27,  2.23it/s]Epoch 14:  80%|███████▉  | 239/300 [01:45<00:27,  2.26it/s]06/19/2022 14:59:43 - INFO - __main__ - global step: 2220; train loss: 8.0704984664917; dev loss: 8.062148094177246
Epoch 14:  80%|████████  | 240/300 [01:45<00:25,  2.39it/s]Epoch 14:  80%|████████  | 241/300 [01:45<00:24,  2.37it/s]Epoch 14:  81%|████████  | 242/300 [01:46<00:25,  2.29it/s]Epoch 14:  81%|████████  | 243/300 [01:46<00:23,  2.43it/s]Epoch 14:  81%|████████▏ | 244/300 [01:47<00:23,  2.39it/s]Epoch 14:  82%|████████▏ | 245/300 [01:47<00:22,  2.48it/s]Epoch 14:  82%|████████▏ | 246/300 [01:47<00:21,  2.57it/s]Epoch 14:  82%|████████▏ | 247/300 [01:48<00:22,  2.38it/s]Epoch 14:  83%|████████▎ | 248/300 [01:48<00:22,  2.34it/s]Epoch 14:  83%|████████▎ | 249/300 [01:49<00:21,  2.37it/s]Epoch 14:  83%|████████▎ | 250/300 [01:49<00:23,  2.15it/s]Epoch 14:  84%|████████▎ | 251/300 [01:50<00:23,  2.09it/s]Epoch 14:  84%|████████▍ | 252/300 [01:50<00:22,  2.17it/s]Epoch 14:  84%|████████▍ | 253/300 [01:51<00:21,  2.15it/s]Epoch 14:  85%|████████▍ | 254/300 [01:51<00:22,  2.06it/s]Epoch 14:  85%|████████▌ | 255/300 [01:52<00:22,  2.03it/s]Epoch 14:  85%|████████▌ | 256/300 [01:52<00:19,  2.21it/s]Epoch 14:  86%|████████▌ | 257/300 [01:53<00:18,  2.36it/s]Epoch 14:  86%|████████▌ | 258/300 [01:53<00:17,  2.41it/s]Epoch 14:  86%|████████▋ | 259/300 [01:53<00:18,  2.24it/s]06/19/2022 14:59:52 - INFO - __main__ - global step: 2230; train loss: 7.377004146575928; dev loss: 7.2569146156311035
Epoch 14:  87%|████████▋ | 260/300 [01:54<00:17,  2.33it/s]Epoch 14:  87%|████████▋ | 261/300 [01:54<00:17,  2.27it/s]Epoch 14:  87%|████████▋ | 262/300 [01:55<00:17,  2.17it/s]Epoch 14:  88%|████████▊ | 263/300 [01:55<00:18,  2.06it/s]Epoch 14:  88%|████████▊ | 264/300 [01:56<00:17,  2.09it/s]Epoch 14:  88%|████████▊ | 265/300 [01:56<00:15,  2.27it/s]Epoch 14:  89%|████████▊ | 266/300 [01:56<00:14,  2.42it/s]Epoch 14:  89%|████████▉ | 267/300 [01:57<00:14,  2.21it/s]Epoch 14:  89%|████████▉ | 268/300 [01:58<00:14,  2.18it/s]Epoch 14:  90%|████████▉ | 269/300 [01:58<00:14,  2.15it/s]Epoch 14:  90%|█████████ | 270/300 [01:58<00:14,  2.12it/s]Epoch 14:  90%|█████████ | 271/300 [01:59<00:13,  2.18it/s]Epoch 14:  91%|█████████ | 272/300 [02:00<00:14,  1.97it/s]Epoch 14:  91%|█████████ | 273/300 [02:00<00:13,  2.02it/s]Epoch 14:  91%|█████████▏| 274/300 [02:00<00:12,  2.11it/s]Epoch 14:  92%|█████████▏| 275/300 [02:01<00:10,  2.28it/s]Epoch 14:  92%|█████████▏| 276/300 [02:01<00:10,  2.21it/s]Epoch 14:  92%|█████████▏| 277/300 [02:02<00:09,  2.38it/s]Epoch 14:  93%|█████████▎| 278/300 [02:02<00:09,  2.37it/s]Epoch 14:  93%|█████████▎| 279/300 [02:02<00:08,  2.37it/s]06/19/2022 15:00:01 - INFO - __main__ - global step: 2240; train loss: 7.487612247467041; dev loss: 7.318398475646973
Epoch 14:  93%|█████████▎| 280/300 [02:03<00:09,  2.22it/s]Epoch 14:  94%|█████████▎| 281/300 [02:03<00:08,  2.35it/s]Epoch 14:  94%|█████████▍| 282/300 [02:04<00:07,  2.40it/s]Epoch 14:  94%|█████████▍| 283/300 [02:04<00:07,  2.19it/s]Epoch 14:  95%|█████████▍| 284/300 [02:05<00:07,  2.02it/s]Epoch 14:  95%|█████████▌| 285/300 [02:05<00:07,  2.05it/s]Epoch 14:  95%|█████████▌| 286/300 [02:06<00:06,  2.07it/s]Epoch 14:  96%|█████████▌| 287/300 [02:06<00:06,  2.14it/s]Epoch 14:  96%|█████████▌| 288/300 [02:07<00:06,  1.95it/s]Epoch 14:  96%|█████████▋| 289/300 [02:07<00:05,  2.02it/s]Epoch 14:  97%|█████████▋| 290/300 [02:08<00:04,  2.07it/s]Epoch 14:  97%|█████████▋| 291/300 [02:08<00:04,  2.12it/s]Epoch 14:  97%|█████████▋| 292/300 [02:09<00:04,  1.92it/s]Epoch 14:  98%|█████████▊| 293/300 [02:09<00:03,  2.04it/s]Epoch 14:  98%|█████████▊| 294/300 [02:10<00:02,  2.08it/s]Epoch 14:  98%|█████████▊| 295/300 [02:10<00:02,  2.07it/s]Epoch 14:  99%|█████████▊| 296/300 [02:11<00:02,  1.84it/s]Epoch 14:  99%|█████████▉| 297/300 [02:11<00:01,  2.05it/s]Epoch 14:  99%|█████████▉| 298/300 [02:12<00:00,  2.18it/s]Epoch 14: 100%|█████████▉| 299/300 [02:12<00:00,  2.26it/s]06/19/2022 15:00:10 - INFO - __main__ - global step: 2250; train loss: 7.514066219329834; dev loss: 7.571393013000488
Epoch 14: 100%|██████████| 300/300 [02:12<00:00,  2.39it/s]Epoch 14: 100%|██████████| 300/300 [02:12<00:00,  2.26it/s]
Epoch 15:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 15:   0%|          | 1/300 [00:00<02:20,  2.13it/s]Epoch 15:   1%|          | 2/300 [00:00<02:13,  2.23it/s]Epoch 15:   1%|          | 3/300 [00:01<02:02,  2.43it/s]Epoch 15:   1%|▏         | 4/300 [00:01<02:03,  2.39it/s]Epoch 15:   2%|▏         | 5/300 [00:02<02:08,  2.30it/s]Epoch 15:   2%|▏         | 6/300 [00:02<02:03,  2.37it/s]Epoch 15:   2%|▏         | 7/300 [00:02<02:04,  2.36it/s]Epoch 15:   3%|▎         | 8/300 [00:03<01:58,  2.47it/s]Epoch 15:   3%|▎         | 9/300 [00:03<02:07,  2.29it/s]Epoch 15:   3%|▎         | 10/300 [00:04<02:11,  2.21it/s]Epoch 15:   4%|▎         | 11/300 [00:04<02:12,  2.18it/s]Epoch 15:   4%|▍         | 12/300 [00:05<02:05,  2.30it/s]Epoch 15:   4%|▍         | 13/300 [00:05<02:07,  2.24it/s]Epoch 15:   5%|▍         | 14/300 [00:06<02:00,  2.38it/s]Epoch 15:   5%|▌         | 15/300 [00:06<01:55,  2.47it/s]Epoch 15:   5%|▌         | 16/300 [00:06<01:54,  2.49it/s]Epoch 15:   6%|▌         | 17/300 [00:07<02:06,  2.24it/s]Epoch 15:   6%|▌         | 18/300 [00:07<02:07,  2.22it/s]Epoch 15:   6%|▋         | 19/300 [00:08<02:07,  2.21it/s]06/19/2022 15:00:19 - INFO - __main__ - global step: 2260; train loss: 7.5846381187438965; dev loss: 7.450778961181641
Epoch 15:   7%|▋         | 20/300 [00:08<02:05,  2.23it/s]Epoch 15:   7%|▋         | 21/300 [00:09<02:08,  2.18it/s]Epoch 15:   7%|▋         | 22/300 [00:09<02:06,  2.20it/s]Epoch 15:   8%|▊         | 23/300 [00:10<02:06,  2.20it/s]Epoch 15:   8%|▊         | 24/300 [00:10<02:02,  2.25it/s]Epoch 15:   8%|▊         | 25/300 [00:10<02:02,  2.25it/s]Epoch 15:   9%|▊         | 26/300 [00:11<02:16,  2.00it/s]Epoch 15:   9%|▉         | 27/300 [00:12<02:15,  2.02it/s]Epoch 15:   9%|▉         | 28/300 [00:12<02:12,  2.06it/s]Epoch 15:  10%|▉         | 29/300 [00:12<02:01,  2.23it/s]Epoch 15:  10%|█         | 30/300 [00:13<02:09,  2.09it/s]Epoch 15:  10%|█         | 31/300 [00:13<02:10,  2.06it/s]Epoch 15:  11%|█         | 32/300 [00:14<02:03,  2.18it/s]Epoch 15:  11%|█         | 33/300 [00:14<02:01,  2.20it/s]Epoch 15:  11%|█▏        | 34/300 [00:15<02:10,  2.04it/s]Epoch 15:  12%|█▏        | 35/300 [00:15<02:10,  2.03it/s]Epoch 15:  12%|█▏        | 36/300 [00:16<01:57,  2.25it/s]Epoch 15:  12%|█▏        | 37/300 [00:16<01:48,  2.42it/s]Epoch 15:  13%|█▎        | 38/300 [00:16<01:51,  2.34it/s]Epoch 15:  13%|█▎        | 39/300 [00:17<01:44,  2.50it/s]06/19/2022 15:00:28 - INFO - __main__ - global step: 2270; train loss: 6.722797393798828; dev loss: 6.816225528717041
Epoch 15:  13%|█▎        | 40/300 [00:17<01:43,  2.52it/s]Epoch 15:  14%|█▎        | 41/300 [00:18<01:46,  2.44it/s]Epoch 15:  14%|█▍        | 42/300 [00:18<01:58,  2.19it/s]Epoch 15:  14%|█▍        | 43/300 [00:19<01:55,  2.22it/s]Epoch 15:  15%|█▍        | 44/300 [00:19<01:58,  2.16it/s]Epoch 15:  15%|█▌        | 45/300 [00:19<01:48,  2.35it/s]Epoch 15:  15%|█▌        | 46/300 [00:20<01:49,  2.33it/s]Epoch 15:  16%|█▌        | 47/300 [00:20<01:45,  2.39it/s]Epoch 15:  16%|█▌        | 48/300 [00:21<01:40,  2.50it/s]Epoch 15:  16%|█▋        | 49/300 [00:21<01:37,  2.57it/s]Epoch 15:  17%|█▋        | 50/300 [00:22<01:43,  2.42it/s]Epoch 15:  17%|█▋        | 51/300 [00:22<01:39,  2.51it/s]Epoch 15:  17%|█▋        | 52/300 [00:22<01:40,  2.47it/s]Epoch 15:  18%|█▊        | 53/300 [00:23<01:40,  2.46it/s]Epoch 15:  18%|█▊        | 54/300 [00:23<01:44,  2.36it/s]Epoch 15:  18%|█▊        | 55/300 [00:24<01:58,  2.08it/s]Epoch 15:  19%|█▊        | 56/300 [00:24<01:54,  2.12it/s]Epoch 15:  19%|█▉        | 57/300 [00:25<01:52,  2.17it/s]Epoch 15:  19%|█▉        | 58/300 [00:25<01:49,  2.21it/s]Epoch 15:  20%|█▉        | 59/300 [00:26<01:58,  2.04it/s]06/19/2022 15:00:37 - INFO - __main__ - global step: 2280; train loss: 7.226329803466797; dev loss: 7.276263236999512
Epoch 15:  20%|██        | 60/300 [00:26<01:50,  2.17it/s]Epoch 15:  20%|██        | 61/300 [00:26<01:46,  2.25it/s]Epoch 15:  21%|██        | 62/300 [00:27<01:49,  2.17it/s]Epoch 15:  21%|██        | 63/300 [00:28<02:03,  1.92it/s]Epoch 15:  21%|██▏       | 64/300 [00:28<01:54,  2.06it/s]Epoch 15:  22%|██▏       | 65/300 [00:28<01:46,  2.21it/s]Epoch 15:  22%|██▏       | 66/300 [00:29<01:37,  2.40it/s]Epoch 15:  22%|██▏       | 67/300 [00:29<01:38,  2.37it/s]Epoch 15:  23%|██▎       | 68/300 [00:30<01:36,  2.41it/s]Epoch 15:  23%|██▎       | 69/300 [00:30<01:30,  2.55it/s]Epoch 15:  23%|██▎       | 70/300 [00:30<01:25,  2.69it/s]Epoch 15:  24%|██▎       | 71/300 [00:31<01:29,  2.57it/s]Epoch 15:  24%|██▍       | 72/300 [00:31<01:30,  2.53it/s]Epoch 15:  24%|██▍       | 73/300 [00:32<01:33,  2.42it/s]Epoch 15:  25%|██▍       | 74/300 [00:32<01:34,  2.38it/s]Epoch 15:  25%|██▌       | 75/300 [00:33<01:46,  2.10it/s]Epoch 15:  25%|██▌       | 76/300 [00:33<01:44,  2.13it/s]Epoch 15:  26%|██▌       | 77/300 [00:34<01:44,  2.13it/s]Epoch 15:  26%|██▌       | 78/300 [00:34<01:44,  2.13it/s]Epoch 15:  26%|██▋       | 79/300 [00:35<01:50,  2.00it/s]06/19/2022 15:00:46 - INFO - __main__ - global step: 2290; train loss: 7.475852966308594; dev loss: 7.369917869567871
Epoch 15:  27%|██▋       | 80/300 [00:35<01:55,  1.90it/s]Epoch 15:  27%|██▋       | 81/300 [00:36<01:45,  2.08it/s]Epoch 15:  27%|██▋       | 82/300 [00:36<01:42,  2.12it/s]Epoch 15:  28%|██▊       | 83/300 [00:36<01:43,  2.09it/s]Epoch 15:  28%|██▊       | 84/300 [00:37<01:49,  1.97it/s]Epoch 15:  28%|██▊       | 85/300 [00:38<01:52,  1.91it/s]Epoch 15:  29%|██▊       | 86/300 [00:38<01:46,  2.01it/s]Epoch 15:  29%|██▉       | 87/300 [00:38<01:41,  2.09it/s]Epoch 15:  29%|██▉       | 88/300 [00:39<01:39,  2.14it/s]Epoch 15:  30%|██▉       | 89/300 [00:39<01:31,  2.30it/s]Epoch 15:  30%|███       | 90/300 [00:40<01:27,  2.39it/s]Epoch 15:  30%|███       | 91/300 [00:40<01:22,  2.53it/s]Epoch 15:  31%|███       | 92/300 [00:40<01:27,  2.39it/s]Epoch 15:  31%|███       | 93/300 [00:41<01:26,  2.40it/s]Epoch 15:  31%|███▏      | 94/300 [00:41<01:31,  2.26it/s]Epoch 15:  32%|███▏      | 95/300 [00:42<01:34,  2.16it/s]Epoch 15:  32%|███▏      | 96/300 [00:43<01:46,  1.92it/s]Epoch 15:  32%|███▏      | 97/300 [00:43<01:38,  2.05it/s]Epoch 15:  33%|███▎      | 98/300 [00:43<01:35,  2.12it/s]Epoch 15:  33%|███▎      | 99/300 [00:44<01:32,  2.16it/s]06/19/2022 15:00:55 - INFO - __main__ - global step: 2300; train loss: 7.8430023193359375; dev loss: 7.903055667877197
Epoch 15:  33%|███▎      | 100/300 [00:44<01:38,  2.02it/s]Epoch 15:  34%|███▎      | 101/300 [00:45<01:38,  2.01it/s]Epoch 15:  34%|███▍      | 102/300 [00:45<01:34,  2.10it/s]Epoch 15:  34%|███▍      | 103/300 [00:46<01:31,  2.16it/s]Epoch 15:  35%|███▍      | 104/300 [00:46<01:35,  2.05it/s]Epoch 15:  35%|███▌      | 105/300 [00:47<01:27,  2.24it/s]Epoch 15:  35%|███▌      | 106/300 [00:47<01:21,  2.38it/s]Epoch 15:  36%|███▌      | 107/300 [00:47<01:16,  2.52it/s]Epoch 15:  36%|███▌      | 108/300 [00:48<01:15,  2.54it/s]Epoch 15:  36%|███▋      | 109/300 [00:48<01:17,  2.47it/s]Epoch 15:  37%|███▋      | 110/300 [00:49<01:12,  2.62it/s]Epoch 15:  37%|███▋      | 111/300 [00:49<01:09,  2.74it/s]Epoch 15:  37%|███▋      | 112/300 [00:49<01:06,  2.83it/s]Epoch 15:  38%|███▊      | 113/300 [00:50<01:11,  2.60it/s]Epoch 15:  38%|███▊      | 114/300 [00:50<01:08,  2.71it/s]Epoch 15:  38%|███▊      | 115/300 [00:50<01:05,  2.82it/s]Epoch 15:  39%|███▊      | 116/300 [00:51<01:03,  2.89it/s]Epoch 15:  39%|███▉      | 117/300 [00:51<01:10,  2.58it/s]Epoch 15:  39%|███▉      | 118/300 [00:52<01:16,  2.38it/s]Epoch 15:  40%|███▉      | 119/300 [00:52<01:15,  2.38it/s]06/19/2022 15:01:03 - INFO - __main__ - global step: 2310; train loss: 7.09451150894165; dev loss: 7.043698787689209
Epoch 15:  40%|████      | 120/300 [00:53<01:20,  2.23it/s]Epoch 15:  40%|████      | 121/300 [00:53<01:25,  2.09it/s]Epoch 15:  41%|████      | 122/300 [00:54<01:29,  1.98it/s]Epoch 15:  41%|████      | 123/300 [00:54<01:29,  1.98it/s]Epoch 15:  41%|████▏     | 124/300 [00:55<01:27,  2.02it/s]Epoch 15:  42%|████▏     | 125/300 [00:55<01:26,  2.02it/s]Epoch 15:  42%|████▏     | 126/300 [00:55<01:20,  2.15it/s]Epoch 15:  42%|████▏     | 127/300 [00:56<01:13,  2.35it/s]Epoch 15:  43%|████▎     | 128/300 [00:56<01:08,  2.51it/s]Epoch 15:  43%|████▎     | 129/300 [00:57<01:10,  2.41it/s]Epoch 15:  43%|████▎     | 130/300 [00:57<01:06,  2.54it/s]Epoch 15:  44%|████▎     | 131/300 [00:57<01:04,  2.64it/s]Epoch 15:  44%|████▍     | 132/300 [00:58<01:01,  2.72it/s]Epoch 15:  44%|████▍     | 133/300 [00:58<01:01,  2.71it/s]Epoch 15:  45%|████▍     | 134/300 [00:58<01:04,  2.57it/s]Epoch 15:  45%|████▌     | 135/300 [00:59<01:01,  2.69it/s]Epoch 15:  45%|████▌     | 136/300 [00:59<00:58,  2.79it/s]Epoch 15:  46%|████▌     | 137/300 [01:00<01:00,  2.68it/s]Epoch 15:  46%|████▌     | 138/300 [01:00<01:04,  2.50it/s]Epoch 15:  46%|████▋     | 139/300 [01:00<01:05,  2.48it/s]06/19/2022 15:01:12 - INFO - __main__ - global step: 2320; train loss: 7.186971187591553; dev loss: 7.222084999084473
Epoch 15:  47%|████▋     | 140/300 [01:01<01:06,  2.41it/s]Epoch 15:  47%|████▋     | 141/300 [01:01<01:05,  2.44it/s]Epoch 15:  47%|████▋     | 142/300 [01:02<01:07,  2.33it/s]Epoch 15:  48%|████▊     | 143/300 [01:02<01:03,  2.46it/s]Epoch 15:  48%|████▊     | 144/300 [01:02<01:01,  2.54it/s]Epoch 15:  48%|████▊     | 145/300 [01:03<01:02,  2.48it/s]Epoch 15:  49%|████▊     | 146/300 [01:03<01:12,  2.12it/s]Epoch 15:  49%|████▉     | 147/300 [01:04<01:12,  2.12it/s]Epoch 15:  49%|████▉     | 148/300 [01:04<01:11,  2.11it/s]Epoch 15:  50%|████▉     | 149/300 [01:05<01:08,  2.20it/s]Epoch 15:  50%|█████     | 150/300 [01:05<01:10,  2.13it/s]Epoch 15:  50%|█████     | 151/300 [01:06<01:06,  2.25it/s]Epoch 15:  51%|█████     | 152/300 [01:06<01:01,  2.43it/s]Epoch 15:  51%|█████     | 153/300 [01:07<01:02,  2.35it/s]Epoch 15:  51%|█████▏    | 154/300 [01:07<01:11,  2.03it/s]Epoch 15:  52%|█████▏    | 155/300 [01:08<01:10,  2.06it/s]Epoch 15:  52%|█████▏    | 156/300 [01:08<01:07,  2.12it/s]Epoch 15:  52%|█████▏    | 157/300 [01:08<01:02,  2.30it/s]Epoch 15:  53%|█████▎    | 158/300 [01:09<01:02,  2.28it/s]Epoch 15:  53%|█████▎    | 159/300 [01:09<00:58,  2.40it/s]06/19/2022 15:01:20 - INFO - __main__ - global step: 2330; train loss: 7.581929683685303; dev loss: 7.711729526519775
Epoch 15:  53%|█████▎    | 160/300 [01:10<00:55,  2.53it/s]Epoch 15:  54%|█████▎    | 161/300 [01:10<00:54,  2.54it/s]Epoch 15:  54%|█████▍    | 162/300 [01:10<00:51,  2.66it/s]Epoch 15:  54%|█████▍    | 163/300 [01:11<00:53,  2.56it/s]Epoch 15:  55%|█████▍    | 164/300 [01:11<00:52,  2.60it/s]Epoch 15:  55%|█████▌    | 165/300 [01:11<00:50,  2.69it/s]Epoch 15:  55%|█████▌    | 166/300 [01:12<00:48,  2.75it/s]Epoch 15:  56%|█████▌    | 167/300 [01:12<00:51,  2.60it/s]Epoch 15:  56%|█████▌    | 168/300 [01:13<00:48,  2.70it/s]Epoch 15:  56%|█████▋    | 169/300 [01:13<00:47,  2.78it/s]Epoch 15:  57%|█████▋    | 170/300 [01:13<00:46,  2.82it/s]Epoch 15:  57%|█████▋    | 171/300 [01:14<00:49,  2.62it/s]Epoch 15:  57%|█████▋    | 172/300 [01:14<00:49,  2.60it/s]Epoch 15:  58%|█████▊    | 173/300 [01:14<00:47,  2.69it/s]Epoch 15:  58%|█████▊    | 174/300 [01:15<00:47,  2.67it/s]Epoch 15:  58%|█████▊    | 175/300 [01:15<00:50,  2.47it/s]Epoch 15:  59%|█████▊    | 176/300 [01:16<00:47,  2.59it/s]Epoch 15:  59%|█████▉    | 177/300 [01:16<00:45,  2.67it/s]Epoch 15:  59%|█████▉    | 178/300 [01:16<00:49,  2.49it/s]Epoch 15:  60%|█████▉    | 179/300 [01:17<00:54,  2.20it/s]06/19/2022 15:01:28 - INFO - __main__ - global step: 2340; train loss: 7.133725643157959; dev loss: 7.037796974182129
Epoch 15:  60%|██████    | 180/300 [01:17<00:54,  2.21it/s]Epoch 15:  60%|██████    | 181/300 [01:18<00:52,  2.29it/s]Epoch 15:  61%|██████    | 182/300 [01:18<00:49,  2.40it/s]Epoch 15:  61%|██████    | 183/300 [01:19<00:50,  2.30it/s]Epoch 15:  61%|██████▏   | 184/300 [01:19<00:50,  2.32it/s]Epoch 15:  62%|██████▏   | 185/300 [01:20<00:49,  2.33it/s]Epoch 15:  62%|██████▏   | 186/300 [01:20<00:49,  2.29it/s]Epoch 15:  62%|██████▏   | 187/300 [01:20<00:50,  2.26it/s]Epoch 15:  63%|██████▎   | 188/300 [01:21<00:54,  2.07it/s]Epoch 15:  63%|██████▎   | 189/300 [01:21<00:51,  2.17it/s]Epoch 15:  63%|██████▎   | 190/300 [01:22<00:51,  2.16it/s]Epoch 15:  64%|██████▎   | 191/300 [01:22<00:51,  2.13it/s]Epoch 15:  64%|██████▍   | 192/300 [01:23<00:56,  1.93it/s]Epoch 15:  64%|██████▍   | 193/300 [01:23<00:52,  2.03it/s]Epoch 15:  65%|██████▍   | 194/300 [01:24<00:52,  2.01it/s]Epoch 15:  65%|██████▌   | 195/300 [01:24<00:49,  2.11it/s]Epoch 15:  65%|██████▌   | 196/300 [01:25<00:52,  1.99it/s]Epoch 15:  66%|██████▌   | 197/300 [01:25<00:49,  2.07it/s]Epoch 15:  66%|██████▌   | 198/300 [01:26<00:48,  2.10it/s]Epoch 15:  66%|██████▋   | 199/300 [01:26<00:47,  2.13it/s]06/19/2022 15:01:38 - INFO - __main__ - global step: 2350; train loss: 7.493494987487793; dev loss: 7.468757629394531
Epoch 15:  67%|██████▋   | 200/300 [01:27<00:50,  1.97it/s]Epoch 15:  67%|██████▋   | 201/300 [01:27<00:48,  2.06it/s]Epoch 15:  67%|██████▋   | 202/300 [01:28<00:46,  2.10it/s]Epoch 15:  68%|██████▊   | 203/300 [01:28<00:46,  2.10it/s]Epoch 15:  68%|██████▊   | 204/300 [01:29<00:48,  1.97it/s]Epoch 15:  68%|██████▊   | 205/300 [01:29<00:44,  2.12it/s]Epoch 15:  69%|██████▊   | 206/300 [01:30<00:41,  2.28it/s]Epoch 15:  69%|██████▉   | 207/300 [01:30<00:38,  2.43it/s]Epoch 15:  69%|██████▉   | 208/300 [01:30<00:39,  2.34it/s]Epoch 15:  70%|██████▉   | 209/300 [01:31<00:37,  2.45it/s]Epoch 15:  70%|███████   | 210/300 [01:31<00:35,  2.52it/s]Epoch 15:  70%|███████   | 211/300 [01:32<00:34,  2.61it/s]Epoch 15:  71%|███████   | 212/300 [01:32<00:35,  2.50it/s]Epoch 15:  71%|███████   | 213/300 [01:32<00:33,  2.58it/s]Epoch 15:  71%|███████▏  | 214/300 [01:33<00:32,  2.62it/s]Epoch 15:  72%|███████▏  | 215/300 [01:33<00:31,  2.68it/s]Epoch 15:  72%|███████▏  | 216/300 [01:33<00:30,  2.75it/s]Epoch 15:  72%|███████▏  | 217/300 [01:34<00:31,  2.59it/s]Epoch 15:  73%|███████▎  | 218/300 [01:34<00:30,  2.67it/s]Epoch 15:  73%|███████▎  | 219/300 [01:35<00:29,  2.73it/s]06/19/2022 15:01:46 - INFO - __main__ - global step: 2360; train loss: 7.475227355957031; dev loss: 7.281742095947266
Epoch 15:  73%|███████▎  | 220/300 [01:35<00:28,  2.77it/s]Epoch 15:  74%|███████▎  | 221/300 [01:35<00:30,  2.59it/s]Epoch 15:  74%|███████▍  | 222/300 [01:36<00:29,  2.69it/s]Epoch 15:  74%|███████▍  | 223/300 [01:36<00:28,  2.73it/s]Epoch 15:  75%|███████▍  | 224/300 [01:36<00:27,  2.76it/s]Epoch 15:  75%|███████▌  | 225/300 [01:37<00:29,  2.59it/s]Epoch 15:  75%|███████▌  | 226/300 [01:37<00:27,  2.68it/s]Epoch 15:  76%|███████▌  | 227/300 [01:37<00:26,  2.72it/s]Epoch 15:  76%|███████▌  | 228/300 [01:38<00:28,  2.52it/s]Epoch 15:  76%|███████▋  | 229/300 [01:39<00:33,  2.12it/s]Epoch 15:  77%|███████▋  | 230/300 [01:39<00:32,  2.18it/s]Epoch 15:  77%|███████▋  | 231/300 [01:39<00:29,  2.30it/s]Epoch 15:  77%|███████▋  | 232/300 [01:40<00:29,  2.31it/s]Epoch 15:  78%|███████▊  | 233/300 [01:40<00:30,  2.23it/s]Epoch 15:  78%|███████▊  | 234/300 [01:41<00:29,  2.22it/s]Epoch 15:  78%|███████▊  | 235/300 [01:41<00:28,  2.27it/s]Epoch 15:  79%|███████▊  | 236/300 [01:42<00:27,  2.29it/s]Epoch 15:  79%|███████▉  | 237/300 [01:42<00:28,  2.18it/s]Epoch 15:  79%|███████▉  | 238/300 [01:43<00:28,  2.21it/s]Epoch 15:  80%|███████▉  | 239/300 [01:43<00:27,  2.21it/s]06/19/2022 15:01:54 - INFO - __main__ - global step: 2370; train loss: 7.322278022766113; dev loss: 7.296385288238525
Epoch 15:  80%|████████  | 240/300 [01:43<00:27,  2.17it/s]Epoch 15:  80%|████████  | 241/300 [01:44<00:26,  2.27it/s]Epoch 15:  81%|████████  | 242/300 [01:45<00:29,  1.99it/s]Epoch 15:  81%|████████  | 243/300 [01:45<00:26,  2.17it/s]Epoch 15:  81%|████████▏ | 244/300 [01:45<00:24,  2.31it/s]Epoch 15:  82%|████████▏ | 245/300 [01:46<00:24,  2.29it/s]Epoch 15:  82%|████████▏ | 246/300 [01:46<00:25,  2.11it/s]Epoch 15:  82%|████████▏ | 247/300 [01:47<00:25,  2.11it/s]Epoch 15:  83%|████████▎ | 248/300 [01:47<00:23,  2.21it/s]Epoch 15:  83%|████████▎ | 249/300 [01:48<00:22,  2.23it/s]Epoch 15:  83%|████████▎ | 250/300 [01:48<00:24,  2.00it/s]Epoch 15:  84%|████████▎ | 251/300 [01:49<00:23,  2.06it/s]Epoch 15:  84%|████████▍ | 252/300 [01:49<00:23,  2.05it/s]Epoch 15:  84%|████████▍ | 253/300 [01:50<00:22,  2.04it/s]Epoch 15:  85%|████████▍ | 254/300 [01:50<00:22,  2.07it/s]Epoch 15:  85%|████████▌ | 255/300 [01:51<00:22,  2.04it/s]Epoch 15:  85%|████████▌ | 256/300 [01:51<00:22,  1.99it/s]Epoch 15:  86%|████████▌ | 257/300 [01:52<00:21,  1.96it/s]Epoch 15:  86%|████████▌ | 258/300 [01:52<00:23,  1.80it/s]Epoch 15:  86%|████████▋ | 259/300 [01:53<00:20,  2.04it/s]06/19/2022 15:02:04 - INFO - __main__ - global step: 2380; train loss: 7.940587043762207; dev loss: 7.84894323348999
Epoch 15:  87%|████████▋ | 260/300 [01:53<00:19,  2.05it/s]Epoch 15:  87%|████████▋ | 261/300 [01:54<00:18,  2.12it/s]Epoch 15:  87%|████████▋ | 262/300 [01:54<00:17,  2.12it/s]Epoch 15:  88%|████████▊ | 263/300 [01:55<00:17,  2.17it/s]Epoch 15:  88%|████████▊ | 264/300 [01:55<00:16,  2.20it/s]Epoch 15:  88%|████████▊ | 265/300 [01:55<00:15,  2.21it/s]Epoch 15:  89%|████████▊ | 266/300 [01:56<00:17,  1.98it/s]Epoch 15:  89%|████████▉ | 267/300 [01:56<00:15,  2.11it/s]Epoch 15:  89%|████████▉ | 268/300 [01:57<00:14,  2.14it/s]Epoch 15:  90%|████████▉ | 269/300 [01:57<00:14,  2.15it/s]Epoch 15:  90%|█████████ | 270/300 [01:58<00:13,  2.16it/s]Epoch 15:  90%|█████████ | 271/300 [01:58<00:14,  1.94it/s]Epoch 15:  91%|█████████ | 272/300 [01:59<00:13,  2.08it/s]Epoch 15:  91%|█████████ | 273/300 [01:59<00:13,  2.07it/s]Epoch 15:  91%|█████████▏| 274/300 [02:00<00:11,  2.18it/s]Epoch 15:  92%|█████████▏| 275/300 [02:00<00:12,  2.07it/s]Epoch 15:  92%|█████████▏| 276/300 [02:01<00:10,  2.22it/s]Epoch 15:  92%|█████████▏| 277/300 [02:01<00:09,  2.37it/s]Epoch 15:  93%|█████████▎| 278/300 [02:01<00:08,  2.51it/s]Epoch 15:  93%|█████████▎| 279/300 [02:02<00:09,  2.31it/s]06/19/2022 15:02:13 - INFO - __main__ - global step: 2390; train loss: 7.895562171936035; dev loss: 7.721973419189453
Epoch 15:  93%|█████████▎| 280/300 [02:02<00:08,  2.45it/s]Epoch 15:  94%|█████████▎| 281/300 [02:03<00:07,  2.57it/s]Epoch 15:  94%|█████████▍| 282/300 [02:03<00:07,  2.50it/s]Epoch 15:  94%|█████████▍| 283/300 [02:03<00:07,  2.43it/s]Epoch 15:  95%|█████████▍| 284/300 [02:04<00:06,  2.55it/s]Epoch 15:  95%|█████████▌| 285/300 [02:04<00:05,  2.65it/s]Epoch 15:  95%|█████████▌| 286/300 [02:05<00:05,  2.56it/s]Epoch 15:  96%|█████████▌| 287/300 [02:05<00:05,  2.46it/s]Epoch 15:  96%|█████████▌| 288/300 [02:05<00:04,  2.56it/s]Epoch 15:  96%|█████████▋| 289/300 [02:06<00:04,  2.52it/s]Epoch 15:  97%|█████████▋| 290/300 [02:06<00:04,  2.18it/s]Epoch 15:  97%|█████████▋| 291/300 [02:07<00:04,  2.14it/s]Epoch 15:  97%|█████████▋| 292/300 [02:07<00:03,  2.24it/s]Epoch 15:  98%|█████████▊| 293/300 [02:08<00:02,  2.40it/s]Epoch 15:  98%|█████████▊| 294/300 [02:08<00:02,  2.53it/s]Epoch 15:  98%|█████████▊| 295/300 [02:08<00:01,  2.62it/s]Epoch 15:  99%|█████████▊| 296/300 [02:09<00:01,  2.38it/s]Epoch 15:  99%|█████████▉| 297/300 [02:09<00:01,  2.37it/s]Epoch 15:  99%|█████████▉| 298/300 [02:10<00:00,  2.44it/s]Epoch 15: 100%|█████████▉| 299/300 [02:10<00:00,  2.31it/s]06/19/2022 15:02:21 - INFO - __main__ - global step: 2400; train loss: 7.257626533508301; dev loss: 7.238888740539551
Epoch 15: 100%|██████████| 300/300 [02:11<00:00,  2.23it/s]Epoch 15: 100%|██████████| 300/300 [02:11<00:00,  2.29it/s]
Epoch 16:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 16:   0%|          | 1/300 [00:00<01:43,  2.89it/s]Epoch 16:   1%|          | 2/300 [00:00<01:43,  2.87it/s]Epoch 16:   1%|          | 3/300 [00:01<01:46,  2.80it/s]Epoch 16:   1%|▏         | 4/300 [00:01<01:55,  2.57it/s]Epoch 16:   2%|▏         | 5/300 [00:01<01:58,  2.48it/s]Epoch 16:   2%|▏         | 6/300 [00:02<01:56,  2.51it/s]Epoch 16:   2%|▏         | 7/300 [00:02<01:55,  2.55it/s]Epoch 16:   3%|▎         | 8/300 [00:03<01:59,  2.44it/s]Epoch 16:   3%|▎         | 9/300 [00:03<02:00,  2.41it/s]Epoch 16:   3%|▎         | 10/300 [00:03<01:53,  2.55it/s]Epoch 16:   4%|▎         | 11/300 [00:04<01:50,  2.63it/s]Epoch 16:   4%|▍         | 12/300 [00:04<01:58,  2.43it/s]Epoch 16:   4%|▍         | 13/300 [00:05<01:51,  2.57it/s]Epoch 16:   5%|▍         | 14/300 [00:05<01:53,  2.51it/s]Epoch 16:   5%|▌         | 15/300 [00:05<01:49,  2.60it/s]Epoch 16:   5%|▌         | 16/300 [00:06<01:54,  2.49it/s]Epoch 16:   6%|▌         | 17/300 [00:06<02:01,  2.33it/s]Epoch 16:   6%|▌         | 18/300 [00:07<02:05,  2.25it/s]Epoch 16:   6%|▋         | 19/300 [00:07<02:12,  2.12it/s]06/19/2022 15:02:29 - INFO - __main__ - global step: 2410; train loss: 7.637894630432129; dev loss: 7.810018062591553
Epoch 16:   7%|▋         | 20/300 [00:08<02:11,  2.14it/s]Epoch 16:   7%|▋         | 21/300 [00:08<02:02,  2.27it/s]Epoch 16:   7%|▋         | 22/300 [00:08<01:54,  2.43it/s]Epoch 16:   8%|▊         | 23/300 [00:09<01:53,  2.43it/s]Epoch 16:   8%|▊         | 24/300 [00:09<01:48,  2.54it/s]Epoch 16:   8%|▊         | 25/300 [00:10<01:52,  2.45it/s]Epoch 16:   9%|▊         | 26/300 [00:10<01:46,  2.57it/s]Epoch 16:   9%|▉         | 27/300 [00:10<01:42,  2.67it/s]Epoch 16:   9%|▉         | 28/300 [00:11<01:45,  2.57it/s]Epoch 16:  10%|▉         | 29/300 [00:11<01:55,  2.34it/s]Epoch 16:  10%|█         | 30/300 [00:12<01:55,  2.34it/s]Epoch 16:  10%|█         | 31/300 [00:12<01:48,  2.48it/s]Epoch 16:  11%|█         | 32/300 [00:12<01:43,  2.59it/s]Epoch 16:  11%|█         | 33/300 [00:13<01:50,  2.43it/s]Epoch 16:  11%|█▏        | 34/300 [00:13<01:44,  2.54it/s]Epoch 16:  12%|█▏        | 35/300 [00:14<01:45,  2.50it/s]Epoch 16:  12%|█▏        | 36/300 [00:14<01:41,  2.60it/s]Epoch 16:  12%|█▏        | 37/300 [00:14<01:45,  2.50it/s]Epoch 16:  13%|█▎        | 38/300 [00:15<01:40,  2.61it/s]Epoch 16:  13%|█▎        | 39/300 [00:15<01:37,  2.69it/s]06/19/2022 15:02:37 - INFO - __main__ - global step: 2420; train loss: 6.995665550231934; dev loss: 6.849356174468994
Epoch 16:  13%|█▎        | 40/300 [00:16<01:35,  2.73it/s]Epoch 16:  14%|█▎        | 41/300 [00:16<01:43,  2.50it/s]Epoch 16:  14%|█▍        | 42/300 [00:16<01:38,  2.62it/s]Epoch 16:  14%|█▍        | 43/300 [00:17<01:40,  2.55it/s]Epoch 16:  15%|█▍        | 44/300 [00:17<01:37,  2.64it/s]Epoch 16:  15%|█▌        | 45/300 [00:18<01:41,  2.51it/s]Epoch 16:  15%|█▌        | 46/300 [00:18<01:37,  2.60it/s]Epoch 16:  16%|█▌        | 47/300 [00:18<01:37,  2.61it/s]Epoch 16:  16%|█▌        | 48/300 [00:19<01:33,  2.68it/s]Epoch 16:  16%|█▋        | 49/300 [00:19<01:31,  2.74it/s]Epoch 16:  17%|█▋        | 50/300 [00:19<01:42,  2.44it/s]Epoch 16:  17%|█▋        | 51/300 [00:20<01:40,  2.48it/s]Epoch 16:  17%|█▋        | 52/300 [00:20<01:40,  2.47it/s]Epoch 16:  18%|█▊        | 53/300 [00:21<01:37,  2.52it/s]Epoch 16:  18%|█▊        | 54/300 [00:21<01:43,  2.37it/s]Epoch 16:  18%|█▊        | 55/300 [00:22<01:47,  2.27it/s]Epoch 16:  19%|█▊        | 56/300 [00:22<01:40,  2.42it/s]Epoch 16:  19%|█▉        | 57/300 [00:22<01:35,  2.54it/s]Epoch 16:  19%|█▉        | 58/300 [00:23<01:43,  2.33it/s]Epoch 16:  20%|█▉        | 59/300 [00:23<01:43,  2.33it/s]06/19/2022 15:02:45 - INFO - __main__ - global step: 2430; train loss: 7.389008522033691; dev loss: 7.527768135070801
Epoch 16:  20%|██        | 60/300 [00:24<01:39,  2.41it/s]Epoch 16:  20%|██        | 61/300 [00:24<01:44,  2.30it/s]Epoch 16:  21%|██        | 62/300 [00:25<01:57,  2.03it/s]Epoch 16:  21%|██        | 63/300 [00:25<01:49,  2.16it/s]Epoch 16:  21%|██▏       | 64/300 [00:26<01:49,  2.16it/s]Epoch 16:  22%|██▏       | 65/300 [00:26<01:48,  2.16it/s]Epoch 16:  22%|██▏       | 66/300 [00:27<01:57,  2.00it/s]Epoch 16:  22%|██▏       | 67/300 [00:27<01:50,  2.10it/s]Epoch 16:  23%|██▎       | 68/300 [00:27<01:41,  2.29it/s]Epoch 16:  23%|██▎       | 69/300 [00:28<01:36,  2.39it/s]Epoch 16:  23%|██▎       | 70/300 [00:28<01:44,  2.19it/s]Epoch 16:  24%|██▎       | 71/300 [00:29<01:37,  2.34it/s]Epoch 16:  24%|██▍       | 72/300 [00:29<01:37,  2.34it/s]Epoch 16:  24%|██▍       | 73/300 [00:29<01:32,  2.46it/s]Epoch 16:  25%|██▍       | 74/300 [00:30<01:35,  2.36it/s]Epoch 16:  25%|██▌       | 75/300 [00:30<01:36,  2.34it/s]Epoch 16:  25%|██▌       | 76/300 [00:31<01:31,  2.45it/s]Epoch 16:  26%|██▌       | 77/300 [00:31<01:30,  2.47it/s]Epoch 16:  26%|██▌       | 78/300 [00:32<01:32,  2.41it/s]Epoch 16:  26%|██▋       | 79/300 [00:32<01:38,  2.25it/s]06/19/2022 15:02:54 - INFO - __main__ - global step: 2440; train loss: 7.268604278564453; dev loss: 7.279670715332031
Epoch 16:  27%|██▋       | 80/300 [00:32<01:32,  2.38it/s]Epoch 16:  27%|██▋       | 81/300 [00:33<01:27,  2.49it/s]Epoch 16:  27%|██▋       | 82/300 [00:33<01:24,  2.58it/s]Epoch 16:  28%|██▊       | 83/300 [00:34<01:31,  2.37it/s]Epoch 16:  28%|██▊       | 84/300 [00:34<01:29,  2.42it/s]Epoch 16:  28%|██▊       | 85/300 [00:34<01:28,  2.44it/s]Epoch 16:  29%|██▊       | 86/300 [00:35<01:27,  2.45it/s]Epoch 16:  29%|██▉       | 87/300 [00:35<01:32,  2.30it/s]Epoch 16:  29%|██▉       | 88/300 [00:36<01:37,  2.18it/s]Epoch 16:  30%|██▉       | 89/300 [00:36<01:32,  2.27it/s]Epoch 16:  30%|███       | 90/300 [00:37<01:32,  2.27it/s]Epoch 16:  30%|███       | 91/300 [00:37<01:44,  2.00it/s]Epoch 16:  31%|███       | 92/300 [00:38<01:41,  2.04it/s]Epoch 16:  31%|███       | 93/300 [00:38<01:39,  2.09it/s]Epoch 16:  31%|███▏      | 94/300 [00:39<01:36,  2.13it/s]Epoch 16:  32%|███▏      | 95/300 [00:39<01:46,  1.92it/s]Epoch 16:  32%|███▏      | 96/300 [00:40<01:47,  1.90it/s]Epoch 16:  32%|███▏      | 97/300 [00:40<01:40,  2.03it/s]Epoch 16:  33%|███▎      | 98/300 [00:41<01:39,  2.03it/s]Epoch 16:  33%|███▎      | 99/300 [00:41<01:47,  1.87it/s]06/19/2022 15:03:04 - INFO - __main__ - global step: 2450; train loss: 7.189839839935303; dev loss: 7.290441989898682
Epoch 16:  33%|███▎      | 100/300 [00:42<01:44,  1.92it/s]Epoch 16:  34%|███▎      | 101/300 [00:43<01:47,  1.85it/s]Epoch 16:  34%|███▍      | 102/300 [00:43<01:48,  1.83it/s]Epoch 16:  34%|███▍      | 103/300 [00:43<01:39,  1.99it/s]Epoch 16:  35%|███▍      | 104/300 [00:44<01:44,  1.88it/s]Epoch 16:  35%|███▌      | 105/300 [00:45<01:40,  1.94it/s]Epoch 16:  35%|███▌      | 106/300 [00:45<01:33,  2.07it/s]Epoch 16:  36%|███▌      | 107/300 [00:45<01:31,  2.10it/s]Epoch 16:  36%|███▌      | 108/300 [00:46<01:37,  1.97it/s]Epoch 16:  36%|███▋      | 109/300 [00:46<01:32,  2.06it/s]Epoch 16:  37%|███▋      | 110/300 [00:47<01:24,  2.25it/s]Epoch 16:  37%|███▋      | 111/300 [00:47<01:18,  2.42it/s]Epoch 16:  37%|███▋      | 112/300 [00:48<01:19,  2.37it/s]Epoch 16:  38%|███▊      | 113/300 [00:48<01:14,  2.51it/s]Epoch 16:  38%|███▊      | 114/300 [00:48<01:13,  2.54it/s]Epoch 16:  38%|███▊      | 115/300 [00:49<01:11,  2.57it/s]Epoch 16:  39%|███▊      | 116/300 [00:49<01:16,  2.41it/s]Epoch 16:  39%|███▉      | 117/300 [00:49<01:11,  2.55it/s]Epoch 16:  39%|███▉      | 118/300 [00:50<01:08,  2.64it/s]Epoch 16:  40%|███▉      | 119/300 [00:50<01:06,  2.72it/s]06/19/2022 15:03:12 - INFO - __main__ - global step: 2460; train loss: 7.384291172027588; dev loss: 7.448822975158691
Epoch 16:  40%|████      | 120/300 [00:51<01:12,  2.49it/s]Epoch 16:  40%|████      | 121/300 [00:51<01:08,  2.60it/s]Epoch 16:  41%|████      | 122/300 [00:51<01:06,  2.66it/s]Epoch 16:  41%|████      | 123/300 [00:52<01:05,  2.72it/s]Epoch 16:  41%|████▏     | 124/300 [00:52<01:08,  2.58it/s]Epoch 16:  42%|████▏     | 125/300 [00:53<01:06,  2.63it/s]Epoch 16:  42%|████▏     | 126/300 [00:53<01:06,  2.63it/s]Epoch 16:  42%|████▏     | 127/300 [00:53<01:10,  2.44it/s]Epoch 16:  43%|████▎     | 128/300 [00:54<01:19,  2.16it/s]Epoch 16:  43%|████▎     | 129/300 [00:54<01:18,  2.17it/s]Epoch 16:  43%|████▎     | 130/300 [00:55<01:17,  2.20it/s]Epoch 16:  44%|████▎     | 131/300 [00:55<01:19,  2.13it/s]Epoch 16:  44%|████▍     | 132/300 [00:56<01:18,  2.14it/s]Epoch 16:  44%|████▍     | 133/300 [00:56<01:22,  2.02it/s]Epoch 16:  45%|████▍     | 134/300 [00:57<01:14,  2.23it/s]Epoch 16:  45%|████▌     | 135/300 [00:57<01:10,  2.36it/s]Epoch 16:  45%|████▌     | 136/300 [00:57<01:05,  2.49it/s]Epoch 16:  46%|████▌     | 137/300 [00:58<01:07,  2.43it/s]Epoch 16:  46%|████▌     | 138/300 [00:58<01:08,  2.36it/s]Epoch 16:  46%|████▋     | 139/300 [00:59<01:09,  2.33it/s]06/19/2022 15:03:21 - INFO - __main__ - global step: 2470; train loss: 7.10521936416626; dev loss: 7.320037841796875
Epoch 16:  47%|████▋     | 140/300 [00:59<01:06,  2.41it/s]Epoch 16:  47%|████▋     | 141/300 [01:00<01:08,  2.31it/s]Epoch 16:  47%|████▋     | 142/300 [01:00<01:05,  2.40it/s]Epoch 16:  48%|████▊     | 143/300 [01:00<01:04,  2.43it/s]Epoch 16:  48%|████▊     | 144/300 [01:01<01:01,  2.54it/s]Epoch 16:  48%|████▊     | 145/300 [01:01<01:03,  2.45it/s]Epoch 16:  49%|████▊     | 146/300 [01:02<00:59,  2.58it/s]Epoch 16:  49%|████▉     | 147/300 [01:02<00:59,  2.57it/s]Epoch 16:  49%|████▉     | 148/300 [01:02<00:59,  2.57it/s]Epoch 16:  50%|████▉     | 149/300 [01:03<01:02,  2.41it/s]Epoch 16:  50%|█████     | 150/300 [01:03<01:00,  2.48it/s]Epoch 16:  50%|█████     | 151/300 [01:04<01:01,  2.44it/s]Epoch 16:  51%|█████     | 152/300 [01:04<01:06,  2.21it/s]Epoch 16:  51%|█████     | 153/300 [01:05<01:16,  1.92it/s]Epoch 16:  51%|█████▏    | 154/300 [01:05<01:08,  2.12it/s]Epoch 16:  52%|█████▏    | 155/300 [01:06<01:02,  2.31it/s]Epoch 16:  52%|█████▏    | 156/300 [01:06<00:59,  2.42it/s]Epoch 16:  52%|█████▏    | 157/300 [01:06<00:57,  2.50it/s]Epoch 16:  53%|█████▎    | 158/300 [01:07<00:59,  2.38it/s]Epoch 16:  53%|█████▎    | 159/300 [01:07<00:57,  2.45it/s]06/19/2022 15:03:29 - INFO - __main__ - global step: 2480; train loss: 7.294827938079834; dev loss: 7.212944030761719
Epoch 16:  53%|█████▎    | 160/300 [01:07<00:54,  2.58it/s]Epoch 16:  54%|█████▎    | 161/300 [01:08<00:52,  2.66it/s]Epoch 16:  54%|█████▍    | 162/300 [01:08<00:57,  2.39it/s]Epoch 16:  54%|█████▍    | 163/300 [01:09<00:57,  2.40it/s]Epoch 16:  55%|█████▍    | 164/300 [01:09<00:57,  2.36it/s]Epoch 16:  55%|█████▌    | 165/300 [01:10<01:01,  2.18it/s]Epoch 16:  55%|█████▌    | 166/300 [01:10<01:08,  1.97it/s]Epoch 16:  56%|█████▌    | 167/300 [01:11<01:08,  1.95it/s]Epoch 16:  56%|█████▌    | 168/300 [01:11<01:06,  1.98it/s]Epoch 16:  56%|█████▋    | 169/300 [01:12<01:06,  1.97it/s]Epoch 16:  57%|█████▋    | 170/300 [01:12<01:10,  1.85it/s]Epoch 16:  57%|█████▋    | 171/300 [01:13<01:04,  2.00it/s]Epoch 16:  57%|█████▋    | 172/300 [01:13<00:58,  2.17it/s]Epoch 16:  58%|█████▊    | 173/300 [01:14<00:54,  2.31it/s]Epoch 16:  58%|█████▊    | 174/300 [01:14<00:55,  2.26it/s]Epoch 16:  58%|█████▊    | 175/300 [01:14<00:52,  2.37it/s]Epoch 16:  59%|█████▊    | 176/300 [01:15<00:57,  2.17it/s]Epoch 16:  59%|█████▉    | 177/300 [01:15<00:53,  2.29it/s]Epoch 16:  59%|█████▉    | 178/300 [01:16<00:59,  2.06it/s]Epoch 16:  60%|█████▉    | 179/300 [01:16<00:53,  2.25it/s]06/19/2022 15:03:38 - INFO - __main__ - global step: 2490; train loss: 7.1622490882873535; dev loss: 7.3898186683654785
Epoch 16:  60%|██████    | 180/300 [01:17<00:49,  2.40it/s]Epoch 16:  60%|██████    | 181/300 [01:17<00:46,  2.53it/s]Epoch 16:  61%|██████    | 182/300 [01:18<00:50,  2.36it/s]Epoch 16:  61%|██████    | 183/300 [01:18<00:46,  2.51it/s]Epoch 16:  61%|██████▏   | 184/300 [01:18<00:47,  2.46it/s]Epoch 16:  62%|██████▏   | 185/300 [01:19<00:49,  2.32it/s]Epoch 16:  62%|██████▏   | 186/300 [01:19<00:46,  2.47it/s]Epoch 16:  62%|██████▏   | 187/300 [01:20<00:46,  2.41it/s]Epoch 16:  63%|██████▎   | 188/300 [01:20<00:46,  2.41it/s]Epoch 16:  63%|██████▎   | 189/300 [01:20<00:46,  2.40it/s]Epoch 16:  63%|██████▎   | 190/300 [01:21<00:43,  2.50it/s]Epoch 16:  64%|██████▎   | 191/300 [01:21<00:47,  2.28it/s]Epoch 16:  64%|██████▍   | 192/300 [01:22<00:48,  2.23it/s]Epoch 16:  64%|██████▍   | 193/300 [01:22<00:47,  2.23it/s]Epoch 16:  65%|██████▍   | 194/300 [01:23<00:47,  2.24it/s]Epoch 16:  65%|██████▌   | 195/300 [01:23<00:49,  2.13it/s]Epoch 16:  65%|██████▌   | 196/300 [01:24<00:47,  2.18it/s]Epoch 16:  66%|██████▌   | 197/300 [01:24<00:43,  2.37it/s]Epoch 16:  66%|██████▌   | 198/300 [01:24<00:41,  2.44it/s]Epoch 16:  66%|██████▋   | 199/300 [01:25<00:46,  2.18it/s]06/19/2022 15:03:47 - INFO - __main__ - global step: 2500; train loss: 7.2581281661987305; dev loss: 7.323221683502197
Epoch 16:  67%|██████▋   | 200/300 [01:25<00:47,  2.10it/s]Epoch 16:  67%|██████▋   | 201/300 [01:26<00:46,  2.14it/s]Epoch 16:  67%|██████▋   | 202/300 [01:26<00:44,  2.21it/s]Epoch 16:  68%|██████▊   | 203/300 [01:27<00:45,  2.14it/s]Epoch 16:  68%|██████▊   | 204/300 [01:27<00:43,  2.22it/s]Epoch 16:  68%|██████▊   | 205/300 [01:28<00:42,  2.24it/s]Epoch 16:  69%|██████▊   | 206/300 [01:28<00:39,  2.38it/s]Epoch 16:  69%|██████▉   | 207/300 [01:28<00:39,  2.36it/s]Epoch 16:  69%|██████▉   | 208/300 [01:29<00:36,  2.51it/s]Epoch 16:  70%|██████▉   | 209/300 [01:29<00:34,  2.60it/s]Epoch 16:  70%|███████   | 210/300 [01:30<00:35,  2.55it/s]Epoch 16:  70%|███████   | 211/300 [01:30<00:33,  2.64it/s]Epoch 16:  71%|███████   | 212/300 [01:30<00:34,  2.54it/s]Epoch 16:  71%|███████   | 213/300 [01:31<00:32,  2.66it/s]Epoch 16:  71%|███████▏  | 214/300 [01:31<00:32,  2.65it/s]Epoch 16:  72%|███████▏  | 215/300 [01:31<00:33,  2.51it/s]Epoch 16:  72%|███████▏  | 216/300 [01:32<00:37,  2.22it/s]Epoch 16:  72%|███████▏  | 217/300 [01:32<00:37,  2.23it/s]Epoch 16:  73%|███████▎  | 218/300 [01:33<00:37,  2.21it/s]Epoch 16:  73%|███████▎  | 219/300 [01:33<00:36,  2.22it/s]06/19/2022 15:03:56 - INFO - __main__ - global step: 2510; train loss: 7.303609371185303; dev loss: 7.242684364318848
Epoch 16:  73%|███████▎  | 220/300 [01:34<00:39,  2.05it/s]Epoch 16:  74%|███████▎  | 221/300 [01:34<00:37,  2.13it/s]Epoch 16:  74%|███████▍  | 222/300 [01:35<00:36,  2.15it/s]Epoch 16:  74%|███████▍  | 223/300 [01:35<00:35,  2.18it/s]Epoch 16:  75%|███████▍  | 224/300 [01:36<00:38,  1.99it/s]Epoch 16:  75%|███████▌  | 225/300 [01:36<00:36,  2.08it/s]Epoch 16:  75%|███████▌  | 226/300 [01:37<00:35,  2.10it/s]Epoch 16:  76%|███████▌  | 227/300 [01:37<00:33,  2.20it/s]Epoch 16:  76%|███████▌  | 228/300 [01:38<00:35,  2.00it/s]Epoch 16:  76%|███████▋  | 229/300 [01:38<00:33,  2.15it/s]Epoch 16:  77%|███████▋  | 230/300 [01:39<00:32,  2.19it/s]Epoch 16:  77%|███████▋  | 231/300 [01:39<00:29,  2.31it/s]Epoch 16:  77%|███████▋  | 232/300 [01:40<00:31,  2.14it/s]Epoch 16:  78%|███████▊  | 233/300 [01:40<00:30,  2.18it/s]Epoch 16:  78%|███████▊  | 234/300 [01:40<00:31,  2.10it/s]Epoch 16:  78%|███████▊  | 235/300 [01:41<00:30,  2.11it/s]Epoch 16:  79%|███████▊  | 236/300 [01:41<00:31,  2.06it/s]Epoch 16:  79%|███████▉  | 237/300 [01:42<00:29,  2.15it/s]Epoch 16:  79%|███████▉  | 238/300 [01:42<00:28,  2.20it/s]Epoch 16:  80%|███████▉  | 239/300 [01:43<00:29,  2.07it/s]06/19/2022 15:04:05 - INFO - __main__ - global step: 2520; train loss: 7.450181484222412; dev loss: 7.481123447418213
Epoch 16:  80%|████████  | 240/300 [01:43<00:27,  2.15it/s]Epoch 16:  80%|████████  | 241/300 [01:44<00:28,  2.04it/s]Epoch 16:  81%|████████  | 242/300 [01:44<00:29,  1.99it/s]Epoch 16:  81%|████████  | 243/300 [01:45<00:29,  1.95it/s]Epoch 16:  81%|████████▏ | 244/300 [01:45<00:28,  1.94it/s]Epoch 16:  82%|████████▏ | 245/300 [01:46<00:26,  2.04it/s]Epoch 16:  82%|████████▏ | 246/300 [01:46<00:23,  2.26it/s]Epoch 16:  82%|████████▏ | 247/300 [01:47<00:21,  2.45it/s]Epoch 16:  83%|████████▎ | 248/300 [01:47<00:20,  2.53it/s]Epoch 16:  83%|████████▎ | 249/300 [01:47<00:20,  2.47it/s]Epoch 16:  83%|████████▎ | 250/300 [01:48<00:19,  2.62it/s]Epoch 16:  84%|████████▎ | 251/300 [01:48<00:18,  2.70it/s]Epoch 16:  84%|████████▍ | 252/300 [01:48<00:17,  2.75it/s]Epoch 16:  84%|████████▍ | 253/300 [01:49<00:18,  2.50it/s]Epoch 16:  85%|████████▍ | 254/300 [01:49<00:17,  2.61it/s]Epoch 16:  85%|████████▌ | 255/300 [01:50<00:17,  2.50it/s]Epoch 16:  85%|████████▌ | 256/300 [01:50<00:18,  2.44it/s]Epoch 16:  86%|████████▌ | 257/300 [01:51<00:19,  2.17it/s]Epoch 16:  86%|████████▌ | 258/300 [01:51<00:19,  2.13it/s]Epoch 16:  86%|████████▋ | 259/300 [01:52<00:19,  2.15it/s]06/19/2022 15:04:14 - INFO - __main__ - global step: 2530; train loss: 7.5915422439575195; dev loss: 7.641329765319824
Epoch 16:  87%|████████▋ | 260/300 [01:52<00:18,  2.11it/s]Epoch 16:  87%|████████▋ | 261/300 [01:53<00:19,  2.00it/s]Epoch 16:  87%|████████▋ | 262/300 [01:53<00:18,  2.08it/s]Epoch 16:  88%|████████▊ | 263/300 [01:53<00:17,  2.13it/s]Epoch 16:  88%|████████▊ | 264/300 [01:54<00:16,  2.20it/s]Epoch 16:  88%|████████▊ | 265/300 [01:54<00:15,  2.31it/s]Epoch 16:  89%|████████▊ | 266/300 [01:55<00:14,  2.31it/s]Epoch 16:  89%|████████▉ | 267/300 [01:55<00:13,  2.49it/s]Epoch 16:  89%|████████▉ | 268/300 [01:55<00:12,  2.64it/s]Epoch 16:  90%|████████▉ | 269/300 [01:56<00:11,  2.75it/s]Epoch 16:  90%|█████████ | 270/300 [01:56<00:11,  2.53it/s]Epoch 16:  90%|█████████ | 271/300 [01:57<00:11,  2.59it/s]Epoch 16:  91%|█████████ | 272/300 [01:57<00:10,  2.63it/s]Epoch 16:  91%|█████████ | 273/300 [01:57<00:10,  2.65it/s]Epoch 16:  91%|█████████▏| 274/300 [01:58<00:10,  2.48it/s]Epoch 16:  92%|█████████▏| 275/300 [01:58<00:09,  2.58it/s]Epoch 16:  92%|█████████▏| 276/300 [01:58<00:09,  2.65it/s]Epoch 16:  92%|█████████▏| 277/300 [01:59<00:08,  2.65it/s]Epoch 16:  93%|█████████▎| 278/300 [01:59<00:09,  2.30it/s]Epoch 16:  93%|█████████▎| 279/300 [02:00<00:09,  2.30it/s]06/19/2022 15:04:22 - INFO - __main__ - global step: 2540; train loss: 7.457772254943848; dev loss: 7.1581525802612305
Epoch 16:  93%|█████████▎| 280/300 [02:00<00:08,  2.29it/s]Epoch 16:  94%|█████████▎| 281/300 [02:01<00:08,  2.34it/s]Epoch 16:  94%|█████████▍| 282/300 [02:01<00:07,  2.27it/s]Epoch 16:  94%|█████████▍| 283/300 [02:01<00:07,  2.40it/s]Epoch 16:  95%|█████████▍| 284/300 [02:02<00:06,  2.50it/s]Epoch 16:  95%|█████████▌| 285/300 [02:02<00:06,  2.44it/s]Epoch 16:  95%|█████████▌| 286/300 [02:03<00:06,  2.19it/s]Epoch 16:  96%|█████████▌| 287/300 [02:03<00:06,  2.11it/s]Epoch 16:  96%|█████████▌| 288/300 [02:04<00:05,  2.11it/s]Epoch 16:  96%|█████████▋| 289/300 [02:04<00:05,  2.18it/s]Epoch 16:  97%|█████████▋| 290/300 [02:05<00:04,  2.03it/s]Epoch 16:  97%|█████████▋| 291/300 [02:05<00:04,  2.09it/s]Epoch 16:  97%|█████████▋| 292/300 [02:06<00:03,  2.08it/s]Epoch 16:  98%|█████████▊| 293/300 [02:06<00:03,  2.25it/s]Epoch 16:  98%|█████████▊| 294/300 [02:06<00:02,  2.36it/s]Epoch 16:  98%|█████████▊| 295/300 [02:07<00:02,  2.22it/s]Epoch 16:  99%|█████████▊| 296/300 [02:07<00:01,  2.30it/s]Epoch 16:  99%|█████████▉| 297/300 [02:08<00:01,  2.38it/s]Epoch 16:  99%|█████████▉| 298/300 [02:08<00:00,  2.47it/s]Epoch 16: 100%|█████████▉| 299/300 [02:09<00:00,  2.10it/s]06/19/2022 15:04:31 - INFO - __main__ - global step: 2550; train loss: 7.15966796875; dev loss: 7.371596336364746
Epoch 16: 100%|██████████| 300/300 [02:09<00:00,  2.21it/s]Epoch 16: 100%|██████████| 300/300 [02:09<00:00,  2.31it/s]
Epoch 17:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 17:   0%|          | 1/300 [00:00<01:45,  2.83it/s]Epoch 17:   1%|          | 2/300 [00:00<01:57,  2.54it/s]Epoch 17:   1%|          | 3/300 [00:01<02:01,  2.44it/s]Epoch 17:   1%|▏         | 4/300 [00:01<02:00,  2.46it/s]Epoch 17:   2%|▏         | 5/300 [00:01<01:54,  2.57it/s]Epoch 17:   2%|▏         | 6/300 [00:02<01:50,  2.65it/s]Epoch 17:   2%|▏         | 7/300 [00:02<02:00,  2.43it/s]Epoch 17:   3%|▎         | 8/300 [00:03<01:57,  2.49it/s]Epoch 17:   3%|▎         | 9/300 [00:03<01:54,  2.53it/s]Epoch 17:   3%|▎         | 10/300 [00:03<01:49,  2.64it/s]Epoch 17:   4%|▎         | 11/300 [00:04<01:56,  2.47it/s]Epoch 17:   4%|▍         | 12/300 [00:04<01:52,  2.56it/s]Epoch 17:   4%|▍         | 13/300 [00:05<01:49,  2.61it/s]Epoch 17:   5%|▍         | 14/300 [00:05<01:50,  2.59it/s]Epoch 17:   5%|▌         | 15/300 [00:05<01:58,  2.41it/s]Epoch 17:   5%|▌         | 16/300 [00:06<01:51,  2.54it/s]Epoch 17:   6%|▌         | 17/300 [00:06<01:50,  2.56it/s]Epoch 17:   6%|▌         | 18/300 [00:07<01:49,  2.58it/s]Epoch 17:   6%|▋         | 19/300 [00:07<01:46,  2.64it/s]06/19/2022 15:04:39 - INFO - __main__ - global step: 2560; train loss: 7.268143653869629; dev loss: 7.193473815917969
Epoch 17:   7%|▋         | 20/300 [00:07<01:55,  2.43it/s]Epoch 17:   7%|▋         | 21/300 [00:08<01:53,  2.45it/s]Epoch 17:   7%|▋         | 22/300 [00:08<01:51,  2.50it/s]Epoch 17:   8%|▊         | 23/300 [00:09<01:46,  2.59it/s]Epoch 17:   8%|▊         | 24/300 [00:09<01:55,  2.39it/s]Epoch 17:   8%|▊         | 25/300 [00:09<01:55,  2.39it/s]Epoch 17:   9%|▊         | 26/300 [00:10<01:50,  2.48it/s]Epoch 17:   9%|▉         | 27/300 [00:10<01:45,  2.59it/s]Epoch 17:   9%|▉         | 28/300 [00:11<01:49,  2.48it/s]Epoch 17:  10%|▉         | 29/300 [00:11<02:04,  2.18it/s]Epoch 17:  10%|█         | 30/300 [00:12<02:05,  2.15it/s]Epoch 17:  10%|█         | 31/300 [00:12<02:05,  2.15it/s]Epoch 17:  11%|█         | 32/300 [00:13<02:07,  2.11it/s]Epoch 17:  11%|█         | 33/300 [00:13<02:05,  2.13it/s]Epoch 17:  11%|█▏        | 34/300 [00:14<02:04,  2.13it/s]Epoch 17:  12%|█▏        | 35/300 [00:14<02:01,  2.18it/s]Epoch 17:  12%|█▏        | 36/300 [00:15<02:12,  1.99it/s]Epoch 17:  12%|█▏        | 37/300 [00:15<02:10,  2.02it/s]Epoch 17:  13%|█▎        | 38/300 [00:16<02:07,  2.06it/s]Epoch 17:  13%|█▎        | 39/300 [00:16<02:02,  2.13it/s]06/19/2022 15:04:48 - INFO - __main__ - global step: 2570; train loss: 6.428367614746094; dev loss: 6.255775451660156
Epoch 17:  13%|█▎        | 40/300 [00:17<02:08,  2.02it/s]Epoch 17:  14%|█▎        | 41/300 [00:17<01:59,  2.17it/s]Epoch 17:  14%|█▍        | 42/300 [00:17<01:51,  2.32it/s]Epoch 17:  14%|█▍        | 43/300 [00:18<01:48,  2.38it/s]Epoch 17:  15%|█▍        | 44/300 [00:18<02:01,  2.11it/s]Epoch 17:  15%|█▌        | 45/300 [00:19<02:00,  2.12it/s]Epoch 17:  15%|█▌        | 46/300 [00:19<01:58,  2.14it/s]Epoch 17:  16%|█▌        | 47/300 [00:20<01:56,  2.17it/s]Epoch 17:  16%|█▌        | 48/300 [00:20<01:56,  2.16it/s]Epoch 17:  16%|█▋        | 49/300 [00:21<02:09,  1.93it/s]Epoch 17:  17%|█▋        | 50/300 [00:21<02:03,  2.02it/s]Epoch 17:  17%|█▋        | 51/300 [00:22<01:59,  2.09it/s]Epoch 17:  17%|█▋        | 52/300 [00:22<01:52,  2.21it/s]Epoch 17:  18%|█▊        | 53/300 [00:23<01:57,  2.10it/s]Epoch 17:  18%|█▊        | 54/300 [00:23<01:50,  2.22it/s]Epoch 17:  18%|█▊        | 55/300 [00:23<01:47,  2.28it/s]Epoch 17:  19%|█▊        | 56/300 [00:24<01:42,  2.38it/s]Epoch 17:  19%|█▉        | 57/300 [00:24<01:47,  2.26it/s]Epoch 17:  19%|█▉        | 58/300 [00:25<01:41,  2.40it/s]Epoch 17:  20%|█▉        | 59/300 [00:25<01:39,  2.43it/s]06/19/2022 15:04:57 - INFO - __main__ - global step: 2580; train loss: 7.250611305236816; dev loss: 7.210295677185059
Epoch 17:  20%|██        | 60/300 [00:25<01:35,  2.52it/s]Epoch 17:  20%|██        | 61/300 [00:26<01:39,  2.40it/s]Epoch 17:  21%|██        | 62/300 [00:26<01:34,  2.51it/s]Epoch 17:  21%|██        | 63/300 [00:27<01:44,  2.26it/s]Epoch 17:  21%|██▏       | 64/300 [00:27<01:41,  2.33it/s]Epoch 17:  22%|██▏       | 65/300 [00:28<01:50,  2.13it/s]Epoch 17:  22%|██▏       | 66/300 [00:28<01:51,  2.11it/s]Epoch 17:  22%|██▏       | 67/300 [00:29<01:43,  2.25it/s]Epoch 17:  23%|██▎       | 68/300 [00:29<01:37,  2.38it/s]Epoch 17:  23%|██▎       | 69/300 [00:29<01:40,  2.30it/s]Epoch 17:  23%|██▎       | 70/300 [00:30<01:36,  2.38it/s]Epoch 17:  24%|██▎       | 71/300 [00:30<01:34,  2.42it/s]Epoch 17:  24%|██▍       | 72/300 [00:31<01:34,  2.42it/s]Epoch 17:  24%|██▍       | 73/300 [00:31<01:31,  2.47it/s]Epoch 17:  25%|██▍       | 74/300 [00:31<01:36,  2.33it/s]Epoch 17:  25%|██▌       | 75/300 [00:32<01:35,  2.34it/s]Epoch 17:  25%|██▌       | 76/300 [00:32<01:30,  2.47it/s]Epoch 17:  26%|██▌       | 77/300 [00:33<01:32,  2.40it/s]Epoch 17:  26%|██▌       | 78/300 [00:33<01:45,  2.10it/s]Epoch 17:  26%|██▋       | 79/300 [00:34<01:43,  2.13it/s]06/19/2022 15:05:06 - INFO - __main__ - global step: 2590; train loss: 7.618613243103027; dev loss: 7.601955413818359
Epoch 17:  27%|██▋       | 80/300 [00:34<01:42,  2.14it/s]Epoch 17:  27%|██▋       | 81/300 [00:35<01:48,  2.02it/s]Epoch 17:  27%|██▋       | 82/300 [00:35<01:53,  1.92it/s]Epoch 17:  28%|██▊       | 83/300 [00:36<01:49,  1.98it/s]Epoch 17:  28%|██▊       | 84/300 [00:36<01:46,  2.03it/s]Epoch 17:  28%|██▊       | 85/300 [00:37<01:44,  2.07it/s]Epoch 17:  29%|██▊       | 86/300 [00:37<01:53,  1.89it/s]Epoch 17:  29%|██▉       | 87/300 [00:38<01:49,  1.94it/s]Epoch 17:  29%|██▉       | 88/300 [00:38<01:48,  1.95it/s]Epoch 17:  30%|██▉       | 89/300 [00:39<01:40,  2.09it/s]Epoch 17:  30%|███       | 90/300 [00:39<01:46,  1.98it/s]Epoch 17:  30%|███       | 91/300 [00:40<01:36,  2.17it/s]Epoch 17:  31%|███       | 92/300 [00:40<01:29,  2.33it/s]Epoch 17:  31%|███       | 93/300 [00:40<01:28,  2.33it/s]Epoch 17:  31%|███▏      | 94/300 [00:41<01:34,  2.17it/s]Epoch 17:  32%|███▏      | 95/300 [00:41<01:36,  2.13it/s]Epoch 17:  32%|███▏      | 96/300 [00:42<01:41,  2.00it/s]Epoch 17:  32%|███▏      | 97/300 [00:43<01:39,  2.03it/s]Epoch 17:  33%|███▎      | 98/300 [00:43<01:43,  1.96it/s]Epoch 17:  33%|███▎      | 99/300 [00:44<01:42,  1.97it/s]06/19/2022 15:05:15 - INFO - __main__ - global step: 2600; train loss: 7.252955436706543; dev loss: 7.040081977844238
Epoch 17:  33%|███▎      | 100/300 [00:44<01:35,  2.09it/s]Epoch 17:  34%|███▎      | 101/300 [00:45<01:37,  2.04it/s]Epoch 17:  34%|███▍      | 102/300 [00:45<01:32,  2.15it/s]Epoch 17:  34%|███▍      | 103/300 [00:45<01:32,  2.14it/s]Epoch 17:  35%|███▍      | 104/300 [00:46<01:25,  2.30it/s]Epoch 17:  35%|███▌      | 105/300 [00:46<01:20,  2.43it/s]Epoch 17:  35%|███▌      | 106/300 [00:46<01:16,  2.54it/s]Epoch 17:  36%|███▌      | 107/300 [00:47<01:24,  2.30it/s]Epoch 17:  36%|███▌      | 108/300 [00:47<01:23,  2.29it/s]Epoch 17:  36%|███▋      | 109/300 [00:48<01:24,  2.25it/s]Epoch 17:  37%|███▋      | 110/300 [00:48<01:26,  2.19it/s]Epoch 17:  37%|███▋      | 111/300 [00:49<01:28,  2.15it/s]Epoch 17:  37%|███▋      | 112/300 [00:49<01:24,  2.22it/s]Epoch 17:  38%|███▊      | 113/300 [00:50<01:23,  2.24it/s]Epoch 17:  38%|███▊      | 114/300 [00:50<01:20,  2.32it/s]Epoch 17:  38%|███▊      | 115/300 [00:51<01:27,  2.12it/s]Epoch 17:  39%|███▊      | 116/300 [00:51<01:21,  2.25it/s]Epoch 17:  39%|███▉      | 117/300 [00:51<01:20,  2.28it/s]Epoch 17:  39%|███▉      | 118/300 [00:52<01:18,  2.31it/s]Epoch 17:  40%|███▉      | 119/300 [00:52<01:25,  2.11it/s]06/19/2022 15:05:24 - INFO - __main__ - global step: 2610; train loss: 7.633440971374512; dev loss: 7.534669399261475
Epoch 17:  40%|████      | 120/300 [00:53<01:27,  2.06it/s]Epoch 17:  40%|████      | 121/300 [00:53<01:23,  2.15it/s]Epoch 17:  41%|████      | 122/300 [00:54<01:23,  2.12it/s]Epoch 17:  41%|████      | 123/300 [00:54<01:26,  2.05it/s]Epoch 17:  41%|████▏     | 124/300 [00:55<01:28,  2.00it/s]Epoch 17:  42%|████▏     | 125/300 [00:55<01:28,  1.99it/s]Epoch 17:  42%|████▏     | 126/300 [00:56<01:26,  2.02it/s]Epoch 17:  42%|████▏     | 127/300 [00:56<01:26,  2.01it/s]Epoch 17:  43%|████▎     | 128/300 [00:57<01:34,  1.82it/s]Epoch 17:  43%|████▎     | 129/300 [00:58<01:33,  1.84it/s]Epoch 17:  43%|████▎     | 130/300 [00:58<01:24,  2.01it/s]Epoch 17:  44%|████▎     | 131/300 [00:58<01:19,  2.12it/s]Epoch 17:  44%|████▍     | 132/300 [00:59<01:21,  2.07it/s]Epoch 17:  44%|████▍     | 133/300 [00:59<01:16,  2.17it/s]Epoch 17:  45%|████▍     | 134/300 [01:00<01:14,  2.22it/s]Epoch 17:  45%|████▌     | 135/300 [01:00<01:10,  2.36it/s]Epoch 17:  45%|████▌     | 136/300 [01:01<01:13,  2.25it/s]Epoch 17:  46%|████▌     | 137/300 [01:01<01:09,  2.33it/s]Epoch 17:  46%|████▌     | 138/300 [01:01<01:07,  2.40it/s]Epoch 17:  46%|████▋     | 139/300 [01:02<01:06,  2.41it/s]06/19/2022 15:05:34 - INFO - __main__ - global step: 2620; train loss: 7.000063419342041; dev loss: 7.1472625732421875
Epoch 17:  47%|████▋     | 140/300 [01:02<01:14,  2.14it/s]Epoch 17:  47%|████▋     | 141/300 [01:03<01:18,  2.03it/s]Epoch 17:  47%|████▋     | 142/300 [01:03<01:15,  2.10it/s]Epoch 17:  48%|████▊     | 143/300 [01:04<01:13,  2.14it/s]Epoch 17:  48%|████▊     | 144/300 [01:05<01:21,  1.92it/s]Epoch 17:  48%|████▊     | 145/300 [01:05<01:16,  2.03it/s]Epoch 17:  49%|████▊     | 146/300 [01:05<01:09,  2.22it/s]Epoch 17:  49%|████▉     | 147/300 [01:06<01:04,  2.37it/s]Epoch 17:  49%|████▉     | 148/300 [01:06<01:05,  2.34it/s]Epoch 17:  50%|████▉     | 149/300 [01:06<01:02,  2.42it/s]Epoch 17:  50%|█████     | 150/300 [01:07<01:01,  2.45it/s]Epoch 17:  50%|█████     | 151/300 [01:07<01:02,  2.37it/s]Epoch 17:  51%|█████     | 152/300 [01:08<01:12,  2.04it/s]Epoch 17:  51%|█████     | 153/300 [01:08<01:06,  2.22it/s]Epoch 17:  51%|█████▏    | 154/300 [01:09<01:02,  2.35it/s]Epoch 17:  52%|█████▏    | 155/300 [01:09<01:01,  2.36it/s]Epoch 17:  52%|█████▏    | 156/300 [01:10<01:02,  2.29it/s]Epoch 17:  52%|█████▏    | 157/300 [01:10<01:09,  2.07it/s]Epoch 17:  53%|█████▎    | 158/300 [01:11<01:08,  2.06it/s]Epoch 17:  53%|█████▎    | 159/300 [01:11<01:10,  1.99it/s]06/19/2022 15:05:43 - INFO - __main__ - global step: 2630; train loss: 6.900661468505859; dev loss: 7.046436309814453
Epoch 17:  53%|█████▎    | 160/300 [01:12<01:04,  2.17it/s]Epoch 17:  54%|█████▎    | 161/300 [01:12<01:07,  2.07it/s]Epoch 17:  54%|█████▍    | 162/300 [01:12<01:01,  2.25it/s]Epoch 17:  54%|█████▍    | 163/300 [01:13<01:01,  2.22it/s]Epoch 17:  55%|█████▍    | 164/300 [01:13<01:03,  2.13it/s]Epoch 17:  55%|█████▌    | 165/300 [01:14<01:08,  1.98it/s]Epoch 17:  55%|█████▌    | 166/300 [01:14<01:05,  2.05it/s]Epoch 17:  56%|█████▌    | 167/300 [01:15<01:04,  2.05it/s]Epoch 17:  56%|█████▌    | 168/300 [01:15<01:00,  2.17it/s]Epoch 17:  56%|█████▋    | 169/300 [01:16<01:00,  2.17it/s]Epoch 17:  57%|█████▋    | 170/300 [01:16<00:55,  2.36it/s]Epoch 17:  57%|█████▋    | 171/300 [01:16<00:51,  2.53it/s]Epoch 17:  57%|█████▋    | 172/300 [01:17<00:48,  2.66it/s]Epoch 17:  58%|█████▊    | 173/300 [01:17<00:49,  2.56it/s]Epoch 17:  58%|█████▊    | 174/300 [01:18<00:47,  2.63it/s]Epoch 17:  58%|█████▊    | 175/300 [01:18<00:46,  2.71it/s]Epoch 17:  59%|█████▊    | 176/300 [01:18<00:46,  2.67it/s]Epoch 17:  59%|█████▉    | 177/300 [01:19<00:48,  2.55it/s]Epoch 17:  59%|█████▉    | 178/300 [01:19<00:45,  2.68it/s]Epoch 17:  60%|█████▉    | 179/300 [01:19<00:43,  2.77it/s]06/19/2022 15:05:51 - INFO - __main__ - global step: 2640; train loss: 6.7269768714904785; dev loss: 6.581211090087891
Epoch 17:  60%|██████    | 180/300 [01:20<00:42,  2.84it/s]Epoch 17:  60%|██████    | 181/300 [01:20<00:43,  2.76it/s]Epoch 17:  61%|██████    | 182/300 [01:21<00:48,  2.43it/s]Epoch 17:  61%|██████    | 183/300 [01:21<00:49,  2.36it/s]Epoch 17:  61%|██████▏   | 184/300 [01:21<00:46,  2.48it/s]Epoch 17:  62%|██████▏   | 185/300 [01:22<00:44,  2.59it/s]Epoch 17:  62%|██████▏   | 186/300 [01:22<00:48,  2.37it/s]Epoch 17:  62%|██████▏   | 187/300 [01:23<00:45,  2.51it/s]Epoch 17:  63%|██████▎   | 188/300 [01:23<00:42,  2.64it/s]Epoch 17:  63%|██████▎   | 189/300 [01:23<00:43,  2.56it/s]Epoch 17:  63%|██████▎   | 190/300 [01:24<00:45,  2.42it/s]Epoch 17:  64%|██████▎   | 191/300 [01:24<00:43,  2.53it/s]Epoch 17:  64%|██████▍   | 192/300 [01:25<00:42,  2.56it/s]Epoch 17:  64%|██████▍   | 193/300 [01:25<00:41,  2.55it/s]Epoch 17:  65%|██████▍   | 194/300 [01:25<00:44,  2.40it/s]Epoch 17:  65%|██████▌   | 195/300 [01:26<00:43,  2.41it/s]Epoch 17:  65%|██████▌   | 196/300 [01:26<00:40,  2.56it/s]Epoch 17:  66%|██████▌   | 197/300 [01:27<00:40,  2.53it/s]Epoch 17:  66%|██████▌   | 198/300 [01:27<00:42,  2.42it/s]Epoch 17:  66%|██████▋   | 199/300 [01:28<00:43,  2.35it/s]06/19/2022 15:05:59 - INFO - __main__ - global step: 2650; train loss: 7.456976413726807; dev loss: 7.493359565734863
Epoch 17:  67%|██████▋   | 200/300 [01:28<00:43,  2.31it/s]Epoch 17:  67%|██████▋   | 201/300 [01:28<00:43,  2.29it/s]Epoch 17:  67%|██████▋   | 202/300 [01:29<00:45,  2.16it/s]Epoch 17:  68%|██████▊   | 203/300 [01:29<00:41,  2.37it/s]Epoch 17:  68%|██████▊   | 204/300 [01:30<00:38,  2.53it/s]Epoch 17:  68%|██████▊   | 205/300 [01:30<00:35,  2.66it/s]Epoch 17:  69%|██████▊   | 206/300 [01:30<00:39,  2.41it/s]Epoch 17:  69%|██████▉   | 207/300 [01:31<00:36,  2.54it/s]Epoch 17:  69%|██████▉   | 208/300 [01:31<00:34,  2.66it/s]Epoch 17:  70%|██████▉   | 209/300 [01:32<00:34,  2.60it/s]Epoch 17:  70%|███████   | 210/300 [01:32<00:33,  2.71it/s]Epoch 17:  70%|███████   | 211/300 [01:32<00:37,  2.40it/s]Epoch 17:  71%|███████   | 212/300 [01:33<00:35,  2.47it/s]Epoch 17:  71%|███████   | 213/300 [01:33<00:33,  2.58it/s]Epoch 17:  71%|███████▏  | 214/300 [01:34<00:34,  2.53it/s]Epoch 17:  72%|███████▏  | 215/300 [01:34<00:36,  2.34it/s]Epoch 17:  72%|███████▏  | 216/300 [01:34<00:35,  2.34it/s]Epoch 17:  72%|███████▏  | 217/300 [01:35<00:35,  2.34it/s]Epoch 17:  73%|███████▎  | 218/300 [01:35<00:35,  2.30it/s]Epoch 17:  73%|███████▎  | 219/300 [01:36<00:38,  2.12it/s]06/19/2022 15:06:08 - INFO - __main__ - global step: 2660; train loss: 7.3434157371521; dev loss: 7.395583152770996
Epoch 17:  73%|███████▎  | 220/300 [01:36<00:37,  2.14it/s]Epoch 17:  74%|███████▎  | 221/300 [01:37<00:36,  2.16it/s]Epoch 17:  74%|███████▍  | 222/300 [01:37<00:35,  2.18it/s]Epoch 17:  74%|███████▍  | 223/300 [01:38<00:38,  1.99it/s]Epoch 17:  75%|███████▍  | 224/300 [01:38<00:35,  2.17it/s]Epoch 17:  75%|███████▌  | 225/300 [01:39<00:31,  2.35it/s]Epoch 17:  75%|███████▌  | 226/300 [01:39<00:29,  2.52it/s]Epoch 17:  76%|███████▌  | 227/300 [01:39<00:29,  2.46it/s]Epoch 17:  76%|███████▌  | 228/300 [01:40<00:27,  2.60it/s]Epoch 17:  76%|███████▋  | 229/300 [01:40<00:26,  2.71it/s]Epoch 17:  77%|███████▋  | 230/300 [01:40<00:24,  2.81it/s]Epoch 17:  77%|███████▋  | 231/300 [01:41<00:25,  2.66it/s]Epoch 17:  77%|███████▋  | 232/300 [01:41<00:24,  2.76it/s]Epoch 17:  78%|███████▊  | 233/300 [01:41<00:23,  2.83it/s]Epoch 17:  78%|███████▊  | 234/300 [01:42<00:22,  2.90it/s]Epoch 17:  78%|███████▊  | 235/300 [01:42<00:22,  2.95it/s]Epoch 17:  79%|███████▊  | 236/300 [01:43<00:26,  2.46it/s]Epoch 17:  79%|███████▉  | 237/300 [01:43<00:27,  2.31it/s]Epoch 17:  79%|███████▉  | 238/300 [01:44<00:27,  2.30it/s]Epoch 17:  80%|███████▉  | 239/300 [01:44<00:26,  2.31it/s]06/19/2022 15:06:16 - INFO - __main__ - global step: 2670; train loss: 7.116534233093262; dev loss: 7.003173828125
Epoch 17:  80%|████████  | 240/300 [01:45<00:27,  2.16it/s]Epoch 17:  80%|████████  | 241/300 [01:45<00:25,  2.30it/s]Epoch 17:  81%|████████  | 242/300 [01:45<00:26,  2.22it/s]Epoch 17:  81%|████████  | 243/300 [01:46<00:25,  2.21it/s]Epoch 17:  81%|████████▏ | 244/300 [01:46<00:25,  2.18it/s]Epoch 17:  82%|████████▏ | 245/300 [01:47<00:24,  2.24it/s]Epoch 17:  82%|████████▏ | 246/300 [01:47<00:24,  2.23it/s]Epoch 17:  82%|████████▏ | 247/300 [01:48<00:22,  2.41it/s]Epoch 17:  83%|████████▎ | 248/300 [01:48<00:21,  2.38it/s]Epoch 17:  83%|████████▎ | 249/300 [01:48<00:21,  2.40it/s]Epoch 17:  83%|████████▎ | 250/300 [01:49<00:19,  2.54it/s]Epoch 17:  84%|████████▎ | 251/300 [01:49<00:18,  2.67it/s]Epoch 17:  84%|████████▍ | 252/300 [01:49<00:18,  2.56it/s]Epoch 17:  84%|████████▍ | 253/300 [01:50<00:17,  2.69it/s]Epoch 17:  85%|████████▍ | 254/300 [01:50<00:16,  2.79it/s]Epoch 17:  85%|████████▌ | 255/300 [01:50<00:15,  2.85it/s]Epoch 17:  85%|████████▌ | 256/300 [01:51<00:16,  2.66it/s]Epoch 17:  86%|████████▌ | 257/300 [01:51<00:15,  2.77it/s]Epoch 17:  86%|████████▌ | 258/300 [01:52<00:15,  2.67it/s]Epoch 17:  86%|████████▋ | 259/300 [01:52<00:14,  2.76it/s]06/19/2022 15:06:24 - INFO - __main__ - global step: 2680; train loss: 7.021864414215088; dev loss: 7.189111232757568
Epoch 17:  87%|████████▋ | 260/300 [01:53<00:17,  2.33it/s]Epoch 17:  87%|████████▋ | 261/300 [01:53<00:16,  2.32it/s]Epoch 17:  87%|████████▋ | 262/300 [01:53<00:16,  2.29it/s]Epoch 17:  88%|████████▊ | 263/300 [01:54<00:16,  2.27it/s]Epoch 17:  88%|████████▊ | 264/300 [01:54<00:15,  2.35it/s]Epoch 17:  88%|████████▊ | 265/300 [01:55<00:14,  2.35it/s]Epoch 17:  89%|████████▊ | 266/300 [01:55<00:13,  2.46it/s]Epoch 17:  89%|████████▉ | 267/300 [01:55<00:13,  2.44it/s]Epoch 17:  89%|████████▉ | 268/300 [01:56<00:12,  2.59it/s]Epoch 17:  90%|████████▉ | 269/300 [01:56<00:12,  2.53it/s]Epoch 17:  90%|█████████ | 270/300 [01:57<00:11,  2.60it/s]Epoch 17:  90%|█████████ | 271/300 [01:57<00:10,  2.69it/s]Epoch 17:  91%|█████████ | 272/300 [01:57<00:10,  2.76it/s]Epoch 17:  91%|█████████ | 273/300 [01:58<00:10,  2.50it/s]Epoch 17:  91%|█████████▏| 274/300 [01:58<00:10,  2.41it/s]Epoch 17:  92%|█████████▏| 275/300 [01:59<00:10,  2.38it/s]Epoch 17:  92%|█████████▏| 276/300 [01:59<00:10,  2.38it/s]Epoch 17:  92%|█████████▏| 277/300 [01:59<00:09,  2.36it/s]Epoch 17:  93%|█████████▎| 278/300 [02:00<00:08,  2.53it/s]Epoch 17:  93%|█████████▎| 279/300 [02:00<00:08,  2.60it/s]06/19/2022 15:06:32 - INFO - __main__ - global step: 2690; train loss: 7.274666786193848; dev loss: 7.242430210113525
Epoch 17:  93%|█████████▎| 280/300 [02:01<00:07,  2.52it/s]Epoch 17:  94%|█████████▎| 281/300 [02:01<00:08,  2.32it/s]Epoch 17:  94%|█████████▍| 282/300 [02:01<00:07,  2.48it/s]Epoch 17:  94%|█████████▍| 283/300 [02:02<00:06,  2.62it/s]Epoch 17:  95%|█████████▍| 284/300 [02:02<00:05,  2.74it/s]Epoch 17:  95%|█████████▌| 285/300 [02:03<00:05,  2.59it/s]Epoch 17:  95%|█████████▌| 286/300 [02:03<00:05,  2.54it/s]Epoch 17:  96%|█████████▌| 287/300 [02:03<00:04,  2.67it/s]Epoch 17:  96%|█████████▌| 288/300 [02:04<00:04,  2.59it/s]Epoch 17:  96%|█████████▋| 289/300 [02:04<00:04,  2.68it/s]Epoch 17:  97%|█████████▋| 290/300 [02:04<00:03,  2.57it/s]Epoch 17:  97%|█████████▋| 291/300 [02:05<00:03,  2.69it/s]Epoch 17:  97%|█████████▋| 292/300 [02:05<00:03,  2.61it/s]Epoch 17:  98%|█████████▊| 293/300 [02:06<00:02,  2.72it/s]Epoch 17:  98%|█████████▊| 294/300 [02:06<00:02,  2.60it/s]Epoch 17:  98%|█████████▊| 295/300 [02:06<00:01,  2.72it/s]Epoch 17:  99%|█████████▊| 296/300 [02:07<00:01,  2.80it/s]Epoch 17:  99%|█████████▉| 297/300 [02:07<00:01,  2.68it/s]Epoch 17:  99%|█████████▉| 298/300 [02:07<00:00,  2.55it/s]Epoch 17: 100%|█████████▉| 299/300 [02:08<00:00,  2.56it/s]06/19/2022 15:06:40 - INFO - __main__ - global step: 2700; train loss: 7.468113899230957; dev loss: 7.582205772399902
Epoch 17: 100%|██████████| 300/300 [02:08<00:00,  2.34it/s]Epoch 17: 100%|██████████| 300/300 [02:08<00:00,  2.33it/s]
Epoch 18:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 18:   0%|          | 1/300 [00:00<02:11,  2.27it/s]Epoch 18:   1%|          | 2/300 [00:01<02:53,  1.71it/s]Epoch 18:   1%|          | 3/300 [00:01<02:18,  2.14it/s]Epoch 18:   1%|▏         | 4/300 [00:01<02:02,  2.42it/s]Epoch 18:   2%|▏         | 5/300 [00:02<01:53,  2.60it/s]Epoch 18:   2%|▏         | 6/300 [00:02<01:59,  2.45it/s]Epoch 18:   2%|▏         | 7/300 [00:02<01:58,  2.47it/s]Epoch 18:   3%|▎         | 8/300 [00:03<01:58,  2.46it/s]Epoch 18:   3%|▎         | 9/300 [00:03<01:51,  2.60it/s]Epoch 18:   3%|▎         | 10/300 [00:04<01:55,  2.51it/s]Epoch 18:   4%|▎         | 11/300 [00:04<01:49,  2.64it/s]Epoch 18:   4%|▍         | 12/300 [00:04<01:44,  2.74it/s]Epoch 18:   4%|▍         | 13/300 [00:05<01:44,  2.74it/s]Epoch 18:   5%|▍         | 14/300 [00:05<01:54,  2.50it/s]Epoch 18:   5%|▌         | 15/300 [00:06<01:54,  2.49it/s]Epoch 18:   5%|▌         | 16/300 [00:06<01:50,  2.57it/s]Epoch 18:   6%|▌         | 17/300 [00:06<01:53,  2.49it/s]Epoch 18:   6%|▌         | 18/300 [00:07<01:57,  2.41it/s]Epoch 18:   6%|▋         | 19/300 [00:07<02:13,  2.11it/s]06/19/2022 15:06:48 - INFO - __main__ - global step: 2710; train loss: 6.726251125335693; dev loss: 6.800772666931152
Epoch 18:   7%|▋         | 20/300 [00:08<02:05,  2.23it/s]Epoch 18:   7%|▋         | 21/300 [00:08<01:59,  2.34it/s]Epoch 18:   7%|▋         | 22/300 [00:09<01:54,  2.42it/s]Epoch 18:   8%|▊         | 23/300 [00:09<02:06,  2.19it/s]Epoch 18:   8%|▊         | 24/300 [00:10<02:04,  2.21it/s]Epoch 18:   8%|▊         | 25/300 [00:10<02:10,  2.10it/s]Epoch 18:   9%|▊         | 26/300 [00:11<02:08,  2.13it/s]Epoch 18:   9%|▉         | 27/300 [00:11<02:17,  1.99it/s]Epoch 18:   9%|▉         | 28/300 [00:12<02:15,  2.01it/s]Epoch 18:  10%|▉         | 29/300 [00:12<02:06,  2.13it/s]Epoch 18:  10%|█         | 30/300 [00:12<02:03,  2.20it/s]Epoch 18:  10%|█         | 31/300 [00:13<02:09,  2.09it/s]Epoch 18:  11%|█         | 32/300 [00:13<02:05,  2.14it/s]Epoch 18:  11%|█         | 33/300 [00:14<01:58,  2.24it/s]Epoch 18:  11%|█▏        | 34/300 [00:14<02:00,  2.21it/s]Epoch 18:  12%|█▏        | 35/300 [00:15<02:04,  2.13it/s]Epoch 18:  12%|█▏        | 36/300 [00:15<02:07,  2.07it/s]Epoch 18:  12%|█▏        | 37/300 [00:16<02:03,  2.14it/s]Epoch 18:  13%|█▎        | 38/300 [00:16<02:03,  2.13it/s]Epoch 18:  13%|█▎        | 39/300 [00:17<02:03,  2.11it/s]06/19/2022 15:06:57 - INFO - __main__ - global step: 2720; train loss: 7.1672821044921875; dev loss: 7.217007637023926
Epoch 18:  13%|█▎        | 40/300 [00:17<02:00,  2.15it/s]Epoch 18:  14%|█▎        | 41/300 [00:18<01:54,  2.26it/s]Epoch 18:  14%|█▍        | 42/300 [00:18<01:48,  2.38it/s]Epoch 18:  14%|█▍        | 43/300 [00:18<01:42,  2.50it/s]Epoch 18:  15%|█▍        | 44/300 [00:19<01:51,  2.29it/s]Epoch 18:  15%|█▌        | 45/300 [00:19<01:44,  2.45it/s]Epoch 18:  15%|█▌        | 46/300 [00:19<01:39,  2.56it/s]Epoch 18:  16%|█▌        | 47/300 [00:20<01:35,  2.65it/s]Epoch 18:  16%|█▌        | 48/300 [00:20<01:43,  2.44it/s]Epoch 18:  16%|█▋        | 49/300 [00:21<01:40,  2.49it/s]Epoch 18:  17%|█▋        | 50/300 [00:21<01:36,  2.59it/s]Epoch 18:  17%|█▋        | 51/300 [00:21<01:33,  2.65it/s]Epoch 18:  17%|█▋        | 52/300 [00:22<01:43,  2.39it/s]Epoch 18:  18%|█▊        | 53/300 [00:22<01:38,  2.50it/s]Epoch 18:  18%|█▊        | 54/300 [00:23<01:37,  2.54it/s]Epoch 18:  18%|█▊        | 55/300 [00:23<01:37,  2.52it/s]Epoch 18:  19%|█▊        | 56/300 [00:24<01:55,  2.11it/s]Epoch 18:  19%|█▉        | 57/300 [00:24<02:01,  2.00it/s]Epoch 18:  19%|█▉        | 58/300 [00:25<02:02,  1.98it/s]Epoch 18:  20%|█▉        | 59/300 [00:25<01:55,  2.08it/s]06/19/2022 15:07:06 - INFO - __main__ - global step: 2730; train loss: 7.2658371925354; dev loss: 7.091526031494141
Epoch 18:  20%|██        | 60/300 [00:26<01:56,  2.06it/s]Epoch 18:  20%|██        | 61/300 [00:26<01:54,  2.09it/s]Epoch 18:  21%|██        | 62/300 [00:27<01:52,  2.12it/s]Epoch 18:  21%|██        | 63/300 [00:27<01:45,  2.24it/s]Epoch 18:  21%|██▏       | 64/300 [00:27<01:48,  2.18it/s]Epoch 18:  22%|██▏       | 65/300 [00:28<01:41,  2.32it/s]Epoch 18:  22%|██▏       | 66/300 [00:28<01:41,  2.31it/s]Epoch 18:  22%|██▏       | 67/300 [00:29<01:41,  2.30it/s]Epoch 18:  23%|██▎       | 68/300 [00:29<01:45,  2.21it/s]Epoch 18:  23%|██▎       | 69/300 [00:30<01:41,  2.29it/s]Epoch 18:  23%|██▎       | 70/300 [00:30<01:41,  2.27it/s]Epoch 18:  24%|██▎       | 71/300 [00:30<01:36,  2.37it/s]Epoch 18:  24%|██▍       | 72/300 [00:31<01:36,  2.37it/s]Epoch 18:  24%|██▍       | 73/300 [00:31<01:47,  2.10it/s]Epoch 18:  25%|██▍       | 74/300 [00:32<01:42,  2.20it/s]Epoch 18:  25%|██▌       | 75/300 [00:32<01:43,  2.18it/s]Epoch 18:  25%|██▌       | 76/300 [00:33<01:43,  2.16it/s]Epoch 18:  26%|██▌       | 77/300 [00:33<01:54,  1.94it/s]Epoch 18:  26%|██▌       | 78/300 [00:34<01:50,  2.00it/s]Epoch 18:  26%|██▋       | 79/300 [00:34<01:45,  2.10it/s]06/19/2022 15:07:15 - INFO - __main__ - global step: 2740; train loss: 7.074729919433594; dev loss: 7.233722686767578
Epoch 18:  27%|██▋       | 80/300 [00:35<01:38,  2.23it/s]Epoch 18:  27%|██▋       | 81/300 [00:35<01:41,  2.16it/s]Epoch 18:  27%|██▋       | 82/300 [00:36<01:37,  2.23it/s]Epoch 18:  28%|██▊       | 83/300 [00:36<01:33,  2.32it/s]Epoch 18:  28%|██▊       | 84/300 [00:36<01:32,  2.32it/s]Epoch 18:  28%|██▊       | 85/300 [00:37<01:36,  2.22it/s]Epoch 18:  29%|██▊       | 86/300 [00:37<01:29,  2.40it/s]Epoch 18:  29%|██▉       | 87/300 [00:38<01:27,  2.44it/s]Epoch 18:  29%|██▉       | 88/300 [00:38<01:25,  2.47it/s]Epoch 18:  30%|██▉       | 89/300 [00:39<01:29,  2.36it/s]Epoch 18:  30%|███       | 90/300 [00:39<01:27,  2.41it/s]Epoch 18:  30%|███       | 91/300 [00:39<01:23,  2.49it/s]Epoch 18:  31%|███       | 92/300 [00:40<01:20,  2.57it/s]Epoch 18:  31%|███       | 93/300 [00:40<01:24,  2.45it/s]Epoch 18:  31%|███▏      | 94/300 [00:41<01:28,  2.33it/s]Epoch 18:  32%|███▏      | 95/300 [00:41<01:29,  2.29it/s]Epoch 18:  32%|███▏      | 96/300 [00:42<01:30,  2.25it/s]Epoch 18:  32%|███▏      | 97/300 [00:42<01:30,  2.24it/s]Epoch 18:  33%|███▎      | 98/300 [00:42<01:29,  2.25it/s]Epoch 18:  33%|███▎      | 99/300 [00:43<01:23,  2.42it/s]06/19/2022 15:07:23 - INFO - __main__ - global step: 2750; train loss: 6.779075622558594; dev loss: 6.8317461013793945
Epoch 18:  33%|███▎      | 100/300 [00:43<01:18,  2.54it/s]Epoch 18:  34%|███▎      | 101/300 [00:43<01:15,  2.63it/s]Epoch 18:  34%|███▍      | 102/300 [00:44<01:23,  2.36it/s]Epoch 18:  34%|███▍      | 103/300 [00:44<01:25,  2.30it/s]Epoch 18:  35%|███▍      | 104/300 [00:45<01:26,  2.28it/s]Epoch 18:  35%|███▌      | 105/300 [00:45<01:28,  2.21it/s]Epoch 18:  35%|███▌      | 106/300 [00:46<01:31,  2.12it/s]Epoch 18:  36%|███▌      | 107/300 [00:46<01:30,  2.14it/s]Epoch 18:  36%|███▌      | 108/300 [00:47<01:29,  2.16it/s]Epoch 18:  36%|███▋      | 109/300 [00:47<01:27,  2.17it/s]Epoch 18:  37%|███▋      | 110/300 [00:48<01:34,  2.01it/s]Epoch 18:  37%|███▋      | 111/300 [00:48<01:35,  1.97it/s]Epoch 18:  37%|███▋      | 112/300 [00:49<01:28,  2.13it/s]Epoch 18:  38%|███▊      | 113/300 [00:49<01:23,  2.25it/s]Epoch 18:  38%|███▊      | 114/300 [00:50<01:26,  2.16it/s]Epoch 18:  38%|███▊      | 115/300 [00:50<01:19,  2.33it/s]Epoch 18:  39%|███▊      | 116/300 [00:50<01:14,  2.48it/s]Epoch 18:  39%|███▉      | 117/300 [00:51<01:10,  2.59it/s]Epoch 18:  39%|███▉      | 118/300 [00:51<01:17,  2.34it/s]Epoch 18:  40%|███▉      | 119/300 [00:52<01:13,  2.47it/s]06/19/2022 15:07:32 - INFO - __main__ - global step: 2760; train loss: 6.925545692443848; dev loss: 6.741513252258301
Epoch 18:  40%|████      | 120/300 [00:52<01:09,  2.58it/s]Epoch 18:  40%|████      | 121/300 [00:52<01:11,  2.51it/s]Epoch 18:  41%|████      | 122/300 [00:53<01:15,  2.35it/s]Epoch 18:  41%|████      | 123/300 [00:53<01:12,  2.43it/s]Epoch 18:  41%|████▏     | 124/300 [00:54<01:09,  2.55it/s]Epoch 18:  42%|████▏     | 125/300 [00:54<01:06,  2.65it/s]Epoch 18:  42%|████▏     | 126/300 [00:54<01:08,  2.56it/s]Epoch 18:  42%|████▏     | 127/300 [00:55<01:13,  2.35it/s]Epoch 18:  43%|████▎     | 128/300 [00:55<01:14,  2.30it/s]Epoch 18:  43%|████▎     | 129/300 [00:56<01:19,  2.14it/s]Epoch 18:  43%|████▎     | 130/300 [00:56<01:15,  2.26it/s]Epoch 18:  44%|████▎     | 131/300 [00:57<01:22,  2.05it/s]Epoch 18:  44%|████▍     | 132/300 [00:57<01:21,  2.07it/s]Epoch 18:  44%|████▍     | 133/300 [00:58<01:17,  2.14it/s]Epoch 18:  45%|████▍     | 134/300 [00:58<01:16,  2.18it/s]Epoch 18:  45%|████▌     | 135/300 [00:59<01:26,  1.92it/s]Epoch 18:  45%|████▌     | 136/300 [00:59<01:19,  2.07it/s]Epoch 18:  46%|████▌     | 137/300 [01:00<01:20,  2.03it/s]Epoch 18:  46%|████▌     | 138/300 [01:00<01:18,  2.07it/s]Epoch 18:  46%|████▋     | 139/300 [01:01<01:22,  1.95it/s]06/19/2022 15:07:41 - INFO - __main__ - global step: 2770; train loss: 7.566317081451416; dev loss: 7.393641471862793
Epoch 18:  47%|████▋     | 140/300 [01:01<01:20,  2.00it/s]Epoch 18:  47%|████▋     | 141/300 [01:02<01:17,  2.05it/s]Epoch 18:  47%|████▋     | 142/300 [01:02<01:10,  2.24it/s]Epoch 18:  48%|████▊     | 143/300 [01:02<01:10,  2.23it/s]Epoch 18:  48%|████▊     | 144/300 [01:03<01:07,  2.32it/s]Epoch 18:  48%|████▊     | 145/300 [01:03<01:04,  2.42it/s]Epoch 18:  49%|████▊     | 146/300 [01:04<01:01,  2.51it/s]Epoch 18:  49%|████▉     | 147/300 [01:04<01:04,  2.39it/s]Epoch 18:  49%|████▉     | 148/300 [01:04<01:00,  2.52it/s]Epoch 18:  50%|████▉     | 149/300 [01:05<00:58,  2.60it/s]Epoch 18:  50%|█████     | 150/300 [01:05<00:57,  2.61it/s]Epoch 18:  50%|█████     | 151/300 [01:06<00:56,  2.63it/s]Epoch 18:  51%|█████     | 152/300 [01:06<01:01,  2.42it/s]Epoch 18:  51%|█████     | 153/300 [01:06<00:58,  2.51it/s]Epoch 18:  51%|█████▏    | 154/300 [01:07<00:55,  2.62it/s]Epoch 18:  52%|█████▏    | 155/300 [01:07<00:57,  2.50it/s]Epoch 18:  52%|█████▏    | 156/300 [01:08<01:04,  2.23it/s]Epoch 18:  52%|█████▏    | 157/300 [01:08<01:06,  2.16it/s]Epoch 18:  53%|█████▎    | 158/300 [01:09<01:09,  2.04it/s]Epoch 18:  53%|█████▎    | 159/300 [01:09<01:07,  2.10it/s]06/19/2022 15:07:50 - INFO - __main__ - global step: 2780; train loss: 6.677691459655762; dev loss: 6.785210609436035
Epoch 18:  53%|█████▎    | 160/300 [01:10<01:06,  2.11it/s]Epoch 18:  54%|█████▎    | 161/300 [01:10<01:00,  2.28it/s]Epoch 18:  54%|█████▍    | 162/300 [01:10<00:57,  2.41it/s]Epoch 18:  54%|█████▍    | 163/300 [01:11<00:57,  2.38it/s]Epoch 18:  55%|█████▍    | 164/300 [01:11<00:58,  2.31it/s]Epoch 18:  55%|█████▌    | 165/300 [01:12<00:57,  2.35it/s]Epoch 18:  55%|█████▌    | 166/300 [01:12<00:57,  2.35it/s]Epoch 18:  56%|█████▌    | 167/300 [01:13<00:59,  2.24it/s]Epoch 18:  56%|█████▌    | 168/300 [01:13<01:01,  2.16it/s]Epoch 18:  56%|█████▋    | 169/300 [01:14<00:58,  2.26it/s]Epoch 18:  57%|█████▋    | 170/300 [01:14<00:55,  2.33it/s]Epoch 18:  57%|█████▋    | 171/300 [01:14<00:53,  2.39it/s]Epoch 18:  57%|█████▋    | 172/300 [01:15<00:55,  2.30it/s]Epoch 18:  58%|█████▊    | 173/300 [01:15<00:52,  2.44it/s]Epoch 18:  58%|█████▊    | 174/300 [01:16<00:49,  2.52it/s]Epoch 18:  58%|█████▊    | 175/300 [01:16<00:49,  2.51it/s]Epoch 18:  59%|█████▊    | 176/300 [01:17<00:56,  2.18it/s]Epoch 18:  59%|█████▉    | 177/300 [01:17<00:56,  2.16it/s]Epoch 18:  59%|█████▉    | 178/300 [01:17<00:57,  2.11it/s]Epoch 18:  60%|█████▉    | 179/300 [01:18<00:53,  2.28it/s]06/19/2022 15:07:59 - INFO - __main__ - global step: 2790; train loss: 7.335411071777344; dev loss: 7.287595272064209
Epoch 18:  60%|██████    | 180/300 [01:18<00:50,  2.36it/s]Epoch 18:  60%|██████    | 181/300 [01:19<00:53,  2.24it/s]Epoch 18:  61%|██████    | 182/300 [01:19<00:49,  2.38it/s]Epoch 18:  61%|██████    | 183/300 [01:19<00:46,  2.49it/s]Epoch 18:  61%|██████▏   | 184/300 [01:20<00:44,  2.58it/s]Epoch 18:  62%|██████▏   | 185/300 [01:20<00:47,  2.40it/s]Epoch 18:  62%|██████▏   | 186/300 [01:21<00:46,  2.48it/s]Epoch 18:  62%|██████▏   | 187/300 [01:21<00:43,  2.58it/s]Epoch 18:  63%|██████▎   | 188/300 [01:21<00:42,  2.61it/s]Epoch 18:  63%|██████▎   | 189/300 [01:22<00:45,  2.45it/s]Epoch 18:  63%|██████▎   | 190/300 [01:22<00:44,  2.49it/s]Epoch 18:  64%|██████▎   | 191/300 [01:23<00:43,  2.53it/s]Epoch 18:  64%|██████▍   | 192/300 [01:23<00:41,  2.61it/s]Epoch 18:  64%|██████▍   | 193/300 [01:23<00:43,  2.44it/s]Epoch 18:  65%|██████▍   | 194/300 [01:24<00:41,  2.55it/s]Epoch 18:  65%|██████▌   | 195/300 [01:24<00:40,  2.61it/s]Epoch 18:  65%|██████▌   | 196/300 [01:25<00:39,  2.61it/s]Epoch 18:  66%|██████▌   | 197/300 [01:25<00:48,  2.13it/s]Epoch 18:  66%|██████▌   | 198/300 [01:26<00:47,  2.13it/s]Epoch 18:  66%|██████▋   | 199/300 [01:26<00:46,  2.16it/s]06/19/2022 15:08:07 - INFO - __main__ - global step: 2800; train loss: 7.280220031738281; dev loss: 7.426217079162598
Epoch 18:  67%|██████▋   | 200/300 [01:27<00:44,  2.22it/s]Epoch 18:  67%|██████▋   | 201/300 [01:27<00:47,  2.07it/s]Epoch 18:  67%|██████▋   | 202/300 [01:27<00:43,  2.24it/s]Epoch 18:  68%|██████▊   | 203/300 [01:28<00:41,  2.33it/s]Epoch 18:  68%|██████▊   | 204/300 [01:28<00:40,  2.38it/s]Epoch 18:  68%|██████▊   | 205/300 [01:29<00:41,  2.28it/s]Epoch 18:  69%|██████▊   | 206/300 [01:29<00:45,  2.05it/s]Epoch 18:  69%|██████▉   | 207/300 [01:30<00:46,  2.01it/s]Epoch 18:  69%|██████▉   | 208/300 [01:30<00:45,  2.04it/s]Epoch 18:  70%|██████▉   | 209/300 [01:31<00:43,  2.08it/s]Epoch 18:  70%|███████   | 210/300 [01:31<00:46,  1.94it/s]Epoch 18:  70%|███████   | 211/300 [01:32<00:43,  2.03it/s]Epoch 18:  71%|███████   | 212/300 [01:32<00:42,  2.08it/s]Epoch 18:  71%|███████   | 213/300 [01:33<00:41,  2.12it/s]Epoch 18:  71%|███████▏  | 214/300 [01:33<00:43,  1.98it/s]Epoch 18:  72%|███████▏  | 215/300 [01:34<00:41,  2.06it/s]Epoch 18:  72%|███████▏  | 216/300 [01:34<00:39,  2.15it/s]Epoch 18:  72%|███████▏  | 217/300 [01:35<00:39,  2.13it/s]Epoch 18:  73%|███████▎  | 218/300 [01:35<00:40,  2.01it/s]Epoch 18:  73%|███████▎  | 219/300 [01:36<00:38,  2.08it/s]06/19/2022 15:08:17 - INFO - __main__ - global step: 2810; train loss: 7.3072381019592285; dev loss: 7.15071964263916
Epoch 18:  73%|███████▎  | 220/300 [01:36<00:40,  1.95it/s]Epoch 18:  74%|███████▎  | 221/300 [01:37<00:40,  1.96it/s]Epoch 18:  74%|███████▍  | 222/300 [01:37<00:39,  1.98it/s]Epoch 18:  74%|███████▍  | 223/300 [01:38<00:35,  2.15it/s]Epoch 18:  75%|███████▍  | 224/300 [01:38<00:33,  2.29it/s]Epoch 18:  75%|███████▌  | 225/300 [01:38<00:31,  2.39it/s]Epoch 18:  75%|███████▌  | 226/300 [01:39<00:32,  2.29it/s]Epoch 18:  76%|███████▌  | 227/300 [01:39<00:31,  2.31it/s]Epoch 18:  76%|███████▌  | 228/300 [01:40<00:30,  2.35it/s]Epoch 18:  76%|███████▋  | 229/300 [01:40<00:28,  2.45it/s]Epoch 18:  77%|███████▋  | 230/300 [01:40<00:29,  2.39it/s]Epoch 18:  77%|███████▋  | 231/300 [01:41<00:27,  2.49it/s]Epoch 18:  77%|███████▋  | 232/300 [01:41<00:26,  2.57it/s]Epoch 18:  78%|███████▊  | 233/300 [01:42<00:25,  2.67it/s]Epoch 18:  78%|███████▊  | 234/300 [01:42<00:24,  2.67it/s]Epoch 18:  78%|███████▊  | 235/300 [01:42<00:25,  2.54it/s]Epoch 18:  79%|███████▊  | 236/300 [01:43<00:24,  2.65it/s]Epoch 18:  79%|███████▉  | 237/300 [01:43<00:23,  2.73it/s]Epoch 18:  79%|███████▉  | 238/300 [01:43<00:22,  2.71it/s]Epoch 18:  80%|███████▉  | 239/300 [01:44<00:23,  2.56it/s]06/19/2022 15:08:25 - INFO - __main__ - global step: 2820; train loss: 7.356699466705322; dev loss: 7.316854953765869
Epoch 18:  80%|████████  | 240/300 [01:44<00:25,  2.34it/s]Epoch 18:  80%|████████  | 241/300 [01:45<00:25,  2.29it/s]Epoch 18:  81%|████████  | 242/300 [01:45<00:25,  2.28it/s]Epoch 18:  81%|████████  | 243/300 [01:46<00:27,  2.09it/s]Epoch 18:  81%|████████▏ | 244/300 [01:46<00:26,  2.10it/s]Epoch 18:  82%|████████▏ | 245/300 [01:47<00:26,  2.05it/s]Epoch 18:  82%|████████▏ | 246/300 [01:47<00:24,  2.20it/s]Epoch 18:  82%|████████▏ | 247/300 [01:48<00:24,  2.18it/s]Epoch 18:  83%|████████▎ | 248/300 [01:48<00:22,  2.31it/s]Epoch 18:  83%|████████▎ | 249/300 [01:48<00:21,  2.42it/s]Epoch 18:  83%|████████▎ | 250/300 [01:49<00:20,  2.50it/s]Epoch 18:  84%|████████▎ | 251/300 [01:49<00:20,  2.39it/s]Epoch 18:  84%|████████▍ | 252/300 [01:50<00:19,  2.47it/s]Epoch 18:  84%|████████▍ | 253/300 [01:50<00:18,  2.50it/s]Epoch 18:  85%|████████▍ | 254/300 [01:50<00:18,  2.53it/s]Epoch 18:  85%|████████▌ | 255/300 [01:51<00:18,  2.38it/s]Epoch 18:  85%|████████▌ | 256/300 [01:51<00:18,  2.40it/s]Epoch 18:  86%|████████▌ | 257/300 [01:52<00:17,  2.49it/s]Epoch 18:  86%|████████▌ | 258/300 [01:52<00:16,  2.54it/s]Epoch 18:  86%|████████▋ | 259/300 [01:52<00:16,  2.42it/s]06/19/2022 15:08:33 - INFO - __main__ - global step: 2830; train loss: 6.74124002456665; dev loss: 6.812385559082031
Epoch 18:  87%|████████▋ | 260/300 [01:53<00:17,  2.34it/s]Epoch 18:  87%|████████▋ | 261/300 [01:53<00:16,  2.33it/s]Epoch 18:  87%|████████▋ | 262/300 [01:54<00:16,  2.36it/s]Epoch 18:  88%|████████▊ | 263/300 [01:54<00:15,  2.40it/s]Epoch 18:  88%|████████▊ | 264/300 [01:55<00:15,  2.26it/s]Epoch 18:  88%|████████▊ | 265/300 [01:55<00:16,  2.16it/s]Epoch 18:  89%|████████▊ | 266/300 [01:56<00:16,  2.06it/s]Epoch 18:  89%|████████▉ | 267/300 [01:56<00:15,  2.11it/s]Epoch 18:  89%|████████▉ | 268/300 [01:57<00:16,  1.99it/s]Epoch 18:  90%|████████▉ | 269/300 [01:57<00:14,  2.08it/s]Epoch 18:  90%|█████████ | 270/300 [01:58<00:13,  2.24it/s]Epoch 18:  90%|█████████ | 271/300 [01:58<00:12,  2.37it/s]Epoch 18:  91%|█████████ | 272/300 [01:58<00:12,  2.31it/s]Epoch 18:  91%|█████████ | 273/300 [01:59<00:11,  2.44it/s]Epoch 18:  91%|█████████▏| 274/300 [01:59<00:11,  2.36it/s]Epoch 18:  92%|█████████▏| 275/300 [02:00<00:10,  2.50it/s]Epoch 18:  92%|█████████▏| 276/300 [02:00<00:09,  2.42it/s]Epoch 18:  92%|█████████▏| 277/300 [02:00<00:09,  2.55it/s]Epoch 18:  93%|█████████▎| 278/300 [02:01<00:08,  2.64it/s]Epoch 18:  93%|█████████▎| 279/300 [02:01<00:08,  2.45it/s]06/19/2022 15:08:42 - INFO - __main__ - global step: 2840; train loss: 7.1871137619018555; dev loss: 7.262579441070557
Epoch 18:  93%|█████████▎| 280/300 [02:02<00:08,  2.33it/s]Epoch 18:  94%|█████████▎| 281/300 [02:02<00:08,  2.27it/s]Epoch 18:  94%|█████████▍| 282/300 [02:03<00:08,  2.21it/s]Epoch 18:  94%|█████████▍| 283/300 [02:03<00:07,  2.16it/s]Epoch 18:  95%|█████████▍| 284/300 [02:04<00:07,  2.08it/s]Epoch 18:  95%|█████████▌| 285/300 [02:04<00:06,  2.23it/s]Epoch 18:  95%|█████████▌| 286/300 [02:04<00:06,  2.24it/s]Epoch 18:  96%|█████████▌| 287/300 [02:05<00:05,  2.38it/s]Epoch 18:  96%|█████████▌| 288/300 [02:05<00:05,  2.39it/s]Epoch 18:  96%|█████████▋| 289/300 [02:06<00:04,  2.32it/s]Epoch 18:  97%|█████████▋| 290/300 [02:06<00:04,  2.48it/s]Epoch 18:  97%|█████████▋| 291/300 [02:06<00:03,  2.59it/s]Epoch 18:  97%|█████████▋| 292/300 [02:07<00:03,  2.41it/s]Epoch 18:  98%|█████████▊| 293/300 [02:07<00:03,  2.31it/s]Epoch 18:  98%|█████████▊| 294/300 [02:08<00:02,  2.20it/s]Epoch 18:  98%|█████████▊| 295/300 [02:08<00:02,  2.20it/s]Epoch 18:  99%|█████████▊| 296/300 [02:09<00:01,  2.29it/s]Epoch 18:  99%|█████████▉| 297/300 [02:09<00:01,  2.26it/s]Epoch 18:  99%|█████████▉| 298/300 [02:10<00:00,  2.16it/s]Epoch 18: 100%|█████████▉| 299/300 [02:10<00:00,  2.14it/s]06/19/2022 15:08:51 - INFO - __main__ - global step: 2850; train loss: 6.529675483703613; dev loss: 6.529473781585693
Epoch 18: 100%|██████████| 300/300 [02:10<00:00,  2.25it/s]Epoch 18: 100%|██████████| 300/300 [02:10<00:00,  2.29it/s]
Epoch 19:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 19:   0%|          | 1/300 [00:00<02:12,  2.25it/s]Epoch 19:   1%|          | 2/300 [00:00<02:00,  2.47it/s]Epoch 19:   1%|          | 3/300 [00:01<01:57,  2.53it/s]Epoch 19:   1%|▏         | 4/300 [00:01<01:53,  2.61it/s]Epoch 19:   2%|▏         | 5/300 [00:02<02:01,  2.44it/s]Epoch 19:   2%|▏         | 6/300 [00:02<01:55,  2.55it/s]Epoch 19:   2%|▏         | 7/300 [00:02<01:55,  2.53it/s]Epoch 19:   3%|▎         | 8/300 [00:03<01:53,  2.57it/s]Epoch 19:   3%|▎         | 9/300 [00:03<02:00,  2.42it/s]Epoch 19:   3%|▎         | 10/300 [00:04<01:59,  2.44it/s]Epoch 19:   4%|▎         | 11/300 [00:04<01:52,  2.57it/s]Epoch 19:   4%|▍         | 12/300 [00:04<01:51,  2.58it/s]Epoch 19:   4%|▍         | 13/300 [00:05<01:54,  2.51it/s]Epoch 19:   5%|▍         | 14/300 [00:05<02:01,  2.36it/s]Epoch 19:   5%|▌         | 15/300 [00:06<02:02,  2.33it/s]Epoch 19:   5%|▌         | 16/300 [00:06<01:57,  2.43it/s]Epoch 19:   6%|▌         | 17/300 [00:06<01:59,  2.36it/s]Epoch 19:   6%|▌         | 18/300 [00:07<02:12,  2.12it/s]Epoch 19:   6%|▋         | 19/300 [00:07<02:05,  2.25it/s]06/19/2022 15:08:59 - INFO - __main__ - global step: 2860; train loss: 6.777931213378906; dev loss: 6.891703128814697
Epoch 19:   7%|▋         | 20/300 [00:08<02:06,  2.22it/s]Epoch 19:   7%|▋         | 21/300 [00:08<02:06,  2.21it/s]Epoch 19:   7%|▋         | 22/300 [00:09<02:19,  1.99it/s]Epoch 19:   8%|▊         | 23/300 [00:09<02:13,  2.07it/s]Epoch 19:   8%|▊         | 24/300 [00:10<02:15,  2.04it/s]Epoch 19:   8%|▊         | 25/300 [00:10<02:03,  2.23it/s]Epoch 19:   9%|▊         | 26/300 [00:11<02:04,  2.20it/s]Epoch 19:   9%|▉         | 27/300 [00:11<01:55,  2.36it/s]Epoch 19:   9%|▉         | 28/300 [00:11<01:50,  2.46it/s]Epoch 19:  10%|▉         | 29/300 [00:12<01:49,  2.46it/s]Epoch 19:  10%|█         | 30/300 [00:12<01:59,  2.26it/s]Epoch 19:  10%|█         | 31/300 [00:13<01:55,  2.33it/s]Epoch 19:  11%|█         | 32/300 [00:13<01:51,  2.40it/s]Epoch 19:  11%|█         | 33/300 [00:14<01:47,  2.47it/s]Epoch 19:  11%|█▏        | 34/300 [00:14<01:56,  2.28it/s]Epoch 19:  12%|█▏        | 35/300 [00:14<01:55,  2.30it/s]Epoch 19:  12%|█▏        | 36/300 [00:15<01:55,  2.29it/s]Epoch 19:  12%|█▏        | 37/300 [00:15<01:50,  2.39it/s]Epoch 19:  13%|█▎        | 38/300 [00:16<01:58,  2.21it/s]Epoch 19:  13%|█▎        | 39/300 [00:16<01:50,  2.36it/s]06/19/2022 15:09:08 - INFO - __main__ - global step: 2870; train loss: 6.8839240074157715; dev loss: 6.716763496398926
Epoch 19:  13%|█▎        | 40/300 [00:17<01:45,  2.47it/s]Epoch 19:  14%|█▎        | 41/300 [00:17<01:40,  2.57it/s]Epoch 19:  14%|█▍        | 42/300 [00:17<01:40,  2.56it/s]Epoch 19:  14%|█▍        | 43/300 [00:18<01:47,  2.39it/s]Epoch 19:  15%|█▍        | 44/300 [00:18<01:52,  2.27it/s]Epoch 19:  15%|█▌        | 45/300 [00:19<01:47,  2.37it/s]Epoch 19:  15%|█▌        | 46/300 [00:19<01:48,  2.34it/s]Epoch 19:  16%|█▌        | 47/300 [00:20<01:55,  2.20it/s]Epoch 19:  16%|█▌        | 48/300 [00:20<01:53,  2.23it/s]Epoch 19:  16%|█▋        | 49/300 [00:20<01:53,  2.21it/s]Epoch 19:  17%|█▋        | 50/300 [00:21<01:56,  2.15it/s]Epoch 19:  17%|█▋        | 51/300 [00:21<01:57,  2.13it/s]Epoch 19:  17%|█▋        | 52/300 [00:22<01:49,  2.27it/s]Epoch 19:  18%|█▊        | 53/300 [00:22<01:46,  2.33it/s]Epoch 19:  18%|█▊        | 54/300 [00:23<01:55,  2.13it/s]Epoch 19:  18%|█▊        | 55/300 [00:23<02:03,  1.98it/s]Epoch 19:  19%|█▊        | 56/300 [00:24<01:58,  2.06it/s]Epoch 19:  19%|█▉        | 57/300 [00:24<01:48,  2.24it/s]Epoch 19:  19%|█▉        | 58/300 [00:25<01:41,  2.38it/s]Epoch 19:  20%|█▉        | 59/300 [00:25<01:44,  2.31it/s]06/19/2022 15:09:17 - INFO - __main__ - global step: 2880; train loss: 7.379240989685059; dev loss: 7.5531005859375
Epoch 19:  20%|██        | 60/300 [00:25<01:43,  2.32it/s]Epoch 19:  20%|██        | 61/300 [00:26<01:46,  2.25it/s]Epoch 19:  21%|██        | 62/300 [00:26<01:38,  2.41it/s]Epoch 19:  21%|██        | 63/300 [00:27<01:48,  2.19it/s]Epoch 19:  21%|██▏       | 64/300 [00:27<01:43,  2.29it/s]Epoch 19:  22%|██▏       | 65/300 [00:28<01:42,  2.29it/s]Epoch 19:  22%|██▏       | 66/300 [00:28<01:41,  2.29it/s]Epoch 19:  22%|██▏       | 67/300 [00:28<01:41,  2.30it/s]Epoch 19:  23%|██▎       | 68/300 [00:29<01:54,  2.03it/s]Epoch 19:  23%|██▎       | 69/300 [00:30<01:50,  2.10it/s]Epoch 19:  23%|██▎       | 70/300 [00:30<01:47,  2.14it/s]Epoch 19:  24%|██▎       | 71/300 [00:30<01:45,  2.16it/s]Epoch 19:  24%|██▍       | 72/300 [00:31<01:53,  2.00it/s]Epoch 19:  24%|██▍       | 73/300 [00:32<01:50,  2.05it/s]Epoch 19:  25%|██▍       | 74/300 [00:32<01:47,  2.11it/s]Epoch 19:  25%|██▌       | 75/300 [00:32<01:46,  2.11it/s]Epoch 19:  25%|██▌       | 76/300 [00:33<01:53,  1.97it/s]Epoch 19:  26%|██▌       | 77/300 [00:33<01:48,  2.05it/s]Epoch 19:  26%|██▌       | 78/300 [00:34<01:47,  2.07it/s]Epoch 19:  26%|██▋       | 79/300 [00:34<01:51,  1.99it/s]06/19/2022 15:09:26 - INFO - __main__ - global step: 2890; train loss: 6.779458522796631; dev loss: 6.859185695648193
Epoch 19:  27%|██▋       | 80/300 [00:35<01:57,  1.88it/s]Epoch 19:  27%|██▋       | 81/300 [00:36<01:51,  1.97it/s]Epoch 19:  27%|██▋       | 82/300 [00:36<01:45,  2.06it/s]Epoch 19:  28%|██▊       | 83/300 [00:36<01:45,  2.06it/s]Epoch 19:  28%|██▊       | 84/300 [00:37<01:55,  1.87it/s]Epoch 19:  28%|██▊       | 85/300 [00:38<01:49,  1.96it/s]Epoch 19:  29%|██▊       | 86/300 [00:38<01:48,  1.97it/s]Epoch 19:  29%|██▉       | 87/300 [00:39<01:45,  2.02it/s]Epoch 19:  29%|██▉       | 88/300 [00:39<01:51,  1.90it/s]Epoch 19:  30%|██▉       | 89/300 [00:40<01:51,  1.89it/s]Epoch 19:  30%|███       | 90/300 [00:40<01:48,  1.93it/s]Epoch 19:  30%|███       | 91/300 [00:41<01:44,  2.00it/s]Epoch 19:  31%|███       | 92/300 [00:41<01:46,  1.95it/s]Epoch 19:  31%|███       | 93/300 [00:42<01:37,  2.12it/s]Epoch 19:  31%|███▏      | 94/300 [00:42<01:35,  2.16it/s]Epoch 19:  32%|███▏      | 95/300 [00:42<01:30,  2.25it/s]Epoch 19:  32%|███▏      | 96/300 [00:43<01:28,  2.29it/s]Epoch 19:  32%|███▏      | 97/300 [00:43<01:39,  2.05it/s]Epoch 19:  33%|███▎      | 98/300 [00:44<01:41,  2.00it/s]Epoch 19:  33%|███▎      | 99/300 [00:44<01:36,  2.07it/s]06/19/2022 15:09:36 - INFO - __main__ - global step: 2900; train loss: 7.042685031890869; dev loss: 7.110835075378418
Epoch 19:  33%|███▎      | 100/300 [00:45<01:29,  2.25it/s]Epoch 19:  34%|███▎      | 101/300 [00:45<01:32,  2.16it/s]Epoch 19:  34%|███▍      | 102/300 [00:46<01:25,  2.32it/s]Epoch 19:  34%|███▍      | 103/300 [00:46<01:20,  2.46it/s]Epoch 19:  35%|███▍      | 104/300 [00:46<01:17,  2.53it/s]Epoch 19:  35%|███▌      | 105/300 [00:47<01:23,  2.35it/s]Epoch 19:  35%|███▌      | 106/300 [00:47<01:20,  2.41it/s]Epoch 19:  36%|███▌      | 107/300 [00:48<01:16,  2.52it/s]Epoch 19:  36%|███▌      | 108/300 [00:48<01:13,  2.60it/s]Epoch 19:  36%|███▋      | 109/300 [00:48<01:20,  2.37it/s]Epoch 19:  37%|███▋      | 110/300 [00:49<01:20,  2.37it/s]Epoch 19:  37%|███▋      | 111/300 [00:49<01:21,  2.31it/s]Epoch 19:  37%|███▋      | 112/300 [00:50<01:22,  2.28it/s]Epoch 19:  38%|███▊      | 113/300 [00:50<01:31,  2.05it/s]Epoch 19:  38%|███▊      | 114/300 [00:51<01:29,  2.07it/s]Epoch 19:  38%|███▊      | 115/300 [00:51<01:27,  2.12it/s]Epoch 19:  39%|███▊      | 116/300 [00:52<01:27,  2.11it/s]Epoch 19:  39%|███▉      | 117/300 [00:52<01:37,  1.88it/s]Epoch 19:  39%|███▉      | 118/300 [00:53<01:33,  1.94it/s]Epoch 19:  40%|███▉      | 119/300 [00:53<01:30,  2.00it/s]06/19/2022 15:09:45 - INFO - __main__ - global step: 2910; train loss: 7.015674591064453; dev loss: 7.0135087966918945
Epoch 19:  40%|████      | 120/300 [00:54<01:29,  2.01it/s]Epoch 19:  40%|████      | 121/300 [00:54<01:27,  2.05it/s]Epoch 19:  41%|████      | 122/300 [00:55<01:33,  1.91it/s]Epoch 19:  41%|████      | 123/300 [00:55<01:25,  2.07it/s]Epoch 19:  41%|████▏     | 124/300 [00:56<01:18,  2.24it/s]Epoch 19:  42%|████▏     | 125/300 [00:56<01:13,  2.39it/s]Epoch 19:  42%|████▏     | 126/300 [00:56<01:15,  2.31it/s]Epoch 19:  42%|████▏     | 127/300 [00:57<01:14,  2.34it/s]Epoch 19:  43%|████▎     | 128/300 [00:57<01:22,  2.08it/s]Epoch 19:  43%|████▎     | 129/300 [00:58<01:22,  2.07it/s]Epoch 19:  43%|████▎     | 130/300 [00:59<01:30,  1.88it/s]Epoch 19:  44%|████▎     | 131/300 [00:59<01:29,  1.89it/s]Epoch 19:  44%|████▍     | 132/300 [01:00<01:25,  1.96it/s]Epoch 19:  44%|████▍     | 133/300 [01:00<01:25,  1.96it/s]Epoch 19:  45%|████▍     | 134/300 [01:01<01:30,  1.83it/s]Epoch 19:  45%|████▌     | 135/300 [01:01<01:22,  2.00it/s]Epoch 19:  45%|████▌     | 136/300 [01:02<01:15,  2.16it/s]Epoch 19:  46%|████▌     | 137/300 [01:02<01:11,  2.28it/s]Epoch 19:  46%|████▌     | 138/300 [01:02<01:14,  2.18it/s]Epoch 19:  46%|████▋     | 139/300 [01:03<01:11,  2.24it/s]06/19/2022 15:09:55 - INFO - __main__ - global step: 2920; train loss: 6.909174919128418; dev loss: 6.5978593826293945
Epoch 19:  47%|████▋     | 140/300 [01:03<01:11,  2.24it/s]Epoch 19:  47%|████▋     | 141/300 [01:04<01:11,  2.23it/s]Epoch 19:  47%|████▋     | 142/300 [01:04<01:18,  2.02it/s]Epoch 19:  48%|████▊     | 143/300 [01:05<01:13,  2.15it/s]Epoch 19:  48%|████▊     | 144/300 [01:05<01:11,  2.19it/s]Epoch 19:  48%|████▊     | 145/300 [01:06<01:09,  2.22it/s]Epoch 19:  49%|████▊     | 146/300 [01:06<01:12,  2.13it/s]Epoch 19:  49%|████▉     | 147/300 [01:06<01:07,  2.26it/s]Epoch 19:  49%|████▉     | 148/300 [01:07<01:05,  2.33it/s]Epoch 19:  50%|████▉     | 149/300 [01:07<01:05,  2.32it/s]Epoch 19:  50%|█████     | 150/300 [01:08<01:03,  2.37it/s]Epoch 19:  50%|█████     | 151/300 [01:08<01:10,  2.12it/s]Epoch 19:  51%|█████     | 152/300 [01:09<01:12,  2.04it/s]Epoch 19:  51%|█████     | 153/300 [01:09<01:10,  2.09it/s]Epoch 19:  51%|█████▏    | 154/300 [01:10<01:09,  2.11it/s]Epoch 19:  52%|█████▏    | 155/300 [01:10<01:15,  1.92it/s]Epoch 19:  52%|█████▏    | 156/300 [01:11<01:14,  1.93it/s]Epoch 19:  52%|█████▏    | 157/300 [01:11<01:13,  1.94it/s]Epoch 19:  53%|█████▎    | 158/300 [01:12<01:10,  2.02it/s]Epoch 19:  53%|█████▎    | 159/300 [01:12<01:14,  1.88it/s]06/19/2022 15:10:04 - INFO - __main__ - global step: 2930; train loss: 7.216020107269287; dev loss: 7.294045448303223
Epoch 19:  53%|█████▎    | 160/300 [01:13<01:11,  1.95it/s]Epoch 19:  54%|█████▎    | 161/300 [01:13<01:09,  2.01it/s]Epoch 19:  54%|█████▍    | 162/300 [01:14<01:07,  2.06it/s]Epoch 19:  54%|█████▍    | 163/300 [01:14<01:11,  1.91it/s]Epoch 19:  55%|█████▍    | 164/300 [01:15<01:10,  1.93it/s]Epoch 19:  55%|█████▌    | 165/300 [01:15<01:09,  1.95it/s]Epoch 19:  55%|█████▌    | 166/300 [01:16<01:06,  2.01it/s]Epoch 19:  56%|█████▌    | 167/300 [01:17<01:11,  1.87it/s]Epoch 19:  56%|█████▌    | 168/300 [01:17<01:09,  1.89it/s]Epoch 19:  56%|█████▋    | 169/300 [01:18<01:08,  1.92it/s]Epoch 19:  57%|█████▋    | 170/300 [01:18<01:05,  1.99it/s]Epoch 19:  57%|█████▋    | 171/300 [01:19<01:11,  1.80it/s]Epoch 19:  57%|█████▋    | 172/300 [01:19<01:06,  1.93it/s]Epoch 19:  58%|█████▊    | 173/300 [01:20<01:02,  2.03it/s]Epoch 19:  58%|█████▊    | 174/300 [01:20<01:01,  2.05it/s]Epoch 19:  58%|█████▊    | 175/300 [01:21<01:00,  2.08it/s]Epoch 19:  59%|█████▊    | 176/300 [01:21<01:04,  1.93it/s]Epoch 19:  59%|█████▉    | 177/300 [01:21<00:57,  2.13it/s]Epoch 19:  59%|█████▉    | 178/300 [01:22<00:53,  2.29it/s]Epoch 19:  60%|█████▉    | 179/300 [01:22<00:50,  2.42it/s]06/19/2022 15:10:14 - INFO - __main__ - global step: 2940; train loss: 7.036763668060303; dev loss: 6.83059024810791
Epoch 19:  60%|██████    | 180/300 [01:23<00:52,  2.29it/s]Epoch 19:  60%|██████    | 181/300 [01:23<00:52,  2.27it/s]Epoch 19:  61%|██████    | 182/300 [01:24<00:53,  2.20it/s]Epoch 19:  61%|██████    | 183/300 [01:24<00:55,  2.10it/s]Epoch 19:  61%|██████▏   | 184/300 [01:25<01:00,  1.93it/s]Epoch 19:  62%|██████▏   | 185/300 [01:25<00:55,  2.07it/s]Epoch 19:  62%|██████▏   | 186/300 [01:26<00:51,  2.20it/s]Epoch 19:  62%|██████▏   | 187/300 [01:26<00:50,  2.24it/s]Epoch 19:  63%|██████▎   | 188/300 [01:27<00:57,  1.95it/s]Epoch 19:  63%|██████▎   | 189/300 [01:27<00:52,  2.11it/s]Epoch 19:  63%|██████▎   | 190/300 [01:27<00:48,  2.25it/s]Epoch 19:  64%|██████▎   | 191/300 [01:28<00:45,  2.40it/s]Epoch 19:  64%|██████▍   | 192/300 [01:28<00:47,  2.29it/s]Epoch 19:  64%|██████▍   | 193/300 [01:29<00:45,  2.38it/s]Epoch 19:  65%|██████▍   | 194/300 [01:29<00:44,  2.40it/s]Epoch 19:  65%|██████▌   | 195/300 [01:29<00:41,  2.51it/s]Epoch 19:  65%|██████▌   | 196/300 [01:30<00:45,  2.28it/s]Epoch 19:  66%|██████▌   | 197/300 [01:30<00:44,  2.31it/s]Epoch 19:  66%|██████▌   | 198/300 [01:31<00:43,  2.34it/s]Epoch 19:  66%|██████▋   | 199/300 [01:31<00:42,  2.36it/s]06/19/2022 15:10:23 - INFO - __main__ - global step: 2950; train loss: 6.87438440322876; dev loss: 6.686234951019287
Epoch 19:  67%|██████▋   | 200/300 [01:32<00:43,  2.29it/s]Epoch 19:  67%|██████▋   | 201/300 [01:32<00:41,  2.39it/s]Epoch 19:  67%|██████▋   | 202/300 [01:32<00:42,  2.30it/s]Epoch 19:  68%|██████▊   | 203/300 [01:33<00:43,  2.21it/s]Epoch 19:  68%|██████▊   | 204/300 [01:33<00:43,  2.22it/s]Epoch 19:  68%|██████▊   | 205/300 [01:34<00:46,  2.04it/s]Epoch 19:  69%|██████▊   | 206/300 [01:34<00:45,  2.05it/s]Epoch 19:  69%|██████▉   | 207/300 [01:35<00:45,  2.06it/s]Epoch 19:  69%|██████▉   | 208/300 [01:35<00:44,  2.06it/s]Epoch 19:  70%|██████▉   | 209/300 [01:36<00:46,  1.95it/s]Epoch 19:  70%|███████   | 210/300 [01:37<00:44,  2.01it/s]Epoch 19:  70%|███████   | 211/300 [01:37<00:43,  2.06it/s]Epoch 19:  71%|███████   | 212/300 [01:37<00:41,  2.11it/s]Epoch 19:  71%|███████   | 213/300 [01:38<00:44,  1.96it/s]Epoch 19:  71%|███████▏  | 214/300 [01:39<00:44,  1.95it/s]Epoch 19:  72%|███████▏  | 215/300 [01:39<00:42,  2.01it/s]Epoch 19:  72%|███████▏  | 216/300 [01:39<00:40,  2.08it/s]Epoch 19:  72%|███████▏  | 217/300 [01:40<00:41,  2.01it/s]Epoch 19:  73%|███████▎  | 218/300 [01:40<00:37,  2.19it/s]Epoch 19:  73%|███████▎  | 219/300 [01:41<00:34,  2.33it/s]06/19/2022 15:10:32 - INFO - __main__ - global step: 2960; train loss: 6.77002477645874; dev loss: 6.677700996398926
Epoch 19:  73%|███████▎  | 220/300 [01:41<00:33,  2.41it/s]Epoch 19:  74%|███████▎  | 221/300 [01:42<00:34,  2.32it/s]Epoch 19:  74%|███████▍  | 222/300 [01:42<00:31,  2.44it/s]Epoch 19:  74%|███████▍  | 223/300 [01:42<00:30,  2.54it/s]Epoch 19:  75%|███████▍  | 224/300 [01:43<00:30,  2.48it/s]Epoch 19:  75%|███████▌  | 225/300 [01:43<00:35,  2.11it/s]Epoch 19:  75%|███████▌  | 226/300 [01:44<00:33,  2.23it/s]Epoch 19:  76%|███████▌  | 227/300 [01:44<00:31,  2.32it/s]Epoch 19:  76%|███████▌  | 228/300 [01:45<00:31,  2.27it/s]Epoch 19:  76%|███████▋  | 229/300 [01:45<00:31,  2.26it/s]Epoch 19:  77%|███████▋  | 230/300 [01:46<00:32,  2.16it/s]Epoch 19:  77%|███████▋  | 231/300 [01:46<00:32,  2.10it/s]Epoch 19:  77%|███████▋  | 232/300 [01:46<00:30,  2.26it/s]Epoch 19:  78%|███████▊  | 233/300 [01:47<00:27,  2.39it/s]Epoch 19:  78%|███████▊  | 234/300 [01:47<00:28,  2.32it/s]Epoch 19:  78%|███████▊  | 235/300 [01:48<00:26,  2.43it/s]Epoch 19:  79%|███████▊  | 236/300 [01:48<00:25,  2.54it/s]Epoch 19:  79%|███████▉  | 237/300 [01:48<00:24,  2.55it/s]Epoch 19:  79%|███████▉  | 238/300 [01:49<00:25,  2.41it/s]Epoch 19:  80%|███████▉  | 239/300 [01:49<00:24,  2.53it/s]06/19/2022 15:10:41 - INFO - __main__ - global step: 2970; train loss: 6.900876045227051; dev loss: 6.737781524658203
Epoch 19:  80%|████████  | 240/300 [01:50<00:24,  2.47it/s]Epoch 19:  80%|████████  | 241/300 [01:50<00:23,  2.52it/s]Epoch 19:  81%|████████  | 242/300 [01:50<00:24,  2.34it/s]Epoch 19:  81%|████████  | 243/300 [01:51<00:24,  2.34it/s]Epoch 19:  81%|████████▏ | 244/300 [01:51<00:24,  2.32it/s]Epoch 19:  82%|████████▏ | 245/300 [01:52<00:25,  2.18it/s]Epoch 19:  82%|████████▏ | 246/300 [01:52<00:27,  1.99it/s]Epoch 19:  82%|████████▏ | 247/300 [01:53<00:25,  2.04it/s]Epoch 19:  83%|████████▎ | 248/300 [01:53<00:23,  2.20it/s]Epoch 19:  83%|████████▎ | 249/300 [01:54<00:22,  2.30it/s]Epoch 19:  83%|████████▎ | 250/300 [01:54<00:22,  2.24it/s]Epoch 19:  84%|████████▎ | 251/300 [01:55<00:20,  2.34it/s]Epoch 19:  84%|████████▍ | 252/300 [01:55<00:21,  2.28it/s]Epoch 19:  84%|████████▍ | 253/300 [01:55<00:21,  2.20it/s]Epoch 19:  85%|████████▍ | 254/300 [01:56<00:22,  2.01it/s]Epoch 19:  85%|████████▌ | 255/300 [01:56<00:21,  2.10it/s]Epoch 19:  85%|████████▌ | 256/300 [01:57<00:20,  2.16it/s]Epoch 19:  86%|████████▌ | 257/300 [01:57<00:20,  2.05it/s]Epoch 19:  86%|████████▌ | 258/300 [01:58<00:19,  2.15it/s]Epoch 19:  86%|████████▋ | 259/300 [01:58<00:19,  2.06it/s]06/19/2022 15:10:50 - INFO - __main__ - global step: 2980; train loss: 6.55154275894165; dev loss: 6.947493553161621
Epoch 19:  87%|████████▋ | 260/300 [01:59<00:18,  2.12it/s]Epoch 19:  87%|████████▋ | 261/300 [01:59<00:17,  2.21it/s]Epoch 19:  87%|████████▋ | 262/300 [02:00<00:17,  2.21it/s]Epoch 19:  88%|████████▊ | 263/300 [02:00<00:17,  2.07it/s]Epoch 19:  88%|████████▊ | 264/300 [02:01<00:16,  2.12it/s]Epoch 19:  88%|████████▊ | 265/300 [02:01<00:16,  2.15it/s]Epoch 19:  89%|████████▊ | 266/300 [02:02<00:15,  2.21it/s]Epoch 19:  89%|████████▉ | 267/300 [02:02<00:16,  2.06it/s]Epoch 19:  89%|████████▉ | 268/300 [02:03<00:15,  2.06it/s]Epoch 19:  90%|████████▉ | 269/300 [02:03<00:14,  2.15it/s]Epoch 19:  90%|█████████ | 270/300 [02:03<00:13,  2.19it/s]Epoch 19:  90%|█████████ | 271/300 [02:04<00:13,  2.08it/s]Epoch 19:  91%|█████████ | 272/300 [02:04<00:13,  2.10it/s]Epoch 19:  91%|█████████ | 273/300 [02:05<00:12,  2.19it/s]Epoch 19:  91%|█████████▏| 274/300 [02:05<00:11,  2.21it/s]Epoch 19:  92%|█████████▏| 275/300 [02:06<00:12,  2.02it/s]Epoch 19:  92%|█████████▏| 276/300 [02:06<00:11,  2.04it/s]Epoch 19:  92%|█████████▏| 277/300 [02:07<00:11,  1.99it/s]Epoch 19:  93%|█████████▎| 278/300 [02:07<00:10,  2.12it/s]Epoch 19:  93%|█████████▎| 279/300 [02:08<00:10,  2.00it/s]06/19/2022 15:11:00 - INFO - __main__ - global step: 2990; train loss: 6.860210418701172; dev loss: 6.633847236633301
Epoch 19:  93%|█████████▎| 280/300 [02:08<00:09,  2.04it/s]Epoch 19:  94%|█████████▎| 281/300 [02:09<00:09,  2.08it/s]Epoch 19:  94%|█████████▍| 282/300 [02:09<00:08,  2.10it/s]Epoch 19:  94%|█████████▍| 283/300 [02:10<00:07,  2.15it/s]Epoch 19:  95%|█████████▍| 284/300 [02:10<00:08,  1.97it/s]Epoch 19:  95%|█████████▌| 285/300 [02:11<00:07,  2.00it/s]Epoch 19:  95%|█████████▌| 286/300 [02:11<00:06,  2.03it/s]Epoch 19:  96%|█████████▌| 287/300 [02:12<00:06,  2.11it/s]Epoch 19:  96%|█████████▌| 288/300 [02:12<00:05,  2.09it/s]Epoch 19:  96%|█████████▋| 289/300 [02:13<00:04,  2.23it/s]Epoch 19:  97%|█████████▋| 290/300 [02:13<00:04,  2.19it/s]Epoch 19:  97%|█████████▋| 291/300 [02:13<00:03,  2.35it/s]Epoch 19:  97%|█████████▋| 292/300 [02:14<00:03,  2.25it/s]Epoch 19:  98%|█████████▊| 293/300 [02:14<00:02,  2.39it/s]Epoch 19:  98%|█████████▊| 294/300 [02:15<00:02,  2.44it/s]Epoch 19:  98%|█████████▊| 295/300 [02:15<00:02,  2.44it/s]Epoch 19:  99%|█████████▊| 296/300 [02:16<00:01,  2.20it/s]Epoch 19:  99%|█████████▉| 297/300 [02:16<00:01,  2.21it/s]Epoch 19:  99%|█████████▉| 298/300 [02:16<00:00,  2.34it/s]Epoch 19: 100%|█████████▉| 299/300 [02:17<00:00,  2.30it/s]06/19/2022 15:11:09 - INFO - __main__ - global step: 3000; train loss: 6.8851799964904785; dev loss: 7.074469566345215
Epoch 19: 100%|██████████| 300/300 [02:17<00:00,  2.12it/s]Epoch 19: 100%|██████████| 300/300 [02:17<00:00,  2.17it/s]
Epoch 20:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 20:   0%|          | 1/300 [00:00<01:54,  2.61it/s]Epoch 20:   1%|          | 2/300 [00:00<01:49,  2.72it/s]Epoch 20:   1%|          | 3/300 [00:01<01:46,  2.79it/s]Epoch 20:   1%|▏         | 4/300 [00:01<02:14,  2.20it/s]Epoch 20:   2%|▏         | 5/300 [00:02<02:17,  2.15it/s]Epoch 20:   2%|▏         | 6/300 [00:02<02:16,  2.15it/s]Epoch 20:   2%|▏         | 7/300 [00:03<02:15,  2.17it/s]Epoch 20:   3%|▎         | 8/300 [00:03<02:29,  1.96it/s]Epoch 20:   3%|▎         | 9/300 [00:04<02:24,  2.01it/s]Epoch 20:   3%|▎         | 10/300 [00:04<02:19,  2.08it/s]Epoch 20:   4%|▎         | 11/300 [00:05<02:14,  2.14it/s]Epoch 20:   4%|▍         | 12/300 [00:05<02:05,  2.29it/s]Epoch 20:   4%|▍         | 13/300 [00:05<02:09,  2.21it/s]Epoch 20:   5%|▍         | 14/300 [00:06<02:00,  2.37it/s]Epoch 20:   5%|▌         | 15/300 [00:06<01:53,  2.50it/s]Epoch 20:   5%|▌         | 16/300 [00:06<01:50,  2.56it/s]Epoch 20:   6%|▌         | 17/300 [00:07<01:56,  2.44it/s]Epoch 20:   6%|▌         | 18/300 [00:07<01:50,  2.54it/s]Epoch 20:   6%|▋         | 19/300 [00:08<01:48,  2.59it/s]06/19/2022 15:11:17 - INFO - __main__ - global step: 3010; train loss: 6.693168640136719; dev loss: 6.517735958099365
Epoch 20:   7%|▋         | 20/300 [00:08<01:47,  2.59it/s]Epoch 20:   7%|▋         | 21/300 [00:09<01:54,  2.45it/s]Epoch 20:   7%|▋         | 22/300 [00:09<01:49,  2.54it/s]Epoch 20:   8%|▊         | 23/300 [00:09<01:48,  2.56it/s]Epoch 20:   8%|▊         | 24/300 [00:10<01:44,  2.64it/s]Epoch 20:   8%|▊         | 25/300 [00:10<01:52,  2.44it/s]Epoch 20:   9%|▊         | 26/300 [00:10<01:47,  2.55it/s]Epoch 20:   9%|▉         | 27/300 [00:11<01:45,  2.58it/s]Epoch 20:   9%|▉         | 28/300 [00:11<01:51,  2.44it/s]Epoch 20:  10%|▉         | 29/300 [00:12<02:06,  2.13it/s]Epoch 20:  10%|█         | 30/300 [00:12<02:05,  2.14it/s]Epoch 20:  10%|█         | 31/300 [00:13<02:03,  2.17it/s]Epoch 20:  11%|█         | 32/300 [00:13<02:03,  2.17it/s]Epoch 20:  11%|█         | 33/300 [00:14<02:17,  1.94it/s]Epoch 20:  11%|█▏        | 34/300 [00:14<02:09,  2.05it/s]Epoch 20:  12%|█▏        | 35/300 [00:15<02:00,  2.20it/s]Epoch 20:  12%|█▏        | 36/300 [00:15<01:55,  2.28it/s]Epoch 20:  12%|█▏        | 37/300 [00:16<01:56,  2.26it/s]Epoch 20:  13%|█▎        | 38/300 [00:16<02:08,  2.04it/s]Epoch 20:  13%|█▎        | 39/300 [00:17<02:08,  2.03it/s]06/19/2022 15:11:26 - INFO - __main__ - global step: 3020; train loss: 7.233309745788574; dev loss: 7.057097434997559
Epoch 20:  13%|█▎        | 40/300 [00:17<02:03,  2.11it/s]Epoch 20:  14%|█▎        | 41/300 [00:17<01:53,  2.28it/s]Epoch 20:  14%|█▍        | 42/300 [00:18<01:55,  2.23it/s]Epoch 20:  14%|█▍        | 43/300 [00:18<01:47,  2.39it/s]Epoch 20:  15%|█▍        | 44/300 [00:19<01:41,  2.52it/s]Epoch 20:  15%|█▌        | 45/300 [00:19<01:41,  2.52it/s]Epoch 20:  15%|█▌        | 46/300 [00:19<01:44,  2.43it/s]Epoch 20:  16%|█▌        | 47/300 [00:20<01:39,  2.54it/s]Epoch 20:  16%|█▌        | 48/300 [00:20<01:37,  2.58it/s]Epoch 20:  16%|█▋        | 49/300 [00:21<01:34,  2.65it/s]Epoch 20:  17%|█▋        | 50/300 [00:21<01:39,  2.52it/s]Epoch 20:  17%|█▋        | 51/300 [00:21<01:36,  2.58it/s]Epoch 20:  17%|█▋        | 52/300 [00:22<01:35,  2.59it/s]Epoch 20:  18%|█▊        | 53/300 [00:22<01:40,  2.47it/s]Epoch 20:  18%|█▊        | 54/300 [00:23<01:53,  2.17it/s]Epoch 20:  18%|█▊        | 55/300 [00:23<01:49,  2.23it/s]Epoch 20:  19%|█▊        | 56/300 [00:24<01:43,  2.36it/s]Epoch 20:  19%|█▉        | 57/300 [00:24<01:45,  2.31it/s]Epoch 20:  19%|█▉        | 58/300 [00:24<01:47,  2.25it/s]Epoch 20:  20%|█▉        | 59/300 [00:25<01:44,  2.31it/s]06/19/2022 15:11:34 - INFO - __main__ - global step: 3030; train loss: 6.868804931640625; dev loss: 6.892871856689453
Epoch 20:  20%|██        | 60/300 [00:25<01:40,  2.40it/s]Epoch 20:  20%|██        | 61/300 [00:26<01:42,  2.32it/s]Epoch 20:  21%|██        | 62/300 [00:26<01:53,  2.09it/s]Epoch 20:  21%|██        | 63/300 [00:27<01:50,  2.14it/s]Epoch 20:  21%|██▏       | 64/300 [00:27<01:45,  2.23it/s]Epoch 20:  22%|██▏       | 65/300 [00:28<01:42,  2.29it/s]Epoch 20:  22%|██▏       | 66/300 [00:28<01:40,  2.33it/s]Epoch 20:  22%|██▏       | 67/300 [00:28<01:45,  2.22it/s]Epoch 20:  23%|██▎       | 68/300 [00:29<01:39,  2.34it/s]Epoch 20:  23%|██▎       | 69/300 [00:29<01:38,  2.34it/s]Epoch 20:  23%|██▎       | 70/300 [00:30<01:40,  2.29it/s]Epoch 20:  24%|██▎       | 71/300 [00:30<01:44,  2.18it/s]Epoch 20:  24%|██▍       | 72/300 [00:31<01:41,  2.25it/s]Epoch 20:  24%|██▍       | 73/300 [00:31<01:38,  2.31it/s]Epoch 20:  25%|██▍       | 74/300 [00:31<01:33,  2.43it/s]Epoch 20:  25%|██▌       | 75/300 [00:32<01:34,  2.37it/s]Epoch 20:  25%|██▌       | 76/300 [00:32<01:31,  2.45it/s]Epoch 20:  26%|██▌       | 77/300 [00:33<01:28,  2.53it/s]Epoch 20:  26%|██▌       | 78/300 [00:33<01:25,  2.60it/s]Epoch 20:  26%|██▋       | 79/300 [00:33<01:30,  2.43it/s]06/19/2022 15:11:43 - INFO - __main__ - global step: 3040; train loss: 6.508819580078125; dev loss: 6.735223293304443
Epoch 20:  27%|██▋       | 80/300 [00:34<01:27,  2.51it/s]Epoch 20:  27%|██▋       | 81/300 [00:34<01:23,  2.61it/s]Epoch 20:  27%|██▋       | 82/300 [00:35<01:24,  2.57it/s]Epoch 20:  28%|██▊       | 83/300 [00:35<01:42,  2.12it/s]Epoch 20:  28%|██▊       | 84/300 [00:36<01:36,  2.25it/s]Epoch 20:  28%|██▊       | 85/300 [00:36<01:31,  2.34it/s]Epoch 20:  29%|██▊       | 86/300 [00:36<01:28,  2.42it/s]Epoch 20:  29%|██▉       | 87/300 [00:37<01:36,  2.20it/s]Epoch 20:  29%|██▉       | 88/300 [00:37<01:31,  2.32it/s]Epoch 20:  30%|██▉       | 89/300 [00:38<01:33,  2.25it/s]Epoch 20:  30%|███       | 90/300 [00:38<01:35,  2.21it/s]Epoch 20:  30%|███       | 91/300 [00:39<01:34,  2.21it/s]Epoch 20:  31%|███       | 92/300 [00:39<01:39,  2.08it/s]Epoch 20:  31%|███       | 93/300 [00:40<01:36,  2.14it/s]Epoch 20:  31%|███▏      | 94/300 [00:40<01:29,  2.31it/s]Epoch 20:  32%|███▏      | 95/300 [00:40<01:22,  2.49it/s]Epoch 20:  32%|███▏      | 96/300 [00:41<01:25,  2.37it/s]Epoch 20:  32%|███▏      | 97/300 [00:41<01:20,  2.53it/s]Epoch 20:  33%|███▎      | 98/300 [00:42<01:18,  2.57it/s]Epoch 20:  33%|███▎      | 99/300 [00:42<01:16,  2.63it/s]06/19/2022 15:11:52 - INFO - __main__ - global step: 3050; train loss: 6.546456813812256; dev loss: 6.513906955718994
Epoch 20:  33%|███▎      | 100/300 [00:42<01:18,  2.54it/s]Epoch 20:  34%|███▎      | 101/300 [00:43<01:14,  2.66it/s]Epoch 20:  34%|███▍      | 102/300 [00:43<01:12,  2.74it/s]Epoch 20:  34%|███▍      | 103/300 [00:43<01:12,  2.70it/s]Epoch 20:  35%|███▍      | 104/300 [00:44<01:16,  2.55it/s]Epoch 20:  35%|███▌      | 105/300 [00:44<01:15,  2.60it/s]Epoch 20:  35%|███▌      | 106/300 [00:45<01:15,  2.55it/s]Epoch 20:  36%|███▌      | 107/300 [00:45<01:14,  2.58it/s]Epoch 20:  36%|███▌      | 108/300 [00:45<01:18,  2.44it/s]Epoch 20:  36%|███▋      | 109/300 [00:46<01:15,  2.52it/s]Epoch 20:  37%|███▋      | 110/300 [00:46<01:19,  2.40it/s]Epoch 20:  37%|███▋      | 111/300 [00:47<01:18,  2.39it/s]Epoch 20:  37%|███▋      | 112/300 [00:47<01:26,  2.18it/s]Epoch 20:  38%|███▊      | 113/300 [00:48<01:30,  2.08it/s]Epoch 20:  38%|███▊      | 114/300 [00:48<01:23,  2.23it/s]Epoch 20:  38%|███▊      | 115/300 [00:48<01:17,  2.39it/s]Epoch 20:  39%|███▊      | 116/300 [00:49<01:18,  2.36it/s]Epoch 20:  39%|███▉      | 117/300 [00:49<01:12,  2.53it/s]Epoch 20:  39%|███▉      | 118/300 [00:50<01:12,  2.50it/s]Epoch 20:  40%|███▉      | 119/300 [00:50<01:13,  2.45it/s]06/19/2022 15:12:00 - INFO - __main__ - global step: 3060; train loss: 7.0246477127075195; dev loss: 7.155941009521484
Epoch 20:  40%|████      | 120/300 [00:51<01:14,  2.42it/s]Epoch 20:  40%|████      | 121/300 [00:51<01:15,  2.37it/s]Epoch 20:  41%|████      | 122/300 [00:51<01:12,  2.45it/s]Epoch 20:  41%|████      | 123/300 [00:52<01:08,  2.59it/s]Epoch 20:  41%|████▏     | 124/300 [00:52<01:06,  2.64it/s]Epoch 20:  42%|████▏     | 125/300 [00:53<01:12,  2.40it/s]Epoch 20:  42%|████▏     | 126/300 [00:53<01:16,  2.29it/s]Epoch 20:  42%|████▏     | 127/300 [00:53<01:16,  2.26it/s]Epoch 20:  43%|████▎     | 128/300 [00:54<01:16,  2.24it/s]Epoch 20:  43%|████▎     | 129/300 [00:55<01:24,  2.02it/s]Epoch 20:  43%|████▎     | 130/300 [00:55<01:17,  2.18it/s]Epoch 20:  44%|████▎     | 131/300 [00:55<01:12,  2.32it/s]Epoch 20:  44%|████▍     | 132/300 [00:56<01:09,  2.43it/s]Epoch 20:  44%|████▍     | 133/300 [00:56<01:12,  2.31it/s]Epoch 20:  45%|████▍     | 134/300 [00:57<01:12,  2.29it/s]Epoch 20:  45%|████▌     | 135/300 [00:57<01:10,  2.33it/s]Epoch 20:  45%|████▌     | 136/300 [00:57<01:08,  2.39it/s]Epoch 20:  46%|████▌     | 137/300 [00:58<01:10,  2.32it/s]Epoch 20:  46%|████▌     | 138/300 [00:58<01:06,  2.43it/s]Epoch 20:  46%|████▋     | 139/300 [00:59<01:05,  2.48it/s]06/19/2022 15:12:08 - INFO - __main__ - global step: 3070; train loss: 6.4200239181518555; dev loss: 6.532950401306152
Epoch 20:  47%|████▋     | 140/300 [00:59<01:03,  2.50it/s]Epoch 20:  47%|████▋     | 141/300 [00:59<01:04,  2.45it/s]Epoch 20:  47%|████▋     | 142/300 [01:00<01:10,  2.24it/s]Epoch 20:  48%|████▊     | 143/300 [01:00<01:12,  2.16it/s]Epoch 20:  48%|████▊     | 144/300 [01:01<01:08,  2.28it/s]Epoch 20:  48%|████▊     | 145/300 [01:01<01:05,  2.38it/s]Epoch 20:  49%|████▊     | 146/300 [01:02<01:06,  2.30it/s]Epoch 20:  49%|████▉     | 147/300 [01:02<01:05,  2.34it/s]Epoch 20:  49%|████▉     | 148/300 [01:03<01:05,  2.33it/s]Epoch 20:  50%|████▉     | 149/300 [01:03<01:01,  2.47it/s]Epoch 20:  50%|█████     | 150/300 [01:03<01:02,  2.39it/s]Epoch 20:  50%|█████     | 151/300 [01:04<01:02,  2.40it/s]Epoch 20:  51%|█████     | 152/300 [01:04<01:03,  2.34it/s]Epoch 20:  51%|█████     | 153/300 [01:05<01:01,  2.41it/s]Epoch 20:  51%|█████▏    | 154/300 [01:05<01:05,  2.23it/s]Epoch 20:  52%|█████▏    | 155/300 [01:05<01:00,  2.38it/s]Epoch 20:  52%|█████▏    | 156/300 [01:06<00:57,  2.48it/s]Epoch 20:  52%|█████▏    | 157/300 [01:06<00:56,  2.52it/s]Epoch 20:  53%|█████▎    | 158/300 [01:07<00:59,  2.38it/s]Epoch 20:  53%|█████▎    | 159/300 [01:07<00:58,  2.39it/s]06/19/2022 15:12:17 - INFO - __main__ - global step: 3080; train loss: 6.549336910247803; dev loss: 6.6664276123046875
Epoch 20:  53%|█████▎    | 160/300 [01:07<00:56,  2.46it/s]Epoch 20:  54%|█████▎    | 161/300 [01:08<00:58,  2.38it/s]Epoch 20:  54%|█████▍    | 162/300 [01:08<01:00,  2.30it/s]Epoch 20:  54%|█████▍    | 163/300 [01:09<00:59,  2.32it/s]Epoch 20:  55%|█████▍    | 164/300 [01:09<01:02,  2.19it/s]Epoch 20:  55%|█████▌    | 165/300 [01:10<01:05,  2.06it/s]Epoch 20:  55%|█████▌    | 166/300 [01:10<01:05,  2.06it/s]Epoch 20:  56%|█████▌    | 167/300 [01:11<01:00,  2.20it/s]Epoch 20:  56%|█████▌    | 168/300 [01:11<00:58,  2.26it/s]Epoch 20:  56%|█████▋    | 169/300 [01:12<00:55,  2.37it/s]Epoch 20:  57%|█████▋    | 170/300 [01:12<00:58,  2.24it/s]Epoch 20:  57%|█████▋    | 171/300 [01:12<00:55,  2.31it/s]Epoch 20:  57%|█████▋    | 172/300 [01:13<00:53,  2.41it/s]Epoch 20:  58%|█████▊    | 173/300 [01:13<00:52,  2.43it/s]Epoch 20:  58%|█████▊    | 174/300 [01:14<00:49,  2.54it/s]Epoch 20:  58%|█████▊    | 175/300 [01:14<00:51,  2.42it/s]Epoch 20:  59%|█████▊    | 176/300 [01:14<00:49,  2.50it/s]Epoch 20:  59%|█████▉    | 177/300 [01:15<00:48,  2.55it/s]Epoch 20:  59%|█████▉    | 178/300 [01:15<00:47,  2.56it/s]Epoch 20:  60%|█████▉    | 179/300 [01:16<00:50,  2.40it/s]06/19/2022 15:12:25 - INFO - __main__ - global step: 3090; train loss: 6.264193058013916; dev loss: 6.361783981323242
Epoch 20:  60%|██████    | 180/300 [01:16<00:48,  2.46it/s]Epoch 20:  60%|██████    | 181/300 [01:16<00:47,  2.50it/s]Epoch 20:  61%|██████    | 182/300 [01:17<00:46,  2.53it/s]Epoch 20:  61%|██████    | 183/300 [01:17<00:48,  2.42it/s]Epoch 20:  61%|██████▏   | 184/300 [01:18<00:45,  2.55it/s]Epoch 20:  62%|██████▏   | 185/300 [01:18<00:43,  2.66it/s]Epoch 20:  62%|██████▏   | 186/300 [01:18<00:41,  2.73it/s]Epoch 20:  62%|██████▏   | 187/300 [01:19<00:48,  2.33it/s]Epoch 20:  63%|██████▎   | 188/300 [01:19<00:51,  2.19it/s]Epoch 20:  63%|██████▎   | 189/300 [01:20<00:51,  2.17it/s]Epoch 20:  63%|██████▎   | 190/300 [01:20<00:50,  2.19it/s]Epoch 20:  64%|██████▎   | 191/300 [01:21<00:52,  2.07it/s]Epoch 20:  64%|██████▍   | 192/300 [01:21<00:51,  2.11it/s]Epoch 20:  64%|██████▍   | 193/300 [01:22<00:50,  2.14it/s]Epoch 20:  65%|██████▍   | 194/300 [01:22<00:49,  2.15it/s]Epoch 20:  65%|██████▌   | 195/300 [01:23<00:51,  2.02it/s]Epoch 20:  65%|██████▌   | 196/300 [01:23<00:52,  1.98it/s]Epoch 20:  66%|██████▌   | 197/300 [01:24<00:50,  2.05it/s]Epoch 20:  66%|██████▌   | 198/300 [01:24<00:48,  2.11it/s]Epoch 20:  66%|██████▋   | 199/300 [01:25<00:50,  2.01it/s]06/19/2022 15:12:35 - INFO - __main__ - global step: 3100; train loss: 6.625088691711426; dev loss: 6.894644737243652
Epoch 20:  67%|██████▋   | 200/300 [01:25<00:54,  1.84it/s]Epoch 20:  67%|██████▋   | 201/300 [01:26<00:52,  1.89it/s]Epoch 20:  67%|██████▋   | 202/300 [01:26<00:50,  1.93it/s]Epoch 20:  68%|██████▊   | 203/300 [01:27<00:45,  2.12it/s]Epoch 20:  68%|██████▊   | 204/300 [01:27<00:45,  2.09it/s]Epoch 20:  68%|██████▊   | 205/300 [01:28<00:41,  2.28it/s]Epoch 20:  69%|██████▊   | 206/300 [01:28<00:38,  2.43it/s]Epoch 20:  69%|██████▉   | 207/300 [01:28<00:39,  2.33it/s]Epoch 20:  69%|██████▉   | 208/300 [01:29<00:41,  2.21it/s]Epoch 20:  70%|██████▉   | 209/300 [01:29<00:43,  2.11it/s]Epoch 20:  70%|███████   | 210/300 [01:30<00:41,  2.18it/s]Epoch 20:  70%|███████   | 211/300 [01:30<00:39,  2.27it/s]Epoch 20:  71%|███████   | 212/300 [01:31<00:40,  2.17it/s]Epoch 20:  71%|███████   | 213/300 [01:31<00:38,  2.28it/s]Epoch 20:  71%|███████▏  | 214/300 [01:31<00:35,  2.44it/s]Epoch 20:  72%|███████▏  | 215/300 [01:32<00:33,  2.54it/s]Epoch 20:  72%|███████▏  | 216/300 [01:32<00:36,  2.30it/s]Epoch 20:  72%|███████▏  | 217/300 [01:33<00:34,  2.43it/s]Epoch 20:  73%|███████▎  | 218/300 [01:33<00:32,  2.54it/s]Epoch 20:  73%|███████▎  | 219/300 [01:33<00:31,  2.54it/s]06/19/2022 15:12:43 - INFO - __main__ - global step: 3110; train loss: 6.681990623474121; dev loss: 6.451970100402832
Epoch 20:  73%|███████▎  | 220/300 [01:34<00:33,  2.39it/s]Epoch 20:  74%|███████▎  | 221/300 [01:34<00:31,  2.51it/s]Epoch 20:  74%|███████▍  | 222/300 [01:35<00:29,  2.61it/s]Epoch 20:  74%|███████▍  | 223/300 [01:35<00:30,  2.56it/s]Epoch 20:  75%|███████▍  | 224/300 [01:36<00:32,  2.34it/s]Epoch 20:  75%|███████▌  | 225/300 [01:36<00:33,  2.25it/s]Epoch 20:  75%|███████▌  | 226/300 [01:36<00:32,  2.28it/s]Epoch 20:  76%|███████▌  | 227/300 [01:37<00:30,  2.37it/s]Epoch 20:  76%|███████▌  | 228/300 [01:37<00:29,  2.45it/s]Epoch 20:  76%|███████▋  | 229/300 [01:38<00:32,  2.20it/s]Epoch 20:  77%|███████▋  | 230/300 [01:38<00:31,  2.25it/s]Epoch 20:  77%|███████▋  | 231/300 [01:39<00:30,  2.26it/s]Epoch 20:  77%|███████▋  | 232/300 [01:39<00:30,  2.21it/s]Epoch 20:  78%|███████▊  | 233/300 [01:40<00:31,  2.12it/s]Epoch 20:  78%|███████▊  | 234/300 [01:40<00:28,  2.31it/s]Epoch 20:  78%|███████▊  | 235/300 [01:40<00:26,  2.43it/s]Epoch 20:  79%|███████▊  | 236/300 [01:41<00:26,  2.42it/s]Epoch 20:  79%|███████▉  | 237/300 [01:41<00:30,  2.06it/s]Epoch 20:  79%|███████▉  | 238/300 [01:42<00:29,  2.12it/s]Epoch 20:  80%|███████▉  | 239/300 [01:42<00:27,  2.20it/s]06/19/2022 15:12:52 - INFO - __main__ - global step: 3120; train loss: 6.244653701782227; dev loss: 6.5386247634887695
Epoch 20:  80%|████████  | 240/300 [01:43<00:27,  2.17it/s]Epoch 20:  80%|████████  | 241/300 [01:43<00:30,  1.90it/s]Epoch 20:  81%|████████  | 242/300 [01:44<00:30,  1.90it/s]Epoch 20:  81%|████████  | 243/300 [01:44<00:27,  2.11it/s]Epoch 20:  81%|████████▏ | 244/300 [01:45<00:24,  2.28it/s]Epoch 20:  82%|████████▏ | 245/300 [01:45<00:26,  2.11it/s]Epoch 20:  82%|████████▏ | 246/300 [01:46<00:24,  2.17it/s]Epoch 20:  82%|████████▏ | 247/300 [01:46<00:23,  2.28it/s]Epoch 20:  83%|████████▎ | 248/300 [01:46<00:22,  2.36it/s]Epoch 20:  83%|████████▎ | 249/300 [01:47<00:23,  2.17it/s]Epoch 20:  83%|████████▎ | 250/300 [01:47<00:21,  2.32it/s]Epoch 20:  84%|████████▎ | 251/300 [01:48<00:19,  2.45it/s]Epoch 20:  84%|████████▍ | 252/300 [01:48<00:19,  2.47it/s]Epoch 20:  84%|████████▍ | 253/300 [01:48<00:18,  2.56it/s]Epoch 20:  85%|████████▍ | 254/300 [01:49<00:19,  2.41it/s]Epoch 20:  85%|████████▌ | 255/300 [01:49<00:17,  2.52it/s]Epoch 20:  85%|████████▌ | 256/300 [01:50<00:17,  2.46it/s]Epoch 20:  86%|████████▌ | 257/300 [01:50<00:17,  2.41it/s]Epoch 20:  86%|████████▌ | 258/300 [01:51<00:19,  2.19it/s]Epoch 20:  86%|████████▋ | 259/300 [01:51<00:17,  2.33it/s]06/19/2022 15:13:01 - INFO - __main__ - global step: 3130; train loss: 6.379847049713135; dev loss: 6.325497627258301
Epoch 20:  87%|████████▋ | 260/300 [01:51<00:16,  2.39it/s]Epoch 20:  87%|████████▋ | 261/300 [01:52<00:15,  2.48it/s]Epoch 20:  87%|████████▋ | 262/300 [01:52<00:16,  2.31it/s]Epoch 20:  88%|████████▊ | 263/300 [01:53<00:16,  2.27it/s]Epoch 20:  88%|████████▊ | 264/300 [01:53<00:17,  2.05it/s]Epoch 20:  88%|████████▊ | 265/300 [01:54<00:17,  2.04it/s]Epoch 20:  89%|████████▊ | 266/300 [01:54<00:17,  1.93it/s]Epoch 20:  89%|████████▉ | 267/300 [01:55<00:16,  1.98it/s]Epoch 20:  89%|████████▉ | 268/300 [01:55<00:15,  2.03it/s]Epoch 20:  90%|████████▉ | 269/300 [01:56<00:15,  2.03it/s]Epoch 20:  90%|█████████ | 270/300 [01:56<00:14,  2.05it/s]Epoch 20:  90%|█████████ | 271/300 [01:57<00:12,  2.23it/s]Epoch 20:  91%|█████████ | 272/300 [01:57<00:12,  2.32it/s]Epoch 20:  91%|█████████ | 273/300 [01:57<00:11,  2.38it/s]Epoch 20:  91%|█████████▏| 274/300 [01:58<00:12,  2.11it/s]Epoch 20:  92%|█████████▏| 275/300 [01:59<00:11,  2.11it/s]Epoch 20:  92%|█████████▏| 276/300 [01:59<00:11,  2.11it/s]Epoch 20:  92%|█████████▏| 277/300 [02:00<00:11,  2.04it/s]Epoch 20:  93%|█████████▎| 278/300 [02:00<00:11,  1.99it/s]Epoch 20:  93%|█████████▎| 279/300 [02:01<00:09,  2.10it/s]06/19/2022 15:13:10 - INFO - __main__ - global step: 3140; train loss: 6.662676811218262; dev loss: 6.711818695068359
Epoch 20:  93%|█████████▎| 280/300 [02:01<00:09,  2.03it/s]Epoch 20:  94%|█████████▎| 281/300 [02:01<00:08,  2.17it/s]Epoch 20:  94%|█████████▍| 282/300 [02:02<00:07,  2.33it/s]Epoch 20:  94%|█████████▍| 283/300 [02:02<00:07,  2.18it/s]Epoch 20:  95%|█████████▍| 284/300 [02:03<00:07,  2.23it/s]Epoch 20:  95%|█████████▌| 285/300 [02:03<00:06,  2.16it/s]Epoch 20:  95%|█████████▌| 286/300 [02:04<00:06,  2.22it/s]Epoch 20:  96%|█████████▌| 287/300 [02:04<00:06,  2.01it/s]Epoch 20:  96%|█████████▌| 288/300 [02:05<00:05,  2.10it/s]Epoch 20:  96%|█████████▋| 289/300 [02:05<00:05,  2.16it/s]Epoch 20:  97%|█████████▋| 290/300 [02:06<00:04,  2.18it/s]Epoch 20:  97%|█████████▋| 291/300 [02:06<00:04,  2.17it/s]Epoch 20:  97%|█████████▋| 292/300 [02:06<00:03,  2.25it/s]Epoch 20:  98%|█████████▊| 293/300 [02:07<00:02,  2.38it/s]Epoch 20:  98%|█████████▊| 294/300 [02:07<00:02,  2.49it/s]Epoch 20:  98%|█████████▊| 295/300 [02:08<00:02,  2.37it/s]Epoch 20:  99%|█████████▊| 296/300 [02:08<00:01,  2.48it/s]Epoch 20:  99%|█████████▉| 297/300 [02:08<00:01,  2.58it/s]Epoch 20:  99%|█████████▉| 298/300 [02:09<00:00,  2.56it/s]Epoch 20: 100%|█████████▉| 299/300 [02:09<00:00,  2.40it/s]06/19/2022 15:13:19 - INFO - __main__ - global step: 3150; train loss: 6.37109375; dev loss: 6.495715141296387
Epoch 20: 100%|██████████| 300/300 [02:10<00:00,  2.51it/s]Epoch 20: 100%|██████████| 300/300 [02:10<00:00,  2.31it/s]
Epoch 21:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 21:   0%|          | 1/300 [00:00<01:43,  2.89it/s]Epoch 21:   1%|          | 2/300 [00:00<01:44,  2.84it/s]Epoch 21:   1%|          | 3/300 [00:01<02:12,  2.25it/s]Epoch 21:   1%|▏         | 4/300 [00:01<02:01,  2.43it/s]Epoch 21:   2%|▏         | 5/300 [00:02<02:03,  2.39it/s]Epoch 21:   2%|▏         | 6/300 [00:02<02:12,  2.21it/s]Epoch 21:   2%|▏         | 7/300 [00:02<02:06,  2.31it/s]Epoch 21:   3%|▎         | 8/300 [00:03<02:11,  2.22it/s]Epoch 21:   3%|▎         | 9/300 [00:03<02:06,  2.30it/s]Epoch 21:   3%|▎         | 10/300 [00:04<02:00,  2.40it/s]Epoch 21:   4%|▎         | 11/300 [00:04<01:57,  2.46it/s]Epoch 21:   4%|▍         | 12/300 [00:05<02:04,  2.32it/s]Epoch 21:   4%|▍         | 13/300 [00:05<01:59,  2.41it/s]Epoch 21:   5%|▍         | 14/300 [00:05<01:56,  2.46it/s]Epoch 21:   5%|▌         | 15/300 [00:06<01:54,  2.49it/s]Epoch 21:   5%|▌         | 16/300 [00:06<02:06,  2.25it/s]Epoch 21:   6%|▌         | 17/300 [00:07<01:59,  2.38it/s]Epoch 21:   6%|▌         | 18/300 [00:07<01:58,  2.39it/s]Epoch 21:   6%|▋         | 19/300 [00:08<02:02,  2.29it/s]06/19/2022 15:13:27 - INFO - __main__ - global step: 3160; train loss: 6.762207984924316; dev loss: 6.685527801513672
Epoch 21:   7%|▋         | 20/300 [00:08<02:04,  2.25it/s]Epoch 21:   7%|▋         | 21/300 [00:08<01:56,  2.40it/s]Epoch 21:   7%|▋         | 22/300 [00:09<01:52,  2.48it/s]Epoch 21:   8%|▊         | 23/300 [00:09<01:58,  2.34it/s]Epoch 21:   8%|▊         | 24/300 [00:10<02:10,  2.12it/s]Epoch 21:   8%|▊         | 25/300 [00:10<02:02,  2.25it/s]Epoch 21:   9%|▊         | 26/300 [00:11<02:01,  2.25it/s]Epoch 21:   9%|▉         | 27/300 [00:11<01:56,  2.35it/s]Epoch 21:   9%|▉         | 28/300 [00:12<02:03,  2.21it/s]Epoch 21:  10%|▉         | 29/300 [00:12<01:54,  2.36it/s]Epoch 21:  10%|█         | 30/300 [00:12<01:50,  2.44it/s]Epoch 21:  10%|█         | 31/300 [00:13<01:49,  2.45it/s]Epoch 21:  11%|█         | 32/300 [00:13<02:04,  2.15it/s]Epoch 21:  11%|█         | 33/300 [00:14<02:09,  2.06it/s]Epoch 21:  11%|█▏        | 34/300 [00:14<02:03,  2.16it/s]Epoch 21:  12%|█▏        | 35/300 [00:15<01:53,  2.33it/s]Epoch 21:  12%|█▏        | 36/300 [00:15<01:48,  2.44it/s]Epoch 21:  12%|█▏        | 37/300 [00:15<02:01,  2.17it/s]Epoch 21:  13%|█▎        | 38/300 [00:16<01:56,  2.25it/s]Epoch 21:  13%|█▎        | 39/300 [00:16<01:51,  2.33it/s]06/19/2022 15:13:36 - INFO - __main__ - global step: 3170; train loss: 6.3511881828308105; dev loss: 6.3812360763549805
Epoch 21:  13%|█▎        | 40/300 [00:17<01:48,  2.39it/s]Epoch 21:  14%|█▎        | 41/300 [00:17<01:52,  2.31it/s]Epoch 21:  14%|█▍        | 42/300 [00:18<01:56,  2.22it/s]Epoch 21:  14%|█▍        | 43/300 [00:18<02:00,  2.14it/s]Epoch 21:  15%|█▍        | 44/300 [00:19<02:09,  1.98it/s]Epoch 21:  15%|█▌        | 45/300 [00:19<02:11,  1.94it/s]Epoch 21:  15%|█▌        | 46/300 [00:20<02:05,  2.02it/s]Epoch 21:  16%|█▌        | 47/300 [00:20<02:01,  2.08it/s]Epoch 21:  16%|█▌        | 48/300 [00:21<01:59,  2.11it/s]Epoch 21:  16%|█▋        | 49/300 [00:21<02:05,  2.01it/s]Epoch 21:  17%|█▋        | 50/300 [00:22<01:58,  2.10it/s]Epoch 21:  17%|█▋        | 51/300 [00:22<01:53,  2.19it/s]Epoch 21:  17%|█▋        | 52/300 [00:22<01:49,  2.26it/s]Epoch 21:  18%|█▊        | 53/300 [00:23<02:03,  2.00it/s]Epoch 21:  18%|█▊        | 54/300 [00:23<01:55,  2.12it/s]Epoch 21:  18%|█▊        | 55/300 [00:24<01:47,  2.29it/s]Epoch 21:  19%|█▊        | 56/300 [00:24<01:40,  2.42it/s]Epoch 21:  19%|█▉        | 57/300 [00:25<01:46,  2.28it/s]Epoch 21:  19%|█▉        | 58/300 [00:25<01:39,  2.43it/s]Epoch 21:  20%|█▉        | 59/300 [00:25<01:36,  2.50it/s]06/19/2022 15:13:45 - INFO - __main__ - global step: 3180; train loss: 6.969287872314453; dev loss: 7.069577693939209
Epoch 21:  20%|██        | 60/300 [00:26<01:34,  2.54it/s]Epoch 21:  20%|██        | 61/300 [00:26<01:31,  2.62it/s]Epoch 21:  21%|██        | 62/300 [00:27<01:41,  2.35it/s]Epoch 21:  21%|██        | 63/300 [00:27<01:43,  2.30it/s]Epoch 21:  21%|██▏       | 64/300 [00:28<01:48,  2.17it/s]Epoch 21:  22%|██▏       | 65/300 [00:28<01:48,  2.17it/s]Epoch 21:  22%|██▏       | 66/300 [00:29<01:48,  2.16it/s]Epoch 21:  22%|██▏       | 67/300 [00:29<01:39,  2.33it/s]Epoch 21:  23%|██▎       | 68/300 [00:29<01:34,  2.46it/s]Epoch 21:  23%|██▎       | 69/300 [00:30<01:32,  2.49it/s]Epoch 21:  23%|██▎       | 70/300 [00:30<01:45,  2.18it/s]Epoch 21:  24%|██▎       | 71/300 [00:31<01:45,  2.17it/s]Epoch 21:  24%|██▍       | 72/300 [00:31<01:44,  2.17it/s]Epoch 21:  24%|██▍       | 73/300 [00:32<01:39,  2.28it/s]Epoch 21:  25%|██▍       | 74/300 [00:32<01:45,  2.15it/s]Epoch 21:  25%|██▌       | 75/300 [00:33<01:43,  2.17it/s]Epoch 21:  25%|██▌       | 76/300 [00:33<01:35,  2.35it/s]Epoch 21:  26%|██▌       | 77/300 [00:33<01:33,  2.39it/s]Epoch 21:  26%|██▌       | 78/300 [00:34<01:38,  2.25it/s]Epoch 21:  26%|██▋       | 79/300 [00:34<01:33,  2.35it/s]06/19/2022 15:13:54 - INFO - __main__ - global step: 3190; train loss: 6.072976112365723; dev loss: 5.996167182922363
Epoch 21:  27%|██▋       | 80/300 [00:35<01:30,  2.44it/s]Epoch 21:  27%|██▋       | 81/300 [00:35<01:27,  2.50it/s]Epoch 21:  27%|██▋       | 82/300 [00:36<01:40,  2.17it/s]Epoch 21:  28%|██▊       | 83/300 [00:36<01:40,  2.16it/s]Epoch 21:  28%|██▊       | 84/300 [00:36<01:39,  2.17it/s]Epoch 21:  28%|██▊       | 85/300 [00:37<01:34,  2.27it/s]Epoch 21:  29%|██▊       | 86/300 [00:37<01:36,  2.22it/s]Epoch 21:  29%|██▉       | 87/300 [00:38<01:30,  2.36it/s]Epoch 21:  29%|██▉       | 88/300 [00:38<01:27,  2.43it/s]Epoch 21:  30%|██▉       | 89/300 [00:39<01:29,  2.35it/s]Epoch 21:  30%|███       | 90/300 [00:39<01:37,  2.15it/s]Epoch 21:  30%|███       | 91/300 [00:40<01:52,  1.85it/s]Epoch 21:  31%|███       | 92/300 [00:40<01:53,  1.83it/s]Epoch 21:  31%|███       | 93/300 [00:41<01:45,  1.96it/s]Epoch 21:  31%|███▏      | 94/300 [00:41<01:35,  2.16it/s]Epoch 21:  32%|███▏      | 95/300 [00:42<01:39,  2.05it/s]Epoch 21:  32%|███▏      | 96/300 [00:42<01:38,  2.08it/s]Epoch 21:  32%|███▏      | 97/300 [00:43<01:34,  2.15it/s]Epoch 21:  33%|███▎      | 98/300 [00:43<01:36,  2.10it/s]Epoch 21:  33%|███▎      | 99/300 [00:44<01:40,  1.99it/s]06/19/2022 15:14:03 - INFO - __main__ - global step: 3200; train loss: 6.223867416381836; dev loss: 6.080250263214111
Epoch 21:  33%|███▎      | 100/300 [00:44<01:32,  2.16it/s]Epoch 21:  34%|███▎      | 101/300 [00:44<01:25,  2.32it/s]Epoch 21:  34%|███▍      | 102/300 [00:45<01:21,  2.44it/s]Epoch 21:  34%|███▍      | 103/300 [00:45<01:23,  2.35it/s]Epoch 21:  35%|███▍      | 104/300 [00:46<01:19,  2.46it/s]Epoch 21:  35%|███▌      | 105/300 [00:46<01:15,  2.57it/s]Epoch 21:  35%|███▌      | 106/300 [00:46<01:18,  2.47it/s]Epoch 21:  36%|███▌      | 107/300 [00:47<01:26,  2.22it/s]Epoch 21:  36%|███▌      | 108/300 [00:47<01:20,  2.39it/s]Epoch 21:  36%|███▋      | 109/300 [00:48<01:18,  2.44it/s]Epoch 21:  37%|███▋      | 110/300 [00:48<01:14,  2.56it/s]Epoch 21:  37%|███▋      | 111/300 [00:48<01:18,  2.41it/s]Epoch 21:  37%|███▋      | 112/300 [00:49<01:20,  2.33it/s]Epoch 21:  38%|███▊      | 113/300 [00:49<01:21,  2.28it/s]Epoch 21:  38%|███▊      | 114/300 [00:50<01:26,  2.15it/s]Epoch 21:  38%|███▊      | 115/300 [00:50<01:23,  2.21it/s]Epoch 21:  39%|███▊      | 116/300 [00:51<01:22,  2.22it/s]Epoch 21:  39%|███▉      | 117/300 [00:51<01:16,  2.38it/s]Epoch 21:  39%|███▉      | 118/300 [00:51<01:12,  2.52it/s]Epoch 21:  40%|███▉      | 119/300 [00:52<01:10,  2.58it/s]06/19/2022 15:14:12 - INFO - __main__ - global step: 3210; train loss: 6.199620723724365; dev loss: 6.3551344871521
Epoch 21:  40%|████      | 120/300 [00:52<01:23,  2.16it/s]Epoch 21:  40%|████      | 121/300 [00:53<01:26,  2.08it/s]Epoch 21:  41%|████      | 122/300 [00:53<01:20,  2.22it/s]Epoch 21:  41%|████      | 123/300 [00:54<01:21,  2.18it/s]Epoch 21:  41%|████▏     | 124/300 [00:54<01:25,  2.05it/s]Epoch 21:  42%|████▏     | 125/300 [00:55<01:23,  2.09it/s]Epoch 21:  42%|████▏     | 126/300 [00:55<01:25,  2.05it/s]Epoch 21:  42%|████▏     | 127/300 [00:56<01:20,  2.16it/s]Epoch 21:  43%|████▎     | 128/300 [00:56<01:18,  2.20it/s]Epoch 21:  43%|████▎     | 129/300 [00:57<01:12,  2.37it/s]Epoch 21:  43%|████▎     | 130/300 [00:57<01:07,  2.52it/s]Epoch 21:  44%|████▎     | 131/300 [00:57<01:04,  2.61it/s]Epoch 21:  44%|████▍     | 132/300 [00:58<01:13,  2.28it/s]Epoch 21:  44%|████▍     | 133/300 [00:58<01:13,  2.26it/s]Epoch 21:  45%|████▍     | 134/300 [00:59<01:13,  2.25it/s]Epoch 21:  45%|████▌     | 135/300 [00:59<01:11,  2.31it/s]Epoch 21:  45%|████▌     | 136/300 [01:00<01:11,  2.29it/s]Epoch 21:  46%|████▌     | 137/300 [01:00<01:06,  2.45it/s]Epoch 21:  46%|████▌     | 138/300 [01:00<01:03,  2.57it/s]Epoch 21:  46%|████▋     | 139/300 [01:01<01:01,  2.62it/s]06/19/2022 15:14:20 - INFO - __main__ - global step: 3220; train loss: 6.2505998611450195; dev loss: 6.178705215454102
Epoch 21:  47%|████▋     | 140/300 [01:01<01:04,  2.49it/s]Epoch 21:  47%|████▋     | 141/300 [01:01<01:00,  2.61it/s]Epoch 21:  47%|████▋     | 142/300 [01:02<00:58,  2.69it/s]Epoch 21:  48%|████▊     | 143/300 [01:02<00:57,  2.72it/s]Epoch 21:  48%|████▊     | 144/300 [01:02<00:56,  2.77it/s]Epoch 21:  48%|████▊     | 145/300 [01:03<00:59,  2.60it/s]Epoch 21:  49%|████▊     | 146/300 [01:03<00:57,  2.68it/s]Epoch 21:  49%|████▉     | 147/300 [01:04<00:55,  2.75it/s]Epoch 21:  49%|████▉     | 148/300 [01:04<00:54,  2.79it/s]Epoch 21:  50%|████▉     | 149/300 [01:04<00:57,  2.61it/s]Epoch 21:  50%|█████     | 150/300 [01:05<00:55,  2.69it/s]Epoch 21:  50%|█████     | 151/300 [01:05<00:54,  2.76it/s]Epoch 21:  51%|█████     | 152/300 [01:05<00:53,  2.79it/s]Epoch 21:  51%|█████     | 153/300 [01:06<00:56,  2.60it/s]Epoch 21:  51%|█████▏    | 154/300 [01:06<00:54,  2.65it/s]Epoch 21:  52%|█████▏    | 155/300 [01:07<00:53,  2.73it/s]Epoch 21:  52%|█████▏    | 156/300 [01:07<00:52,  2.77it/s]Epoch 21:  52%|█████▏    | 157/300 [01:07<00:57,  2.50it/s]Epoch 21:  53%|█████▎    | 158/300 [01:08<00:56,  2.54it/s]Epoch 21:  53%|█████▎    | 159/300 [01:08<00:58,  2.40it/s]06/19/2022 15:14:28 - INFO - __main__ - global step: 3230; train loss: 6.094099998474121; dev loss: 6.160171985626221
Epoch 21:  53%|█████▎    | 160/300 [01:09<00:59,  2.34it/s]Epoch 21:  54%|█████▎    | 161/300 [01:09<01:06,  2.08it/s]Epoch 21:  54%|█████▍    | 162/300 [01:10<01:05,  2.11it/s]Epoch 21:  54%|█████▍    | 163/300 [01:10<01:04,  2.14it/s]Epoch 21:  55%|█████▍    | 164/300 [01:11<01:03,  2.16it/s]Epoch 21:  55%|█████▌    | 165/300 [01:11<01:02,  2.15it/s]Epoch 21:  55%|█████▌    | 166/300 [01:12<01:00,  2.23it/s]Epoch 21:  56%|█████▌    | 167/300 [01:12<00:56,  2.35it/s]Epoch 21:  56%|█████▌    | 168/300 [01:12<00:55,  2.39it/s]Epoch 21:  56%|█████▋    | 169/300 [01:13<00:55,  2.37it/s]Epoch 21:  57%|█████▋    | 170/300 [01:13<00:55,  2.35it/s]Epoch 21:  57%|█████▋    | 171/300 [01:14<00:52,  2.45it/s]Epoch 21:  57%|█████▋    | 172/300 [01:14<00:52,  2.45it/s]Epoch 21:  58%|█████▊    | 173/300 [01:14<00:52,  2.43it/s]Epoch 21:  58%|█████▊    | 174/300 [01:15<00:55,  2.27it/s]Epoch 21:  58%|█████▊    | 175/300 [01:15<00:51,  2.42it/s]Epoch 21:  59%|█████▊    | 176/300 [01:16<00:51,  2.41it/s]Epoch 21:  59%|█████▉    | 177/300 [01:16<00:48,  2.52it/s]Epoch 21:  59%|█████▉    | 178/300 [01:16<00:51,  2.38it/s]Epoch 21:  60%|█████▉    | 179/300 [01:17<00:50,  2.38it/s]06/19/2022 15:14:37 - INFO - __main__ - global step: 3240; train loss: 6.444601535797119; dev loss: 6.654227256774902
Epoch 21:  60%|██████    | 180/300 [01:17<00:50,  2.36it/s]Epoch 21:  60%|██████    | 181/300 [01:18<00:47,  2.49it/s]Epoch 21:  61%|██████    | 182/300 [01:18<00:53,  2.20it/s]Epoch 21:  61%|██████    | 183/300 [01:19<00:53,  2.20it/s]Epoch 21:  61%|██████▏   | 184/300 [01:19<00:52,  2.19it/s]Epoch 21:  62%|██████▏   | 185/300 [01:20<00:51,  2.23it/s]Epoch 21:  62%|██████▏   | 186/300 [01:20<00:55,  2.05it/s]Epoch 21:  62%|██████▏   | 187/300 [01:21<00:55,  2.04it/s]Epoch 21:  63%|██████▎   | 188/300 [01:21<00:53,  2.11it/s]Epoch 21:  63%|██████▎   | 189/300 [01:21<00:48,  2.30it/s]Epoch 21:  63%|██████▎   | 190/300 [01:22<00:47,  2.30it/s]Epoch 21:  64%|██████▎   | 191/300 [01:22<00:44,  2.46it/s]Epoch 21:  64%|██████▍   | 192/300 [01:23<00:41,  2.58it/s]Epoch 21:  64%|██████▍   | 193/300 [01:23<00:42,  2.55it/s]Epoch 21:  65%|██████▍   | 194/300 [01:24<00:51,  2.06it/s]Epoch 21:  65%|██████▌   | 195/300 [01:24<00:48,  2.17it/s]Epoch 21:  65%|██████▌   | 196/300 [01:25<00:47,  2.18it/s]Epoch 21:  66%|██████▌   | 197/300 [01:25<00:45,  2.27it/s]Epoch 21:  66%|██████▌   | 198/300 [01:25<00:44,  2.28it/s]Epoch 21:  66%|██████▋   | 199/300 [01:26<00:46,  2.15it/s]06/19/2022 15:14:46 - INFO - __main__ - global step: 3250; train loss: 6.336426734924316; dev loss: 6.514040946960449
Epoch 21:  67%|██████▋   | 200/300 [01:26<00:45,  2.19it/s]Epoch 21:  67%|██████▋   | 201/300 [01:27<00:42,  2.30it/s]Epoch 21:  67%|██████▋   | 202/300 [01:27<00:42,  2.30it/s]Epoch 21:  68%|██████▊   | 203/300 [01:28<00:45,  2.15it/s]Epoch 21:  68%|██████▊   | 204/300 [01:28<00:43,  2.22it/s]Epoch 21:  68%|██████▊   | 205/300 [01:29<00:43,  2.21it/s]Epoch 21:  69%|██████▊   | 206/300 [01:29<00:44,  2.12it/s]Epoch 21:  69%|██████▉   | 207/300 [01:30<00:44,  2.11it/s]Epoch 21:  69%|██████▉   | 208/300 [01:30<00:42,  2.17it/s]Epoch 21:  70%|██████▉   | 209/300 [01:30<00:38,  2.35it/s]Epoch 21:  70%|███████   | 210/300 [01:31<00:38,  2.35it/s]Epoch 21:  70%|███████   | 211/300 [01:31<00:38,  2.31it/s]Epoch 21:  71%|███████   | 212/300 [01:32<00:36,  2.41it/s]Epoch 21:  71%|███████   | 213/300 [01:32<00:35,  2.47it/s]Epoch 21:  71%|███████▏  | 214/300 [01:32<00:36,  2.35it/s]Epoch 21:  72%|███████▏  | 215/300 [01:33<00:41,  2.03it/s]Epoch 21:  72%|███████▏  | 216/300 [01:33<00:37,  2.23it/s]Epoch 21:  72%|███████▏  | 217/300 [01:34<00:34,  2.39it/s]Epoch 21:  73%|███████▎  | 218/300 [01:34<00:33,  2.48it/s]Epoch 21:  73%|███████▎  | 219/300 [01:35<00:33,  2.39it/s]06/19/2022 15:14:54 - INFO - __main__ - global step: 3260; train loss: 6.287533283233643; dev loss: 6.249156475067139
Epoch 21:  73%|███████▎  | 220/300 [01:35<00:31,  2.53it/s]Epoch 21:  74%|███████▎  | 221/300 [01:35<00:30,  2.62it/s]Epoch 21:  74%|███████▍  | 222/300 [01:36<00:29,  2.66it/s]Epoch 21:  74%|███████▍  | 223/300 [01:36<00:28,  2.74it/s]Epoch 21:  75%|███████▍  | 224/300 [01:37<00:32,  2.35it/s]Epoch 21:  75%|███████▌  | 225/300 [01:37<00:32,  2.29it/s]Epoch 21:  75%|███████▌  | 226/300 [01:38<00:33,  2.23it/s]Epoch 21:  76%|███████▌  | 227/300 [01:38<00:33,  2.19it/s]Epoch 21:  76%|███████▌  | 228/300 [01:38<00:33,  2.14it/s]Epoch 21:  76%|███████▋  | 229/300 [01:39<00:31,  2.27it/s]Epoch 21:  77%|███████▋  | 230/300 [01:39<00:29,  2.39it/s]Epoch 21:  77%|███████▋  | 231/300 [01:40<00:28,  2.43it/s]Epoch 21:  77%|███████▋  | 232/300 [01:40<00:28,  2.37it/s]Epoch 21:  78%|███████▊  | 233/300 [01:40<00:27,  2.46it/s]Epoch 21:  78%|███████▊  | 234/300 [01:41<00:25,  2.59it/s]Epoch 21:  78%|███████▊  | 235/300 [01:41<00:24,  2.60it/s]Epoch 21:  79%|███████▊  | 236/300 [01:42<00:26,  2.42it/s]Epoch 21:  79%|███████▉  | 237/300 [01:42<00:27,  2.27it/s]Epoch 21:  79%|███████▉  | 238/300 [01:43<00:27,  2.27it/s]Epoch 21:  80%|███████▉  | 239/300 [01:43<00:27,  2.22it/s]06/19/2022 15:15:03 - INFO - __main__ - global step: 3270; train loss: 6.1869916915893555; dev loss: 6.170984745025635
Epoch 21:  80%|████████  | 240/300 [01:44<00:27,  2.18it/s]Epoch 21:  80%|████████  | 241/300 [01:44<00:25,  2.36it/s]Epoch 21:  81%|████████  | 242/300 [01:44<00:24,  2.39it/s]Epoch 21:  81%|████████  | 243/300 [01:45<00:23,  2.46it/s]Epoch 21:  81%|████████▏ | 244/300 [01:45<00:24,  2.30it/s]Epoch 21:  82%|████████▏ | 245/300 [01:46<00:22,  2.43it/s]Epoch 21:  82%|████████▏ | 246/300 [01:46<00:22,  2.41it/s]Epoch 21:  82%|████████▏ | 247/300 [01:46<00:21,  2.45it/s]Epoch 21:  83%|████████▎ | 248/300 [01:47<00:21,  2.39it/s]Epoch 21:  83%|████████▎ | 249/300 [01:47<00:20,  2.49it/s]Epoch 21:  83%|████████▎ | 250/300 [01:47<00:19,  2.58it/s]Epoch 21:  84%|████████▎ | 251/300 [01:48<00:18,  2.68it/s]Epoch 21:  84%|████████▍ | 252/300 [01:48<00:17,  2.74it/s]Epoch 21:  84%|████████▍ | 253/300 [01:49<00:20,  2.33it/s]Epoch 21:  85%|████████▍ | 254/300 [01:49<00:20,  2.29it/s]Epoch 21:  85%|████████▌ | 255/300 [01:50<00:20,  2.23it/s]Epoch 21:  85%|████████▌ | 256/300 [01:50<00:19,  2.30it/s]Epoch 21:  86%|████████▌ | 257/300 [01:51<00:21,  1.98it/s]Epoch 21:  86%|████████▌ | 258/300 [01:51<00:20,  2.01it/s]Epoch 21:  86%|████████▋ | 259/300 [01:52<00:20,  2.04it/s]06/19/2022 15:15:12 - INFO - __main__ - global step: 3280; train loss: 6.3064961433410645; dev loss: 6.259219169616699
Epoch 21:  87%|████████▋ | 260/300 [01:52<00:20,  2.00it/s]Epoch 21:  87%|████████▋ | 261/300 [01:53<00:20,  1.92it/s]Epoch 21:  87%|████████▋ | 262/300 [01:53<00:18,  2.00it/s]Epoch 21:  88%|████████▊ | 263/300 [01:54<00:17,  2.06it/s]Epoch 21:  88%|████████▊ | 264/300 [01:54<00:16,  2.24it/s]Epoch 21:  88%|████████▊ | 265/300 [01:55<00:16,  2.18it/s]Epoch 21:  89%|████████▊ | 266/300 [01:55<00:14,  2.31it/s]Epoch 21:  89%|████████▉ | 267/300 [01:55<00:14,  2.21it/s]Epoch 21:  89%|████████▉ | 268/300 [01:56<00:13,  2.30it/s]Epoch 21:  90%|████████▉ | 269/300 [01:56<00:14,  2.19it/s]Epoch 21:  90%|█████████ | 270/300 [01:57<00:13,  2.24it/s]Epoch 21:  90%|█████████ | 271/300 [01:57<00:12,  2.39it/s]Epoch 21:  91%|█████████ | 272/300 [01:57<00:11,  2.52it/s]Epoch 21:  91%|█████████ | 273/300 [01:58<00:11,  2.37it/s]Epoch 21:  91%|█████████▏| 274/300 [01:58<00:11,  2.36it/s]Epoch 21:  92%|█████████▏| 275/300 [01:59<00:10,  2.39it/s]Epoch 21:  92%|█████████▏| 276/300 [01:59<00:09,  2.43it/s]Epoch 21:  92%|█████████▏| 277/300 [02:00<00:09,  2.39it/s]Epoch 21:  93%|█████████▎| 278/300 [02:00<00:09,  2.30it/s]Epoch 21:  93%|█████████▎| 279/300 [02:00<00:09,  2.30it/s]06/19/2022 15:15:20 - INFO - __main__ - global step: 3290; train loss: 6.3909711837768555; dev loss: 6.321135520935059
Epoch 21:  93%|█████████▎| 280/300 [02:01<00:08,  2.35it/s]Epoch 21:  94%|█████████▎| 281/300 [02:01<00:07,  2.39it/s]Epoch 21:  94%|█████████▍| 282/300 [02:02<00:07,  2.31it/s]Epoch 21:  94%|█████████▍| 283/300 [02:02<00:07,  2.39it/s]Epoch 21:  95%|█████████▍| 284/300 [02:03<00:06,  2.49it/s]Epoch 21:  95%|█████████▌| 285/300 [02:03<00:06,  2.43it/s]Epoch 21:  95%|█████████▌| 286/300 [02:03<00:06,  2.32it/s]Epoch 21:  96%|█████████▌| 287/300 [02:04<00:05,  2.41it/s]Epoch 21:  96%|█████████▌| 288/300 [02:04<00:04,  2.43it/s]Epoch 21:  96%|█████████▋| 289/300 [02:05<00:04,  2.37it/s]Epoch 21:  97%|█████████▋| 290/300 [02:05<00:04,  2.27it/s]Epoch 21:  97%|█████████▋| 291/300 [02:06<00:03,  2.32it/s]Epoch 21:  97%|█████████▋| 292/300 [02:06<00:03,  2.37it/s]Epoch 21:  98%|█████████▊| 293/300 [02:06<00:02,  2.42it/s]Epoch 21:  98%|█████████▊| 294/300 [02:07<00:02,  2.22it/s]Epoch 21:  98%|█████████▊| 295/300 [02:07<00:02,  2.31it/s]Epoch 21:  99%|█████████▊| 296/300 [02:08<00:01,  2.40it/s]Epoch 21:  99%|█████████▉| 297/300 [02:08<00:01,  2.40it/s]Epoch 21:  99%|█████████▉| 298/300 [02:09<00:00,  2.32it/s]Epoch 21: 100%|█████████▉| 299/300 [02:09<00:00,  2.36it/s]06/19/2022 15:15:29 - INFO - __main__ - global step: 3300; train loss: 5.804751873016357; dev loss: 5.961160182952881
Epoch 21: 100%|██████████| 300/300 [02:09<00:00,  2.33it/s]Epoch 21: 100%|██████████| 300/300 [02:09<00:00,  2.31it/s]
Epoch 22:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 22:   0%|          | 1/300 [00:00<01:53,  2.63it/s]Epoch 22:   1%|          | 2/300 [00:00<02:06,  2.35it/s]Epoch 22:   1%|          | 3/300 [00:01<02:08,  2.31it/s]Epoch 22:   1%|▏         | 4/300 [00:01<02:10,  2.26it/s]Epoch 22:   2%|▏         | 5/300 [00:02<02:09,  2.27it/s]Epoch 22:   2%|▏         | 6/300 [00:02<02:03,  2.39it/s]Epoch 22:   2%|▏         | 7/300 [00:03<02:16,  2.15it/s]Epoch 22:   3%|▎         | 8/300 [00:03<02:10,  2.23it/s]Epoch 22:   3%|▎         | 9/300 [00:03<02:07,  2.28it/s]Epoch 22:   3%|▎         | 10/300 [00:04<02:07,  2.28it/s]Epoch 22:   4%|▎         | 11/300 [00:04<02:10,  2.22it/s]Epoch 22:   4%|▍         | 12/300 [00:05<02:03,  2.33it/s]Epoch 22:   4%|▍         | 13/300 [00:05<01:57,  2.44it/s]Epoch 22:   5%|▍         | 14/300 [00:05<01:53,  2.52it/s]Epoch 22:   5%|▌         | 15/300 [00:06<02:01,  2.35it/s]Epoch 22:   5%|▌         | 16/300 [00:06<01:59,  2.38it/s]Epoch 22:   6%|▌         | 17/300 [00:07<02:10,  2.16it/s]Epoch 22:   6%|▌         | 18/300 [00:07<02:12,  2.14it/s]Epoch 22:   6%|▋         | 19/300 [00:08<02:12,  2.13it/s]06/19/2022 15:15:37 - INFO - __main__ - global step: 3310; train loss: 5.856875419616699; dev loss: 5.773724555969238
Epoch 22:   7%|▋         | 20/300 [00:08<02:01,  2.31it/s]Epoch 22:   7%|▋         | 21/300 [00:09<01:55,  2.41it/s]Epoch 22:   7%|▋         | 22/300 [00:09<02:05,  2.22it/s]Epoch 22:   8%|▊         | 23/300 [00:10<02:25,  1.90it/s]Epoch 22:   8%|▊         | 24/300 [00:10<02:10,  2.11it/s]Epoch 22:   8%|▊         | 25/300 [00:11<02:00,  2.29it/s]Epoch 22:   9%|▊         | 26/300 [00:11<01:53,  2.41it/s]Epoch 22:   9%|▉         | 27/300 [00:11<01:55,  2.37it/s]Epoch 22:   9%|▉         | 28/300 [00:12<01:52,  2.42it/s]Epoch 22:  10%|▉         | 29/300 [00:12<01:45,  2.57it/s]Epoch 22:  10%|█         | 30/300 [00:12<01:43,  2.60it/s]Epoch 22:  10%|█         | 31/300 [00:13<01:43,  2.61it/s]Epoch 22:  11%|█         | 32/300 [00:13<01:50,  2.43it/s]Epoch 22:  11%|█         | 33/300 [00:14<01:55,  2.32it/s]Epoch 22:  11%|█▏        | 34/300 [00:14<01:49,  2.43it/s]Epoch 22:  12%|█▏        | 35/300 [00:15<01:54,  2.31it/s]Epoch 22:  12%|█▏        | 36/300 [00:15<02:11,  2.00it/s]Epoch 22:  12%|█▏        | 37/300 [00:16<02:06,  2.07it/s]Epoch 22:  13%|█▎        | 38/300 [00:16<02:03,  2.12it/s]Epoch 22:  13%|█▎        | 39/300 [00:17<02:02,  2.14it/s]06/19/2022 15:15:46 - INFO - __main__ - global step: 3320; train loss: 6.204136848449707; dev loss: 6.474949836730957
Epoch 22:  13%|█▎        | 40/300 [00:17<02:10,  1.99it/s]Epoch 22:  14%|█▎        | 41/300 [00:18<02:07,  2.04it/s]Epoch 22:  14%|█▍        | 42/300 [00:18<02:05,  2.06it/s]Epoch 22:  14%|█▍        | 43/300 [00:19<02:05,  2.05it/s]Epoch 22:  15%|█▍        | 44/300 [00:19<02:12,  1.94it/s]Epoch 22:  15%|█▌        | 45/300 [00:20<02:05,  2.03it/s]Epoch 22:  15%|█▌        | 46/300 [00:20<01:54,  2.22it/s]Epoch 22:  16%|█▌        | 47/300 [00:20<01:46,  2.38it/s]Epoch 22:  16%|█▌        | 48/300 [00:21<01:47,  2.35it/s]Epoch 22:  16%|█▋        | 49/300 [00:21<01:44,  2.40it/s]Epoch 22:  17%|█▋        | 50/300 [00:22<01:42,  2.43it/s]Epoch 22:  17%|█▋        | 51/300 [00:22<01:47,  2.32it/s]Epoch 22:  17%|█▋        | 52/300 [00:23<01:54,  2.16it/s]Epoch 22:  18%|█▊        | 53/300 [00:23<01:47,  2.31it/s]Epoch 22:  18%|█▊        | 54/300 [00:23<01:49,  2.25it/s]Epoch 22:  18%|█▊        | 55/300 [00:24<01:47,  2.28it/s]Epoch 22:  19%|█▊        | 56/300 [00:24<01:48,  2.26it/s]Epoch 22:  19%|█▉        | 57/300 [00:25<01:46,  2.29it/s]Epoch 22:  19%|█▉        | 58/300 [00:25<01:44,  2.31it/s]Epoch 22:  20%|█▉        | 59/300 [00:26<01:42,  2.34it/s]06/19/2022 15:15:55 - INFO - __main__ - global step: 3330; train loss: 6.178580284118652; dev loss: 5.891897201538086
Epoch 22:  20%|██        | 60/300 [00:26<01:41,  2.37it/s]Epoch 22:  20%|██        | 61/300 [00:27<01:59,  2.01it/s]Epoch 22:  21%|██        | 62/300 [00:27<01:48,  2.19it/s]Epoch 22:  21%|██        | 63/300 [00:27<01:46,  2.23it/s]Epoch 22:  21%|██▏       | 64/300 [00:28<01:39,  2.38it/s]Epoch 22:  22%|██▏       | 65/300 [00:28<01:41,  2.31it/s]Epoch 22:  22%|██▏       | 66/300 [00:29<01:35,  2.45it/s]Epoch 22:  22%|██▏       | 67/300 [00:29<01:31,  2.56it/s]Epoch 22:  23%|██▎       | 68/300 [00:29<01:27,  2.66it/s]Epoch 22:  23%|██▎       | 69/300 [00:30<01:30,  2.54it/s]Epoch 22:  23%|██▎       | 70/300 [00:30<01:27,  2.64it/s]Epoch 22:  24%|██▎       | 71/300 [00:30<01:24,  2.69it/s]Epoch 22:  24%|██▍       | 72/300 [00:31<01:24,  2.70it/s]Epoch 22:  24%|██▍       | 73/300 [00:31<01:30,  2.52it/s]Epoch 22:  25%|██▍       | 74/300 [00:32<01:26,  2.60it/s]Epoch 22:  25%|██▌       | 75/300 [00:32<01:25,  2.62it/s]Epoch 22:  25%|██▌       | 76/300 [00:32<01:30,  2.49it/s]Epoch 22:  26%|██▌       | 77/300 [00:33<01:41,  2.19it/s]Epoch 22:  26%|██▌       | 78/300 [00:33<01:36,  2.29it/s]Epoch 22:  26%|██▋       | 79/300 [00:34<01:31,  2.40it/s]06/19/2022 15:16:03 - INFO - __main__ - global step: 3340; train loss: 6.214146614074707; dev loss: 6.1314287185668945
Epoch 22:  27%|██▋       | 80/300 [00:34<01:29,  2.46it/s]Epoch 22:  27%|██▋       | 81/300 [00:35<01:35,  2.30it/s]Epoch 22:  27%|██▋       | 82/300 [00:35<01:31,  2.39it/s]Epoch 22:  28%|██▊       | 83/300 [00:35<01:28,  2.44it/s]Epoch 22:  28%|██▊       | 84/300 [00:36<01:26,  2.49it/s]Epoch 22:  28%|██▊       | 85/300 [00:36<01:24,  2.53it/s]Epoch 22:  29%|██▊       | 86/300 [00:37<01:30,  2.38it/s]Epoch 22:  29%|██▉       | 87/300 [00:37<01:33,  2.28it/s]Epoch 22:  29%|██▉       | 88/300 [00:38<01:27,  2.43it/s]Epoch 22:  30%|██▉       | 89/300 [00:38<01:22,  2.56it/s]Epoch 22:  30%|███       | 90/300 [00:38<01:25,  2.47it/s]Epoch 22:  30%|███       | 91/300 [00:39<01:20,  2.58it/s]Epoch 22:  31%|███       | 92/300 [00:39<01:17,  2.67it/s]Epoch 22:  31%|███       | 93/300 [00:39<01:16,  2.72it/s]Epoch 22:  31%|███▏      | 94/300 [00:40<01:20,  2.56it/s]Epoch 22:  32%|███▏      | 95/300 [00:40<01:22,  2.50it/s]Epoch 22:  32%|███▏      | 96/300 [00:41<01:19,  2.58it/s]Epoch 22:  32%|███▏      | 97/300 [00:41<01:22,  2.46it/s]Epoch 22:  33%|███▎      | 98/300 [00:42<01:34,  2.15it/s]Epoch 22:  33%|███▎      | 99/300 [00:42<01:37,  2.07it/s]06/19/2022 15:16:12 - INFO - __main__ - global step: 3350; train loss: 5.777766227722168; dev loss: 5.786706924438477
Epoch 22:  33%|███▎      | 100/300 [00:43<01:37,  2.06it/s]Epoch 22:  34%|███▎      | 101/300 [00:43<01:35,  2.08it/s]Epoch 22:  34%|███▍      | 102/300 [00:44<01:43,  1.91it/s]Epoch 22:  34%|███▍      | 103/300 [00:44<01:39,  1.97it/s]Epoch 22:  35%|███▍      | 104/300 [00:45<01:33,  2.09it/s]Epoch 22:  35%|███▌      | 105/300 [00:45<01:29,  2.17it/s]Epoch 22:  35%|███▌      | 106/300 [00:46<01:36,  2.02it/s]Epoch 22:  36%|███▌      | 107/300 [00:46<01:34,  2.05it/s]Epoch 22:  36%|███▌      | 108/300 [00:47<01:30,  2.13it/s]Epoch 22:  36%|███▋      | 109/300 [00:47<01:22,  2.31it/s]Epoch 22:  37%|███▋      | 110/300 [00:47<01:22,  2.29it/s]Epoch 22:  37%|███▋      | 111/300 [00:48<01:17,  2.45it/s]Epoch 22:  37%|███▋      | 112/300 [00:48<01:13,  2.54it/s]Epoch 22:  38%|███▊      | 113/300 [00:48<01:16,  2.44it/s]Epoch 22:  38%|███▊      | 114/300 [00:49<01:13,  2.55it/s]Epoch 22:  38%|███▊      | 115/300 [00:49<01:22,  2.23it/s]Epoch 22:  39%|███▊      | 116/300 [00:50<01:19,  2.31it/s]Epoch 22:  39%|███▉      | 117/300 [00:50<01:15,  2.43it/s]Epoch 22:  39%|███▉      | 118/300 [00:51<01:13,  2.46it/s]Epoch 22:  40%|███▉      | 119/300 [00:51<01:15,  2.40it/s]06/19/2022 15:16:21 - INFO - __main__ - global step: 3360; train loss: 5.8850507736206055; dev loss: 6.0351762771606445
Epoch 22:  40%|████      | 120/300 [00:51<01:16,  2.34it/s]Epoch 22:  40%|████      | 121/300 [00:52<01:13,  2.42it/s]Epoch 22:  41%|████      | 122/300 [00:52<01:17,  2.29it/s]Epoch 22:  41%|████      | 123/300 [00:53<01:29,  1.98it/s]Epoch 22:  41%|████▏     | 124/300 [00:53<01:26,  2.04it/s]Epoch 22:  42%|████▏     | 125/300 [00:54<01:22,  2.12it/s]Epoch 22:  42%|████▏     | 126/300 [00:54<01:19,  2.18it/s]Epoch 22:  42%|████▏     | 127/300 [00:55<01:21,  2.13it/s]Epoch 22:  43%|████▎     | 128/300 [00:55<01:16,  2.25it/s]Epoch 22:  43%|████▎     | 129/300 [00:56<01:13,  2.34it/s]Epoch 22:  43%|████▎     | 130/300 [00:56<01:08,  2.48it/s]Epoch 22:  44%|████▎     | 131/300 [00:56<01:14,  2.27it/s]Epoch 22:  44%|████▍     | 132/300 [00:57<01:10,  2.38it/s]Epoch 22:  44%|████▍     | 133/300 [00:57<01:08,  2.43it/s]Epoch 22:  45%|████▍     | 134/300 [00:58<01:06,  2.48it/s]Epoch 22:  45%|████▌     | 135/300 [00:58<01:12,  2.27it/s]Epoch 22:  45%|████▌     | 136/300 [00:59<01:12,  2.28it/s]Epoch 22:  46%|████▌     | 137/300 [00:59<01:12,  2.24it/s]Epoch 22:  46%|████▌     | 138/300 [00:59<01:13,  2.22it/s]Epoch 22:  46%|████▋     | 139/300 [01:00<01:17,  2.09it/s]06/19/2022 15:16:30 - INFO - __main__ - global step: 3370; train loss: 5.5811238288879395; dev loss: 5.776519775390625
Epoch 22:  47%|████▋     | 140/300 [01:00<01:15,  2.12it/s]Epoch 22:  47%|████▋     | 141/300 [01:01<01:10,  2.25it/s]Epoch 22:  47%|████▋     | 142/300 [01:01<01:07,  2.35it/s]Epoch 22:  48%|████▊     | 143/300 [01:02<01:06,  2.38it/s]Epoch 22:  48%|████▊     | 144/300 [01:02<01:09,  2.24it/s]Epoch 22:  48%|████▊     | 145/300 [01:03<01:10,  2.19it/s]Epoch 22:  49%|████▊     | 146/300 [01:03<01:09,  2.22it/s]Epoch 22:  49%|████▉     | 147/300 [01:04<01:12,  2.11it/s]Epoch 22:  49%|████▉     | 148/300 [01:04<01:13,  2.06it/s]Epoch 22:  50%|████▉     | 149/300 [01:05<01:11,  2.10it/s]Epoch 22:  50%|█████     | 150/300 [01:05<01:12,  2.06it/s]Epoch 22:  50%|█████     | 151/300 [01:06<01:13,  2.02it/s]Epoch 22:  51%|█████     | 152/300 [01:06<01:13,  2.02it/s]Epoch 22:  51%|█████     | 153/300 [01:06<01:08,  2.16it/s]Epoch 22:  51%|█████▏    | 154/300 [01:07<01:05,  2.24it/s]Epoch 22:  52%|█████▏    | 155/300 [01:07<01:01,  2.34it/s]Epoch 22:  52%|█████▏    | 156/300 [01:08<01:05,  2.18it/s]Epoch 22:  52%|█████▏    | 157/300 [01:08<01:01,  2.34it/s]Epoch 22:  53%|█████▎    | 158/300 [01:09<01:01,  2.32it/s]Epoch 22:  53%|█████▎    | 159/300 [01:09<00:58,  2.42it/s]06/19/2022 15:16:39 - INFO - __main__ - global step: 3380; train loss: 5.86488151550293; dev loss: 5.7258195877075195
Epoch 22:  53%|█████▎    | 160/300 [01:09<01:02,  2.25it/s]Epoch 22:  54%|█████▎    | 161/300 [01:10<00:58,  2.38it/s]Epoch 22:  54%|█████▍    | 162/300 [01:10<00:58,  2.36it/s]Epoch 22:  54%|█████▍    | 163/300 [01:11<00:55,  2.46it/s]Epoch 22:  55%|█████▍    | 164/300 [01:11<00:59,  2.29it/s]Epoch 22:  55%|█████▌    | 165/300 [01:12<00:57,  2.34it/s]Epoch 22:  55%|█████▌    | 166/300 [01:12<00:56,  2.36it/s]Epoch 22:  56%|█████▌    | 167/300 [01:12<00:55,  2.41it/s]Epoch 22:  56%|█████▌    | 168/300 [01:13<00:53,  2.45it/s]Epoch 22:  56%|█████▋    | 169/300 [01:13<00:56,  2.32it/s]Epoch 22:  57%|█████▋    | 170/300 [01:14<00:55,  2.35it/s]Epoch 22:  57%|█████▋    | 171/300 [01:14<00:53,  2.41it/s]Epoch 22:  57%|█████▋    | 172/300 [01:14<00:51,  2.48it/s]Epoch 22:  58%|█████▊    | 173/300 [01:15<00:53,  2.36it/s]Epoch 22:  58%|█████▊    | 174/300 [01:15<00:54,  2.31it/s]Epoch 22:  58%|█████▊    | 175/300 [01:16<00:52,  2.40it/s]Epoch 22:  59%|█████▊    | 176/300 [01:16<00:50,  2.45it/s]Epoch 22:  59%|█████▉    | 177/300 [01:17<00:58,  2.11it/s]Epoch 22:  59%|█████▉    | 178/300 [01:17<01:00,  2.00it/s]Epoch 22:  60%|█████▉    | 179/300 [01:18<00:59,  2.05it/s]06/19/2022 15:16:47 - INFO - __main__ - global step: 3390; train loss: 5.9247589111328125; dev loss: 5.93295955657959
Epoch 22:  60%|██████    | 180/300 [01:18<00:56,  2.13it/s]Epoch 22:  60%|██████    | 181/300 [01:19<00:59,  1.99it/s]Epoch 22:  61%|██████    | 182/300 [01:19<01:01,  1.93it/s]Epoch 22:  61%|██████    | 183/300 [01:20<00:58,  1.99it/s]Epoch 22:  61%|██████▏   | 184/300 [01:20<00:54,  2.11it/s]Epoch 22:  62%|██████▏   | 185/300 [01:21<00:57,  2.00it/s]Epoch 22:  62%|██████▏   | 186/300 [01:21<00:53,  2.14it/s]Epoch 22:  62%|██████▏   | 187/300 [01:22<00:51,  2.19it/s]Epoch 22:  63%|██████▎   | 188/300 [01:22<00:50,  2.20it/s]Epoch 22:  63%|██████▎   | 189/300 [01:23<00:54,  2.03it/s]Epoch 22:  63%|██████▎   | 190/300 [01:23<00:53,  2.06it/s]Epoch 22:  64%|██████▎   | 191/300 [01:24<00:53,  2.04it/s]Epoch 22:  64%|██████▍   | 192/300 [01:24<00:51,  2.08it/s]Epoch 22:  64%|██████▍   | 193/300 [01:24<00:50,  2.11it/s]Epoch 22:  65%|██████▍   | 194/300 [01:25<00:54,  1.95it/s]Epoch 22:  65%|██████▌   | 195/300 [01:25<00:50,  2.08it/s]Epoch 22:  65%|██████▌   | 196/300 [01:26<00:45,  2.27it/s]Epoch 22:  66%|██████▌   | 197/300 [01:26<00:42,  2.43it/s]Epoch 22:  66%|██████▌   | 198/300 [01:27<00:43,  2.37it/s]Epoch 22:  66%|██████▋   | 199/300 [01:27<00:40,  2.48it/s]06/19/2022 15:16:57 - INFO - __main__ - global step: 3400; train loss: 6.557670593261719; dev loss: 6.517275333404541
Epoch 22:  67%|██████▋   | 200/300 [01:27<00:41,  2.40it/s]Epoch 22:  67%|██████▋   | 201/300 [01:28<00:43,  2.26it/s]Epoch 22:  67%|██████▋   | 202/300 [01:29<00:48,  2.00it/s]Epoch 22:  68%|██████▊   | 203/300 [01:29<00:47,  2.03it/s]Epoch 22:  68%|██████▊   | 204/300 [01:29<00:45,  2.10it/s]Epoch 22:  68%|██████▊   | 205/300 [01:30<00:42,  2.24it/s]Epoch 22:  69%|██████▊   | 206/300 [01:30<00:43,  2.18it/s]Epoch 22:  69%|██████▉   | 207/300 [01:31<00:42,  2.21it/s]Epoch 22:  69%|██████▉   | 208/300 [01:31<00:39,  2.35it/s]Epoch 22:  70%|██████▉   | 209/300 [01:31<00:36,  2.49it/s]Epoch 22:  70%|███████   | 210/300 [01:32<00:36,  2.44it/s]Epoch 22:  70%|███████   | 211/300 [01:32<00:35,  2.52it/s]Epoch 22:  71%|███████   | 212/300 [01:33<00:35,  2.45it/s]Epoch 22:  71%|███████   | 213/300 [01:33<00:34,  2.50it/s]Epoch 22:  71%|███████▏  | 214/300 [01:34<00:40,  2.13it/s]Epoch 22:  72%|███████▏  | 215/300 [01:34<00:38,  2.21it/s]Epoch 22:  72%|███████▏  | 216/300 [01:35<00:36,  2.28it/s]Epoch 22:  72%|███████▏  | 217/300 [01:35<00:35,  2.31it/s]Epoch 22:  73%|███████▎  | 218/300 [01:36<00:38,  2.11it/s]Epoch 22:  73%|███████▎  | 219/300 [01:36<00:37,  2.16it/s]06/19/2022 15:17:06 - INFO - __main__ - global step: 3410; train loss: 6.163193702697754; dev loss: 6.264035701751709
Epoch 22:  73%|███████▎  | 220/300 [01:36<00:37,  2.15it/s]Epoch 22:  74%|███████▎  | 221/300 [01:37<00:36,  2.16it/s]Epoch 22:  74%|███████▍  | 222/300 [01:37<00:35,  2.19it/s]Epoch 22:  74%|███████▍  | 223/300 [01:38<00:36,  2.13it/s]Epoch 22:  75%|███████▍  | 224/300 [01:38<00:32,  2.33it/s]Epoch 22:  75%|███████▌  | 225/300 [01:39<00:29,  2.51it/s]Epoch 22:  75%|███████▌  | 226/300 [01:39<00:28,  2.57it/s]Epoch 22:  76%|███████▌  | 227/300 [01:39<00:29,  2.49it/s]Epoch 22:  76%|███████▌  | 228/300 [01:40<00:27,  2.63it/s]Epoch 22:  76%|███████▋  | 229/300 [01:40<00:25,  2.74it/s]Epoch 22:  77%|███████▋  | 230/300 [01:40<00:25,  2.73it/s]Epoch 22:  77%|███████▋  | 231/300 [01:41<00:26,  2.60it/s]Epoch 22:  77%|███████▋  | 232/300 [01:41<00:25,  2.62it/s]Epoch 22:  78%|███████▊  | 233/300 [01:41<00:24,  2.72it/s]Epoch 22:  78%|███████▊  | 234/300 [01:42<00:23,  2.80it/s]Epoch 22:  78%|███████▊  | 235/300 [01:42<00:24,  2.62it/s]Epoch 22:  79%|███████▊  | 236/300 [01:43<00:24,  2.59it/s]Epoch 22:  79%|███████▉  | 237/300 [01:43<00:25,  2.48it/s]Epoch 22:  79%|███████▉  | 238/300 [01:44<00:25,  2.41it/s]Epoch 22:  80%|███████▉  | 239/300 [01:44<00:27,  2.22it/s]06/19/2022 15:17:14 - INFO - __main__ - global step: 3420; train loss: 6.364973545074463; dev loss: 6.385134696960449
Epoch 22:  80%|████████  | 240/300 [01:44<00:24,  2.41it/s]Epoch 22:  80%|████████  | 241/300 [01:45<00:23,  2.56it/s]Epoch 22:  81%|████████  | 242/300 [01:45<00:21,  2.67it/s]Epoch 22:  81%|████████  | 243/300 [01:45<00:22,  2.56it/s]Epoch 22:  81%|████████▏ | 244/300 [01:46<00:20,  2.69it/s]Epoch 22:  82%|████████▏ | 245/300 [01:46<00:19,  2.80it/s]Epoch 22:  82%|████████▏ | 246/300 [01:46<00:18,  2.86it/s]Epoch 22:  82%|████████▏ | 247/300 [01:47<00:18,  2.89it/s]Epoch 22:  83%|████████▎ | 248/300 [01:47<00:21,  2.44it/s]Epoch 22:  83%|████████▎ | 249/300 [01:48<00:20,  2.46it/s]Epoch 22:  83%|████████▎ | 250/300 [01:48<00:20,  2.39it/s]Epoch 22:  84%|████████▎ | 251/300 [01:49<00:21,  2.33it/s]Epoch 22:  84%|████████▍ | 252/300 [01:49<00:22,  2.15it/s]Epoch 22:  84%|████████▍ | 253/300 [01:50<00:22,  2.12it/s]Epoch 22:  85%|████████▍ | 254/300 [01:50<00:21,  2.18it/s]Epoch 22:  85%|████████▌ | 255/300 [01:51<00:19,  2.30it/s]Epoch 22:  85%|████████▌ | 256/300 [01:51<00:20,  2.13it/s]Epoch 22:  86%|████████▌ | 257/300 [01:51<00:19,  2.20it/s]Epoch 22:  86%|████████▌ | 258/300 [01:52<00:18,  2.24it/s]Epoch 22:  86%|████████▋ | 259/300 [01:52<00:17,  2.28it/s]06/19/2022 15:17:22 - INFO - __main__ - global step: 3430; train loss: 5.731640815734863; dev loss: 5.788999557495117
Epoch 22:  87%|████████▋ | 260/300 [01:53<00:19,  2.07it/s]Epoch 22:  87%|████████▋ | 261/300 [01:53<00:19,  2.03it/s]Epoch 22:  87%|████████▋ | 262/300 [01:54<00:18,  2.08it/s]Epoch 22:  88%|████████▊ | 263/300 [01:54<00:16,  2.28it/s]Epoch 22:  88%|████████▊ | 264/300 [01:55<00:16,  2.16it/s]Epoch 22:  88%|████████▊ | 265/300 [01:55<00:16,  2.19it/s]Epoch 22:  89%|████████▊ | 266/300 [01:56<00:15,  2.14it/s]Epoch 22:  89%|████████▉ | 267/300 [01:56<00:15,  2.15it/s]Epoch 22:  89%|████████▉ | 268/300 [01:57<00:16,  1.98it/s]Epoch 22:  90%|████████▉ | 269/300 [01:57<00:15,  2.02it/s]Epoch 22:  90%|█████████ | 270/300 [01:58<00:14,  2.09it/s]Epoch 22:  90%|█████████ | 271/300 [01:58<00:13,  2.12it/s]Epoch 22:  91%|█████████ | 272/300 [01:59<00:14,  1.95it/s]Epoch 22:  91%|█████████ | 273/300 [01:59<00:13,  2.03it/s]Epoch 22:  91%|█████████▏| 274/300 [02:00<00:12,  2.02it/s]Epoch 22:  92%|█████████▏| 275/300 [02:00<00:11,  2.09it/s]Epoch 22:  92%|█████████▏| 276/300 [02:01<00:10,  2.18it/s]Epoch 22:  92%|█████████▏| 277/300 [02:01<00:11,  2.04it/s]Epoch 22:  93%|█████████▎| 278/300 [02:02<00:10,  2.09it/s]Epoch 22:  93%|█████████▎| 279/300 [02:02<00:09,  2.19it/s]06/19/2022 15:17:31 - INFO - __main__ - global step: 3440; train loss: 5.896050453186035; dev loss: 5.635786056518555
Epoch 22:  93%|█████████▎| 280/300 [02:02<00:08,  2.32it/s]Epoch 22:  94%|█████████▎| 281/300 [02:03<00:08,  2.25it/s]Epoch 22:  94%|█████████▍| 282/300 [02:03<00:07,  2.43it/s]Epoch 22:  94%|█████████▍| 283/300 [02:03<00:06,  2.58it/s]Epoch 22:  95%|█████████▍| 284/300 [02:04<00:06,  2.63it/s]Epoch 22:  95%|█████████▌| 285/300 [02:04<00:06,  2.47it/s]Epoch 22:  95%|█████████▌| 286/300 [02:05<00:05,  2.53it/s]Epoch 22:  96%|█████████▌| 287/300 [02:05<00:04,  2.65it/s]Epoch 22:  96%|█████████▌| 288/300 [02:05<00:04,  2.62it/s]Epoch 22:  96%|█████████▋| 289/300 [02:06<00:05,  2.18it/s]Epoch 22:  97%|█████████▋| 290/300 [02:06<00:04,  2.17it/s]Epoch 22:  97%|█████████▋| 291/300 [02:07<00:04,  2.18it/s]Epoch 22:  97%|█████████▋| 292/300 [02:08<00:03,  2.02it/s]Epoch 22:  98%|█████████▊| 293/300 [02:08<00:03,  1.81it/s]Epoch 22:  98%|█████████▊| 294/300 [02:09<00:02,  2.01it/s]Epoch 22:  98%|█████████▊| 295/300 [02:09<00:02,  2.17it/s]Epoch 22:  99%|█████████▊| 296/300 [02:09<00:01,  2.27it/s]Epoch 22:  99%|█████████▉| 297/300 [02:10<00:01,  1.95it/s]Epoch 22:  99%|█████████▉| 298/300 [02:11<00:01,  1.95it/s]Epoch 22: 100%|█████████▉| 299/300 [02:11<00:00,  2.02it/s]06/19/2022 15:17:41 - INFO - __main__ - global step: 3450; train loss: 5.590684413909912; dev loss: 5.3237504959106445
Epoch 22: 100%|██████████| 300/300 [02:11<00:00,  2.14it/s]Epoch 22: 100%|██████████| 300/300 [02:11<00:00,  2.27it/s]
Epoch 23:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 23:   0%|          | 1/300 [00:00<01:52,  2.65it/s]Epoch 23:   1%|          | 2/300 [00:00<02:07,  2.34it/s]Epoch 23:   1%|          | 3/300 [00:01<02:02,  2.43it/s]Epoch 23:   1%|▏         | 4/300 [00:01<02:08,  2.30it/s]Epoch 23:   2%|▏         | 5/300 [00:02<02:13,  2.20it/s]Epoch 23:   2%|▏         | 6/300 [00:02<02:28,  1.98it/s]Epoch 23:   2%|▏         | 7/300 [00:03<02:15,  2.16it/s]Epoch 23:   3%|▎         | 8/300 [00:03<02:09,  2.26it/s]Epoch 23:   3%|▎         | 9/300 [00:03<02:01,  2.40it/s]Epoch 23:   3%|▎         | 10/300 [00:04<02:04,  2.33it/s]Epoch 23:   4%|▎         | 11/300 [00:04<01:57,  2.45it/s]Epoch 23:   4%|▍         | 12/300 [00:05<02:01,  2.36it/s]Epoch 23:   4%|▍         | 13/300 [00:05<02:04,  2.30it/s]Epoch 23:   5%|▍         | 14/300 [00:06<02:24,  1.98it/s]Epoch 23:   5%|▌         | 15/300 [00:06<02:13,  2.14it/s]Epoch 23:   5%|▌         | 16/300 [00:07<02:08,  2.22it/s]Epoch 23:   6%|▌         | 17/300 [00:07<02:08,  2.20it/s]Epoch 23:   6%|▌         | 18/300 [00:08<02:22,  1.98it/s]Epoch 23:   6%|▋         | 19/300 [00:08<02:09,  2.16it/s]06/19/2022 15:17:50 - INFO - __main__ - global step: 3460; train loss: 5.725189208984375; dev loss: 5.882124900817871
Epoch 23:   7%|▋         | 20/300 [00:08<02:03,  2.27it/s]Epoch 23:   7%|▋         | 21/300 [00:09<02:01,  2.30it/s]Epoch 23:   7%|▋         | 22/300 [00:09<02:06,  2.19it/s]Epoch 23:   8%|▊         | 23/300 [00:10<01:58,  2.34it/s]Epoch 23:   8%|▊         | 24/300 [00:10<01:51,  2.47it/s]Epoch 23:   8%|▊         | 25/300 [00:10<01:50,  2.50it/s]Epoch 23:   9%|▊         | 26/300 [00:11<01:55,  2.38it/s]Epoch 23:   9%|▉         | 27/300 [00:11<01:52,  2.42it/s]Epoch 23:   9%|▉         | 28/300 [00:12<01:54,  2.38it/s]Epoch 23:  10%|▉         | 29/300 [00:12<01:52,  2.41it/s]Epoch 23:  10%|█         | 30/300 [00:13<01:49,  2.47it/s]Epoch 23:  10%|█         | 31/300 [00:13<01:58,  2.28it/s]Epoch 23:  11%|█         | 32/300 [00:14<01:54,  2.33it/s]Epoch 23:  11%|█         | 33/300 [00:14<01:56,  2.30it/s]Epoch 23:  11%|█▏        | 34/300 [00:14<01:56,  2.27it/s]Epoch 23:  12%|█▏        | 35/300 [00:15<02:02,  2.16it/s]Epoch 23:  12%|█▏        | 36/300 [00:15<02:02,  2.16it/s]Epoch 23:  12%|█▏        | 37/300 [00:16<02:01,  2.17it/s]Epoch 23:  13%|█▎        | 38/300 [00:16<01:58,  2.22it/s]Epoch 23:  13%|█▎        | 39/300 [00:17<02:00,  2.16it/s]06/19/2022 15:17:58 - INFO - __main__ - global step: 3470; train loss: 5.993256568908691; dev loss: 6.038264274597168
Epoch 23:  13%|█▎        | 40/300 [00:17<01:54,  2.27it/s]Epoch 23:  14%|█▎        | 41/300 [00:18<01:49,  2.36it/s]Epoch 23:  14%|█▍        | 42/300 [00:18<01:52,  2.30it/s]Epoch 23:  14%|█▍        | 43/300 [00:18<01:54,  2.25it/s]Epoch 23:  15%|█▍        | 44/300 [00:19<01:47,  2.38it/s]Epoch 23:  15%|█▌        | 45/300 [00:19<01:48,  2.34it/s]Epoch 23:  15%|█▌        | 46/300 [00:20<01:43,  2.46it/s]Epoch 23:  16%|█▌        | 47/300 [00:20<01:53,  2.23it/s]Epoch 23:  16%|█▌        | 48/300 [00:21<01:52,  2.23it/s]Epoch 23:  16%|█▋        | 49/300 [00:21<01:45,  2.38it/s]Epoch 23:  17%|█▋        | 50/300 [00:21<01:40,  2.48it/s]Epoch 23:  17%|█▋        | 51/300 [00:22<01:51,  2.24it/s]Epoch 23:  17%|█▋        | 52/300 [00:22<01:43,  2.39it/s]Epoch 23:  18%|█▊        | 53/300 [00:23<01:38,  2.51it/s]Epoch 23:  18%|█▊        | 54/300 [00:23<01:40,  2.44it/s]Epoch 23:  18%|█▊        | 55/300 [00:23<01:44,  2.34it/s]Epoch 23:  19%|█▊        | 56/300 [00:24<01:58,  2.06it/s]Epoch 23:  19%|█▉        | 57/300 [00:25<02:01,  2.01it/s]Epoch 23:  19%|█▉        | 58/300 [00:25<01:50,  2.19it/s]Epoch 23:  20%|█▉        | 59/300 [00:25<01:41,  2.36it/s]06/19/2022 15:18:07 - INFO - __main__ - global step: 3480; train loss: 6.1982645988464355; dev loss: 5.9122185707092285
Epoch 23:  20%|██        | 60/300 [00:26<01:44,  2.30it/s]Epoch 23:  20%|██        | 61/300 [00:26<01:40,  2.37it/s]Epoch 23:  21%|██        | 62/300 [00:27<01:40,  2.38it/s]Epoch 23:  21%|██        | 63/300 [00:27<01:36,  2.46it/s]Epoch 23:  21%|██▏       | 64/300 [00:28<01:47,  2.19it/s]Epoch 23:  22%|██▏       | 65/300 [00:28<01:46,  2.20it/s]Epoch 23:  22%|██▏       | 66/300 [00:28<01:43,  2.26it/s]Epoch 23:  22%|██▏       | 67/300 [00:29<01:35,  2.44it/s]Epoch 23:  23%|██▎       | 68/300 [00:29<01:36,  2.41it/s]Epoch 23:  23%|██▎       | 69/300 [00:30<01:30,  2.56it/s]Epoch 23:  23%|██▎       | 70/300 [00:30<01:25,  2.68it/s]Epoch 23:  24%|██▎       | 71/300 [00:30<01:28,  2.59it/s]Epoch 23:  24%|██▍       | 72/300 [00:31<01:43,  2.20it/s]Epoch 23:  24%|██▍       | 73/300 [00:31<01:37,  2.34it/s]Epoch 23:  25%|██▍       | 74/300 [00:32<01:38,  2.29it/s]Epoch 23:  25%|██▌       | 75/300 [00:32<01:31,  2.46it/s]Epoch 23:  25%|██▌       | 76/300 [00:32<01:32,  2.43it/s]Epoch 23:  26%|██▌       | 77/300 [00:33<01:26,  2.58it/s]Epoch 23:  26%|██▌       | 78/300 [00:33<01:25,  2.61it/s]Epoch 23:  26%|██▋       | 79/300 [00:34<01:21,  2.70it/s]06/19/2022 15:18:15 - INFO - __main__ - global step: 3490; train loss: 5.581448078155518; dev loss: 5.387670040130615
Epoch 23:  27%|██▋       | 80/300 [00:34<01:26,  2.55it/s]Epoch 23:  27%|██▋       | 81/300 [00:34<01:21,  2.68it/s]Epoch 23:  27%|██▋       | 82/300 [00:35<01:18,  2.79it/s]Epoch 23:  28%|██▊       | 83/300 [00:35<01:15,  2.87it/s]Epoch 23:  28%|██▊       | 84/300 [00:35<01:19,  2.72it/s]Epoch 23:  28%|██▊       | 85/300 [00:36<01:26,  2.49it/s]Epoch 23:  29%|██▊       | 86/300 [00:36<01:22,  2.61it/s]Epoch 23:  29%|██▉       | 87/300 [00:37<01:23,  2.55it/s]Epoch 23:  29%|██▉       | 88/300 [00:37<01:19,  2.67it/s]Epoch 23:  30%|██▉       | 89/300 [00:37<01:22,  2.56it/s]Epoch 23:  30%|███       | 90/300 [00:38<01:18,  2.69it/s]Epoch 23:  30%|███       | 91/300 [00:38<01:17,  2.70it/s]Epoch 23:  31%|███       | 92/300 [00:38<01:16,  2.70it/s]Epoch 23:  31%|███       | 93/300 [00:39<01:28,  2.33it/s]Epoch 23:  31%|███▏      | 94/300 [00:39<01:29,  2.29it/s]Epoch 23:  32%|███▏      | 95/300 [00:40<01:31,  2.23it/s]Epoch 23:  32%|███▏      | 96/300 [00:40<01:28,  2.31it/s]Epoch 23:  32%|███▏      | 97/300 [00:41<01:36,  2.11it/s]Epoch 23:  33%|███▎      | 98/300 [00:41<01:34,  2.15it/s]Epoch 23:  33%|███▎      | 99/300 [00:42<01:32,  2.17it/s]06/19/2022 15:18:23 - INFO - __main__ - global step: 3500; train loss: 5.751253604888916; dev loss: 6.005888938903809
Epoch 23:  33%|███▎      | 100/300 [00:42<01:31,  2.18it/s]Epoch 23:  34%|███▎      | 101/300 [00:43<01:38,  2.02it/s]Epoch 23:  34%|███▍      | 102/300 [00:43<01:35,  2.08it/s]Epoch 23:  34%|███▍      | 103/300 [00:44<01:32,  2.12it/s]Epoch 23:  35%|███▍      | 104/300 [00:44<01:31,  2.15it/s]Epoch 23:  35%|███▌      | 105/300 [00:45<01:30,  2.15it/s]Epoch 23:  35%|███▌      | 106/300 [00:45<01:22,  2.34it/s]Epoch 23:  36%|███▌      | 107/300 [00:45<01:17,  2.50it/s]Epoch 23:  36%|███▌      | 108/300 [00:46<01:12,  2.63it/s]Epoch 23:  36%|███▋      | 109/300 [00:46<01:09,  2.75it/s]Epoch 23:  37%|███▋      | 110/300 [00:46<01:13,  2.59it/s]Epoch 23:  37%|███▋      | 111/300 [00:47<01:10,  2.70it/s]Epoch 23:  37%|███▋      | 112/300 [00:47<01:07,  2.78it/s]Epoch 23:  38%|███▊      | 113/300 [00:47<01:06,  2.80it/s]Epoch 23:  38%|███▊      | 114/300 [00:48<01:15,  2.48it/s]Epoch 23:  38%|███▊      | 115/300 [00:48<01:14,  2.47it/s]Epoch 23:  39%|███▊      | 116/300 [00:49<01:20,  2.28it/s]Epoch 23:  39%|███▉      | 117/300 [00:49<01:16,  2.39it/s]Epoch 23:  39%|███▉      | 118/300 [00:50<01:18,  2.31it/s]Epoch 23:  40%|███▉      | 119/300 [00:50<01:18,  2.30it/s]06/19/2022 15:18:32 - INFO - __main__ - global step: 3510; train loss: 6.3786492347717285; dev loss: 6.034566879272461
Epoch 23:  40%|████      | 120/300 [00:51<01:17,  2.33it/s]Epoch 23:  40%|████      | 121/300 [00:51<01:15,  2.36it/s]Epoch 23:  41%|████      | 122/300 [00:52<01:26,  2.05it/s]Epoch 23:  41%|████      | 123/300 [00:52<01:24,  2.09it/s]Epoch 23:  41%|████▏     | 124/300 [00:52<01:17,  2.28it/s]Epoch 23:  42%|████▏     | 125/300 [00:53<01:11,  2.45it/s]Epoch 23:  42%|████▏     | 126/300 [00:53<01:16,  2.29it/s]Epoch 23:  42%|████▏     | 127/300 [00:54<01:13,  2.36it/s]Epoch 23:  43%|████▎     | 128/300 [00:54<01:08,  2.51it/s]Epoch 23:  43%|████▎     | 129/300 [00:54<01:07,  2.52it/s]Epoch 23:  43%|████▎     | 130/300 [00:55<01:11,  2.37it/s]Epoch 23:  44%|████▎     | 131/300 [00:55<01:08,  2.46it/s]Epoch 23:  44%|████▍     | 132/300 [00:56<01:04,  2.59it/s]Epoch 23:  44%|████▍     | 133/300 [00:56<01:03,  2.64it/s]Epoch 23:  45%|████▍     | 134/300 [00:56<01:09,  2.40it/s]Epoch 23:  45%|████▌     | 135/300 [00:57<01:09,  2.38it/s]Epoch 23:  45%|████▌     | 136/300 [00:57<01:12,  2.26it/s]Epoch 23:  46%|████▌     | 137/300 [00:58<01:12,  2.25it/s]Epoch 23:  46%|████▌     | 138/300 [00:58<01:12,  2.24it/s]Epoch 23:  46%|████▋     | 139/300 [00:59<01:11,  2.27it/s]06/19/2022 15:18:40 - INFO - __main__ - global step: 3520; train loss: 6.158824443817139; dev loss: 5.9123969078063965
Epoch 23:  47%|████▋     | 140/300 [00:59<01:05,  2.44it/s]Epoch 23:  47%|████▋     | 141/300 [00:59<01:01,  2.58it/s]Epoch 23:  47%|████▋     | 142/300 [01:00<00:58,  2.69it/s]Epoch 23:  48%|████▊     | 143/300 [01:00<01:01,  2.57it/s]Epoch 23:  48%|████▊     | 144/300 [01:00<00:58,  2.68it/s]Epoch 23:  48%|████▊     | 145/300 [01:01<00:55,  2.77it/s]Epoch 23:  49%|████▊     | 146/300 [01:01<00:54,  2.84it/s]Epoch 23:  49%|████▉     | 147/300 [01:02<00:57,  2.67it/s]Epoch 23:  49%|████▉     | 148/300 [01:02<00:55,  2.76it/s]Epoch 23:  50%|████▉     | 149/300 [01:02<00:53,  2.84it/s]Epoch 23:  50%|█████     | 150/300 [01:03<00:51,  2.89it/s]Epoch 23:  50%|█████     | 151/300 [01:03<00:55,  2.69it/s]Epoch 23:  51%|█████     | 152/300 [01:03<00:56,  2.63it/s]Epoch 23:  51%|█████     | 153/300 [01:04<00:56,  2.61it/s]Epoch 23:  51%|█████▏    | 154/300 [01:04<00:54,  2.68it/s]Epoch 23:  52%|█████▏    | 155/300 [01:05<00:58,  2.48it/s]Epoch 23:  52%|█████▏    | 156/300 [01:05<00:57,  2.49it/s]Epoch 23:  52%|█████▏    | 157/300 [01:05<00:56,  2.55it/s]Epoch 23:  53%|█████▎    | 158/300 [01:06<00:54,  2.61it/s]Epoch 23:  53%|█████▎    | 159/300 [01:06<00:57,  2.44it/s]06/19/2022 15:18:48 - INFO - __main__ - global step: 3530; train loss: 5.779146194458008; dev loss: 5.728997230529785
Epoch 23:  53%|█████▎    | 160/300 [01:07<00:58,  2.39it/s]Epoch 23:  54%|█████▎    | 161/300 [01:07<01:00,  2.31it/s]Epoch 23:  54%|█████▍    | 162/300 [01:08<01:02,  2.22it/s]Epoch 23:  54%|█████▍    | 163/300 [01:08<01:02,  2.20it/s]Epoch 23:  55%|█████▍    | 164/300 [01:09<01:10,  1.94it/s]Epoch 23:  55%|█████▌    | 165/300 [01:09<01:06,  2.02it/s]Epoch 23:  55%|█████▌    | 166/300 [01:10<01:07,  1.98it/s]Epoch 23:  56%|█████▌    | 167/300 [01:10<01:06,  1.99it/s]Epoch 23:  56%|█████▌    | 168/300 [01:11<01:12,  1.83it/s]Epoch 23:  56%|█████▋    | 169/300 [01:11<01:09,  1.90it/s]Epoch 23:  57%|█████▋    | 170/300 [01:12<01:08,  1.91it/s]Epoch 23:  57%|█████▋    | 171/300 [01:12<01:03,  2.04it/s]Epoch 23:  57%|█████▋    | 172/300 [01:13<01:06,  1.91it/s]Epoch 23:  58%|█████▊    | 173/300 [01:13<01:03,  2.01it/s]Epoch 23:  58%|█████▊    | 174/300 [01:14<01:02,  2.02it/s]Epoch 23:  58%|█████▊    | 175/300 [01:14<01:01,  2.02it/s]Epoch 23:  59%|█████▊    | 176/300 [01:15<01:04,  1.91it/s]Epoch 23:  59%|█████▉    | 177/300 [01:15<00:59,  2.08it/s]Epoch 23:  59%|█████▉    | 178/300 [01:16<00:57,  2.14it/s]Epoch 23:  60%|█████▉    | 179/300 [01:16<00:55,  2.20it/s]06/19/2022 15:18:58 - INFO - __main__ - global step: 3540; train loss: 5.463200569152832; dev loss: 5.469689846038818
Epoch 23:  60%|██████    | 180/300 [01:17<00:58,  2.04it/s]Epoch 23:  60%|██████    | 181/300 [01:17<00:56,  2.11it/s]Epoch 23:  61%|██████    | 182/300 [01:18<00:54,  2.17it/s]Epoch 23:  61%|██████    | 183/300 [01:18<00:50,  2.30it/s]Epoch 23:  61%|██████▏   | 184/300 [01:18<00:55,  2.10it/s]Epoch 23:  62%|██████▏   | 185/300 [01:19<00:52,  2.20it/s]Epoch 23:  62%|██████▏   | 186/300 [01:19<00:50,  2.25it/s]Epoch 23:  62%|██████▏   | 187/300 [01:20<00:50,  2.22it/s]Epoch 23:  63%|██████▎   | 188/300 [01:20<00:57,  1.96it/s]Epoch 23:  63%|██████▎   | 189/300 [01:21<00:54,  2.04it/s]Epoch 23:  63%|██████▎   | 190/300 [01:21<00:49,  2.20it/s]Epoch 23:  64%|██████▎   | 191/300 [01:22<00:45,  2.40it/s]Epoch 23:  64%|██████▍   | 192/300 [01:22<00:43,  2.46it/s]Epoch 23:  64%|██████▍   | 193/300 [01:22<00:46,  2.30it/s]Epoch 23:  65%|██████▍   | 194/300 [01:23<00:46,  2.29it/s]Epoch 23:  65%|██████▌   | 195/300 [01:23<00:43,  2.42it/s]Epoch 23:  65%|██████▌   | 196/300 [01:24<00:43,  2.39it/s]Epoch 23:  66%|██████▌   | 197/300 [01:24<00:45,  2.28it/s]Epoch 23:  66%|██████▌   | 198/300 [01:25<00:47,  2.16it/s]Epoch 23:  66%|██████▋   | 199/300 [01:25<00:44,  2.27it/s]06/19/2022 15:19:06 - INFO - __main__ - global step: 3550; train loss: 5.4645771980285645; dev loss: 5.461166858673096
Epoch 23:  67%|██████▋   | 200/300 [01:25<00:42,  2.36it/s]Epoch 23:  67%|██████▋   | 201/300 [01:26<00:44,  2.24it/s]Epoch 23:  67%|██████▋   | 202/300 [01:26<00:44,  2.18it/s]Epoch 23:  68%|██████▊   | 203/300 [01:27<00:43,  2.25it/s]Epoch 23:  68%|██████▊   | 204/300 [01:27<00:43,  2.23it/s]Epoch 23:  68%|██████▊   | 205/300 [01:28<00:46,  2.06it/s]Epoch 23:  69%|██████▊   | 206/300 [01:28<00:41,  2.24it/s]Epoch 23:  69%|██████▉   | 207/300 [01:29<00:39,  2.38it/s]Epoch 23:  69%|██████▉   | 208/300 [01:29<00:37,  2.42it/s]Epoch 23:  70%|██████▉   | 209/300 [01:29<00:39,  2.29it/s]Epoch 23:  70%|███████   | 210/300 [01:30<00:38,  2.33it/s]Epoch 23:  70%|███████   | 211/300 [01:30<00:37,  2.35it/s]Epoch 23:  71%|███████   | 212/300 [01:31<00:36,  2.41it/s]Epoch 23:  71%|███████   | 213/300 [01:31<00:38,  2.26it/s]Epoch 23:  71%|███████▏  | 214/300 [01:32<00:36,  2.34it/s]Epoch 23:  72%|███████▏  | 215/300 [01:32<00:34,  2.44it/s]Epoch 23:  72%|███████▏  | 216/300 [01:32<00:34,  2.45it/s]Epoch 23:  72%|███████▏  | 217/300 [01:33<00:33,  2.49it/s]Epoch 23:  73%|███████▎  | 218/300 [01:33<00:35,  2.32it/s]Epoch 23:  73%|███████▎  | 219/300 [01:34<00:34,  2.36it/s]06/19/2022 15:19:15 - INFO - __main__ - global step: 3560; train loss: 5.938752174377441; dev loss: 6.114648342132568
Epoch 23:  73%|███████▎  | 220/300 [01:34<00:34,  2.31it/s]Epoch 23:  74%|███████▎  | 221/300 [01:35<00:37,  2.12it/s]Epoch 23:  74%|███████▍  | 222/300 [01:35<00:42,  1.83it/s]Epoch 23:  74%|███████▍  | 223/300 [01:36<00:38,  1.98it/s]Epoch 23:  75%|███████▍  | 224/300 [01:36<00:35,  2.12it/s]Epoch 23:  75%|███████▌  | 225/300 [01:37<00:33,  2.24it/s]Epoch 23:  75%|███████▌  | 226/300 [01:37<00:33,  2.18it/s]Epoch 23:  76%|███████▌  | 227/300 [01:37<00:32,  2.22it/s]Epoch 23:  76%|███████▌  | 228/300 [01:38<00:32,  2.18it/s]Epoch 23:  76%|███████▋  | 229/300 [01:39<00:34,  2.05it/s]Epoch 23:  77%|███████▋  | 230/300 [01:39<00:36,  1.94it/s]Epoch 23:  77%|███████▋  | 231/300 [01:40<00:34,  2.00it/s]Epoch 23:  77%|███████▋  | 232/300 [01:40<00:34,  1.95it/s]Epoch 23:  78%|███████▊  | 233/300 [01:40<00:32,  2.09it/s]Epoch 23:  78%|███████▊  | 234/300 [01:41<00:33,  1.99it/s]Epoch 23:  78%|███████▊  | 235/300 [01:41<00:30,  2.14it/s]Epoch 23:  79%|███████▊  | 236/300 [01:42<00:28,  2.25it/s]Epoch 23:  79%|███████▉  | 237/300 [01:42<00:28,  2.20it/s]Epoch 23:  79%|███████▉  | 238/300 [01:43<00:29,  2.13it/s]Epoch 23:  80%|███████▉  | 239/300 [01:43<00:27,  2.25it/s]06/19/2022 15:19:25 - INFO - __main__ - global step: 3570; train loss: 5.840912818908691; dev loss: 5.742524147033691
Epoch 23:  80%|████████  | 240/300 [01:44<00:25,  2.34it/s]Epoch 23:  80%|████████  | 241/300 [01:44<00:24,  2.40it/s]Epoch 23:  81%|████████  | 242/300 [01:45<00:26,  2.19it/s]Epoch 23:  81%|████████  | 243/300 [01:45<00:26,  2.15it/s]Epoch 23:  81%|████████▏ | 244/300 [01:45<00:26,  2.14it/s]Epoch 23:  82%|████████▏ | 245/300 [01:46<00:26,  2.09it/s]Epoch 23:  82%|████████▏ | 246/300 [01:46<00:24,  2.18it/s]Epoch 23:  82%|████████▏ | 247/300 [01:47<00:26,  2.00it/s]Epoch 23:  83%|████████▎ | 248/300 [01:47<00:25,  2.07it/s]Epoch 23:  83%|████████▎ | 249/300 [01:48<00:23,  2.13it/s]Epoch 23:  83%|████████▎ | 250/300 [01:48<00:22,  2.18it/s]Epoch 23:  84%|████████▎ | 251/300 [01:49<00:24,  2.03it/s]Epoch 23:  84%|████████▍ | 252/300 [01:49<00:22,  2.09it/s]Epoch 23:  84%|████████▍ | 253/300 [01:50<00:22,  2.10it/s]Epoch 23:  85%|████████▍ | 254/300 [01:50<00:20,  2.25it/s]Epoch 23:  85%|████████▌ | 255/300 [01:51<00:19,  2.27it/s]Epoch 23:  85%|████████▌ | 256/300 [01:51<00:17,  2.45it/s]Epoch 23:  86%|████████▌ | 257/300 [01:51<00:16,  2.58it/s]Epoch 23:  86%|████████▌ | 258/300 [01:52<00:16,  2.55it/s]Epoch 23:  86%|████████▋ | 259/300 [01:52<00:18,  2.21it/s]06/19/2022 15:19:34 - INFO - __main__ - global step: 3580; train loss: 5.335688591003418; dev loss: 5.496443748474121
Epoch 23:  87%|████████▋ | 260/300 [01:53<00:17,  2.33it/s]Epoch 23:  87%|████████▋ | 261/300 [01:53<00:17,  2.23it/s]Epoch 23:  87%|████████▋ | 262/300 [01:54<00:16,  2.34it/s]Epoch 23:  88%|████████▊ | 263/300 [01:54<00:17,  2.14it/s]Epoch 23:  88%|████████▊ | 264/300 [01:55<00:17,  2.10it/s]Epoch 23:  88%|████████▊ | 265/300 [01:55<00:17,  2.03it/s]Epoch 23:  89%|████████▊ | 266/300 [01:56<00:16,  2.10it/s]Epoch 23:  89%|████████▉ | 267/300 [01:56<00:17,  1.90it/s]Epoch 23:  89%|████████▉ | 268/300 [01:57<00:16,  1.94it/s]Epoch 23:  90%|████████▉ | 269/300 [01:57<00:15,  2.02it/s]Epoch 23:  90%|█████████ | 270/300 [01:58<00:14,  2.04it/s]Epoch 23:  90%|█████████ | 271/300 [01:58<00:13,  2.20it/s]Epoch 23:  91%|█████████ | 272/300 [01:59<00:13,  2.04it/s]Epoch 23:  91%|█████████ | 273/300 [01:59<00:12,  2.10it/s]Epoch 23:  91%|█████████▏| 274/300 [01:59<00:11,  2.21it/s]Epoch 23:  92%|█████████▏| 275/300 [02:00<00:10,  2.40it/s]Epoch 23:  92%|█████████▏| 276/300 [02:00<00:10,  2.37it/s]Epoch 23:  92%|█████████▏| 277/300 [02:00<00:09,  2.50it/s]Epoch 23:  93%|█████████▎| 278/300 [02:01<00:08,  2.64it/s]Epoch 23:  93%|█████████▎| 279/300 [02:01<00:07,  2.69it/s]06/19/2022 15:19:43 - INFO - __main__ - global step: 3590; train loss: 5.506008625030518; dev loss: 5.267500400543213
Epoch 23:  93%|█████████▎| 280/300 [02:02<00:08,  2.43it/s]Epoch 23:  94%|█████████▎| 281/300 [02:02<00:07,  2.39it/s]Epoch 23:  94%|█████████▍| 282/300 [02:02<00:07,  2.50it/s]Epoch 23:  94%|█████████▍| 283/300 [02:03<00:06,  2.45it/s]Epoch 23:  95%|█████████▍| 284/300 [02:04<00:07,  2.13it/s]Epoch 23:  95%|█████████▌| 285/300 [02:04<00:07,  2.00it/s]Epoch 23:  95%|█████████▌| 286/300 [02:04<00:06,  2.13it/s]Epoch 23:  96%|█████████▌| 287/300 [02:05<00:05,  2.24it/s]Epoch 23:  96%|█████████▌| 288/300 [02:05<00:05,  2.18it/s]Epoch 23:  96%|█████████▋| 289/300 [02:06<00:04,  2.25it/s]Epoch 23:  97%|█████████▋| 290/300 [02:06<00:04,  2.27it/s]Epoch 23:  97%|█████████▋| 291/300 [02:07<00:03,  2.40it/s]Epoch 23:  97%|█████████▋| 292/300 [02:07<00:03,  2.31it/s]Epoch 23:  98%|█████████▊| 293/300 [02:07<00:02,  2.43it/s]Epoch 23:  98%|█████████▊| 294/300 [02:08<00:02,  2.52it/s]Epoch 23:  98%|█████████▊| 295/300 [02:08<00:01,  2.61it/s]Epoch 23:  99%|█████████▊| 296/300 [02:09<00:01,  2.45it/s]Epoch 23:  99%|█████████▉| 297/300 [02:09<00:01,  2.55it/s]Epoch 23:  99%|█████████▉| 298/300 [02:09<00:00,  2.34it/s]Epoch 23: 100%|█████████▉| 299/300 [02:10<00:00,  2.43it/s]06/19/2022 15:19:51 - INFO - __main__ - global step: 3600; train loss: 5.524237155914307; dev loss: 5.329540729522705
Epoch 23: 100%|██████████| 300/300 [02:10<00:00,  2.31it/s]Epoch 23: 100%|██████████| 300/300 [02:10<00:00,  2.29it/s]
Epoch 24:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 24:   0%|          | 1/300 [00:00<02:56,  1.70it/s]Epoch 24:   1%|          | 2/300 [00:01<02:25,  2.04it/s]Epoch 24:   1%|          | 3/300 [00:01<02:14,  2.21it/s]Epoch 24:   1%|▏         | 4/300 [00:01<02:08,  2.30it/s]Epoch 24:   2%|▏         | 5/300 [00:02<02:15,  2.18it/s]Epoch 24:   2%|▏         | 6/300 [00:02<02:11,  2.23it/s]Epoch 24:   2%|▏         | 7/300 [00:03<02:02,  2.38it/s]Epoch 24:   3%|▎         | 8/300 [00:03<01:57,  2.49it/s]Epoch 24:   3%|▎         | 9/300 [00:04<02:18,  2.10it/s]Epoch 24:   3%|▎         | 10/300 [00:04<02:19,  2.08it/s]Epoch 24:   4%|▎         | 11/300 [00:05<02:18,  2.09it/s]Epoch 24:   4%|▍         | 12/300 [00:05<02:14,  2.13it/s]Epoch 24:   4%|▍         | 13/300 [00:05<02:14,  2.13it/s]Epoch 24:   5%|▍         | 14/300 [00:06<02:05,  2.29it/s]Epoch 24:   5%|▌         | 15/300 [00:06<01:58,  2.41it/s]Epoch 24:   5%|▌         | 16/300 [00:07<01:54,  2.47it/s]Epoch 24:   6%|▌         | 17/300 [00:07<02:06,  2.24it/s]Epoch 24:   6%|▌         | 18/300 [00:08<01:58,  2.37it/s]Epoch 24:   6%|▋         | 19/300 [00:08<01:59,  2.34it/s]06/19/2022 15:20:00 - INFO - __main__ - global step: 3610; train loss: 5.257607460021973; dev loss: 5.331382751464844
Epoch 24:   7%|▋         | 20/300 [00:08<02:08,  2.17it/s]Epoch 24:   7%|▋         | 21/300 [00:09<02:15,  2.06it/s]Epoch 24:   7%|▋         | 22/300 [00:09<02:13,  2.08it/s]Epoch 24:   8%|▊         | 23/300 [00:10<02:05,  2.20it/s]Epoch 24:   8%|▊         | 24/300 [00:10<01:58,  2.33it/s]Epoch 24:   8%|▊         | 25/300 [00:11<01:52,  2.44it/s]Epoch 24:   9%|▊         | 26/300 [00:11<02:00,  2.28it/s]Epoch 24:   9%|▉         | 27/300 [00:12<02:01,  2.25it/s]Epoch 24:   9%|▉         | 28/300 [00:12<02:03,  2.20it/s]Epoch 24:  10%|▉         | 29/300 [00:13<02:04,  2.19it/s]Epoch 24:  10%|█         | 30/300 [00:13<02:15,  1.99it/s]Epoch 24:  10%|█         | 31/300 [00:14<02:04,  2.17it/s]Epoch 24:  11%|█         | 32/300 [00:14<01:56,  2.31it/s]Epoch 24:  11%|█         | 33/300 [00:14<01:50,  2.42it/s]Epoch 24:  11%|█▏        | 34/300 [00:15<01:59,  2.23it/s]Epoch 24:  12%|█▏        | 35/300 [00:15<02:00,  2.19it/s]Epoch 24:  12%|█▏        | 36/300 [00:16<02:04,  2.11it/s]Epoch 24:  12%|█▏        | 37/300 [00:16<02:13,  1.97it/s]Epoch 24:  13%|█▎        | 38/300 [00:17<02:20,  1.87it/s]Epoch 24:  13%|█▎        | 39/300 [00:17<02:18,  1.89it/s]06/19/2022 15:20:10 - INFO - __main__ - global step: 3620; train loss: 5.469237804412842; dev loss: 5.505556583404541
Epoch 24:  13%|█▎        | 40/300 [00:18<02:16,  1.90it/s]Epoch 24:  14%|█▎        | 41/300 [00:18<02:11,  1.97it/s]Epoch 24:  14%|█▍        | 42/300 [00:19<02:25,  1.77it/s]Epoch 24:  14%|█▍        | 43/300 [00:20<02:15,  1.90it/s]Epoch 24:  15%|█▍        | 44/300 [00:20<02:09,  1.97it/s]Epoch 24:  15%|█▌        | 45/300 [00:20<02:04,  2.04it/s]Epoch 24:  15%|█▌        | 46/300 [00:21<02:17,  1.85it/s]Epoch 24:  16%|█▌        | 47/300 [00:22<02:07,  1.98it/s]Epoch 24:  16%|█▌        | 48/300 [00:22<02:02,  2.06it/s]Epoch 24:  16%|█▋        | 49/300 [00:22<02:00,  2.09it/s]Epoch 24:  17%|█▋        | 50/300 [00:23<02:09,  1.92it/s]Epoch 24:  17%|█▋        | 51/300 [00:23<02:00,  2.07it/s]Epoch 24:  17%|█▋        | 52/300 [00:24<02:02,  2.03it/s]Epoch 24:  18%|█▊        | 53/300 [00:24<01:54,  2.16it/s]Epoch 24:  18%|█▊        | 54/300 [00:25<01:53,  2.17it/s]Epoch 24:  18%|█▊        | 55/300 [00:25<02:04,  1.97it/s]Epoch 24:  19%|█▊        | 56/300 [00:26<01:59,  2.05it/s]Epoch 24:  19%|█▉        | 57/300 [00:26<01:50,  2.20it/s]Epoch 24:  19%|█▉        | 58/300 [00:27<01:42,  2.36it/s]Epoch 24:  20%|█▉        | 59/300 [00:27<01:43,  2.33it/s]06/19/2022 15:20:19 - INFO - __main__ - global step: 3630; train loss: 5.025548458099365; dev loss: 5.120251655578613
Epoch 24:  20%|██        | 60/300 [00:27<01:37,  2.47it/s]Epoch 24:  20%|██        | 61/300 [00:28<01:33,  2.56it/s]Epoch 24:  21%|██        | 62/300 [00:28<01:36,  2.47it/s]Epoch 24:  21%|██        | 63/300 [00:29<01:48,  2.19it/s]Epoch 24:  21%|██▏       | 64/300 [00:29<01:51,  2.12it/s]Epoch 24:  22%|██▏       | 65/300 [00:30<01:52,  2.09it/s]Epoch 24:  22%|██▏       | 66/300 [00:30<01:50,  2.12it/s]Epoch 24:  22%|██▏       | 67/300 [00:31<01:57,  1.99it/s]Epoch 24:  23%|██▎       | 68/300 [00:31<01:53,  2.05it/s]Epoch 24:  23%|██▎       | 69/300 [00:32<01:46,  2.17it/s]Epoch 24:  23%|██▎       | 70/300 [00:32<01:37,  2.37it/s]Epoch 24:  24%|██▎       | 71/300 [00:32<01:37,  2.35it/s]Epoch 24:  24%|██▍       | 72/300 [00:33<01:30,  2.52it/s]Epoch 24:  24%|██▍       | 73/300 [00:33<01:25,  2.64it/s]Epoch 24:  25%|██▍       | 74/300 [00:34<01:31,  2.47it/s]Epoch 24:  25%|██▌       | 75/300 [00:34<01:45,  2.12it/s]Epoch 24:  25%|██▌       | 76/300 [00:35<01:44,  2.15it/s]Epoch 24:  26%|██▌       | 77/300 [00:35<01:45,  2.12it/s]Epoch 24:  26%|██▌       | 78/300 [00:36<01:47,  2.06it/s]Epoch 24:  26%|██▋       | 79/300 [00:36<01:48,  2.04it/s]06/19/2022 15:20:29 - INFO - __main__ - global step: 3640; train loss: 5.8186750411987305; dev loss: 5.854016304016113
Epoch 24:  27%|██▋       | 80/300 [00:37<01:52,  1.95it/s]Epoch 24:  27%|██▋       | 81/300 [00:37<01:40,  2.18it/s]Epoch 24:  27%|██▋       | 82/300 [00:37<01:33,  2.34it/s]Epoch 24:  28%|██▊       | 83/300 [00:38<01:31,  2.37it/s]Epoch 24:  28%|██▊       | 84/300 [00:38<01:45,  2.06it/s]Epoch 24:  28%|██▊       | 85/300 [00:39<01:43,  2.07it/s]Epoch 24:  29%|██▊       | 86/300 [00:39<01:42,  2.10it/s]Epoch 24:  29%|██▉       | 87/300 [00:40<01:41,  2.11it/s]Epoch 24:  29%|██▉       | 88/300 [00:41<01:53,  1.86it/s]Epoch 24:  30%|██▉       | 89/300 [00:41<01:51,  1.90it/s]Epoch 24:  30%|███       | 90/300 [00:42<01:51,  1.89it/s]Epoch 24:  30%|███       | 91/300 [00:42<01:50,  1.90it/s]Epoch 24:  31%|███       | 92/300 [00:43<01:57,  1.77it/s]Epoch 24:  31%|███       | 93/300 [00:43<01:47,  1.93it/s]Epoch 24:  31%|███▏      | 94/300 [00:44<01:38,  2.09it/s]Epoch 24:  32%|███▏      | 95/300 [00:44<01:31,  2.23it/s]Epoch 24:  32%|███▏      | 96/300 [00:45<01:41,  2.01it/s]Epoch 24:  32%|███▏      | 97/300 [00:45<01:37,  2.09it/s]Epoch 24:  33%|███▎      | 98/300 [00:45<01:35,  2.12it/s]Epoch 24:  33%|███▎      | 99/300 [00:46<01:32,  2.18it/s]06/19/2022 15:20:38 - INFO - __main__ - global step: 3650; train loss: 5.242447853088379; dev loss: 5.19248104095459
Epoch 24:  33%|███▎      | 100/300 [00:47<01:42,  1.95it/s]Epoch 24:  34%|███▎      | 101/300 [00:47<01:34,  2.10it/s]Epoch 24:  34%|███▍      | 102/300 [00:47<01:34,  2.09it/s]Epoch 24:  34%|███▍      | 103/300 [00:48<01:35,  2.07it/s]Epoch 24:  35%|███▍      | 104/300 [00:48<01:34,  2.08it/s]Epoch 24:  35%|███▌      | 105/300 [00:49<01:29,  2.18it/s]Epoch 24:  35%|███▌      | 106/300 [00:49<01:25,  2.28it/s]Epoch 24:  36%|███▌      | 107/300 [00:50<01:22,  2.34it/s]Epoch 24:  36%|███▌      | 108/300 [00:50<01:19,  2.43it/s]Epoch 24:  36%|███▋      | 109/300 [00:50<01:22,  2.33it/s]Epoch 24:  37%|███▋      | 110/300 [00:51<01:17,  2.45it/s]Epoch 24:  37%|███▋      | 111/300 [00:51<01:13,  2.56it/s]Epoch 24:  37%|███▋      | 112/300 [00:51<01:11,  2.62it/s]Epoch 24:  38%|███▊      | 113/300 [00:52<01:20,  2.33it/s]Epoch 24:  38%|███▊      | 114/300 [00:52<01:18,  2.38it/s]Epoch 24:  38%|███▊      | 115/300 [00:53<01:15,  2.46it/s]Epoch 24:  39%|███▊      | 116/300 [00:53<01:16,  2.39it/s]Epoch 24:  39%|███▉      | 117/300 [00:54<01:22,  2.23it/s]Epoch 24:  39%|███▉      | 118/300 [00:54<01:21,  2.23it/s]Epoch 24:  40%|███▉      | 119/300 [00:55<01:19,  2.28it/s]06/19/2022 15:20:47 - INFO - __main__ - global step: 3660; train loss: 5.426901817321777; dev loss: 5.206485748291016
Epoch 24:  40%|████      | 120/300 [00:55<01:18,  2.28it/s]Epoch 24:  40%|████      | 121/300 [00:56<01:19,  2.25it/s]Epoch 24:  41%|████      | 122/300 [00:56<01:16,  2.33it/s]Epoch 24:  41%|████      | 123/300 [00:56<01:14,  2.38it/s]Epoch 24:  41%|████▏     | 124/300 [00:57<01:12,  2.44it/s]Epoch 24:  42%|████▏     | 125/300 [00:57<01:18,  2.22it/s]Epoch 24:  42%|████▏     | 126/300 [00:58<01:13,  2.36it/s]Epoch 24:  42%|████▏     | 127/300 [00:58<01:11,  2.42it/s]Epoch 24:  43%|████▎     | 128/300 [00:58<01:12,  2.37it/s]Epoch 24:  43%|████▎     | 129/300 [00:59<01:14,  2.28it/s]Epoch 24:  43%|████▎     | 130/300 [00:59<01:10,  2.42it/s]Epoch 24:  44%|████▎     | 131/300 [01:00<01:11,  2.38it/s]Epoch 24:  44%|████▍     | 132/300 [01:00<01:09,  2.40it/s]Epoch 24:  44%|████▍     | 133/300 [01:00<01:06,  2.50it/s]Epoch 24:  45%|████▍     | 134/300 [01:01<01:10,  2.35it/s]Epoch 24:  45%|████▌     | 135/300 [01:01<01:07,  2.44it/s]Epoch 24:  45%|████▌     | 136/300 [01:02<01:07,  2.43it/s]Epoch 24:  46%|████▌     | 137/300 [01:02<01:10,  2.30it/s]Epoch 24:  46%|████▌     | 138/300 [01:03<01:20,  2.02it/s]Epoch 24:  46%|████▋     | 139/300 [01:03<01:16,  2.10it/s]06/19/2022 15:20:56 - INFO - __main__ - global step: 3670; train loss: 5.268761157989502; dev loss: 5.237525939941406
Epoch 24:  47%|████▋     | 140/300 [01:04<01:10,  2.26it/s]Epoch 24:  47%|████▋     | 141/300 [01:04<01:06,  2.39it/s]Epoch 24:  47%|████▋     | 142/300 [01:04<01:07,  2.33it/s]Epoch 24:  48%|████▊     | 143/300 [01:05<01:08,  2.30it/s]Epoch 24:  48%|████▊     | 144/300 [01:05<01:05,  2.38it/s]Epoch 24:  48%|████▊     | 145/300 [01:06<01:01,  2.50it/s]Epoch 24:  49%|████▊     | 146/300 [01:06<01:07,  2.28it/s]Epoch 24:  49%|████▉     | 147/300 [01:07<01:07,  2.27it/s]Epoch 24:  49%|████▉     | 148/300 [01:07<01:06,  2.27it/s]Epoch 24:  50%|████▉     | 149/300 [01:08<01:08,  2.22it/s]Epoch 24:  50%|█████     | 150/300 [01:08<01:14,  2.01it/s]Epoch 24:  50%|█████     | 151/300 [01:09<01:15,  1.98it/s]Epoch 24:  51%|█████     | 152/300 [01:09<01:08,  2.15it/s]Epoch 24:  51%|█████     | 153/300 [01:10<01:08,  2.13it/s]Epoch 24:  51%|█████▏    | 154/300 [01:10<01:10,  2.09it/s]Epoch 24:  52%|█████▏    | 155/300 [01:10<01:08,  2.11it/s]Epoch 24:  52%|█████▏    | 156/300 [01:11<01:07,  2.14it/s]Epoch 24:  52%|█████▏    | 157/300 [01:11<01:05,  2.18it/s]Epoch 24:  53%|█████▎    | 158/300 [01:12<01:12,  1.95it/s]Epoch 24:  53%|█████▎    | 159/300 [01:12<01:06,  2.13it/s]06/19/2022 15:21:05 - INFO - __main__ - global step: 3680; train loss: 5.379691123962402; dev loss: 5.502819538116455
Epoch 24:  53%|█████▎    | 160/300 [01:13<01:04,  2.15it/s]Epoch 24:  54%|█████▎    | 161/300 [01:13<01:05,  2.12it/s]Epoch 24:  54%|█████▍    | 162/300 [01:14<01:00,  2.28it/s]Epoch 24:  54%|█████▍    | 163/300 [01:14<01:04,  2.12it/s]Epoch 24:  55%|█████▍    | 164/300 [01:15<01:05,  2.08it/s]Epoch 24:  55%|█████▌    | 165/300 [01:15<01:01,  2.21it/s]Epoch 24:  55%|█████▌    | 166/300 [01:16<00:59,  2.26it/s]Epoch 24:  56%|█████▌    | 167/300 [01:16<01:02,  2.12it/s]Epoch 24:  56%|█████▌    | 168/300 [01:17<01:02,  2.10it/s]Epoch 24:  56%|█████▋    | 169/300 [01:17<00:59,  2.21it/s]Epoch 24:  57%|█████▋    | 170/300 [01:17<00:54,  2.37it/s]Epoch 24:  57%|█████▋    | 171/300 [01:18<00:55,  2.34it/s]Epoch 24:  57%|█████▋    | 172/300 [01:18<00:51,  2.50it/s]Epoch 24:  58%|█████▊    | 173/300 [01:18<00:48,  2.62it/s]Epoch 24:  58%|█████▊    | 174/300 [01:19<00:50,  2.51it/s]Epoch 24:  58%|█████▊    | 175/300 [01:19<00:57,  2.17it/s]Epoch 24:  59%|█████▊    | 176/300 [01:20<00:58,  2.11it/s]Epoch 24:  59%|█████▉    | 177/300 [01:20<00:55,  2.20it/s]Epoch 24:  59%|█████▉    | 178/300 [01:21<00:51,  2.36it/s]Epoch 24:  60%|█████▉    | 179/300 [01:21<00:54,  2.24it/s]06/19/2022 15:21:13 - INFO - __main__ - global step: 3690; train loss: 5.379076957702637; dev loss: 5.312781810760498
Epoch 24:  60%|██████    | 180/300 [01:22<00:49,  2.43it/s]Epoch 24:  60%|██████    | 181/300 [01:22<00:46,  2.58it/s]Epoch 24:  61%|██████    | 182/300 [01:22<00:43,  2.69it/s]Epoch 24:  61%|██████    | 183/300 [01:23<00:45,  2.55it/s]Epoch 24:  61%|██████▏   | 184/300 [01:23<00:43,  2.67it/s]Epoch 24:  62%|██████▏   | 185/300 [01:23<00:42,  2.73it/s]Epoch 24:  62%|██████▏   | 186/300 [01:24<00:45,  2.49it/s]Epoch 24:  62%|██████▏   | 187/300 [01:24<00:48,  2.34it/s]Epoch 24:  63%|██████▎   | 188/300 [01:25<00:51,  2.15it/s]Epoch 24:  63%|██████▎   | 189/300 [01:25<00:50,  2.18it/s]Epoch 24:  63%|██████▎   | 190/300 [01:26<00:51,  2.14it/s]Epoch 24:  64%|██████▎   | 191/300 [01:26<00:51,  2.11it/s]Epoch 24:  64%|██████▍   | 192/300 [01:27<00:55,  1.94it/s]Epoch 24:  64%|██████▍   | 193/300 [01:27<00:53,  2.01it/s]Epoch 24:  65%|██████▍   | 194/300 [01:28<00:53,  1.99it/s]Epoch 24:  65%|██████▌   | 195/300 [01:28<00:51,  2.05it/s]Epoch 24:  65%|██████▌   | 196/300 [01:29<00:53,  1.95it/s]Epoch 24:  66%|██████▌   | 197/300 [01:29<00:50,  2.06it/s]Epoch 24:  66%|██████▌   | 198/300 [01:30<00:48,  2.09it/s]Epoch 24:  66%|██████▋   | 199/300 [01:30<00:48,  2.08it/s]06/19/2022 15:21:23 - INFO - __main__ - global step: 3700; train loss: 5.276683807373047; dev loss: 5.128382205963135
Epoch 24:  67%|██████▋   | 200/300 [01:31<00:50,  1.97it/s]Epoch 24:  67%|██████▋   | 201/300 [01:31<00:48,  2.02it/s]Epoch 24:  67%|██████▋   | 202/300 [01:32<00:46,  2.09it/s]Epoch 24:  68%|██████▊   | 203/300 [01:32<00:45,  2.12it/s]Epoch 24:  68%|██████▊   | 204/300 [01:33<00:47,  2.01it/s]Epoch 24:  68%|██████▊   | 205/300 [01:33<00:45,  2.07it/s]Epoch 24:  69%|██████▊   | 206/300 [01:34<00:44,  2.12it/s]Epoch 24:  69%|██████▉   | 207/300 [01:34<00:43,  2.15it/s]Epoch 24:  69%|██████▉   | 208/300 [01:35<00:46,  1.99it/s]Epoch 24:  70%|██████▉   | 209/300 [01:35<00:44,  2.05it/s]Epoch 24:  70%|███████   | 210/300 [01:36<00:42,  2.11it/s]Epoch 24:  70%|███████   | 211/300 [01:36<00:40,  2.21it/s]Epoch 24:  71%|███████   | 212/300 [01:36<00:39,  2.24it/s]Epoch 24:  71%|███████   | 213/300 [01:37<00:36,  2.41it/s]Epoch 24:  71%|███████▏  | 214/300 [01:37<00:34,  2.48it/s]Epoch 24:  72%|███████▏  | 215/300 [01:38<00:33,  2.54it/s]Epoch 24:  72%|███████▏  | 216/300 [01:38<00:34,  2.44it/s]Epoch 24:  72%|███████▏  | 217/300 [01:39<00:37,  2.19it/s]Epoch 24:  73%|███████▎  | 218/300 [01:39<00:38,  2.14it/s]Epoch 24:  73%|███████▎  | 219/300 [01:39<00:34,  2.33it/s]06/19/2022 15:21:32 - INFO - __main__ - global step: 3710; train loss: 5.67918586730957; dev loss: 5.648161888122559
Epoch 24:  73%|███████▎  | 220/300 [01:40<00:33,  2.41it/s]Epoch 24:  74%|███████▎  | 221/300 [01:40<00:36,  2.18it/s]Epoch 24:  74%|███████▍  | 222/300 [01:41<00:35,  2.20it/s]Epoch 24:  74%|███████▍  | 223/300 [01:41<00:34,  2.24it/s]Epoch 24:  75%|███████▍  | 224/300 [01:42<00:34,  2.23it/s]Epoch 24:  75%|███████▌  | 225/300 [01:42<00:36,  2.06it/s]Epoch 24:  75%|███████▌  | 226/300 [01:43<00:35,  2.11it/s]Epoch 24:  76%|███████▌  | 227/300 [01:43<00:31,  2.29it/s]Epoch 24:  76%|███████▌  | 228/300 [01:43<00:29,  2.46it/s]Epoch 24:  76%|███████▋  | 229/300 [01:44<00:29,  2.42it/s]Epoch 24:  77%|███████▋  | 230/300 [01:44<00:27,  2.56it/s]Epoch 24:  77%|███████▋  | 231/300 [01:44<00:26,  2.59it/s]Epoch 24:  77%|███████▋  | 232/300 [01:45<00:26,  2.54it/s]Epoch 24:  78%|███████▊  | 233/300 [01:45<00:29,  2.24it/s]Epoch 24:  78%|███████▊  | 234/300 [01:46<00:30,  2.15it/s]Epoch 24:  78%|███████▊  | 235/300 [01:46<00:29,  2.20it/s]Epoch 24:  79%|███████▊  | 236/300 [01:47<00:27,  2.30it/s]Epoch 24:  79%|███████▉  | 237/300 [01:47<00:28,  2.22it/s]Epoch 24:  79%|███████▉  | 238/300 [01:48<00:27,  2.26it/s]Epoch 24:  80%|███████▉  | 239/300 [01:48<00:26,  2.32it/s]06/19/2022 15:21:40 - INFO - __main__ - global step: 3720; train loss: 5.583883285522461; dev loss: 5.499755382537842
Epoch 24:  80%|████████  | 240/300 [01:49<00:26,  2.28it/s]Epoch 24:  80%|████████  | 241/300 [01:49<00:26,  2.26it/s]Epoch 24:  81%|████████  | 242/300 [01:50<00:27,  2.08it/s]Epoch 24:  81%|████████  | 243/300 [01:50<00:25,  2.24it/s]Epoch 24:  81%|████████▏ | 244/300 [01:50<00:23,  2.43it/s]Epoch 24:  82%|████████▏ | 245/300 [01:51<00:21,  2.59it/s]Epoch 24:  82%|████████▏ | 246/300 [01:51<00:21,  2.51it/s]Epoch 24:  82%|████████▏ | 247/300 [01:51<00:20,  2.54it/s]Epoch 24:  83%|████████▎ | 248/300 [01:52<00:21,  2.45it/s]Epoch 24:  83%|████████▎ | 249/300 [01:52<00:21,  2.40it/s]Epoch 24:  83%|████████▎ | 250/300 [01:53<00:23,  2.13it/s]Epoch 24:  84%|████████▎ | 251/300 [01:53<00:21,  2.23it/s]Epoch 24:  84%|████████▍ | 252/300 [01:54<00:20,  2.39it/s]Epoch 24:  84%|████████▍ | 253/300 [01:54<00:18,  2.52it/s]Epoch 24:  85%|████████▍ | 254/300 [01:54<00:19,  2.35it/s]Epoch 24:  85%|████████▌ | 255/300 [01:55<00:18,  2.45it/s]Epoch 24:  85%|████████▌ | 256/300 [01:55<00:18,  2.34it/s]Epoch 24:  86%|████████▌ | 257/300 [01:56<00:18,  2.33it/s]Epoch 24:  86%|████████▌ | 258/300 [01:56<00:19,  2.11it/s]Epoch 24:  86%|████████▋ | 259/300 [01:57<00:18,  2.17it/s]06/19/2022 15:21:49 - INFO - __main__ - global step: 3730; train loss: 5.450329780578613; dev loss: 5.571646213531494
Epoch 24:  87%|████████▋ | 260/300 [01:57<00:18,  2.16it/s]Epoch 24:  87%|████████▋ | 261/300 [01:58<00:18,  2.10it/s]Epoch 24:  87%|████████▋ | 262/300 [01:58<00:18,  2.00it/s]Epoch 24:  88%|████████▊ | 263/300 [01:59<00:18,  2.00it/s]Epoch 24:  88%|████████▊ | 264/300 [01:59<00:17,  2.00it/s]Epoch 24:  88%|████████▊ | 265/300 [02:00<00:18,  1.89it/s]Epoch 24:  89%|████████▊ | 266/300 [02:01<00:19,  1.76it/s]Epoch 24:  89%|████████▉ | 267/300 [02:01<00:17,  1.91it/s]Epoch 24:  89%|████████▉ | 268/300 [02:01<00:16,  1.95it/s]Epoch 24:  90%|████████▉ | 269/300 [02:02<00:15,  1.99it/s]Epoch 24:  90%|█████████ | 270/300 [02:02<00:15,  1.93it/s]Epoch 24:  90%|█████████ | 271/300 [02:03<00:14,  1.94it/s]Epoch 24:  91%|█████████ | 272/300 [02:03<00:13,  2.15it/s]Epoch 24:  91%|█████████ | 273/300 [02:04<00:11,  2.28it/s]Epoch 24:  91%|█████████▏| 274/300 [02:04<00:10,  2.41it/s]Epoch 24:  92%|█████████▏| 275/300 [02:05<00:10,  2.31it/s]Epoch 24:  92%|█████████▏| 276/300 [02:05<00:10,  2.30it/s]Epoch 24:  92%|█████████▏| 277/300 [02:05<00:10,  2.30it/s]Epoch 24:  93%|█████████▎| 278/300 [02:06<00:09,  2.28it/s]Epoch 24:  93%|█████████▎| 279/300 [02:06<00:09,  2.14it/s]06/19/2022 15:21:59 - INFO - __main__ - global step: 3740; train loss: 5.45472526550293; dev loss: 5.587263584136963
Epoch 24:  93%|█████████▎| 280/300 [02:07<00:09,  2.18it/s]Epoch 24:  94%|█████████▎| 281/300 [02:07<00:08,  2.21it/s]Epoch 24:  94%|█████████▍| 282/300 [02:08<00:08,  2.20it/s]Epoch 24:  94%|█████████▍| 283/300 [02:08<00:07,  2.14it/s]Epoch 24:  95%|█████████▍| 284/300 [02:09<00:06,  2.32it/s]Epoch 24:  95%|█████████▌| 285/300 [02:09<00:06,  2.48it/s]Epoch 24:  95%|█████████▌| 286/300 [02:09<00:05,  2.61it/s]Epoch 24:  96%|█████████▌| 287/300 [02:10<00:05,  2.49it/s]Epoch 24:  96%|█████████▌| 288/300 [02:10<00:05,  2.39it/s]Epoch 24:  96%|█████████▋| 289/300 [02:11<00:04,  2.34it/s]Epoch 24:  97%|█████████▋| 290/300 [02:11<00:04,  2.22it/s]Epoch 24:  97%|█████████▋| 291/300 [02:12<00:04,  2.06it/s]Epoch 24:  97%|█████████▋| 292/300 [02:12<00:03,  2.25it/s]Epoch 24:  98%|█████████▊| 293/300 [02:12<00:02,  2.44it/s]Epoch 24:  98%|█████████▊| 294/300 [02:13<00:02,  2.59it/s]Epoch 24:  98%|█████████▊| 295/300 [02:13<00:01,  2.69it/s]Epoch 24:  99%|█████████▊| 296/300 [02:13<00:01,  2.52it/s]Epoch 24:  99%|█████████▉| 297/300 [02:14<00:01,  2.65it/s]Epoch 24:  99%|█████████▉| 298/300 [02:14<00:00,  2.67it/s]Epoch 24: 100%|█████████▉| 299/300 [02:15<00:00,  2.66it/s]06/19/2022 15:22:07 - INFO - __main__ - global step: 3750; train loss: 5.320268154144287; dev loss: 5.345821380615234
Epoch 24: 100%|██████████| 300/300 [02:15<00:00,  2.38it/s]Epoch 24: 100%|██████████| 300/300 [02:15<00:00,  2.21it/s]
Epoch 25:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 25:   0%|          | 1/300 [00:00<02:26,  2.04it/s]Epoch 25:   1%|          | 2/300 [00:00<02:23,  2.08it/s]Epoch 25:   1%|          | 3/300 [00:01<02:21,  2.10it/s]Epoch 25:   1%|▏         | 4/300 [00:01<02:24,  2.04it/s]Epoch 25:   2%|▏         | 5/300 [00:02<02:17,  2.14it/s]Epoch 25:   2%|▏         | 6/300 [00:02<02:12,  2.23it/s]Epoch 25:   2%|▏         | 7/300 [00:03<02:02,  2.38it/s]Epoch 25:   3%|▎         | 8/300 [00:03<02:14,  2.17it/s]Epoch 25:   3%|▎         | 9/300 [00:04<02:06,  2.30it/s]Epoch 25:   3%|▎         | 10/300 [00:04<02:07,  2.27it/s]Epoch 25:   4%|▎         | 11/300 [00:04<02:03,  2.33it/s]Epoch 25:   4%|▍         | 12/300 [00:05<02:06,  2.28it/s]Epoch 25:   4%|▍         | 13/300 [00:05<01:59,  2.41it/s]Epoch 25:   5%|▍         | 14/300 [00:06<01:53,  2.51it/s]Epoch 25:   5%|▌         | 15/300 [00:06<01:51,  2.56it/s]Epoch 25:   5%|▌         | 16/300 [00:07<02:05,  2.27it/s]Epoch 25:   6%|▌         | 17/300 [00:07<01:57,  2.40it/s]Epoch 25:   6%|▌         | 18/300 [00:07<01:53,  2.47it/s]Epoch 25:   6%|▋         | 19/300 [00:08<01:57,  2.40it/s]06/19/2022 15:22:16 - INFO - __main__ - global step: 3760; train loss: 5.069887638092041; dev loss: 5.297550678253174
Epoch 25:   7%|▋         | 20/300 [00:08<02:11,  2.13it/s]Epoch 25:   7%|▋         | 21/300 [00:09<02:10,  2.14it/s]Epoch 25:   7%|▋         | 22/300 [00:09<02:02,  2.27it/s]Epoch 25:   8%|▊         | 23/300 [00:10<01:56,  2.37it/s]Epoch 25:   8%|▊         | 24/300 [00:10<01:51,  2.48it/s]Epoch 25:   8%|▊         | 25/300 [00:10<01:55,  2.37it/s]Epoch 25:   9%|▊         | 26/300 [00:11<01:55,  2.37it/s]Epoch 25:   9%|▉         | 27/300 [00:11<02:06,  2.16it/s]Epoch 25:   9%|▉         | 28/300 [00:12<02:05,  2.16it/s]Epoch 25:  10%|▉         | 29/300 [00:12<02:16,  1.98it/s]Epoch 25:  10%|█         | 30/300 [00:13<02:13,  2.02it/s]Epoch 25:  10%|█         | 31/300 [00:13<02:14,  1.99it/s]Epoch 25:  11%|█         | 32/300 [00:14<02:12,  2.02it/s]Epoch 25:  11%|█         | 33/300 [00:14<02:16,  1.95it/s]Epoch 25:  11%|█▏        | 34/300 [00:15<02:12,  2.00it/s]Epoch 25:  12%|█▏        | 35/300 [00:15<02:09,  2.04it/s]Epoch 25:  12%|█▏        | 36/300 [00:16<02:12,  2.00it/s]Epoch 25:  12%|█▏        | 37/300 [00:16<02:20,  1.87it/s]Epoch 25:  13%|█▎        | 38/300 [00:17<02:14,  1.95it/s]Epoch 25:  13%|█▎        | 39/300 [00:17<02:10,  2.00it/s]06/19/2022 15:22:25 - INFO - __main__ - global step: 3770; train loss: 5.583763599395752; dev loss: 5.568446159362793
Epoch 25:  13%|█▎        | 40/300 [00:18<02:05,  2.07it/s]Epoch 25:  14%|█▎        | 41/300 [00:18<02:04,  2.08it/s]Epoch 25:  14%|█▍        | 42/300 [00:19<01:55,  2.24it/s]Epoch 25:  14%|█▍        | 43/300 [00:19<01:48,  2.37it/s]Epoch 25:  15%|█▍        | 44/300 [00:19<01:44,  2.46it/s]Epoch 25:  15%|█▌        | 45/300 [00:20<01:51,  2.29it/s]Epoch 25:  15%|█▌        | 46/300 [00:20<01:45,  2.41it/s]Epoch 25:  16%|█▌        | 47/300 [00:21<01:41,  2.49it/s]Epoch 25:  16%|█▌        | 48/300 [00:21<01:41,  2.47it/s]Epoch 25:  16%|█▋        | 49/300 [00:21<01:36,  2.60it/s]Epoch 25:  17%|█▋        | 50/300 [00:22<01:40,  2.50it/s]Epoch 25:  17%|█▋        | 51/300 [00:22<01:34,  2.64it/s]Epoch 25:  17%|█▋        | 52/300 [00:23<01:30,  2.74it/s]Epoch 25:  18%|█▊        | 53/300 [00:23<01:29,  2.75it/s]Epoch 25:  18%|█▊        | 54/300 [00:23<01:43,  2.37it/s]Epoch 25:  18%|█▊        | 55/300 [00:24<01:44,  2.33it/s]Epoch 25:  19%|█▊        | 56/300 [00:24<01:48,  2.25it/s]Epoch 25:  19%|█▉        | 57/300 [00:25<01:46,  2.28it/s]Epoch 25:  19%|█▉        | 58/300 [00:25<01:55,  2.09it/s]Epoch 25:  20%|█▉        | 59/300 [00:26<01:53,  2.12it/s]06/19/2022 15:22:34 - INFO - __main__ - global step: 3780; train loss: 5.576590538024902; dev loss: 5.502894878387451
Epoch 25:  20%|██        | 60/300 [00:26<01:53,  2.12it/s]Epoch 25:  20%|██        | 61/300 [00:27<01:45,  2.27it/s]Epoch 25:  21%|██        | 62/300 [00:27<01:43,  2.30it/s]Epoch 25:  21%|██        | 63/300 [00:27<01:35,  2.48it/s]Epoch 25:  21%|██▏       | 64/300 [00:28<01:30,  2.62it/s]Epoch 25:  22%|██▏       | 65/300 [00:28<01:28,  2.67it/s]Epoch 25:  22%|██▏       | 66/300 [00:29<01:33,  2.50it/s]Epoch 25:  22%|██▏       | 67/300 [00:29<01:30,  2.56it/s]Epoch 25:  23%|██▎       | 68/300 [00:29<01:26,  2.68it/s]Epoch 25:  23%|██▎       | 69/300 [00:30<01:24,  2.74it/s]Epoch 25:  23%|██▎       | 70/300 [00:30<01:41,  2.26it/s]Epoch 25:  24%|██▎       | 71/300 [00:31<01:34,  2.42it/s]Epoch 25:  24%|██▍       | 72/300 [00:31<01:29,  2.56it/s]Epoch 25:  24%|██▍       | 73/300 [00:31<01:29,  2.54it/s]Epoch 25:  25%|██▍       | 74/300 [00:32<01:38,  2.29it/s]Epoch 25:  25%|██▌       | 75/300 [00:32<01:31,  2.47it/s]Epoch 25:  25%|██▌       | 76/300 [00:33<01:25,  2.61it/s]Epoch 25:  26%|██▌       | 77/300 [00:33<01:21,  2.72it/s]Epoch 25:  26%|██▌       | 78/300 [00:33<01:18,  2.82it/s]Epoch 25:  26%|██▋       | 79/300 [00:34<01:23,  2.64it/s]06/19/2022 15:22:41 - INFO - __main__ - global step: 3790; train loss: 5.409809589385986; dev loss: 5.463890552520752
Epoch 25:  27%|██▋       | 80/300 [00:34<01:20,  2.74it/s]Epoch 25:  27%|██▋       | 81/300 [00:34<01:18,  2.81it/s]Epoch 25:  27%|██▋       | 82/300 [00:35<01:21,  2.69it/s]Epoch 25:  28%|██▊       | 83/300 [00:35<01:31,  2.38it/s]Epoch 25:  28%|██▊       | 84/300 [00:36<01:24,  2.54it/s]Epoch 25:  28%|██▊       | 85/300 [00:36<01:20,  2.67it/s]Epoch 25:  29%|██▊       | 86/300 [00:36<01:17,  2.75it/s]Epoch 25:  29%|██▉       | 87/300 [00:37<01:21,  2.62it/s]Epoch 25:  29%|██▉       | 88/300 [00:37<01:17,  2.73it/s]Epoch 25:  30%|██▉       | 89/300 [00:37<01:15,  2.81it/s]Epoch 25:  30%|███       | 90/300 [00:38<01:13,  2.85it/s]Epoch 25:  30%|███       | 91/300 [00:38<01:19,  2.64it/s]Epoch 25:  31%|███       | 92/300 [00:38<01:16,  2.73it/s]Epoch 25:  31%|███       | 93/300 [00:39<01:13,  2.83it/s]Epoch 25:  31%|███▏      | 94/300 [00:39<01:13,  2.79it/s]Epoch 25:  32%|███▏      | 95/300 [00:40<01:17,  2.63it/s]Epoch 25:  32%|███▏      | 96/300 [00:40<01:15,  2.71it/s]Epoch 25:  32%|███▏      | 97/300 [00:40<01:12,  2.80it/s]Epoch 25:  33%|███▎      | 98/300 [00:41<01:10,  2.88it/s]Epoch 25:  33%|███▎      | 99/300 [00:41<01:15,  2.68it/s]06/19/2022 15:22:49 - INFO - __main__ - global step: 3800; train loss: 5.47580623626709; dev loss: 5.30576229095459
Epoch 25:  33%|███▎      | 100/300 [00:41<01:13,  2.73it/s]Epoch 25:  34%|███▎      | 101/300 [00:42<01:17,  2.57it/s]Epoch 25:  34%|███▍      | 102/300 [00:42<01:19,  2.48it/s]Epoch 25:  34%|███▍      | 103/300 [00:43<01:26,  2.28it/s]Epoch 25:  35%|███▍      | 104/300 [00:43<01:35,  2.06it/s]Epoch 25:  35%|███▌      | 105/300 [00:44<01:34,  2.07it/s]Epoch 25:  35%|███▌      | 106/300 [00:44<01:33,  2.07it/s]Epoch 25:  36%|███▌      | 107/300 [00:45<01:34,  2.04it/s]Epoch 25:  36%|███▌      | 108/300 [00:45<01:42,  1.87it/s]Epoch 25:  36%|███▋      | 109/300 [00:46<01:31,  2.09it/s]Epoch 25:  37%|███▋      | 110/300 [00:46<01:29,  2.11it/s]Epoch 25:  37%|███▋      | 111/300 [00:47<01:26,  2.18it/s]Epoch 25:  37%|███▋      | 112/300 [00:47<01:37,  1.93it/s]Epoch 25:  38%|███▊      | 113/300 [00:48<01:27,  2.15it/s]Epoch 25:  38%|███▊      | 114/300 [00:48<01:20,  2.31it/s]Epoch 25:  38%|███▊      | 115/300 [00:48<01:20,  2.29it/s]Epoch 25:  39%|███▊      | 116/300 [00:49<01:26,  2.13it/s]Epoch 25:  39%|███▉      | 117/300 [00:49<01:23,  2.20it/s]Epoch 25:  39%|███▉      | 118/300 [00:50<01:21,  2.25it/s]Epoch 25:  40%|███▉      | 119/300 [00:50<01:23,  2.16it/s]06/19/2022 15:22:58 - INFO - __main__ - global step: 3810; train loss: 5.362360000610352; dev loss: 5.123176097869873
Epoch 25:  40%|████      | 120/300 [00:51<01:28,  2.04it/s]Epoch 25:  40%|████      | 121/300 [00:51<01:23,  2.14it/s]Epoch 25:  41%|████      | 122/300 [00:52<01:17,  2.30it/s]Epoch 25:  41%|████      | 123/300 [00:52<01:15,  2.33it/s]Epoch 25:  41%|████▏     | 124/300 [00:53<01:18,  2.25it/s]Epoch 25:  42%|████▏     | 125/300 [00:53<01:12,  2.43it/s]Epoch 25:  42%|████▏     | 126/300 [00:53<01:07,  2.58it/s]Epoch 25:  42%|████▏     | 127/300 [00:54<01:07,  2.55it/s]Epoch 25:  43%|████▎     | 128/300 [00:54<01:09,  2.48it/s]Epoch 25:  43%|████▎     | 129/300 [00:54<01:05,  2.62it/s]Epoch 25:  43%|████▎     | 130/300 [00:55<01:04,  2.64it/s]Epoch 25:  44%|████▎     | 131/300 [00:55<01:01,  2.74it/s]Epoch 25:  44%|████▍     | 132/300 [00:55<00:59,  2.84it/s]Epoch 25:  44%|████▍     | 133/300 [00:56<01:06,  2.52it/s]Epoch 25:  45%|████▍     | 134/300 [00:56<01:05,  2.53it/s]Epoch 25:  45%|████▌     | 135/300 [00:57<01:04,  2.55it/s]Epoch 25:  45%|████▌     | 136/300 [00:57<01:06,  2.48it/s]Epoch 25:  46%|████▌     | 137/300 [00:58<01:14,  2.19it/s]Epoch 25:  46%|████▌     | 138/300 [00:58<01:09,  2.32it/s]Epoch 25:  46%|████▋     | 139/300 [00:58<01:04,  2.50it/s]06/19/2022 15:23:06 - INFO - __main__ - global step: 3820; train loss: 5.257615089416504; dev loss: 4.94672155380249
Epoch 25:  47%|████▋     | 140/300 [00:59<01:00,  2.64it/s]Epoch 25:  47%|████▋     | 141/300 [00:59<01:02,  2.53it/s]Epoch 25:  47%|████▋     | 142/300 [01:00<01:00,  2.61it/s]Epoch 25:  48%|████▊     | 143/300 [01:00<00:57,  2.72it/s]Epoch 25:  48%|████▊     | 144/300 [01:00<00:55,  2.82it/s]Epoch 25:  48%|████▊     | 145/300 [01:01<01:00,  2.55it/s]Epoch 25:  49%|████▊     | 146/300 [01:01<01:00,  2.56it/s]Epoch 25:  49%|████▉     | 147/300 [01:01<01:00,  2.52it/s]Epoch 25:  49%|████▉     | 148/300 [01:02<01:04,  2.37it/s]Epoch 25:  50%|████▉     | 149/300 [01:03<01:13,  2.07it/s]Epoch 25:  50%|█████     | 150/300 [01:03<01:09,  2.16it/s]Epoch 25:  50%|█████     | 151/300 [01:03<01:05,  2.29it/s]Epoch 25:  51%|█████     | 152/300 [01:04<01:03,  2.34it/s]Epoch 25:  51%|█████     | 153/300 [01:04<01:07,  2.18it/s]Epoch 25:  51%|█████▏    | 154/300 [01:05<01:06,  2.21it/s]Epoch 25:  52%|█████▏    | 155/300 [01:05<01:02,  2.30it/s]Epoch 25:  52%|█████▏    | 156/300 [01:05<00:58,  2.48it/s]Epoch 25:  52%|█████▏    | 157/300 [01:06<00:55,  2.60it/s]Epoch 25:  53%|█████▎    | 158/300 [01:06<00:56,  2.51it/s]Epoch 25:  53%|█████▎    | 159/300 [01:07<00:53,  2.64it/s]06/19/2022 15:23:14 - INFO - __main__ - global step: 3830; train loss: 5.122906684875488; dev loss: 5.276290416717529
Epoch 25:  53%|█████▎    | 160/300 [01:07<00:51,  2.74it/s]Epoch 25:  54%|█████▎    | 161/300 [01:07<00:49,  2.81it/s]Epoch 25:  54%|█████▍    | 162/300 [01:08<00:52,  2.65it/s]Epoch 25:  54%|█████▍    | 163/300 [01:08<00:49,  2.75it/s]Epoch 25:  55%|█████▍    | 164/300 [01:08<00:48,  2.80it/s]Epoch 25:  55%|█████▌    | 165/300 [01:09<00:47,  2.82it/s]Epoch 25:  55%|█████▌    | 166/300 [01:09<00:50,  2.64it/s]Epoch 25:  56%|█████▌    | 167/300 [01:09<00:48,  2.73it/s]Epoch 25:  56%|█████▌    | 168/300 [01:10<00:46,  2.82it/s]Epoch 25:  56%|█████▋    | 169/300 [01:10<00:46,  2.79it/s]Epoch 25:  57%|█████▋    | 170/300 [01:11<00:55,  2.36it/s]Epoch 25:  57%|█████▋    | 171/300 [01:11<00:57,  2.25it/s]Epoch 25:  57%|█████▋    | 172/300 [01:12<00:57,  2.24it/s]Epoch 25:  58%|█████▊    | 173/300 [01:12<00:55,  2.28it/s]Epoch 25:  58%|█████▊    | 174/300 [01:13<01:02,  2.03it/s]Epoch 25:  58%|█████▊    | 175/300 [01:13<00:59,  2.09it/s]Epoch 25:  59%|█████▊    | 176/300 [01:14<01:01,  2.01it/s]Epoch 25:  59%|█████▉    | 177/300 [01:14<01:00,  2.02it/s]Epoch 25:  59%|█████▉    | 178/300 [01:15<01:04,  1.88it/s]Epoch 25:  60%|█████▉    | 179/300 [01:15<01:02,  1.95it/s]06/19/2022 15:23:23 - INFO - __main__ - global step: 3840; train loss: 5.228480339050293; dev loss: 5.258419513702393
Epoch 25:  60%|██████    | 180/300 [01:16<01:01,  1.96it/s]Epoch 25:  60%|██████    | 181/300 [01:16<00:56,  2.10it/s]Epoch 25:  61%|██████    | 182/300 [01:17<00:58,  2.01it/s]Epoch 25:  61%|██████    | 183/300 [01:17<00:55,  2.09it/s]Epoch 25:  61%|██████▏   | 184/300 [01:18<00:53,  2.17it/s]Epoch 25:  62%|██████▏   | 185/300 [01:18<00:51,  2.23it/s]Epoch 25:  62%|██████▏   | 186/300 [01:18<00:47,  2.40it/s]Epoch 25:  62%|██████▏   | 187/300 [01:19<00:47,  2.38it/s]Epoch 25:  63%|██████▎   | 188/300 [01:19<00:43,  2.55it/s]Epoch 25:  63%|██████▎   | 189/300 [01:20<00:43,  2.54it/s]Epoch 25:  63%|██████▎   | 190/300 [01:20<00:41,  2.63it/s]Epoch 25:  64%|██████▎   | 191/300 [01:20<00:42,  2.56it/s]Epoch 25:  64%|██████▍   | 192/300 [01:21<00:40,  2.68it/s]Epoch 25:  64%|██████▍   | 193/300 [01:21<00:38,  2.78it/s]Epoch 25:  65%|██████▍   | 194/300 [01:21<00:39,  2.68it/s]Epoch 25:  65%|██████▌   | 195/300 [01:22<00:44,  2.34it/s]Epoch 25:  65%|██████▌   | 196/300 [01:22<00:44,  2.33it/s]Epoch 25:  66%|██████▌   | 197/300 [01:23<00:44,  2.32it/s]Epoch 25:  66%|██████▌   | 198/300 [01:23<00:43,  2.36it/s]Epoch 25:  66%|██████▋   | 199/300 [01:24<00:44,  2.28it/s]06/19/2022 15:23:31 - INFO - __main__ - global step: 3850; train loss: 5.6599321365356445; dev loss: 5.537272930145264
Epoch 25:  67%|██████▋   | 200/300 [01:24<00:41,  2.41it/s]Epoch 25:  67%|██████▋   | 201/300 [01:24<00:39,  2.50it/s]Epoch 25:  67%|██████▋   | 202/300 [01:25<00:42,  2.33it/s]Epoch 25:  68%|██████▊   | 203/300 [01:25<00:43,  2.26it/s]Epoch 25:  68%|██████▊   | 204/300 [01:26<00:39,  2.42it/s]Epoch 25:  68%|██████▊   | 205/300 [01:26<00:36,  2.58it/s]Epoch 25:  69%|██████▊   | 206/300 [01:26<00:35,  2.68it/s]Epoch 25:  69%|██████▉   | 207/300 [01:27<00:35,  2.58it/s]Epoch 25:  69%|██████▉   | 208/300 [01:27<00:33,  2.71it/s]Epoch 25:  70%|██████▉   | 209/300 [01:27<00:32,  2.82it/s]Epoch 25:  70%|███████   | 210/300 [01:28<00:31,  2.86it/s]Epoch 25:  70%|███████   | 211/300 [01:28<00:30,  2.91it/s]Epoch 25:  71%|███████   | 212/300 [01:29<00:35,  2.46it/s]Epoch 25:  71%|███████   | 213/300 [01:29<00:37,  2.34it/s]Epoch 25:  71%|███████▏  | 214/300 [01:30<00:37,  2.29it/s]Epoch 25:  72%|███████▏  | 215/300 [01:30<00:36,  2.32it/s]Epoch 25:  72%|███████▏  | 216/300 [01:30<00:35,  2.33it/s]Epoch 25:  72%|███████▏  | 217/300 [01:31<00:33,  2.51it/s]Epoch 25:  73%|███████▎  | 218/300 [01:31<00:30,  2.65it/s]Epoch 25:  73%|███████▎  | 219/300 [01:31<00:29,  2.73it/s]06/19/2022 15:23:39 - INFO - __main__ - global step: 3860; train loss: 5.232874870300293; dev loss: 5.2688493728637695
Epoch 25:  73%|███████▎  | 220/300 [01:32<00:31,  2.54it/s]Epoch 25:  74%|███████▎  | 221/300 [01:32<00:30,  2.55it/s]Epoch 25:  74%|███████▍  | 222/300 [01:33<00:29,  2.69it/s]Epoch 25:  74%|███████▍  | 223/300 [01:33<00:27,  2.76it/s]Epoch 25:  75%|███████▍  | 224/300 [01:33<00:29,  2.56it/s]Epoch 25:  75%|███████▌  | 225/300 [01:34<00:28,  2.68it/s]Epoch 25:  75%|███████▌  | 226/300 [01:34<00:27,  2.74it/s]Epoch 25:  76%|███████▌  | 227/300 [01:34<00:27,  2.69it/s]Epoch 25:  76%|███████▌  | 228/300 [01:35<00:28,  2.49it/s]Epoch 25:  76%|███████▋  | 229/300 [01:35<00:29,  2.39it/s]Epoch 25:  77%|███████▋  | 230/300 [01:36<00:30,  2.32it/s]Epoch 25:  77%|███████▋  | 231/300 [01:36<00:30,  2.27it/s]Epoch 25:  77%|███████▋  | 232/300 [01:37<00:32,  2.09it/s]Epoch 25:  78%|███████▊  | 233/300 [01:37<00:29,  2.24it/s]Epoch 25:  78%|███████▊  | 234/300 [01:38<00:29,  2.26it/s]Epoch 25:  78%|███████▊  | 235/300 [01:38<00:28,  2.27it/s]Epoch 25:  79%|███████▊  | 236/300 [01:39<00:30,  2.08it/s]Epoch 25:  79%|███████▉  | 237/300 [01:39<00:29,  2.16it/s]Epoch 25:  79%|███████▉  | 238/300 [01:39<00:26,  2.33it/s]Epoch 25:  80%|███████▉  | 239/300 [01:40<00:25,  2.38it/s]06/19/2022 15:23:48 - INFO - __main__ - global step: 3870; train loss: 5.253101348876953; dev loss: 5.601196765899658
Epoch 25:  80%|████████  | 240/300 [01:40<00:25,  2.38it/s]Epoch 25:  80%|████████  | 241/300 [01:41<00:24,  2.37it/s]Epoch 25:  81%|████████  | 242/300 [01:41<00:22,  2.54it/s]Epoch 25:  81%|████████  | 243/300 [01:41<00:21,  2.66it/s]Epoch 25:  81%|████████▏ | 244/300 [01:42<00:20,  2.75it/s]Epoch 25:  82%|████████▏ | 245/300 [01:42<00:21,  2.62it/s]Epoch 25:  82%|████████▏ | 246/300 [01:43<00:20,  2.62it/s]Epoch 25:  82%|████████▏ | 247/300 [01:43<00:21,  2.50it/s]Epoch 25:  83%|████████▎ | 248/300 [01:43<00:21,  2.42it/s]Epoch 25:  83%|████████▎ | 249/300 [01:44<00:23,  2.18it/s]Epoch 25:  83%|████████▎ | 250/300 [01:44<00:22,  2.18it/s]Epoch 25:  84%|████████▎ | 251/300 [01:45<00:22,  2.20it/s]Epoch 25:  84%|████████▍ | 252/300 [01:45<00:21,  2.21it/s]Epoch 25:  84%|████████▍ | 253/300 [01:46<00:22,  2.04it/s]Epoch 25:  85%|████████▍ | 254/300 [01:46<00:21,  2.12it/s]Epoch 25:  85%|████████▌ | 255/300 [01:47<00:21,  2.08it/s]Epoch 25:  85%|████████▌ | 256/300 [01:47<00:20,  2.15it/s]Epoch 25:  86%|████████▌ | 257/300 [01:48<00:22,  1.95it/s]Epoch 25:  86%|████████▌ | 258/300 [01:48<00:21,  1.97it/s]Epoch 25:  86%|████████▋ | 259/300 [01:49<00:19,  2.05it/s]06/19/2022 15:23:57 - INFO - __main__ - global step: 3880; train loss: 4.464036464691162; dev loss: 4.43597936630249
Epoch 25:  87%|████████▋ | 260/300 [01:49<00:19,  2.08it/s]Epoch 25:  87%|████████▋ | 261/300 [01:50<00:19,  1.95it/s]Epoch 25:  87%|████████▋ | 262/300 [01:50<00:19,  1.98it/s]Epoch 25:  88%|████████▊ | 263/300 [01:51<00:17,  2.14it/s]Epoch 25:  88%|████████▊ | 264/300 [01:51<00:17,  2.06it/s]Epoch 25:  88%|████████▊ | 265/300 [01:52<00:15,  2.21it/s]Epoch 25:  89%|████████▊ | 266/300 [01:52<00:15,  2.21it/s]Epoch 25:  89%|████████▉ | 267/300 [01:52<00:14,  2.34it/s]Epoch 25:  89%|████████▉ | 268/300 [01:53<00:12,  2.47it/s]Epoch 25:  90%|████████▉ | 269/300 [01:53<00:12,  2.53it/s]Epoch 25:  90%|█████████ | 270/300 [01:54<00:12,  2.44it/s]Epoch 25:  90%|█████████ | 271/300 [01:54<00:11,  2.56it/s]Epoch 25:  91%|█████████ | 272/300 [01:54<00:10,  2.63it/s]Epoch 25:  91%|█████████ | 273/300 [01:55<00:10,  2.68it/s]Epoch 25:  91%|█████████▏| 274/300 [01:55<00:11,  2.29it/s]Epoch 25:  92%|█████████▏| 275/300 [01:56<00:11,  2.17it/s]Epoch 25:  92%|█████████▏| 276/300 [01:56<00:11,  2.05it/s]Epoch 25:  92%|█████████▏| 277/300 [01:57<00:10,  2.14it/s]Epoch 25:  93%|█████████▎| 278/300 [01:57<00:11,  1.97it/s]Epoch 25:  93%|█████████▎| 279/300 [01:58<00:10,  1.94it/s]06/19/2022 15:24:06 - INFO - __main__ - global step: 3890; train loss: 4.557620048522949; dev loss: 4.518355369567871
Epoch 25:  93%|█████████▎| 280/300 [01:58<00:09,  2.10it/s]Epoch 25:  94%|█████████▎| 281/300 [01:59<00:08,  2.26it/s]Epoch 25:  94%|█████████▍| 282/300 [01:59<00:08,  2.21it/s]Epoch 25:  94%|█████████▍| 283/300 [01:59<00:07,  2.36it/s]Epoch 25:  95%|█████████▍| 284/300 [02:00<00:06,  2.36it/s]Epoch 25:  95%|█████████▌| 285/300 [02:00<00:06,  2.46it/s]Epoch 25:  95%|█████████▌| 286/300 [02:01<00:06,  2.18it/s]Epoch 25:  96%|█████████▌| 287/300 [02:01<00:05,  2.25it/s]Epoch 25:  96%|█████████▌| 288/300 [02:02<00:05,  2.23it/s]Epoch 25:  96%|█████████▋| 289/300 [02:02<00:05,  2.20it/s]Epoch 25:  97%|█████████▋| 290/300 [02:03<00:05,  1.95it/s]Epoch 25:  97%|█████████▋| 291/300 [02:03<00:04,  2.14it/s]Epoch 25:  97%|█████████▋| 292/300 [02:04<00:03,  2.28it/s]Epoch 25:  98%|█████████▊| 293/300 [02:04<00:03,  2.21it/s]Epoch 25:  98%|█████████▊| 294/300 [02:05<00:02,  2.20it/s]Epoch 25:  98%|█████████▊| 295/300 [02:05<00:02,  2.15it/s]Epoch 25:  99%|█████████▊| 296/300 [02:05<00:01,  2.31it/s]Epoch 25:  99%|█████████▉| 297/300 [02:06<00:01,  2.45it/s]Epoch 25:  99%|█████████▉| 298/300 [02:06<00:00,  2.46it/s]Epoch 25: 100%|█████████▉| 299/300 [02:07<00:00,  2.14it/s]06/19/2022 15:24:15 - INFO - __main__ - global step: 3900; train loss: 5.444702625274658; dev loss: 5.388527870178223
Epoch 25: 100%|██████████| 300/300 [02:07<00:00,  2.19it/s]Epoch 25: 100%|██████████| 300/300 [02:07<00:00,  2.35it/s]
Epoch 26:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 26:   0%|          | 1/300 [00:00<02:10,  2.29it/s]Epoch 26:   1%|          | 2/300 [00:00<02:12,  2.26it/s]Epoch 26:   1%|          | 3/300 [00:01<02:23,  2.07it/s]Epoch 26:   1%|▏         | 4/300 [00:01<02:18,  2.14it/s]Epoch 26:   2%|▏         | 5/300 [00:02<02:08,  2.30it/s]Epoch 26:   2%|▏         | 6/300 [00:02<02:10,  2.25it/s]Epoch 26:   2%|▏         | 7/300 [00:03<02:12,  2.20it/s]Epoch 26:   3%|▎         | 8/300 [00:03<02:10,  2.25it/s]Epoch 26:   3%|▎         | 9/300 [00:03<02:01,  2.39it/s]Epoch 26:   3%|▎         | 10/300 [00:04<01:57,  2.47it/s]Epoch 26:   4%|▎         | 11/300 [00:04<02:02,  2.37it/s]Epoch 26:   4%|▍         | 12/300 [00:05<02:02,  2.35it/s]Epoch 26:   4%|▍         | 13/300 [00:05<02:02,  2.34it/s]Epoch 26:   5%|▍         | 14/300 [00:06<01:59,  2.39it/s]Epoch 26:   5%|▌         | 15/300 [00:06<02:10,  2.19it/s]Epoch 26:   5%|▌         | 16/300 [00:06<02:00,  2.35it/s]Epoch 26:   6%|▌         | 17/300 [00:07<01:53,  2.49it/s]Epoch 26:   6%|▌         | 18/300 [00:07<01:55,  2.44it/s]Epoch 26:   6%|▋         | 19/300 [00:08<01:52,  2.49it/s]06/19/2022 15:24:23 - INFO - __main__ - global step: 3910; train loss: 4.881333351135254; dev loss: 5.085341930389404
Epoch 26:   7%|▋         | 20/300 [00:08<02:07,  2.19it/s]Epoch 26:   7%|▋         | 21/300 [00:09<02:01,  2.30it/s]Epoch 26:   7%|▋         | 22/300 [00:09<01:56,  2.38it/s]Epoch 26:   8%|▊         | 23/300 [00:09<01:53,  2.45it/s]Epoch 26:   8%|▊         | 24/300 [00:10<02:04,  2.22it/s]Epoch 26:   8%|▊         | 25/300 [00:10<02:15,  2.03it/s]Epoch 26:   9%|▊         | 26/300 [00:11<02:08,  2.13it/s]Epoch 26:   9%|▉         | 27/300 [00:11<02:06,  2.16it/s]Epoch 26:   9%|▉         | 28/300 [00:12<02:05,  2.17it/s]Epoch 26:  10%|▉         | 29/300 [00:12<01:56,  2.33it/s]Epoch 26:  10%|█         | 30/300 [00:13<01:49,  2.47it/s]Epoch 26:  10%|█         | 31/300 [00:13<01:44,  2.57it/s]Epoch 26:  11%|█         | 32/300 [00:13<01:50,  2.42it/s]Epoch 26:  11%|█         | 33/300 [00:14<01:45,  2.54it/s]Epoch 26:  11%|█▏        | 34/300 [00:14<01:48,  2.45it/s]Epoch 26:  12%|█▏        | 35/300 [00:15<01:47,  2.46it/s]Epoch 26:  12%|█▏        | 36/300 [00:15<02:02,  2.16it/s]Epoch 26:  12%|█▏        | 37/300 [00:16<02:00,  2.18it/s]Epoch 26:  13%|█▎        | 38/300 [00:16<02:02,  2.13it/s]Epoch 26:  13%|█▎        | 39/300 [00:16<01:53,  2.30it/s]06/19/2022 15:24:32 - INFO - __main__ - global step: 3920; train loss: 4.9594831466674805; dev loss: 4.975930213928223
Epoch 26:  13%|█▎        | 40/300 [00:17<01:55,  2.25it/s]Epoch 26:  14%|█▎        | 41/300 [00:17<01:50,  2.34it/s]Epoch 26:  14%|█▍        | 42/300 [00:18<01:48,  2.39it/s]Epoch 26:  14%|█▍        | 43/300 [00:18<01:47,  2.39it/s]Epoch 26:  15%|█▍        | 44/300 [00:19<01:49,  2.33it/s]Epoch 26:  15%|█▌        | 45/300 [00:19<01:43,  2.47it/s]Epoch 26:  15%|█▌        | 46/300 [00:19<01:38,  2.58it/s]Epoch 26:  16%|█▌        | 47/300 [00:20<01:40,  2.52it/s]Epoch 26:  16%|█▌        | 48/300 [00:20<01:36,  2.61it/s]Epoch 26:  16%|█▋        | 49/300 [00:20<01:42,  2.46it/s]Epoch 26:  17%|█▋        | 50/300 [00:21<01:40,  2.50it/s]Epoch 26:  17%|█▋        | 51/300 [00:21<01:35,  2.61it/s]Epoch 26:  17%|█▋        | 52/300 [00:22<01:33,  2.66it/s]Epoch 26:  18%|█▊        | 53/300 [00:22<01:38,  2.50it/s]Epoch 26:  18%|█▊        | 54/300 [00:22<01:34,  2.61it/s]Epoch 26:  18%|█▊        | 55/300 [00:23<01:34,  2.60it/s]Epoch 26:  19%|█▊        | 56/300 [00:23<01:31,  2.67it/s]Epoch 26:  19%|█▉        | 57/300 [00:24<01:42,  2.36it/s]Epoch 26:  19%|█▉        | 58/300 [00:24<01:40,  2.41it/s]Epoch 26:  20%|█▉        | 59/300 [00:24<01:40,  2.39it/s]06/19/2022 15:24:40 - INFO - __main__ - global step: 3930; train loss: 5.33353328704834; dev loss: 5.411792755126953
Epoch 26:  20%|██        | 60/300 [00:25<01:40,  2.38it/s]Epoch 26:  20%|██        | 61/300 [00:25<01:48,  2.21it/s]Epoch 26:  21%|██        | 62/300 [00:26<01:40,  2.36it/s]Epoch 26:  21%|██        | 63/300 [00:26<01:40,  2.36it/s]Epoch 26:  21%|██▏       | 64/300 [00:27<01:40,  2.36it/s]Epoch 26:  22%|██▏       | 65/300 [00:27<01:42,  2.30it/s]Epoch 26:  22%|██▏       | 66/300 [00:27<01:35,  2.46it/s]Epoch 26:  22%|██▏       | 67/300 [00:28<01:31,  2.55it/s]Epoch 26:  23%|██▎       | 68/300 [00:28<01:27,  2.64it/s]Epoch 26:  23%|██▎       | 69/300 [00:29<01:32,  2.49it/s]Epoch 26:  23%|██▎       | 70/300 [00:29<01:28,  2.59it/s]Epoch 26:  24%|██▎       | 71/300 [00:29<01:30,  2.53it/s]Epoch 26:  24%|██▍       | 72/300 [00:30<01:27,  2.60it/s]Epoch 26:  24%|██▍       | 73/300 [00:30<01:41,  2.24it/s]Epoch 26:  25%|██▍       | 74/300 [00:31<01:44,  2.17it/s]Epoch 26:  25%|██▌       | 75/300 [00:31<01:38,  2.29it/s]Epoch 26:  25%|██▌       | 76/300 [00:32<01:37,  2.30it/s]Epoch 26:  26%|██▌       | 77/300 [00:32<01:39,  2.25it/s]Epoch 26:  26%|██▌       | 78/300 [00:33<01:50,  2.02it/s]Epoch 26:  26%|██▋       | 79/300 [00:33<01:46,  2.07it/s]06/19/2022 15:24:49 - INFO - __main__ - global step: 3940; train loss: 5.118212699890137; dev loss: 5.034361839294434
Epoch 26:  27%|██▋       | 80/300 [00:34<01:43,  2.12it/s]Epoch 26:  27%|██▋       | 81/300 [00:34<01:43,  2.12it/s]Epoch 26:  27%|██▋       | 82/300 [00:35<01:51,  1.96it/s]Epoch 26:  28%|██▊       | 83/300 [00:35<01:47,  2.02it/s]Epoch 26:  28%|██▊       | 84/300 [00:36<01:45,  2.05it/s]Epoch 26:  28%|██▊       | 85/300 [00:36<01:46,  2.03it/s]Epoch 26:  29%|██▊       | 86/300 [00:37<01:52,  1.90it/s]Epoch 26:  29%|██▉       | 87/300 [00:37<01:46,  2.00it/s]Epoch 26:  29%|██▉       | 88/300 [00:38<01:41,  2.08it/s]Epoch 26:  30%|██▉       | 89/300 [00:38<01:40,  2.10it/s]Epoch 26:  30%|███       | 90/300 [00:39<01:48,  1.94it/s]Epoch 26:  30%|███       | 91/300 [00:39<01:43,  2.02it/s]Epoch 26:  31%|███       | 92/300 [00:40<01:43,  2.00it/s]Epoch 26:  31%|███       | 93/300 [00:40<01:34,  2.20it/s]Epoch 26:  31%|███▏      | 94/300 [00:41<01:45,  1.96it/s]Epoch 26:  32%|███▏      | 95/300 [00:41<01:36,  2.12it/s]Epoch 26:  32%|███▏      | 96/300 [00:41<01:34,  2.15it/s]Epoch 26:  32%|███▏      | 97/300 [00:42<01:33,  2.17it/s]Epoch 26:  33%|███▎      | 98/300 [00:42<01:42,  1.97it/s]Epoch 26:  33%|███▎      | 99/300 [00:43<01:40,  2.01it/s]06/19/2022 15:24:58 - INFO - __main__ - global step: 3950; train loss: 5.261016368865967; dev loss: 5.188756465911865
Epoch 26:  33%|███▎      | 100/300 [00:43<01:36,  2.08it/s]Epoch 26:  34%|███▎      | 101/300 [00:44<01:34,  2.11it/s]Epoch 26:  34%|███▍      | 102/300 [00:44<01:33,  2.12it/s]Epoch 26:  34%|███▍      | 103/300 [00:45<01:40,  1.96it/s]Epoch 26:  35%|███▍      | 104/300 [00:45<01:33,  2.09it/s]Epoch 26:  35%|███▌      | 105/300 [00:46<01:39,  1.95it/s]Epoch 26:  35%|███▌      | 106/300 [00:46<01:35,  2.02it/s]Epoch 26:  36%|███▌      | 107/300 [00:47<01:40,  1.93it/s]Epoch 26:  36%|███▌      | 108/300 [00:47<01:29,  2.14it/s]Epoch 26:  36%|███▋      | 109/300 [00:48<01:22,  2.30it/s]Epoch 26:  37%|███▋      | 110/300 [00:48<01:17,  2.45it/s]Epoch 26:  37%|███▋      | 111/300 [00:48<01:19,  2.37it/s]Epoch 26:  37%|███▋      | 112/300 [00:49<01:15,  2.50it/s]Epoch 26:  38%|███▊      | 113/300 [00:49<01:13,  2.55it/s]Epoch 26:  38%|███▊      | 114/300 [00:50<01:11,  2.61it/s]Epoch 26:  38%|███▊      | 115/300 [00:50<01:15,  2.46it/s]Epoch 26:  39%|███▊      | 116/300 [00:50<01:14,  2.46it/s]Epoch 26:  39%|███▉      | 117/300 [00:51<01:19,  2.31it/s]Epoch 26:  39%|███▉      | 118/300 [00:51<01:24,  2.16it/s]Epoch 26:  40%|███▉      | 119/300 [00:52<01:31,  1.99it/s]06/19/2022 15:25:08 - INFO - __main__ - global step: 3960; train loss: 4.896170616149902; dev loss: 5.071930885314941
Epoch 26:  40%|████      | 120/300 [00:52<01:27,  2.06it/s]Epoch 26:  40%|████      | 121/300 [00:53<01:25,  2.08it/s]Epoch 26:  41%|████      | 122/300 [00:53<01:26,  2.07it/s]Epoch 26:  41%|████      | 123/300 [00:54<01:28,  1.99it/s]Epoch 26:  41%|████▏     | 124/300 [00:54<01:22,  2.14it/s]Epoch 26:  42%|████▏     | 125/300 [00:55<01:20,  2.19it/s]Epoch 26:  42%|████▏     | 126/300 [00:55<01:15,  2.29it/s]Epoch 26:  42%|████▏     | 127/300 [00:56<01:12,  2.37it/s]Epoch 26:  43%|████▎     | 128/300 [00:56<01:16,  2.25it/s]Epoch 26:  43%|████▎     | 129/300 [00:56<01:14,  2.31it/s]Epoch 26:  43%|████▎     | 130/300 [00:57<01:11,  2.37it/s]Epoch 26:  44%|████▎     | 131/300 [00:57<01:10,  2.41it/s]Epoch 26:  44%|████▍     | 132/300 [00:58<01:14,  2.25it/s]Epoch 26:  44%|████▍     | 133/300 [00:58<01:10,  2.37it/s]Epoch 26:  45%|████▍     | 134/300 [00:59<01:08,  2.43it/s]Epoch 26:  45%|████▌     | 135/300 [00:59<01:08,  2.40it/s]Epoch 26:  45%|████▌     | 136/300 [01:00<01:14,  2.21it/s]Epoch 26:  46%|████▌     | 137/300 [01:00<01:11,  2.27it/s]Epoch 26:  46%|████▌     | 138/300 [01:00<01:08,  2.38it/s]Epoch 26:  46%|████▋     | 139/300 [01:01<01:05,  2.45it/s]06/19/2022 15:25:16 - INFO - __main__ - global step: 3970; train loss: 5.189973831176758; dev loss: 5.1868133544921875
Epoch 26:  47%|████▋     | 140/300 [01:01<01:09,  2.29it/s]Epoch 26:  47%|████▋     | 141/300 [01:02<01:06,  2.38it/s]Epoch 26:  47%|████▋     | 142/300 [01:02<01:04,  2.45it/s]Epoch 26:  48%|████▊     | 143/300 [01:02<01:04,  2.45it/s]Epoch 26:  48%|████▊     | 144/300 [01:03<01:08,  2.26it/s]Epoch 26:  48%|████▊     | 145/300 [01:03<01:05,  2.36it/s]Epoch 26:  49%|████▊     | 146/300 [01:04<01:04,  2.39it/s]Epoch 26:  49%|████▉     | 147/300 [01:04<01:02,  2.44it/s]Epoch 26:  49%|████▉     | 148/300 [01:05<01:05,  2.31it/s]Epoch 26:  50%|████▉     | 149/300 [01:05<01:03,  2.39it/s]Epoch 26:  50%|█████     | 150/300 [01:05<01:01,  2.43it/s]Epoch 26:  50%|█████     | 151/300 [01:06<01:00,  2.47it/s]Epoch 26:  51%|█████     | 152/300 [01:06<01:04,  2.29it/s]Epoch 26:  51%|█████     | 153/300 [01:07<01:01,  2.38it/s]Epoch 26:  51%|█████▏    | 154/300 [01:07<00:59,  2.47it/s]Epoch 26:  52%|█████▏    | 155/300 [01:07<00:57,  2.52it/s]Epoch 26:  52%|█████▏    | 156/300 [01:08<00:59,  2.42it/s]Epoch 26:  52%|█████▏    | 157/300 [01:08<01:02,  2.29it/s]Epoch 26:  53%|█████▎    | 158/300 [01:09<00:59,  2.37it/s]Epoch 26:  53%|█████▎    | 159/300 [01:09<00:57,  2.44it/s]06/19/2022 15:25:25 - INFO - __main__ - global step: 3980; train loss: 5.106112003326416; dev loss: 5.219119548797607
Epoch 26:  53%|█████▎    | 160/300 [01:09<00:56,  2.49it/s]Epoch 26:  54%|█████▎    | 161/300 [01:10<00:58,  2.36it/s]Epoch 26:  54%|█████▍    | 162/300 [01:10<00:56,  2.43it/s]Epoch 26:  54%|█████▍    | 163/300 [01:11<00:55,  2.47it/s]Epoch 26:  55%|█████▍    | 164/300 [01:11<00:58,  2.31it/s]Epoch 26:  55%|█████▌    | 165/300 [01:12<01:01,  2.20it/s]Epoch 26:  55%|█████▌    | 166/300 [01:12<01:00,  2.22it/s]Epoch 26:  56%|█████▌    | 167/300 [01:13<00:57,  2.30it/s]Epoch 26:  56%|█████▌    | 168/300 [01:13<00:55,  2.36it/s]Epoch 26:  56%|█████▋    | 169/300 [01:13<00:58,  2.24it/s]Epoch 26:  57%|█████▋    | 170/300 [01:14<00:55,  2.36it/s]Epoch 26:  57%|█████▋    | 171/300 [01:14<00:53,  2.41it/s]Epoch 26:  57%|█████▋    | 172/300 [01:15<00:51,  2.47it/s]Epoch 26:  58%|█████▊    | 173/300 [01:15<00:54,  2.35it/s]Epoch 26:  58%|█████▊    | 174/300 [01:15<00:53,  2.34it/s]Epoch 26:  58%|█████▊    | 175/300 [01:16<00:52,  2.39it/s]Epoch 26:  59%|█████▊    | 176/300 [01:16<00:50,  2.43it/s]Epoch 26:  59%|█████▉    | 177/300 [01:17<00:57,  2.13it/s]Epoch 26:  59%|█████▉    | 178/300 [01:17<00:57,  2.13it/s]Epoch 26:  60%|█████▉    | 179/300 [01:18<00:56,  2.16it/s]06/19/2022 15:25:33 - INFO - __main__ - global step: 3990; train loss: 4.825623512268066; dev loss: 4.897502899169922
Epoch 26:  60%|██████    | 180/300 [01:18<00:54,  2.21it/s]Epoch 26:  60%|██████    | 181/300 [01:19<00:54,  2.20it/s]Epoch 26:  61%|██████    | 182/300 [01:19<00:58,  2.01it/s]Epoch 26:  61%|██████    | 183/300 [01:20<01:01,  1.92it/s]Epoch 26:  61%|██████▏   | 184/300 [01:20<00:55,  2.09it/s]Epoch 26:  62%|██████▏   | 185/300 [01:21<00:51,  2.25it/s]Epoch 26:  62%|██████▏   | 186/300 [01:21<00:52,  2.19it/s]Epoch 26:  62%|██████▏   | 187/300 [01:21<00:48,  2.33it/s]Epoch 26:  63%|██████▎   | 188/300 [01:22<00:45,  2.44it/s]Epoch 26:  63%|██████▎   | 189/300 [01:22<00:46,  2.40it/s]Epoch 26:  63%|██████▎   | 190/300 [01:23<00:50,  2.19it/s]Epoch 26:  64%|██████▎   | 191/300 [01:23<00:52,  2.08it/s]Epoch 26:  64%|██████▍   | 192/300 [01:24<00:48,  2.21it/s]Epoch 26:  64%|██████▍   | 193/300 [01:24<00:48,  2.22it/s]Epoch 26:  65%|██████▍   | 194/300 [01:25<00:50,  2.11it/s]Epoch 26:  65%|██████▌   | 195/300 [01:25<00:50,  2.09it/s]Epoch 26:  65%|██████▌   | 196/300 [01:26<00:46,  2.23it/s]Epoch 26:  66%|██████▌   | 197/300 [01:26<00:43,  2.35it/s]Epoch 26:  66%|██████▌   | 198/300 [01:27<00:49,  2.07it/s]Epoch 26:  66%|██████▋   | 199/300 [01:27<00:44,  2.25it/s]06/19/2022 15:25:42 - INFO - __main__ - global step: 4000; train loss: 6.084228038787842; dev loss: 6.247879981994629
Epoch 26:  67%|██████▋   | 200/300 [01:27<00:44,  2.25it/s]Epoch 26:  67%|██████▋   | 201/300 [01:28<00:43,  2.29it/s]Epoch 26:  67%|██████▋   | 202/300 [01:28<00:46,  2.13it/s]Epoch 26:  68%|██████▊   | 203/300 [01:29<00:45,  2.12it/s]Epoch 26:  68%|██████▊   | 204/300 [01:29<00:46,  2.06it/s]Epoch 26:  68%|██████▊   | 205/300 [01:30<00:45,  2.08it/s]Epoch 26:  69%|██████▊   | 206/300 [01:30<00:46,  2.04it/s]Epoch 26:  69%|██████▉   | 207/300 [01:31<00:42,  2.20it/s]Epoch 26:  69%|██████▉   | 208/300 [01:31<00:39,  2.36it/s]Epoch 26:  70%|██████▉   | 209/300 [01:31<00:36,  2.49it/s]Epoch 26:  70%|███████   | 210/300 [01:32<00:37,  2.40it/s]Epoch 26:  70%|███████   | 211/300 [01:32<00:42,  2.11it/s]Epoch 26:  71%|███████   | 212/300 [01:33<00:40,  2.17it/s]Epoch 26:  71%|███████   | 213/300 [01:33<00:39,  2.20it/s]Epoch 26:  71%|███████▏  | 214/300 [01:34<00:36,  2.34it/s]Epoch 26:  72%|███████▏  | 215/300 [01:34<00:37,  2.30it/s]Epoch 26:  72%|███████▏  | 216/300 [01:34<00:34,  2.43it/s]Epoch 26:  72%|███████▏  | 217/300 [01:35<00:33,  2.45it/s]Epoch 26:  73%|███████▎  | 218/300 [01:35<00:36,  2.27it/s]Epoch 26:  73%|███████▎  | 219/300 [01:36<00:38,  2.08it/s]06/19/2022 15:25:52 - INFO - __main__ - global step: 4010; train loss: 4.9986467361450195; dev loss: 4.970339775085449
Epoch 26:  73%|███████▎  | 220/300 [01:36<00:39,  2.03it/s]Epoch 26:  74%|███████▎  | 221/300 [01:37<00:38,  2.05it/s]Epoch 26:  74%|███████▍  | 222/300 [01:37<00:36,  2.14it/s]Epoch 26:  74%|███████▍  | 223/300 [01:38<00:36,  2.10it/s]Epoch 26:  75%|███████▍  | 224/300 [01:38<00:35,  2.13it/s]Epoch 26:  75%|███████▌  | 225/300 [01:39<00:34,  2.20it/s]Epoch 26:  75%|███████▌  | 226/300 [01:39<00:33,  2.20it/s]Epoch 26:  76%|███████▌  | 227/300 [01:40<00:36,  2.02it/s]Epoch 26:  76%|███████▌  | 228/300 [01:40<00:34,  2.07it/s]Epoch 26:  76%|███████▋  | 229/300 [01:41<00:34,  2.06it/s]Epoch 26:  77%|███████▋  | 230/300 [01:41<00:34,  2.04it/s]Epoch 26:  77%|███████▋  | 231/300 [01:42<00:33,  2.03it/s]Epoch 26:  77%|███████▋  | 232/300 [01:42<00:31,  2.19it/s]Epoch 26:  78%|███████▊  | 233/300 [01:42<00:28,  2.32it/s]Epoch 26:  78%|███████▊  | 234/300 [01:43<00:27,  2.43it/s]Epoch 26:  78%|███████▊  | 235/300 [01:43<00:27,  2.40it/s]Epoch 26:  79%|███████▊  | 236/300 [01:44<00:28,  2.27it/s]Epoch 26:  79%|███████▉  | 237/300 [01:44<00:26,  2.37it/s]Epoch 26:  79%|███████▉  | 238/300 [01:45<00:25,  2.45it/s]Epoch 26:  80%|███████▉  | 239/300 [01:45<00:24,  2.52it/s]06/19/2022 15:26:00 - INFO - __main__ - global step: 4020; train loss: 5.104393005371094; dev loss: 5.050484657287598
Epoch 26:  80%|████████  | 240/300 [01:45<00:25,  2.37it/s]Epoch 26:  80%|████████  | 241/300 [01:46<00:23,  2.46it/s]Epoch 26:  81%|████████  | 242/300 [01:46<00:22,  2.57it/s]Epoch 26:  81%|████████  | 243/300 [01:46<00:21,  2.63it/s]Epoch 26:  81%|████████▏ | 244/300 [01:47<00:22,  2.44it/s]Epoch 26:  82%|████████▏ | 245/300 [01:47<00:22,  2.44it/s]Epoch 26:  82%|████████▏ | 246/300 [01:48<00:21,  2.51it/s]Epoch 26:  82%|████████▏ | 247/300 [01:48<00:22,  2.40it/s]Epoch 26:  83%|████████▎ | 248/300 [01:49<00:24,  2.14it/s]Epoch 26:  83%|████████▎ | 249/300 [01:49<00:23,  2.15it/s]Epoch 26:  83%|████████▎ | 250/300 [01:50<00:22,  2.24it/s]Epoch 26:  84%|████████▎ | 251/300 [01:50<00:20,  2.40it/s]Epoch 26:  84%|████████▍ | 252/300 [01:50<00:21,  2.28it/s]Epoch 26:  84%|████████▍ | 253/300 [01:51<00:19,  2.39it/s]Epoch 26:  85%|████████▍ | 254/300 [01:51<00:20,  2.26it/s]Epoch 26:  85%|████████▌ | 255/300 [01:52<00:20,  2.18it/s]Epoch 26:  85%|████████▌ | 256/300 [01:52<00:21,  2.01it/s]Epoch 26:  86%|████████▌ | 257/300 [01:53<00:20,  2.09it/s]Epoch 26:  86%|████████▌ | 258/300 [01:53<00:19,  2.17it/s]Epoch 26:  86%|████████▋ | 259/300 [01:54<00:18,  2.24it/s]06/19/2022 15:26:09 - INFO - __main__ - global step: 4030; train loss: 5.049185752868652; dev loss: 4.974106788635254
Epoch 26:  87%|████████▋ | 260/300 [01:54<00:20,  1.94it/s]Epoch 26:  87%|████████▋ | 261/300 [01:55<00:20,  1.92it/s]Epoch 26:  87%|████████▋ | 262/300 [01:55<00:17,  2.12it/s]Epoch 26:  88%|████████▊ | 263/300 [01:56<00:16,  2.28it/s]Epoch 26:  88%|████████▊ | 264/300 [01:56<00:14,  2.44it/s]Epoch 26:  88%|████████▊ | 265/300 [01:56<00:14,  2.34it/s]Epoch 26:  89%|████████▊ | 266/300 [01:57<00:13,  2.45it/s]Epoch 26:  89%|████████▉ | 267/300 [01:57<00:12,  2.56it/s]Epoch 26:  89%|████████▉ | 268/300 [01:57<00:12,  2.65it/s]Epoch 26:  90%|████████▉ | 269/300 [01:58<00:12,  2.50it/s]Epoch 26:  90%|█████████ | 270/300 [01:58<00:11,  2.60it/s]Epoch 26:  90%|█████████ | 271/300 [01:59<00:10,  2.65it/s]Epoch 26:  91%|█████████ | 272/300 [01:59<00:10,  2.72it/s]Epoch 26:  91%|█████████ | 273/300 [01:59<00:10,  2.52it/s]Epoch 26:  91%|█████████▏| 274/300 [02:00<00:09,  2.60it/s]Epoch 26:  92%|█████████▏| 275/300 [02:00<00:09,  2.65it/s]Epoch 26:  92%|█████████▏| 276/300 [02:01<00:09,  2.56it/s]Epoch 26:  92%|█████████▏| 277/300 [02:01<00:09,  2.38it/s]Epoch 26:  93%|█████████▎| 278/300 [02:02<00:09,  2.26it/s]Epoch 26:  93%|█████████▎| 279/300 [02:02<00:08,  2.37it/s]06/19/2022 15:26:18 - INFO - __main__ - global step: 4040; train loss: 4.394155025482178; dev loss: 4.532046794891357
Epoch 26:  93%|█████████▎| 280/300 [02:02<00:08,  2.28it/s]Epoch 26:  94%|█████████▎| 281/300 [02:03<00:09,  2.02it/s]Epoch 26:  94%|█████████▍| 282/300 [02:04<00:08,  2.03it/s]Epoch 26:  94%|█████████▍| 283/300 [02:04<00:08,  2.03it/s]Epoch 26:  95%|█████████▍| 284/300 [02:05<00:08,  1.93it/s]Epoch 26:  95%|█████████▌| 285/300 [02:05<00:07,  1.96it/s]Epoch 26:  95%|█████████▌| 286/300 [02:05<00:06,  2.15it/s]Epoch 26:  96%|█████████▌| 287/300 [02:06<00:05,  2.28it/s]Epoch 26:  96%|█████████▌| 288/300 [02:06<00:04,  2.40it/s]Epoch 26:  96%|█████████▋| 289/300 [02:07<00:04,  2.50it/s]Epoch 26:  97%|█████████▋| 290/300 [02:07<00:04,  2.26it/s]Epoch 26:  97%|█████████▋| 291/300 [02:07<00:03,  2.40it/s]Epoch 26:  97%|█████████▋| 292/300 [02:08<00:03,  2.53it/s]Epoch 26:  98%|█████████▊| 293/300 [02:08<00:02,  2.56it/s]Epoch 26:  98%|█████████▊| 294/300 [02:09<00:02,  2.37it/s]Epoch 26:  98%|█████████▊| 295/300 [02:09<00:02,  2.44it/s]Epoch 26:  99%|█████████▊| 296/300 [02:09<00:01,  2.49it/s]Epoch 26:  99%|█████████▉| 297/300 [02:10<00:01,  2.45it/s]Epoch 26:  99%|█████████▉| 298/300 [02:10<00:00,  2.25it/s]Epoch 26: 100%|█████████▉| 299/300 [02:11<00:00,  2.24it/s]06/19/2022 15:26:26 - INFO - __main__ - global step: 4050; train loss: 4.428675174713135; dev loss: 4.745612144470215
Epoch 26: 100%|██████████| 300/300 [02:11<00:00,  2.23it/s]Epoch 26: 100%|██████████| 300/300 [02:11<00:00,  2.28it/s]
Epoch 27:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 27:   0%|          | 1/300 [00:00<02:12,  2.26it/s]Epoch 27:   1%|          | 2/300 [00:00<02:14,  2.22it/s]Epoch 27:   1%|          | 3/300 [00:01<02:00,  2.47it/s]Epoch 27:   1%|▏         | 4/300 [00:01<01:59,  2.47it/s]Epoch 27:   2%|▏         | 5/300 [00:02<01:59,  2.48it/s]Epoch 27:   2%|▏         | 6/300 [00:02<02:11,  2.24it/s]Epoch 27:   2%|▏         | 7/300 [00:02<02:07,  2.31it/s]Epoch 27:   3%|▎         | 8/300 [00:03<02:00,  2.42it/s]Epoch 27:   3%|▎         | 9/300 [00:03<02:00,  2.42it/s]Epoch 27:   3%|▎         | 10/300 [00:04<02:06,  2.28it/s]Epoch 27:   4%|▎         | 11/300 [00:04<02:04,  2.32it/s]Epoch 27:   4%|▍         | 12/300 [00:05<01:57,  2.45it/s]Epoch 27:   4%|▍         | 13/300 [00:05<01:56,  2.46it/s]Epoch 27:   5%|▍         | 14/300 [00:05<02:04,  2.29it/s]Epoch 27:   5%|▌         | 15/300 [00:06<01:57,  2.42it/s]Epoch 27:   5%|▌         | 16/300 [00:06<01:51,  2.54it/s]Epoch 27:   6%|▌         | 17/300 [00:07<01:47,  2.62it/s]Epoch 27:   6%|▌         | 18/300 [00:07<01:49,  2.58it/s]Epoch 27:   6%|▋         | 19/300 [00:07<01:55,  2.44it/s]06/19/2022 15:26:35 - INFO - __main__ - global step: 4060; train loss: 5.208436012268066; dev loss: 5.1039652824401855
Epoch 27:   7%|▋         | 20/300 [00:08<01:49,  2.55it/s]Epoch 27:   7%|▋         | 21/300 [00:08<01:45,  2.64it/s]Epoch 27:   7%|▋         | 22/300 [00:08<01:42,  2.70it/s]Epoch 27:   8%|▊         | 23/300 [00:09<02:05,  2.20it/s]Epoch 27:   8%|▊         | 24/300 [00:10<02:08,  2.15it/s]Epoch 27:   8%|▊         | 25/300 [00:10<02:11,  2.09it/s]Epoch 27:   9%|▊         | 26/300 [00:11<02:07,  2.14it/s]Epoch 27:   9%|▉         | 27/300 [00:11<02:17,  1.98it/s]Epoch 27:   9%|▉         | 28/300 [00:12<02:13,  2.04it/s]Epoch 27:  10%|▉         | 29/300 [00:12<02:12,  2.04it/s]Epoch 27:  10%|█         | 30/300 [00:12<02:03,  2.18it/s]Epoch 27:  10%|█         | 31/300 [00:13<02:04,  2.17it/s]Epoch 27:  11%|█         | 32/300 [00:13<01:54,  2.35it/s]Epoch 27:  11%|█         | 33/300 [00:14<01:48,  2.47it/s]Epoch 27:  11%|█▏        | 34/300 [00:14<01:46,  2.50it/s]Epoch 27:  12%|█▏        | 35/300 [00:14<01:52,  2.35it/s]Epoch 27:  12%|█▏        | 36/300 [00:15<01:52,  2.35it/s]Epoch 27:  12%|█▏        | 37/300 [00:15<01:45,  2.49it/s]Epoch 27:  13%|█▎        | 38/300 [00:16<01:41,  2.59it/s]Epoch 27:  13%|█▎        | 39/300 [00:16<01:57,  2.21it/s]06/19/2022 15:26:43 - INFO - __main__ - global step: 4070; train loss: 4.7085065841674805; dev loss: 4.645121097564697
Epoch 27:  13%|█▎        | 40/300 [00:17<01:53,  2.30it/s]Epoch 27:  14%|█▎        | 41/300 [00:17<01:57,  2.20it/s]Epoch 27:  14%|█▍        | 42/300 [00:18<01:57,  2.20it/s]Epoch 27:  14%|█▍        | 43/300 [00:18<01:54,  2.24it/s]Epoch 27:  15%|█▍        | 44/300 [00:18<01:59,  2.14it/s]Epoch 27:  15%|█▌        | 45/300 [00:19<01:55,  2.20it/s]Epoch 27:  15%|█▌        | 46/300 [00:19<01:47,  2.36it/s]Epoch 27:  16%|█▌        | 47/300 [00:20<01:42,  2.46it/s]Epoch 27:  16%|█▌        | 48/300 [00:20<01:58,  2.12it/s]Epoch 27:  16%|█▋        | 49/300 [00:21<02:00,  2.09it/s]Epoch 27:  17%|█▋        | 50/300 [00:21<01:57,  2.13it/s]Epoch 27:  17%|█▋        | 51/300 [00:22<01:53,  2.20it/s]Epoch 27:  17%|█▋        | 52/300 [00:22<01:53,  2.19it/s]Epoch 27:  18%|█▊        | 53/300 [00:23<01:51,  2.22it/s]Epoch 27:  18%|█▊        | 54/300 [00:23<01:44,  2.35it/s]Epoch 27:  18%|█▊        | 55/300 [00:23<01:51,  2.21it/s]Epoch 27:  19%|█▊        | 56/300 [00:24<02:06,  1.93it/s]Epoch 27:  19%|█▉        | 57/300 [00:25<02:04,  1.95it/s]Epoch 27:  19%|█▉        | 58/300 [00:25<02:02,  1.98it/s]Epoch 27:  20%|█▉        | 59/300 [00:26<01:59,  2.01it/s]06/19/2022 15:26:53 - INFO - __main__ - global step: 4080; train loss: 4.7493720054626465; dev loss: 4.709689617156982
Epoch 27:  20%|██        | 60/300 [00:26<02:00,  2.00it/s]Epoch 27:  20%|██        | 61/300 [00:26<01:51,  2.14it/s]Epoch 27:  21%|██        | 62/300 [00:27<01:43,  2.30it/s]Epoch 27:  21%|██        | 63/300 [00:27<01:37,  2.43it/s]Epoch 27:  21%|██▏       | 64/300 [00:28<01:44,  2.26it/s]Epoch 27:  22%|██▏       | 65/300 [00:28<01:37,  2.41it/s]Epoch 27:  22%|██▏       | 66/300 [00:28<01:35,  2.46it/s]Epoch 27:  22%|██▏       | 67/300 [00:29<01:34,  2.47it/s]Epoch 27:  23%|██▎       | 68/300 [00:29<01:38,  2.36it/s]Epoch 27:  23%|██▎       | 69/300 [00:30<01:33,  2.48it/s]Epoch 27:  23%|██▎       | 70/300 [00:30<01:29,  2.58it/s]Epoch 27:  24%|██▎       | 71/300 [00:30<01:31,  2.50it/s]Epoch 27:  24%|██▍       | 72/300 [00:31<01:28,  2.58it/s]Epoch 27:  24%|██▍       | 73/300 [00:31<01:38,  2.30it/s]Epoch 27:  25%|██▍       | 74/300 [00:32<01:35,  2.36it/s]Epoch 27:  25%|██▌       | 75/300 [00:32<01:41,  2.22it/s]Epoch 27:  25%|██▌       | 76/300 [00:33<01:52,  1.99it/s]Epoch 27:  26%|██▌       | 77/300 [00:34<02:04,  1.80it/s]Epoch 27:  26%|██▌       | 78/300 [00:34<02:01,  1.82it/s]Epoch 27:  26%|██▋       | 79/300 [00:35<02:02,  1.80it/s]06/19/2022 15:27:02 - INFO - __main__ - global step: 4090; train loss: 5.311369895935059; dev loss: 5.306239128112793
Epoch 27:  27%|██▋       | 80/300 [00:35<02:00,  1.82it/s]Epoch 27:  27%|██▋       | 81/300 [00:36<01:59,  1.83it/s]Epoch 27:  27%|██▋       | 82/300 [00:36<01:48,  2.01it/s]Epoch 27:  28%|██▊       | 83/300 [00:36<01:38,  2.20it/s]Epoch 27:  28%|██▊       | 84/300 [00:37<01:31,  2.37it/s]Epoch 27:  28%|██▊       | 85/300 [00:37<01:32,  2.33it/s]Epoch 27:  29%|██▊       | 86/300 [00:38<01:33,  2.29it/s]Epoch 27:  29%|██▉       | 87/300 [00:38<01:27,  2.44it/s]Epoch 27:  29%|██▉       | 88/300 [00:38<01:25,  2.48it/s]Epoch 27:  30%|██▉       | 89/300 [00:39<01:27,  2.42it/s]Epoch 27:  30%|███       | 90/300 [00:39<01:22,  2.55it/s]Epoch 27:  30%|███       | 91/300 [00:40<01:19,  2.63it/s]Epoch 27:  31%|███       | 92/300 [00:40<01:18,  2.65it/s]Epoch 27:  31%|███       | 93/300 [00:40<01:26,  2.38it/s]Epoch 27:  31%|███▏      | 94/300 [00:41<01:32,  2.23it/s]Epoch 27:  32%|███▏      | 95/300 [00:41<01:32,  2.21it/s]Epoch 27:  32%|███▏      | 96/300 [00:42<01:32,  2.21it/s]Epoch 27:  32%|███▏      | 97/300 [00:42<01:32,  2.20it/s]Epoch 27:  33%|███▎      | 98/300 [00:43<01:40,  2.02it/s]Epoch 27:  33%|███▎      | 99/300 [00:43<01:37,  2.06it/s]06/19/2022 15:27:11 - INFO - __main__ - global step: 4100; train loss: 4.730095863342285; dev loss: 4.717512607574463
Epoch 27:  33%|███▎      | 100/300 [00:44<01:30,  2.20it/s]Epoch 27:  34%|███▎      | 101/300 [00:44<01:23,  2.38it/s]Epoch 27:  34%|███▍      | 102/300 [00:45<01:24,  2.35it/s]Epoch 27:  34%|███▍      | 103/300 [00:45<01:19,  2.49it/s]Epoch 27:  35%|███▍      | 104/300 [00:45<01:15,  2.58it/s]Epoch 27:  35%|███▌      | 105/300 [00:46<01:15,  2.58it/s]Epoch 27:  35%|███▌      | 106/300 [00:46<01:21,  2.39it/s]Epoch 27:  36%|███▌      | 107/300 [00:47<01:18,  2.45it/s]Epoch 27:  36%|███▌      | 108/300 [00:47<01:21,  2.35it/s]Epoch 27:  36%|███▋      | 109/300 [00:47<01:17,  2.47it/s]Epoch 27:  37%|███▋      | 110/300 [00:48<01:23,  2.28it/s]Epoch 27:  37%|███▋      | 111/300 [00:48<01:17,  2.44it/s]Epoch 27:  37%|███▋      | 112/300 [00:49<01:13,  2.57it/s]Epoch 27:  38%|███▊      | 113/300 [00:49<01:10,  2.66it/s]Epoch 27:  38%|███▊      | 114/300 [00:49<01:13,  2.53it/s]Epoch 27:  38%|███▊      | 115/300 [00:50<01:16,  2.41it/s]Epoch 27:  39%|███▊      | 116/300 [00:50<01:14,  2.47it/s]Epoch 27:  39%|███▉      | 117/300 [00:51<01:19,  2.31it/s]Epoch 27:  39%|███▉      | 118/300 [00:51<01:27,  2.08it/s]Epoch 27:  40%|███▉      | 119/300 [00:52<01:24,  2.14it/s]06/19/2022 15:27:19 - INFO - __main__ - global step: 4110; train loss: 4.818802356719971; dev loss: 4.724910736083984
Epoch 27:  40%|████      | 120/300 [00:52<01:23,  2.15it/s]Epoch 27:  40%|████      | 121/300 [00:53<01:25,  2.10it/s]Epoch 27:  41%|████      | 122/300 [00:53<01:29,  1.98it/s]Epoch 27:  41%|████      | 123/300 [00:54<01:20,  2.21it/s]Epoch 27:  41%|████▏     | 124/300 [00:54<01:13,  2.39it/s]Epoch 27:  42%|████▏     | 125/300 [00:54<01:09,  2.52it/s]Epoch 27:  42%|████▏     | 126/300 [00:55<01:06,  2.62it/s]Epoch 27:  42%|████▏     | 127/300 [00:55<01:09,  2.50it/s]Epoch 27:  43%|████▎     | 128/300 [00:55<01:06,  2.57it/s]Epoch 27:  43%|████▎     | 129/300 [00:56<01:04,  2.67it/s]Epoch 27:  43%|████▎     | 130/300 [00:56<01:02,  2.74it/s]Epoch 27:  44%|████▎     | 131/300 [00:57<01:05,  2.58it/s]Epoch 27:  44%|████▍     | 132/300 [00:57<01:02,  2.67it/s]Epoch 27:  44%|████▍     | 133/300 [00:57<01:02,  2.68it/s]Epoch 27:  45%|████▍     | 134/300 [00:58<01:09,  2.38it/s]Epoch 27:  45%|████▌     | 135/300 [00:58<01:21,  2.02it/s]Epoch 27:  45%|████▌     | 136/300 [00:59<01:22,  1.98it/s]Epoch 27:  46%|████▌     | 137/300 [00:59<01:22,  1.97it/s]Epoch 27:  46%|████▌     | 138/300 [01:00<01:23,  1.94it/s]Epoch 27:  46%|████▋     | 139/300 [01:00<01:21,  1.98it/s]06/19/2022 15:27:28 - INFO - __main__ - global step: 4120; train loss: 5.024667263031006; dev loss: 5.133257865905762
Epoch 27:  47%|████▋     | 140/300 [01:01<01:13,  2.17it/s]Epoch 27:  47%|████▋     | 141/300 [01:01<01:07,  2.35it/s]Epoch 27:  47%|████▋     | 142/300 [01:02<01:03,  2.47it/s]Epoch 27:  48%|████▊     | 143/300 [01:02<01:05,  2.39it/s]Epoch 27:  48%|████▊     | 144/300 [01:02<01:02,  2.48it/s]Epoch 27:  48%|████▊     | 145/300 [01:03<00:59,  2.60it/s]Epoch 27:  49%|████▊     | 146/300 [01:03<00:57,  2.69it/s]Epoch 27:  49%|████▉     | 147/300 [01:04<01:02,  2.44it/s]Epoch 27:  49%|████▉     | 148/300 [01:04<01:04,  2.35it/s]Epoch 27:  50%|████▉     | 149/300 [01:04<01:05,  2.30it/s]Epoch 27:  50%|█████     | 150/300 [01:05<01:06,  2.25it/s]Epoch 27:  50%|█████     | 151/300 [01:05<01:07,  2.21it/s]Epoch 27:  51%|█████     | 152/300 [01:06<01:12,  2.04it/s]Epoch 27:  51%|█████     | 153/300 [01:06<01:12,  2.03it/s]Epoch 27:  51%|█████▏    | 154/300 [01:07<01:12,  2.02it/s]Epoch 27:  52%|█████▏    | 155/300 [01:07<01:05,  2.22it/s]Epoch 27:  52%|█████▏    | 156/300 [01:08<01:05,  2.21it/s]Epoch 27:  52%|█████▏    | 157/300 [01:08<01:01,  2.33it/s]Epoch 27:  53%|█████▎    | 158/300 [01:09<00:58,  2.45it/s]Epoch 27:  53%|█████▎    | 159/300 [01:09<00:58,  2.41it/s]06/19/2022 15:27:36 - INFO - __main__ - global step: 4130; train loss: 4.7988481521606445; dev loss: 4.708256244659424
Epoch 27:  53%|█████▎    | 160/300 [01:10<01:08,  2.06it/s]Epoch 27:  54%|█████▎    | 161/300 [01:10<01:05,  2.13it/s]Epoch 27:  54%|█████▍    | 162/300 [01:10<01:01,  2.25it/s]Epoch 27:  54%|█████▍    | 163/300 [01:11<01:02,  2.20it/s]Epoch 27:  55%|█████▍    | 164/300 [01:11<01:06,  2.04it/s]Epoch 27:  55%|█████▌    | 165/300 [01:12<01:03,  2.13it/s]Epoch 27:  55%|█████▌    | 166/300 [01:12<01:01,  2.16it/s]Epoch 27:  56%|█████▌    | 167/300 [01:13<00:56,  2.34it/s]Epoch 27:  56%|█████▌    | 168/300 [01:13<00:57,  2.28it/s]Epoch 27:  56%|█████▋    | 169/300 [01:14<00:53,  2.43it/s]Epoch 27:  57%|█████▋    | 170/300 [01:14<00:50,  2.55it/s]Epoch 27:  57%|█████▋    | 171/300 [01:14<00:48,  2.65it/s]Epoch 27:  57%|█████▋    | 172/300 [01:15<00:54,  2.34it/s]Epoch 27:  58%|█████▊    | 173/300 [01:15<00:51,  2.47it/s]Epoch 27:  58%|█████▊    | 174/300 [01:15<00:50,  2.51it/s]Epoch 27:  58%|█████▊    | 175/300 [01:16<00:49,  2.53it/s]Epoch 27:  59%|█████▊    | 176/300 [01:16<00:56,  2.21it/s]Epoch 27:  59%|█████▉    | 177/300 [01:17<00:57,  2.15it/s]Epoch 27:  59%|█████▉    | 178/300 [01:17<00:55,  2.18it/s]Epoch 27:  60%|█████▉    | 179/300 [01:18<00:59,  2.03it/s]06/19/2022 15:27:45 - INFO - __main__ - global step: 4140; train loss: 4.253474712371826; dev loss: 4.344277381896973
Epoch 27:  60%|██████    | 180/300 [01:18<01:00,  1.99it/s]Epoch 27:  60%|██████    | 181/300 [01:19<01:08,  1.74it/s]Epoch 27:  61%|██████    | 182/300 [01:20<01:03,  1.86it/s]Epoch 27:  61%|██████    | 183/300 [01:20<01:01,  1.91it/s]Epoch 27:  61%|██████▏   | 184/300 [01:21<00:58,  1.98it/s]Epoch 27:  62%|██████▏   | 185/300 [01:21<01:01,  1.87it/s]Epoch 27:  62%|██████▏   | 186/300 [01:22<00:57,  1.99it/s]Epoch 27:  62%|██████▏   | 187/300 [01:22<00:55,  2.05it/s]Epoch 27:  63%|██████▎   | 188/300 [01:23<00:54,  2.07it/s]Epoch 27:  63%|██████▎   | 189/300 [01:23<00:55,  1.99it/s]Epoch 27:  63%|██████▎   | 190/300 [01:23<00:50,  2.18it/s]Epoch 27:  64%|██████▎   | 191/300 [01:24<00:46,  2.36it/s]Epoch 27:  64%|██████▍   | 192/300 [01:24<00:45,  2.37it/s]Epoch 27:  64%|██████▍   | 193/300 [01:25<00:47,  2.25it/s]Epoch 27:  65%|██████▍   | 194/300 [01:25<00:49,  2.12it/s]Epoch 27:  65%|██████▌   | 195/300 [01:26<00:48,  2.16it/s]Epoch 27:  65%|██████▌   | 196/300 [01:26<00:48,  2.13it/s]Epoch 27:  66%|██████▌   | 197/300 [01:27<00:52,  1.96it/s]Epoch 27:  66%|██████▌   | 198/300 [01:27<00:47,  2.16it/s]Epoch 27:  66%|██████▋   | 199/300 [01:28<00:43,  2.34it/s]06/19/2022 15:27:55 - INFO - __main__ - global step: 4150; train loss: 4.925047874450684; dev loss: 4.972362041473389
Epoch 27:  67%|██████▋   | 200/300 [01:28<00:40,  2.47it/s]Epoch 27:  67%|██████▋   | 201/300 [01:28<00:42,  2.34it/s]Epoch 27:  67%|██████▋   | 202/300 [01:29<00:41,  2.38it/s]Epoch 27:  68%|██████▊   | 203/300 [01:29<00:42,  2.29it/s]Epoch 27:  68%|██████▊   | 204/300 [01:30<00:45,  2.10it/s]Epoch 27:  68%|██████▊   | 205/300 [01:30<00:44,  2.11it/s]Epoch 27:  69%|██████▊   | 206/300 [01:31<00:46,  2.04it/s]Epoch 27:  69%|██████▉   | 207/300 [01:31<00:48,  1.93it/s]Epoch 27:  69%|██████▉   | 208/300 [01:32<00:44,  2.09it/s]Epoch 27:  70%|██████▉   | 209/300 [01:32<00:41,  2.18it/s]Epoch 27:  70%|███████   | 210/300 [01:33<00:42,  2.13it/s]Epoch 27:  70%|███████   | 211/300 [01:33<00:38,  2.31it/s]Epoch 27:  71%|███████   | 212/300 [01:33<00:35,  2.45it/s]Epoch 27:  71%|███████   | 213/300 [01:34<00:33,  2.57it/s]Epoch 27:  71%|███████▏  | 214/300 [01:34<00:35,  2.45it/s]Epoch 27:  72%|███████▏  | 215/300 [01:34<00:33,  2.56it/s]Epoch 27:  72%|███████▏  | 216/300 [01:35<00:32,  2.62it/s]Epoch 27:  72%|███████▏  | 217/300 [01:35<00:33,  2.49it/s]Epoch 27:  73%|███████▎  | 218/300 [01:36<00:39,  2.10it/s]Epoch 27:  73%|███████▎  | 219/300 [01:36<00:38,  2.09it/s]06/19/2022 15:28:04 - INFO - __main__ - global step: 4160; train loss: 5.230193614959717; dev loss: 5.149402141571045
Epoch 27:  73%|███████▎  | 220/300 [01:37<00:38,  2.09it/s]Epoch 27:  74%|███████▎  | 221/300 [01:37<00:37,  2.11it/s]Epoch 27:  74%|███████▍  | 222/300 [01:38<00:41,  1.90it/s]Epoch 27:  74%|███████▍  | 223/300 [01:39<00:41,  1.87it/s]Epoch 27:  75%|███████▍  | 224/300 [01:39<00:37,  2.01it/s]Epoch 27:  75%|███████▌  | 225/300 [01:40<00:38,  1.95it/s]Epoch 27:  75%|███████▌  | 226/300 [01:40<00:38,  1.94it/s]Epoch 27:  76%|███████▌  | 227/300 [01:41<00:37,  1.96it/s]Epoch 27:  76%|███████▌  | 228/300 [01:41<00:37,  1.91it/s]Epoch 27:  76%|███████▋  | 229/300 [01:42<00:36,  1.94it/s]Epoch 27:  77%|███████▋  | 230/300 [01:42<00:36,  1.91it/s]Epoch 27:  77%|███████▋  | 231/300 [01:43<00:34,  2.00it/s]Epoch 27:  77%|███████▋  | 232/300 [01:43<00:31,  2.13it/s]Epoch 27:  78%|███████▊  | 233/300 [01:43<00:28,  2.32it/s]Epoch 27:  78%|███████▊  | 234/300 [01:44<00:27,  2.38it/s]Epoch 27:  78%|███████▊  | 235/300 [01:44<00:28,  2.29it/s]Epoch 27:  79%|███████▊  | 236/300 [01:45<00:25,  2.47it/s]Epoch 27:  79%|███████▉  | 237/300 [01:45<00:24,  2.56it/s]Epoch 27:  79%|███████▉  | 238/300 [01:45<00:23,  2.63it/s]Epoch 27:  80%|███████▉  | 239/300 [01:46<00:24,  2.45it/s]06/19/2022 15:28:13 - INFO - __main__ - global step: 4170; train loss: 4.815447807312012; dev loss: 5.005204677581787
Epoch 27:  80%|████████  | 240/300 [01:46<00:23,  2.54it/s]Epoch 27:  80%|████████  | 241/300 [01:46<00:23,  2.54it/s]Epoch 27:  81%|████████  | 242/300 [01:47<00:23,  2.46it/s]Epoch 27:  81%|████████  | 243/300 [01:47<00:24,  2.36it/s]Epoch 27:  81%|████████▏ | 244/300 [01:48<00:23,  2.35it/s]Epoch 27:  82%|████████▏ | 245/300 [01:48<00:22,  2.48it/s]Epoch 27:  82%|████████▏ | 246/300 [01:49<00:22,  2.43it/s]Epoch 27:  82%|████████▏ | 247/300 [01:49<00:24,  2.16it/s]Epoch 27:  83%|████████▎ | 248/300 [01:50<00:24,  2.09it/s]Epoch 27:  83%|████████▎ | 249/300 [01:50<00:24,  2.12it/s]Epoch 27:  83%|████████▎ | 250/300 [01:51<00:23,  2.12it/s]Epoch 27:  84%|████████▎ | 251/300 [01:51<00:24,  2.02it/s]Epoch 27:  84%|████████▍ | 252/300 [01:52<00:21,  2.21it/s]Epoch 27:  84%|████████▍ | 253/300 [01:52<00:19,  2.37it/s]Epoch 27:  85%|████████▍ | 254/300 [01:52<00:18,  2.49it/s]Epoch 27:  85%|████████▌ | 255/300 [01:53<00:19,  2.35it/s]Epoch 27:  85%|████████▌ | 256/300 [01:53<00:19,  2.30it/s]Epoch 27:  86%|████████▌ | 257/300 [01:54<00:18,  2.34it/s]Epoch 27:  86%|████████▌ | 258/300 [01:54<00:18,  2.27it/s]Epoch 27:  86%|████████▋ | 259/300 [01:54<00:17,  2.28it/s]06/19/2022 15:28:22 - INFO - __main__ - global step: 4180; train loss: 4.285191535949707; dev loss: 4.402475833892822
Epoch 27:  87%|████████▋ | 260/300 [01:55<00:19,  2.01it/s]Epoch 27:  87%|████████▋ | 261/300 [01:55<00:17,  2.19it/s]Epoch 27:  87%|████████▋ | 262/300 [01:56<00:16,  2.28it/s]Epoch 27:  88%|████████▊ | 263/300 [01:56<00:16,  2.30it/s]Epoch 27:  88%|████████▊ | 264/300 [01:57<00:16,  2.14it/s]Epoch 27:  88%|████████▊ | 265/300 [01:57<00:15,  2.25it/s]Epoch 27:  89%|████████▊ | 266/300 [01:58<00:14,  2.33it/s]Epoch 27:  89%|████████▉ | 267/300 [01:58<00:14,  2.34it/s]Epoch 27:  89%|████████▉ | 268/300 [01:59<00:14,  2.15it/s]Epoch 27:  90%|████████▉ | 269/300 [01:59<00:14,  2.15it/s]Epoch 27:  90%|█████████ | 270/300 [02:00<00:13,  2.16it/s]Epoch 27:  90%|█████████ | 271/300 [02:00<00:13,  2.16it/s]Epoch 27:  91%|█████████ | 272/300 [02:01<00:13,  2.04it/s]Epoch 27:  91%|█████████ | 273/300 [02:01<00:13,  2.07it/s]Epoch 27:  91%|█████████▏| 274/300 [02:01<00:12,  2.10it/s]Epoch 27:  92%|█████████▏| 275/300 [02:02<00:12,  2.04it/s]Epoch 27:  92%|█████████▏| 276/300 [02:02<00:11,  2.04it/s]Epoch 27:  92%|█████████▏| 277/300 [02:03<00:10,  2.17it/s]Epoch 27:  93%|█████████▎| 278/300 [02:03<00:10,  2.10it/s]Epoch 27:  93%|█████████▎| 279/300 [02:04<00:09,  2.23it/s]06/19/2022 15:28:31 - INFO - __main__ - global step: 4190; train loss: 4.652760982513428; dev loss: 4.761094570159912
Epoch 27:  93%|█████████▎| 280/300 [02:04<00:09,  2.17it/s]Epoch 27:  94%|█████████▎| 281/300 [02:05<00:08,  2.33it/s]Epoch 27:  94%|█████████▍| 282/300 [02:05<00:07,  2.43it/s]Epoch 27:  94%|█████████▍| 283/300 [02:05<00:06,  2.53it/s]Epoch 27:  95%|█████████▍| 284/300 [02:06<00:06,  2.40it/s]Epoch 27:  95%|█████████▌| 285/300 [02:06<00:06,  2.43it/s]Epoch 27:  95%|█████████▌| 286/300 [02:07<00:05,  2.51it/s]Epoch 27:  96%|█████████▌| 287/300 [02:07<00:05,  2.50it/s]Epoch 27:  96%|█████████▌| 288/300 [02:07<00:04,  2.47it/s]Epoch 27:  96%|█████████▋| 289/300 [02:08<00:04,  2.35it/s]Epoch 27:  97%|█████████▋| 290/300 [02:08<00:04,  2.43it/s]Epoch 27:  97%|█████████▋| 291/300 [02:09<00:03,  2.54it/s]Epoch 27:  97%|█████████▋| 292/300 [02:09<00:03,  2.57it/s]Epoch 27:  98%|█████████▊| 293/300 [02:10<00:03,  2.23it/s]Epoch 27:  98%|█████████▊| 294/300 [02:10<00:02,  2.21it/s]Epoch 27:  98%|█████████▊| 295/300 [02:10<00:02,  2.33it/s]Epoch 27:  99%|█████████▊| 296/300 [02:11<00:01,  2.40it/s]Epoch 27:  99%|█████████▉| 297/300 [02:11<00:01,  2.30it/s]Epoch 27:  99%|█████████▉| 298/300 [02:12<00:00,  2.12it/s]Epoch 27: 100%|█████████▉| 299/300 [02:12<00:00,  2.12it/s]06/19/2022 15:28:40 - INFO - __main__ - global step: 4200; train loss: 4.848665237426758; dev loss: 5.079745292663574
Epoch 27: 100%|██████████| 300/300 [02:13<00:00,  2.05it/s]Epoch 27: 100%|██████████| 300/300 [02:13<00:00,  2.25it/s]
Epoch 28:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 28:   0%|          | 1/300 [00:00<02:20,  2.13it/s]Epoch 28:   1%|          | 2/300 [00:00<02:00,  2.48it/s]Epoch 28:   1%|          | 3/300 [00:01<02:05,  2.36it/s]Epoch 28:   1%|▏         | 4/300 [00:01<02:06,  2.33it/s]Epoch 28:   2%|▏         | 5/300 [00:02<02:14,  2.19it/s]Epoch 28:   2%|▏         | 6/300 [00:02<02:12,  2.22it/s]Epoch 28:   2%|▏         | 7/300 [00:03<02:11,  2.23it/s]Epoch 28:   3%|▎         | 8/300 [00:03<02:19,  2.09it/s]Epoch 28:   3%|▎         | 9/300 [00:04<02:26,  1.98it/s]Epoch 28:   3%|▎         | 10/300 [00:04<02:15,  2.14it/s]Epoch 28:   4%|▎         | 11/300 [00:04<02:07,  2.26it/s]Epoch 28:   4%|▍         | 12/300 [00:05<02:16,  2.11it/s]Epoch 28:   4%|▍         | 13/300 [00:05<02:06,  2.27it/s]Epoch 28:   5%|▍         | 14/300 [00:06<02:11,  2.18it/s]Epoch 28:   5%|▌         | 15/300 [00:06<02:02,  2.32it/s]Epoch 28:   5%|▌         | 16/300 [00:07<01:56,  2.44it/s]Epoch 28:   6%|▌         | 17/300 [00:07<01:54,  2.47it/s]Epoch 28:   6%|▌         | 18/300 [00:08<02:02,  2.31it/s]Epoch 28:   6%|▋         | 19/300 [00:08<02:01,  2.32it/s]06/19/2022 15:28:49 - INFO - __main__ - global step: 4210; train loss: 4.419395923614502; dev loss: 4.448945045471191
Epoch 28:   7%|▋         | 20/300 [00:08<02:10,  2.15it/s]Epoch 28:   7%|▋         | 21/300 [00:09<02:05,  2.22it/s]Epoch 28:   7%|▋         | 22/300 [00:09<02:05,  2.21it/s]Epoch 28:   8%|▊         | 23/300 [00:10<02:00,  2.30it/s]Epoch 28:   8%|▊         | 24/300 [00:10<01:53,  2.42it/s]Epoch 28:   8%|▊         | 25/300 [00:11<02:01,  2.25it/s]Epoch 28:   9%|▊         | 26/300 [00:11<02:14,  2.03it/s]Epoch 28:   9%|▉         | 27/300 [00:12<02:12,  2.06it/s]Epoch 28:   9%|▉         | 28/300 [00:12<02:08,  2.11it/s]Epoch 28:  10%|▉         | 29/300 [00:12<01:58,  2.28it/s]Epoch 28:  10%|█         | 30/300 [00:13<02:03,  2.18it/s]Epoch 28:  10%|█         | 31/300 [00:13<01:56,  2.32it/s]Epoch 28:  11%|█         | 32/300 [00:14<01:50,  2.43it/s]Epoch 28:  11%|█         | 33/300 [00:14<01:48,  2.46it/s]Epoch 28:  11%|█▏        | 34/300 [00:15<01:53,  2.35it/s]Epoch 28:  12%|█▏        | 35/300 [00:15<01:46,  2.48it/s]Epoch 28:  12%|█▏        | 36/300 [00:15<01:42,  2.57it/s]Epoch 28:  12%|█▏        | 37/300 [00:16<01:41,  2.58it/s]Epoch 28:  13%|█▎        | 38/300 [00:16<02:04,  2.10it/s]Epoch 28:  13%|█▎        | 39/300 [00:17<02:01,  2.15it/s]06/19/2022 15:28:58 - INFO - __main__ - global step: 4220; train loss: 3.6890053749084473; dev loss: 3.769664764404297
Epoch 28:  13%|█▎        | 40/300 [00:17<02:04,  2.09it/s]Epoch 28:  14%|█▎        | 41/300 [00:18<02:04,  2.09it/s]Epoch 28:  14%|█▍        | 42/300 [00:18<01:54,  2.26it/s]Epoch 28:  14%|█▍        | 43/300 [00:19<01:56,  2.21it/s]Epoch 28:  15%|█▍        | 44/300 [00:19<01:48,  2.36it/s]Epoch 28:  15%|█▌        | 45/300 [00:19<01:42,  2.49it/s]Epoch 28:  15%|█▌        | 46/300 [00:20<01:38,  2.58it/s]Epoch 28:  16%|█▌        | 47/300 [00:20<01:43,  2.46it/s]Epoch 28:  16%|█▌        | 48/300 [00:21<01:40,  2.50it/s]Epoch 28:  16%|█▋        | 49/300 [00:21<01:36,  2.59it/s]Epoch 28:  17%|█▋        | 50/300 [00:21<01:37,  2.57it/s]Epoch 28:  17%|█▋        | 51/300 [00:22<01:52,  2.21it/s]Epoch 28:  17%|█▋        | 52/300 [00:22<01:53,  2.19it/s]Epoch 28:  18%|█▊        | 53/300 [00:23<01:53,  2.18it/s]Epoch 28:  18%|█▊        | 54/300 [00:23<01:57,  2.09it/s]Epoch 28:  18%|█▊        | 55/300 [00:24<02:10,  1.88it/s]Epoch 28:  19%|█▊        | 56/300 [00:24<02:01,  2.01it/s]Epoch 28:  19%|█▉        | 57/300 [00:25<01:50,  2.20it/s]Epoch 28:  19%|█▉        | 58/300 [00:25<01:42,  2.35it/s]Epoch 28:  20%|█▉        | 59/300 [00:26<01:45,  2.29it/s]06/19/2022 15:29:06 - INFO - __main__ - global step: 4230; train loss: 5.059700965881348; dev loss: 4.997715950012207
Epoch 28:  20%|██        | 60/300 [00:26<01:39,  2.40it/s]Epoch 28:  20%|██        | 61/300 [00:26<01:45,  2.27it/s]Epoch 28:  21%|██        | 62/300 [00:27<01:50,  2.15it/s]Epoch 28:  21%|██        | 63/300 [00:28<02:01,  1.95it/s]Epoch 28:  21%|██▏       | 64/300 [00:28<01:58,  1.99it/s]Epoch 28:  22%|██▏       | 65/300 [00:29<02:00,  1.95it/s]Epoch 28:  22%|██▏       | 66/300 [00:29<01:56,  2.01it/s]Epoch 28:  22%|██▏       | 67/300 [00:30<01:57,  1.98it/s]Epoch 28:  23%|██▎       | 68/300 [00:30<02:05,  1.85it/s]Epoch 28:  23%|██▎       | 69/300 [00:31<02:00,  1.92it/s]Epoch 28:  23%|██▎       | 70/300 [00:31<01:55,  1.99it/s]Epoch 28:  24%|██▎       | 71/300 [00:32<01:51,  2.06it/s]Epoch 28:  24%|██▍       | 72/300 [00:32<01:50,  2.06it/s]Epoch 28:  24%|██▍       | 73/300 [00:32<01:41,  2.25it/s]Epoch 28:  25%|██▍       | 74/300 [00:33<01:35,  2.38it/s]Epoch 28:  25%|██▌       | 75/300 [00:33<01:30,  2.49it/s]Epoch 28:  25%|██▌       | 76/300 [00:34<01:34,  2.38it/s]Epoch 28:  26%|██▌       | 77/300 [00:34<01:33,  2.39it/s]Epoch 28:  26%|██▌       | 78/300 [00:34<01:29,  2.49it/s]Epoch 28:  26%|██▋       | 79/300 [00:35<01:28,  2.51it/s]06/19/2022 15:29:15 - INFO - __main__ - global step: 4240; train loss: 4.466604709625244; dev loss: 4.672266960144043
Epoch 28:  27%|██▋       | 80/300 [00:35<01:32,  2.38it/s]Epoch 28:  27%|██▋       | 81/300 [00:36<01:27,  2.50it/s]Epoch 28:  27%|██▋       | 82/300 [00:36<01:24,  2.59it/s]Epoch 28:  28%|██▊       | 83/300 [00:36<01:21,  2.66it/s]Epoch 28:  28%|██▊       | 84/300 [00:37<01:27,  2.46it/s]Epoch 28:  28%|██▊       | 85/300 [00:37<01:31,  2.34it/s]Epoch 28:  29%|██▊       | 86/300 [00:38<01:33,  2.29it/s]Epoch 28:  29%|██▉       | 87/300 [00:38<01:33,  2.29it/s]Epoch 28:  29%|██▉       | 88/300 [00:39<01:41,  2.10it/s]Epoch 28:  30%|██▉       | 89/300 [00:39<01:39,  2.12it/s]Epoch 28:  30%|███       | 90/300 [00:40<01:38,  2.14it/s]Epoch 28:  30%|███       | 91/300 [00:40<01:36,  2.16it/s]Epoch 28:  31%|███       | 92/300 [00:41<01:36,  2.15it/s]Epoch 28:  31%|███       | 93/300 [00:41<01:29,  2.32it/s]Epoch 28:  31%|███▏      | 94/300 [00:41<01:25,  2.40it/s]Epoch 28:  32%|███▏      | 95/300 [00:42<01:22,  2.50it/s]Epoch 28:  32%|███▏      | 96/300 [00:42<01:20,  2.52it/s]Epoch 28:  32%|███▏      | 97/300 [00:43<01:33,  2.16it/s]Epoch 28:  33%|███▎      | 98/300 [00:43<01:32,  2.18it/s]Epoch 28:  33%|███▎      | 99/300 [00:44<01:32,  2.17it/s]06/19/2022 15:29:24 - INFO - __main__ - global step: 4250; train loss: 4.59788703918457; dev loss: 4.6085991859436035
Epoch 28:  33%|███▎      | 100/300 [00:44<01:27,  2.28it/s]Epoch 28:  34%|███▎      | 101/300 [00:44<01:29,  2.23it/s]Epoch 28:  34%|███▍      | 102/300 [00:45<01:24,  2.35it/s]Epoch 28:  34%|███▍      | 103/300 [00:45<01:21,  2.42it/s]Epoch 28:  35%|███▍      | 104/300 [00:46<01:21,  2.41it/s]Epoch 28:  35%|███▌      | 105/300 [00:46<01:28,  2.20it/s]Epoch 28:  35%|███▌      | 106/300 [00:47<01:24,  2.29it/s]Epoch 28:  36%|███▌      | 107/300 [00:47<01:21,  2.37it/s]Epoch 28:  36%|███▌      | 108/300 [00:47<01:21,  2.37it/s]Epoch 28:  36%|███▋      | 109/300 [00:48<01:25,  2.23it/s]Epoch 28:  37%|███▋      | 110/300 [00:48<01:25,  2.23it/s]Epoch 28:  37%|███▋      | 111/300 [00:49<01:25,  2.22it/s]Epoch 28:  37%|███▋      | 112/300 [00:49<01:25,  2.21it/s]Epoch 28:  38%|███▊      | 113/300 [00:50<01:30,  2.08it/s]Epoch 28:  38%|███▊      | 114/300 [00:50<01:23,  2.24it/s]Epoch 28:  38%|███▊      | 115/300 [00:51<01:25,  2.17it/s]Epoch 28:  39%|███▊      | 116/300 [00:51<01:25,  2.15it/s]Epoch 28:  39%|███▉      | 117/300 [00:52<01:35,  1.91it/s]Epoch 28:  39%|███▉      | 118/300 [00:52<01:33,  1.94it/s]Epoch 28:  40%|███▉      | 119/300 [00:53<01:29,  2.03it/s]06/19/2022 15:29:33 - INFO - __main__ - global step: 4260; train loss: 4.843195915222168; dev loss: 5.02728271484375
Epoch 28:  40%|████      | 120/300 [00:53<01:28,  2.04it/s]Epoch 28:  40%|████      | 121/300 [00:54<01:23,  2.14it/s]Epoch 28:  41%|████      | 122/300 [00:54<01:24,  2.11it/s]Epoch 28:  41%|████      | 123/300 [00:55<01:21,  2.17it/s]Epoch 28:  41%|████▏     | 124/300 [00:55<01:17,  2.27it/s]Epoch 28:  42%|████▏     | 125/300 [00:55<01:16,  2.28it/s]Epoch 28:  42%|████▏     | 126/300 [00:56<01:19,  2.19it/s]Epoch 28:  42%|████▏     | 127/300 [00:56<01:13,  2.35it/s]Epoch 28:  43%|████▎     | 128/300 [00:57<01:11,  2.41it/s]Epoch 28:  43%|████▎     | 129/300 [00:57<01:09,  2.47it/s]Epoch 28:  43%|████▎     | 130/300 [00:58<01:18,  2.16it/s]Epoch 28:  44%|████▎     | 131/300 [00:58<01:18,  2.16it/s]Epoch 28:  44%|████▍     | 132/300 [00:59<01:18,  2.15it/s]Epoch 28:  44%|████▍     | 133/300 [00:59<01:13,  2.29it/s]Epoch 28:  45%|████▍     | 134/300 [00:59<01:13,  2.25it/s]Epoch 28:  45%|████▌     | 135/300 [01:00<01:10,  2.35it/s]Epoch 28:  45%|████▌     | 136/300 [01:00<01:06,  2.48it/s]Epoch 28:  46%|████▌     | 137/300 [01:01<01:06,  2.46it/s]Epoch 28:  46%|████▌     | 138/300 [01:01<01:17,  2.10it/s]Epoch 28:  46%|████▋     | 139/300 [01:02<01:16,  2.10it/s]06/19/2022 15:29:42 - INFO - __main__ - global step: 4270; train loss: 4.391180992126465; dev loss: 4.276806354522705
Epoch 28:  47%|████▋     | 140/300 [01:02<01:18,  2.03it/s]Epoch 28:  47%|████▋     | 141/300 [01:03<01:12,  2.20it/s]Epoch 28:  47%|████▋     | 142/300 [01:03<01:12,  2.19it/s]Epoch 28:  48%|████▊     | 143/300 [01:03<01:06,  2.35it/s]Epoch 28:  48%|████▊     | 144/300 [01:04<01:05,  2.38it/s]Epoch 28:  48%|████▊     | 145/300 [01:04<01:06,  2.34it/s]Epoch 28:  49%|████▊     | 146/300 [01:05<01:08,  2.24it/s]Epoch 28:  49%|████▉     | 147/300 [01:05<01:11,  2.15it/s]Epoch 28:  49%|████▉     | 148/300 [01:06<01:10,  2.16it/s]Epoch 28:  50%|████▉     | 149/300 [01:06<01:11,  2.12it/s]Epoch 28:  50%|█████     | 150/300 [01:07<01:06,  2.25it/s]Epoch 28:  50%|█████     | 151/300 [01:07<01:15,  1.97it/s]Epoch 28:  51%|█████     | 152/300 [01:08<01:13,  2.01it/s]Epoch 28:  51%|█████     | 153/300 [01:08<01:16,  1.92it/s]Epoch 28:  51%|█████▏    | 154/300 [01:09<01:14,  1.97it/s]Epoch 28:  52%|█████▏    | 155/300 [01:09<01:18,  1.84it/s]Epoch 28:  52%|█████▏    | 156/300 [01:10<01:16,  1.87it/s]Epoch 28:  52%|█████▏    | 157/300 [01:10<01:14,  1.91it/s]Epoch 28:  53%|█████▎    | 158/300 [01:11<01:15,  1.88it/s]Epoch 28:  53%|█████▎    | 159/300 [01:12<01:18,  1.79it/s]06/19/2022 15:29:52 - INFO - __main__ - global step: 4280; train loss: 5.015781402587891; dev loss: 4.7361602783203125
Epoch 28:  53%|█████▎    | 160/300 [01:12<01:15,  1.86it/s]Epoch 28:  54%|█████▎    | 161/300 [01:12<01:10,  1.97it/s]Epoch 28:  54%|█████▍    | 162/300 [01:13<01:06,  2.09it/s]Epoch 28:  54%|█████▍    | 163/300 [01:14<01:12,  1.88it/s]Epoch 28:  55%|█████▍    | 164/300 [01:14<01:05,  2.08it/s]Epoch 28:  55%|█████▌    | 165/300 [01:14<01:00,  2.24it/s]Epoch 28:  55%|█████▌    | 166/300 [01:15<00:56,  2.37it/s]Epoch 28:  56%|█████▌    | 167/300 [01:15<00:58,  2.27it/s]Epoch 28:  56%|█████▌    | 168/300 [01:15<00:55,  2.39it/s]Epoch 28:  56%|█████▋    | 169/300 [01:16<00:52,  2.50it/s]Epoch 28:  57%|█████▋    | 170/300 [01:16<00:51,  2.53it/s]Epoch 28:  57%|█████▋    | 171/300 [01:17<00:55,  2.32it/s]Epoch 28:  57%|█████▋    | 172/300 [01:17<00:54,  2.34it/s]Epoch 28:  58%|█████▊    | 173/300 [01:17<00:51,  2.45it/s]Epoch 28:  58%|█████▊    | 174/300 [01:18<00:49,  2.55it/s]Epoch 28:  58%|█████▊    | 175/300 [01:18<00:47,  2.62it/s]Epoch 28:  59%|█████▊    | 176/300 [01:19<00:51,  2.42it/s]Epoch 28:  59%|█████▉    | 177/300 [01:19<00:50,  2.46it/s]Epoch 28:  59%|█████▉    | 178/300 [01:19<00:49,  2.45it/s]Epoch 28:  60%|█████▉    | 179/300 [01:20<00:47,  2.54it/s]06/19/2022 15:30:01 - INFO - __main__ - global step: 4290; train loss: 4.662728786468506; dev loss: 4.805110454559326
Epoch 28:  60%|██████    | 180/300 [01:21<00:57,  2.09it/s]Epoch 28:  60%|██████    | 181/300 [01:21<00:54,  2.17it/s]Epoch 28:  61%|██████    | 182/300 [01:21<00:56,  2.08it/s]Epoch 28:  61%|██████    | 183/300 [01:22<00:56,  2.06it/s]Epoch 28:  61%|██████▏   | 184/300 [01:23<01:00,  1.90it/s]Epoch 28:  62%|██████▏   | 185/300 [01:23<00:56,  2.03it/s]Epoch 28:  62%|██████▏   | 186/300 [01:23<00:51,  2.20it/s]Epoch 28:  62%|██████▏   | 187/300 [01:24<00:48,  2.32it/s]Epoch 28:  63%|██████▎   | 188/300 [01:24<00:50,  2.22it/s]Epoch 28:  63%|██████▎   | 189/300 [01:25<00:49,  2.24it/s]Epoch 28:  63%|██████▎   | 190/300 [01:25<00:50,  2.16it/s]Epoch 28:  64%|██████▎   | 191/300 [01:26<00:49,  2.18it/s]Epoch 28:  64%|██████▍   | 192/300 [01:26<00:53,  2.01it/s]Epoch 28:  64%|██████▍   | 193/300 [01:27<00:49,  2.15it/s]Epoch 28:  65%|██████▍   | 194/300 [01:27<00:48,  2.20it/s]Epoch 28:  65%|██████▌   | 195/300 [01:28<00:48,  2.16it/s]Epoch 28:  65%|██████▌   | 196/300 [01:28<00:51,  2.03it/s]Epoch 28:  66%|██████▌   | 197/300 [01:29<00:48,  2.11it/s]Epoch 28:  66%|██████▌   | 198/300 [01:29<00:47,  2.13it/s]Epoch 28:  66%|██████▋   | 199/300 [01:29<00:47,  2.14it/s]06/19/2022 15:30:10 - INFO - __main__ - global step: 4300; train loss: 4.947211742401123; dev loss: 4.756974697113037
Epoch 28:  67%|██████▋   | 200/300 [01:30<00:49,  2.03it/s]Epoch 28:  67%|██████▋   | 201/300 [01:30<00:45,  2.16it/s]Epoch 28:  67%|██████▋   | 202/300 [01:31<00:44,  2.18it/s]Epoch 28:  68%|██████▊   | 203/300 [01:31<00:40,  2.37it/s]Epoch 28:  68%|██████▊   | 204/300 [01:32<00:39,  2.42it/s]Epoch 28:  68%|██████▊   | 205/300 [01:32<00:42,  2.26it/s]Epoch 28:  69%|██████▊   | 206/300 [01:32<00:38,  2.44it/s]Epoch 28:  69%|██████▉   | 207/300 [01:33<00:37,  2.51it/s]Epoch 28:  69%|██████▉   | 208/300 [01:33<00:36,  2.49it/s]Epoch 28:  70%|██████▉   | 209/300 [01:34<00:37,  2.45it/s]Epoch 28:  70%|███████   | 210/300 [01:34<00:35,  2.53it/s]Epoch 28:  70%|███████   | 211/300 [01:34<00:33,  2.66it/s]Epoch 28:  71%|███████   | 212/300 [01:35<00:31,  2.75it/s]Epoch 28:  71%|███████   | 213/300 [01:35<00:34,  2.56it/s]Epoch 28:  71%|███████▏  | 214/300 [01:36<00:35,  2.45it/s]Epoch 28:  72%|███████▏  | 215/300 [01:36<00:36,  2.36it/s]Epoch 28:  72%|███████▏  | 216/300 [01:37<00:38,  2.20it/s]Epoch 28:  72%|███████▏  | 217/300 [01:37<00:37,  2.22it/s]Epoch 28:  73%|███████▎  | 218/300 [01:37<00:33,  2.42it/s]Epoch 28:  73%|███████▎  | 219/300 [01:38<00:31,  2.57it/s]06/19/2022 15:30:18 - INFO - __main__ - global step: 4310; train loss: 4.614418983459473; dev loss: 4.9026994705200195
Epoch 28:  73%|███████▎  | 220/300 [01:38<00:30,  2.61it/s]Epoch 28:  74%|███████▎  | 221/300 [01:38<00:32,  2.43it/s]Epoch 28:  74%|███████▍  | 222/300 [01:39<00:30,  2.57it/s]Epoch 28:  74%|███████▍  | 223/300 [01:39<00:28,  2.70it/s]Epoch 28:  75%|███████▍  | 224/300 [01:39<00:27,  2.78it/s]Epoch 28:  75%|███████▌  | 225/300 [01:40<00:28,  2.60it/s]Epoch 28:  75%|███████▌  | 226/300 [01:40<00:30,  2.42it/s]Epoch 28:  76%|███████▌  | 227/300 [01:41<00:31,  2.34it/s]Epoch 28:  76%|███████▌  | 228/300 [01:41<00:30,  2.34it/s]Epoch 28:  76%|███████▋  | 229/300 [01:42<00:30,  2.32it/s]Epoch 28:  77%|███████▋  | 230/300 [01:42<00:30,  2.32it/s]Epoch 28:  77%|███████▋  | 231/300 [01:42<00:27,  2.50it/s]Epoch 28:  77%|███████▋  | 232/300 [01:43<00:25,  2.64it/s]Epoch 28:  78%|███████▊  | 233/300 [01:43<00:24,  2.70it/s]Epoch 28:  78%|███████▊  | 234/300 [01:44<00:26,  2.48it/s]Epoch 28:  78%|███████▊  | 235/300 [01:44<00:27,  2.40it/s]Epoch 28:  79%|███████▊  | 236/300 [01:45<00:27,  2.29it/s]Epoch 28:  79%|███████▉  | 237/300 [01:45<00:27,  2.27it/s]Epoch 28:  79%|███████▉  | 238/300 [01:45<00:27,  2.28it/s]Epoch 28:  80%|███████▉  | 239/300 [01:46<00:25,  2.41it/s]06/19/2022 15:30:26 - INFO - __main__ - global step: 4320; train loss: 4.499506950378418; dev loss: 4.4679694175720215
Epoch 28:  80%|████████  | 240/300 [01:46<00:23,  2.54it/s]Epoch 28:  80%|████████  | 241/300 [01:46<00:22,  2.68it/s]Epoch 28:  81%|████████  | 242/300 [01:47<00:22,  2.57it/s]Epoch 28:  81%|████████  | 243/300 [01:47<00:21,  2.69it/s]Epoch 28:  81%|████████▏ | 244/300 [01:48<00:21,  2.61it/s]Epoch 28:  82%|████████▏ | 245/300 [01:48<00:20,  2.63it/s]Epoch 28:  82%|████████▏ | 246/300 [01:48<00:21,  2.53it/s]Epoch 28:  82%|████████▏ | 247/300 [01:49<00:19,  2.68it/s]Epoch 28:  83%|████████▎ | 248/300 [01:49<00:18,  2.77it/s]Epoch 28:  83%|████████▎ | 249/300 [01:50<00:19,  2.66it/s]Epoch 28:  83%|████████▎ | 250/300 [01:50<00:19,  2.55it/s]Epoch 28:  84%|████████▎ | 251/300 [01:50<00:18,  2.68it/s]Epoch 28:  84%|████████▍ | 252/300 [01:51<00:17,  2.70it/s]Epoch 28:  84%|████████▍ | 253/300 [01:51<00:17,  2.69it/s]Epoch 28:  85%|████████▍ | 254/300 [01:51<00:17,  2.58it/s]Epoch 28:  85%|████████▌ | 255/300 [01:52<00:17,  2.54it/s]Epoch 28:  85%|████████▌ | 256/300 [01:52<00:16,  2.65it/s]Epoch 28:  86%|████████▌ | 257/300 [01:53<00:16,  2.59it/s]Epoch 28:  86%|████████▌ | 258/300 [01:53<00:15,  2.69it/s]Epoch 28:  86%|████████▋ | 259/300 [01:53<00:15,  2.57it/s]06/19/2022 15:30:34 - INFO - __main__ - global step: 4330; train loss: 4.899936676025391; dev loss: 4.905797958374023
Epoch 28:  87%|████████▋ | 260/300 [01:54<00:14,  2.69it/s]Epoch 28:  87%|████████▋ | 261/300 [01:54<00:13,  2.79it/s]Epoch 28:  87%|████████▋ | 262/300 [01:54<00:13,  2.87it/s]Epoch 28:  88%|████████▊ | 263/300 [01:55<00:14,  2.53it/s]Epoch 28:  88%|████████▊ | 264/300 [01:55<00:13,  2.65it/s]Epoch 28:  88%|████████▊ | 265/300 [01:56<00:12,  2.73it/s]Epoch 28:  89%|████████▊ | 266/300 [01:56<00:13,  2.58it/s]Epoch 28:  89%|████████▉ | 267/300 [01:57<00:14,  2.28it/s]Epoch 28:  89%|████████▉ | 268/300 [01:57<00:13,  2.40it/s]Epoch 28:  90%|████████▉ | 269/300 [01:57<00:12,  2.50it/s]Epoch 28:  90%|█████████ | 270/300 [01:58<00:11,  2.63it/s]Epoch 28:  90%|█████████ | 271/300 [01:58<00:11,  2.46it/s]Epoch 28:  91%|█████████ | 272/300 [01:58<00:10,  2.58it/s]Epoch 28:  91%|█████████ | 273/300 [01:59<00:10,  2.59it/s]Epoch 28:  91%|█████████▏| 274/300 [01:59<00:09,  2.71it/s]Epoch 28:  92%|█████████▏| 275/300 [02:00<00:09,  2.59it/s]Epoch 28:  92%|█████████▏| 276/300 [02:00<00:09,  2.53it/s]Epoch 28:  92%|█████████▏| 277/300 [02:00<00:09,  2.55it/s]Epoch 28:  93%|█████████▎| 278/300 [02:01<00:08,  2.50it/s]Epoch 28:  93%|█████████▎| 279/300 [02:01<00:09,  2.32it/s]06/19/2022 15:30:42 - INFO - __main__ - global step: 4340; train loss: 4.676825046539307; dev loss: 4.459376335144043
Epoch 28:  93%|█████████▎| 280/300 [02:02<00:08,  2.39it/s]Epoch 28:  94%|█████████▎| 281/300 [02:02<00:07,  2.49it/s]Epoch 28:  94%|█████████▍| 282/300 [02:02<00:07,  2.40it/s]Epoch 28:  94%|█████████▍| 283/300 [02:03<00:06,  2.48it/s]Epoch 28:  95%|█████████▍| 284/300 [02:03<00:06,  2.35it/s]Epoch 28:  95%|█████████▌| 285/300 [02:04<00:06,  2.47it/s]Epoch 28:  95%|█████████▌| 286/300 [02:04<00:05,  2.60it/s]Epoch 28:  96%|█████████▌| 287/300 [02:04<00:04,  2.64it/s]Epoch 28:  96%|█████████▌| 288/300 [02:05<00:04,  2.44it/s]Epoch 28:  96%|█████████▋| 289/300 [02:05<00:04,  2.59it/s]Epoch 28:  97%|█████████▋| 290/300 [02:06<00:04,  2.50it/s]Epoch 28:  97%|█████████▋| 291/300 [02:06<00:03,  2.43it/s]Epoch 28:  97%|█████████▋| 292/300 [02:07<00:03,  2.19it/s]Epoch 28:  98%|█████████▊| 293/300 [02:07<00:03,  2.27it/s]Epoch 28:  98%|█████████▊| 294/300 [02:07<00:02,  2.29it/s]Epoch 28:  98%|█████████▊| 295/300 [02:08<00:02,  2.39it/s]Epoch 28:  99%|█████████▊| 296/300 [02:08<00:01,  2.24it/s]Epoch 28:  99%|█████████▉| 297/300 [02:09<00:01,  2.26it/s]Epoch 28:  99%|█████████▉| 298/300 [02:09<00:00,  2.36it/s]Epoch 28: 100%|█████████▉| 299/300 [02:10<00:00,  2.36it/s]06/19/2022 15:30:50 - INFO - __main__ - global step: 4350; train loss: 4.280057430267334; dev loss: 4.439740180969238
Epoch 28: 100%|██████████| 300/300 [02:10<00:00,  2.12it/s]Epoch 28: 100%|██████████| 300/300 [02:10<00:00,  2.30it/s]
Epoch 29:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 29:   0%|          | 1/300 [00:00<02:07,  2.34it/s]Epoch 29:   1%|          | 2/300 [00:00<02:24,  2.07it/s]Epoch 29:   1%|          | 3/300 [00:01<02:06,  2.35it/s]Epoch 29:   1%|▏         | 4/300 [00:01<02:10,  2.27it/s]Epoch 29:   2%|▏         | 5/300 [00:02<02:01,  2.42it/s]Epoch 29:   2%|▏         | 6/300 [00:02<01:54,  2.57it/s]Epoch 29:   2%|▏         | 7/300 [00:02<01:59,  2.45it/s]Epoch 29:   3%|▎         | 8/300 [00:03<02:05,  2.32it/s]Epoch 29:   3%|▎         | 9/300 [00:03<01:58,  2.45it/s]Epoch 29:   3%|▎         | 10/300 [00:04<01:55,  2.52it/s]Epoch 29:   4%|▎         | 11/300 [00:04<01:52,  2.57it/s]Epoch 29:   4%|▍         | 12/300 [00:04<01:49,  2.64it/s]Epoch 29:   4%|▍         | 13/300 [00:05<01:59,  2.41it/s]Epoch 29:   5%|▍         | 14/300 [00:05<02:01,  2.35it/s]Epoch 29:   5%|▌         | 15/300 [00:06<02:02,  2.32it/s]Epoch 29:   5%|▌         | 16/300 [00:06<02:02,  2.32it/s]Epoch 29:   6%|▌         | 17/300 [00:07<02:16,  2.07it/s]Epoch 29:   6%|▌         | 18/300 [00:07<02:10,  2.15it/s]Epoch 29:   6%|▋         | 19/300 [00:08<02:09,  2.17it/s]06/19/2022 15:30:59 - INFO - __main__ - global step: 4360; train loss: 4.770683288574219; dev loss: 4.831820487976074
Epoch 29:   7%|▋         | 20/300 [00:08<02:04,  2.24it/s]Epoch 29:   7%|▋         | 21/300 [00:09<02:03,  2.26it/s]Epoch 29:   7%|▋         | 22/300 [00:09<01:55,  2.41it/s]Epoch 29:   8%|▊         | 23/300 [00:09<01:49,  2.52it/s]Epoch 29:   8%|▊         | 24/300 [00:10<01:43,  2.65it/s]Epoch 29:   8%|▊         | 25/300 [00:10<01:51,  2.48it/s]Epoch 29:   9%|▊         | 26/300 [00:10<01:45,  2.60it/s]Epoch 29:   9%|▉         | 27/300 [00:11<01:44,  2.62it/s]Epoch 29:   9%|▉         | 28/300 [00:11<01:40,  2.71it/s]Epoch 29:  10%|▉         | 29/300 [00:12<01:53,  2.38it/s]Epoch 29:  10%|█         | 30/300 [00:12<01:48,  2.50it/s]Epoch 29:  10%|█         | 31/300 [00:12<01:54,  2.35it/s]Epoch 29:  11%|█         | 32/300 [00:13<01:46,  2.51it/s]Epoch 29:  11%|█         | 33/300 [00:13<01:50,  2.42it/s]Epoch 29:  11%|█▏        | 34/300 [00:14<01:43,  2.57it/s]Epoch 29:  12%|█▏        | 35/300 [00:14<01:38,  2.69it/s]Epoch 29:  12%|█▏        | 36/300 [00:14<01:34,  2.78it/s]Epoch 29:  12%|█▏        | 37/300 [00:15<01:35,  2.76it/s]Epoch 29:  13%|█▎        | 38/300 [00:15<01:44,  2.50it/s]Epoch 29:  13%|█▎        | 39/300 [00:16<01:47,  2.43it/s]06/19/2022 15:31:07 - INFO - __main__ - global step: 4370; train loss: 4.558202743530273; dev loss: 4.479555606842041
Epoch 29:  13%|█▎        | 40/300 [00:16<01:49,  2.37it/s]Epoch 29:  14%|█▎        | 41/300 [00:16<01:54,  2.26it/s]Epoch 29:  14%|█▍        | 42/300 [00:17<01:54,  2.26it/s]Epoch 29:  14%|█▍        | 43/300 [00:17<01:45,  2.44it/s]Epoch 29:  15%|█▍        | 44/300 [00:18<01:44,  2.45it/s]Epoch 29:  15%|█▌        | 45/300 [00:18<01:41,  2.51it/s]Epoch 29:  15%|█▌        | 46/300 [00:18<01:44,  2.43it/s]Epoch 29:  16%|█▌        | 47/300 [00:19<01:45,  2.41it/s]Epoch 29:  16%|█▌        | 48/300 [00:19<01:50,  2.28it/s]Epoch 29:  16%|█▋        | 49/300 [00:20<01:51,  2.26it/s]Epoch 29:  17%|█▋        | 50/300 [00:20<01:57,  2.12it/s]Epoch 29:  17%|█▋        | 51/300 [00:21<01:58,  2.11it/s]Epoch 29:  17%|█▋        | 52/300 [00:21<01:52,  2.21it/s]Epoch 29:  18%|█▊        | 53/300 [00:22<01:48,  2.27it/s]Epoch 29:  18%|█▊        | 54/300 [00:22<01:54,  2.15it/s]Epoch 29:  18%|█▊        | 55/300 [00:23<02:01,  2.01it/s]Epoch 29:  19%|█▊        | 56/300 [00:23<01:51,  2.19it/s]Epoch 29:  19%|█▉        | 57/300 [00:24<01:48,  2.25it/s]Epoch 29:  19%|█▉        | 58/300 [00:24<01:46,  2.27it/s]Epoch 29:  20%|█▉        | 59/300 [00:24<01:38,  2.44it/s]06/19/2022 15:31:16 - INFO - __main__ - global step: 4380; train loss: 4.712718963623047; dev loss: 4.700422286987305
Epoch 29:  20%|██        | 60/300 [00:25<01:35,  2.52it/s]Epoch 29:  20%|██        | 61/300 [00:25<01:31,  2.62it/s]Epoch 29:  21%|██        | 62/300 [00:26<01:40,  2.36it/s]Epoch 29:  21%|██        | 63/300 [00:26<01:43,  2.29it/s]Epoch 29:  21%|██▏       | 64/300 [00:26<01:45,  2.23it/s]Epoch 29:  22%|██▏       | 65/300 [00:27<01:45,  2.22it/s]Epoch 29:  22%|██▏       | 66/300 [00:27<01:47,  2.17it/s]Epoch 29:  22%|██▏       | 67/300 [00:28<01:58,  1.96it/s]Epoch 29:  23%|██▎       | 68/300 [00:29<01:54,  2.02it/s]Epoch 29:  23%|██▎       | 69/300 [00:29<01:53,  2.03it/s]Epoch 29:  23%|██▎       | 70/300 [00:29<01:52,  2.04it/s]Epoch 29:  24%|██▎       | 71/300 [00:30<02:01,  1.88it/s]Epoch 29:  24%|██▍       | 72/300 [00:30<01:51,  2.05it/s]Epoch 29:  24%|██▍       | 73/300 [00:31<01:44,  2.17it/s]Epoch 29:  25%|██▍       | 74/300 [00:31<01:41,  2.24it/s]Epoch 29:  25%|██▌       | 75/300 [00:32<01:39,  2.25it/s]Epoch 29:  25%|██▌       | 76/300 [00:32<01:37,  2.31it/s]Epoch 29:  26%|██▌       | 77/300 [00:32<01:30,  2.46it/s]Epoch 29:  26%|██▌       | 78/300 [00:33<01:25,  2.61it/s]Epoch 29:  26%|██▋       | 79/300 [00:33<01:27,  2.53it/s]06/19/2022 15:31:25 - INFO - __main__ - global step: 4390; train loss: 4.1859130859375; dev loss: 4.204093933105469
Epoch 29:  27%|██▋       | 80/300 [00:34<01:28,  2.49it/s]Epoch 29:  27%|██▋       | 81/300 [00:34<01:23,  2.61it/s]Epoch 29:  27%|██▋       | 82/300 [00:34<01:20,  2.71it/s]Epoch 29:  28%|██▊       | 83/300 [00:35<01:24,  2.58it/s]Epoch 29:  28%|██▊       | 84/300 [00:35<01:25,  2.52it/s]Epoch 29:  28%|██▊       | 85/300 [00:36<01:30,  2.38it/s]Epoch 29:  29%|██▊       | 86/300 [00:36<01:26,  2.47it/s]Epoch 29:  29%|██▉       | 87/300 [00:37<01:36,  2.20it/s]Epoch 29:  29%|██▉       | 88/300 [00:37<01:31,  2.33it/s]Epoch 29:  30%|██▉       | 89/300 [00:37<01:36,  2.19it/s]Epoch 29:  30%|███       | 90/300 [00:38<01:33,  2.25it/s]Epoch 29:  30%|███       | 91/300 [00:38<01:33,  2.23it/s]Epoch 29:  31%|███       | 92/300 [00:39<01:36,  2.15it/s]Epoch 29:  31%|███       | 93/300 [00:39<01:28,  2.34it/s]Epoch 29:  31%|███▏      | 94/300 [00:40<01:24,  2.44it/s]Epoch 29:  32%|███▏      | 95/300 [00:40<01:22,  2.47it/s]Epoch 29:  32%|███▏      | 96/300 [00:40<01:29,  2.27it/s]Epoch 29:  32%|███▏      | 97/300 [00:41<01:24,  2.41it/s]Epoch 29:  33%|███▎      | 98/300 [00:41<01:19,  2.53it/s]Epoch 29:  33%|███▎      | 99/300 [00:42<01:17,  2.59it/s]06/19/2022 15:31:33 - INFO - __main__ - global step: 4400; train loss: 4.2668657302856445; dev loss: 4.094181060791016
Epoch 29:  33%|███▎      | 100/300 [00:42<01:20,  2.50it/s]Epoch 29:  34%|███▎      | 101/300 [00:42<01:16,  2.59it/s]Epoch 29:  34%|███▍      | 102/300 [00:43<01:14,  2.66it/s]Epoch 29:  34%|███▍      | 103/300 [00:43<01:13,  2.69it/s]Epoch 29:  35%|███▍      | 104/300 [00:44<01:17,  2.52it/s]Epoch 29:  35%|███▌      | 105/300 [00:44<01:14,  2.63it/s]Epoch 29:  35%|███▌      | 106/300 [00:44<01:13,  2.65it/s]Epoch 29:  36%|███▌      | 107/300 [00:45<01:13,  2.63it/s]Epoch 29:  36%|███▌      | 108/300 [00:45<01:24,  2.26it/s]Epoch 29:  36%|███▋      | 109/300 [00:46<01:25,  2.24it/s]Epoch 29:  37%|███▋      | 110/300 [00:46<01:27,  2.18it/s]Epoch 29:  37%|███▋      | 111/300 [00:47<01:23,  2.27it/s]Epoch 29:  37%|███▋      | 112/300 [00:47<01:30,  2.08it/s]Epoch 29:  38%|███▊      | 113/300 [00:48<01:25,  2.18it/s]Epoch 29:  38%|███▊      | 114/300 [00:48<01:23,  2.24it/s]Epoch 29:  38%|███▊      | 115/300 [00:48<01:22,  2.25it/s]Epoch 29:  39%|███▊      | 116/300 [00:49<01:28,  2.07it/s]Epoch 29:  39%|███▉      | 117/300 [00:49<01:23,  2.19it/s]Epoch 29:  39%|███▉      | 118/300 [00:50<01:19,  2.28it/s]Epoch 29:  40%|███▉      | 119/300 [00:50<01:16,  2.37it/s]06/19/2022 15:31:41 - INFO - __main__ - global step: 4410; train loss: 4.90299129486084; dev loss: 4.833268165588379
Epoch 29:  40%|████      | 120/300 [00:50<01:11,  2.52it/s]Epoch 29:  40%|████      | 121/300 [00:51<01:21,  2.21it/s]Epoch 29:  41%|████      | 122/300 [00:52<01:21,  2.18it/s]Epoch 29:  41%|████      | 123/300 [00:52<01:19,  2.24it/s]Epoch 29:  41%|████▏     | 124/300 [00:52<01:19,  2.21it/s]Epoch 29:  42%|████▏     | 125/300 [00:53<01:23,  2.09it/s]Epoch 29:  42%|████▏     | 126/300 [00:53<01:22,  2.12it/s]Epoch 29:  42%|████▏     | 127/300 [00:54<01:19,  2.18it/s]Epoch 29:  43%|████▎     | 128/300 [00:54<01:17,  2.21it/s]Epoch 29:  43%|████▎     | 129/300 [00:55<01:22,  2.08it/s]Epoch 29:  43%|████▎     | 130/300 [00:55<01:15,  2.24it/s]Epoch 29:  44%|████▎     | 131/300 [00:56<01:16,  2.22it/s]Epoch 29:  44%|████▍     | 132/300 [00:56<01:15,  2.24it/s]Epoch 29:  44%|████▍     | 133/300 [00:57<01:23,  2.00it/s]Epoch 29:  45%|████▍     | 134/300 [00:57<01:22,  2.02it/s]Epoch 29:  45%|████▌     | 135/300 [00:58<01:20,  2.04it/s]Epoch 29:  45%|████▌     | 136/300 [00:58<01:27,  1.88it/s]Epoch 29:  46%|████▌     | 137/300 [00:59<01:33,  1.74it/s]Epoch 29:  46%|████▌     | 138/300 [00:59<01:28,  1.83it/s]Epoch 29:  46%|████▋     | 139/300 [01:00<01:26,  1.87it/s]06/19/2022 15:31:51 - INFO - __main__ - global step: 4420; train loss: 4.7000555992126465; dev loss: 4.60801362991333
Epoch 29:  47%|████▋     | 140/300 [01:00<01:24,  1.90it/s]Epoch 29:  47%|████▋     | 141/300 [01:01<01:31,  1.74it/s]Epoch 29:  47%|████▋     | 142/300 [01:02<01:28,  1.79it/s]Epoch 29:  48%|████▊     | 143/300 [01:02<01:25,  1.85it/s]Epoch 29:  48%|████▊     | 144/300 [01:03<01:18,  1.98it/s]Epoch 29:  48%|████▊     | 145/300 [01:03<01:15,  2.05it/s]Epoch 29:  49%|████▊     | 146/300 [01:04<01:19,  1.93it/s]Epoch 29:  49%|████▉     | 147/300 [01:04<01:15,  2.01it/s]Epoch 29:  49%|████▉     | 148/300 [01:04<01:09,  2.19it/s]Epoch 29:  50%|████▉     | 149/300 [01:05<01:03,  2.39it/s]Epoch 29:  50%|█████     | 150/300 [01:05<01:02,  2.38it/s]Epoch 29:  50%|█████     | 151/300 [01:06<00:58,  2.54it/s]Epoch 29:  51%|█████     | 152/300 [01:06<00:58,  2.54it/s]Epoch 29:  51%|█████     | 153/300 [01:06<01:00,  2.45it/s]Epoch 29:  51%|█████▏    | 154/300 [01:07<01:08,  2.13it/s]Epoch 29:  52%|█████▏    | 155/300 [01:07<01:07,  2.15it/s]Epoch 29:  52%|█████▏    | 156/300 [01:08<01:03,  2.28it/s]Epoch 29:  52%|█████▏    | 157/300 [01:08<00:58,  2.46it/s]Epoch 29:  53%|█████▎    | 158/300 [01:09<00:58,  2.42it/s]Epoch 29:  53%|█████▎    | 159/300 [01:09<00:55,  2.55it/s]06/19/2022 15:32:00 - INFO - __main__ - global step: 4430; train loss: 4.357603549957275; dev loss: 4.103257656097412
Epoch 29:  53%|█████▎    | 160/300 [01:09<00:55,  2.54it/s]Epoch 29:  54%|█████▎    | 161/300 [01:10<00:57,  2.43it/s]Epoch 29:  54%|█████▍    | 162/300 [01:10<01:04,  2.15it/s]Epoch 29:  54%|█████▍    | 163/300 [01:11<01:02,  2.19it/s]Epoch 29:  55%|█████▍    | 164/300 [01:11<00:57,  2.38it/s]Epoch 29:  55%|█████▌    | 165/300 [01:11<00:53,  2.54it/s]Epoch 29:  55%|█████▌    | 166/300 [01:12<00:54,  2.48it/s]Epoch 29:  56%|█████▌    | 167/300 [01:12<00:50,  2.62it/s]Epoch 29:  56%|█████▌    | 168/300 [01:13<00:50,  2.60it/s]Epoch 29:  56%|█████▋    | 169/300 [01:13<00:54,  2.41it/s]Epoch 29:  57%|█████▋    | 170/300 [01:14<01:00,  2.16it/s]Epoch 29:  57%|█████▋    | 171/300 [01:14<00:59,  2.19it/s]Epoch 29:  57%|█████▋    | 172/300 [01:15<00:59,  2.14it/s]Epoch 29:  58%|█████▊    | 173/300 [01:15<00:55,  2.29it/s]Epoch 29:  58%|█████▊    | 174/300 [01:15<00:56,  2.24it/s]Epoch 29:  58%|█████▊    | 175/300 [01:16<01:04,  1.95it/s]Epoch 29:  59%|█████▊    | 176/300 [01:16<00:58,  2.13it/s]Epoch 29:  59%|█████▉    | 177/300 [01:17<00:53,  2.29it/s]Epoch 29:  59%|█████▉    | 178/300 [01:17<00:50,  2.42it/s]Epoch 29:  60%|█████▉    | 179/300 [01:18<00:54,  2.24it/s]06/19/2022 15:32:09 - INFO - __main__ - global step: 4440; train loss: 4.261181831359863; dev loss: 4.264901161193848
Epoch 29:  60%|██████    | 180/300 [01:18<00:51,  2.35it/s]Epoch 29:  60%|██████    | 181/300 [01:19<00:55,  2.14it/s]Epoch 29:  61%|██████    | 182/300 [01:19<00:58,  2.01it/s]Epoch 29:  61%|██████    | 183/300 [01:20<00:59,  1.96it/s]Epoch 29:  61%|██████▏   | 184/300 [01:20<00:54,  2.11it/s]Epoch 29:  62%|██████▏   | 185/300 [01:21<00:51,  2.23it/s]Epoch 29:  62%|██████▏   | 186/300 [01:21<00:53,  2.15it/s]Epoch 29:  62%|██████▏   | 187/300 [01:22<00:57,  1.97it/s]Epoch 29:  63%|██████▎   | 188/300 [01:22<00:53,  2.10it/s]Epoch 29:  63%|██████▎   | 189/300 [01:22<00:50,  2.20it/s]Epoch 29:  63%|██████▎   | 190/300 [01:23<00:46,  2.34it/s]Epoch 29:  64%|██████▎   | 191/300 [01:23<00:48,  2.23it/s]Epoch 29:  64%|██████▍   | 192/300 [01:24<00:49,  2.17it/s]Epoch 29:  64%|██████▍   | 193/300 [01:24<00:48,  2.20it/s]Epoch 29:  65%|██████▍   | 194/300 [01:25<00:49,  2.13it/s]Epoch 29:  65%|██████▌   | 195/300 [01:25<00:52,  2.00it/s]Epoch 29:  65%|██████▌   | 196/300 [01:26<00:50,  2.07it/s]Epoch 29:  66%|██████▌   | 197/300 [01:26<00:45,  2.25it/s]Epoch 29:  66%|██████▌   | 198/300 [01:26<00:42,  2.40it/s]Epoch 29:  66%|██████▋   | 199/300 [01:27<00:40,  2.51it/s]06/19/2022 15:32:18 - INFO - __main__ - global step: 4450; train loss: 4.30281925201416; dev loss: 4.061156272888184
Epoch 29:  67%|██████▋   | 200/300 [01:28<00:48,  2.08it/s]Epoch 29:  67%|██████▋   | 201/300 [01:28<00:44,  2.20it/s]Epoch 29:  67%|██████▋   | 202/300 [01:28<00:45,  2.18it/s]Epoch 29:  68%|██████▊   | 203/300 [01:29<00:44,  2.18it/s]Epoch 29:  68%|██████▊   | 204/300 [01:30<00:50,  1.90it/s]Epoch 29:  68%|██████▊   | 205/300 [01:30<00:48,  1.97it/s]Epoch 29:  69%|██████▊   | 206/300 [01:30<00:47,  1.97it/s]Epoch 29:  69%|██████▉   | 207/300 [01:31<00:47,  1.97it/s]Epoch 29:  69%|██████▉   | 208/300 [01:32<00:47,  1.95it/s]Epoch 29:  70%|██████▉   | 209/300 [01:32<00:42,  2.13it/s]Epoch 29:  70%|███████   | 210/300 [01:32<00:42,  2.10it/s]Epoch 29:  70%|███████   | 211/300 [01:33<00:39,  2.25it/s]Epoch 29:  71%|███████   | 212/300 [01:33<00:39,  2.21it/s]Epoch 29:  71%|███████   | 213/300 [01:34<00:36,  2.37it/s]Epoch 29:  71%|███████▏  | 214/300 [01:34<00:34,  2.50it/s]Epoch 29:  72%|███████▏  | 215/300 [01:34<00:32,  2.60it/s]Epoch 29:  72%|███████▏  | 216/300 [01:35<00:35,  2.34it/s]Epoch 29:  72%|███████▏  | 217/300 [01:35<00:35,  2.31it/s]Epoch 29:  73%|███████▎  | 218/300 [01:36<00:37,  2.20it/s]Epoch 29:  73%|███████▎  | 219/300 [01:36<00:35,  2.25it/s]06/19/2022 15:32:27 - INFO - __main__ - global step: 4460; train loss: 4.37005090713501; dev loss: 4.367188930511475
Epoch 29:  73%|███████▎  | 220/300 [01:37<00:35,  2.28it/s]Epoch 29:  74%|███████▎  | 221/300 [01:37<00:32,  2.40it/s]Epoch 29:  74%|███████▍  | 222/300 [01:37<00:30,  2.56it/s]Epoch 29:  74%|███████▍  | 223/300 [01:38<00:28,  2.68it/s]Epoch 29:  75%|███████▍  | 224/300 [01:38<00:31,  2.42it/s]Epoch 29:  75%|███████▌  | 225/300 [01:38<00:29,  2.50it/s]Epoch 29:  75%|███████▌  | 226/300 [01:39<00:28,  2.57it/s]Epoch 29:  76%|███████▌  | 227/300 [01:39<00:29,  2.44it/s]Epoch 29:  76%|███████▌  | 228/300 [01:40<00:31,  2.32it/s]Epoch 29:  76%|███████▋  | 229/300 [01:40<00:31,  2.26it/s]Epoch 29:  77%|███████▋  | 230/300 [01:41<00:30,  2.26it/s]Epoch 29:  77%|███████▋  | 231/300 [01:41<00:31,  2.17it/s]Epoch 29:  77%|███████▋  | 232/300 [01:42<00:29,  2.27it/s]Epoch 29:  78%|███████▊  | 233/300 [01:42<00:30,  2.23it/s]Epoch 29:  78%|███████▊  | 234/300 [01:42<00:28,  2.34it/s]Epoch 29:  78%|███████▊  | 235/300 [01:43<00:26,  2.43it/s]Epoch 29:  79%|███████▊  | 236/300 [01:43<00:27,  2.35it/s]Epoch 29:  79%|███████▉  | 237/300 [01:44<00:27,  2.32it/s]Epoch 29:  79%|███████▉  | 238/300 [01:44<00:25,  2.43it/s]Epoch 29:  80%|███████▉  | 239/300 [01:44<00:24,  2.47it/s]06/19/2022 15:32:36 - INFO - __main__ - global step: 4470; train loss: 4.277068138122559; dev loss: 4.180599212646484
Epoch 29:  80%|████████  | 240/300 [01:45<00:23,  2.58it/s]Epoch 29:  80%|████████  | 241/300 [01:45<00:24,  2.44it/s]Epoch 29:  81%|████████  | 242/300 [01:46<00:23,  2.49it/s]Epoch 29:  81%|████████  | 243/300 [01:46<00:22,  2.57it/s]Epoch 29:  81%|████████▏ | 244/300 [01:46<00:22,  2.53it/s]Epoch 29:  82%|████████▏ | 245/300 [01:47<00:23,  2.32it/s]Epoch 29:  82%|████████▏ | 246/300 [01:47<00:22,  2.42it/s]Epoch 29:  82%|████████▏ | 247/300 [01:48<00:21,  2.45it/s]Epoch 29:  83%|████████▎ | 248/300 [01:48<00:20,  2.58it/s]Epoch 29:  83%|████████▎ | 249/300 [01:49<00:22,  2.27it/s]Epoch 29:  83%|████████▎ | 250/300 [01:49<00:22,  2.24it/s]Epoch 29:  84%|████████▎ | 251/300 [01:50<00:21,  2.28it/s]Epoch 29:  84%|████████▍ | 252/300 [01:50<00:21,  2.23it/s]Epoch 29:  84%|████████▍ | 253/300 [01:50<00:20,  2.30it/s]Epoch 29:  85%|████████▍ | 254/300 [01:51<00:20,  2.24it/s]Epoch 29:  85%|████████▌ | 255/300 [01:51<00:18,  2.42it/s]Epoch 29:  85%|████████▌ | 256/300 [01:52<00:17,  2.48it/s]Epoch 29:  86%|████████▌ | 257/300 [01:52<00:17,  2.48it/s]Epoch 29:  86%|████████▌ | 258/300 [01:52<00:17,  2.33it/s]Epoch 29:  86%|████████▋ | 259/300 [01:53<00:16,  2.51it/s]06/19/2022 15:32:44 - INFO - __main__ - global step: 4480; train loss: 4.451164245605469; dev loss: 4.525128364562988
Epoch 29:  87%|████████▋ | 260/300 [01:53<00:15,  2.58it/s]Epoch 29:  87%|████████▋ | 261/300 [01:54<00:15,  2.52it/s]Epoch 29:  87%|████████▋ | 262/300 [01:54<00:16,  2.32it/s]Epoch 29:  88%|████████▊ | 263/300 [01:54<00:15,  2.47it/s]Epoch 29:  88%|████████▊ | 264/300 [01:55<00:15,  2.29it/s]Epoch 29:  88%|████████▊ | 265/300 [01:55<00:15,  2.29it/s]Epoch 29:  89%|████████▊ | 266/300 [01:56<00:15,  2.25it/s]Epoch 29:  89%|████████▉ | 267/300 [01:56<00:13,  2.42it/s]Epoch 29:  89%|████████▉ | 268/300 [01:57<00:12,  2.52it/s]Epoch 29:  90%|████████▉ | 269/300 [01:57<00:11,  2.60it/s]Epoch 29:  90%|█████████ | 270/300 [01:57<00:12,  2.44it/s]Epoch 29:  90%|█████████ | 271/300 [01:58<00:11,  2.52it/s]Epoch 29:  91%|█████████ | 272/300 [01:58<00:10,  2.61it/s]Epoch 29:  91%|█████████ | 273/300 [01:58<00:10,  2.68it/s]Epoch 29:  91%|█████████▏| 274/300 [01:59<00:10,  2.47it/s]Epoch 29:  92%|█████████▏| 275/300 [01:59<00:09,  2.54it/s]Epoch 29:  92%|█████████▏| 276/300 [02:00<00:10,  2.39it/s]Epoch 29:  92%|█████████▏| 277/300 [02:00<00:10,  2.24it/s]Epoch 29:  93%|█████████▎| 278/300 [02:01<00:09,  2.24it/s]Epoch 29:  93%|█████████▎| 279/300 [02:01<00:08,  2.40it/s]06/19/2022 15:32:52 - INFO - __main__ - global step: 4490; train loss: 4.193542957305908; dev loss: 3.9937329292297363
Epoch 29:  93%|█████████▎| 280/300 [02:01<00:07,  2.54it/s]Epoch 29:  94%|█████████▎| 281/300 [02:02<00:07,  2.67it/s]Epoch 29:  94%|█████████▍| 282/300 [02:02<00:06,  2.74it/s]Epoch 29:  94%|█████████▍| 283/300 [02:03<00:07,  2.30it/s]Epoch 29:  95%|█████████▍| 284/300 [02:03<00:07,  2.24it/s]Epoch 29:  95%|█████████▌| 285/300 [02:04<00:06,  2.19it/s]Epoch 29:  95%|█████████▌| 286/300 [02:04<00:06,  2.18it/s]Epoch 29:  96%|█████████▌| 287/300 [02:05<00:06,  1.94it/s]Epoch 29:  96%|█████████▌| 288/300 [02:05<00:05,  2.12it/s]Epoch 29:  96%|█████████▋| 289/300 [02:06<00:05,  2.18it/s]Epoch 29:  97%|█████████▋| 290/300 [02:06<00:04,  2.27it/s]Epoch 29:  97%|█████████▋| 291/300 [02:06<00:03,  2.26it/s]Epoch 29:  97%|█████████▋| 292/300 [02:07<00:03,  2.44it/s]Epoch 29:  98%|█████████▊| 293/300 [02:07<00:02,  2.58it/s]Epoch 29:  98%|█████████▊| 294/300 [02:07<00:02,  2.70it/s]Epoch 29:  98%|█████████▊| 295/300 [02:08<00:02,  2.42it/s]Epoch 29:  99%|█████████▊| 296/300 [02:08<00:01,  2.23it/s]Epoch 29:  99%|█████████▉| 297/300 [02:09<00:01,  2.21it/s]Epoch 29:  99%|█████████▉| 298/300 [02:09<00:00,  2.11it/s]Epoch 29: 100%|█████████▉| 299/300 [02:10<00:00,  2.16it/s]06/19/2022 15:33:01 - INFO - __main__ - global step: 4500; train loss: 3.721332550048828; dev loss: 3.7324702739715576
Epoch 29: 100%|██████████| 300/300 [02:10<00:00,  2.25it/s]Epoch 29: 100%|██████████| 300/300 [02:10<00:00,  2.29it/s]
Epoch 30:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 30:   0%|          | 1/300 [00:00<01:41,  2.95it/s]Epoch 30:   1%|          | 2/300 [00:00<01:40,  2.97it/s]Epoch 30:   1%|          | 3/300 [00:01<01:53,  2.63it/s]Epoch 30:   1%|▏         | 4/300 [00:01<01:52,  2.64it/s]Epoch 30:   2%|▏         | 5/300 [00:01<02:05,  2.35it/s]Epoch 30:   2%|▏         | 6/300 [00:02<02:00,  2.45it/s]Epoch 30:   2%|▏         | 7/300 [00:02<01:57,  2.49it/s]Epoch 30:   3%|▎         | 8/300 [00:03<02:04,  2.34it/s]Epoch 30:   3%|▎         | 9/300 [00:03<01:55,  2.52it/s]Epoch 30:   3%|▎         | 10/300 [00:03<01:49,  2.65it/s]Epoch 30:   4%|▎         | 11/300 [00:04<01:46,  2.72it/s]Epoch 30:   4%|▍         | 12/300 [00:04<01:55,  2.50it/s]Epoch 30:   4%|▍         | 13/300 [00:05<02:04,  2.31it/s]Epoch 30:   5%|▍         | 14/300 [00:05<02:04,  2.30it/s]Epoch 30:   5%|▌         | 15/300 [00:06<02:02,  2.33it/s]Epoch 30:   5%|▌         | 16/300 [00:06<02:10,  2.18it/s]Epoch 30:   6%|▌         | 17/300 [00:07<02:10,  2.18it/s]Epoch 30:   6%|▌         | 18/300 [00:07<02:08,  2.20it/s]Epoch 30:   6%|▋         | 19/300 [00:07<02:07,  2.21it/s]06/19/2022 15:33:10 - INFO - __main__ - global step: 4510; train loss: 5.1158366203308105; dev loss: 5.164564609527588
Epoch 30:   7%|▋         | 20/300 [00:08<02:13,  2.10it/s]Epoch 30:   7%|▋         | 21/300 [00:08<02:01,  2.29it/s]Epoch 30:   7%|▋         | 22/300 [00:09<01:53,  2.46it/s]Epoch 30:   8%|▊         | 23/300 [00:09<01:46,  2.61it/s]Epoch 30:   8%|▊         | 24/300 [00:09<01:51,  2.49it/s]Epoch 30:   8%|▊         | 25/300 [00:10<01:52,  2.45it/s]Epoch 30:   9%|▊         | 26/300 [00:10<01:49,  2.51it/s]Epoch 30:   9%|▉         | 27/300 [00:11<01:46,  2.56it/s]Epoch 30:   9%|▉         | 28/300 [00:11<02:00,  2.25it/s]Epoch 30:  10%|▉         | 29/300 [00:12<01:58,  2.28it/s]Epoch 30:  10%|█         | 30/300 [00:12<02:03,  2.19it/s]Epoch 30:  10%|█         | 31/300 [00:13<02:00,  2.23it/s]Epoch 30:  11%|█         | 32/300 [00:13<02:09,  2.07it/s]Epoch 30:  11%|█         | 33/300 [00:13<02:00,  2.22it/s]Epoch 30:  11%|█▏        | 34/300 [00:14<01:58,  2.25it/s]Epoch 30:  12%|█▏        | 35/300 [00:14<01:58,  2.24it/s]Epoch 30:  12%|█▏        | 36/300 [00:15<01:56,  2.26it/s]Epoch 30:  12%|█▏        | 37/300 [00:15<02:09,  2.03it/s]Epoch 30:  13%|█▎        | 38/300 [00:16<02:00,  2.18it/s]Epoch 30:  13%|█▎        | 39/300 [00:16<01:59,  2.19it/s]06/19/2022 15:33:18 - INFO - __main__ - global step: 4520; train loss: 4.076505184173584; dev loss: 4.441372394561768
Epoch 30:  13%|█▎        | 40/300 [00:17<01:52,  2.31it/s]Epoch 30:  14%|█▎        | 41/300 [00:17<01:54,  2.25it/s]Epoch 30:  14%|█▍        | 42/300 [00:17<01:51,  2.31it/s]Epoch 30:  14%|█▍        | 43/300 [00:18<01:54,  2.24it/s]Epoch 30:  15%|█▍        | 44/300 [00:18<01:49,  2.34it/s]Epoch 30:  15%|█▌        | 45/300 [00:19<01:49,  2.33it/s]Epoch 30:  15%|█▌        | 46/300 [00:19<01:42,  2.48it/s]Epoch 30:  16%|█▌        | 47/300 [00:20<01:39,  2.53it/s]Epoch 30:  16%|█▌        | 48/300 [00:20<01:40,  2.50it/s]Epoch 30:  16%|█▋        | 49/300 [00:20<01:46,  2.36it/s]Epoch 30:  17%|█▋        | 50/300 [00:21<01:44,  2.39it/s]Epoch 30:  17%|█▋        | 51/300 [00:21<01:55,  2.16it/s]Epoch 30:  17%|█▋        | 52/300 [00:22<01:51,  2.22it/s]Epoch 30:  18%|█▊        | 53/300 [00:22<02:00,  2.04it/s]Epoch 30:  18%|█▊        | 54/300 [00:23<02:00,  2.04it/s]Epoch 30:  18%|█▊        | 55/300 [00:23<01:51,  2.19it/s]Epoch 30:  19%|█▊        | 56/300 [00:24<01:47,  2.28it/s]Epoch 30:  19%|█▉        | 57/300 [00:24<01:50,  2.19it/s]Epoch 30:  19%|█▉        | 58/300 [00:25<01:49,  2.20it/s]Epoch 30:  20%|█▉        | 59/300 [00:25<01:47,  2.25it/s]06/19/2022 15:33:27 - INFO - __main__ - global step: 4530; train loss: 4.671055793762207; dev loss: 4.477524757385254
Epoch 30:  20%|██        | 60/300 [00:26<01:53,  2.12it/s]Epoch 30:  20%|██        | 61/300 [00:26<01:47,  2.23it/s]Epoch 30:  21%|██        | 62/300 [00:27<01:56,  2.04it/s]Epoch 30:  21%|██        | 63/300 [00:27<01:52,  2.11it/s]Epoch 30:  21%|██▏       | 64/300 [00:27<01:50,  2.14it/s]Epoch 30:  22%|██▏       | 65/300 [00:28<01:48,  2.17it/s]Epoch 30:  22%|██▏       | 66/300 [00:28<01:55,  2.02it/s]Epoch 30:  22%|██▏       | 67/300 [00:29<01:46,  2.19it/s]Epoch 30:  23%|██▎       | 68/300 [00:29<01:40,  2.32it/s]Epoch 30:  23%|██▎       | 69/300 [00:30<01:38,  2.34it/s]Epoch 30:  23%|██▎       | 70/300 [00:30<01:40,  2.30it/s]Epoch 30:  24%|██▎       | 71/300 [00:30<01:39,  2.30it/s]Epoch 30:  24%|██▍       | 72/300 [00:31<01:34,  2.41it/s]Epoch 30:  24%|██▍       | 73/300 [00:31<01:36,  2.36it/s]Epoch 30:  25%|██▍       | 74/300 [00:32<01:44,  2.16it/s]Epoch 30:  25%|██▌       | 75/300 [00:32<01:48,  2.08it/s]Epoch 30:  25%|██▌       | 76/300 [00:33<01:41,  2.21it/s]Epoch 30:  26%|██▌       | 77/300 [00:33<01:43,  2.16it/s]Epoch 30:  26%|██▌       | 78/300 [00:34<01:44,  2.13it/s]Epoch 30:  26%|██▋       | 79/300 [00:34<01:41,  2.18it/s]06/19/2022 15:33:36 - INFO - __main__ - global step: 4540; train loss: 4.063193321228027; dev loss: 4.268144607543945
Epoch 30:  27%|██▋       | 80/300 [00:35<01:37,  2.26it/s]Epoch 30:  27%|██▋       | 81/300 [00:35<01:31,  2.39it/s]Epoch 30:  27%|██▋       | 82/300 [00:35<01:32,  2.35it/s]Epoch 30:  28%|██▊       | 83/300 [00:36<01:27,  2.49it/s]Epoch 30:  28%|██▊       | 84/300 [00:36<01:28,  2.45it/s]Epoch 30:  28%|██▊       | 85/300 [00:37<01:31,  2.36it/s]Epoch 30:  29%|██▊       | 86/300 [00:37<01:42,  2.09it/s]Epoch 30:  29%|██▉       | 87/300 [00:38<01:46,  1.99it/s]Epoch 30:  29%|██▉       | 88/300 [00:38<01:40,  2.10it/s]Epoch 30:  30%|██▉       | 89/300 [00:39<01:33,  2.26it/s]Epoch 30:  30%|███       | 90/300 [00:39<01:31,  2.31it/s]Epoch 30:  30%|███       | 91/300 [00:39<01:33,  2.23it/s]Epoch 30:  31%|███       | 92/300 [00:40<01:27,  2.38it/s]Epoch 30:  31%|███       | 93/300 [00:40<01:22,  2.51it/s]Epoch 30:  31%|███▏      | 94/300 [00:40<01:19,  2.60it/s]Epoch 30:  32%|███▏      | 95/300 [00:41<01:22,  2.49it/s]Epoch 30:  32%|███▏      | 96/300 [00:41<01:21,  2.52it/s]Epoch 30:  32%|███▏      | 97/300 [00:42<01:23,  2.43it/s]Epoch 30:  33%|███▎      | 98/300 [00:42<01:23,  2.41it/s]Epoch 30:  33%|███▎      | 99/300 [00:43<01:29,  2.25it/s]06/19/2022 15:33:45 - INFO - __main__ - global step: 4550; train loss: 4.084898471832275; dev loss: 3.8690266609191895
Epoch 30:  33%|███▎      | 100/300 [00:43<01:23,  2.40it/s]Epoch 30:  34%|███▎      | 101/300 [00:43<01:21,  2.44it/s]Epoch 30:  34%|███▍      | 102/300 [00:44<01:20,  2.45it/s]Epoch 30:  34%|███▍      | 103/300 [00:44<01:25,  2.30it/s]Epoch 30:  35%|███▍      | 104/300 [00:45<01:25,  2.29it/s]Epoch 30:  35%|███▌      | 105/300 [00:45<01:23,  2.33it/s]Epoch 30:  35%|███▌      | 106/300 [00:46<01:24,  2.30it/s]Epoch 30:  36%|███▌      | 107/300 [00:46<01:29,  2.15it/s]Epoch 30:  36%|███▌      | 108/300 [00:47<01:26,  2.21it/s]Epoch 30:  36%|███▋      | 109/300 [00:47<01:23,  2.28it/s]Epoch 30:  37%|███▋      | 110/300 [00:47<01:18,  2.42it/s]Epoch 30:  37%|███▋      | 111/300 [00:48<01:20,  2.34it/s]Epoch 30:  37%|███▋      | 112/300 [00:48<01:16,  2.44it/s]Epoch 30:  38%|███▊      | 113/300 [00:49<01:13,  2.53it/s]Epoch 30:  38%|███▊      | 114/300 [00:49<01:13,  2.52it/s]Epoch 30:  38%|███▊      | 115/300 [00:49<01:12,  2.56it/s]Epoch 30:  39%|███▊      | 116/300 [00:50<01:18,  2.36it/s]Epoch 30:  39%|███▉      | 117/300 [00:50<01:18,  2.34it/s]Epoch 30:  39%|███▉      | 118/300 [00:51<01:15,  2.41it/s]Epoch 30:  40%|███▉      | 119/300 [00:51<01:18,  2.31it/s]06/19/2022 15:33:53 - INFO - __main__ - global step: 4560; train loss: 4.342272758483887; dev loss: 4.538759708404541
Epoch 30:  40%|████      | 120/300 [00:52<01:20,  2.23it/s]Epoch 30:  40%|████      | 121/300 [00:52<01:18,  2.27it/s]Epoch 30:  41%|████      | 122/300 [00:53<01:20,  2.21it/s]Epoch 30:  41%|████      | 123/300 [00:53<01:15,  2.34it/s]Epoch 30:  41%|████▏     | 124/300 [00:53<01:15,  2.32it/s]Epoch 30:  42%|████▏     | 125/300 [00:54<01:11,  2.46it/s]Epoch 30:  42%|████▏     | 126/300 [00:54<01:07,  2.58it/s]Epoch 30:  42%|████▏     | 127/300 [00:54<01:07,  2.57it/s]Epoch 30:  43%|████▎     | 128/300 [00:55<01:12,  2.36it/s]Epoch 30:  43%|████▎     | 129/300 [00:55<01:08,  2.48it/s]Epoch 30:  43%|████▎     | 130/300 [00:56<01:05,  2.60it/s]Epoch 30:  44%|████▎     | 131/300 [00:56<01:02,  2.68it/s]Epoch 30:  44%|████▍     | 132/300 [00:56<01:06,  2.51it/s]Epoch 30:  44%|████▍     | 133/300 [00:57<01:08,  2.45it/s]Epoch 30:  45%|████▍     | 134/300 [00:57<01:09,  2.41it/s]Epoch 30:  45%|████▌     | 135/300 [00:58<01:14,  2.22it/s]Epoch 30:  45%|████▌     | 136/300 [00:58<01:17,  2.11it/s]Epoch 30:  46%|████▌     | 137/300 [00:59<01:12,  2.24it/s]Epoch 30:  46%|████▌     | 138/300 [00:59<01:12,  2.24it/s]Epoch 30:  46%|████▋     | 139/300 [01:00<01:14,  2.17it/s]06/19/2022 15:34:02 - INFO - __main__ - global step: 4570; train loss: 4.552605628967285; dev loss: 4.585946083068848
Epoch 30:  47%|████▋     | 140/300 [01:00<01:18,  2.04it/s]Epoch 30:  47%|████▋     | 141/300 [01:01<01:15,  2.10it/s]Epoch 30:  47%|████▋     | 142/300 [01:01<01:14,  2.12it/s]Epoch 30:  48%|████▊     | 143/300 [01:02<01:13,  2.14it/s]Epoch 30:  48%|████▊     | 144/300 [01:02<01:12,  2.15it/s]Epoch 30:  48%|████▊     | 145/300 [01:03<01:17,  2.01it/s]Epoch 30:  49%|████▊     | 146/300 [01:03<01:16,  2.00it/s]Epoch 30:  49%|████▉     | 147/300 [01:04<01:14,  2.07it/s]Epoch 30:  49%|████▉     | 148/300 [01:04<01:13,  2.06it/s]Epoch 30:  50%|████▉     | 149/300 [01:04<01:11,  2.12it/s]Epoch 30:  50%|█████     | 150/300 [01:05<01:07,  2.23it/s]Epoch 30:  50%|█████     | 151/300 [01:05<01:03,  2.35it/s]Epoch 30:  51%|█████     | 152/300 [01:06<01:01,  2.42it/s]Epoch 30:  51%|█████     | 153/300 [01:06<01:02,  2.34it/s]Epoch 30:  51%|█████▏    | 154/300 [01:06<01:00,  2.40it/s]Epoch 30:  52%|█████▏    | 155/300 [01:07<00:57,  2.51it/s]Epoch 30:  52%|█████▏    | 156/300 [01:07<00:55,  2.61it/s]Epoch 30:  52%|█████▏    | 157/300 [01:08<00:58,  2.45it/s]Epoch 30:  53%|█████▎    | 158/300 [01:08<00:58,  2.42it/s]Epoch 30:  53%|█████▎    | 159/300 [01:08<00:56,  2.52it/s]06/19/2022 15:34:10 - INFO - __main__ - global step: 4580; train loss: 4.080906867980957; dev loss: 4.0541486740112305
Epoch 30:  53%|█████▎    | 160/300 [01:09<00:55,  2.52it/s]Epoch 30:  54%|█████▎    | 161/300 [01:09<01:01,  2.28it/s]Epoch 30:  54%|█████▍    | 162/300 [01:10<01:00,  2.28it/s]Epoch 30:  54%|█████▍    | 163/300 [01:10<00:58,  2.34it/s]Epoch 30:  55%|█████▍    | 164/300 [01:11<00:55,  2.45it/s]Epoch 30:  55%|█████▌    | 165/300 [01:11<00:57,  2.35it/s]Epoch 30:  55%|█████▌    | 166/300 [01:11<00:54,  2.45it/s]Epoch 30:  56%|█████▌    | 167/300 [01:12<00:57,  2.32it/s]Epoch 30:  56%|█████▌    | 168/300 [01:12<00:58,  2.27it/s]Epoch 30:  56%|█████▋    | 169/300 [01:13<00:57,  2.27it/s]Epoch 30:  57%|█████▋    | 170/300 [01:13<01:01,  2.11it/s]Epoch 30:  57%|█████▋    | 171/300 [01:14<01:00,  2.12it/s]Epoch 30:  57%|█████▋    | 172/300 [01:14<01:00,  2.12it/s]Epoch 30:  58%|█████▊    | 173/300 [01:15<01:03,  2.01it/s]Epoch 30:  58%|█████▊    | 174/300 [01:15<01:01,  2.03it/s]Epoch 30:  58%|█████▊    | 175/300 [01:16<00:56,  2.21it/s]Epoch 30:  59%|█████▊    | 176/300 [01:16<00:55,  2.25it/s]Epoch 30:  59%|█████▉    | 177/300 [01:17<00:55,  2.21it/s]Epoch 30:  59%|█████▉    | 178/300 [01:17<00:56,  2.16it/s]Epoch 30:  60%|█████▉    | 179/300 [01:18<00:56,  2.16it/s]06/19/2022 15:34:20 - INFO - __main__ - global step: 4590; train loss: 4.315792083740234; dev loss: 4.51602840423584
Epoch 30:  60%|██████    | 180/300 [01:18<00:56,  2.12it/s]Epoch 30:  60%|██████    | 181/300 [01:18<00:53,  2.20it/s]Epoch 30:  61%|██████    | 182/300 [01:19<00:54,  2.18it/s]Epoch 30:  61%|██████    | 183/300 [01:19<00:50,  2.34it/s]Epoch 30:  61%|██████▏   | 184/300 [01:20<00:46,  2.48it/s]Epoch 30:  62%|██████▏   | 185/300 [01:20<00:44,  2.59it/s]Epoch 30:  62%|██████▏   | 186/300 [01:20<00:48,  2.37it/s]Epoch 30:  62%|██████▏   | 187/300 [01:21<00:45,  2.49it/s]Epoch 30:  63%|██████▎   | 188/300 [01:21<00:43,  2.56it/s]Epoch 30:  63%|██████▎   | 189/300 [01:22<00:42,  2.63it/s]Epoch 30:  63%|██████▎   | 190/300 [01:22<00:44,  2.45it/s]Epoch 30:  64%|██████▎   | 191/300 [01:22<00:42,  2.55it/s]Epoch 30:  64%|██████▍   | 192/300 [01:23<00:41,  2.61it/s]Epoch 30:  64%|██████▍   | 193/300 [01:23<00:40,  2.67it/s]Epoch 30:  65%|██████▍   | 194/300 [01:24<00:42,  2.48it/s]Epoch 30:  65%|██████▌   | 195/300 [01:24<00:41,  2.56it/s]Epoch 30:  65%|██████▌   | 196/300 [01:24<00:40,  2.59it/s]Epoch 30:  66%|██████▌   | 197/300 [01:25<00:39,  2.63it/s]Epoch 30:  66%|██████▌   | 198/300 [01:25<00:38,  2.65it/s]Epoch 30:  66%|██████▋   | 199/300 [01:26<00:40,  2.47it/s]06/19/2022 15:34:28 - INFO - __main__ - global step: 4600; train loss: 3.8596713542938232; dev loss: 3.8294568061828613
Epoch 30:  67%|██████▋   | 200/300 [01:26<00:40,  2.49it/s]Epoch 30:  67%|██████▋   | 201/300 [01:26<00:39,  2.51it/s]Epoch 30:  67%|██████▋   | 202/300 [01:27<00:39,  2.51it/s]Epoch 30:  68%|██████▊   | 203/300 [01:27<00:41,  2.35it/s]Epoch 30:  68%|██████▊   | 204/300 [01:28<00:40,  2.38it/s]Epoch 30:  68%|██████▊   | 205/300 [01:28<00:39,  2.43it/s]Epoch 30:  69%|██████▊   | 206/300 [01:28<00:37,  2.48it/s]Epoch 30:  69%|██████▉   | 207/300 [01:29<00:40,  2.30it/s]Epoch 30:  69%|██████▉   | 208/300 [01:29<00:38,  2.38it/s]Epoch 30:  70%|██████▉   | 209/300 [01:30<00:36,  2.48it/s]Epoch 30:  70%|███████   | 210/300 [01:30<00:36,  2.50it/s]Epoch 30:  70%|███████   | 211/300 [01:31<00:38,  2.32it/s]Epoch 30:  71%|███████   | 212/300 [01:31<00:36,  2.42it/s]Epoch 30:  71%|███████   | 213/300 [01:31<00:37,  2.34it/s]Epoch 30:  71%|███████▏  | 214/300 [01:32<00:36,  2.38it/s]Epoch 30:  72%|███████▏  | 215/300 [01:32<00:40,  2.12it/s]Epoch 30:  72%|███████▏  | 216/300 [01:33<00:39,  2.11it/s]Epoch 30:  72%|███████▏  | 217/300 [01:33<00:39,  2.10it/s]Epoch 30:  73%|███████▎  | 218/300 [01:34<00:39,  2.09it/s]Epoch 30:  73%|███████▎  | 219/300 [01:34<00:42,  1.89it/s]06/19/2022 15:34:37 - INFO - __main__ - global step: 4610; train loss: 3.592428684234619; dev loss: 3.645203113555908
Epoch 30:  73%|███████▎  | 220/300 [01:35<00:41,  1.91it/s]Epoch 30:  74%|███████▎  | 221/300 [01:36<00:42,  1.86it/s]Epoch 30:  74%|███████▍  | 222/300 [01:36<00:42,  1.82it/s]Epoch 30:  74%|███████▍  | 223/300 [01:37<00:42,  1.83it/s]Epoch 30:  75%|███████▍  | 224/300 [01:37<00:41,  1.83it/s]Epoch 30:  75%|███████▌  | 225/300 [01:38<00:36,  2.04it/s]Epoch 30:  75%|███████▌  | 226/300 [01:38<00:33,  2.19it/s]Epoch 30:  76%|███████▌  | 227/300 [01:38<00:32,  2.22it/s]Epoch 30:  76%|███████▌  | 228/300 [01:39<00:32,  2.21it/s]Epoch 30:  76%|███████▋  | 229/300 [01:39<00:31,  2.29it/s]Epoch 30:  77%|███████▋  | 230/300 [01:40<00:30,  2.29it/s]Epoch 30:  77%|███████▋  | 231/300 [01:40<00:30,  2.29it/s]Epoch 30:  77%|███████▋  | 232/300 [01:41<00:30,  2.24it/s]Epoch 30:  78%|███████▊  | 233/300 [01:41<00:29,  2.26it/s]Epoch 30:  78%|███████▊  | 234/300 [01:41<00:27,  2.40it/s]Epoch 30:  78%|███████▊  | 235/300 [01:42<00:25,  2.52it/s]Epoch 30:  79%|███████▊  | 236/300 [01:42<00:26,  2.39it/s]Epoch 30:  79%|███████▉  | 237/300 [01:43<00:25,  2.50it/s]Epoch 30:  79%|███████▉  | 238/300 [01:43<00:23,  2.59it/s]Epoch 30:  80%|███████▉  | 239/300 [01:43<00:22,  2.67it/s]06/19/2022 15:34:45 - INFO - __main__ - global step: 4620; train loss: 3.828969955444336; dev loss: 3.9373574256896973
Epoch 30:  80%|████████  | 240/300 [01:44<00:25,  2.36it/s]Epoch 30:  80%|████████  | 241/300 [01:44<00:23,  2.48it/s]Epoch 30:  81%|████████  | 242/300 [01:44<00:22,  2.57it/s]Epoch 30:  81%|████████  | 243/300 [01:45<00:21,  2.64it/s]Epoch 30:  81%|████████▏ | 244/300 [01:45<00:24,  2.33it/s]Epoch 30:  82%|████████▏ | 245/300 [01:46<00:22,  2.46it/s]Epoch 30:  82%|████████▏ | 246/300 [01:46<00:22,  2.40it/s]Epoch 30:  82%|████████▏ | 247/300 [01:47<00:22,  2.32it/s]Epoch 30:  83%|████████▎ | 248/300 [01:47<00:25,  2.03it/s]Epoch 30:  83%|████████▎ | 249/300 [01:48<00:23,  2.17it/s]Epoch 30:  83%|████████▎ | 250/300 [01:48<00:22,  2.26it/s]Epoch 30:  84%|████████▎ | 251/300 [01:49<00:22,  2.17it/s]Epoch 30:  84%|████████▍ | 252/300 [01:49<00:21,  2.21it/s]Epoch 30:  84%|████████▍ | 253/300 [01:50<00:23,  2.02it/s]Epoch 30:  85%|████████▍ | 254/300 [01:50<00:22,  2.05it/s]Epoch 30:  85%|████████▌ | 255/300 [01:51<00:22,  2.03it/s]Epoch 30:  85%|████████▌ | 256/300 [01:51<00:21,  2.03it/s]Epoch 30:  86%|████████▌ | 257/300 [01:52<00:20,  2.06it/s]Epoch 30:  86%|████████▌ | 258/300 [01:52<00:19,  2.11it/s]Epoch 30:  86%|████████▋ | 259/300 [01:52<00:19,  2.12it/s]06/19/2022 15:34:55 - INFO - __main__ - global step: 4630; train loss: 4.887446403503418; dev loss: 4.974958896636963
Epoch 30:  87%|████████▋ | 260/300 [01:53<00:19,  2.09it/s]Epoch 30:  87%|████████▋ | 261/300 [01:53<00:19,  2.01it/s]Epoch 30:  87%|████████▋ | 262/300 [01:54<00:17,  2.21it/s]Epoch 30:  88%|████████▊ | 263/300 [01:54<00:15,  2.36it/s]Epoch 30:  88%|████████▊ | 264/300 [01:55<00:14,  2.50it/s]Epoch 30:  88%|████████▊ | 265/300 [01:55<00:14,  2.40it/s]Epoch 30:  89%|████████▊ | 266/300 [01:55<00:13,  2.52it/s]Epoch 30:  89%|████████▉ | 267/300 [01:56<00:12,  2.60it/s]Epoch 30:  89%|████████▉ | 268/300 [01:56<00:12,  2.51it/s]Epoch 30:  90%|████████▉ | 269/300 [01:57<00:13,  2.38it/s]Epoch 30:  90%|█████████ | 270/300 [01:57<00:12,  2.36it/s]Epoch 30:  90%|█████████ | 271/300 [01:57<00:12,  2.29it/s]Epoch 30:  91%|█████████ | 272/300 [01:58<00:12,  2.21it/s]Epoch 30:  91%|█████████ | 273/300 [01:59<00:13,  2.00it/s]Epoch 30:  91%|█████████▏| 274/300 [01:59<00:12,  2.07it/s]Epoch 30:  92%|█████████▏| 275/300 [01:59<00:11,  2.09it/s]Epoch 30:  92%|█████████▏| 276/300 [02:00<00:11,  2.08it/s]Epoch 30:  92%|█████████▏| 277/300 [02:00<00:10,  2.11it/s]Epoch 30:  93%|█████████▎| 278/300 [02:01<00:10,  2.07it/s]Epoch 30:  93%|█████████▎| 279/300 [02:01<00:09,  2.26it/s]06/19/2022 15:35:03 - INFO - __main__ - global step: 4640; train loss: 4.296534061431885; dev loss: 4.331676483154297
Epoch 30:  93%|█████████▎| 280/300 [02:02<00:08,  2.34it/s]Epoch 30:  94%|█████████▎| 281/300 [02:02<00:08,  2.28it/s]Epoch 30:  94%|█████████▍| 282/300 [02:03<00:08,  2.06it/s]Epoch 30:  94%|█████████▍| 283/300 [02:03<00:07,  2.13it/s]Epoch 30:  95%|█████████▍| 284/300 [02:04<00:07,  2.05it/s]Epoch 30:  95%|█████████▌| 285/300 [02:04<00:07,  2.08it/s]Epoch 30:  95%|█████████▌| 286/300 [02:05<00:07,  1.94it/s]Epoch 30:  96%|█████████▌| 287/300 [02:05<00:06,  2.09it/s]Epoch 30:  96%|█████████▌| 288/300 [02:06<00:05,  2.26it/s]Epoch 30:  96%|█████████▋| 289/300 [02:06<00:04,  2.33it/s]Epoch 30:  97%|█████████▋| 290/300 [02:06<00:04,  2.28it/s]Epoch 30:  97%|█████████▋| 291/300 [02:07<00:03,  2.35it/s]Epoch 30:  97%|█████████▋| 292/300 [02:07<00:03,  2.27it/s]Epoch 30:  98%|█████████▊| 293/300 [02:08<00:03,  2.23it/s]Epoch 30:  98%|█████████▊| 294/300 [02:08<00:02,  2.02it/s]Epoch 30:  98%|█████████▊| 295/300 [02:09<00:02,  2.00it/s]Epoch 30:  99%|█████████▊| 296/300 [02:09<00:01,  2.13it/s]Epoch 30:  99%|█████████▉| 297/300 [02:10<00:01,  2.15it/s]Epoch 30:  99%|█████████▉| 298/300 [02:10<00:01,  2.00it/s]Epoch 30: 100%|█████████▉| 299/300 [02:11<00:00,  2.06it/s]06/19/2022 15:35:13 - INFO - __main__ - global step: 4650; train loss: 4.623421669006348; dev loss: 4.4612531661987305
Epoch 30: 100%|██████████| 300/300 [02:11<00:00,  2.00it/s]Epoch 30: 100%|██████████| 300/300 [02:11<00:00,  2.28it/s]
Epoch 31:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 31:   0%|          | 1/300 [00:00<02:01,  2.47it/s]Epoch 31:   1%|          | 2/300 [00:01<02:35,  1.91it/s]Epoch 31:   1%|          | 3/300 [00:01<02:19,  2.12it/s]Epoch 31:   1%|▏         | 4/300 [00:01<02:27,  2.01it/s]Epoch 31:   2%|▏         | 5/300 [00:02<02:23,  2.06it/s]Epoch 31:   2%|▏         | 6/300 [00:02<02:20,  2.09it/s]Epoch 31:   2%|▏         | 7/300 [00:03<02:31,  1.93it/s]Epoch 31:   3%|▎         | 8/300 [00:03<02:24,  2.01it/s]Epoch 31:   3%|▎         | 9/300 [00:04<02:23,  2.02it/s]Epoch 31:   3%|▎         | 10/300 [00:04<02:18,  2.09it/s]Epoch 31:   4%|▎         | 11/300 [00:05<02:28,  1.94it/s]Epoch 31:   4%|▍         | 12/300 [00:05<02:26,  1.96it/s]Epoch 31:   4%|▍         | 13/300 [00:06<02:31,  1.90it/s]Epoch 31:   5%|▍         | 14/300 [00:06<02:19,  2.05it/s]Epoch 31:   5%|▌         | 15/300 [00:07<02:20,  2.03it/s]Epoch 31:   5%|▌         | 16/300 [00:07<02:16,  2.09it/s]Epoch 31:   6%|▌         | 17/300 [00:08<02:13,  2.12it/s]Epoch 31:   6%|▌         | 18/300 [00:08<02:12,  2.13it/s]Epoch 31:   6%|▋         | 19/300 [00:09<02:19,  2.02it/s]06/19/2022 15:35:23 - INFO - __main__ - global step: 4660; train loss: 4.434422969818115; dev loss: 4.348001480102539
Epoch 31:   7%|▋         | 20/300 [00:09<02:19,  2.01it/s]Epoch 31:   7%|▋         | 21/300 [00:10<02:10,  2.13it/s]Epoch 31:   7%|▋         | 22/300 [00:10<02:06,  2.19it/s]Epoch 31:   8%|▊         | 23/300 [00:11<02:09,  2.13it/s]Epoch 31:   8%|▊         | 24/300 [00:11<02:00,  2.29it/s]Epoch 31:   8%|▊         | 25/300 [00:11<01:53,  2.43it/s]Epoch 31:   9%|▊         | 26/300 [00:12<01:52,  2.43it/s]Epoch 31:   9%|▉         | 27/300 [00:12<01:59,  2.28it/s]Epoch 31:   9%|▉         | 28/300 [00:13<01:56,  2.34it/s]Epoch 31:  10%|▉         | 29/300 [00:13<01:52,  2.40it/s]Epoch 31:  10%|█         | 30/300 [00:14<01:55,  2.34it/s]Epoch 31:  10%|█         | 31/300 [00:14<01:55,  2.32it/s]Epoch 31:  11%|█         | 32/300 [00:14<01:59,  2.25it/s]Epoch 31:  11%|█         | 33/300 [00:15<01:57,  2.27it/s]Epoch 31:  11%|█▏        | 34/300 [00:15<01:51,  2.39it/s]Epoch 31:  12%|█▏        | 35/300 [00:16<01:46,  2.49it/s]Epoch 31:  12%|█▏        | 36/300 [00:16<01:50,  2.38it/s]Epoch 31:  12%|█▏        | 37/300 [00:16<01:46,  2.48it/s]Epoch 31:  13%|█▎        | 38/300 [00:17<01:47,  2.43it/s]Epoch 31:  13%|█▎        | 39/300 [00:17<01:45,  2.46it/s]06/19/2022 15:35:31 - INFO - __main__ - global step: 4670; train loss: 4.509731292724609; dev loss: 4.6706862449646
Epoch 31:  13%|█▎        | 40/300 [00:18<01:49,  2.37it/s]Epoch 31:  14%|█▎        | 41/300 [00:18<01:49,  2.36it/s]Epoch 31:  14%|█▍        | 42/300 [00:19<01:50,  2.34it/s]Epoch 31:  14%|█▍        | 43/300 [00:19<01:44,  2.45it/s]Epoch 31:  15%|█▍        | 44/300 [00:19<01:48,  2.36it/s]Epoch 31:  15%|█▌        | 45/300 [00:20<01:48,  2.35it/s]Epoch 31:  15%|█▌        | 46/300 [00:20<01:49,  2.33it/s]Epoch 31:  16%|█▌        | 47/300 [00:21<01:49,  2.31it/s]Epoch 31:  16%|█▌        | 48/300 [00:21<01:52,  2.25it/s]Epoch 31:  16%|█▋        | 49/300 [00:22<01:47,  2.33it/s]Epoch 31:  17%|█▋        | 50/300 [00:22<01:41,  2.45it/s]Epoch 31:  17%|█▋        | 51/300 [00:22<01:37,  2.55it/s]Epoch 31:  17%|█▋        | 52/300 [00:23<01:42,  2.41it/s]Epoch 31:  18%|█▊        | 53/300 [00:23<01:43,  2.39it/s]Epoch 31:  18%|█▊        | 54/300 [00:24<01:38,  2.50it/s]Epoch 31:  18%|█▊        | 55/300 [00:24<01:34,  2.58it/s]Epoch 31:  19%|█▊        | 56/300 [00:24<01:41,  2.41it/s]Epoch 31:  19%|█▉        | 57/300 [00:25<01:36,  2.52it/s]Epoch 31:  19%|█▉        | 58/300 [00:25<01:32,  2.61it/s]Epoch 31:  20%|█▉        | 59/300 [00:25<01:30,  2.67it/s]06/19/2022 15:35:39 - INFO - __main__ - global step: 4680; train loss: 3.9292449951171875; dev loss: 3.9848549365997314
Epoch 31:  20%|██        | 60/300 [00:26<01:35,  2.52it/s]Epoch 31:  20%|██        | 61/300 [00:26<01:46,  2.24it/s]Epoch 31:  21%|██        | 62/300 [00:27<01:47,  2.22it/s]Epoch 31:  21%|██        | 63/300 [00:27<01:45,  2.24it/s]Epoch 31:  21%|██▏       | 64/300 [00:28<01:48,  2.17it/s]Epoch 31:  22%|██▏       | 65/300 [00:28<01:52,  2.09it/s]Epoch 31:  22%|██▏       | 66/300 [00:29<01:47,  2.17it/s]Epoch 31:  22%|██▏       | 67/300 [00:29<01:40,  2.33it/s]Epoch 31:  23%|██▎       | 68/300 [00:30<01:40,  2.30it/s]Epoch 31:  23%|██▎       | 69/300 [00:30<01:49,  2.10it/s]Epoch 31:  23%|██▎       | 70/300 [00:31<01:46,  2.16it/s]Epoch 31:  24%|██▎       | 71/300 [00:31<01:47,  2.13it/s]Epoch 31:  24%|██▍       | 72/300 [00:32<01:42,  2.23it/s]Epoch 31:  24%|██▍       | 73/300 [00:32<01:45,  2.16it/s]Epoch 31:  25%|██▍       | 74/300 [00:32<01:44,  2.15it/s]Epoch 31:  25%|██▌       | 75/300 [00:33<01:44,  2.15it/s]Epoch 31:  25%|██▌       | 76/300 [00:33<01:44,  2.14it/s]Epoch 31:  26%|██▌       | 77/300 [00:34<01:53,  1.96it/s]Epoch 31:  26%|██▌       | 78/300 [00:34<01:50,  2.00it/s]Epoch 31:  26%|██▋       | 79/300 [00:35<01:50,  2.01it/s]06/19/2022 15:35:49 - INFO - __main__ - global step: 4690; train loss: 4.537353992462158; dev loss: 4.573840141296387
Epoch 31:  27%|██▋       | 80/300 [00:35<01:44,  2.11it/s]Epoch 31:  27%|██▋       | 81/300 [00:36<01:51,  1.97it/s]Epoch 31:  27%|██▋       | 82/300 [00:36<01:43,  2.10it/s]Epoch 31:  28%|██▊       | 83/300 [00:37<01:37,  2.23it/s]Epoch 31:  28%|██▊       | 84/300 [00:37<01:37,  2.21it/s]Epoch 31:  28%|██▊       | 85/300 [00:38<01:31,  2.35it/s]Epoch 31:  29%|██▊       | 86/300 [00:38<01:34,  2.27it/s]Epoch 31:  29%|██▉       | 87/300 [00:38<01:30,  2.36it/s]Epoch 31:  29%|██▉       | 88/300 [00:39<01:28,  2.39it/s]Epoch 31:  30%|██▉       | 89/300 [00:39<01:24,  2.51it/s]Epoch 31:  30%|███       | 90/300 [00:40<01:30,  2.33it/s]Epoch 31:  30%|███       | 91/300 [00:40<01:36,  2.17it/s]Epoch 31:  31%|███       | 92/300 [00:41<01:31,  2.27it/s]Epoch 31:  31%|███       | 93/300 [00:41<01:29,  2.31it/s]Epoch 31:  31%|███▏      | 94/300 [00:42<01:36,  2.14it/s]Epoch 31:  32%|███▏      | 95/300 [00:42<01:31,  2.23it/s]Epoch 31:  32%|███▏      | 96/300 [00:42<01:28,  2.31it/s]Epoch 31:  32%|███▏      | 97/300 [00:43<01:24,  2.39it/s]Epoch 31:  33%|███▎      | 98/300 [00:43<01:33,  2.16it/s]Epoch 31:  33%|███▎      | 99/300 [00:44<01:29,  2.24it/s]06/19/2022 15:35:58 - INFO - __main__ - global step: 4700; train loss: 4.132802486419678; dev loss: 4.189898490905762
Epoch 31:  33%|███▎      | 100/300 [00:44<01:28,  2.27it/s]Epoch 31:  34%|███▎      | 101/300 [00:45<01:25,  2.34it/s]Epoch 31:  34%|███▍      | 102/300 [00:45<01:32,  2.14it/s]Epoch 31:  34%|███▍      | 103/300 [00:46<01:29,  2.20it/s]Epoch 31:  35%|███▍      | 104/300 [00:46<01:26,  2.26it/s]Epoch 31:  35%|███▌      | 105/300 [00:46<01:26,  2.24it/s]Epoch 31:  35%|███▌      | 106/300 [00:47<01:29,  2.18it/s]Epoch 31:  36%|███▌      | 107/300 [00:47<01:27,  2.21it/s]Epoch 31:  36%|███▌      | 108/300 [00:48<01:29,  2.15it/s]Epoch 31:  36%|███▋      | 109/300 [00:48<01:22,  2.30it/s]Epoch 31:  37%|███▋      | 110/300 [00:49<01:32,  2.05it/s]Epoch 31:  37%|███▋      | 111/300 [00:49<01:26,  2.19it/s]Epoch 31:  37%|███▋      | 112/300 [00:50<01:23,  2.24it/s]Epoch 31:  38%|███▊      | 113/300 [00:50<01:19,  2.36it/s]Epoch 31:  38%|███▊      | 114/300 [00:51<01:22,  2.26it/s]Epoch 31:  38%|███▊      | 115/300 [00:51<01:32,  2.00it/s]Epoch 31:  39%|███▊      | 116/300 [00:52<01:26,  2.14it/s]Epoch 31:  39%|███▉      | 117/300 [00:52<01:21,  2.25it/s]Epoch 31:  39%|███▉      | 118/300 [00:52<01:22,  2.20it/s]Epoch 31:  40%|███▉      | 119/300 [00:53<01:30,  2.01it/s]06/19/2022 15:36:07 - INFO - __main__ - global step: 4710; train loss: 4.0715179443359375; dev loss: 4.316981792449951
Epoch 31:  40%|████      | 120/300 [00:53<01:27,  2.07it/s]Epoch 31:  40%|████      | 121/300 [00:54<01:19,  2.25it/s]Epoch 31:  41%|████      | 122/300 [00:54<01:15,  2.36it/s]Epoch 31:  41%|████      | 123/300 [00:55<01:17,  2.27it/s]Epoch 31:  41%|████▏     | 124/300 [00:55<01:18,  2.25it/s]Epoch 31:  42%|████▏     | 125/300 [00:55<01:12,  2.40it/s]Epoch 31:  42%|████▏     | 126/300 [00:56<01:12,  2.41it/s]Epoch 31:  42%|████▏     | 127/300 [00:56<01:16,  2.27it/s]Epoch 31:  43%|████▎     | 128/300 [00:57<01:11,  2.40it/s]Epoch 31:  43%|████▎     | 129/300 [00:57<01:09,  2.44it/s]Epoch 31:  43%|████▎     | 130/300 [00:58<01:09,  2.43it/s]Epoch 31:  44%|████▎     | 131/300 [00:58<01:23,  2.02it/s]Epoch 31:  44%|████▍     | 132/300 [00:59<01:21,  2.06it/s]Epoch 31:  44%|████▍     | 133/300 [00:59<01:14,  2.24it/s]Epoch 31:  45%|████▍     | 134/300 [00:59<01:12,  2.30it/s]Epoch 31:  45%|████▌     | 135/300 [01:00<01:14,  2.21it/s]Epoch 31:  45%|████▌     | 136/300 [01:00<01:14,  2.22it/s]Epoch 31:  46%|████▌     | 137/300 [01:01<01:10,  2.32it/s]Epoch 31:  46%|████▌     | 138/300 [01:01<01:10,  2.30it/s]Epoch 31:  46%|████▋     | 139/300 [01:02<01:14,  2.17it/s]06/19/2022 15:36:16 - INFO - __main__ - global step: 4720; train loss: 3.915409564971924; dev loss: 3.9946072101593018
Epoch 31:  47%|████▋     | 140/300 [01:02<01:20,  2.00it/s]Epoch 31:  47%|████▋     | 141/300 [01:03<01:16,  2.09it/s]Epoch 31:  47%|████▋     | 142/300 [01:03<01:14,  2.13it/s]Epoch 31:  48%|████▊     | 143/300 [01:04<01:12,  2.15it/s]Epoch 31:  48%|████▊     | 144/300 [01:04<01:21,  1.92it/s]Epoch 31:  48%|████▊     | 145/300 [01:05<01:14,  2.08it/s]Epoch 31:  49%|████▊     | 146/300 [01:05<01:10,  2.19it/s]Epoch 31:  49%|████▉     | 147/300 [01:06<01:07,  2.27it/s]Epoch 31:  49%|████▉     | 148/300 [01:06<01:12,  2.11it/s]Epoch 31:  50%|████▉     | 149/300 [01:06<01:07,  2.22it/s]Epoch 31:  50%|█████     | 150/300 [01:07<01:04,  2.32it/s]Epoch 31:  50%|█████     | 151/300 [01:07<01:02,  2.40it/s]Epoch 31:  51%|█████     | 152/300 [01:08<01:05,  2.27it/s]Epoch 31:  51%|█████     | 153/300 [01:08<01:02,  2.35it/s]Epoch 31:  51%|█████▏    | 154/300 [01:09<01:01,  2.39it/s]Epoch 31:  52%|█████▏    | 155/300 [01:09<00:59,  2.45it/s]Epoch 31:  52%|█████▏    | 156/300 [01:09<01:03,  2.28it/s]Epoch 31:  52%|█████▏    | 157/300 [01:10<01:04,  2.20it/s]Epoch 31:  53%|█████▎    | 158/300 [01:10<01:04,  2.20it/s]Epoch 31:  53%|█████▎    | 159/300 [01:11<01:04,  2.18it/s]06/19/2022 15:36:25 - INFO - __main__ - global step: 4730; train loss: 3.8866448402404785; dev loss: 3.913167953491211
Epoch 31:  53%|█████▎    | 160/300 [01:11<01:04,  2.17it/s]Epoch 31:  54%|█████▎    | 161/300 [01:12<01:01,  2.27it/s]Epoch 31:  54%|█████▍    | 162/300 [01:12<01:00,  2.28it/s]Epoch 31:  54%|█████▍    | 163/300 [01:13<01:01,  2.21it/s]Epoch 31:  55%|█████▍    | 164/300 [01:13<01:08,  1.99it/s]Epoch 31:  55%|█████▌    | 165/300 [01:14<01:04,  2.09it/s]Epoch 31:  55%|█████▌    | 166/300 [01:14<01:02,  2.14it/s]Epoch 31:  56%|█████▌    | 167/300 [01:15<01:02,  2.12it/s]Epoch 31:  56%|█████▌    | 168/300 [01:15<00:59,  2.21it/s]Epoch 31:  56%|█████▋    | 169/300 [01:15<01:00,  2.16it/s]Epoch 31:  57%|█████▋    | 170/300 [01:16<00:57,  2.26it/s]Epoch 31:  57%|█████▋    | 171/300 [01:16<00:53,  2.40it/s]Epoch 31:  57%|█████▋    | 172/300 [01:17<00:51,  2.51it/s]Epoch 31:  58%|█████▊    | 173/300 [01:17<00:52,  2.40it/s]Epoch 31:  58%|█████▊    | 174/300 [01:17<00:50,  2.51it/s]Epoch 31:  58%|█████▊    | 175/300 [01:18<00:51,  2.45it/s]Epoch 31:  59%|█████▊    | 176/300 [01:18<00:48,  2.55it/s]Epoch 31:  59%|█████▉    | 177/300 [01:19<00:50,  2.42it/s]Epoch 31:  59%|█████▉    | 178/300 [01:19<00:48,  2.52it/s]Epoch 31:  60%|█████▉    | 179/300 [01:19<00:47,  2.52it/s]06/19/2022 15:36:33 - INFO - __main__ - global step: 4740; train loss: 4.321907997131348; dev loss: 4.138749122619629
Epoch 31:  60%|██████    | 180/300 [01:20<00:46,  2.61it/s]Epoch 31:  60%|██████    | 181/300 [01:20<00:48,  2.46it/s]Epoch 31:  61%|██████    | 182/300 [01:21<00:46,  2.55it/s]Epoch 31:  61%|██████    | 183/300 [01:21<00:44,  2.63it/s]Epoch 31:  61%|██████▏   | 184/300 [01:21<00:43,  2.68it/s]Epoch 31:  62%|██████▏   | 185/300 [01:22<00:47,  2.42it/s]Epoch 31:  62%|██████▏   | 186/300 [01:22<00:46,  2.45it/s]Epoch 31:  62%|██████▏   | 187/300 [01:23<00:47,  2.36it/s]Epoch 31:  63%|██████▎   | 188/300 [01:23<00:50,  2.20it/s]Epoch 31:  63%|██████▎   | 189/300 [01:24<00:52,  2.13it/s]Epoch 31:  63%|██████▎   | 190/300 [01:24<00:50,  2.17it/s]Epoch 31:  64%|██████▎   | 191/300 [01:25<00:49,  2.19it/s]Epoch 31:  64%|██████▍   | 192/300 [01:25<00:49,  2.17it/s]Epoch 31:  64%|██████▍   | 193/300 [01:26<00:52,  2.02it/s]Epoch 31:  65%|██████▍   | 194/300 [01:26<00:54,  1.95it/s]Epoch 31:  65%|██████▌   | 195/300 [01:27<00:51,  2.02it/s]Epoch 31:  65%|██████▌   | 196/300 [01:27<00:48,  2.16it/s]Epoch 31:  66%|██████▌   | 197/300 [01:27<00:46,  2.23it/s]Epoch 31:  66%|██████▌   | 198/300 [01:28<00:46,  2.18it/s]Epoch 31:  66%|██████▋   | 199/300 [01:28<00:44,  2.25it/s]06/19/2022 15:36:42 - INFO - __main__ - global step: 4750; train loss: 4.309722423553467; dev loss: 4.475854396820068
Epoch 31:  67%|██████▋   | 200/300 [01:29<00:44,  2.26it/s]Epoch 31:  67%|██████▋   | 201/300 [01:29<00:42,  2.35it/s]Epoch 31:  67%|██████▋   | 202/300 [01:30<00:46,  2.11it/s]Epoch 31:  68%|██████▊   | 203/300 [01:30<00:44,  2.18it/s]Epoch 31:  68%|██████▊   | 204/300 [01:31<00:42,  2.27it/s]Epoch 31:  68%|██████▊   | 205/300 [01:31<00:40,  2.36it/s]Epoch 31:  69%|██████▊   | 206/300 [01:31<00:42,  2.21it/s]Epoch 31:  69%|██████▉   | 207/300 [01:32<00:39,  2.36it/s]Epoch 31:  69%|██████▉   | 208/300 [01:32<00:38,  2.36it/s]Epoch 31:  70%|██████▉   | 209/300 [01:33<00:39,  2.32it/s]Epoch 31:  70%|███████   | 210/300 [01:33<00:39,  2.27it/s]Epoch 31:  70%|███████   | 211/300 [01:33<00:37,  2.40it/s]Epoch 31:  71%|███████   | 212/300 [01:34<00:35,  2.50it/s]Epoch 31:  71%|███████   | 213/300 [01:34<00:33,  2.56it/s]Epoch 31:  71%|███████▏  | 214/300 [01:35<00:35,  2.42it/s]Epoch 31:  72%|███████▏  | 215/300 [01:35<00:35,  2.42it/s]Epoch 31:  72%|███████▏  | 216/300 [01:36<00:35,  2.36it/s]Epoch 31:  72%|███████▏  | 217/300 [01:36<00:36,  2.29it/s]Epoch 31:  73%|███████▎  | 218/300 [01:37<00:39,  2.05it/s]Epoch 31:  73%|███████▎  | 219/300 [01:37<00:37,  2.18it/s]06/19/2022 15:36:51 - INFO - __main__ - global step: 4760; train loss: 4.107575416564941; dev loss: 3.937614917755127
Epoch 31:  73%|███████▎  | 220/300 [01:37<00:34,  2.29it/s]Epoch 31:  74%|███████▎  | 221/300 [01:38<00:33,  2.38it/s]Epoch 31:  74%|███████▍  | 222/300 [01:38<00:31,  2.50it/s]Epoch 31:  74%|███████▍  | 223/300 [01:39<00:33,  2.33it/s]Epoch 31:  75%|███████▍  | 224/300 [01:39<00:31,  2.41it/s]Epoch 31:  75%|███████▌  | 225/300 [01:39<00:30,  2.47it/s]Epoch 31:  75%|███████▌  | 226/300 [01:40<00:30,  2.42it/s]Epoch 31:  76%|███████▌  | 227/300 [01:40<00:31,  2.32it/s]Epoch 31:  76%|███████▌  | 228/300 [01:41<00:30,  2.40it/s]Epoch 31:  76%|███████▋  | 229/300 [01:41<00:30,  2.33it/s]Epoch 31:  77%|███████▋  | 230/300 [01:42<00:30,  2.28it/s]Epoch 31:  77%|███████▋  | 231/300 [01:42<00:31,  2.19it/s]Epoch 31:  77%|███████▋  | 232/300 [01:42<00:29,  2.32it/s]Epoch 31:  78%|███████▊  | 233/300 [01:43<00:27,  2.46it/s]Epoch 31:  78%|███████▊  | 234/300 [01:43<00:26,  2.50it/s]Epoch 31:  78%|███████▊  | 235/300 [01:44<00:27,  2.38it/s]Epoch 31:  79%|███████▊  | 236/300 [01:44<00:25,  2.51it/s]Epoch 31:  79%|███████▉  | 237/300 [01:44<00:24,  2.58it/s]Epoch 31:  79%|███████▉  | 238/300 [01:45<00:23,  2.62it/s]Epoch 31:  80%|███████▉  | 239/300 [01:45<00:26,  2.32it/s]06/19/2022 15:36:59 - INFO - __main__ - global step: 4770; train loss: 4.3744401931762695; dev loss: 4.41694450378418
Epoch 31:  80%|████████  | 240/300 [01:46<00:26,  2.24it/s]Epoch 31:  80%|████████  | 241/300 [01:46<00:28,  2.07it/s]Epoch 31:  81%|████████  | 242/300 [01:47<00:26,  2.22it/s]Epoch 31:  81%|████████  | 243/300 [01:47<00:26,  2.14it/s]Epoch 31:  81%|████████▏ | 244/300 [01:48<00:25,  2.20it/s]Epoch 31:  82%|████████▏ | 245/300 [01:48<00:25,  2.18it/s]Epoch 31:  82%|████████▏ | 246/300 [01:49<00:23,  2.30it/s]Epoch 31:  82%|████████▏ | 247/300 [01:49<00:22,  2.34it/s]Epoch 31:  83%|████████▎ | 248/300 [01:50<00:24,  2.08it/s]Epoch 31:  83%|████████▎ | 249/300 [01:50<00:24,  2.10it/s]Epoch 31:  83%|████████▎ | 250/300 [01:50<00:22,  2.21it/s]Epoch 31:  84%|████████▎ | 251/300 [01:51<00:21,  2.32it/s]Epoch 31:  84%|████████▍ | 252/300 [01:51<00:21,  2.21it/s]Epoch 31:  84%|████████▍ | 253/300 [01:52<00:19,  2.36it/s]Epoch 31:  85%|████████▍ | 254/300 [01:52<00:18,  2.49it/s]Epoch 31:  85%|████████▌ | 255/300 [01:52<00:17,  2.59it/s]Epoch 31:  85%|████████▌ | 256/300 [01:53<00:17,  2.46it/s]Epoch 31:  86%|████████▌ | 257/300 [01:53<00:16,  2.56it/s]Epoch 31:  86%|████████▌ | 258/300 [01:53<00:16,  2.61it/s]Epoch 31:  86%|████████▋ | 259/300 [01:54<00:15,  2.66it/s]06/19/2022 15:37:08 - INFO - __main__ - global step: 4780; train loss: 4.187334060668945; dev loss: 4.290626049041748
Epoch 31:  87%|████████▋ | 260/300 [01:54<00:16,  2.42it/s]Epoch 31:  87%|████████▋ | 261/300 [01:55<00:15,  2.47it/s]Epoch 31:  87%|████████▋ | 262/300 [01:55<00:15,  2.51it/s]Epoch 31:  88%|████████▊ | 263/300 [01:56<00:14,  2.53it/s]Epoch 31:  88%|████████▊ | 264/300 [01:56<00:15,  2.31it/s]Epoch 31:  88%|████████▊ | 265/300 [01:56<00:14,  2.44it/s]Epoch 31:  89%|████████▊ | 266/300 [01:57<00:13,  2.54it/s]Epoch 31:  89%|████████▉ | 267/300 [01:57<00:12,  2.63it/s]Epoch 31:  89%|████████▉ | 268/300 [01:58<00:13,  2.43it/s]Epoch 31:  90%|████████▉ | 269/300 [01:58<00:12,  2.48it/s]Epoch 31:  90%|█████████ | 270/300 [01:58<00:11,  2.57it/s]Epoch 31:  90%|█████████ | 271/300 [01:59<00:11,  2.55it/s]Epoch 31:  91%|█████████ | 272/300 [01:59<00:11,  2.34it/s]Epoch 31:  91%|█████████ | 273/300 [02:00<00:11,  2.33it/s]Epoch 31:  91%|█████████▏| 274/300 [02:00<00:10,  2.45it/s]Epoch 31:  92%|█████████▏| 275/300 [02:00<00:10,  2.41it/s]Epoch 31:  92%|█████████▏| 276/300 [02:01<00:09,  2.51it/s]Epoch 31:  92%|█████████▏| 277/300 [02:01<00:09,  2.39it/s]Epoch 31:  93%|█████████▎| 278/300 [02:02<00:08,  2.51it/s]Epoch 31:  93%|█████████▎| 279/300 [02:02<00:08,  2.61it/s]06/19/2022 15:37:16 - INFO - __main__ - global step: 4790; train loss: 3.860196352005005; dev loss: 3.62519907951355
Epoch 31:  93%|█████████▎| 280/300 [02:02<00:07,  2.64it/s]Epoch 31:  94%|█████████▎| 281/300 [02:03<00:07,  2.48it/s]Epoch 31:  94%|█████████▍| 282/300 [02:03<00:07,  2.41it/s]Epoch 31:  94%|█████████▍| 283/300 [02:04<00:06,  2.46it/s]Epoch 31:  95%|█████████▍| 284/300 [02:04<00:06,  2.49it/s]Epoch 31:  95%|█████████▌| 285/300 [02:04<00:06,  2.39it/s]Epoch 31:  95%|█████████▌| 286/300 [02:05<00:05,  2.42it/s]Epoch 31:  96%|█████████▌| 287/300 [02:05<00:05,  2.51it/s]Epoch 31:  96%|█████████▌| 288/300 [02:06<00:04,  2.51it/s]Epoch 31:  96%|█████████▋| 289/300 [02:06<00:04,  2.39it/s]Epoch 31:  97%|█████████▋| 290/300 [02:06<00:04,  2.49it/s]Epoch 31:  97%|█████████▋| 291/300 [02:07<00:03,  2.58it/s]Epoch 31:  97%|█████████▋| 292/300 [02:07<00:02,  2.67it/s]Epoch 31:  98%|█████████▊| 293/300 [02:08<00:02,  2.41it/s]Epoch 31:  98%|█████████▊| 294/300 [02:08<00:02,  2.53it/s]Epoch 31:  98%|█████████▊| 295/300 [02:08<00:01,  2.52it/s]Epoch 31:  99%|█████████▊| 296/300 [02:09<00:01,  2.59it/s]Epoch 31:  99%|█████████▉| 297/300 [02:09<00:01,  2.41it/s]Epoch 31:  99%|█████████▉| 298/300 [02:10<00:00,  2.34it/s]Epoch 31: 100%|█████████▉| 299/300 [02:10<00:00,  2.31it/s]06/19/2022 15:37:24 - INFO - __main__ - global step: 4800; train loss: 4.078799247741699; dev loss: 4.026501655578613
Epoch 31: 100%|██████████| 300/300 [02:11<00:00,  2.13it/s]Epoch 31: 100%|██████████| 300/300 [02:11<00:00,  2.29it/s]
Epoch 32:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 32:   0%|          | 1/300 [00:00<01:54,  2.61it/s]Epoch 32:   1%|          | 2/300 [00:00<02:15,  2.20it/s]Epoch 32:   1%|          | 3/300 [00:01<02:18,  2.14it/s]Epoch 32:   1%|▏         | 4/300 [00:01<02:21,  2.09it/s]Epoch 32:   2%|▏         | 5/300 [00:02<02:10,  2.26it/s]Epoch 32:   2%|▏         | 6/300 [00:02<02:26,  2.01it/s]Epoch 32:   2%|▏         | 7/300 [00:03<02:15,  2.17it/s]Epoch 32:   3%|▎         | 8/300 [00:03<02:13,  2.19it/s]Epoch 32:   3%|▎         | 9/300 [00:04<02:09,  2.24it/s]Epoch 32:   3%|▎         | 10/300 [00:04<02:21,  2.04it/s]Epoch 32:   4%|▎         | 11/300 [00:05<02:16,  2.11it/s]Epoch 32:   4%|▍         | 12/300 [00:05<02:15,  2.12it/s]Epoch 32:   4%|▍         | 13/300 [00:06<02:13,  2.15it/s]Epoch 32:   5%|▍         | 14/300 [00:06<02:25,  1.97it/s]Epoch 32:   5%|▌         | 15/300 [00:07<02:19,  2.04it/s]Epoch 32:   5%|▌         | 16/300 [00:07<02:15,  2.10it/s]Epoch 32:   6%|▌         | 17/300 [00:08<02:16,  2.07it/s]Epoch 32:   6%|▌         | 18/300 [00:08<02:15,  2.08it/s]Epoch 32:   6%|▋         | 19/300 [00:08<02:14,  2.10it/s]06/19/2022 15:37:33 - INFO - __main__ - global step: 4810; train loss: 4.282950401306152; dev loss: 4.105977535247803
Epoch 32:   7%|▋         | 20/300 [00:09<02:03,  2.26it/s]Epoch 32:   7%|▋         | 21/300 [00:09<01:59,  2.33it/s]Epoch 32:   7%|▋         | 22/300 [00:10<02:06,  2.20it/s]Epoch 32:   8%|▊         | 23/300 [00:10<02:03,  2.24it/s]Epoch 32:   8%|▊         | 24/300 [00:11<01:59,  2.32it/s]Epoch 32:   8%|▊         | 25/300 [00:11<01:57,  2.34it/s]Epoch 32:   9%|▊         | 26/300 [00:12<02:05,  2.18it/s]Epoch 32:   9%|▉         | 27/300 [00:12<02:05,  2.18it/s]Epoch 32:   9%|▉         | 28/300 [00:12<02:07,  2.13it/s]Epoch 32:  10%|▉         | 29/300 [00:13<02:08,  2.11it/s]Epoch 32:  10%|█         | 30/300 [00:13<02:07,  2.11it/s]Epoch 32:  10%|█         | 31/300 [00:14<02:18,  1.94it/s]Epoch 32:  11%|█         | 32/300 [00:15<02:13,  2.00it/s]Epoch 32:  11%|█         | 33/300 [00:15<02:06,  2.12it/s]Epoch 32:  11%|█▏        | 34/300 [00:15<02:04,  2.14it/s]Epoch 32:  12%|█▏        | 35/300 [00:16<02:11,  2.02it/s]Epoch 32:  12%|█▏        | 36/300 [00:16<02:06,  2.09it/s]Epoch 32:  12%|█▏        | 37/300 [00:17<01:56,  2.26it/s]Epoch 32:  13%|█▎        | 38/300 [00:17<01:55,  2.27it/s]Epoch 32:  13%|█▎        | 39/300 [00:18<02:04,  2.10it/s]06/19/2022 15:37:43 - INFO - __main__ - global step: 4820; train loss: 4.356274604797363; dev loss: 4.020181179046631
Epoch 32:  13%|█▎        | 40/300 [00:18<02:04,  2.08it/s]Epoch 32:  14%|█▎        | 41/300 [00:19<02:00,  2.16it/s]Epoch 32:  14%|█▍        | 42/300 [00:19<01:59,  2.16it/s]Epoch 32:  14%|█▍        | 43/300 [00:20<02:11,  1.96it/s]Epoch 32:  15%|█▍        | 44/300 [00:20<02:06,  2.02it/s]Epoch 32:  15%|█▌        | 45/300 [00:21<02:02,  2.08it/s]Epoch 32:  15%|█▌        | 46/300 [00:21<02:01,  2.09it/s]Epoch 32:  16%|█▌        | 47/300 [00:22<02:10,  1.94it/s]Epoch 32:  16%|█▌        | 48/300 [00:22<02:06,  2.00it/s]Epoch 32:  16%|█▋        | 49/300 [00:23<02:03,  2.03it/s]Epoch 32:  17%|█▋        | 50/300 [00:23<02:01,  2.06it/s]Epoch 32:  17%|█▋        | 51/300 [00:24<02:00,  2.07it/s]Epoch 32:  17%|█▋        | 52/300 [00:24<01:50,  2.25it/s]Epoch 32:  18%|█▊        | 53/300 [00:24<01:45,  2.35it/s]Epoch 32:  18%|█▊        | 54/300 [00:25<01:45,  2.33it/s]Epoch 32:  18%|█▊        | 55/300 [00:25<01:45,  2.33it/s]Epoch 32:  19%|█▊        | 56/300 [00:26<02:00,  2.03it/s]Epoch 32:  19%|█▉        | 57/300 [00:26<01:56,  2.08it/s]Epoch 32:  19%|█▉        | 58/300 [00:27<01:55,  2.10it/s]Epoch 32:  20%|█▉        | 59/300 [00:27<01:47,  2.24it/s]06/19/2022 15:37:52 - INFO - __main__ - global step: 4830; train loss: 4.250975608825684; dev loss: 3.9721035957336426
Epoch 32:  20%|██        | 60/300 [00:28<01:48,  2.22it/s]Epoch 32:  20%|██        | 61/300 [00:28<01:41,  2.35it/s]Epoch 32:  21%|██        | 62/300 [00:28<01:36,  2.46it/s]Epoch 32:  21%|██        | 63/300 [00:29<01:39,  2.38it/s]Epoch 32:  21%|██▏       | 64/300 [00:29<01:53,  2.09it/s]Epoch 32:  22%|██▏       | 65/300 [00:30<01:54,  2.05it/s]Epoch 32:  22%|██▏       | 66/300 [00:30<01:55,  2.02it/s]Epoch 32:  22%|██▏       | 67/300 [00:31<01:57,  1.99it/s]Epoch 32:  23%|██▎       | 68/300 [00:31<01:58,  1.95it/s]Epoch 32:  23%|██▎       | 69/300 [00:32<01:54,  2.01it/s]Epoch 32:  23%|██▎       | 70/300 [00:32<01:49,  2.10it/s]Epoch 32:  24%|██▎       | 71/300 [00:33<01:45,  2.16it/s]Epoch 32:  24%|██▍       | 72/300 [00:33<01:49,  2.08it/s]Epoch 32:  24%|██▍       | 73/300 [00:34<01:41,  2.24it/s]Epoch 32:  25%|██▍       | 74/300 [00:34<01:34,  2.38it/s]Epoch 32:  25%|██▌       | 75/300 [00:34<01:34,  2.39it/s]Epoch 32:  25%|██▌       | 76/300 [00:35<01:46,  2.09it/s]Epoch 32:  26%|██▌       | 77/300 [00:36<01:46,  2.10it/s]Epoch 32:  26%|██▌       | 78/300 [00:36<01:45,  2.11it/s]Epoch 32:  26%|██▋       | 79/300 [00:36<01:44,  2.11it/s]06/19/2022 15:38:02 - INFO - __main__ - global step: 4840; train loss: 4.141045093536377; dev loss: 4.185442924499512
Epoch 32:  27%|██▋       | 80/300 [00:37<01:53,  1.94it/s]Epoch 32:  27%|██▋       | 81/300 [00:38<01:57,  1.86it/s]Epoch 32:  27%|██▋       | 82/300 [00:38<01:52,  1.94it/s]Epoch 32:  28%|██▊       | 83/300 [00:39<01:47,  2.01it/s]Epoch 32:  28%|██▊       | 84/300 [00:39<01:43,  2.10it/s]Epoch 32:  28%|██▊       | 85/300 [00:40<01:44,  2.07it/s]Epoch 32:  29%|██▊       | 86/300 [00:40<01:34,  2.27it/s]Epoch 32:  29%|██▉       | 87/300 [00:40<01:26,  2.45it/s]Epoch 32:  29%|██▉       | 88/300 [00:41<01:21,  2.59it/s]Epoch 32:  30%|██▉       | 89/300 [00:41<01:24,  2.51it/s]Epoch 32:  30%|███       | 90/300 [00:41<01:20,  2.61it/s]Epoch 32:  30%|███       | 91/300 [00:42<01:18,  2.65it/s]Epoch 32:  31%|███       | 92/300 [00:42<01:15,  2.76it/s]Epoch 32:  31%|███       | 93/300 [00:42<01:18,  2.63it/s]Epoch 32:  31%|███▏      | 94/300 [00:43<01:22,  2.50it/s]Epoch 32:  32%|███▏      | 95/300 [00:43<01:25,  2.40it/s]Epoch 32:  32%|███▏      | 96/300 [00:44<01:27,  2.34it/s]Epoch 32:  32%|███▏      | 97/300 [00:44<01:32,  2.19it/s]Epoch 32:  33%|███▎      | 98/300 [00:45<01:24,  2.39it/s]Epoch 32:  33%|███▎      | 99/300 [00:45<01:18,  2.55it/s]06/19/2022 15:38:10 - INFO - __main__ - global step: 4850; train loss: 4.133444786071777; dev loss: 4.300552845001221
Epoch 32:  33%|███▎      | 100/300 [00:45<01:14,  2.68it/s]Epoch 32:  34%|███▎      | 101/300 [00:46<01:17,  2.58it/s]Epoch 32:  34%|███▍      | 102/300 [00:46<01:16,  2.58it/s]Epoch 32:  34%|███▍      | 103/300 [00:47<01:19,  2.47it/s]Epoch 32:  35%|███▍      | 104/300 [00:47<01:21,  2.41it/s]Epoch 32:  35%|███▌      | 105/300 [00:48<01:32,  2.11it/s]Epoch 32:  35%|███▌      | 106/300 [00:48<01:27,  2.21it/s]Epoch 32:  36%|███▌      | 107/300 [00:48<01:20,  2.40it/s]Epoch 32:  36%|███▌      | 108/300 [00:49<01:14,  2.56it/s]Epoch 32:  36%|███▋      | 109/300 [00:49<01:11,  2.68it/s]Epoch 32:  37%|███▋      | 110/300 [00:49<01:14,  2.55it/s]Epoch 32:  37%|███▋      | 111/300 [00:50<01:17,  2.44it/s]Epoch 32:  37%|███▋      | 112/300 [00:50<01:21,  2.31it/s]Epoch 32:  38%|███▊      | 113/300 [00:51<01:20,  2.32it/s]Epoch 32:  38%|███▊      | 114/300 [00:51<01:26,  2.15it/s]Epoch 32:  38%|███▊      | 115/300 [00:52<01:26,  2.14it/s]Epoch 32:  39%|███▊      | 116/300 [00:52<01:24,  2.17it/s]Epoch 32:  39%|███▉      | 117/300 [00:53<01:26,  2.12it/s]Epoch 32:  39%|███▉      | 118/300 [00:53<01:33,  1.94it/s]Epoch 32:  40%|███▉      | 119/300 [00:54<01:35,  1.90it/s]06/19/2022 15:38:19 - INFO - __main__ - global step: 4860; train loss: 4.217138767242432; dev loss: 4.163788318634033
Epoch 32:  40%|████      | 120/300 [00:54<01:33,  1.93it/s]Epoch 32:  40%|████      | 121/300 [00:55<01:23,  2.13it/s]Epoch 32:  41%|████      | 122/300 [00:55<01:22,  2.16it/s]Epoch 32:  41%|████      | 123/300 [00:56<01:15,  2.34it/s]Epoch 32:  41%|████▏     | 124/300 [00:56<01:14,  2.36it/s]Epoch 32:  42%|████▏     | 125/300 [00:56<01:13,  2.39it/s]Epoch 32:  42%|████▏     | 126/300 [00:57<01:16,  2.27it/s]Epoch 32:  42%|████▏     | 127/300 [00:57<01:18,  2.19it/s]Epoch 32:  43%|████▎     | 128/300 [00:58<01:18,  2.18it/s]Epoch 32:  43%|████▎     | 129/300 [00:58<01:19,  2.16it/s]Epoch 32:  43%|████▎     | 130/300 [00:59<01:20,  2.11it/s]Epoch 32:  44%|████▎     | 131/300 [00:59<01:16,  2.21it/s]Epoch 32:  44%|████▍     | 132/300 [01:00<01:11,  2.35it/s]Epoch 32:  44%|████▍     | 133/300 [01:00<01:09,  2.40it/s]Epoch 32:  45%|████▍     | 134/300 [01:01<01:20,  2.05it/s]Epoch 32:  45%|████▌     | 135/300 [01:01<01:18,  2.11it/s]Epoch 32:  45%|████▌     | 136/300 [01:02<01:16,  2.13it/s]Epoch 32:  46%|████▌     | 137/300 [01:02<01:13,  2.23it/s]Epoch 32:  46%|████▌     | 138/300 [01:02<01:06,  2.42it/s]Epoch 32:  46%|████▋     | 139/300 [01:03<01:07,  2.39it/s]06/19/2022 15:38:28 - INFO - __main__ - global step: 4870; train loss: 3.7649426460266113; dev loss: 3.7499451637268066
Epoch 32:  47%|████▋     | 140/300 [01:03<01:04,  2.47it/s]Epoch 32:  47%|████▋     | 141/300 [01:03<01:01,  2.59it/s]Epoch 32:  47%|████▋     | 142/300 [01:04<01:08,  2.31it/s]Epoch 32:  48%|████▊     | 143/300 [01:05<01:14,  2.12it/s]Epoch 32:  48%|████▊     | 144/300 [01:05<01:14,  2.08it/s]Epoch 32:  48%|████▊     | 145/300 [01:05<01:10,  2.19it/s]Epoch 32:  49%|████▊     | 146/300 [01:06<01:08,  2.25it/s]Epoch 32:  49%|████▉     | 147/300 [01:06<01:15,  2.03it/s]Epoch 32:  49%|████▉     | 148/300 [01:07<01:08,  2.21it/s]Epoch 32:  50%|████▉     | 149/300 [01:07<01:03,  2.36it/s]Epoch 32:  50%|█████     | 150/300 [01:08<01:00,  2.47it/s]Epoch 32:  50%|█████     | 151/300 [01:08<01:03,  2.36it/s]Epoch 32:  51%|█████     | 152/300 [01:08<00:58,  2.55it/s]Epoch 32:  51%|█████     | 153/300 [01:09<00:56,  2.61it/s]Epoch 32:  51%|█████▏    | 154/300 [01:09<00:54,  2.69it/s]Epoch 32:  52%|█████▏    | 155/300 [01:09<00:57,  2.50it/s]Epoch 32:  52%|█████▏    | 156/300 [01:10<01:01,  2.36it/s]Epoch 32:  52%|█████▏    | 157/300 [01:10<01:03,  2.27it/s]Epoch 32:  53%|█████▎    | 158/300 [01:11<00:59,  2.39it/s]Epoch 32:  53%|█████▎    | 159/300 [01:11<00:59,  2.37it/s]06/19/2022 15:38:36 - INFO - __main__ - global step: 4880; train loss: 4.122002124786377; dev loss: 4.052205562591553
Epoch 32:  53%|█████▎    | 160/300 [01:12<00:55,  2.54it/s]Epoch 32:  54%|█████▎    | 161/300 [01:12<00:52,  2.63it/s]Epoch 32:  54%|█████▍    | 162/300 [01:12<00:50,  2.74it/s]Epoch 32:  54%|█████▍    | 163/300 [01:13<00:48,  2.81it/s]Epoch 32:  55%|█████▍    | 164/300 [01:13<00:50,  2.67it/s]Epoch 32:  55%|█████▌    | 165/300 [01:13<00:49,  2.73it/s]Epoch 32:  55%|█████▌    | 166/300 [01:14<00:47,  2.81it/s]Epoch 32:  56%|█████▌    | 167/300 [01:14<00:46,  2.86it/s]Epoch 32:  56%|█████▌    | 168/300 [01:15<00:56,  2.35it/s]Epoch 32:  56%|█████▋    | 169/300 [01:15<00:57,  2.26it/s]Epoch 32:  57%|█████▋    | 170/300 [01:16<01:01,  2.13it/s]Epoch 32:  57%|█████▋    | 171/300 [01:16<00:59,  2.18it/s]Epoch 32:  57%|█████▋    | 172/300 [01:17<01:05,  1.94it/s]Epoch 32:  58%|█████▊    | 173/300 [01:17<01:05,  1.95it/s]Epoch 32:  58%|█████▊    | 174/300 [01:18<01:01,  2.04it/s]Epoch 32:  58%|█████▊    | 175/300 [01:18<01:00,  2.05it/s]Epoch 32:  59%|█████▊    | 176/300 [01:19<00:57,  2.14it/s]Epoch 32:  59%|█████▉    | 177/300 [01:19<00:52,  2.36it/s]Epoch 32:  59%|█████▉    | 178/300 [01:19<00:48,  2.53it/s]Epoch 32:  60%|█████▉    | 179/300 [01:20<00:45,  2.65it/s]06/19/2022 15:38:45 - INFO - __main__ - global step: 4890; train loss: 3.8101391792297363; dev loss: 3.5903332233428955
Epoch 32:  60%|██████    | 180/300 [01:20<00:47,  2.52it/s]Epoch 32:  60%|██████    | 181/300 [01:20<00:44,  2.65it/s]Epoch 32:  61%|██████    | 182/300 [01:21<00:43,  2.74it/s]Epoch 32:  61%|██████    | 183/300 [01:21<00:41,  2.82it/s]Epoch 32:  61%|██████▏   | 184/300 [01:22<00:51,  2.25it/s]Epoch 32:  62%|██████▏   | 185/300 [01:22<00:50,  2.26it/s]Epoch 32:  62%|██████▏   | 186/300 [01:23<00:50,  2.25it/s]Epoch 32:  62%|██████▏   | 187/300 [01:23<00:48,  2.34it/s]Epoch 32:  63%|██████▎   | 188/300 [01:23<00:52,  2.13it/s]Epoch 32:  63%|██████▎   | 189/300 [01:24<00:51,  2.17it/s]Epoch 32:  63%|██████▎   | 190/300 [01:24<00:49,  2.24it/s]Epoch 32:  64%|██████▎   | 191/300 [01:25<00:47,  2.31it/s]Epoch 32:  64%|██████▍   | 192/300 [01:25<00:44,  2.43it/s]Epoch 32:  64%|██████▍   | 193/300 [01:26<00:47,  2.26it/s]Epoch 32:  65%|██████▍   | 194/300 [01:26<00:43,  2.44it/s]Epoch 32:  65%|██████▌   | 195/300 [01:26<00:42,  2.45it/s]Epoch 32:  65%|██████▌   | 196/300 [01:27<00:39,  2.60it/s]Epoch 32:  66%|██████▌   | 197/300 [01:27<00:43,  2.39it/s]Epoch 32:  66%|██████▌   | 198/300 [01:28<00:40,  2.54it/s]Epoch 32:  66%|██████▋   | 199/300 [01:28<00:37,  2.68it/s]06/19/2022 15:38:53 - INFO - __main__ - global step: 4900; train loss: 4.153952598571777; dev loss: 4.118348121643066
Epoch 32:  67%|██████▋   | 200/300 [01:28<00:36,  2.77it/s]Epoch 32:  67%|██████▋   | 201/300 [01:29<00:37,  2.64it/s]Epoch 32:  67%|██████▋   | 202/300 [01:29<00:35,  2.76it/s]Epoch 32:  68%|██████▊   | 203/300 [01:29<00:34,  2.85it/s]Epoch 32:  68%|██████▊   | 204/300 [01:30<00:33,  2.89it/s]Epoch 32:  68%|██████▊   | 205/300 [01:30<00:46,  2.06it/s]Epoch 32:  69%|██████▊   | 206/300 [01:31<00:45,  2.07it/s]Epoch 32:  69%|██████▉   | 207/300 [01:31<00:44,  2.09it/s]Epoch 32:  69%|██████▉   | 208/300 [01:32<00:41,  2.19it/s]Epoch 32:  70%|██████▉   | 209/300 [01:32<00:44,  2.04it/s]Epoch 32:  70%|███████   | 210/300 [01:33<00:42,  2.11it/s]Epoch 32:  70%|███████   | 211/300 [01:33<00:41,  2.12it/s]Epoch 32:  71%|███████   | 212/300 [01:34<00:40,  2.15it/s]Epoch 32:  71%|███████   | 213/300 [01:34<00:43,  2.02it/s]Epoch 32:  71%|███████▏  | 214/300 [01:35<00:41,  2.09it/s]Epoch 32:  72%|███████▏  | 215/300 [01:35<00:38,  2.23it/s]Epoch 32:  72%|███████▏  | 216/300 [01:35<00:34,  2.43it/s]Epoch 32:  72%|███████▏  | 217/300 [01:36<00:32,  2.52it/s]Epoch 32:  73%|███████▎  | 218/300 [01:36<00:34,  2.38it/s]Epoch 32:  73%|███████▎  | 219/300 [01:37<00:35,  2.29it/s]06/19/2022 15:39:02 - INFO - __main__ - global step: 4910; train loss: 3.851952075958252; dev loss: 3.6961886882781982
Epoch 32:  73%|███████▎  | 220/300 [01:37<00:35,  2.23it/s]Epoch 32:  74%|███████▎  | 221/300 [01:38<00:38,  2.07it/s]Epoch 32:  74%|███████▍  | 222/300 [01:38<00:40,  1.93it/s]Epoch 32:  74%|███████▍  | 223/300 [01:39<00:38,  2.01it/s]Epoch 32:  75%|███████▍  | 224/300 [01:39<00:37,  2.00it/s]Epoch 32:  75%|███████▌  | 225/300 [01:40<00:36,  2.06it/s]Epoch 32:  75%|███████▌  | 226/300 [01:40<00:38,  1.94it/s]Epoch 32:  76%|███████▌  | 227/300 [01:41<00:34,  2.12it/s]Epoch 32:  76%|███████▌  | 228/300 [01:41<00:32,  2.22it/s]Epoch 32:  76%|███████▋  | 229/300 [01:41<00:31,  2.25it/s]Epoch 32:  77%|███████▋  | 230/300 [01:42<00:32,  2.17it/s]Epoch 32:  77%|███████▋  | 231/300 [01:42<00:30,  2.23it/s]Epoch 32:  77%|███████▋  | 232/300 [01:43<00:30,  2.22it/s]Epoch 32:  78%|███████▊  | 233/300 [01:43<00:29,  2.27it/s]Epoch 32:  78%|███████▊  | 234/300 [01:44<00:31,  2.11it/s]Epoch 32:  78%|███████▊  | 235/300 [01:44<00:30,  2.11it/s]Epoch 32:  79%|███████▊  | 236/300 [01:45<00:30,  2.08it/s]Epoch 32:  79%|███████▉  | 237/300 [01:45<00:29,  2.15it/s]Epoch 32:  79%|███████▉  | 238/300 [01:46<00:29,  2.14it/s]Epoch 32:  80%|███████▉  | 239/300 [01:46<00:27,  2.21it/s]06/19/2022 15:39:11 - INFO - __main__ - global step: 4920; train loss: 4.363994598388672; dev loss: 4.636006832122803
Epoch 32:  80%|████████  | 240/300 [01:47<00:26,  2.26it/s]Epoch 32:  80%|████████  | 241/300 [01:47<00:24,  2.39it/s]Epoch 32:  81%|████████  | 242/300 [01:47<00:26,  2.16it/s]Epoch 32:  81%|████████  | 243/300 [01:48<00:26,  2.12it/s]Epoch 32:  81%|████████▏ | 244/300 [01:48<00:27,  2.07it/s]Epoch 32:  82%|████████▏ | 245/300 [01:49<00:25,  2.15it/s]Epoch 32:  82%|████████▏ | 246/300 [01:49<00:24,  2.24it/s]Epoch 32:  82%|████████▏ | 247/300 [01:50<00:25,  2.12it/s]Epoch 32:  83%|████████▎ | 248/300 [01:50<00:23,  2.23it/s]Epoch 32:  83%|████████▎ | 249/300 [01:51<00:22,  2.30it/s]Epoch 32:  83%|████████▎ | 250/300 [01:51<00:21,  2.35it/s]Epoch 32:  84%|████████▎ | 251/300 [01:52<00:21,  2.26it/s]Epoch 32:  84%|████████▍ | 252/300 [01:52<00:20,  2.40it/s]Epoch 32:  84%|████████▍ | 253/300 [01:52<00:18,  2.48it/s]Epoch 32:  85%|████████▍ | 254/300 [01:53<00:17,  2.58it/s]Epoch 32:  85%|████████▌ | 255/300 [01:53<00:18,  2.41it/s]Epoch 32:  85%|████████▌ | 256/300 [01:54<00:18,  2.35it/s]Epoch 32:  86%|████████▌ | 257/300 [01:54<00:17,  2.45it/s]Epoch 32:  86%|████████▌ | 258/300 [01:54<00:17,  2.41it/s]Epoch 32:  86%|████████▋ | 259/300 [01:55<00:17,  2.28it/s]06/19/2022 15:39:20 - INFO - __main__ - global step: 4930; train loss: 4.37337064743042; dev loss: 4.2049126625061035
Epoch 32:  87%|████████▋ | 260/300 [01:55<00:18,  2.21it/s]Epoch 32:  87%|████████▋ | 261/300 [01:56<00:17,  2.22it/s]Epoch 32:  87%|████████▋ | 262/300 [01:56<00:17,  2.17it/s]Epoch 32:  88%|████████▊ | 263/300 [01:57<00:18,  2.04it/s]Epoch 32:  88%|████████▊ | 264/300 [01:57<00:17,  2.11it/s]Epoch 32:  88%|████████▊ | 265/300 [01:58<00:16,  2.11it/s]Epoch 32:  89%|████████▊ | 266/300 [01:58<00:15,  2.13it/s]Epoch 32:  89%|████████▉ | 267/300 [01:59<00:16,  1.98it/s]Epoch 32:  89%|████████▉ | 268/300 [01:59<00:16,  1.98it/s]Epoch 32:  90%|████████▉ | 269/300 [02:00<00:16,  1.85it/s]Epoch 32:  90%|█████████ | 270/300 [02:00<00:15,  1.92it/s]Epoch 32:  90%|█████████ | 271/300 [02:01<00:14,  2.02it/s]Epoch 32:  91%|█████████ | 272/300 [02:01<00:14,  1.91it/s]Epoch 32:  91%|█████████ | 273/300 [02:02<00:12,  2.11it/s]Epoch 32:  91%|█████████▏| 274/300 [02:02<00:12,  2.07it/s]Epoch 32:  92%|█████████▏| 275/300 [02:03<00:11,  2.21it/s]Epoch 32:  92%|█████████▏| 276/300 [02:03<00:11,  2.01it/s]Epoch 32:  92%|█████████▏| 277/300 [02:04<00:10,  2.16it/s]Epoch 32:  93%|█████████▎| 278/300 [02:04<00:09,  2.22it/s]Epoch 32:  93%|█████████▎| 279/300 [02:04<00:08,  2.36it/s]06/19/2022 15:39:29 - INFO - __main__ - global step: 4940; train loss: 4.093836307525635; dev loss: 4.176120758056641
Epoch 32:  93%|█████████▎| 280/300 [02:05<00:08,  2.28it/s]Epoch 32:  94%|█████████▎| 281/300 [02:05<00:08,  2.20it/s]Epoch 32:  94%|█████████▍| 282/300 [02:06<00:08,  2.12it/s]Epoch 32:  94%|█████████▍| 283/300 [02:06<00:07,  2.13it/s]Epoch 32:  95%|█████████▍| 284/300 [02:07<00:07,  2.03it/s]Epoch 32:  95%|█████████▌| 285/300 [02:07<00:06,  2.22it/s]Epoch 32:  95%|█████████▌| 286/300 [02:08<00:06,  2.32it/s]Epoch 32:  96%|█████████▌| 287/300 [02:08<00:05,  2.45it/s]Epoch 32:  96%|█████████▌| 288/300 [02:08<00:05,  2.29it/s]Epoch 32:  96%|█████████▋| 289/300 [02:09<00:05,  2.09it/s]Epoch 32:  97%|█████████▋| 290/300 [02:10<00:04,  2.03it/s]Epoch 32:  97%|█████████▋| 291/300 [02:10<00:04,  2.02it/s]Epoch 32:  97%|█████████▋| 292/300 [02:11<00:04,  1.95it/s]Epoch 32:  98%|█████████▊| 293/300 [02:11<00:03,  2.08it/s]Epoch 32:  98%|█████████▊| 294/300 [02:11<00:02,  2.19it/s]Epoch 32:  98%|█████████▊| 295/300 [02:12<00:02,  2.20it/s]Epoch 32:  99%|█████████▊| 296/300 [02:12<00:01,  2.08it/s]Epoch 32:  99%|█████████▉| 297/300 [02:13<00:01,  2.21it/s]Epoch 32:  99%|█████████▉| 298/300 [02:13<00:00,  2.32it/s]Epoch 32: 100%|█████████▉| 299/300 [02:14<00:00,  2.27it/s]06/19/2022 15:39:39 - INFO - __main__ - global step: 4950; train loss: 4.9010725021362305; dev loss: 4.524133682250977
Epoch 32: 100%|██████████| 300/300 [02:14<00:00,  2.19it/s]Epoch 32: 100%|██████████| 300/300 [02:14<00:00,  2.23it/s]
Epoch 33:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 33:   0%|          | 1/300 [00:00<02:25,  2.06it/s]Epoch 33:   1%|          | 2/300 [00:00<02:19,  2.14it/s]Epoch 33:   1%|          | 3/300 [00:01<02:12,  2.24it/s]Epoch 33:   1%|▏         | 4/300 [00:01<02:13,  2.22it/s]Epoch 33:   2%|▏         | 5/300 [00:02<02:26,  2.01it/s]Epoch 33:   2%|▏         | 6/300 [00:02<02:24,  2.03it/s]Epoch 33:   2%|▏         | 7/300 [00:03<02:12,  2.21it/s]Epoch 33:   3%|▎         | 8/300 [00:03<02:05,  2.32it/s]Epoch 33:   3%|▎         | 9/300 [00:04<02:06,  2.30it/s]Epoch 33:   3%|▎         | 10/300 [00:04<01:57,  2.46it/s]Epoch 33:   4%|▎         | 11/300 [00:04<01:59,  2.42it/s]Epoch 33:   4%|▍         | 12/300 [00:05<02:03,  2.33it/s]Epoch 33:   4%|▍         | 13/300 [00:05<02:16,  2.10it/s]Epoch 33:   5%|▍         | 14/300 [00:06<02:11,  2.17it/s]Epoch 33:   5%|▌         | 15/300 [00:06<02:10,  2.18it/s]Epoch 33:   5%|▌         | 16/300 [00:07<02:10,  2.17it/s]Epoch 33:   6%|▌         | 17/300 [00:07<02:22,  1.98it/s]Epoch 33:   6%|▌         | 18/300 [00:08<02:16,  2.06it/s]Epoch 33:   6%|▋         | 19/300 [00:08<02:05,  2.24it/s]06/19/2022 15:39:48 - INFO - __main__ - global step: 4960; train loss: 4.103193283081055; dev loss: 3.9663262367248535
Epoch 33:   7%|▋         | 20/300 [00:09<02:05,  2.23it/s]Epoch 33:   7%|▋         | 21/300 [00:09<02:13,  2.09it/s]Epoch 33:   7%|▋         | 22/300 [00:10<02:05,  2.21it/s]Epoch 33:   8%|▊         | 23/300 [00:10<01:58,  2.34it/s]Epoch 33:   8%|▊         | 24/300 [00:10<01:58,  2.33it/s]Epoch 33:   8%|▊         | 25/300 [00:11<01:57,  2.34it/s]Epoch 33:   9%|▊         | 26/300 [00:11<02:06,  2.17it/s]Epoch 33:   9%|▉         | 27/300 [00:12<02:06,  2.17it/s]Epoch 33:   9%|▉         | 28/300 [00:12<02:07,  2.14it/s]Epoch 33:  10%|▉         | 29/300 [00:13<02:11,  2.06it/s]Epoch 33:  10%|█         | 30/300 [00:13<02:22,  1.89it/s]Epoch 33:  10%|█         | 31/300 [00:14<02:07,  2.11it/s]Epoch 33:  11%|█         | 32/300 [00:14<01:58,  2.27it/s]Epoch 33:  11%|█         | 33/300 [00:14<01:51,  2.40it/s]Epoch 33:  11%|█▏        | 34/300 [00:15<01:57,  2.26it/s]Epoch 33:  12%|█▏        | 35/300 [00:15<01:52,  2.35it/s]Epoch 33:  12%|█▏        | 36/300 [00:16<01:56,  2.26it/s]Epoch 33:  12%|█▏        | 37/300 [00:16<01:59,  2.19it/s]Epoch 33:  13%|█▎        | 38/300 [00:17<02:11,  1.99it/s]Epoch 33:  13%|█▎        | 39/300 [00:17<02:02,  2.13it/s]06/19/2022 15:39:57 - INFO - __main__ - global step: 4970; train loss: 4.717625617980957; dev loss: 4.499103546142578
Epoch 33:  13%|█▎        | 40/300 [00:18<01:51,  2.34it/s]Epoch 33:  14%|█▎        | 41/300 [00:18<01:44,  2.49it/s]Epoch 33:  14%|█▍        | 42/300 [00:19<01:52,  2.29it/s]Epoch 33:  14%|█▍        | 43/300 [00:19<01:50,  2.32it/s]Epoch 33:  15%|█▍        | 44/300 [00:19<01:44,  2.44it/s]Epoch 33:  15%|█▌        | 45/300 [00:20<01:45,  2.42it/s]Epoch 33:  15%|█▌        | 46/300 [00:20<01:52,  2.25it/s]Epoch 33:  16%|█▌        | 47/300 [00:21<01:47,  2.35it/s]Epoch 33:  16%|█▌        | 48/300 [00:21<01:40,  2.50it/s]Epoch 33:  16%|█▋        | 49/300 [00:21<01:34,  2.65it/s]Epoch 33:  17%|█▋        | 50/300 [00:22<01:38,  2.53it/s]Epoch 33:  17%|█▋        | 51/300 [00:22<01:33,  2.65it/s]Epoch 33:  17%|█▋        | 52/300 [00:22<01:30,  2.74it/s]Epoch 33:  18%|█▊        | 53/300 [00:23<01:27,  2.81it/s]Epoch 33:  18%|█▊        | 54/300 [00:23<01:25,  2.88it/s]Epoch 33:  18%|█▊        | 55/300 [00:23<01:31,  2.68it/s]Epoch 33:  19%|█▊        | 56/300 [00:24<01:28,  2.77it/s]Epoch 33:  19%|█▉        | 57/300 [00:24<01:25,  2.83it/s]Epoch 33:  19%|█▉        | 58/300 [00:24<01:24,  2.88it/s]Epoch 33:  20%|█▉        | 59/300 [00:25<01:30,  2.67it/s]06/19/2022 15:40:05 - INFO - __main__ - global step: 4980; train loss: 4.281176567077637; dev loss: 4.370337963104248
Epoch 33:  20%|██        | 60/300 [00:25<01:29,  2.67it/s]Epoch 33:  20%|██        | 61/300 [00:26<01:34,  2.53it/s]Epoch 33:  21%|██        | 62/300 [00:26<01:41,  2.35it/s]Epoch 33:  21%|██        | 63/300 [00:27<01:51,  2.12it/s]Epoch 33:  21%|██▏       | 64/300 [00:27<01:51,  2.11it/s]Epoch 33:  22%|██▏       | 65/300 [00:28<01:44,  2.26it/s]Epoch 33:  22%|██▏       | 66/300 [00:28<01:43,  2.26it/s]Epoch 33:  22%|██▏       | 67/300 [00:29<01:44,  2.22it/s]Epoch 33:  23%|██▎       | 68/300 [00:29<01:40,  2.31it/s]Epoch 33:  23%|██▎       | 69/300 [00:29<01:38,  2.35it/s]Epoch 33:  23%|██▎       | 70/300 [00:30<01:43,  2.23it/s]Epoch 33:  24%|██▎       | 71/300 [00:30<01:44,  2.20it/s]Epoch 33:  24%|██▍       | 72/300 [00:31<01:37,  2.33it/s]Epoch 33:  24%|██▍       | 73/300 [00:31<01:36,  2.35it/s]Epoch 33:  25%|██▍       | 74/300 [00:32<01:31,  2.46it/s]Epoch 33:  25%|██▌       | 75/300 [00:32<01:35,  2.35it/s]Epoch 33:  25%|██▌       | 76/300 [00:32<01:39,  2.25it/s]Epoch 33:  26%|██▌       | 77/300 [00:33<01:36,  2.30it/s]Epoch 33:  26%|██▌       | 78/300 [00:33<01:29,  2.49it/s]Epoch 33:  26%|██▋       | 79/300 [00:34<01:24,  2.62it/s]06/19/2022 15:40:13 - INFO - __main__ - global step: 4990; train loss: 3.439271926879883; dev loss: 3.546583652496338
Epoch 33:  27%|██▋       | 80/300 [00:34<01:27,  2.52it/s]Epoch 33:  27%|██▋       | 81/300 [00:34<01:25,  2.57it/s]Epoch 33:  27%|██▋       | 82/300 [00:35<01:35,  2.28it/s]Epoch 33:  28%|██▊       | 83/300 [00:35<01:33,  2.31it/s]Epoch 33:  28%|██▊       | 84/300 [00:36<01:44,  2.07it/s]Epoch 33:  28%|██▊       | 85/300 [00:36<01:37,  2.22it/s]Epoch 33:  29%|██▊       | 86/300 [00:37<01:30,  2.37it/s]Epoch 33:  29%|██▉       | 87/300 [00:37<01:25,  2.48it/s]Epoch 33:  29%|██▉       | 88/300 [00:37<01:30,  2.35it/s]Epoch 33:  30%|██▉       | 89/300 [00:38<01:28,  2.40it/s]Epoch 33:  30%|███       | 90/300 [00:38<01:29,  2.35it/s]Epoch 33:  30%|███       | 91/300 [00:39<01:30,  2.31it/s]Epoch 33:  31%|███       | 92/300 [00:39<01:39,  2.08it/s]Epoch 33:  31%|███       | 93/300 [00:40<01:37,  2.13it/s]Epoch 33:  31%|███▏      | 94/300 [00:40<01:35,  2.15it/s]Epoch 33:  32%|███▏      | 95/300 [00:41<01:34,  2.17it/s]Epoch 33:  32%|███▏      | 96/300 [00:41<01:42,  2.00it/s]Epoch 33:  32%|███▏      | 97/300 [00:42<01:37,  2.08it/s]Epoch 33:  33%|███▎      | 98/300 [00:42<01:38,  2.05it/s]Epoch 33:  33%|███▎      | 99/300 [00:43<01:39,  2.01it/s]06/19/2022 15:40:22 - INFO - __main__ - global step: 5000; train loss: 4.291531562805176; dev loss: 4.441502094268799
Epoch 33:  33%|███▎      | 99/300 [00:43<01:28,  2.26it/s]
06/19/2022 15:40:22 - INFO - __main__ - save model!
t5base para fomaml downstream
Task: glue-mrpc, Checkpoint: models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt, Identifier: T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nopara2para.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nopara2para.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-mrpc/log.txt'
06/19/2022 15:40:27 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 15:40:27 - INFO - __main__ - models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-mrpc
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_nopara2para.py', '--local_rank=1', '--task_dir', 'data/glue-mrpc/', '--task_name', 'glue-mrpc', '--identifier', 'T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-mrpc', '--cuda', '2,3', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', '--model', 'google/t5-v1_1-base', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 87738
Killing subprocess 87739
++++++++++++++++++++++++++++++
kill: (87745): No such process
Task: glue-qqp, Checkpoint: models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt, Identifier: T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nopara2para.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nopara2para.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-qqp/log.txt'
06/19/2022 15:40:30 - INFO - __main__ - Namespace(task_dir='data/glue-qqp/', task_name='glue-qqp', identifier='T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-qqp', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 15:40:30 - INFO - __main__ - models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-qqp
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_nopara2para.py', '--local_rank=1', '--task_dir', 'data/glue-qqp/', '--task_name', 'glue-qqp', '--identifier', 'T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-qqp', '--cuda', '2,3', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', '--model', 'google/t5-v1_1-base', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 87750
Killing subprocess 87751
++++++++++++++++++++++++++++++
kill: (87755): No such process
Task: medical_questions_pairs, Checkpoint: models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt, Identifier: T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nopara2para.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nopara2para.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-medical_questions_pairs/log.txt'
06/19/2022 15:40:34 - INFO - __main__ - Namespace(task_dir='data/medical_questions_pairs/', task_name='medical_questions_pairs', identifier='T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-medical_questions_pairs', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 15:40:34 - INFO - __main__ - models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-medical_questions_pairs
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_nopara2para.py', '--local_rank=1', '--task_dir', 'data/medical_questions_pairs/', '--task_name', 'medical_questions_pairs', '--identifier', 'T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-medical_questions_pairs', '--cuda', '2,3', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', '--model', 'google/t5-v1_1-base', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 87760
Killing subprocess 87761
++++++++++++++++++++++++++++++
kill: (87767): No such process
Task: paws, Checkpoint: models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt, Identifier: T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1
06/19/2022 15:40:38 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 15:40:38 - INFO - __main__ - models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws
06/19/2022 15:40:38 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 15:40:38 - INFO - __main__ - models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws
06/19/2022 15:40:38 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/19/2022 15:40:38 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/19/2022 15:40:38 - INFO - __main__ - args.device: cuda:0
06/19/2022 15:40:38 - INFO - __main__ - Using 2 gpus
06/19/2022 15:40:38 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/19/2022 15:40:38 - INFO - __main__ - args.device: cuda:1
06/19/2022 15:40:38 - INFO - __main__ - Using 2 gpus
06/19/2022 15:40:38 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/19/2022 15:40:43 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.5, bsz=8 ...
06/19/2022 15:40:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:40:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:40:44 - INFO - __main__ - Printing 3 examples
06/19/2022 15:40:44 - INFO - __main__ - Printing 3 examples
06/19/2022 15:40:44 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 15:40:44 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 15:40:44 - INFO - __main__ - ['1']
06/19/2022 15:40:44 - INFO - __main__ - ['1']
06/19/2022 15:40:44 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 15:40:44 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 15:40:44 - INFO - __main__ - ['1']
06/19/2022 15:40:44 - INFO - __main__ - ['1']
06/19/2022 15:40:44 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 15:40:44 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 15:40:44 - INFO - __main__ - ['1']
06/19/2022 15:40:44 - INFO - __main__ - ['1']
06/19/2022 15:40:44 - INFO - __main__ - Tokenizing Input ...
06/19/2022 15:40:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 15:40:44 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:40:44 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:40:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 15:40:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:40:44 - INFO - __main__ - Printing 3 examples
06/19/2022 15:40:44 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 15:40:44 - INFO - __main__ - ['1']
06/19/2022 15:40:44 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 15:40:44 - INFO - __main__ - ['1']
06/19/2022 15:40:44 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 15:40:44 - INFO - __main__ - ['1']
06/19/2022 15:40:44 - INFO - __main__ - Tokenizing Input ...
06/19/2022 15:40:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 15:40:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:40:44 - INFO - __main__ - Printing 3 examples
06/19/2022 15:40:44 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 15:40:44 - INFO - __main__ - ['1']
06/19/2022 15:40:44 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 15:40:44 - INFO - __main__ - ['1']
06/19/2022 15:40:44 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 15:40:44 - INFO - __main__ - ['1']
06/19/2022 15:40:44 - INFO - __main__ - Tokenizing Input ...
06/19/2022 15:40:44 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:40:44 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:40:44 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 15:40:44 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 15:40:50 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 15:40:50 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 15:40:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 15:40:51 - INFO - __main__ - Starting training!
06/19/2022 15:40:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 15:40:56 - INFO - __main__ - Starting training!
06/19/2022 15:40:59 - INFO - __main__ - Step 10 Global step 10 Train loss 4.05 on epoch=4
06/19/2022 15:41:00 - INFO - __main__ - Step 20 Global step 20 Train loss 3.57 on epoch=9
06/19/2022 15:41:01 - INFO - __main__ - Step 30 Global step 30 Train loss 2.80 on epoch=14
06/19/2022 15:41:03 - INFO - __main__ - Step 40 Global step 40 Train loss 2.18 on epoch=19
06/19/2022 15:41:04 - INFO - __main__ - Step 50 Global step 50 Train loss 1.87 on epoch=24
06/19/2022 15:41:05 - INFO - __main__ - Global step 50 Train loss 2.89 Classification-F1 0.3333333333333333 on epoch=24
06/19/2022 15:41:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/19/2022 15:41:06 - INFO - __main__ - Step 60 Global step 60 Train loss 1.64 on epoch=29
06/19/2022 15:41:07 - INFO - __main__ - Step 70 Global step 70 Train loss 1.44 on epoch=34
06/19/2022 15:41:08 - INFO - __main__ - Step 80 Global step 80 Train loss 1.31 on epoch=39
06/19/2022 15:41:10 - INFO - __main__ - Step 90 Global step 90 Train loss 1.10 on epoch=44
06/19/2022 15:41:11 - INFO - __main__ - Step 100 Global step 100 Train loss 1.18 on epoch=49
06/19/2022 15:41:12 - INFO - __main__ - Global step 100 Train loss 1.33 Classification-F1 0.3333333333333333 on epoch=49
06/19/2022 15:41:13 - INFO - __main__ - Step 110 Global step 110 Train loss 1.11 on epoch=54
06/19/2022 15:41:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.97 on epoch=59
06/19/2022 15:41:15 - INFO - __main__ - Step 130 Global step 130 Train loss 1.04 on epoch=64
06/19/2022 15:41:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.82 on epoch=69
06/19/2022 15:41:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.96 on epoch=74
06/19/2022 15:41:18 - INFO - __main__ - Global step 150 Train loss 0.98 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 15:41:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.91 on epoch=79
06/19/2022 15:41:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.75 on epoch=84
06/19/2022 15:41:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.80 on epoch=89
06/19/2022 15:41:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.78 on epoch=94
06/19/2022 15:41:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.85 on epoch=99
06/19/2022 15:41:25 - INFO - __main__ - Global step 200 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 15:41:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.73 on epoch=104
06/19/2022 15:41:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.73 on epoch=109
06/19/2022 15:41:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.76 on epoch=114
06/19/2022 15:41:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.67 on epoch=119
06/19/2022 15:41:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.72 on epoch=124
06/19/2022 15:41:31 - INFO - __main__ - Global step 250 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 15:41:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.63 on epoch=129
06/19/2022 15:41:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.71 on epoch=134
06/19/2022 15:41:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.66 on epoch=139
06/19/2022 15:41:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.69 on epoch=144
06/19/2022 15:41:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.64 on epoch=149
06/19/2022 15:41:38 - INFO - __main__ - Global step 300 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 15:41:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.64 on epoch=154
06/19/2022 15:41:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.64 on epoch=159
06/19/2022 15:41:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.65 on epoch=164
06/19/2022 15:41:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.57 on epoch=169
06/19/2022 15:41:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.65 on epoch=174
06/19/2022 15:41:45 - INFO - __main__ - Global step 350 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 15:41:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.56 on epoch=179
06/19/2022 15:41:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.64 on epoch=184
06/19/2022 15:41:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.71 on epoch=189
06/19/2022 15:41:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.56 on epoch=194
06/19/2022 15:41:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.62 on epoch=199
06/19/2022 15:41:52 - INFO - __main__ - Global step 400 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 15:41:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.55 on epoch=204
06/19/2022 15:41:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.57 on epoch=209
06/19/2022 15:41:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.53 on epoch=214
06/19/2022 15:41:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.55 on epoch=219
06/19/2022 15:41:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.53 on epoch=224
06/19/2022 15:41:59 - INFO - __main__ - Global step 450 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 15:42:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.55 on epoch=229
06/19/2022 15:42:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=234
06/19/2022 15:42:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.54 on epoch=239
06/19/2022 15:42:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.61 on epoch=244
06/19/2022 15:42:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.59 on epoch=249
06/19/2022 15:42:06 - INFO - __main__ - Global step 500 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 15:42:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=254
06/19/2022 15:42:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.52 on epoch=259
06/19/2022 15:42:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.58 on epoch=264
06/19/2022 15:42:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.60 on epoch=269
06/19/2022 15:42:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.59 on epoch=274
06/19/2022 15:42:13 - INFO - __main__ - Global step 550 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 15:42:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.52 on epoch=279
06/19/2022 15:42:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.57 on epoch=284
06/19/2022 15:42:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.47 on epoch=289
06/19/2022 15:42:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=294
06/19/2022 15:42:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.56 on epoch=299
06/19/2022 15:42:21 - INFO - __main__ - Global step 600 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 15:42:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=304
06/19/2022 15:42:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=309
06/19/2022 15:42:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.60 on epoch=314
06/19/2022 15:42:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.57 on epoch=319
06/19/2022 15:42:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.56 on epoch=324
06/19/2022 15:42:28 - INFO - __main__ - Global step 650 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 15:42:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=329
06/19/2022 15:42:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.50 on epoch=334
06/19/2022 15:42:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=339
06/19/2022 15:42:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.56 on epoch=344
06/19/2022 15:42:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.54 on epoch=349
06/19/2022 15:42:34 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 15:42:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.51 on epoch=354
06/19/2022 15:42:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.60 on epoch=359
06/19/2022 15:42:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.53 on epoch=364
06/19/2022 15:42:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=369
06/19/2022 15:42:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=374
06/19/2022 15:42:42 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 15:42:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.57 on epoch=379
06/19/2022 15:42:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.47 on epoch=384
06/19/2022 15:42:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.52 on epoch=389
06/19/2022 15:42:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.49 on epoch=394
06/19/2022 15:42:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.59 on epoch=399
06/19/2022 15:42:49 - INFO - __main__ - Global step 800 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 15:42:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=404
06/19/2022 15:42:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.53 on epoch=409
06/19/2022 15:42:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=414
06/19/2022 15:42:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=419
06/19/2022 15:42:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.55 on epoch=424
06/19/2022 15:42:56 - INFO - __main__ - Global step 850 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 15:42:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=429
06/19/2022 15:42:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.52 on epoch=434
06/19/2022 15:43:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=439
06/19/2022 15:43:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=444
06/19/2022 15:43:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=449
06/19/2022 15:43:04 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 15:43:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=454
06/19/2022 15:43:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=459
06/19/2022 15:43:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.46 on epoch=464
06/19/2022 15:43:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.45 on epoch=469
06/19/2022 15:43:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=474
06/19/2022 15:43:11 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 15:43:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=479
06/19/2022 15:43:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=484
06/19/2022 15:43:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=489
06/19/2022 15:43:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=494
06/19/2022 15:43:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.52 on epoch=499
06/19/2022 15:43:19 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 15:43:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=504
06/19/2022 15:43:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=509
06/19/2022 15:43:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=514
06/19/2022 15:43:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=519
06/19/2022 15:43:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.46 on epoch=524
06/19/2022 15:43:26 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 15:43:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.53 on epoch=529
06/19/2022 15:43:28 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=534
06/19/2022 15:43:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=539
06/19/2022 15:43:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.52 on epoch=544
06/19/2022 15:43:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=549
06/19/2022 15:43:33 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 15:43:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.48 on epoch=554
06/19/2022 15:43:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=559
06/19/2022 15:43:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=564
06/19/2022 15:43:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.50 on epoch=569
06/19/2022 15:43:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=574
06/19/2022 15:43:40 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 15:43:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=579
06/19/2022 15:43:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=584
06/19/2022 15:43:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=589
06/19/2022 15:43:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=594
06/19/2022 15:43:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=599
06/19/2022 15:43:46 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.3191489361702127 on epoch=599
06/19/2022 15:43:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=604
06/19/2022 15:43:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.33 on epoch=609
06/19/2022 15:43:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.51 on epoch=614
06/19/2022 15:43:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=619
06/19/2022 15:43:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=624
06/19/2022 15:43:53 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 15:43:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=629
06/19/2022 15:43:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.46 on epoch=634
06/19/2022 15:43:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=639
06/19/2022 15:43:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=644
06/19/2022 15:44:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=649
06/19/2022 15:44:00 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 15:44:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=654
06/19/2022 15:44:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.47 on epoch=659
06/19/2022 15:44:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.44 on epoch=664
06/19/2022 15:44:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.44 on epoch=669
06/19/2022 15:44:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=674
06/19/2022 15:44:07 - INFO - __main__ - Global step 1350 Train loss 0.45 Classification-F1 0.4385964912280702 on epoch=674
06/19/2022 15:44:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4385964912280702 on epoch=674, global_step=1350
06/19/2022 15:44:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.49 on epoch=679
06/19/2022 15:44:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.47 on epoch=684
06/19/2022 15:44:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=689
06/19/2022 15:44:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=694
06/19/2022 15:44:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=699
06/19/2022 15:44:15 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 15:44:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=704
06/19/2022 15:44:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.46 on epoch=709
06/19/2022 15:44:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
06/19/2022 15:44:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=719
06/19/2022 15:44:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=724
06/19/2022 15:44:22 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 15:44:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=729
06/19/2022 15:44:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=734
06/19/2022 15:44:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=739
06/19/2022 15:44:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=744
06/19/2022 15:44:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.37 on epoch=749
06/19/2022 15:44:30 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 15:44:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=754
06/19/2022 15:44:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.52 on epoch=759
06/19/2022 15:44:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.42 on epoch=764
06/19/2022 15:44:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=769
06/19/2022 15:44:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=774
06/19/2022 15:44:37 - INFO - __main__ - Global step 1550 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 15:44:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=779
06/19/2022 15:44:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=784
06/19/2022 15:44:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=789
06/19/2022 15:44:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=794
06/19/2022 15:44:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.47 on epoch=799
06/19/2022 15:44:44 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 15:44:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=804
06/19/2022 15:44:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=809
06/19/2022 15:44:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=814
06/19/2022 15:44:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=819
06/19/2022 15:44:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=824
06/19/2022 15:44:52 - INFO - __main__ - Global step 1650 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 15:44:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=829
06/19/2022 15:44:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=834
06/19/2022 15:44:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.39 on epoch=839
06/19/2022 15:44:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=844
06/19/2022 15:44:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.46 on epoch=849
06/19/2022 15:44:59 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 15:45:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=854
06/19/2022 15:45:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=859
06/19/2022 15:45:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=864
06/19/2022 15:45:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
06/19/2022 15:45:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=874
06/19/2022 15:45:06 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 15:45:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.43 on epoch=879
06/19/2022 15:45:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
06/19/2022 15:45:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=889
06/19/2022 15:45:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
06/19/2022 15:45:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
06/19/2022 15:45:12 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 15:45:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=904
06/19/2022 15:45:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=909
06/19/2022 15:45:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.47 on epoch=914
06/19/2022 15:45:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=919
06/19/2022 15:45:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=924
06/19/2022 15:45:19 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 15:45:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=929
06/19/2022 15:45:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=934
06/19/2022 15:45:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=939
06/19/2022 15:45:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
06/19/2022 15:45:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=949
06/19/2022 15:45:26 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 15:45:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=954
06/19/2022 15:45:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=959
06/19/2022 15:45:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.40 on epoch=964
06/19/2022 15:45:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=969
06/19/2022 15:45:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=974
06/19/2022 15:45:33 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 15:45:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
06/19/2022 15:45:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=984
06/19/2022 15:45:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=989
06/19/2022 15:45:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.47 on epoch=994
06/19/2022 15:45:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=999
06/19/2022 15:45:40 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 15:45:40 - INFO - __main__ - save last model!
06/19/2022 15:45:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 15:45:40 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 15:45:40 - INFO - __main__ - Printing 3 examples
06/19/2022 15:45:40 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 15:45:40 - INFO - __main__ - ['0']
06/19/2022 15:45:40 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 15:45:40 - INFO - __main__ - ['1']
06/19/2022 15:45:40 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 15:45:40 - INFO - __main__ - ['1']
06/19/2022 15:45:40 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 15:45:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:45:41 - INFO - __main__ - Printing 3 examples
06/19/2022 15:45:41 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 15:45:41 - INFO - __main__ - ['1']
06/19/2022 15:45:41 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 15:45:41 - INFO - __main__ - ['1']
06/19/2022 15:45:41 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 15:45:41 - INFO - __main__ - ['1']
06/19/2022 15:45:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 15:45:41 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:45:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 15:45:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:45:41 - INFO - __main__ - Printing 3 examples
06/19/2022 15:45:41 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 15:45:41 - INFO - __main__ - ['1']
06/19/2022 15:45:41 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 15:45:41 - INFO - __main__ - ['1']
06/19/2022 15:45:41 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 15:45:41 - INFO - __main__ - ['1']
06/19/2022 15:45:41 - INFO - __main__ - Tokenizing Input ...
06/19/2022 15:45:41 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:45:41 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 15:45:44 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:45:47 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 15:45:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 15:45:47 - INFO - __main__ - Starting training!
06/19/2022 15:45:55 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 15:47:29 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_100_0.5_8_predictions.txt
06/19/2022 15:47:29 - INFO - __main__ - Classification-F1 on test data: 0.3072
06/19/2022 15:47:29 - INFO - __main__ - prefix=paws_16_100, lr=0.5, bsz=8, dev_performance=0.4385964912280702, test_performance=0.30720031767172695
06/19/2022 15:47:29 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.4, bsz=8 ...
06/19/2022 15:47:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:47:30 - INFO - __main__ - Printing 3 examples
06/19/2022 15:47:30 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 15:47:30 - INFO - __main__ - ['1']
06/19/2022 15:47:30 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 15:47:30 - INFO - __main__ - ['1']
06/19/2022 15:47:30 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 15:47:30 - INFO - __main__ - ['1']
06/19/2022 15:47:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 15:47:30 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:47:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 15:47:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:47:30 - INFO - __main__ - Printing 3 examples
06/19/2022 15:47:30 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 15:47:30 - INFO - __main__ - ['1']
06/19/2022 15:47:30 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 15:47:30 - INFO - __main__ - ['1']
06/19/2022 15:47:30 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 15:47:30 - INFO - __main__ - ['1']
06/19/2022 15:47:30 - INFO - __main__ - Tokenizing Input ...
06/19/2022 15:47:30 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:47:30 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 15:47:36 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 15:47:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 15:47:36 - INFO - __main__ - Starting training!
06/19/2022 15:47:37 - INFO - __main__ - Step 10 Global step 10 Train loss 4.11 on epoch=4
06/19/2022 15:47:39 - INFO - __main__ - Step 20 Global step 20 Train loss 3.69 on epoch=9
06/19/2022 15:47:40 - INFO - __main__ - Step 30 Global step 30 Train loss 3.00 on epoch=14
06/19/2022 15:47:41 - INFO - __main__ - Step 40 Global step 40 Train loss 2.69 on epoch=19
06/19/2022 15:47:43 - INFO - __main__ - Step 50 Global step 50 Train loss 2.22 on epoch=24
06/19/2022 15:47:43 - INFO - __main__ - Global step 50 Train loss 3.14 Classification-F1 0.3333333333333333 on epoch=24
06/19/2022 15:47:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/19/2022 15:47:45 - INFO - __main__ - Step 60 Global step 60 Train loss 1.89 on epoch=29
06/19/2022 15:47:46 - INFO - __main__ - Step 70 Global step 70 Train loss 1.67 on epoch=34
06/19/2022 15:47:48 - INFO - __main__ - Step 80 Global step 80 Train loss 1.61 on epoch=39
06/19/2022 15:47:49 - INFO - __main__ - Step 90 Global step 90 Train loss 1.42 on epoch=44
06/19/2022 15:47:51 - INFO - __main__ - Step 100 Global step 100 Train loss 1.31 on epoch=49
06/19/2022 15:47:51 - INFO - __main__ - Global step 100 Train loss 1.58 Classification-F1 0.3333333333333333 on epoch=49
06/19/2022 15:47:53 - INFO - __main__ - Step 110 Global step 110 Train loss 1.29 on epoch=54
06/19/2022 15:47:54 - INFO - __main__ - Step 120 Global step 120 Train loss 1.27 on epoch=59
06/19/2022 15:47:56 - INFO - __main__ - Step 130 Global step 130 Train loss 1.19 on epoch=64
06/19/2022 15:47:57 - INFO - __main__ - Step 140 Global step 140 Train loss 1.14 on epoch=69
06/19/2022 15:47:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.98 on epoch=74
06/19/2022 15:47:59 - INFO - __main__ - Global step 150 Train loss 1.17 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 15:48:01 - INFO - __main__ - Step 160 Global step 160 Train loss 1.04 on epoch=79
06/19/2022 15:48:02 - INFO - __main__ - Step 170 Global step 170 Train loss 1.07 on epoch=84
06/19/2022 15:48:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.97 on epoch=89
06/19/2022 15:48:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.86 on epoch=94
06/19/2022 15:48:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.85 on epoch=99
06/19/2022 15:48:07 - INFO - __main__ - Global step 200 Train loss 0.96 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 15:48:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.89 on epoch=104
06/19/2022 15:48:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.77 on epoch=109
06/19/2022 15:48:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.76 on epoch=114
06/19/2022 15:48:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.83 on epoch=119
06/19/2022 15:48:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.82 on epoch=124
06/19/2022 15:48:16 - INFO - __main__ - Global step 250 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 15:48:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.75 on epoch=129
06/19/2022 15:48:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.82 on epoch=134
06/19/2022 15:48:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.71 on epoch=139
06/19/2022 15:48:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.71 on epoch=144
06/19/2022 15:48:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.76 on epoch=149
06/19/2022 15:48:25 - INFO - __main__ - Global step 300 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 15:48:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=154
06/19/2022 15:48:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.63 on epoch=159
06/19/2022 15:48:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.66 on epoch=164
06/19/2022 15:48:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.59 on epoch=169
06/19/2022 15:48:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.65 on epoch=174
06/19/2022 15:48:33 - INFO - __main__ - Global step 350 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 15:48:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.59 on epoch=179
06/19/2022 15:48:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.60 on epoch=184
06/19/2022 15:48:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.65 on epoch=189
06/19/2022 15:48:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.58 on epoch=194
06/19/2022 15:48:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.63 on epoch=199
06/19/2022 15:48:40 - INFO - __main__ - Global step 400 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 15:48:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.70 on epoch=204
06/19/2022 15:48:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.63 on epoch=209
06/19/2022 15:48:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.56 on epoch=214
06/19/2022 15:48:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.64 on epoch=219
06/19/2022 15:48:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.52 on epoch=224
06/19/2022 15:48:48 - INFO - __main__ - Global step 450 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 15:48:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.59 on epoch=229
06/19/2022 15:48:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.57 on epoch=234
06/19/2022 15:48:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=239
06/19/2022 15:48:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=244
06/19/2022 15:48:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=249
06/19/2022 15:48:56 - INFO - __main__ - Global step 500 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 15:48:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.63 on epoch=254
06/19/2022 15:48:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.60 on epoch=259
06/19/2022 15:49:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.62 on epoch=264
06/19/2022 15:49:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.61 on epoch=269
06/19/2022 15:49:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.66 on epoch=274
06/19/2022 15:49:03 - INFO - __main__ - Global step 550 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 15:49:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.56 on epoch=279
06/19/2022 15:49:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.67 on epoch=284
06/19/2022 15:49:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.62 on epoch=289
06/19/2022 15:49:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.53 on epoch=294
06/19/2022 15:49:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.57 on epoch=299
06/19/2022 15:49:11 - INFO - __main__ - Global step 600 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 15:49:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.58 on epoch=304
06/19/2022 15:49:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.50 on epoch=309
06/19/2022 15:49:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=314
06/19/2022 15:49:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=319
06/19/2022 15:49:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=324
06/19/2022 15:49:18 - INFO - __main__ - Global step 650 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 15:49:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=329
06/19/2022 15:49:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.50 on epoch=334
06/19/2022 15:49:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.61 on epoch=339
06/19/2022 15:49:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=344
06/19/2022 15:49:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=349
06/19/2022 15:49:26 - INFO - __main__ - Global step 700 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 15:49:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=354
06/19/2022 15:49:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.54 on epoch=359
06/19/2022 15:49:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=364
06/19/2022 15:49:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.61 on epoch=369
06/19/2022 15:49:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=374
06/19/2022 15:49:33 - INFO - __main__ - Global step 750 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 15:49:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=379
06/19/2022 15:49:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.53 on epoch=384
06/19/2022 15:49:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.56 on epoch=389
06/19/2022 15:49:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=394
06/19/2022 15:49:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.56 on epoch=399
06/19/2022 15:49:40 - INFO - __main__ - Global step 800 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 15:49:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=404
06/19/2022 15:49:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.50 on epoch=409
06/19/2022 15:49:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=414
06/19/2022 15:49:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=419
06/19/2022 15:49:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=424
06/19/2022 15:49:48 - INFO - __main__ - Global step 850 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 15:49:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.46 on epoch=429
06/19/2022 15:49:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=434
06/19/2022 15:49:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=439
06/19/2022 15:49:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=444
06/19/2022 15:49:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.51 on epoch=449
06/19/2022 15:49:55 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 15:49:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.55 on epoch=454
06/19/2022 15:49:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=459
06/19/2022 15:50:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=464
06/19/2022 15:50:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=469
06/19/2022 15:50:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.49 on epoch=474
06/19/2022 15:50:03 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 15:50:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=479
06/19/2022 15:50:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=484
06/19/2022 15:50:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.47 on epoch=489
06/19/2022 15:50:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=494
06/19/2022 15:50:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=499
06/19/2022 15:50:10 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 15:50:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.47 on epoch=504
06/19/2022 15:50:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=509
06/19/2022 15:50:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=514
06/19/2022 15:50:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=519
06/19/2022 15:50:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=524
06/19/2022 15:50:17 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 15:50:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=529
06/19/2022 15:50:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=534
06/19/2022 15:50:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=539
06/19/2022 15:50:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=544
06/19/2022 15:50:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=549
06/19/2022 15:50:24 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 15:50:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=554
06/19/2022 15:50:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=559
06/19/2022 15:50:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.51 on epoch=564
06/19/2022 15:50:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.50 on epoch=569
06/19/2022 15:50:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.52 on epoch=574
06/19/2022 15:50:31 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 15:50:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=579
06/19/2022 15:50:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=584
06/19/2022 15:50:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=589
06/19/2022 15:50:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=594
06/19/2022 15:50:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=599
06/19/2022 15:50:38 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 15:50:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=604
06/19/2022 15:50:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=609
06/19/2022 15:50:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=614
06/19/2022 15:50:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=619
06/19/2022 15:50:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.54 on epoch=624
06/19/2022 15:50:47 - INFO - __main__ - Global step 1250 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 15:50:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=629
06/19/2022 15:50:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=634
06/19/2022 15:50:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.45 on epoch=639
06/19/2022 15:50:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.49 on epoch=644
06/19/2022 15:50:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.50 on epoch=649
06/19/2022 15:50:55 - INFO - __main__ - Global step 1300 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 15:50:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=654
06/19/2022 15:50:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=659
06/19/2022 15:51:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=664
06/19/2022 15:51:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.48 on epoch=669
06/19/2022 15:51:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=674
06/19/2022 15:51:03 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 15:51:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=679
06/19/2022 15:51:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.50 on epoch=684
06/19/2022 15:51:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=689
06/19/2022 15:51:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=694
06/19/2022 15:51:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.36 on epoch=699
06/19/2022 15:51:10 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 15:51:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=704
06/19/2022 15:51:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=709
06/19/2022 15:51:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=714
06/19/2022 15:51:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=719
06/19/2022 15:51:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=724
06/19/2022 15:51:17 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.4181818181818182 on epoch=724
06/19/2022 15:51:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4181818181818182 on epoch=724, global_step=1450
06/19/2022 15:51:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=729
06/19/2022 15:51:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=734
06/19/2022 15:51:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.46 on epoch=739
06/19/2022 15:51:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=744
06/19/2022 15:51:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=749
06/19/2022 15:51:25 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 15:51:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=754
06/19/2022 15:51:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=759
06/19/2022 15:51:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.48 on epoch=764
06/19/2022 15:51:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=769
06/19/2022 15:51:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=774
06/19/2022 15:51:32 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 15:51:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=779
06/19/2022 15:51:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.42 on epoch=784
06/19/2022 15:51:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=789
06/19/2022 15:51:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=794
06/19/2022 15:51:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
06/19/2022 15:51:39 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 15:51:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=804
06/19/2022 15:51:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=809
06/19/2022 15:51:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=814
06/19/2022 15:51:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
06/19/2022 15:51:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.40 on epoch=824
06/19/2022 15:51:46 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 15:51:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=829
06/19/2022 15:51:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=834
06/19/2022 15:51:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=839
06/19/2022 15:51:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=844
06/19/2022 15:51:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.41 on epoch=849
06/19/2022 15:51:54 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 15:51:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=854
06/19/2022 15:51:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=859
06/19/2022 15:51:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=864
06/19/2022 15:52:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=869
06/19/2022 15:52:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
06/19/2022 15:52:02 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 15:52:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=879
06/19/2022 15:52:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.45 on epoch=884
06/19/2022 15:52:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=889
06/19/2022 15:52:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=894
06/19/2022 15:52:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=899
06/19/2022 15:52:09 - INFO - __main__ - Global step 1800 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 15:52:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=904
06/19/2022 15:52:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=909
06/19/2022 15:52:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=914
06/19/2022 15:52:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.45 on epoch=919
06/19/2022 15:52:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=924
06/19/2022 15:52:17 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 15:52:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=929
06/19/2022 15:52:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=934
06/19/2022 15:52:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.42 on epoch=939
06/19/2022 15:52:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=944
06/19/2022 15:52:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=949
06/19/2022 15:52:24 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 15:52:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
06/19/2022 15:52:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=959
06/19/2022 15:52:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=964
06/19/2022 15:52:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.38 on epoch=969
06/19/2022 15:52:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
06/19/2022 15:52:31 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 15:52:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
06/19/2022 15:52:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.40 on epoch=984
06/19/2022 15:52:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=989
06/19/2022 15:52:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=994
06/19/2022 15:52:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=999
06/19/2022 15:52:39 - INFO - __main__ - Global step 2000 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 15:52:39 - INFO - __main__ - save last model!
06/19/2022 15:52:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 15:52:39 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 15:52:39 - INFO - __main__ - Printing 3 examples
06/19/2022 15:52:39 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 15:52:39 - INFO - __main__ - ['0']
06/19/2022 15:52:39 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 15:52:39 - INFO - __main__ - ['1']
06/19/2022 15:52:39 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 15:52:39 - INFO - __main__ - ['1']
06/19/2022 15:52:39 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 15:52:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:52:39 - INFO - __main__ - Printing 3 examples
06/19/2022 15:52:39 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 15:52:39 - INFO - __main__ - ['1']
06/19/2022 15:52:39 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 15:52:39 - INFO - __main__ - ['1']
06/19/2022 15:52:39 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 15:52:39 - INFO - __main__ - ['1']
06/19/2022 15:52:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 15:52:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:52:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 15:52:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:52:40 - INFO - __main__ - Printing 3 examples
06/19/2022 15:52:40 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 15:52:40 - INFO - __main__ - ['1']
06/19/2022 15:52:40 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 15:52:40 - INFO - __main__ - ['1']
06/19/2022 15:52:40 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 15:52:40 - INFO - __main__ - ['1']
06/19/2022 15:52:40 - INFO - __main__ - Tokenizing Input ...
06/19/2022 15:52:40 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:52:40 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 15:52:44 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:52:46 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 15:52:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 15:52:46 - INFO - __main__ - Starting training!
06/19/2022 15:52:54 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 15:54:19 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_100_0.4_8_predictions.txt
06/19/2022 15:54:19 - INFO - __main__ - Classification-F1 on test data: 0.3282
06/19/2022 15:54:19 - INFO - __main__ - prefix=paws_16_100, lr=0.4, bsz=8, dev_performance=0.4181818181818182, test_performance=0.32822601543635
06/19/2022 15:54:19 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.3, bsz=8 ...
06/19/2022 15:54:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:54:20 - INFO - __main__ - Printing 3 examples
06/19/2022 15:54:20 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 15:54:20 - INFO - __main__ - ['1']
06/19/2022 15:54:20 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 15:54:20 - INFO - __main__ - ['1']
06/19/2022 15:54:20 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 15:54:20 - INFO - __main__ - ['1']
06/19/2022 15:54:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 15:54:20 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:54:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 15:54:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:54:20 - INFO - __main__ - Printing 3 examples
06/19/2022 15:54:20 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 15:54:20 - INFO - __main__ - ['1']
06/19/2022 15:54:20 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 15:54:20 - INFO - __main__ - ['1']
06/19/2022 15:54:20 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 15:54:20 - INFO - __main__ - ['1']
06/19/2022 15:54:20 - INFO - __main__ - Tokenizing Input ...
06/19/2022 15:54:20 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:54:20 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 15:54:25 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 15:54:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 15:54:25 - INFO - __main__ - Starting training!
06/19/2022 15:54:27 - INFO - __main__ - Step 10 Global step 10 Train loss 4.15 on epoch=4
06/19/2022 15:54:29 - INFO - __main__ - Step 20 Global step 20 Train loss 3.67 on epoch=9
06/19/2022 15:54:30 - INFO - __main__ - Step 30 Global step 30 Train loss 3.31 on epoch=14
06/19/2022 15:54:31 - INFO - __main__ - Step 40 Global step 40 Train loss 2.92 on epoch=19
06/19/2022 15:54:33 - INFO - __main__ - Step 50 Global step 50 Train loss 2.53 on epoch=24
06/19/2022 15:54:33 - INFO - __main__ - Global step 50 Train loss 3.31 Classification-F1 0.19696969696969693 on epoch=24
06/19/2022 15:54:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.19696969696969693 on epoch=24, global_step=50
06/19/2022 15:54:34 - INFO - __main__ - Step 60 Global step 60 Train loss 2.12 on epoch=29
06/19/2022 15:54:36 - INFO - __main__ - Step 70 Global step 70 Train loss 1.85 on epoch=34
06/19/2022 15:54:37 - INFO - __main__ - Step 80 Global step 80 Train loss 1.66 on epoch=39
06/19/2022 15:54:38 - INFO - __main__ - Step 90 Global step 90 Train loss 1.61 on epoch=44
06/19/2022 15:54:40 - INFO - __main__ - Step 100 Global step 100 Train loss 1.48 on epoch=49
06/19/2022 15:54:40 - INFO - __main__ - Global step 100 Train loss 1.74 Classification-F1 0.4458874458874459 on epoch=49
06/19/2022 15:54:40 - INFO - __main__ - Saving model with best Classification-F1: 0.19696969696969693 -> 0.4458874458874459 on epoch=49, global_step=100
06/19/2022 15:54:42 - INFO - __main__ - Step 110 Global step 110 Train loss 1.52 on epoch=54
06/19/2022 15:54:43 - INFO - __main__ - Step 120 Global step 120 Train loss 1.40 on epoch=59
06/19/2022 15:54:44 - INFO - __main__ - Step 130 Global step 130 Train loss 1.25 on epoch=64
06/19/2022 15:54:46 - INFO - __main__ - Step 140 Global step 140 Train loss 1.16 on epoch=69
06/19/2022 15:54:47 - INFO - __main__ - Step 150 Global step 150 Train loss 1.25 on epoch=74
06/19/2022 15:54:48 - INFO - __main__ - Global step 150 Train loss 1.31 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 15:54:49 - INFO - __main__ - Step 160 Global step 160 Train loss 1.05 on epoch=79
06/19/2022 15:54:50 - INFO - __main__ - Step 170 Global step 170 Train loss 1.10 on epoch=84
06/19/2022 15:54:52 - INFO - __main__ - Step 180 Global step 180 Train loss 1.12 on epoch=89
06/19/2022 15:54:53 - INFO - __main__ - Step 190 Global step 190 Train loss 1.05 on epoch=94
06/19/2022 15:54:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.96 on epoch=99
06/19/2022 15:54:55 - INFO - __main__ - Global step 200 Train loss 1.06 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 15:54:57 - INFO - __main__ - Step 210 Global step 210 Train loss 1.04 on epoch=104
06/19/2022 15:54:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.95 on epoch=109
06/19/2022 15:55:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.98 on epoch=114
06/19/2022 15:55:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.95 on epoch=119
06/19/2022 15:55:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.87 on epoch=124
06/19/2022 15:55:03 - INFO - __main__ - Global step 250 Train loss 0.96 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 15:55:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=129
06/19/2022 15:55:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.88 on epoch=134
06/19/2022 15:55:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.78 on epoch=139
06/19/2022 15:55:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.83 on epoch=144
06/19/2022 15:55:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=149
06/19/2022 15:55:10 - INFO - __main__ - Global step 300 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 15:55:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.72 on epoch=154
06/19/2022 15:55:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.80 on epoch=159
06/19/2022 15:55:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.67 on epoch=164
06/19/2022 15:55:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.68 on epoch=169
06/19/2022 15:55:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.72 on epoch=174
06/19/2022 15:55:17 - INFO - __main__ - Global step 350 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 15:55:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.71 on epoch=179
06/19/2022 15:55:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.71 on epoch=184
06/19/2022 15:55:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.66 on epoch=189
06/19/2022 15:55:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.60 on epoch=194
06/19/2022 15:55:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.60 on epoch=199
06/19/2022 15:55:25 - INFO - __main__ - Global step 400 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 15:55:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.66 on epoch=204
06/19/2022 15:55:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.73 on epoch=209
06/19/2022 15:55:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.70 on epoch=214
06/19/2022 15:55:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.62 on epoch=219
06/19/2022 15:55:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.59 on epoch=224
06/19/2022 15:55:33 - INFO - __main__ - Global step 450 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 15:55:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.57 on epoch=229
06/19/2022 15:55:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.67 on epoch=234
06/19/2022 15:55:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.61 on epoch=239
06/19/2022 15:55:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.68 on epoch=244
06/19/2022 15:55:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.56 on epoch=249
06/19/2022 15:55:40 - INFO - __main__ - Global step 500 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 15:55:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.63 on epoch=254
06/19/2022 15:55:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.57 on epoch=259
06/19/2022 15:55:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.55 on epoch=264
06/19/2022 15:55:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.63 on epoch=269
06/19/2022 15:55:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=274
06/19/2022 15:55:48 - INFO - __main__ - Global step 550 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 15:55:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.58 on epoch=279
06/19/2022 15:55:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.60 on epoch=284
06/19/2022 15:55:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.60 on epoch=289
06/19/2022 15:55:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.64 on epoch=294
06/19/2022 15:55:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.56 on epoch=299
06/19/2022 15:55:56 - INFO - __main__ - Global step 600 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 15:55:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.59 on epoch=304
06/19/2022 15:55:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.54 on epoch=309
06/19/2022 15:56:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.59 on epoch=314
06/19/2022 15:56:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=319
06/19/2022 15:56:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.54 on epoch=324
06/19/2022 15:56:03 - INFO - __main__ - Global step 650 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 15:56:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.58 on epoch=329
06/19/2022 15:56:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.54 on epoch=334
06/19/2022 15:56:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.56 on epoch=339
06/19/2022 15:56:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.53 on epoch=344
06/19/2022 15:56:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=349
06/19/2022 15:56:10 - INFO - __main__ - Global step 700 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 15:56:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.49 on epoch=354
06/19/2022 15:56:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.53 on epoch=359
06/19/2022 15:56:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=364
06/19/2022 15:56:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.54 on epoch=369
06/19/2022 15:56:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=374
06/19/2022 15:56:18 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 15:56:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.50 on epoch=379
06/19/2022 15:56:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.51 on epoch=384
06/19/2022 15:56:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=389
06/19/2022 15:56:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.51 on epoch=394
06/19/2022 15:56:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=399
06/19/2022 15:56:26 - INFO - __main__ - Global step 800 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 15:56:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.56 on epoch=404
06/19/2022 15:56:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.58 on epoch=409
06/19/2022 15:56:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.53 on epoch=414
06/19/2022 15:56:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.59 on epoch=419
06/19/2022 15:56:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=424
06/19/2022 15:56:33 - INFO - __main__ - Global step 850 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 15:56:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.55 on epoch=429
06/19/2022 15:56:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.53 on epoch=434
06/19/2022 15:56:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=439
06/19/2022 15:56:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.51 on epoch=444
06/19/2022 15:56:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=449
06/19/2022 15:56:40 - INFO - __main__ - Global step 900 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 15:56:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.48 on epoch=454
06/19/2022 15:56:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=459
06/19/2022 15:56:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=464
06/19/2022 15:56:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=469
06/19/2022 15:56:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.51 on epoch=474
06/19/2022 15:56:48 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 15:56:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.50 on epoch=479
06/19/2022 15:56:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.57 on epoch=484
06/19/2022 15:56:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.51 on epoch=489
06/19/2022 15:56:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.48 on epoch=494
06/19/2022 15:56:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.50 on epoch=499
06/19/2022 15:56:54 - INFO - __main__ - Global step 1000 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 15:56:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.53 on epoch=504
06/19/2022 15:56:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=509
06/19/2022 15:56:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.54 on epoch=514
06/19/2022 15:56:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.47 on epoch=519
06/19/2022 15:57:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=524
06/19/2022 15:57:01 - INFO - __main__ - Global step 1050 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 15:57:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=529
06/19/2022 15:57:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=534
06/19/2022 15:57:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.49 on epoch=539
06/19/2022 15:57:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=544
06/19/2022 15:57:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.50 on epoch=549
06/19/2022 15:57:08 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 15:57:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.51 on epoch=554
06/19/2022 15:57:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=559
06/19/2022 15:57:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=564
06/19/2022 15:57:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.51 on epoch=569
06/19/2022 15:57:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=574
06/19/2022 15:57:15 - INFO - __main__ - Global step 1150 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 15:57:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=579
06/19/2022 15:57:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=584
06/19/2022 15:57:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.48 on epoch=589
06/19/2022 15:57:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=594
06/19/2022 15:57:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.46 on epoch=599
06/19/2022 15:57:22 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 15:57:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=604
06/19/2022 15:57:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=609
06/19/2022 15:57:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=614
06/19/2022 15:57:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=619
06/19/2022 15:57:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=624
06/19/2022 15:57:29 - INFO - __main__ - Global step 1250 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 15:57:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=629
06/19/2022 15:57:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=634
06/19/2022 15:57:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=639
06/19/2022 15:57:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=644
06/19/2022 15:57:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=649
06/19/2022 15:57:36 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 15:57:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=654
06/19/2022 15:57:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=659
06/19/2022 15:57:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=664
06/19/2022 15:57:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.44 on epoch=669
06/19/2022 15:57:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=674
06/19/2022 15:57:42 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 15:57:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.45 on epoch=679
06/19/2022 15:57:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.46 on epoch=684
06/19/2022 15:57:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=689
06/19/2022 15:57:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.45 on epoch=694
06/19/2022 15:57:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.45 on epoch=699
06/19/2022 15:57:50 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 15:57:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.46 on epoch=704
06/19/2022 15:57:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=709
06/19/2022 15:57:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=714
06/19/2022 15:57:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.49 on epoch=719
06/19/2022 15:57:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.50 on epoch=724
06/19/2022 15:57:57 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 15:57:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=729
06/19/2022 15:57:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=734
06/19/2022 15:58:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.55 on epoch=739
06/19/2022 15:58:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=744
06/19/2022 15:58:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.50 on epoch=749
06/19/2022 15:58:04 - INFO - __main__ - Global step 1500 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 15:58:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
06/19/2022 15:58:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.51 on epoch=759
06/19/2022 15:58:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.57 on epoch=764
06/19/2022 15:58:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.51 on epoch=769
06/19/2022 15:58:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.51 on epoch=774
06/19/2022 15:58:12 - INFO - __main__ - Global step 1550 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 15:58:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=779
06/19/2022 15:58:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.49 on epoch=784
06/19/2022 15:58:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.46 on epoch=789
06/19/2022 15:58:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=794
06/19/2022 15:58:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
06/19/2022 15:58:19 - INFO - __main__ - Global step 1600 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 15:58:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=804
06/19/2022 15:58:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.49 on epoch=809
06/19/2022 15:58:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=814
06/19/2022 15:58:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
06/19/2022 15:58:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=824
06/19/2022 15:58:26 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 15:58:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=829
06/19/2022 15:58:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=834
06/19/2022 15:58:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=839
06/19/2022 15:58:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=844
06/19/2022 15:58:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=849
06/19/2022 15:58:34 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 15:58:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=854
06/19/2022 15:58:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=859
06/19/2022 15:58:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=864
06/19/2022 15:58:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=869
06/19/2022 15:58:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=874
06/19/2022 15:58:42 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.3992490613266583 on epoch=874
06/19/2022 15:58:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=879
06/19/2022 15:58:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=884
06/19/2022 15:58:46 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.43 on epoch=889
06/19/2022 15:58:47 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
06/19/2022 15:58:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=899
06/19/2022 15:58:49 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 15:58:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=904
06/19/2022 15:58:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=909
06/19/2022 15:58:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=914
06/19/2022 15:58:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=919
06/19/2022 15:58:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=924
06/19/2022 15:58:56 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 15:58:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.46 on epoch=929
06/19/2022 15:58:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
06/19/2022 15:59:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.51 on epoch=939
06/19/2022 15:59:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.45 on epoch=944
06/19/2022 15:59:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=949
06/19/2022 15:59:04 - INFO - __main__ - Global step 1900 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 15:59:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=954
06/19/2022 15:59:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=959
06/19/2022 15:59:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.43 on epoch=964
06/19/2022 15:59:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=969
06/19/2022 15:59:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=974
06/19/2022 15:59:11 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 15:59:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
06/19/2022 15:59:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.44 on epoch=984
06/19/2022 15:59:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.40 on epoch=989
06/19/2022 15:59:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=994
06/19/2022 15:59:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=999
06/19/2022 15:59:19 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 15:59:19 - INFO - __main__ - save last model!
06/19/2022 15:59:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 15:59:19 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 15:59:19 - INFO - __main__ - Printing 3 examples
06/19/2022 15:59:19 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 15:59:19 - INFO - __main__ - ['0']
06/19/2022 15:59:19 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 15:59:19 - INFO - __main__ - ['1']
06/19/2022 15:59:19 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 15:59:19 - INFO - __main__ - ['1']
06/19/2022 15:59:19 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 15:59:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:59:20 - INFO - __main__ - Printing 3 examples
06/19/2022 15:59:20 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 15:59:20 - INFO - __main__ - ['1']
06/19/2022 15:59:20 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 15:59:20 - INFO - __main__ - ['1']
06/19/2022 15:59:20 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 15:59:20 - INFO - __main__ - ['1']
06/19/2022 15:59:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 15:59:20 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:59:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 15:59:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 15:59:20 - INFO - __main__ - Printing 3 examples
06/19/2022 15:59:20 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 15:59:20 - INFO - __main__ - ['1']
06/19/2022 15:59:20 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 15:59:20 - INFO - __main__ - ['1']
06/19/2022 15:59:20 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 15:59:20 - INFO - __main__ - ['1']
06/19/2022 15:59:20 - INFO - __main__ - Tokenizing Input ...
06/19/2022 15:59:20 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:59:20 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 15:59:24 - INFO - __main__ - Tokenizing Output ...
06/19/2022 15:59:25 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 15:59:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 15:59:26 - INFO - __main__ - Starting training!
06/19/2022 15:59:32 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 16:00:55 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_100_0.3_8_predictions.txt
06/19/2022 16:00:55 - INFO - __main__ - Classification-F1 on test data: 0.3076
06/19/2022 16:00:55 - INFO - __main__ - prefix=paws_16_100, lr=0.3, bsz=8, dev_performance=0.4458874458874459, test_performance=0.30764093090767447
06/19/2022 16:00:55 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.2, bsz=8 ...
06/19/2022 16:00:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:00:56 - INFO - __main__ - Printing 3 examples
06/19/2022 16:00:56 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 16:00:56 - INFO - __main__ - ['1']
06/19/2022 16:00:56 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 16:00:56 - INFO - __main__ - ['1']
06/19/2022 16:00:56 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 16:00:56 - INFO - __main__ - ['1']
06/19/2022 16:00:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:00:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:00:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:00:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:00:56 - INFO - __main__ - Printing 3 examples
06/19/2022 16:00:56 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 16:00:56 - INFO - __main__ - ['1']
06/19/2022 16:00:56 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 16:00:56 - INFO - __main__ - ['1']
06/19/2022 16:00:56 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 16:00:56 - INFO - __main__ - ['1']
06/19/2022 16:00:56 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:00:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:00:56 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:01:02 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:01:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:01:03 - INFO - __main__ - Starting training!
06/19/2022 16:01:05 - INFO - __main__ - Step 10 Global step 10 Train loss 4.18 on epoch=4
06/19/2022 16:01:06 - INFO - __main__ - Step 20 Global step 20 Train loss 3.81 on epoch=9
06/19/2022 16:01:08 - INFO - __main__ - Step 30 Global step 30 Train loss 3.45 on epoch=14
06/19/2022 16:01:09 - INFO - __main__ - Step 40 Global step 40 Train loss 3.19 on epoch=19
06/19/2022 16:01:11 - INFO - __main__ - Step 50 Global step 50 Train loss 2.92 on epoch=24
06/19/2022 16:01:11 - INFO - __main__ - Global step 50 Train loss 3.51 Classification-F1 0.0 on epoch=24
06/19/2022 16:01:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 16:01:12 - INFO - __main__ - Step 60 Global step 60 Train loss 2.58 on epoch=29
06/19/2022 16:01:14 - INFO - __main__ - Step 70 Global step 70 Train loss 2.32 on epoch=34
06/19/2022 16:01:15 - INFO - __main__ - Step 80 Global step 80 Train loss 2.13 on epoch=39
06/19/2022 16:01:17 - INFO - __main__ - Step 90 Global step 90 Train loss 1.97 on epoch=44
06/19/2022 16:01:18 - INFO - __main__ - Step 100 Global step 100 Train loss 1.97 on epoch=49
06/19/2022 16:01:18 - INFO - __main__ - Global step 100 Train loss 2.19 Classification-F1 0.3333333333333333 on epoch=49
06/19/2022 16:01:19 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=49, global_step=100
06/19/2022 16:01:20 - INFO - __main__ - Step 110 Global step 110 Train loss 1.75 on epoch=54
06/19/2022 16:01:21 - INFO - __main__ - Step 120 Global step 120 Train loss 1.71 on epoch=59
06/19/2022 16:01:22 - INFO - __main__ - Step 130 Global step 130 Train loss 1.54 on epoch=64
06/19/2022 16:01:24 - INFO - __main__ - Step 140 Global step 140 Train loss 1.57 on epoch=69
06/19/2022 16:01:26 - INFO - __main__ - Step 150 Global step 150 Train loss 1.44 on epoch=74
06/19/2022 16:01:26 - INFO - __main__ - Global step 150 Train loss 1.60 Classification-F1 0.5333333333333333 on epoch=74
06/19/2022 16:01:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5333333333333333 on epoch=74, global_step=150
06/19/2022 16:01:27 - INFO - __main__ - Step 160 Global step 160 Train loss 1.43 on epoch=79
06/19/2022 16:01:29 - INFO - __main__ - Step 170 Global step 170 Train loss 1.39 on epoch=84
06/19/2022 16:01:30 - INFO - __main__ - Step 180 Global step 180 Train loss 1.49 on epoch=89
06/19/2022 16:01:32 - INFO - __main__ - Step 190 Global step 190 Train loss 1.28 on epoch=94
06/19/2022 16:01:33 - INFO - __main__ - Step 200 Global step 200 Train loss 1.23 on epoch=99
06/19/2022 16:01:33 - INFO - __main__ - Global step 200 Train loss 1.36 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 16:01:34 - INFO - __main__ - Step 210 Global step 210 Train loss 1.31 on epoch=104
06/19/2022 16:01:36 - INFO - __main__ - Step 220 Global step 220 Train loss 1.19 on epoch=109
06/19/2022 16:01:37 - INFO - __main__ - Step 230 Global step 230 Train loss 1.12 on epoch=114
06/19/2022 16:01:39 - INFO - __main__ - Step 240 Global step 240 Train loss 1.16 on epoch=119
06/19/2022 16:01:40 - INFO - __main__ - Step 250 Global step 250 Train loss 1.12 on epoch=124
06/19/2022 16:01:41 - INFO - __main__ - Global step 250 Train loss 1.18 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 16:01:42 - INFO - __main__ - Step 260 Global step 260 Train loss 1.12 on epoch=129
06/19/2022 16:01:43 - INFO - __main__ - Step 270 Global step 270 Train loss 1.20 on epoch=134
06/19/2022 16:01:45 - INFO - __main__ - Step 280 Global step 280 Train loss 1.06 on epoch=139
06/19/2022 16:01:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.99 on epoch=144
06/19/2022 16:01:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.98 on epoch=149
06/19/2022 16:01:48 - INFO - __main__ - Global step 300 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 16:01:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.97 on epoch=154
06/19/2022 16:01:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.95 on epoch=159
06/19/2022 16:01:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.92 on epoch=164
06/19/2022 16:01:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.97 on epoch=169
06/19/2022 16:01:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.90 on epoch=174
06/19/2022 16:01:56 - INFO - __main__ - Global step 350 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 16:01:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.80 on epoch=179
06/19/2022 16:01:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.92 on epoch=184
06/19/2022 16:02:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.84 on epoch=189
06/19/2022 16:02:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.94 on epoch=194
06/19/2022 16:02:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.94 on epoch=199
06/19/2022 16:02:03 - INFO - __main__ - Global step 400 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 16:02:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.77 on epoch=204
06/19/2022 16:02:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.81 on epoch=209
06/19/2022 16:02:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.83 on epoch=214
06/19/2022 16:02:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.77 on epoch=219
06/19/2022 16:02:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.68 on epoch=224
06/19/2022 16:02:11 - INFO - __main__ - Global step 450 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 16:02:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.75 on epoch=229
06/19/2022 16:02:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.71 on epoch=234
06/19/2022 16:02:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.82 on epoch=239
06/19/2022 16:02:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.71 on epoch=244
06/19/2022 16:02:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.71 on epoch=249
06/19/2022 16:02:19 - INFO - __main__ - Global step 500 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 16:02:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.72 on epoch=254
06/19/2022 16:02:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.73 on epoch=259
06/19/2022 16:02:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.64 on epoch=264
06/19/2022 16:02:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.75 on epoch=269
06/19/2022 16:02:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.76 on epoch=274
06/19/2022 16:02:27 - INFO - __main__ - Global step 550 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 16:02:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.74 on epoch=279
06/19/2022 16:02:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.73 on epoch=284
06/19/2022 16:02:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.60 on epoch=289
06/19/2022 16:02:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.62 on epoch=294
06/19/2022 16:02:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.73 on epoch=299
06/19/2022 16:02:34 - INFO - __main__ - Global step 600 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 16:02:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.66 on epoch=304
06/19/2022 16:02:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.67 on epoch=309
06/19/2022 16:02:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.64 on epoch=314
06/19/2022 16:02:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.62 on epoch=319
06/19/2022 16:02:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.66 on epoch=324
06/19/2022 16:02:43 - INFO - __main__ - Global step 650 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 16:02:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.64 on epoch=329
06/19/2022 16:02:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.66 on epoch=334
06/19/2022 16:02:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.63 on epoch=339
06/19/2022 16:02:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.63 on epoch=344
06/19/2022 16:02:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.58 on epoch=349
06/19/2022 16:02:50 - INFO - __main__ - Global step 700 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 16:02:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.70 on epoch=354
06/19/2022 16:02:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.62 on epoch=359
06/19/2022 16:02:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.65 on epoch=364
06/19/2022 16:02:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.56 on epoch=369
06/19/2022 16:02:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.63 on epoch=374
06/19/2022 16:02:58 - INFO - __main__ - Global step 750 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 16:02:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.60 on epoch=379
06/19/2022 16:03:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.64 on epoch=384
06/19/2022 16:03:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.56 on epoch=389
06/19/2022 16:03:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.70 on epoch=394
06/19/2022 16:03:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.62 on epoch=399
06/19/2022 16:03:05 - INFO - __main__ - Global step 800 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 16:03:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.58 on epoch=404
06/19/2022 16:03:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.53 on epoch=409
06/19/2022 16:03:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.54 on epoch=414
06/19/2022 16:03:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.61 on epoch=419
06/19/2022 16:03:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.56 on epoch=424
06/19/2022 16:03:12 - INFO - __main__ - Global step 850 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 16:03:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.60 on epoch=429
06/19/2022 16:03:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.59 on epoch=434
06/19/2022 16:03:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.53 on epoch=439
06/19/2022 16:03:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.58 on epoch=444
06/19/2022 16:03:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=449
06/19/2022 16:03:19 - INFO - __main__ - Global step 900 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 16:03:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.58 on epoch=454
06/19/2022 16:03:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.56 on epoch=459
06/19/2022 16:03:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=464
06/19/2022 16:03:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.63 on epoch=469
06/19/2022 16:03:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.52 on epoch=474
06/19/2022 16:03:26 - INFO - __main__ - Global step 950 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 16:03:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.54 on epoch=479
06/19/2022 16:03:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.62 on epoch=484
06/19/2022 16:03:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.56 on epoch=489
06/19/2022 16:03:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.54 on epoch=494
06/19/2022 16:03:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.54 on epoch=499
06/19/2022 16:03:34 - INFO - __main__ - Global step 1000 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 16:03:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.51 on epoch=504
06/19/2022 16:03:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.64 on epoch=509
06/19/2022 16:03:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=514
06/19/2022 16:03:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.52 on epoch=519
06/19/2022 16:03:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.61 on epoch=524
06/19/2022 16:03:41 - INFO - __main__ - Global step 1050 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 16:03:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.56 on epoch=529
06/19/2022 16:03:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.51 on epoch=534
06/19/2022 16:03:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.51 on epoch=539
06/19/2022 16:03:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.51 on epoch=544
06/19/2022 16:03:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.55 on epoch=549
06/19/2022 16:03:49 - INFO - __main__ - Global step 1100 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 16:03:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.56 on epoch=554
06/19/2022 16:03:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.56 on epoch=559
06/19/2022 16:03:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.56 on epoch=564
06/19/2022 16:03:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.53 on epoch=569
06/19/2022 16:03:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.59 on epoch=574
06/19/2022 16:03:57 - INFO - __main__ - Global step 1150 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 16:03:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=579
06/19/2022 16:04:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.54 on epoch=584
06/19/2022 16:04:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.53 on epoch=589
06/19/2022 16:04:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.50 on epoch=594
06/19/2022 16:04:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=599
06/19/2022 16:04:05 - INFO - __main__ - Global step 1200 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 16:04:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.53 on epoch=604
06/19/2022 16:04:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.56 on epoch=609
06/19/2022 16:04:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.49 on epoch=614
06/19/2022 16:04:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.48 on epoch=619
06/19/2022 16:04:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.47 on epoch=624
06/19/2022 16:04:13 - INFO - __main__ - Global step 1250 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 16:04:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.58 on epoch=629
06/19/2022 16:04:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=634
06/19/2022 16:04:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.59 on epoch=639
06/19/2022 16:04:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.51 on epoch=644
06/19/2022 16:04:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.57 on epoch=649
06/19/2022 16:04:21 - INFO - __main__ - Global step 1300 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 16:04:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.53 on epoch=654
06/19/2022 16:04:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.46 on epoch=659
06/19/2022 16:04:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=664
06/19/2022 16:04:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=669
06/19/2022 16:04:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=674
06/19/2022 16:04:27 - INFO - __main__ - Global step 1350 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 16:04:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=679
06/19/2022 16:04:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.56 on epoch=684
06/19/2022 16:04:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=689
06/19/2022 16:04:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.48 on epoch=694
06/19/2022 16:04:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=699
06/19/2022 16:04:35 - INFO - __main__ - Global step 1400 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 16:04:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.50 on epoch=704
06/19/2022 16:04:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.59 on epoch=709
06/19/2022 16:04:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.58 on epoch=714
06/19/2022 16:04:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.56 on epoch=719
06/19/2022 16:04:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.56 on epoch=724
06/19/2022 16:04:42 - INFO - __main__ - Global step 1450 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 16:04:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.47 on epoch=729
06/19/2022 16:04:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=734
06/19/2022 16:04:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.55 on epoch=739
06/19/2022 16:04:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.55 on epoch=744
06/19/2022 16:04:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.49 on epoch=749
06/19/2022 16:04:49 - INFO - __main__ - Global step 1500 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 16:04:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.47 on epoch=754
06/19/2022 16:04:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.55 on epoch=759
06/19/2022 16:04:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=764
06/19/2022 16:04:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.50 on epoch=769
06/19/2022 16:04:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.51 on epoch=774
06/19/2022 16:04:57 - INFO - __main__ - Global step 1550 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 16:04:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.49 on epoch=779
06/19/2022 16:04:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.45 on epoch=784
06/19/2022 16:05:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.52 on epoch=789
06/19/2022 16:05:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.51 on epoch=794
06/19/2022 16:05:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.52 on epoch=799
06/19/2022 16:05:04 - INFO - __main__ - Global step 1600 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 16:05:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.49 on epoch=804
06/19/2022 16:05:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.55 on epoch=809
06/19/2022 16:05:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.46 on epoch=814
06/19/2022 16:05:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.48 on epoch=819
06/19/2022 16:05:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.46 on epoch=824
06/19/2022 16:05:11 - INFO - __main__ - Global step 1650 Train loss 0.49 Classification-F1 0.3992490613266583 on epoch=824
06/19/2022 16:05:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.50 on epoch=829
06/19/2022 16:05:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=834
06/19/2022 16:05:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.45 on epoch=839
06/19/2022 16:05:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.49 on epoch=844
06/19/2022 16:05:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=849
06/19/2022 16:05:19 - INFO - __main__ - Global step 1700 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 16:05:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.47 on epoch=854
06/19/2022 16:05:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.45 on epoch=859
06/19/2022 16:05:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.47 on epoch=864
06/19/2022 16:05:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=869
06/19/2022 16:05:26 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=874
06/19/2022 16:05:27 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 16:05:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=879
06/19/2022 16:05:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=884
06/19/2022 16:05:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.51 on epoch=889
06/19/2022 16:05:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.48 on epoch=894
06/19/2022 16:05:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.51 on epoch=899
06/19/2022 16:05:34 - INFO - __main__ - Global step 1800 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 16:05:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.52 on epoch=904
06/19/2022 16:05:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=909
06/19/2022 16:05:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=914
06/19/2022 16:05:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=919
06/19/2022 16:05:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=924
06/19/2022 16:05:41 - INFO - __main__ - Global step 1850 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 16:05:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.55 on epoch=929
06/19/2022 16:05:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=934
06/19/2022 16:05:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=939
06/19/2022 16:05:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.45 on epoch=944
06/19/2022 16:05:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=949
06/19/2022 16:05:50 - INFO - __main__ - Global step 1900 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 16:05:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=954
06/19/2022 16:05:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.46 on epoch=959
06/19/2022 16:05:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.50 on epoch=964
06/19/2022 16:05:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=969
06/19/2022 16:05:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.50 on epoch=974
06/19/2022 16:05:58 - INFO - __main__ - Global step 1950 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 16:05:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.50 on epoch=979
06/19/2022 16:06:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.44 on epoch=984
06/19/2022 16:06:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=989
06/19/2022 16:06:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=994
06/19/2022 16:06:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.52 on epoch=999
06/19/2022 16:06:05 - INFO - __main__ - Global step 2000 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 16:06:05 - INFO - __main__ - save last model!
06/19/2022 16:06:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 16:06:05 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 16:06:05 - INFO - __main__ - Printing 3 examples
06/19/2022 16:06:05 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 16:06:05 - INFO - __main__ - ['0']
06/19/2022 16:06:05 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 16:06:05 - INFO - __main__ - ['1']
06/19/2022 16:06:05 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 16:06:05 - INFO - __main__ - ['1']
06/19/2022 16:06:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:06:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:06:06 - INFO - __main__ - Printing 3 examples
06/19/2022 16:06:06 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 16:06:06 - INFO - __main__ - ['1']
06/19/2022 16:06:06 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 16:06:06 - INFO - __main__ - ['1']
06/19/2022 16:06:06 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 16:06:06 - INFO - __main__ - ['1']
06/19/2022 16:06:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 16:06:06 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:06:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:06:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:06:06 - INFO - __main__ - Printing 3 examples
06/19/2022 16:06:06 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 16:06:06 - INFO - __main__ - ['1']
06/19/2022 16:06:06 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 16:06:06 - INFO - __main__ - ['1']
06/19/2022 16:06:06 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 16:06:06 - INFO - __main__ - ['1']
06/19/2022 16:06:06 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:06:06 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:06:06 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:06:10 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:06:12 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:06:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:06:13 - INFO - __main__ - Starting training!
06/19/2022 16:06:21 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 16:07:49 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_100_0.2_8_predictions.txt
06/19/2022 16:07:49 - INFO - __main__ - Classification-F1 on test data: 0.3072
06/19/2022 16:07:49 - INFO - __main__ - prefix=paws_16_100, lr=0.2, bsz=8, dev_performance=0.5333333333333333, test_performance=0.3072185365356206
06/19/2022 16:07:49 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.5, bsz=8 ...
06/19/2022 16:07:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:07:50 - INFO - __main__ - Printing 3 examples
06/19/2022 16:07:50 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 16:07:50 - INFO - __main__ - ['1']
06/19/2022 16:07:50 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 16:07:50 - INFO - __main__ - ['1']
06/19/2022 16:07:50 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 16:07:50 - INFO - __main__ - ['1']
06/19/2022 16:07:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:07:50 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:07:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:07:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:07:50 - INFO - __main__ - Printing 3 examples
06/19/2022 16:07:50 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 16:07:50 - INFO - __main__ - ['1']
06/19/2022 16:07:50 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 16:07:50 - INFO - __main__ - ['1']
06/19/2022 16:07:50 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 16:07:50 - INFO - __main__ - ['1']
06/19/2022 16:07:50 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:07:50 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:07:50 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:07:57 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:07:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:07:57 - INFO - __main__ - Starting training!
06/19/2022 16:07:58 - INFO - __main__ - Step 10 Global step 10 Train loss 4.12 on epoch=4
06/19/2022 16:08:00 - INFO - __main__ - Step 20 Global step 20 Train loss 3.47 on epoch=9
06/19/2022 16:08:01 - INFO - __main__ - Step 30 Global step 30 Train loss 2.69 on epoch=14
06/19/2022 16:08:02 - INFO - __main__ - Step 40 Global step 40 Train loss 2.32 on epoch=19
06/19/2022 16:08:04 - INFO - __main__ - Step 50 Global step 50 Train loss 1.77 on epoch=24
06/19/2022 16:08:05 - INFO - __main__ - Global step 50 Train loss 2.87 Classification-F1 0.3333333333333333 on epoch=24
06/19/2022 16:08:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/19/2022 16:08:06 - INFO - __main__ - Step 60 Global step 60 Train loss 1.54 on epoch=29
06/19/2022 16:08:07 - INFO - __main__ - Step 70 Global step 70 Train loss 1.43 on epoch=34
06/19/2022 16:08:09 - INFO - __main__ - Step 80 Global step 80 Train loss 1.35 on epoch=39
06/19/2022 16:08:11 - INFO - __main__ - Step 90 Global step 90 Train loss 1.26 on epoch=44
06/19/2022 16:08:12 - INFO - __main__ - Step 100 Global step 100 Train loss 1.16 on epoch=49
06/19/2022 16:08:12 - INFO - __main__ - Global step 100 Train loss 1.35 Classification-F1 0.3333333333333333 on epoch=49
06/19/2022 16:08:14 - INFO - __main__ - Step 110 Global step 110 Train loss 1.13 on epoch=54
06/19/2022 16:08:15 - INFO - __main__ - Step 120 Global step 120 Train loss 1.00 on epoch=59
06/19/2022 16:08:16 - INFO - __main__ - Step 130 Global step 130 Train loss 1.07 on epoch=64
06/19/2022 16:08:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.97 on epoch=69
06/19/2022 16:08:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.91 on epoch=74
06/19/2022 16:08:19 - INFO - __main__ - Global step 150 Train loss 1.02 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 16:08:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.82 on epoch=79
06/19/2022 16:08:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.98 on epoch=84
06/19/2022 16:08:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.85 on epoch=89
06/19/2022 16:08:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=94
06/19/2022 16:08:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.75 on epoch=99
06/19/2022 16:08:26 - INFO - __main__ - Global step 200 Train loss 0.86 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 16:08:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.74 on epoch=104
06/19/2022 16:08:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.72 on epoch=109
06/19/2022 16:08:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.68 on epoch=114
06/19/2022 16:08:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.70 on epoch=119
06/19/2022 16:08:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.67 on epoch=124
06/19/2022 16:08:33 - INFO - __main__ - Global step 250 Train loss 0.70 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 16:08:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.66 on epoch=129
06/19/2022 16:08:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.66 on epoch=134
06/19/2022 16:08:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.67 on epoch=139
06/19/2022 16:08:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.69 on epoch=144
06/19/2022 16:08:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.73 on epoch=149
06/19/2022 16:08:41 - INFO - __main__ - Global step 300 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 16:08:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.66 on epoch=154
06/19/2022 16:08:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.64 on epoch=159
06/19/2022 16:08:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.61 on epoch=164
06/19/2022 16:08:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.54 on epoch=169
06/19/2022 16:08:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=174
06/19/2022 16:08:48 - INFO - __main__ - Global step 350 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 16:08:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.57 on epoch=179
06/19/2022 16:08:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.70 on epoch=184
06/19/2022 16:08:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.59 on epoch=189
06/19/2022 16:08:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.59 on epoch=194
06/19/2022 16:08:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=199
06/19/2022 16:08:55 - INFO - __main__ - Global step 400 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 16:08:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.55 on epoch=204
06/19/2022 16:08:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.60 on epoch=209
06/19/2022 16:08:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.58 on epoch=214
06/19/2022 16:09:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.56 on epoch=219
06/19/2022 16:09:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=224
06/19/2022 16:09:02 - INFO - __main__ - Global step 450 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 16:09:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.64 on epoch=229
06/19/2022 16:09:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=234
06/19/2022 16:09:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.61 on epoch=239
06/19/2022 16:09:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.56 on epoch=244
06/19/2022 16:09:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.54 on epoch=249
06/19/2022 16:09:09 - INFO - __main__ - Global step 500 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 16:09:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.54 on epoch=254
06/19/2022 16:09:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.62 on epoch=259
06/19/2022 16:09:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=264
06/19/2022 16:09:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=269
06/19/2022 16:09:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.58 on epoch=274
06/19/2022 16:09:16 - INFO - __main__ - Global step 550 Train loss 0.56 Classification-F1 0.3191489361702127 on epoch=274
06/19/2022 16:09:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.59 on epoch=279
06/19/2022 16:09:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.59 on epoch=284
06/19/2022 16:09:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=289
06/19/2022 16:09:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.57 on epoch=294
06/19/2022 16:09:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.53 on epoch=299
06/19/2022 16:09:24 - INFO - __main__ - Global step 600 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 16:09:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.60 on epoch=304
06/19/2022 16:09:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=309
06/19/2022 16:09:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.54 on epoch=314
06/19/2022 16:09:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=319
06/19/2022 16:09:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=324
06/19/2022 16:09:32 - INFO - __main__ - Global step 650 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 16:09:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=329
06/19/2022 16:09:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.54 on epoch=334
06/19/2022 16:09:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.55 on epoch=339
06/19/2022 16:09:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.53 on epoch=344
06/19/2022 16:09:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.52 on epoch=349
06/19/2022 16:09:40 - INFO - __main__ - Global step 700 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 16:09:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.51 on epoch=354
06/19/2022 16:09:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=359
06/19/2022 16:09:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.59 on epoch=364
06/19/2022 16:09:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.53 on epoch=369
06/19/2022 16:09:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.57 on epoch=374
06/19/2022 16:09:47 - INFO - __main__ - Global step 750 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 16:09:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.55 on epoch=379
06/19/2022 16:09:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.47 on epoch=384
06/19/2022 16:09:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=389
06/19/2022 16:09:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.51 on epoch=394
06/19/2022 16:09:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=399
06/19/2022 16:09:55 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 16:09:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=404
06/19/2022 16:09:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.52 on epoch=409
06/19/2022 16:09:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=414
06/19/2022 16:10:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=419
06/19/2022 16:10:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=424
06/19/2022 16:10:03 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 16:10:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=429
06/19/2022 16:10:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=434
06/19/2022 16:10:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=439
06/19/2022 16:10:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.52 on epoch=444
06/19/2022 16:10:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.52 on epoch=449
06/19/2022 16:10:10 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 16:10:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=454
06/19/2022 16:10:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=459
06/19/2022 16:10:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.51 on epoch=464
06/19/2022 16:10:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=469
06/19/2022 16:10:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=474
06/19/2022 16:10:17 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.3992490613266583 on epoch=474
06/19/2022 16:10:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=474, global_step=950
06/19/2022 16:10:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=479
06/19/2022 16:10:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=484
06/19/2022 16:10:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.48 on epoch=489
06/19/2022 16:10:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.51 on epoch=494
06/19/2022 16:10:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.48 on epoch=499
06/19/2022 16:10:25 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 16:10:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.48 on epoch=504
06/19/2022 16:10:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.51 on epoch=509
06/19/2022 16:10:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=514
06/19/2022 16:10:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=519
06/19/2022 16:10:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.46 on epoch=524
06/19/2022 16:10:32 - INFO - __main__ - Global step 1050 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 16:10:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=529
06/19/2022 16:10:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.47 on epoch=534
06/19/2022 16:10:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=539
06/19/2022 16:10:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=544
06/19/2022 16:10:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=549
06/19/2022 16:10:40 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 16:10:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.53 on epoch=554
06/19/2022 16:10:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=559
06/19/2022 16:10:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=564
06/19/2022 16:10:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=569
06/19/2022 16:10:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=574
06/19/2022 16:10:48 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 16:10:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.54 on epoch=579
06/19/2022 16:10:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=584
06/19/2022 16:10:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=589
06/19/2022 16:10:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=594
06/19/2022 16:10:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=599
06/19/2022 16:10:55 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 16:10:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.48 on epoch=604
06/19/2022 16:10:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=609
06/19/2022 16:10:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=614
06/19/2022 16:11:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=619
06/19/2022 16:11:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=624
06/19/2022 16:11:02 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 16:11:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=629
06/19/2022 16:11:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=634
06/19/2022 16:11:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=639
06/19/2022 16:11:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=644
06/19/2022 16:11:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=649
06/19/2022 16:11:09 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 16:11:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=654
06/19/2022 16:11:12 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=659
06/19/2022 16:11:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.42 on epoch=664
06/19/2022 16:11:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.48 on epoch=669
06/19/2022 16:11:16 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=674
06/19/2022 16:11:17 - INFO - __main__ - Global step 1350 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 16:11:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=679
06/19/2022 16:11:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=684
06/19/2022 16:11:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=689
06/19/2022 16:11:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=694
06/19/2022 16:11:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=699
06/19/2022 16:11:24 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 16:11:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=704
06/19/2022 16:11:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=709
06/19/2022 16:11:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=714
06/19/2022 16:11:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=719
06/19/2022 16:11:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=724
06/19/2022 16:11:32 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 16:11:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.39 on epoch=729
06/19/2022 16:11:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=734
06/19/2022 16:11:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.52 on epoch=739
06/19/2022 16:11:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=744
06/19/2022 16:11:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=749
06/19/2022 16:11:40 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 16:11:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=754
06/19/2022 16:11:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=759
06/19/2022 16:11:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=764
06/19/2022 16:11:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=769
06/19/2022 16:11:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.47 on epoch=774
06/19/2022 16:11:47 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 16:11:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=779
06/19/2022 16:11:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
06/19/2022 16:11:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=789
06/19/2022 16:11:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.45 on epoch=794
06/19/2022 16:11:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.48 on epoch=799
06/19/2022 16:11:55 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 16:11:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=804
06/19/2022 16:11:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=809
06/19/2022 16:12:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=814
06/19/2022 16:12:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=819
06/19/2022 16:12:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=824
06/19/2022 16:12:03 - INFO - __main__ - Global step 1650 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 16:12:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=829
06/19/2022 16:12:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=834
06/19/2022 16:12:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.50 on epoch=839
06/19/2022 16:12:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=844
06/19/2022 16:12:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=849
06/19/2022 16:12:11 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 16:12:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.44 on epoch=854
06/19/2022 16:12:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.45 on epoch=859
06/19/2022 16:12:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.45 on epoch=864
06/19/2022 16:12:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=869
06/19/2022 16:12:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=874
06/19/2022 16:12:18 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.3191489361702127 on epoch=874
06/19/2022 16:12:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=879
06/19/2022 16:12:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
06/19/2022 16:12:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=889
06/19/2022 16:12:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
06/19/2022 16:12:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.45 on epoch=899
06/19/2022 16:12:26 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 16:12:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=904
06/19/2022 16:12:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=909
06/19/2022 16:12:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=914
06/19/2022 16:12:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=919
06/19/2022 16:12:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.39 on epoch=924
06/19/2022 16:12:33 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 16:12:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=929
06/19/2022 16:12:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=934
06/19/2022 16:12:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=939
06/19/2022 16:12:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=944
06/19/2022 16:12:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=949
06/19/2022 16:12:40 - INFO - __main__ - Global step 1900 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 16:12:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.45 on epoch=954
06/19/2022 16:12:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=959
06/19/2022 16:12:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
06/19/2022 16:12:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=969
06/19/2022 16:12:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=974
06/19/2022 16:12:47 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 16:12:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=979
06/19/2022 16:12:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=984
06/19/2022 16:12:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=989
06/19/2022 16:12:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=994
06/19/2022 16:12:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=999
06/19/2022 16:12:54 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 16:12:54 - INFO - __main__ - save last model!
06/19/2022 16:12:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 16:12:55 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 16:12:55 - INFO - __main__ - Printing 3 examples
06/19/2022 16:12:55 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 16:12:55 - INFO - __main__ - ['0']
06/19/2022 16:12:55 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 16:12:55 - INFO - __main__ - ['1']
06/19/2022 16:12:55 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 16:12:55 - INFO - __main__ - ['1']
06/19/2022 16:12:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:12:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:12:55 - INFO - __main__ - Printing 3 examples
06/19/2022 16:12:55 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 16:12:55 - INFO - __main__ - ['1']
06/19/2022 16:12:55 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 16:12:55 - INFO - __main__ - ['1']
06/19/2022 16:12:55 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 16:12:55 - INFO - __main__ - ['1']
06/19/2022 16:12:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 16:12:55 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:12:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:12:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:12:55 - INFO - __main__ - Printing 3 examples
06/19/2022 16:12:55 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 16:12:55 - INFO - __main__ - ['1']
06/19/2022 16:12:55 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 16:12:55 - INFO - __main__ - ['1']
06/19/2022 16:12:55 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 16:12:55 - INFO - __main__ - ['1']
06/19/2022 16:12:55 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:12:55 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:12:55 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:12:59 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:13:01 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:13:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:13:01 - INFO - __main__ - Starting training!
06/19/2022 16:13:11 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 16:14:43 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_13_0.5_8_predictions.txt
06/19/2022 16:14:43 - INFO - __main__ - Classification-F1 on test data: 0.3066
06/19/2022 16:14:43 - INFO - __main__ - prefix=paws_16_13, lr=0.5, bsz=8, dev_performance=0.3992490613266583, test_performance=0.3066389322239556
06/19/2022 16:14:43 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.4, bsz=8 ...
06/19/2022 16:14:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:14:44 - INFO - __main__ - Printing 3 examples
06/19/2022 16:14:44 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 16:14:44 - INFO - __main__ - ['1']
06/19/2022 16:14:44 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 16:14:44 - INFO - __main__ - ['1']
06/19/2022 16:14:44 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 16:14:44 - INFO - __main__ - ['1']
06/19/2022 16:14:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:14:44 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:14:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:14:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:14:44 - INFO - __main__ - Printing 3 examples
06/19/2022 16:14:44 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 16:14:44 - INFO - __main__ - ['1']
06/19/2022 16:14:44 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 16:14:44 - INFO - __main__ - ['1']
06/19/2022 16:14:44 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 16:14:44 - INFO - __main__ - ['1']
06/19/2022 16:14:44 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:14:44 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:14:44 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:14:51 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:14:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:14:51 - INFO - __main__ - Starting training!
06/19/2022 16:14:53 - INFO - __main__ - Step 10 Global step 10 Train loss 4.06 on epoch=4
06/19/2022 16:14:54 - INFO - __main__ - Step 20 Global step 20 Train loss 3.50 on epoch=9
06/19/2022 16:14:56 - INFO - __main__ - Step 30 Global step 30 Train loss 2.85 on epoch=14
06/19/2022 16:14:57 - INFO - __main__ - Step 40 Global step 40 Train loss 2.39 on epoch=19
06/19/2022 16:14:58 - INFO - __main__ - Step 50 Global step 50 Train loss 2.08 on epoch=24
06/19/2022 16:14:58 - INFO - __main__ - Global step 50 Train loss 2.98 Classification-F1 0.3333333333333333 on epoch=24
06/19/2022 16:14:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/19/2022 16:15:00 - INFO - __main__ - Step 60 Global step 60 Train loss 1.78 on epoch=29
06/19/2022 16:15:02 - INFO - __main__ - Step 70 Global step 70 Train loss 1.65 on epoch=34
06/19/2022 16:15:03 - INFO - __main__ - Step 80 Global step 80 Train loss 1.61 on epoch=39
06/19/2022 16:15:05 - INFO - __main__ - Step 90 Global step 90 Train loss 1.43 on epoch=44
06/19/2022 16:15:06 - INFO - __main__ - Step 100 Global step 100 Train loss 1.35 on epoch=49
06/19/2022 16:15:06 - INFO - __main__ - Global step 100 Train loss 1.56 Classification-F1 0.3454545454545454 on epoch=49
06/19/2022 16:15:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3454545454545454 on epoch=49, global_step=100
06/19/2022 16:15:08 - INFO - __main__ - Step 110 Global step 110 Train loss 1.42 on epoch=54
06/19/2022 16:15:09 - INFO - __main__ - Step 120 Global step 120 Train loss 1.16 on epoch=59
06/19/2022 16:15:11 - INFO - __main__ - Step 130 Global step 130 Train loss 1.21 on epoch=64
06/19/2022 16:15:12 - INFO - __main__ - Step 140 Global step 140 Train loss 1.28 on epoch=69
06/19/2022 16:15:13 - INFO - __main__ - Step 150 Global step 150 Train loss 1.16 on epoch=74
06/19/2022 16:15:14 - INFO - __main__ - Global step 150 Train loss 1.25 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 16:15:15 - INFO - __main__ - Step 160 Global step 160 Train loss 1.19 on epoch=79
06/19/2022 16:15:16 - INFO - __main__ - Step 170 Global step 170 Train loss 1.02 on epoch=84
06/19/2022 16:15:18 - INFO - __main__ - Step 180 Global step 180 Train loss 1.01 on epoch=89
06/19/2022 16:15:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.90 on epoch=94
06/19/2022 16:15:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.97 on epoch=99
06/19/2022 16:15:21 - INFO - __main__ - Global step 200 Train loss 1.02 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 16:15:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=104
06/19/2022 16:15:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=109
06/19/2022 16:15:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.92 on epoch=114
06/19/2022 16:15:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=119
06/19/2022 16:15:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.83 on epoch=124
06/19/2022 16:15:29 - INFO - __main__ - Global step 250 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 16:15:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.80 on epoch=129
06/19/2022 16:15:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.69 on epoch=134
06/19/2022 16:15:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.71 on epoch=139
06/19/2022 16:15:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.82 on epoch=144
06/19/2022 16:15:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.71 on epoch=149
06/19/2022 16:15:36 - INFO - __main__ - Global step 300 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 16:15:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.78 on epoch=154
06/19/2022 16:15:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.78 on epoch=159
06/19/2022 16:15:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.69 on epoch=164
06/19/2022 16:15:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.72 on epoch=169
06/19/2022 16:15:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.70 on epoch=174
06/19/2022 16:15:42 - INFO - __main__ - Global step 350 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 16:15:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.72 on epoch=179
06/19/2022 16:15:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.73 on epoch=184
06/19/2022 16:15:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.64 on epoch=189
06/19/2022 16:15:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.71 on epoch=194
06/19/2022 16:15:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.68 on epoch=199
06/19/2022 16:15:49 - INFO - __main__ - Global step 400 Train loss 0.70 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 16:15:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.68 on epoch=204
06/19/2022 16:15:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.65 on epoch=209
06/19/2022 16:15:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.65 on epoch=214
06/19/2022 16:15:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.61 on epoch=219
06/19/2022 16:15:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.60 on epoch=224
06/19/2022 16:15:56 - INFO - __main__ - Global step 450 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 16:15:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.63 on epoch=229
06/19/2022 16:15:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=234
06/19/2022 16:16:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.61 on epoch=239
06/19/2022 16:16:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.59 on epoch=244
06/19/2022 16:16:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.60 on epoch=249
06/19/2022 16:16:03 - INFO - __main__ - Global step 500 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 16:16:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.58 on epoch=254
06/19/2022 16:16:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.57 on epoch=259
06/19/2022 16:16:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.52 on epoch=264
06/19/2022 16:16:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.54 on epoch=269
06/19/2022 16:16:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=274
06/19/2022 16:16:11 - INFO - __main__ - Global step 550 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 16:16:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=279
06/19/2022 16:16:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.53 on epoch=284
06/19/2022 16:16:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.53 on epoch=289
06/19/2022 16:16:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.53 on epoch=294
06/19/2022 16:16:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.58 on epoch=299
06/19/2022 16:16:18 - INFO - __main__ - Global step 600 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 16:16:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=304
06/19/2022 16:16:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=309
06/19/2022 16:16:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=314
06/19/2022 16:16:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.55 on epoch=319
06/19/2022 16:16:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.58 on epoch=324
06/19/2022 16:16:24 - INFO - __main__ - Global step 650 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 16:16:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.52 on epoch=329
06/19/2022 16:16:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.55 on epoch=334
06/19/2022 16:16:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.53 on epoch=339
06/19/2022 16:16:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.55 on epoch=344
06/19/2022 16:16:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.52 on epoch=349
06/19/2022 16:16:31 - INFO - __main__ - Global step 700 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 16:16:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=354
06/19/2022 16:16:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=359
06/19/2022 16:16:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.60 on epoch=364
06/19/2022 16:16:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.51 on epoch=369
06/19/2022 16:16:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=374
06/19/2022 16:16:38 - INFO - __main__ - Global step 750 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 16:16:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=379
06/19/2022 16:16:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.53 on epoch=384
06/19/2022 16:16:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=389
06/19/2022 16:16:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.58 on epoch=394
06/19/2022 16:16:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=399
06/19/2022 16:16:45 - INFO - __main__ - Global step 800 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 16:16:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.54 on epoch=404
06/19/2022 16:16:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=409
06/19/2022 16:16:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.49 on epoch=414
06/19/2022 16:16:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.56 on epoch=419
06/19/2022 16:16:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=424
06/19/2022 16:16:53 - INFO - __main__ - Global step 850 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 16:16:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=429
06/19/2022 16:16:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=434
06/19/2022 16:16:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=439
06/19/2022 16:16:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.51 on epoch=444
06/19/2022 16:17:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=449
06/19/2022 16:17:01 - INFO - __main__ - Global step 900 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 16:17:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.60 on epoch=454
06/19/2022 16:17:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.54 on epoch=459
06/19/2022 16:17:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=464
06/19/2022 16:17:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=469
06/19/2022 16:17:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=474
06/19/2022 16:17:08 - INFO - __main__ - Global step 950 Train loss 0.52 Classification-F1 0.3191489361702127 on epoch=474
06/19/2022 16:17:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=479
06/19/2022 16:17:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=484
06/19/2022 16:17:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.55 on epoch=489
06/19/2022 16:17:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.50 on epoch=494
06/19/2022 16:17:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.52 on epoch=499
06/19/2022 16:17:16 - INFO - __main__ - Global step 1000 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 16:17:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=504
06/19/2022 16:17:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=509
06/19/2022 16:17:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=514
06/19/2022 16:17:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.53 on epoch=519
06/19/2022 16:17:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=524
06/19/2022 16:17:23 - INFO - __main__ - Global step 1050 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 16:17:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=529
06/19/2022 16:17:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=534
06/19/2022 16:17:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.48 on epoch=539
06/19/2022 16:17:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=544
06/19/2022 16:17:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=549
06/19/2022 16:17:30 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 16:17:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=554
06/19/2022 16:17:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.54 on epoch=559
06/19/2022 16:17:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.49 on epoch=564
06/19/2022 16:17:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=569
06/19/2022 16:17:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.51 on epoch=574
06/19/2022 16:17:38 - INFO - __main__ - Global step 1150 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 16:17:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=579
06/19/2022 16:17:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=584
06/19/2022 16:17:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=589
06/19/2022 16:17:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.49 on epoch=594
06/19/2022 16:17:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.53 on epoch=599
06/19/2022 16:17:45 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 16:17:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=604
06/19/2022 16:17:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.49 on epoch=609
06/19/2022 16:17:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=614
06/19/2022 16:17:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=619
06/19/2022 16:17:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.49 on epoch=624
06/19/2022 16:17:53 - INFO - __main__ - Global step 1250 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 16:17:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.53 on epoch=629
06/19/2022 16:17:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.46 on epoch=634
06/19/2022 16:17:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.46 on epoch=639
06/19/2022 16:17:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=644
06/19/2022 16:18:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=649
06/19/2022 16:18:01 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 16:18:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.46 on epoch=654
06/19/2022 16:18:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.53 on epoch=659
06/19/2022 16:18:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.48 on epoch=664
06/19/2022 16:18:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=669
06/19/2022 16:18:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.51 on epoch=674
06/19/2022 16:18:09 - INFO - __main__ - Global step 1350 Train loss 0.49 Classification-F1 0.5901477832512315 on epoch=674
06/19/2022 16:18:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3454545454545454 -> 0.5901477832512315 on epoch=674, global_step=1350
06/19/2022 16:18:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=679
06/19/2022 16:18:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.47 on epoch=684
06/19/2022 16:18:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.46 on epoch=689
06/19/2022 16:18:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.49 on epoch=694
06/19/2022 16:18:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.54 on epoch=699
06/19/2022 16:18:17 - INFO - __main__ - Global step 1400 Train loss 0.49 Classification-F1 0.49090909090909085 on epoch=699
06/19/2022 16:18:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.48 on epoch=704
06/19/2022 16:18:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.46 on epoch=709
06/19/2022 16:18:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=714
06/19/2022 16:18:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=719
06/19/2022 16:18:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.46 on epoch=724
06/19/2022 16:18:24 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.5 on epoch=724
06/19/2022 16:18:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=729
06/19/2022 16:18:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=734
06/19/2022 16:18:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=739
06/19/2022 16:18:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.48 on epoch=744
06/19/2022 16:18:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.47 on epoch=749
06/19/2022 16:18:31 - INFO - __main__ - Global step 1500 Train loss 0.45 Classification-F1 0.40566959921798634 on epoch=749
06/19/2022 16:18:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.46 on epoch=754
06/19/2022 16:18:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=759
06/19/2022 16:18:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.48 on epoch=764
06/19/2022 16:18:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=769
06/19/2022 16:18:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.47 on epoch=774
06/19/2022 16:18:39 - INFO - __main__ - Global step 1550 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 16:18:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=779
06/19/2022 16:18:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=784
06/19/2022 16:18:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.49 on epoch=789
06/19/2022 16:18:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=794
06/19/2022 16:18:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
06/19/2022 16:18:46 - INFO - __main__ - Global step 1600 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 16:18:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=804
06/19/2022 16:18:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.48 on epoch=809
06/19/2022 16:18:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.45 on epoch=814
06/19/2022 16:18:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
06/19/2022 16:18:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=824
06/19/2022 16:18:53 - INFO - __main__ - Global step 1650 Train loss 0.45 Classification-F1 0.3992490613266583 on epoch=824
06/19/2022 16:18:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=829
06/19/2022 16:18:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=834
06/19/2022 16:18:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
06/19/2022 16:18:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=844
06/19/2022 16:19:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=849
06/19/2022 16:19:00 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 16:19:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=854
06/19/2022 16:19:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=859
06/19/2022 16:19:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=864
06/19/2022 16:19:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.41 on epoch=869
06/19/2022 16:19:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=874
06/19/2022 16:19:08 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 16:19:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=879
06/19/2022 16:19:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.48 on epoch=884
06/19/2022 16:19:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.43 on epoch=889
06/19/2022 16:19:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=894
06/19/2022 16:19:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.45 on epoch=899
06/19/2022 16:19:15 - INFO - __main__ - Global step 1800 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 16:19:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=904
06/19/2022 16:19:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.38 on epoch=909
06/19/2022 16:19:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=914
06/19/2022 16:19:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=919
06/19/2022 16:19:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=924
06/19/2022 16:19:22 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 16:19:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.43 on epoch=929
06/19/2022 16:19:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=934
06/19/2022 16:19:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=939
06/19/2022 16:19:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.45 on epoch=944
06/19/2022 16:19:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=949
06/19/2022 16:19:30 - INFO - __main__ - Global step 1900 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 16:19:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.42 on epoch=954
06/19/2022 16:19:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.42 on epoch=959
06/19/2022 16:19:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=964
06/19/2022 16:19:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=969
06/19/2022 16:19:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.42 on epoch=974
06/19/2022 16:19:38 - INFO - __main__ - Global step 1950 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 16:19:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.50 on epoch=979
06/19/2022 16:19:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=984
06/19/2022 16:19:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=989
06/19/2022 16:19:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=994
06/19/2022 16:19:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.47 on epoch=999
06/19/2022 16:19:46 - INFO - __main__ - Global step 2000 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 16:19:46 - INFO - __main__ - save last model!
06/19/2022 16:19:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 16:19:46 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 16:19:46 - INFO - __main__ - Printing 3 examples
06/19/2022 16:19:46 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 16:19:46 - INFO - __main__ - ['0']
06/19/2022 16:19:46 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 16:19:46 - INFO - __main__ - ['1']
06/19/2022 16:19:46 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 16:19:46 - INFO - __main__ - ['1']
06/19/2022 16:19:46 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:19:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:19:47 - INFO - __main__ - Printing 3 examples
06/19/2022 16:19:47 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 16:19:47 - INFO - __main__ - ['1']
06/19/2022 16:19:47 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 16:19:47 - INFO - __main__ - ['1']
06/19/2022 16:19:47 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 16:19:47 - INFO - __main__ - ['1']
06/19/2022 16:19:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 16:19:47 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:19:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:19:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:19:47 - INFO - __main__ - Printing 3 examples
06/19/2022 16:19:47 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 16:19:47 - INFO - __main__ - ['1']
06/19/2022 16:19:47 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 16:19:47 - INFO - __main__ - ['1']
06/19/2022 16:19:47 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 16:19:47 - INFO - __main__ - ['1']
06/19/2022 16:19:47 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:19:47 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:19:47 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:19:53 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:19:54 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:19:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:19:54 - INFO - __main__ - Starting training!
06/19/2022 16:20:03 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 16:21:40 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_13_0.4_8_predictions.txt
06/19/2022 16:21:40 - INFO - __main__ - Classification-F1 on test data: 0.3068
06/19/2022 16:21:40 - INFO - __main__ - prefix=paws_16_13, lr=0.4, bsz=8, dev_performance=0.5901477832512315, test_performance=0.3068294234908704
06/19/2022 16:21:40 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.3, bsz=8 ...
06/19/2022 16:21:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:21:42 - INFO - __main__ - Printing 3 examples
06/19/2022 16:21:42 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 16:21:42 - INFO - __main__ - ['1']
06/19/2022 16:21:42 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 16:21:42 - INFO - __main__ - ['1']
06/19/2022 16:21:42 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 16:21:42 - INFO - __main__ - ['1']
06/19/2022 16:21:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:21:42 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:21:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:21:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:21:42 - INFO - __main__ - Printing 3 examples
06/19/2022 16:21:42 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 16:21:42 - INFO - __main__ - ['1']
06/19/2022 16:21:42 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 16:21:42 - INFO - __main__ - ['1']
06/19/2022 16:21:42 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 16:21:42 - INFO - __main__ - ['1']
06/19/2022 16:21:42 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:21:42 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:21:42 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:21:48 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:21:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:21:48 - INFO - __main__ - Starting training!
06/19/2022 16:21:50 - INFO - __main__ - Step 10 Global step 10 Train loss 4.05 on epoch=4
06/19/2022 16:21:51 - INFO - __main__ - Step 20 Global step 20 Train loss 3.73 on epoch=9
06/19/2022 16:21:53 - INFO - __main__ - Step 30 Global step 30 Train loss 3.35 on epoch=14
06/19/2022 16:21:54 - INFO - __main__ - Step 40 Global step 40 Train loss 3.03 on epoch=19
06/19/2022 16:21:56 - INFO - __main__ - Step 50 Global step 50 Train loss 2.73 on epoch=24
06/19/2022 16:21:56 - INFO - __main__ - Global step 50 Train loss 3.38 Classification-F1 0.13333333333333333 on epoch=24
06/19/2022 16:21:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13333333333333333 on epoch=24, global_step=50
06/19/2022 16:21:57 - INFO - __main__ - Step 60 Global step 60 Train loss 2.40 on epoch=29
06/19/2022 16:21:59 - INFO - __main__ - Step 70 Global step 70 Train loss 1.97 on epoch=34
06/19/2022 16:22:00 - INFO - __main__ - Step 80 Global step 80 Train loss 1.99 on epoch=39
06/19/2022 16:22:02 - INFO - __main__ - Step 90 Global step 90 Train loss 1.70 on epoch=44
06/19/2022 16:22:03 - INFO - __main__ - Step 100 Global step 100 Train loss 1.65 on epoch=49
06/19/2022 16:22:03 - INFO - __main__ - Global step 100 Train loss 1.94 Classification-F1 0.5307917888563051 on epoch=49
06/19/2022 16:22:03 - INFO - __main__ - Saving model with best Classification-F1: 0.13333333333333333 -> 0.5307917888563051 on epoch=49, global_step=100
06/19/2022 16:22:05 - INFO - __main__ - Step 110 Global step 110 Train loss 1.49 on epoch=54
06/19/2022 16:22:06 - INFO - __main__ - Step 120 Global step 120 Train loss 1.41 on epoch=59
06/19/2022 16:22:07 - INFO - __main__ - Step 130 Global step 130 Train loss 1.28 on epoch=64
06/19/2022 16:22:09 - INFO - __main__ - Step 140 Global step 140 Train loss 1.17 on epoch=69
06/19/2022 16:22:10 - INFO - __main__ - Step 150 Global step 150 Train loss 1.29 on epoch=74
06/19/2022 16:22:11 - INFO - __main__ - Global step 150 Train loss 1.33 Classification-F1 0.3992490613266583 on epoch=74
06/19/2022 16:22:12 - INFO - __main__ - Step 160 Global step 160 Train loss 1.21 on epoch=79
06/19/2022 16:22:14 - INFO - __main__ - Step 170 Global step 170 Train loss 1.18 on epoch=84
06/19/2022 16:22:16 - INFO - __main__ - Step 180 Global step 180 Train loss 1.13 on epoch=89
06/19/2022 16:22:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.98 on epoch=94
06/19/2022 16:22:18 - INFO - __main__ - Step 200 Global step 200 Train loss 1.01 on epoch=99
06/19/2022 16:22:19 - INFO - __main__ - Global step 200 Train loss 1.10 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 16:22:20 - INFO - __main__ - Step 210 Global step 210 Train loss 1.01 on epoch=104
06/19/2022 16:22:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.96 on epoch=109
06/19/2022 16:22:23 - INFO - __main__ - Step 230 Global step 230 Train loss 1.00 on epoch=114
06/19/2022 16:22:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=119
06/19/2022 16:22:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.92 on epoch=124
06/19/2022 16:22:26 - INFO - __main__ - Global step 250 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 16:22:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.91 on epoch=129
06/19/2022 16:22:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.92 on epoch=134
06/19/2022 16:22:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.80 on epoch=139
06/19/2022 16:22:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.90 on epoch=144
06/19/2022 16:22:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.78 on epoch=149
06/19/2022 16:22:33 - INFO - __main__ - Global step 300 Train loss 0.86 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 16:22:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.78 on epoch=154
06/19/2022 16:22:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.80 on epoch=159
06/19/2022 16:22:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.78 on epoch=164
06/19/2022 16:22:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.77 on epoch=169
06/19/2022 16:22:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.75 on epoch=174
06/19/2022 16:22:40 - INFO - __main__ - Global step 350 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 16:22:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.66 on epoch=179
06/19/2022 16:22:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.73 on epoch=184
06/19/2022 16:22:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.61 on epoch=189
06/19/2022 16:22:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.68 on epoch=194
06/19/2022 16:22:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.78 on epoch=199
06/19/2022 16:22:47 - INFO - __main__ - Global step 400 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 16:22:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.71 on epoch=204
06/19/2022 16:22:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.66 on epoch=209
06/19/2022 16:22:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.66 on epoch=214
06/19/2022 16:22:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.68 on epoch=219
06/19/2022 16:22:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.63 on epoch=224
06/19/2022 16:22:55 - INFO - __main__ - Global step 450 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 16:22:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=229
06/19/2022 16:22:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.60 on epoch=234
06/19/2022 16:22:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.71 on epoch=239
06/19/2022 16:23:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.58 on epoch=244
06/19/2022 16:23:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.56 on epoch=249
06/19/2022 16:23:02 - INFO - __main__ - Global step 500 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 16:23:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.61 on epoch=254
06/19/2022 16:23:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.67 on epoch=259
06/19/2022 16:23:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.65 on epoch=264
06/19/2022 16:23:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.63 on epoch=269
06/19/2022 16:23:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.58 on epoch=274
06/19/2022 16:23:09 - INFO - __main__ - Global step 550 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 16:23:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.60 on epoch=279
06/19/2022 16:23:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.62 on epoch=284
06/19/2022 16:23:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.64 on epoch=289
06/19/2022 16:23:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.67 on epoch=294
06/19/2022 16:23:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.56 on epoch=299
06/19/2022 16:23:17 - INFO - __main__ - Global step 600 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 16:23:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=304
06/19/2022 16:23:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.65 on epoch=309
06/19/2022 16:23:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.59 on epoch=314
06/19/2022 16:23:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.57 on epoch=319
06/19/2022 16:23:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=324
06/19/2022 16:23:25 - INFO - __main__ - Global step 650 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 16:23:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.58 on epoch=329
06/19/2022 16:23:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.57 on epoch=334
06/19/2022 16:23:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.60 on epoch=339
06/19/2022 16:23:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.62 on epoch=344
06/19/2022 16:23:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=349
06/19/2022 16:23:33 - INFO - __main__ - Global step 700 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 16:23:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.53 on epoch=354
06/19/2022 16:23:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=359
06/19/2022 16:23:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.55 on epoch=364
06/19/2022 16:23:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=369
06/19/2022 16:23:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.50 on epoch=374
06/19/2022 16:23:40 - INFO - __main__ - Global step 750 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 16:23:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.56 on epoch=379
06/19/2022 16:23:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.52 on epoch=384
06/19/2022 16:23:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.61 on epoch=389
06/19/2022 16:23:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.56 on epoch=394
06/19/2022 16:23:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=399
06/19/2022 16:23:48 - INFO - __main__ - Global step 800 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 16:23:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=404
06/19/2022 16:23:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.56 on epoch=409
06/19/2022 16:23:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.54 on epoch=414
06/19/2022 16:23:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=419
06/19/2022 16:23:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.50 on epoch=424
06/19/2022 16:23:54 - INFO - __main__ - Global step 850 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 16:23:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.55 on epoch=429
06/19/2022 16:23:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=434
06/19/2022 16:23:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=439
06/19/2022 16:24:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.55 on epoch=444
06/19/2022 16:24:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.52 on epoch=449
06/19/2022 16:24:02 - INFO - __main__ - Global step 900 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 16:24:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.49 on epoch=454
06/19/2022 16:24:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.57 on epoch=459
06/19/2022 16:24:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.54 on epoch=464
06/19/2022 16:24:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.53 on epoch=469
06/19/2022 16:24:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=474
06/19/2022 16:24:10 - INFO - __main__ - Global step 950 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 16:24:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.53 on epoch=479
06/19/2022 16:24:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.63 on epoch=484
06/19/2022 16:24:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.60 on epoch=489
06/19/2022 16:24:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.57 on epoch=494
06/19/2022 16:24:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.57 on epoch=499
06/19/2022 16:24:17 - INFO - __main__ - Global step 1000 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 16:24:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.55 on epoch=504
06/19/2022 16:24:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=509
06/19/2022 16:24:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.53 on epoch=514
06/19/2022 16:24:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=519
06/19/2022 16:24:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.51 on epoch=524
06/19/2022 16:24:24 - INFO - __main__ - Global step 1050 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 16:24:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.53 on epoch=529
06/19/2022 16:24:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.51 on epoch=534
06/19/2022 16:24:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=539
06/19/2022 16:24:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=544
06/19/2022 16:24:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.50 on epoch=549
06/19/2022 16:24:31 - INFO - __main__ - Global step 1100 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 16:24:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=554
06/19/2022 16:24:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=559
06/19/2022 16:24:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.55 on epoch=564
06/19/2022 16:24:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.47 on epoch=569
06/19/2022 16:24:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=574
06/19/2022 16:24:38 - INFO - __main__ - Global step 1150 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 16:24:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.58 on epoch=579
06/19/2022 16:24:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=584
06/19/2022 16:24:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=589
06/19/2022 16:24:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.56 on epoch=594
06/19/2022 16:24:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.47 on epoch=599
06/19/2022 16:24:46 - INFO - __main__ - Global step 1200 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 16:24:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=604
06/19/2022 16:24:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=609
06/19/2022 16:24:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=614
06/19/2022 16:24:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=619
06/19/2022 16:24:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=624
06/19/2022 16:24:53 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 16:24:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=629
06/19/2022 16:24:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=634
06/19/2022 16:24:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.51 on epoch=639
06/19/2022 16:24:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=644
06/19/2022 16:25:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.50 on epoch=649
06/19/2022 16:25:01 - INFO - __main__ - Global step 1300 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 16:25:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=654
06/19/2022 16:25:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=659
06/19/2022 16:25:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.42 on epoch=664
06/19/2022 16:25:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=669
06/19/2022 16:25:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=674
06/19/2022 16:25:08 - INFO - __main__ - Global step 1350 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 16:25:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.45 on epoch=679
06/19/2022 16:25:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=684
06/19/2022 16:25:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.44 on epoch=689
06/19/2022 16:25:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.51 on epoch=694
06/19/2022 16:25:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.51 on epoch=699
06/19/2022 16:25:16 - INFO - __main__ - Global step 1400 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 16:25:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.50 on epoch=704
06/19/2022 16:25:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.50 on epoch=709
06/19/2022 16:25:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=714
06/19/2022 16:25:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=719
06/19/2022 16:25:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.45 on epoch=724
06/19/2022 16:25:23 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 16:25:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
06/19/2022 16:25:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=734
06/19/2022 16:25:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.49 on epoch=739
06/19/2022 16:25:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.46 on epoch=744
06/19/2022 16:25:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=749
06/19/2022 16:25:30 - INFO - __main__ - Global step 1500 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 16:25:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.51 on epoch=754
06/19/2022 16:25:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.48 on epoch=759
06/19/2022 16:25:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.45 on epoch=764
06/19/2022 16:25:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.53 on epoch=769
06/19/2022 16:25:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=774
06/19/2022 16:25:38 - INFO - __main__ - Global step 1550 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 16:25:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=779
06/19/2022 16:25:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.48 on epoch=784
06/19/2022 16:25:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=789
06/19/2022 16:25:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.48 on epoch=794
06/19/2022 16:25:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=799
06/19/2022 16:25:45 - INFO - __main__ - Global step 1600 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 16:25:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.47 on epoch=804
06/19/2022 16:25:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=809
06/19/2022 16:25:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.45 on epoch=814
06/19/2022 16:25:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=819
06/19/2022 16:25:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.46 on epoch=824
06/19/2022 16:25:52 - INFO - __main__ - Global step 1650 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 16:25:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=829
06/19/2022 16:25:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.43 on epoch=834
06/19/2022 16:25:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
06/19/2022 16:25:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.43 on epoch=844
06/19/2022 16:25:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=849
06/19/2022 16:26:00 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 16:26:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.44 on epoch=854
06/19/2022 16:26:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=859
06/19/2022 16:26:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=864
06/19/2022 16:26:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.52 on epoch=869
06/19/2022 16:26:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=874
06/19/2022 16:26:08 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 16:26:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.45 on epoch=879
06/19/2022 16:26:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=884
06/19/2022 16:26:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=889
06/19/2022 16:26:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.45 on epoch=894
06/19/2022 16:26:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=899
06/19/2022 16:26:15 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 16:26:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=904
06/19/2022 16:26:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=909
06/19/2022 16:26:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=914
06/19/2022 16:26:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.46 on epoch=919
06/19/2022 16:26:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=924
06/19/2022 16:26:23 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 16:26:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=929
06/19/2022 16:26:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=934
06/19/2022 16:26:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=939
06/19/2022 16:26:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.45 on epoch=944
06/19/2022 16:26:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=949
06/19/2022 16:26:30 - INFO - __main__ - Global step 1900 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 16:26:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.45 on epoch=954
06/19/2022 16:26:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
06/19/2022 16:26:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=964
06/19/2022 16:26:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.45 on epoch=969
06/19/2022 16:26:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=974
06/19/2022 16:26:37 - INFO - __main__ - Global step 1950 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 16:26:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
06/19/2022 16:26:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=984
06/19/2022 16:26:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.45 on epoch=989
06/19/2022 16:26:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=994
06/19/2022 16:26:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.47 on epoch=999
06/19/2022 16:26:45 - INFO - __main__ - Global step 2000 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 16:26:45 - INFO - __main__ - save last model!
06/19/2022 16:26:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 16:26:45 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 16:26:45 - INFO - __main__ - Printing 3 examples
06/19/2022 16:26:45 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 16:26:45 - INFO - __main__ - ['0']
06/19/2022 16:26:45 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 16:26:45 - INFO - __main__ - ['1']
06/19/2022 16:26:45 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 16:26:45 - INFO - __main__ - ['1']
06/19/2022 16:26:45 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:26:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:26:45 - INFO - __main__ - Printing 3 examples
06/19/2022 16:26:45 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 16:26:45 - INFO - __main__ - ['1']
06/19/2022 16:26:45 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 16:26:45 - INFO - __main__ - ['1']
06/19/2022 16:26:45 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 16:26:45 - INFO - __main__ - ['1']
06/19/2022 16:26:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 16:26:45 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:26:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:26:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:26:46 - INFO - __main__ - Printing 3 examples
06/19/2022 16:26:46 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 16:26:46 - INFO - __main__ - ['1']
06/19/2022 16:26:46 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 16:26:46 - INFO - __main__ - ['1']
06/19/2022 16:26:46 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 16:26:46 - INFO - __main__ - ['1']
06/19/2022 16:26:46 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:26:46 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:26:46 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:26:49 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:26:51 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:26:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:26:51 - INFO - __main__ - Starting training!
06/19/2022 16:26:59 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 16:28:31 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_13_0.3_8_predictions.txt
06/19/2022 16:28:31 - INFO - __main__ - Classification-F1 on test data: 0.3066
06/19/2022 16:28:32 - INFO - __main__ - prefix=paws_16_13, lr=0.3, bsz=8, dev_performance=0.5307917888563051, test_performance=0.3066389322239556
06/19/2022 16:28:32 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.2, bsz=8 ...
06/19/2022 16:28:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:28:33 - INFO - __main__ - Printing 3 examples
06/19/2022 16:28:33 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 16:28:33 - INFO - __main__ - ['1']
06/19/2022 16:28:33 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 16:28:33 - INFO - __main__ - ['1']
06/19/2022 16:28:33 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 16:28:33 - INFO - __main__ - ['1']
06/19/2022 16:28:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:28:33 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:28:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:28:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:28:33 - INFO - __main__ - Printing 3 examples
06/19/2022 16:28:33 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 16:28:33 - INFO - __main__ - ['1']
06/19/2022 16:28:33 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 16:28:33 - INFO - __main__ - ['1']
06/19/2022 16:28:33 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 16:28:33 - INFO - __main__ - ['1']
06/19/2022 16:28:33 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:28:33 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:28:33 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:28:39 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:28:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:28:39 - INFO - __main__ - Starting training!
06/19/2022 16:28:41 - INFO - __main__ - Step 10 Global step 10 Train loss 4.12 on epoch=4
06/19/2022 16:28:42 - INFO - __main__ - Step 20 Global step 20 Train loss 3.88 on epoch=9
06/19/2022 16:28:43 - INFO - __main__ - Step 30 Global step 30 Train loss 3.42 on epoch=14
06/19/2022 16:28:45 - INFO - __main__ - Step 40 Global step 40 Train loss 3.12 on epoch=19
06/19/2022 16:28:46 - INFO - __main__ - Step 50 Global step 50 Train loss 2.92 on epoch=24
06/19/2022 16:28:47 - INFO - __main__ - Global step 50 Train loss 3.49 Classification-F1 0.029411764705882353 on epoch=24
06/19/2022 16:28:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.029411764705882353 on epoch=24, global_step=50
06/19/2022 16:28:48 - INFO - __main__ - Step 60 Global step 60 Train loss 2.52 on epoch=29
06/19/2022 16:28:49 - INFO - __main__ - Step 70 Global step 70 Train loss 2.33 on epoch=34
06/19/2022 16:28:51 - INFO - __main__ - Step 80 Global step 80 Train loss 2.19 on epoch=39
06/19/2022 16:28:52 - INFO - __main__ - Step 90 Global step 90 Train loss 1.92 on epoch=44
06/19/2022 16:28:53 - INFO - __main__ - Step 100 Global step 100 Train loss 1.83 on epoch=49
06/19/2022 16:28:54 - INFO - __main__ - Global step 100 Train loss 2.16 Classification-F1 0.3333333333333333 on epoch=49
06/19/2022 16:28:54 - INFO - __main__ - Saving model with best Classification-F1: 0.029411764705882353 -> 0.3333333333333333 on epoch=49, global_step=100
06/19/2022 16:28:55 - INFO - __main__ - Step 110 Global step 110 Train loss 1.73 on epoch=54
06/19/2022 16:28:56 - INFO - __main__ - Step 120 Global step 120 Train loss 1.59 on epoch=59
06/19/2022 16:28:57 - INFO - __main__ - Step 130 Global step 130 Train loss 1.53 on epoch=64
06/19/2022 16:28:59 - INFO - __main__ - Step 140 Global step 140 Train loss 1.57 on epoch=69
06/19/2022 16:29:00 - INFO - __main__ - Step 150 Global step 150 Train loss 1.48 on epoch=74
06/19/2022 16:29:00 - INFO - __main__ - Global step 150 Train loss 1.58 Classification-F1 0.5835835835835835 on epoch=74
06/19/2022 16:29:00 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5835835835835835 on epoch=74, global_step=150
06/19/2022 16:29:02 - INFO - __main__ - Step 160 Global step 160 Train loss 1.35 on epoch=79
06/19/2022 16:29:03 - INFO - __main__ - Step 170 Global step 170 Train loss 1.32 on epoch=84
06/19/2022 16:29:04 - INFO - __main__ - Step 180 Global step 180 Train loss 1.21 on epoch=89
06/19/2022 16:29:06 - INFO - __main__ - Step 190 Global step 190 Train loss 1.21 on epoch=94
06/19/2022 16:29:07 - INFO - __main__ - Step 200 Global step 200 Train loss 1.19 on epoch=99
06/19/2022 16:29:08 - INFO - __main__ - Global step 200 Train loss 1.26 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 16:29:09 - INFO - __main__ - Step 210 Global step 210 Train loss 1.18 on epoch=104
06/19/2022 16:29:10 - INFO - __main__ - Step 220 Global step 220 Train loss 1.08 on epoch=109
06/19/2022 16:29:11 - INFO - __main__ - Step 230 Global step 230 Train loss 1.05 on epoch=114
06/19/2022 16:29:13 - INFO - __main__ - Step 240 Global step 240 Train loss 1.14 on epoch=119
06/19/2022 16:29:14 - INFO - __main__ - Step 250 Global step 250 Train loss 1.11 on epoch=124
06/19/2022 16:29:15 - INFO - __main__ - Global step 250 Train loss 1.11 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 16:29:16 - INFO - __main__ - Step 260 Global step 260 Train loss 1.03 on epoch=129
06/19/2022 16:29:17 - INFO - __main__ - Step 270 Global step 270 Train loss 1.01 on epoch=134
06/19/2022 16:29:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.97 on epoch=139
06/19/2022 16:29:20 - INFO - __main__ - Step 290 Global step 290 Train loss 1.02 on epoch=144
06/19/2022 16:29:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.98 on epoch=149
06/19/2022 16:29:22 - INFO - __main__ - Global step 300 Train loss 1.00 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 16:29:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.89 on epoch=154
06/19/2022 16:29:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.95 on epoch=159
06/19/2022 16:29:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.89 on epoch=164
06/19/2022 16:29:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.92 on epoch=169
06/19/2022 16:29:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.82 on epoch=174
06/19/2022 16:29:30 - INFO - __main__ - Global step 350 Train loss 0.90 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 16:29:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.95 on epoch=179
06/19/2022 16:29:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.83 on epoch=184
06/19/2022 16:29:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.82 on epoch=189
06/19/2022 16:29:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.74 on epoch=194
06/19/2022 16:29:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.79 on epoch=199
06/19/2022 16:29:38 - INFO - __main__ - Global step 400 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 16:29:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.83 on epoch=204
06/19/2022 16:29:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.74 on epoch=209
06/19/2022 16:29:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.78 on epoch=214
06/19/2022 16:29:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.78 on epoch=219
06/19/2022 16:29:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.75 on epoch=224
06/19/2022 16:29:45 - INFO - __main__ - Global step 450 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 16:29:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.84 on epoch=229
06/19/2022 16:29:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.88 on epoch=234
06/19/2022 16:29:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.74 on epoch=239
06/19/2022 16:29:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.76 on epoch=244
06/19/2022 16:29:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.78 on epoch=249
06/19/2022 16:29:52 - INFO - __main__ - Global step 500 Train loss 0.80 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 16:29:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.70 on epoch=254
06/19/2022 16:29:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.67 on epoch=259
06/19/2022 16:29:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.70 on epoch=264
06/19/2022 16:29:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=269
06/19/2022 16:30:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.70 on epoch=274
06/19/2022 16:30:00 - INFO - __main__ - Global step 550 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 16:30:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.78 on epoch=279
06/19/2022 16:30:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.70 on epoch=284
06/19/2022 16:30:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.73 on epoch=289
06/19/2022 16:30:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.71 on epoch=294
06/19/2022 16:30:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.68 on epoch=299
06/19/2022 16:30:07 - INFO - __main__ - Global step 600 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 16:30:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.69 on epoch=304
06/19/2022 16:30:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.61 on epoch=309
06/19/2022 16:30:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.64 on epoch=314
06/19/2022 16:30:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.67 on epoch=319
06/19/2022 16:30:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.66 on epoch=324
06/19/2022 16:30:15 - INFO - __main__ - Global step 650 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 16:30:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.66 on epoch=329
06/19/2022 16:30:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.60 on epoch=334
06/19/2022 16:30:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.70 on epoch=339
06/19/2022 16:30:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.62 on epoch=344
06/19/2022 16:30:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.61 on epoch=349
06/19/2022 16:30:22 - INFO - __main__ - Global step 700 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 16:30:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.62 on epoch=354
06/19/2022 16:30:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.74 on epoch=359
06/19/2022 16:30:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.65 on epoch=364
06/19/2022 16:30:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.69 on epoch=369
06/19/2022 16:30:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.56 on epoch=374
06/19/2022 16:30:29 - INFO - __main__ - Global step 750 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 16:30:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.58 on epoch=379
06/19/2022 16:30:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.67 on epoch=384
06/19/2022 16:30:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.53 on epoch=389
06/19/2022 16:30:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.59 on epoch=394
06/19/2022 16:30:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.56 on epoch=399
06/19/2022 16:30:37 - INFO - __main__ - Global step 800 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 16:30:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.57 on epoch=404
06/19/2022 16:30:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.65 on epoch=409
06/19/2022 16:30:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.65 on epoch=414
06/19/2022 16:30:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.63 on epoch=419
06/19/2022 16:30:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.60 on epoch=424
06/19/2022 16:30:45 - INFO - __main__ - Global step 850 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 16:30:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.60 on epoch=429
06/19/2022 16:30:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.64 on epoch=434
06/19/2022 16:30:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.63 on epoch=439
06/19/2022 16:30:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.58 on epoch=444
06/19/2022 16:30:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.54 on epoch=449
06/19/2022 16:30:52 - INFO - __main__ - Global step 900 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 16:30:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.59 on epoch=454
06/19/2022 16:30:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.59 on epoch=459
06/19/2022 16:30:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.57 on epoch=464
06/19/2022 16:30:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.57 on epoch=469
06/19/2022 16:30:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.57 on epoch=474
06/19/2022 16:31:00 - INFO - __main__ - Global step 950 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 16:31:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.57 on epoch=479
06/19/2022 16:31:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.57 on epoch=484
06/19/2022 16:31:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.56 on epoch=489
06/19/2022 16:31:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.58 on epoch=494
06/19/2022 16:31:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.57 on epoch=499
06/19/2022 16:31:07 - INFO - __main__ - Global step 1000 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 16:31:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.59 on epoch=504
06/19/2022 16:31:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.52 on epoch=509
06/19/2022 16:31:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.52 on epoch=514
06/19/2022 16:31:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.56 on epoch=519
06/19/2022 16:31:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.51 on epoch=524
06/19/2022 16:31:14 - INFO - __main__ - Global step 1050 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 16:31:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.58 on epoch=529
06/19/2022 16:31:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.57 on epoch=534
06/19/2022 16:31:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.59 on epoch=539
06/19/2022 16:31:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.52 on epoch=544
06/19/2022 16:31:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.51 on epoch=549
06/19/2022 16:31:21 - INFO - __main__ - Global step 1100 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 16:31:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.60 on epoch=554
06/19/2022 16:31:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.59 on epoch=559
06/19/2022 16:31:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.60 on epoch=564
06/19/2022 16:31:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.53 on epoch=569
06/19/2022 16:31:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.57 on epoch=574
06/19/2022 16:31:29 - INFO - __main__ - Global step 1150 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 16:31:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.52 on epoch=579
06/19/2022 16:31:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.50 on epoch=584
06/19/2022 16:31:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.60 on epoch=589
06/19/2022 16:31:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.56 on epoch=594
06/19/2022 16:31:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.43 on epoch=599
06/19/2022 16:31:35 - INFO - __main__ - Global step 1200 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 16:31:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.54 on epoch=604
06/19/2022 16:31:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.58 on epoch=609
06/19/2022 16:31:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.57 on epoch=614
06/19/2022 16:31:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.48 on epoch=619
06/19/2022 16:31:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.49 on epoch=624
06/19/2022 16:31:42 - INFO - __main__ - Global step 1250 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 16:31:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.52 on epoch=629
06/19/2022 16:31:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=634
06/19/2022 16:31:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.51 on epoch=639
06/19/2022 16:31:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.55 on epoch=644
06/19/2022 16:31:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.59 on epoch=649
06/19/2022 16:31:50 - INFO - __main__ - Global step 1300 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 16:31:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.55 on epoch=654
06/19/2022 16:31:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.52 on epoch=659
06/19/2022 16:31:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.49 on epoch=664
06/19/2022 16:31:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.62 on epoch=669
06/19/2022 16:31:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.57 on epoch=674
06/19/2022 16:31:57 - INFO - __main__ - Global step 1350 Train loss 0.55 Classification-F1 0.3191489361702127 on epoch=674
06/19/2022 16:31:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.49 on epoch=679
06/19/2022 16:32:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.53 on epoch=684
06/19/2022 16:32:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=689
06/19/2022 16:32:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.49 on epoch=694
06/19/2022 16:32:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.51 on epoch=699
06/19/2022 16:32:05 - INFO - __main__ - Global step 1400 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 16:32:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.49 on epoch=704
06/19/2022 16:32:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=709
06/19/2022 16:32:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=714
06/19/2022 16:32:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=719
06/19/2022 16:32:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.49 on epoch=724
06/19/2022 16:32:12 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 16:32:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.49 on epoch=729
06/19/2022 16:32:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.53 on epoch=734
06/19/2022 16:32:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.48 on epoch=739
06/19/2022 16:32:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.48 on epoch=744
06/19/2022 16:32:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.48 on epoch=749
06/19/2022 16:32:20 - INFO - __main__ - Global step 1500 Train loss 0.49 Classification-F1 0.3191489361702127 on epoch=749
06/19/2022 16:32:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.50 on epoch=754
06/19/2022 16:32:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.46 on epoch=759
06/19/2022 16:32:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.49 on epoch=764
06/19/2022 16:32:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.54 on epoch=769
06/19/2022 16:32:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=774
06/19/2022 16:32:28 - INFO - __main__ - Global step 1550 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 16:32:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.46 on epoch=779
06/19/2022 16:32:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.48 on epoch=784
06/19/2022 16:32:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.48 on epoch=789
06/19/2022 16:32:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.49 on epoch=794
06/19/2022 16:32:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.51 on epoch=799
06/19/2022 16:32:35 - INFO - __main__ - Global step 1600 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 16:32:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.57 on epoch=804
06/19/2022 16:32:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=809
06/19/2022 16:32:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.51 on epoch=814
06/19/2022 16:32:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=819
06/19/2022 16:32:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.46 on epoch=824
06/19/2022 16:32:42 - INFO - __main__ - Global step 1650 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 16:32:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=829
06/19/2022 16:32:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.46 on epoch=834
06/19/2022 16:32:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.49 on epoch=839
06/19/2022 16:32:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.43 on epoch=844
06/19/2022 16:32:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.51 on epoch=849
06/19/2022 16:32:49 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 16:32:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.44 on epoch=854
06/19/2022 16:32:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.61 on epoch=859
06/19/2022 16:32:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.56 on epoch=864
06/19/2022 16:32:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=869
06/19/2022 16:32:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.47 on epoch=874
06/19/2022 16:32:57 - INFO - __main__ - Global step 1750 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 16:32:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.45 on epoch=879
06/19/2022 16:33:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=884
06/19/2022 16:33:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=889
06/19/2022 16:33:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.45 on epoch=894
06/19/2022 16:33:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=899
06/19/2022 16:33:04 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 16:33:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.49 on epoch=904
06/19/2022 16:33:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.50 on epoch=909
06/19/2022 16:33:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.45 on epoch=914
06/19/2022 16:33:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=919
06/19/2022 16:33:12 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.49 on epoch=924
06/19/2022 16:33:12 - INFO - __main__ - Global step 1850 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 16:33:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=929
06/19/2022 16:33:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.48 on epoch=934
06/19/2022 16:33:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.46 on epoch=939
06/19/2022 16:33:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.41 on epoch=944
06/19/2022 16:33:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=949
06/19/2022 16:33:20 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 16:33:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.48 on epoch=954
06/19/2022 16:33:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=959
06/19/2022 16:33:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.51 on epoch=964
06/19/2022 16:33:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.46 on epoch=969
06/19/2022 16:33:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.45 on epoch=974
06/19/2022 16:33:27 - INFO - __main__ - Global step 1950 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 16:33:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=979
06/19/2022 16:33:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=984
06/19/2022 16:33:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=989
06/19/2022 16:33:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=994
06/19/2022 16:33:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=999
06/19/2022 16:33:35 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 16:33:35 - INFO - __main__ - save last model!
06/19/2022 16:33:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 16:33:35 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 16:33:35 - INFO - __main__ - Printing 3 examples
06/19/2022 16:33:35 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 16:33:35 - INFO - __main__ - ['0']
06/19/2022 16:33:35 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 16:33:35 - INFO - __main__ - ['1']
06/19/2022 16:33:35 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 16:33:35 - INFO - __main__ - ['1']
06/19/2022 16:33:35 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:33:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:33:36 - INFO - __main__ - Printing 3 examples
06/19/2022 16:33:36 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 16:33:36 - INFO - __main__ - ['1']
06/19/2022 16:33:36 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 16:33:36 - INFO - __main__ - ['1']
06/19/2022 16:33:36 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 16:33:36 - INFO - __main__ - ['1']
06/19/2022 16:33:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 16:33:36 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:33:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:33:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:33:36 - INFO - __main__ - Printing 3 examples
06/19/2022 16:33:36 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 16:33:36 - INFO - __main__ - ['1']
06/19/2022 16:33:36 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 16:33:36 - INFO - __main__ - ['1']
06/19/2022 16:33:36 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 16:33:36 - INFO - __main__ - ['1']
06/19/2022 16:33:36 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:33:36 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:33:36 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:33:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:33:42 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:33:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:33:42 - INFO - __main__ - Starting training!
06/19/2022 16:33:47 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 16:35:09 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_13_0.2_8_predictions.txt
06/19/2022 16:35:09 - INFO - __main__ - Classification-F1 on test data: 0.3065
06/19/2022 16:35:09 - INFO - __main__ - prefix=paws_16_13, lr=0.2, bsz=8, dev_performance=0.5835835835835835, test_performance=0.3065187239944522
06/19/2022 16:35:09 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.5, bsz=8 ...
06/19/2022 16:35:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:35:10 - INFO - __main__ - Printing 3 examples
06/19/2022 16:35:10 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 16:35:10 - INFO - __main__ - ['1']
06/19/2022 16:35:10 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 16:35:10 - INFO - __main__ - ['1']
06/19/2022 16:35:10 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 16:35:10 - INFO - __main__ - ['1']
06/19/2022 16:35:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:35:10 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:35:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:35:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:35:10 - INFO - __main__ - Printing 3 examples
06/19/2022 16:35:10 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 16:35:10 - INFO - __main__ - ['1']
06/19/2022 16:35:10 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 16:35:10 - INFO - __main__ - ['1']
06/19/2022 16:35:10 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 16:35:10 - INFO - __main__ - ['1']
06/19/2022 16:35:10 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:35:11 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:35:11 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:35:17 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:35:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:35:17 - INFO - __main__ - Starting training!
06/19/2022 16:35:19 - INFO - __main__ - Step 10 Global step 10 Train loss 4.03 on epoch=4
06/19/2022 16:35:20 - INFO - __main__ - Step 20 Global step 20 Train loss 3.36 on epoch=9
06/19/2022 16:35:21 - INFO - __main__ - Step 30 Global step 30 Train loss 2.75 on epoch=14
06/19/2022 16:35:22 - INFO - __main__ - Step 40 Global step 40 Train loss 2.27 on epoch=19
06/19/2022 16:35:24 - INFO - __main__ - Step 50 Global step 50 Train loss 1.80 on epoch=24
06/19/2022 16:35:24 - INFO - __main__ - Global step 50 Train loss 2.84 Classification-F1 0.3333333333333333 on epoch=24
06/19/2022 16:35:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/19/2022 16:35:26 - INFO - __main__ - Step 60 Global step 60 Train loss 1.58 on epoch=29
06/19/2022 16:35:27 - INFO - __main__ - Step 70 Global step 70 Train loss 1.37 on epoch=34
06/19/2022 16:35:28 - INFO - __main__ - Step 80 Global step 80 Train loss 1.35 on epoch=39
06/19/2022 16:35:30 - INFO - __main__ - Step 90 Global step 90 Train loss 1.21 on epoch=44
06/19/2022 16:35:31 - INFO - __main__ - Step 100 Global step 100 Train loss 1.06 on epoch=49
06/19/2022 16:35:31 - INFO - __main__ - Global step 100 Train loss 1.31 Classification-F1 0.3333333333333333 on epoch=49
06/19/2022 16:35:33 - INFO - __main__ - Step 110 Global step 110 Train loss 1.11 on epoch=54
06/19/2022 16:35:34 - INFO - __main__ - Step 120 Global step 120 Train loss 0.93 on epoch=59
06/19/2022 16:35:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.96 on epoch=64
06/19/2022 16:35:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.85 on epoch=69
06/19/2022 16:35:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.90 on epoch=74
06/19/2022 16:35:39 - INFO - __main__ - Global step 150 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 16:35:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.80 on epoch=79
06/19/2022 16:35:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.79 on epoch=84
06/19/2022 16:35:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.75 on epoch=89
06/19/2022 16:35:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.79 on epoch=94
06/19/2022 16:35:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.68 on epoch=99
06/19/2022 16:35:46 - INFO - __main__ - Global step 200 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 16:35:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.69 on epoch=104
06/19/2022 16:35:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.75 on epoch=109
06/19/2022 16:35:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.67 on epoch=114
06/19/2022 16:35:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.65 on epoch=119
06/19/2022 16:35:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.69 on epoch=124
06/19/2022 16:35:54 - INFO - __main__ - Global step 250 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 16:35:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.65 on epoch=129
06/19/2022 16:35:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.60 on epoch=134
06/19/2022 16:35:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.61 on epoch=139
06/19/2022 16:36:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.57 on epoch=144
06/19/2022 16:36:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.66 on epoch=149
06/19/2022 16:36:02 - INFO - __main__ - Global step 300 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 16:36:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.59 on epoch=154
06/19/2022 16:36:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=159
06/19/2022 16:36:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=164
06/19/2022 16:36:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.64 on epoch=169
06/19/2022 16:36:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=174
06/19/2022 16:36:09 - INFO - __main__ - Global step 350 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 16:36:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=179
06/19/2022 16:36:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=184
06/19/2022 16:36:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.54 on epoch=189
06/19/2022 16:36:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.60 on epoch=194
06/19/2022 16:36:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=199
06/19/2022 16:36:15 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 16:36:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.54 on epoch=204
06/19/2022 16:36:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=209
06/19/2022 16:36:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=214
06/19/2022 16:36:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.57 on epoch=219
06/19/2022 16:36:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.56 on epoch=224
06/19/2022 16:36:23 - INFO - __main__ - Global step 450 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 16:36:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=229
06/19/2022 16:36:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=234
06/19/2022 16:36:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.52 on epoch=239
06/19/2022 16:36:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=244
06/19/2022 16:36:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=249
06/19/2022 16:36:30 - INFO - __main__ - Global step 500 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 16:36:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=254
06/19/2022 16:36:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=259
06/19/2022 16:36:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=264
06/19/2022 16:36:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.52 on epoch=269
06/19/2022 16:36:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=274
06/19/2022 16:36:37 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 16:36:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=279
06/19/2022 16:36:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=284
06/19/2022 16:36:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=289
06/19/2022 16:36:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.53 on epoch=294
06/19/2022 16:36:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=299
06/19/2022 16:36:45 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 16:36:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=304
06/19/2022 16:36:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=309
06/19/2022 16:36:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=314
06/19/2022 16:36:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=319
06/19/2022 16:36:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=324
06/19/2022 16:36:52 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 16:36:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=329
06/19/2022 16:36:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=334
06/19/2022 16:36:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=339
06/19/2022 16:36:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=344
06/19/2022 16:37:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=349
06/19/2022 16:37:00 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.3191489361702127 on epoch=349
06/19/2022 16:37:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.51 on epoch=354
06/19/2022 16:37:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=359
06/19/2022 16:37:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=364
06/19/2022 16:37:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=369
06/19/2022 16:37:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=374
06/19/2022 16:37:08 - INFO - __main__ - Global step 750 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 16:37:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=379
06/19/2022 16:37:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=384
06/19/2022 16:37:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=389
06/19/2022 16:37:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=394
06/19/2022 16:37:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=399
06/19/2022 16:37:16 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.3816425120772947 on epoch=399
06/19/2022 16:37:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3816425120772947 on epoch=399, global_step=800
06/19/2022 16:37:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=404
06/19/2022 16:37:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=409
06/19/2022 16:37:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=414
06/19/2022 16:37:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=419
06/19/2022 16:37:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=424
06/19/2022 16:37:23 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 16:37:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=429
06/19/2022 16:37:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=434
06/19/2022 16:37:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=439
06/19/2022 16:37:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=444
06/19/2022 16:37:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=449
06/19/2022 16:37:30 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 16:37:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=454
06/19/2022 16:37:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.44 on epoch=459
06/19/2022 16:37:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=464
06/19/2022 16:37:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=469
06/19/2022 16:37:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.49 on epoch=474
06/19/2022 16:37:37 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 16:37:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=479
06/19/2022 16:37:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=484
06/19/2022 16:37:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=489
06/19/2022 16:37:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=494
06/19/2022 16:37:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.42 on epoch=499
06/19/2022 16:37:45 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 16:37:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=504
06/19/2022 16:37:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.58 on epoch=509
06/19/2022 16:37:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=514
06/19/2022 16:37:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=519
06/19/2022 16:37:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=524
06/19/2022 16:37:53 - INFO - __main__ - Global step 1050 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 16:37:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=529
06/19/2022 16:37:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.49 on epoch=534
06/19/2022 16:37:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=539
06/19/2022 16:37:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=544
06/19/2022 16:38:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=549
06/19/2022 16:38:01 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 16:38:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=554
06/19/2022 16:38:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=559
06/19/2022 16:38:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=564
06/19/2022 16:38:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=569
06/19/2022 16:38:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=574
06/19/2022 16:38:08 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 16:38:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=579
06/19/2022 16:38:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=584
06/19/2022 16:38:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.40 on epoch=589
06/19/2022 16:38:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=594
06/19/2022 16:38:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=599
06/19/2022 16:38:16 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 16:38:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=604
06/19/2022 16:38:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=609
06/19/2022 16:38:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=614
06/19/2022 16:38:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=619
06/19/2022 16:38:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=624
06/19/2022 16:38:23 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 16:38:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=629
06/19/2022 16:38:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=634
06/19/2022 16:38:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=639
06/19/2022 16:38:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=644
06/19/2022 16:38:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=649
06/19/2022 16:38:30 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 16:38:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=654
06/19/2022 16:38:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=659
06/19/2022 16:38:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=664
06/19/2022 16:38:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=669
06/19/2022 16:38:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=674
06/19/2022 16:38:38 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 16:38:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=679
06/19/2022 16:38:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=684
06/19/2022 16:38:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=689
06/19/2022 16:38:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=694
06/19/2022 16:38:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=699
06/19/2022 16:38:46 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 16:38:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=704
06/19/2022 16:38:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=709
06/19/2022 16:38:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=714
06/19/2022 16:38:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=719
06/19/2022 16:38:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=724
06/19/2022 16:38:53 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.3992490613266583 on epoch=724
06/19/2022 16:38:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.3992490613266583 on epoch=724, global_step=1450
06/19/2022 16:38:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=729
06/19/2022 16:38:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=734
06/19/2022 16:38:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=739
06/19/2022 16:38:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=744
06/19/2022 16:39:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=749
06/19/2022 16:39:01 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 16:39:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=754
06/19/2022 16:39:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=759
06/19/2022 16:39:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=764
06/19/2022 16:39:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.41 on epoch=769
06/19/2022 16:39:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=774
06/19/2022 16:39:08 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 16:39:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=779
06/19/2022 16:39:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
06/19/2022 16:39:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=789
06/19/2022 16:39:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=794
06/19/2022 16:39:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=799
06/19/2022 16:39:16 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 16:39:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=804
06/19/2022 16:39:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=809
06/19/2022 16:39:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=814
06/19/2022 16:39:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.48 on epoch=819
06/19/2022 16:39:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=824
06/19/2022 16:39:24 - INFO - __main__ - Global step 1650 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 16:39:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=829
06/19/2022 16:39:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=834
06/19/2022 16:39:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=839
06/19/2022 16:39:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=844
06/19/2022 16:39:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=849
06/19/2022 16:39:31 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 16:39:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=854
06/19/2022 16:39:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.36 on epoch=859
06/19/2022 16:39:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=864
06/19/2022 16:39:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.36 on epoch=869
06/19/2022 16:39:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
06/19/2022 16:39:39 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 16:39:41 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=879
06/19/2022 16:39:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=884
06/19/2022 16:39:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=889
06/19/2022 16:39:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
06/19/2022 16:39:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=899
06/19/2022 16:39:47 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 16:39:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=904
06/19/2022 16:39:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=909
06/19/2022 16:39:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=914
06/19/2022 16:39:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=919
06/19/2022 16:39:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=924
06/19/2022 16:39:54 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 16:39:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=929
06/19/2022 16:39:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
06/19/2022 16:39:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=939
06/19/2022 16:40:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=944
06/19/2022 16:40:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.38 on epoch=949
06/19/2022 16:40:02 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 16:40:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=954
06/19/2022 16:40:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=959
06/19/2022 16:40:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.40 on epoch=964
06/19/2022 16:40:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=969
06/19/2022 16:40:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=974
06/19/2022 16:40:09 - INFO - __main__ - Global step 1950 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 16:40:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=979
06/19/2022 16:40:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=984
06/19/2022 16:40:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=989
06/19/2022 16:40:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.39 on epoch=994
06/19/2022 16:40:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=999
06/19/2022 16:40:17 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 16:40:17 - INFO - __main__ - save last model!
06/19/2022 16:40:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 16:40:17 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 16:40:17 - INFO - __main__ - Printing 3 examples
06/19/2022 16:40:17 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 16:40:17 - INFO - __main__ - ['0']
06/19/2022 16:40:17 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 16:40:17 - INFO - __main__ - ['1']
06/19/2022 16:40:17 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 16:40:17 - INFO - __main__ - ['1']
06/19/2022 16:40:17 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:40:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:40:17 - INFO - __main__ - Printing 3 examples
06/19/2022 16:40:17 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 16:40:17 - INFO - __main__ - ['1']
06/19/2022 16:40:17 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 16:40:17 - INFO - __main__ - ['1']
06/19/2022 16:40:17 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 16:40:17 - INFO - __main__ - ['1']
06/19/2022 16:40:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 16:40:17 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:40:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:40:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:40:17 - INFO - __main__ - Printing 3 examples
06/19/2022 16:40:17 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 16:40:17 - INFO - __main__ - ['1']
06/19/2022 16:40:17 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 16:40:17 - INFO - __main__ - ['1']
06/19/2022 16:40:17 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 16:40:17 - INFO - __main__ - ['1']
06/19/2022 16:40:17 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:40:17 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:40:17 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:40:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:40:23 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:40:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:40:24 - INFO - __main__ - Starting training!
06/19/2022 16:40:30 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 16:41:57 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_21_0.5_8_predictions.txt
06/19/2022 16:41:57 - INFO - __main__ - Classification-F1 on test data: 0.3069
06/19/2022 16:41:57 - INFO - __main__ - prefix=paws_16_21, lr=0.5, bsz=8, dev_performance=0.3992490613266583, test_performance=0.30694971710417795
06/19/2022 16:41:57 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.4, bsz=8 ...
06/19/2022 16:41:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:41:58 - INFO - __main__ - Printing 3 examples
06/19/2022 16:41:58 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 16:41:58 - INFO - __main__ - ['1']
06/19/2022 16:41:58 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 16:41:58 - INFO - __main__ - ['1']
06/19/2022 16:41:58 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 16:41:58 - INFO - __main__ - ['1']
06/19/2022 16:41:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:41:58 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:41:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:41:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:41:58 - INFO - __main__ - Printing 3 examples
06/19/2022 16:41:58 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 16:41:58 - INFO - __main__ - ['1']
06/19/2022 16:41:58 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 16:41:58 - INFO - __main__ - ['1']
06/19/2022 16:41:58 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 16:41:58 - INFO - __main__ - ['1']
06/19/2022 16:41:58 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:41:58 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:41:58 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:42:04 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:42:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:42:04 - INFO - __main__ - Starting training!
06/19/2022 16:42:06 - INFO - __main__ - Step 10 Global step 10 Train loss 4.05 on epoch=4
06/19/2022 16:42:07 - INFO - __main__ - Step 20 Global step 20 Train loss 3.75 on epoch=9
06/19/2022 16:42:08 - INFO - __main__ - Step 30 Global step 30 Train loss 3.06 on epoch=14
06/19/2022 16:42:10 - INFO - __main__ - Step 40 Global step 40 Train loss 2.56 on epoch=19
06/19/2022 16:42:11 - INFO - __main__ - Step 50 Global step 50 Train loss 2.12 on epoch=24
06/19/2022 16:42:11 - INFO - __main__ - Global step 50 Train loss 3.11 Classification-F1 0.3333333333333333 on epoch=24
06/19/2022 16:42:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/19/2022 16:42:13 - INFO - __main__ - Step 60 Global step 60 Train loss 1.93 on epoch=29
06/19/2022 16:42:14 - INFO - __main__ - Step 70 Global step 70 Train loss 1.66 on epoch=34
06/19/2022 16:42:15 - INFO - __main__ - Step 80 Global step 80 Train loss 1.55 on epoch=39
06/19/2022 16:42:17 - INFO - __main__ - Step 90 Global step 90 Train loss 1.37 on epoch=44
06/19/2022 16:42:18 - INFO - __main__ - Step 100 Global step 100 Train loss 1.36 on epoch=49
06/19/2022 16:42:18 - INFO - __main__ - Global step 100 Train loss 1.57 Classification-F1 0.3992490613266583 on epoch=49
06/19/2022 16:42:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=49, global_step=100
06/19/2022 16:42:20 - INFO - __main__ - Step 110 Global step 110 Train loss 1.22 on epoch=54
06/19/2022 16:42:21 - INFO - __main__ - Step 120 Global step 120 Train loss 1.18 on epoch=59
06/19/2022 16:42:22 - INFO - __main__ - Step 130 Global step 130 Train loss 1.11 on epoch=64
06/19/2022 16:42:23 - INFO - __main__ - Step 140 Global step 140 Train loss 1.10 on epoch=69
06/19/2022 16:42:25 - INFO - __main__ - Step 150 Global step 150 Train loss 1.02 on epoch=74
06/19/2022 16:42:25 - INFO - __main__ - Global step 150 Train loss 1.13 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 16:42:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.94 on epoch=79
06/19/2022 16:42:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=84
06/19/2022 16:42:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.87 on epoch=89
06/19/2022 16:42:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.86 on epoch=94
06/19/2022 16:42:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.86 on epoch=99
06/19/2022 16:42:32 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 16:42:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.86 on epoch=104
06/19/2022 16:42:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.86 on epoch=109
06/19/2022 16:42:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.77 on epoch=114
06/19/2022 16:42:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.81 on epoch=119
06/19/2022 16:42:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.74 on epoch=124
06/19/2022 16:42:39 - INFO - __main__ - Global step 250 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 16:42:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.72 on epoch=129
06/19/2022 16:42:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.75 on epoch=134
06/19/2022 16:42:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.78 on epoch=139
06/19/2022 16:42:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.71 on epoch=144
06/19/2022 16:42:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.74 on epoch=149
06/19/2022 16:42:46 - INFO - __main__ - Global step 300 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 16:42:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.73 on epoch=154
06/19/2022 16:42:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.66 on epoch=159
06/19/2022 16:42:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.65 on epoch=164
06/19/2022 16:42:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.67 on epoch=169
06/19/2022 16:42:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.64 on epoch=174
06/19/2022 16:42:53 - INFO - __main__ - Global step 350 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 16:42:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.64 on epoch=179
06/19/2022 16:42:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.68 on epoch=184
06/19/2022 16:42:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.56 on epoch=189
06/19/2022 16:42:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.69 on epoch=194
06/19/2022 16:43:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.61 on epoch=199
06/19/2022 16:43:00 - INFO - __main__ - Global step 400 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 16:43:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=204
06/19/2022 16:43:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.58 on epoch=209
06/19/2022 16:43:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.53 on epoch=214
06/19/2022 16:43:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.57 on epoch=219
06/19/2022 16:43:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.55 on epoch=224
06/19/2022 16:43:08 - INFO - __main__ - Global step 450 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 16:43:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.67 on epoch=229
06/19/2022 16:43:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=234
06/19/2022 16:43:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.63 on epoch=239
06/19/2022 16:43:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.59 on epoch=244
06/19/2022 16:43:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.62 on epoch=249
06/19/2022 16:43:15 - INFO - __main__ - Global step 500 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 16:43:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.53 on epoch=254
06/19/2022 16:43:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.58 on epoch=259
06/19/2022 16:43:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.51 on epoch=264
06/19/2022 16:43:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.55 on epoch=269
06/19/2022 16:43:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=274
06/19/2022 16:43:22 - INFO - __main__ - Global step 550 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 16:43:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=279
06/19/2022 16:43:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=284
06/19/2022 16:43:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=289
06/19/2022 16:43:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=294
06/19/2022 16:43:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.54 on epoch=299
06/19/2022 16:43:29 - INFO - __main__ - Global step 600 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 16:43:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.57 on epoch=304
06/19/2022 16:43:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=309
06/19/2022 16:43:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.53 on epoch=314
06/19/2022 16:43:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.59 on epoch=319
06/19/2022 16:43:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.51 on epoch=324
06/19/2022 16:43:37 - INFO - __main__ - Global step 650 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 16:43:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.55 on epoch=329
06/19/2022 16:43:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=334
06/19/2022 16:43:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=339
06/19/2022 16:43:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.59 on epoch=344
06/19/2022 16:43:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=349
06/19/2022 16:43:44 - INFO - __main__ - Global step 700 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 16:43:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.54 on epoch=354
06/19/2022 16:43:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.55 on epoch=359
06/19/2022 16:43:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=364
06/19/2022 16:43:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.57 on epoch=369
06/19/2022 16:43:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=374
06/19/2022 16:43:51 - INFO - __main__ - Global step 750 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 16:43:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.56 on epoch=379
06/19/2022 16:43:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.55 on epoch=384
06/19/2022 16:43:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.54 on epoch=389
06/19/2022 16:43:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.53 on epoch=394
06/19/2022 16:43:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=399
06/19/2022 16:43:58 - INFO - __main__ - Global step 800 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 16:44:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=404
06/19/2022 16:44:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=409
06/19/2022 16:44:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.54 on epoch=414
06/19/2022 16:44:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=419
06/19/2022 16:44:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=424
06/19/2022 16:44:05 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 16:44:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=429
06/19/2022 16:44:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.57 on epoch=434
06/19/2022 16:44:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.55 on epoch=439
06/19/2022 16:44:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=444
06/19/2022 16:44:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.51 on epoch=449
06/19/2022 16:44:12 - INFO - __main__ - Global step 900 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 16:44:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.48 on epoch=454
06/19/2022 16:44:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=459
06/19/2022 16:44:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.51 on epoch=464
06/19/2022 16:44:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=469
06/19/2022 16:44:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.53 on epoch=474
06/19/2022 16:44:18 - INFO - __main__ - Global step 950 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 16:44:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.47 on epoch=479
06/19/2022 16:44:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.54 on epoch=484
06/19/2022 16:44:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.52 on epoch=489
06/19/2022 16:44:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.52 on epoch=494
06/19/2022 16:44:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.53 on epoch=499
06/19/2022 16:44:25 - INFO - __main__ - Global step 1000 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 16:44:27 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=504
06/19/2022 16:44:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=509
06/19/2022 16:44:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=514
06/19/2022 16:44:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=519
06/19/2022 16:44:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.51 on epoch=524
06/19/2022 16:44:33 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 16:44:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=529
06/19/2022 16:44:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.50 on epoch=534
06/19/2022 16:44:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.49 on epoch=539
06/19/2022 16:44:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=544
06/19/2022 16:44:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.52 on epoch=549
06/19/2022 16:44:40 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 16:44:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.55 on epoch=554
06/19/2022 16:44:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=559
06/19/2022 16:44:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=564
06/19/2022 16:44:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=569
06/19/2022 16:44:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=574
06/19/2022 16:44:46 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 16:44:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=579
06/19/2022 16:44:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.51 on epoch=584
06/19/2022 16:44:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=589
06/19/2022 16:44:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=594
06/19/2022 16:44:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=599
06/19/2022 16:44:53 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 16:44:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=604
06/19/2022 16:44:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=609
06/19/2022 16:44:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=614
06/19/2022 16:44:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.48 on epoch=619
06/19/2022 16:45:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=624
06/19/2022 16:45:01 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 16:45:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=629
06/19/2022 16:45:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=634
06/19/2022 16:45:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=639
06/19/2022 16:45:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=644
06/19/2022 16:45:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=649
06/19/2022 16:45:08 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 16:45:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=654
06/19/2022 16:45:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=659
06/19/2022 16:45:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=664
06/19/2022 16:45:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=669
06/19/2022 16:45:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.47 on epoch=674
06/19/2022 16:45:15 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 16:45:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=679
06/19/2022 16:45:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.43 on epoch=684
06/19/2022 16:45:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.46 on epoch=689
06/19/2022 16:45:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=694
06/19/2022 16:45:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=699
06/19/2022 16:45:23 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 16:45:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=704
06/19/2022 16:45:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.40 on epoch=709
06/19/2022 16:45:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=714
06/19/2022 16:45:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=719
06/19/2022 16:45:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=724
06/19/2022 16:45:29 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 16:45:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=729
06/19/2022 16:45:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=734
06/19/2022 16:45:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=739
06/19/2022 16:45:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=744
06/19/2022 16:45:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=749
06/19/2022 16:45:36 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 16:45:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=754
06/19/2022 16:45:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.46 on epoch=759
06/19/2022 16:45:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=764
06/19/2022 16:45:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.48 on epoch=769
06/19/2022 16:45:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=774
06/19/2022 16:45:43 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 16:45:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=779
06/19/2022 16:45:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=784
06/19/2022 16:45:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=789
06/19/2022 16:45:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.45 on epoch=794
06/19/2022 16:45:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=799
06/19/2022 16:45:50 - INFO - __main__ - Global step 1600 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 16:45:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=804
06/19/2022 16:45:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.41 on epoch=809
06/19/2022 16:45:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=814
06/19/2022 16:45:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.36 on epoch=819
06/19/2022 16:45:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=824
06/19/2022 16:45:57 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 16:45:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=829
06/19/2022 16:45:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=834
06/19/2022 16:46:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=839
06/19/2022 16:46:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=844
06/19/2022 16:46:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=849
06/19/2022 16:46:03 - INFO - __main__ - Global step 1700 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 16:46:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
06/19/2022 16:46:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=859
06/19/2022 16:46:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=864
06/19/2022 16:46:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.36 on epoch=869
06/19/2022 16:46:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=874
06/19/2022 16:46:11 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 16:46:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=879
06/19/2022 16:46:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=884
06/19/2022 16:46:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.51 on epoch=889
06/19/2022 16:46:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
06/19/2022 16:46:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=899
06/19/2022 16:46:18 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 16:46:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=904
06/19/2022 16:46:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=909
06/19/2022 16:46:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.48 on epoch=914
06/19/2022 16:46:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=919
06/19/2022 16:46:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=924
06/19/2022 16:46:25 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 16:46:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=929
06/19/2022 16:46:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=934
06/19/2022 16:46:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=939
06/19/2022 16:46:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
06/19/2022 16:46:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=949
06/19/2022 16:46:33 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 16:46:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
06/19/2022 16:46:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=959
06/19/2022 16:46:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
06/19/2022 16:46:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=969
06/19/2022 16:46:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
06/19/2022 16:46:40 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 16:46:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=979
06/19/2022 16:46:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
06/19/2022 16:46:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=989
06/19/2022 16:46:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=994
06/19/2022 16:46:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=999
06/19/2022 16:46:47 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 16:46:47 - INFO - __main__ - save last model!
06/19/2022 16:46:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 16:46:47 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 16:46:47 - INFO - __main__ - Printing 3 examples
06/19/2022 16:46:47 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 16:46:47 - INFO - __main__ - ['0']
06/19/2022 16:46:47 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 16:46:47 - INFO - __main__ - ['1']
06/19/2022 16:46:47 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 16:46:47 - INFO - __main__ - ['1']
06/19/2022 16:46:47 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:46:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:46:48 - INFO - __main__ - Printing 3 examples
06/19/2022 16:46:48 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 16:46:48 - INFO - __main__ - ['1']
06/19/2022 16:46:48 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 16:46:48 - INFO - __main__ - ['1']
06/19/2022 16:46:48 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 16:46:48 - INFO - __main__ - ['1']
06/19/2022 16:46:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 16:46:48 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:46:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:46:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:46:48 - INFO - __main__ - Printing 3 examples
06/19/2022 16:46:48 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 16:46:48 - INFO - __main__ - ['1']
06/19/2022 16:46:48 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 16:46:48 - INFO - __main__ - ['1']
06/19/2022 16:46:48 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 16:46:48 - INFO - __main__ - ['1']
06/19/2022 16:46:48 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:46:48 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:46:48 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:46:52 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:46:54 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:46:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:46:54 - INFO - __main__ - Starting training!
06/19/2022 16:47:03 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 16:48:33 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_21_0.4_8_predictions.txt
06/19/2022 16:48:33 - INFO - __main__ - Classification-F1 on test data: 0.3074
06/19/2022 16:48:33 - INFO - __main__ - prefix=paws_16_21, lr=0.4, bsz=8, dev_performance=0.3992490613266583, test_performance=0.30739057476854853
06/19/2022 16:48:33 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.3, bsz=8 ...
06/19/2022 16:48:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:48:34 - INFO - __main__ - Printing 3 examples
06/19/2022 16:48:34 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 16:48:34 - INFO - __main__ - ['1']
06/19/2022 16:48:34 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 16:48:34 - INFO - __main__ - ['1']
06/19/2022 16:48:34 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 16:48:34 - INFO - __main__ - ['1']
06/19/2022 16:48:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:48:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:48:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:48:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:48:34 - INFO - __main__ - Printing 3 examples
06/19/2022 16:48:34 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 16:48:34 - INFO - __main__ - ['1']
06/19/2022 16:48:34 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 16:48:34 - INFO - __main__ - ['1']
06/19/2022 16:48:34 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 16:48:34 - INFO - __main__ - ['1']
06/19/2022 16:48:34 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:48:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:48:34 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:48:41 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:48:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:48:41 - INFO - __main__ - Starting training!
06/19/2022 16:48:43 - INFO - __main__ - Step 10 Global step 10 Train loss 4.04 on epoch=4
06/19/2022 16:48:45 - INFO - __main__ - Step 20 Global step 20 Train loss 3.68 on epoch=9
06/19/2022 16:48:46 - INFO - __main__ - Step 30 Global step 30 Train loss 3.12 on epoch=14
06/19/2022 16:48:48 - INFO - __main__ - Step 40 Global step 40 Train loss 2.71 on epoch=19
06/19/2022 16:48:50 - INFO - __main__ - Step 50 Global step 50 Train loss 2.60 on epoch=24
06/19/2022 16:48:50 - INFO - __main__ - Global step 50 Train loss 3.23 Classification-F1 0.21739130434782608 on epoch=24
06/19/2022 16:48:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.21739130434782608 on epoch=24, global_step=50
06/19/2022 16:48:52 - INFO - __main__ - Step 60 Global step 60 Train loss 2.16 on epoch=29
06/19/2022 16:48:54 - INFO - __main__ - Step 70 Global step 70 Train loss 2.01 on epoch=34
06/19/2022 16:48:55 - INFO - __main__ - Step 80 Global step 80 Train loss 1.85 on epoch=39
06/19/2022 16:48:57 - INFO - __main__ - Step 90 Global step 90 Train loss 1.64 on epoch=44
06/19/2022 16:48:59 - INFO - __main__ - Step 100 Global step 100 Train loss 1.53 on epoch=49
06/19/2022 16:48:59 - INFO - __main__ - Global step 100 Train loss 1.84 Classification-F1 0.4385964912280702 on epoch=49
06/19/2022 16:48:59 - INFO - __main__ - Saving model with best Classification-F1: 0.21739130434782608 -> 0.4385964912280702 on epoch=49, global_step=100
06/19/2022 16:49:01 - INFO - __main__ - Step 110 Global step 110 Train loss 1.41 on epoch=54
06/19/2022 16:49:02 - INFO - __main__ - Step 120 Global step 120 Train loss 1.36 on epoch=59
06/19/2022 16:49:04 - INFO - __main__ - Step 130 Global step 130 Train loss 1.27 on epoch=64
06/19/2022 16:49:05 - INFO - __main__ - Step 140 Global step 140 Train loss 1.19 on epoch=69
06/19/2022 16:49:07 - INFO - __main__ - Step 150 Global step 150 Train loss 1.20 on epoch=74
06/19/2022 16:49:07 - INFO - __main__ - Global step 150 Train loss 1.29 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 16:49:09 - INFO - __main__ - Step 160 Global step 160 Train loss 1.19 on epoch=79
06/19/2022 16:49:10 - INFO - __main__ - Step 170 Global step 170 Train loss 1.16 on epoch=84
06/19/2022 16:49:11 - INFO - __main__ - Step 180 Global step 180 Train loss 1.08 on epoch=89
06/19/2022 16:49:13 - INFO - __main__ - Step 190 Global step 190 Train loss 1.02 on epoch=94
06/19/2022 16:49:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.99 on epoch=99
06/19/2022 16:49:15 - INFO - __main__ - Global step 200 Train loss 1.09 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 16:49:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.98 on epoch=104
06/19/2022 16:49:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.98 on epoch=109
06/19/2022 16:49:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.95 on epoch=114
06/19/2022 16:49:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.87 on epoch=119
06/19/2022 16:49:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.83 on epoch=124
06/19/2022 16:49:22 - INFO - __main__ - Global step 250 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 16:49:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.90 on epoch=129
06/19/2022 16:49:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.85 on epoch=134
06/19/2022 16:49:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.76 on epoch=139
06/19/2022 16:49:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.88 on epoch=144
06/19/2022 16:49:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.81 on epoch=149
06/19/2022 16:49:29 - INFO - __main__ - Global step 300 Train loss 0.84 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 16:49:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=154
06/19/2022 16:49:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.78 on epoch=159
06/19/2022 16:49:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.74 on epoch=164
06/19/2022 16:49:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.74 on epoch=169
06/19/2022 16:49:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.82 on epoch=174
06/19/2022 16:49:37 - INFO - __main__ - Global step 350 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 16:49:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.77 on epoch=179
06/19/2022 16:49:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.67 on epoch=184
06/19/2022 16:49:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.73 on epoch=189
06/19/2022 16:49:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.77 on epoch=194
06/19/2022 16:49:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.68 on epoch=199
06/19/2022 16:49:45 - INFO - __main__ - Global step 400 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 16:49:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.78 on epoch=204
06/19/2022 16:49:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.74 on epoch=209
06/19/2022 16:49:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.69 on epoch=214
06/19/2022 16:49:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.68 on epoch=219
06/19/2022 16:49:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.68 on epoch=224
06/19/2022 16:49:53 - INFO - __main__ - Global step 450 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 16:49:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.61 on epoch=229
06/19/2022 16:49:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.68 on epoch=234
06/19/2022 16:49:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.70 on epoch=239
06/19/2022 16:49:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=244
06/19/2022 16:50:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.63 on epoch=249
06/19/2022 16:50:02 - INFO - __main__ - Global step 500 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 16:50:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.63 on epoch=254
06/19/2022 16:50:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.59 on epoch=259
06/19/2022 16:50:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=264
06/19/2022 16:50:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.64 on epoch=269
06/19/2022 16:50:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.65 on epoch=274
06/19/2022 16:50:11 - INFO - __main__ - Global step 550 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 16:50:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.63 on epoch=279
06/19/2022 16:50:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.64 on epoch=284
06/19/2022 16:50:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.58 on epoch=289
06/19/2022 16:50:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.54 on epoch=294
06/19/2022 16:50:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.53 on epoch=299
06/19/2022 16:50:18 - INFO - __main__ - Global step 600 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 16:50:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.52 on epoch=304
06/19/2022 16:50:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=309
06/19/2022 16:50:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.53 on epoch=314
06/19/2022 16:50:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=319
06/19/2022 16:50:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.62 on epoch=324
06/19/2022 16:50:26 - INFO - __main__ - Global step 650 Train loss 0.56 Classification-F1 0.3992490613266583 on epoch=324
06/19/2022 16:50:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.56 on epoch=329
06/19/2022 16:50:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.55 on epoch=334
06/19/2022 16:50:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.57 on epoch=339
06/19/2022 16:50:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.60 on epoch=344
06/19/2022 16:50:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=349
06/19/2022 16:50:34 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 16:50:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.59 on epoch=354
06/19/2022 16:50:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.63 on epoch=359
06/19/2022 16:50:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=364
06/19/2022 16:50:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.59 on epoch=369
06/19/2022 16:50:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.58 on epoch=374
06/19/2022 16:50:42 - INFO - __main__ - Global step 750 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 16:50:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.50 on epoch=379
06/19/2022 16:50:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.60 on epoch=384
06/19/2022 16:50:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.70 on epoch=389
06/19/2022 16:50:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.54 on epoch=394
06/19/2022 16:50:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.57 on epoch=399
06/19/2022 16:50:50 - INFO - __main__ - Global step 800 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 16:50:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=404
06/19/2022 16:50:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.54 on epoch=409
06/19/2022 16:50:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.68 on epoch=414
06/19/2022 16:50:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.55 on epoch=419
06/19/2022 16:50:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.58 on epoch=424
06/19/2022 16:51:00 - INFO - __main__ - Global step 850 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 16:51:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=429
06/19/2022 16:51:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.49 on epoch=434
06/19/2022 16:51:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.50 on epoch=439
06/19/2022 16:51:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.52 on epoch=444
06/19/2022 16:51:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.60 on epoch=449
06/19/2022 16:51:08 - INFO - __main__ - Global step 900 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 16:51:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.48 on epoch=454
06/19/2022 16:51:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.53 on epoch=459
06/19/2022 16:51:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.58 on epoch=464
06/19/2022 16:51:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.49 on epoch=469
06/19/2022 16:51:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.52 on epoch=474
06/19/2022 16:51:16 - INFO - __main__ - Global step 950 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 16:51:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.57 on epoch=479
06/19/2022 16:51:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.54 on epoch=484
06/19/2022 16:51:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.53 on epoch=489
06/19/2022 16:51:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.53 on epoch=494
06/19/2022 16:51:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=499
06/19/2022 16:51:23 - INFO - __main__ - Global step 1000 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 16:51:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.47 on epoch=504
06/19/2022 16:51:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=509
06/19/2022 16:51:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=514
06/19/2022 16:51:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=519
06/19/2022 16:51:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.51 on epoch=524
06/19/2022 16:51:30 - INFO - __main__ - Global step 1050 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 16:51:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.52 on epoch=529
06/19/2022 16:51:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=534
06/19/2022 16:51:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.56 on epoch=539
06/19/2022 16:51:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.48 on epoch=544
06/19/2022 16:51:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=549
06/19/2022 16:51:38 - INFO - __main__ - Global step 1100 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 16:51:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.53 on epoch=554
06/19/2022 16:51:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=559
06/19/2022 16:51:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=564
06/19/2022 16:51:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=569
06/19/2022 16:51:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=574
06/19/2022 16:51:46 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 16:51:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=579
06/19/2022 16:51:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.51 on epoch=584
06/19/2022 16:51:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=589
06/19/2022 16:51:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=594
06/19/2022 16:51:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=599
06/19/2022 16:51:53 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 16:51:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=604
06/19/2022 16:51:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=609
06/19/2022 16:51:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=614
06/19/2022 16:51:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.50 on epoch=619
06/19/2022 16:51:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.51 on epoch=624
06/19/2022 16:52:00 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 16:52:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=629
06/19/2022 16:52:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=634
06/19/2022 16:52:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=639
06/19/2022 16:52:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.46 on epoch=644
06/19/2022 16:52:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.51 on epoch=649
06/19/2022 16:52:07 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 16:52:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=654
06/19/2022 16:52:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.49 on epoch=659
06/19/2022 16:52:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.54 on epoch=664
06/19/2022 16:52:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=669
06/19/2022 16:52:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.53 on epoch=674
06/19/2022 16:52:15 - INFO - __main__ - Global step 1350 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 16:52:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.51 on epoch=679
06/19/2022 16:52:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.50 on epoch=684
06/19/2022 16:52:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.54 on epoch=689
06/19/2022 16:52:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=694
06/19/2022 16:52:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.49 on epoch=699
06/19/2022 16:52:23 - INFO - __main__ - Global step 1400 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 16:52:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=704
06/19/2022 16:52:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=709
06/19/2022 16:52:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.47 on epoch=714
06/19/2022 16:52:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.49 on epoch=719
06/19/2022 16:52:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.45 on epoch=724
06/19/2022 16:52:30 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 16:52:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=729
06/19/2022 16:52:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.48 on epoch=734
06/19/2022 16:52:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=739
06/19/2022 16:52:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.46 on epoch=744
06/19/2022 16:52:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.53 on epoch=749
06/19/2022 16:52:37 - INFO - __main__ - Global step 1500 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 16:52:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.49 on epoch=754
06/19/2022 16:52:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=759
06/19/2022 16:52:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=764
06/19/2022 16:52:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.48 on epoch=769
06/19/2022 16:52:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=774
06/19/2022 16:52:45 - INFO - __main__ - Global step 1550 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 16:52:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.50 on epoch=779
06/19/2022 16:52:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=784
06/19/2022 16:52:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.49 on epoch=789
06/19/2022 16:52:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.45 on epoch=794
06/19/2022 16:52:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.47 on epoch=799
06/19/2022 16:52:52 - INFO - __main__ - Global step 1600 Train loss 0.47 Classification-F1 0.3992490613266583 on epoch=799
06/19/2022 16:52:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=804
06/19/2022 16:52:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=809
06/19/2022 16:52:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.47 on epoch=814
06/19/2022 16:52:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
06/19/2022 16:52:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.48 on epoch=824
06/19/2022 16:53:00 - INFO - __main__ - Global step 1650 Train loss 0.46 Classification-F1 0.3992490613266583 on epoch=824
06/19/2022 16:53:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=829
06/19/2022 16:53:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.49 on epoch=834
06/19/2022 16:53:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=839
06/19/2022 16:53:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=844
06/19/2022 16:53:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=849
06/19/2022 16:53:08 - INFO - __main__ - Global step 1700 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 16:53:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.43 on epoch=854
06/19/2022 16:53:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=859
06/19/2022 16:53:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=864
06/19/2022 16:53:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.36 on epoch=869
06/19/2022 16:53:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=874
06/19/2022 16:53:15 - INFO - __main__ - Global step 1750 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 16:53:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=879
06/19/2022 16:53:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=884
06/19/2022 16:53:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=889
06/19/2022 16:53:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.45 on epoch=894
06/19/2022 16:53:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.51 on epoch=899
06/19/2022 16:53:23 - INFO - __main__ - Global step 1800 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 16:53:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=904
06/19/2022 16:53:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=909
06/19/2022 16:53:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=914
06/19/2022 16:53:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.47 on epoch=919
06/19/2022 16:53:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=924
06/19/2022 16:53:31 - INFO - __main__ - Global step 1850 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 16:53:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.44 on epoch=929
06/19/2022 16:53:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=934
06/19/2022 16:53:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.44 on epoch=939
06/19/2022 16:53:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.44 on epoch=944
06/19/2022 16:53:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.48 on epoch=949
06/19/2022 16:53:38 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 16:53:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.49 on epoch=954
06/19/2022 16:53:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.49 on epoch=959
06/19/2022 16:53:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
06/19/2022 16:53:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.46 on epoch=969
06/19/2022 16:53:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=974
06/19/2022 16:53:46 - INFO - __main__ - Global step 1950 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 16:53:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=979
06/19/2022 16:53:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
06/19/2022 16:53:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.47 on epoch=989
06/19/2022 16:53:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=994
06/19/2022 16:53:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=999
06/19/2022 16:53:53 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 16:53:53 - INFO - __main__ - save last model!
06/19/2022 16:53:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 16:53:54 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 16:53:54 - INFO - __main__ - Printing 3 examples
06/19/2022 16:53:54 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 16:53:54 - INFO - __main__ - ['0']
06/19/2022 16:53:54 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 16:53:54 - INFO - __main__ - ['1']
06/19/2022 16:53:54 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 16:53:54 - INFO - __main__ - ['1']
06/19/2022 16:53:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:53:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:53:54 - INFO - __main__ - Printing 3 examples
06/19/2022 16:53:54 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 16:53:54 - INFO - __main__ - ['1']
06/19/2022 16:53:54 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 16:53:54 - INFO - __main__ - ['1']
06/19/2022 16:53:54 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 16:53:54 - INFO - __main__ - ['1']
06/19/2022 16:53:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 16:53:54 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:53:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:53:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:53:54 - INFO - __main__ - Printing 3 examples
06/19/2022 16:53:54 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 16:53:54 - INFO - __main__ - ['1']
06/19/2022 16:53:54 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 16:53:54 - INFO - __main__ - ['1']
06/19/2022 16:53:54 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 16:53:54 - INFO - __main__ - ['1']
06/19/2022 16:53:54 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:53:54 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:53:54 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:53:58 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:54:01 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:54:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:54:01 - INFO - __main__ - Starting training!
06/19/2022 16:54:06 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 16:55:30 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_21_0.3_8_predictions.txt
06/19/2022 16:55:30 - INFO - __main__ - Classification-F1 on test data: 0.3067
06/19/2022 16:55:30 - INFO - __main__ - prefix=paws_16_21, lr=0.3, bsz=8, dev_performance=0.4385964912280702, test_performance=0.30669902071236677
06/19/2022 16:55:30 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.2, bsz=8 ...
06/19/2022 16:55:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:55:31 - INFO - __main__ - Printing 3 examples
06/19/2022 16:55:31 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 16:55:31 - INFO - __main__ - ['1']
06/19/2022 16:55:31 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 16:55:31 - INFO - __main__ - ['1']
06/19/2022 16:55:31 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 16:55:31 - INFO - __main__ - ['1']
06/19/2022 16:55:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 16:55:31 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:55:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 16:55:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 16:55:31 - INFO - __main__ - Printing 3 examples
06/19/2022 16:55:31 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 16:55:31 - INFO - __main__ - ['1']
06/19/2022 16:55:31 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 16:55:31 - INFO - __main__ - ['1']
06/19/2022 16:55:31 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 16:55:31 - INFO - __main__ - ['1']
06/19/2022 16:55:31 - INFO - __main__ - Tokenizing Input ...
06/19/2022 16:55:31 - INFO - __main__ - Tokenizing Output ...
06/19/2022 16:55:31 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 16:55:37 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 16:55:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 16:55:38 - INFO - __main__ - Starting training!
06/19/2022 16:55:39 - INFO - __main__ - Step 10 Global step 10 Train loss 4.09 on epoch=4
06/19/2022 16:55:41 - INFO - __main__ - Step 20 Global step 20 Train loss 4.01 on epoch=9
06/19/2022 16:55:42 - INFO - __main__ - Step 30 Global step 30 Train loss 3.65 on epoch=14
06/19/2022 16:55:44 - INFO - __main__ - Step 40 Global step 40 Train loss 3.38 on epoch=19
06/19/2022 16:55:45 - INFO - __main__ - Step 50 Global step 50 Train loss 3.17 on epoch=24
06/19/2022 16:55:46 - INFO - __main__ - Global step 50 Train loss 3.66 Classification-F1 0.0 on epoch=24
06/19/2022 16:55:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 16:55:47 - INFO - __main__ - Step 60 Global step 60 Train loss 2.80 on epoch=29
06/19/2022 16:55:48 - INFO - __main__ - Step 70 Global step 70 Train loss 2.37 on epoch=34
06/19/2022 16:55:50 - INFO - __main__ - Step 80 Global step 80 Train loss 2.26 on epoch=39
06/19/2022 16:55:51 - INFO - __main__ - Step 90 Global step 90 Train loss 1.96 on epoch=44
06/19/2022 16:55:52 - INFO - __main__ - Step 100 Global step 100 Train loss 1.91 on epoch=49
06/19/2022 16:55:53 - INFO - __main__ - Global step 100 Train loss 2.26 Classification-F1 0.3333333333333333 on epoch=49
06/19/2022 16:55:53 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=49, global_step=100
06/19/2022 16:55:54 - INFO - __main__ - Step 110 Global step 110 Train loss 1.65 on epoch=54
06/19/2022 16:55:55 - INFO - __main__ - Step 120 Global step 120 Train loss 1.63 on epoch=59
06/19/2022 16:55:57 - INFO - __main__ - Step 130 Global step 130 Train loss 1.59 on epoch=64
06/19/2022 16:55:58 - INFO - __main__ - Step 140 Global step 140 Train loss 1.46 on epoch=69
06/19/2022 16:55:59 - INFO - __main__ - Step 150 Global step 150 Train loss 1.37 on epoch=74
06/19/2022 16:56:00 - INFO - __main__ - Global step 150 Train loss 1.54 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 16:56:01 - INFO - __main__ - Step 160 Global step 160 Train loss 1.34 on epoch=79
06/19/2022 16:56:02 - INFO - __main__ - Step 170 Global step 170 Train loss 1.35 on epoch=84
06/19/2022 16:56:03 - INFO - __main__ - Step 180 Global step 180 Train loss 1.36 on epoch=89
06/19/2022 16:56:05 - INFO - __main__ - Step 190 Global step 190 Train loss 1.31 on epoch=94
06/19/2022 16:56:06 - INFO - __main__ - Step 200 Global step 200 Train loss 1.25 on epoch=99
06/19/2022 16:56:06 - INFO - __main__ - Global step 200 Train loss 1.32 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 16:56:08 - INFO - __main__ - Step 210 Global step 210 Train loss 1.19 on epoch=104
06/19/2022 16:56:10 - INFO - __main__ - Step 220 Global step 220 Train loss 1.20 on epoch=109
06/19/2022 16:56:11 - INFO - __main__ - Step 230 Global step 230 Train loss 1.21 on epoch=114
06/19/2022 16:56:12 - INFO - __main__ - Step 240 Global step 240 Train loss 1.08 on epoch=119
06/19/2022 16:56:14 - INFO - __main__ - Step 250 Global step 250 Train loss 1.06 on epoch=124
06/19/2022 16:56:14 - INFO - __main__ - Global step 250 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 16:56:15 - INFO - __main__ - Step 260 Global step 260 Train loss 1.12 on epoch=129
06/19/2022 16:56:16 - INFO - __main__ - Step 270 Global step 270 Train loss 1.11 on epoch=134
06/19/2022 16:56:18 - INFO - __main__ - Step 280 Global step 280 Train loss 1.06 on epoch=139
06/19/2022 16:56:19 - INFO - __main__ - Step 290 Global step 290 Train loss 1.12 on epoch=144
06/19/2022 16:56:20 - INFO - __main__ - Step 300 Global step 300 Train loss 1.02 on epoch=149
06/19/2022 16:56:21 - INFO - __main__ - Global step 300 Train loss 1.09 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 16:56:22 - INFO - __main__ - Step 310 Global step 310 Train loss 1.07 on epoch=154
06/19/2022 16:56:23 - INFO - __main__ - Step 320 Global step 320 Train loss 1.01 on epoch=159
06/19/2022 16:56:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.98 on epoch=164
06/19/2022 16:56:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.87 on epoch=169
06/19/2022 16:56:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.87 on epoch=174
06/19/2022 16:56:28 - INFO - __main__ - Global step 350 Train loss 0.96 Classification-F1 0.3992490613266583 on epoch=174
06/19/2022 16:56:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=174, global_step=350
06/19/2022 16:56:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.96 on epoch=179
06/19/2022 16:56:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.87 on epoch=184
06/19/2022 16:56:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.88 on epoch=189
06/19/2022 16:56:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=194
06/19/2022 16:56:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.88 on epoch=199
06/19/2022 16:56:35 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.3992490613266583 on epoch=199
06/19/2022 16:56:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.90 on epoch=204
06/19/2022 16:56:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.88 on epoch=209
06/19/2022 16:56:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.86 on epoch=214
06/19/2022 16:56:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.79 on epoch=219
06/19/2022 16:56:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.88 on epoch=224
06/19/2022 16:56:42 - INFO - __main__ - Global step 450 Train loss 0.86 Classification-F1 0.3992490613266583 on epoch=224
06/19/2022 16:56:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.87 on epoch=229
06/19/2022 16:56:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.73 on epoch=234
06/19/2022 16:56:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.71 on epoch=239
06/19/2022 16:56:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.77 on epoch=244
06/19/2022 16:56:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.73 on epoch=249
06/19/2022 16:56:49 - INFO - __main__ - Global step 500 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 16:56:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.77 on epoch=254
06/19/2022 16:56:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.69 on epoch=259
06/19/2022 16:56:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.68 on epoch=264
06/19/2022 16:56:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.57 on epoch=269
06/19/2022 16:56:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.76 on epoch=274
06/19/2022 16:56:56 - INFO - __main__ - Global step 550 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 16:56:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.81 on epoch=279
06/19/2022 16:56:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.75 on epoch=284
06/19/2022 16:57:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.72 on epoch=289
06/19/2022 16:57:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.73 on epoch=294
06/19/2022 16:57:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.74 on epoch=299
06/19/2022 16:57:04 - INFO - __main__ - Global step 600 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 16:57:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.70 on epoch=304
06/19/2022 16:57:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.74 on epoch=309
06/19/2022 16:57:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.67 on epoch=314
06/19/2022 16:57:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.67 on epoch=319
06/19/2022 16:57:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.67 on epoch=324
06/19/2022 16:57:11 - INFO - __main__ - Global step 650 Train loss 0.69 Classification-F1 0.3992490613266583 on epoch=324
06/19/2022 16:57:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.66 on epoch=329
06/19/2022 16:57:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.60 on epoch=334
06/19/2022 16:57:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.60 on epoch=339
06/19/2022 16:57:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.62 on epoch=344
06/19/2022 16:57:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.64 on epoch=349
06/19/2022 16:57:19 - INFO - __main__ - Global step 700 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 16:57:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.64 on epoch=354
06/19/2022 16:57:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.61 on epoch=359
06/19/2022 16:57:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.66 on epoch=364
06/19/2022 16:57:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.55 on epoch=369
06/19/2022 16:57:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.57 on epoch=374
06/19/2022 16:57:26 - INFO - __main__ - Global step 750 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 16:57:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.57 on epoch=379
06/19/2022 16:57:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.58 on epoch=384
06/19/2022 16:57:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.62 on epoch=389
06/19/2022 16:57:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.56 on epoch=394
06/19/2022 16:57:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.59 on epoch=399
06/19/2022 16:57:33 - INFO - __main__ - Global step 800 Train loss 0.58 Classification-F1 0.3992490613266583 on epoch=399
06/19/2022 16:57:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.62 on epoch=404
06/19/2022 16:57:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.57 on epoch=409
06/19/2022 16:57:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.59 on epoch=414
06/19/2022 16:57:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.64 on epoch=419
06/19/2022 16:57:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.54 on epoch=424
06/19/2022 16:57:40 - INFO - __main__ - Global step 850 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 16:57:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.61 on epoch=429
06/19/2022 16:57:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.61 on epoch=434
06/19/2022 16:57:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.61 on epoch=439
06/19/2022 16:57:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.54 on epoch=444
06/19/2022 16:57:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.58 on epoch=449
06/19/2022 16:57:47 - INFO - __main__ - Global step 900 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 16:57:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.59 on epoch=454
06/19/2022 16:57:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=459
06/19/2022 16:57:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.59 on epoch=464
06/19/2022 16:57:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.60 on epoch=469
06/19/2022 16:57:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.58 on epoch=474
06/19/2022 16:57:54 - INFO - __main__ - Global step 950 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 16:57:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.59 on epoch=479
06/19/2022 16:57:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.57 on epoch=484
06/19/2022 16:57:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.58 on epoch=489
06/19/2022 16:57:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.55 on epoch=494
06/19/2022 16:58:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.64 on epoch=499
06/19/2022 16:58:01 - INFO - __main__ - Global step 1000 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 16:58:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.53 on epoch=504
06/19/2022 16:58:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.51 on epoch=509
06/19/2022 16:58:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.55 on epoch=514
06/19/2022 16:58:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.61 on epoch=519
06/19/2022 16:58:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.60 on epoch=524
06/19/2022 16:58:08 - INFO - __main__ - Global step 1050 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 16:58:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.55 on epoch=529
06/19/2022 16:58:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.54 on epoch=534
06/19/2022 16:58:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.54 on epoch=539
06/19/2022 16:58:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.53 on epoch=544
06/19/2022 16:58:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.50 on epoch=549
06/19/2022 16:58:15 - INFO - __main__ - Global step 1100 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 16:58:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.51 on epoch=554
06/19/2022 16:58:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.58 on epoch=559
06/19/2022 16:58:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.56 on epoch=564
06/19/2022 16:58:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=569
06/19/2022 16:58:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.54 on epoch=574
06/19/2022 16:58:22 - INFO - __main__ - Global step 1150 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 16:58:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.54 on epoch=579
06/19/2022 16:58:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.60 on epoch=584
06/19/2022 16:58:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.57 on epoch=589
06/19/2022 16:58:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.54 on epoch=594
06/19/2022 16:58:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=599
06/19/2022 16:58:29 - INFO - __main__ - Global step 1200 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 16:58:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.52 on epoch=604
06/19/2022 16:58:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.47 on epoch=609
06/19/2022 16:58:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=614
06/19/2022 16:58:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.54 on epoch=619
06/19/2022 16:58:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.56 on epoch=624
06/19/2022 16:58:36 - INFO - __main__ - Global step 1250 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 16:58:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=629
06/19/2022 16:58:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=634
06/19/2022 16:58:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.51 on epoch=639
06/19/2022 16:58:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.50 on epoch=644
06/19/2022 16:58:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.56 on epoch=649
06/19/2022 16:58:43 - INFO - __main__ - Global step 1300 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 16:58:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.59 on epoch=654
06/19/2022 16:58:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=659
06/19/2022 16:58:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=664
06/19/2022 16:58:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=669
06/19/2022 16:58:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.50 on epoch=674
06/19/2022 16:58:50 - INFO - __main__ - Global step 1350 Train loss 0.49 Classification-F1 0.3992490613266583 on epoch=674
06/19/2022 16:58:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.49 on epoch=679
06/19/2022 16:58:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.49 on epoch=684
06/19/2022 16:58:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.56 on epoch=689
06/19/2022 16:58:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.52 on epoch=694
06/19/2022 16:58:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.55 on epoch=699
06/19/2022 16:58:57 - INFO - __main__ - Global step 1400 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 16:58:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.53 on epoch=704
06/19/2022 16:59:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.50 on epoch=709
06/19/2022 16:59:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.49 on epoch=714
06/19/2022 16:59:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=719
06/19/2022 16:59:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.47 on epoch=724
06/19/2022 16:59:04 - INFO - __main__ - Global step 1450 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 16:59:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=729
06/19/2022 16:59:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=734
06/19/2022 16:59:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.47 on epoch=739
06/19/2022 16:59:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.55 on epoch=744
06/19/2022 16:59:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.48 on epoch=749
06/19/2022 16:59:11 - INFO - __main__ - Global step 1500 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 16:59:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.50 on epoch=754
06/19/2022 16:59:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.53 on epoch=759
06/19/2022 16:59:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.56 on epoch=764
06/19/2022 16:59:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=769
06/19/2022 16:59:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.49 on epoch=774
06/19/2022 16:59:17 - INFO - __main__ - Global step 1550 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 16:59:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=779
06/19/2022 16:59:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=784
06/19/2022 16:59:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=789
06/19/2022 16:59:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.53 on epoch=794
06/19/2022 16:59:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=799
06/19/2022 16:59:25 - INFO - __main__ - Global step 1600 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 16:59:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.44 on epoch=804
06/19/2022 16:59:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.53 on epoch=809
06/19/2022 16:59:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.51 on epoch=814
06/19/2022 16:59:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=819
06/19/2022 16:59:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.48 on epoch=824
06/19/2022 16:59:32 - INFO - __main__ - Global step 1650 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 16:59:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.52 on epoch=829
06/19/2022 16:59:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.52 on epoch=834
06/19/2022 16:59:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=839
06/19/2022 16:59:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.56 on epoch=844
06/19/2022 16:59:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=849
06/19/2022 16:59:39 - INFO - __main__ - Global step 1700 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 16:59:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=854
06/19/2022 16:59:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=859
06/19/2022 16:59:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.47 on epoch=864
06/19/2022 16:59:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.50 on epoch=869
06/19/2022 16:59:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.47 on epoch=874
06/19/2022 16:59:46 - INFO - __main__ - Global step 1750 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 16:59:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.48 on epoch=879
06/19/2022 16:59:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.51 on epoch=884
06/19/2022 16:59:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=889
06/19/2022 16:59:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.49 on epoch=894
06/19/2022 16:59:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=899
06/19/2022 16:59:53 - INFO - __main__ - Global step 1800 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 16:59:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.49 on epoch=904
06/19/2022 16:59:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.47 on epoch=909
06/19/2022 16:59:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=914
06/19/2022 16:59:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=919
06/19/2022 17:00:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.53 on epoch=924
06/19/2022 17:00:00 - INFO - __main__ - Global step 1850 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 17:00:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.50 on epoch=929
06/19/2022 17:00:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.49 on epoch=934
06/19/2022 17:00:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.44 on epoch=939
06/19/2022 17:00:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.47 on epoch=944
06/19/2022 17:00:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=949
06/19/2022 17:00:07 - INFO - __main__ - Global step 1900 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 17:00:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.47 on epoch=954
06/19/2022 17:00:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
06/19/2022 17:00:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.45 on epoch=964
06/19/2022 17:00:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.47 on epoch=969
06/19/2022 17:00:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=974
06/19/2022 17:00:13 - INFO - __main__ - Global step 1950 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 17:00:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
06/19/2022 17:00:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=984
06/19/2022 17:00:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=989
06/19/2022 17:00:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.53 on epoch=994
06/19/2022 17:00:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.45 on epoch=999
06/19/2022 17:00:20 - INFO - __main__ - Global step 2000 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 17:00:20 - INFO - __main__ - save last model!
06/19/2022 17:00:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 17:00:20 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 17:00:20 - INFO - __main__ - Printing 3 examples
06/19/2022 17:00:20 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 17:00:20 - INFO - __main__ - ['0']
06/19/2022 17:00:20 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 17:00:20 - INFO - __main__ - ['1']
06/19/2022 17:00:20 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 17:00:20 - INFO - __main__ - ['1']
06/19/2022 17:00:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:00:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:00:21 - INFO - __main__ - Printing 3 examples
06/19/2022 17:00:21 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 17:00:21 - INFO - __main__ - ['1']
06/19/2022 17:00:21 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 17:00:21 - INFO - __main__ - ['1']
06/19/2022 17:00:21 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 17:00:21 - INFO - __main__ - ['1']
06/19/2022 17:00:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 17:00:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:00:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:00:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:00:21 - INFO - __main__ - Printing 3 examples
06/19/2022 17:00:21 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 17:00:21 - INFO - __main__ - ['1']
06/19/2022 17:00:21 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 17:00:21 - INFO - __main__ - ['1']
06/19/2022 17:00:21 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 17:00:21 - INFO - __main__ - ['1']
06/19/2022 17:00:21 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:00:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:00:21 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:00:25 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:00:28 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:00:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:00:28 - INFO - __main__ - Starting training!
06/19/2022 17:00:34 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 17:02:09 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_21_0.2_8_predictions.txt
06/19/2022 17:02:10 - INFO - __main__ - Classification-F1 on test data: 0.3067
06/19/2022 17:02:10 - INFO - __main__ - prefix=paws_16_21, lr=0.2, bsz=8, dev_performance=0.3992490613266583, test_performance=0.30669902071236677
06/19/2022 17:02:10 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.5, bsz=8 ...
06/19/2022 17:02:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:02:11 - INFO - __main__ - Printing 3 examples
06/19/2022 17:02:11 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 17:02:11 - INFO - __main__ - ['1']
06/19/2022 17:02:11 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 17:02:11 - INFO - __main__ - ['1']
06/19/2022 17:02:11 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 17:02:11 - INFO - __main__ - ['1']
06/19/2022 17:02:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:02:11 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:02:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:02:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:02:11 - INFO - __main__ - Printing 3 examples
06/19/2022 17:02:11 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 17:02:11 - INFO - __main__ - ['1']
06/19/2022 17:02:11 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 17:02:11 - INFO - __main__ - ['1']
06/19/2022 17:02:11 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 17:02:11 - INFO - __main__ - ['1']
06/19/2022 17:02:11 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:02:11 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:02:11 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:02:17 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:02:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:02:17 - INFO - __main__ - Starting training!
06/19/2022 17:02:19 - INFO - __main__ - Step 10 Global step 10 Train loss 4.05 on epoch=4
06/19/2022 17:02:20 - INFO - __main__ - Step 20 Global step 20 Train loss 3.48 on epoch=9
06/19/2022 17:02:22 - INFO - __main__ - Step 30 Global step 30 Train loss 2.79 on epoch=14
06/19/2022 17:02:23 - INFO - __main__ - Step 40 Global step 40 Train loss 2.20 on epoch=19
06/19/2022 17:02:25 - INFO - __main__ - Step 50 Global step 50 Train loss 1.73 on epoch=24
06/19/2022 17:02:25 - INFO - __main__ - Global step 50 Train loss 2.85 Classification-F1 0.3333333333333333 on epoch=24
06/19/2022 17:02:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/19/2022 17:02:26 - INFO - __main__ - Step 60 Global step 60 Train loss 1.58 on epoch=29
06/19/2022 17:02:28 - INFO - __main__ - Step 70 Global step 70 Train loss 1.45 on epoch=34
06/19/2022 17:02:29 - INFO - __main__ - Step 80 Global step 80 Train loss 1.32 on epoch=39
06/19/2022 17:02:31 - INFO - __main__ - Step 90 Global step 90 Train loss 1.27 on epoch=44
06/19/2022 17:02:32 - INFO - __main__ - Step 100 Global step 100 Train loss 1.17 on epoch=49
06/19/2022 17:02:32 - INFO - __main__ - Global step 100 Train loss 1.36 Classification-F1 0.3333333333333333 on epoch=49
06/19/2022 17:02:34 - INFO - __main__ - Step 110 Global step 110 Train loss 1.09 on epoch=54
06/19/2022 17:02:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.97 on epoch=59
06/19/2022 17:02:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.93 on epoch=64
06/19/2022 17:02:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.93 on epoch=69
06/19/2022 17:02:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=74
06/19/2022 17:02:39 - INFO - __main__ - Global step 150 Train loss 0.96 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 17:02:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.86 on epoch=79
06/19/2022 17:02:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=84
06/19/2022 17:02:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.79 on epoch=89
06/19/2022 17:02:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.76 on epoch=94
06/19/2022 17:02:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=99
06/19/2022 17:02:46 - INFO - __main__ - Global step 200 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 17:02:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.72 on epoch=104
06/19/2022 17:02:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.68 on epoch=109
06/19/2022 17:02:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.81 on epoch=114
06/19/2022 17:02:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.75 on epoch=119
06/19/2022 17:02:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=124
06/19/2022 17:02:54 - INFO - __main__ - Global step 250 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 17:02:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.74 on epoch=129
06/19/2022 17:02:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=134
06/19/2022 17:02:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.77 on epoch=139
06/19/2022 17:02:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.67 on epoch=144
06/19/2022 17:03:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.72 on epoch=149
06/19/2022 17:03:01 - INFO - __main__ - Global step 300 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 17:03:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.69 on epoch=154
06/19/2022 17:03:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.64 on epoch=159
06/19/2022 17:03:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.73 on epoch=164
06/19/2022 17:03:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.69 on epoch=169
06/19/2022 17:03:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.59 on epoch=174
06/19/2022 17:03:08 - INFO - __main__ - Global step 350 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 17:03:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=179
06/19/2022 17:03:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.57 on epoch=184
06/19/2022 17:03:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.58 on epoch=189
06/19/2022 17:03:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.62 on epoch=194
06/19/2022 17:03:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.69 on epoch=199
06/19/2022 17:03:15 - INFO - __main__ - Global step 400 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 17:03:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.55 on epoch=204
06/19/2022 17:03:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.62 on epoch=209
06/19/2022 17:03:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.56 on epoch=214
06/19/2022 17:03:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.54 on epoch=219
06/19/2022 17:03:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.61 on epoch=224
06/19/2022 17:03:22 - INFO - __main__ - Global step 450 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 17:03:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.63 on epoch=229
06/19/2022 17:03:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=234
06/19/2022 17:03:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.56 on epoch=239
06/19/2022 17:03:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=244
06/19/2022 17:03:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.53 on epoch=249
06/19/2022 17:03:29 - INFO - __main__ - Global step 500 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 17:03:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.53 on epoch=254
06/19/2022 17:03:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.57 on epoch=259
06/19/2022 17:03:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.56 on epoch=264
06/19/2022 17:03:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.59 on epoch=269
06/19/2022 17:03:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=274
06/19/2022 17:03:37 - INFO - __main__ - Global step 550 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 17:03:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=279
06/19/2022 17:03:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=284
06/19/2022 17:03:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.52 on epoch=289
06/19/2022 17:03:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.57 on epoch=294
06/19/2022 17:03:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.59 on epoch=299
06/19/2022 17:03:44 - INFO - __main__ - Global step 600 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 17:03:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.50 on epoch=304
06/19/2022 17:03:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=309
06/19/2022 17:03:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=314
06/19/2022 17:03:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=319
06/19/2022 17:03:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=324
06/19/2022 17:03:52 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 17:03:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=329
06/19/2022 17:03:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=334
06/19/2022 17:03:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=339
06/19/2022 17:03:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=344
06/19/2022 17:03:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.55 on epoch=349
06/19/2022 17:03:59 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 17:04:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.49 on epoch=354
06/19/2022 17:04:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=359
06/19/2022 17:04:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=364
06/19/2022 17:04:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.53 on epoch=369
06/19/2022 17:04:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.53 on epoch=374
06/19/2022 17:04:08 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 17:04:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=379
06/19/2022 17:04:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.47 on epoch=384
06/19/2022 17:04:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.52 on epoch=389
06/19/2022 17:04:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.50 on epoch=394
06/19/2022 17:04:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.52 on epoch=399
06/19/2022 17:04:15 - INFO - __main__ - Global step 800 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 17:04:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=404
06/19/2022 17:04:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=409
06/19/2022 17:04:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=414
06/19/2022 17:04:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.51 on epoch=419
06/19/2022 17:04:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=424
06/19/2022 17:04:22 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 17:04:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=429
06/19/2022 17:04:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=434
06/19/2022 17:04:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=439
06/19/2022 17:04:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.48 on epoch=444
06/19/2022 17:04:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=449
06/19/2022 17:04:29 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 17:04:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=454
06/19/2022 17:04:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.45 on epoch=459
06/19/2022 17:04:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=464
06/19/2022 17:04:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=469
06/19/2022 17:04:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=474
06/19/2022 17:04:37 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.36374269005847953 on epoch=474
06/19/2022 17:04:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.36374269005847953 on epoch=474, global_step=950
06/19/2022 17:04:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=479
06/19/2022 17:04:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=484
06/19/2022 17:04:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=489
06/19/2022 17:04:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.53 on epoch=494
06/19/2022 17:04:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.42 on epoch=499
06/19/2022 17:04:45 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 17:04:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=504
06/19/2022 17:04:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=509
06/19/2022 17:04:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=514
06/19/2022 17:04:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=519
06/19/2022 17:04:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=524
06/19/2022 17:04:52 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 17:04:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=529
06/19/2022 17:04:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=534
06/19/2022 17:04:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.47 on epoch=539
06/19/2022 17:04:57 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=544
06/19/2022 17:04:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=549
06/19/2022 17:04:59 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 17:05:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=554
06/19/2022 17:05:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=559
06/19/2022 17:05:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.49 on epoch=564
06/19/2022 17:05:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=569
06/19/2022 17:05:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=574
06/19/2022 17:05:06 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.3992490613266583 on epoch=574
06/19/2022 17:05:06 - INFO - __main__ - Saving model with best Classification-F1: 0.36374269005847953 -> 0.3992490613266583 on epoch=574, global_step=1150
06/19/2022 17:05:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=579
06/19/2022 17:05:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=584
06/19/2022 17:05:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=589
06/19/2022 17:05:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=594
06/19/2022 17:05:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=599
06/19/2022 17:05:14 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 17:05:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=604
06/19/2022 17:05:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=609
06/19/2022 17:05:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=614
06/19/2022 17:05:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.42 on epoch=619
06/19/2022 17:05:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=624
06/19/2022 17:05:21 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 17:05:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=629
06/19/2022 17:05:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=634
06/19/2022 17:05:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.45 on epoch=639
06/19/2022 17:05:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=644
06/19/2022 17:05:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=649
06/19/2022 17:05:28 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 17:05:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=654
06/19/2022 17:05:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=659
06/19/2022 17:05:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=664
06/19/2022 17:05:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=669
06/19/2022 17:05:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=674
06/19/2022 17:05:36 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 17:05:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=679
06/19/2022 17:05:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.47 on epoch=684
06/19/2022 17:05:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=689
06/19/2022 17:05:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.49 on epoch=694
06/19/2022 17:05:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=699
06/19/2022 17:05:43 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 17:05:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=704
06/19/2022 17:05:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
06/19/2022 17:05:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=714
06/19/2022 17:05:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=719
06/19/2022 17:05:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.44 on epoch=724
06/19/2022 17:05:51 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 17:05:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=729
06/19/2022 17:05:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=734
06/19/2022 17:05:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=739
06/19/2022 17:05:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=744
06/19/2022 17:05:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=749
06/19/2022 17:05:58 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 17:06:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
06/19/2022 17:06:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=759
06/19/2022 17:06:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.44 on epoch=764
06/19/2022 17:06:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=769
06/19/2022 17:06:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.49 on epoch=774
06/19/2022 17:06:06 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 17:06:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=779
06/19/2022 17:06:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=784
06/19/2022 17:06:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=789
06/19/2022 17:06:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=794
06/19/2022 17:06:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.46 on epoch=799
06/19/2022 17:06:13 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 17:06:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=804
06/19/2022 17:06:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.41 on epoch=809
06/19/2022 17:06:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=814
06/19/2022 17:06:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=819
06/19/2022 17:06:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=824
06/19/2022 17:06:20 - INFO - __main__ - Global step 1650 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 17:06:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.43 on epoch=829
06/19/2022 17:06:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=834
06/19/2022 17:06:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
06/19/2022 17:06:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=844
06/19/2022 17:06:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=849
06/19/2022 17:06:27 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 17:06:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
06/19/2022 17:06:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=859
06/19/2022 17:06:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=864
06/19/2022 17:06:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
06/19/2022 17:06:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
06/19/2022 17:06:35 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 17:06:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=879
06/19/2022 17:06:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.46 on epoch=884
06/19/2022 17:06:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=889
06/19/2022 17:06:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=894
06/19/2022 17:06:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.43 on epoch=899
06/19/2022 17:06:41 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 17:06:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=904
06/19/2022 17:06:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=909
06/19/2022 17:06:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=914
06/19/2022 17:06:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.36 on epoch=919
06/19/2022 17:06:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=924
06/19/2022 17:06:49 - INFO - __main__ - Global step 1850 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 17:06:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=929
06/19/2022 17:06:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=934
06/19/2022 17:06:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=939
06/19/2022 17:06:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
06/19/2022 17:06:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
06/19/2022 17:06:57 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 17:06:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
06/19/2022 17:07:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=959
06/19/2022 17:07:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.40 on epoch=964
06/19/2022 17:07:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=969
06/19/2022 17:07:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
06/19/2022 17:07:05 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 17:07:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.39 on epoch=979
06/19/2022 17:07:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
06/19/2022 17:07:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=989
06/19/2022 17:07:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=994
06/19/2022 17:07:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=999
06/19/2022 17:07:12 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 17:07:12 - INFO - __main__ - save last model!
06/19/2022 17:07:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 17:07:12 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 17:07:12 - INFO - __main__ - Printing 3 examples
06/19/2022 17:07:12 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 17:07:12 - INFO - __main__ - ['0']
06/19/2022 17:07:12 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 17:07:12 - INFO - __main__ - ['1']
06/19/2022 17:07:12 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 17:07:12 - INFO - __main__ - ['1']
06/19/2022 17:07:12 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:07:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:07:13 - INFO - __main__ - Printing 3 examples
06/19/2022 17:07:13 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 17:07:13 - INFO - __main__ - ['1']
06/19/2022 17:07:13 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 17:07:13 - INFO - __main__ - ['1']
06/19/2022 17:07:13 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 17:07:13 - INFO - __main__ - ['1']
06/19/2022 17:07:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 17:07:13 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:07:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:07:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:07:13 - INFO - __main__ - Printing 3 examples
06/19/2022 17:07:13 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 17:07:13 - INFO - __main__ - ['1']
06/19/2022 17:07:13 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 17:07:13 - INFO - __main__ - ['1']
06/19/2022 17:07:13 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 17:07:13 - INFO - __main__ - ['1']
06/19/2022 17:07:13 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:07:13 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:07:13 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:07:19 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:07:20 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:07:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:07:21 - INFO - __main__ - Starting training!
06/19/2022 17:07:29 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 17:08:59 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_42_0.5_8_predictions.txt
06/19/2022 17:08:59 - INFO - __main__ - Classification-F1 on test data: 0.3067
06/19/2022 17:08:59 - INFO - __main__ - prefix=paws_16_42, lr=0.5, bsz=8, dev_performance=0.3992490613266583, test_performance=0.30669902071236677
06/19/2022 17:08:59 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.4, bsz=8 ...
06/19/2022 17:09:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:09:00 - INFO - __main__ - Printing 3 examples
06/19/2022 17:09:00 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 17:09:00 - INFO - __main__ - ['1']
06/19/2022 17:09:00 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 17:09:00 - INFO - __main__ - ['1']
06/19/2022 17:09:00 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 17:09:00 - INFO - __main__ - ['1']
06/19/2022 17:09:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:09:00 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:09:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:09:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:09:00 - INFO - __main__ - Printing 3 examples
06/19/2022 17:09:00 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 17:09:00 - INFO - __main__ - ['1']
06/19/2022 17:09:00 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 17:09:00 - INFO - __main__ - ['1']
06/19/2022 17:09:00 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 17:09:00 - INFO - __main__ - ['1']
06/19/2022 17:09:00 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:09:00 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:09:00 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:09:06 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:09:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:09:06 - INFO - __main__ - Starting training!
06/19/2022 17:09:08 - INFO - __main__ - Step 10 Global step 10 Train loss 4.19 on epoch=4
06/19/2022 17:09:09 - INFO - __main__ - Step 20 Global step 20 Train loss 3.76 on epoch=9
06/19/2022 17:09:10 - INFO - __main__ - Step 30 Global step 30 Train loss 3.31 on epoch=14
06/19/2022 17:09:11 - INFO - __main__ - Step 40 Global step 40 Train loss 2.75 on epoch=19
06/19/2022 17:09:13 - INFO - __main__ - Step 50 Global step 50 Train loss 2.29 on epoch=24
06/19/2022 17:09:13 - INFO - __main__ - Global step 50 Train loss 3.26 Classification-F1 0.3333333333333333 on epoch=24
06/19/2022 17:09:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/19/2022 17:09:14 - INFO - __main__ - Step 60 Global step 60 Train loss 1.88 on epoch=29
06/19/2022 17:09:16 - INFO - __main__ - Step 70 Global step 70 Train loss 1.68 on epoch=34
06/19/2022 17:09:17 - INFO - __main__ - Step 80 Global step 80 Train loss 1.42 on epoch=39
06/19/2022 17:09:19 - INFO - __main__ - Step 90 Global step 90 Train loss 1.40 on epoch=44
06/19/2022 17:09:20 - INFO - __main__ - Step 100 Global step 100 Train loss 1.31 on epoch=49
06/19/2022 17:09:20 - INFO - __main__ - Global step 100 Train loss 1.54 Classification-F1 0.3333333333333333 on epoch=49
06/19/2022 17:09:21 - INFO - __main__ - Step 110 Global step 110 Train loss 1.19 on epoch=54
06/19/2022 17:09:23 - INFO - __main__ - Step 120 Global step 120 Train loss 1.18 on epoch=59
06/19/2022 17:09:25 - INFO - __main__ - Step 130 Global step 130 Train loss 1.13 on epoch=64
06/19/2022 17:09:26 - INFO - __main__ - Step 140 Global step 140 Train loss 1.15 on epoch=69
06/19/2022 17:09:27 - INFO - __main__ - Step 150 Global step 150 Train loss 1.00 on epoch=74
06/19/2022 17:09:27 - INFO - __main__ - Global step 150 Train loss 1.13 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 17:09:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.93 on epoch=79
06/19/2022 17:09:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.99 on epoch=84
06/19/2022 17:09:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.95 on epoch=89
06/19/2022 17:09:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.76 on epoch=94
06/19/2022 17:09:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.87 on epoch=99
06/19/2022 17:09:34 - INFO - __main__ - Global step 200 Train loss 0.90 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 17:09:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.77 on epoch=104
06/19/2022 17:09:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.80 on epoch=109
06/19/2022 17:09:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.82 on epoch=114
06/19/2022 17:09:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.74 on epoch=119
06/19/2022 17:09:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.73 on epoch=124
06/19/2022 17:09:41 - INFO - __main__ - Global step 250 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 17:09:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.76 on epoch=129
06/19/2022 17:09:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.66 on epoch=134
06/19/2022 17:09:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.69 on epoch=139
06/19/2022 17:09:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.70 on epoch=144
06/19/2022 17:09:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.67 on epoch=149
06/19/2022 17:09:49 - INFO - __main__ - Global step 300 Train loss 0.70 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 17:09:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.65 on epoch=154
06/19/2022 17:09:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.60 on epoch=159
06/19/2022 17:09:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.59 on epoch=164
06/19/2022 17:09:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.59 on epoch=169
06/19/2022 17:09:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.66 on epoch=174
06/19/2022 17:09:57 - INFO - __main__ - Global step 350 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 17:09:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.59 on epoch=179
06/19/2022 17:10:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.57 on epoch=184
06/19/2022 17:10:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.66 on epoch=189
06/19/2022 17:10:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.56 on epoch=194
06/19/2022 17:10:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.69 on epoch=199
06/19/2022 17:10:04 - INFO - __main__ - Global step 400 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 17:10:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.69 on epoch=204
06/19/2022 17:10:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.64 on epoch=209
06/19/2022 17:10:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.61 on epoch=214
06/19/2022 17:10:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.53 on epoch=219
06/19/2022 17:10:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.70 on epoch=224
06/19/2022 17:10:12 - INFO - __main__ - Global step 450 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 17:10:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=229
06/19/2022 17:10:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.61 on epoch=234
06/19/2022 17:10:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=239
06/19/2022 17:10:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=244
06/19/2022 17:10:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=249
06/19/2022 17:10:18 - INFO - __main__ - Global step 500 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 17:10:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.54 on epoch=254
06/19/2022 17:10:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.53 on epoch=259
06/19/2022 17:10:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.53 on epoch=264
06/19/2022 17:10:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=269
06/19/2022 17:10:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=274
06/19/2022 17:10:26 - INFO - __main__ - Global step 550 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 17:10:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.54 on epoch=279
06/19/2022 17:10:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=284
06/19/2022 17:10:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=289
06/19/2022 17:10:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=294
06/19/2022 17:10:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.56 on epoch=299
06/19/2022 17:10:33 - INFO - __main__ - Global step 600 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 17:10:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.56 on epoch=304
06/19/2022 17:10:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=309
06/19/2022 17:10:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.49 on epoch=314
06/19/2022 17:10:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=319
06/19/2022 17:10:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=324
06/19/2022 17:10:40 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 17:10:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.54 on epoch=329
06/19/2022 17:10:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=334
06/19/2022 17:10:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=339
06/19/2022 17:10:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.56 on epoch=344
06/19/2022 17:10:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=349
06/19/2022 17:10:47 - INFO - __main__ - Global step 700 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 17:10:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.49 on epoch=354
06/19/2022 17:10:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.53 on epoch=359
06/19/2022 17:10:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=364
06/19/2022 17:10:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=369
06/19/2022 17:10:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=374
06/19/2022 17:10:54 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 17:10:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=379
06/19/2022 17:10:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=384
06/19/2022 17:10:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=389
06/19/2022 17:11:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.49 on epoch=394
06/19/2022 17:11:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=399
06/19/2022 17:11:02 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 17:11:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=404
06/19/2022 17:11:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=409
06/19/2022 17:11:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=414
06/19/2022 17:11:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=419
06/19/2022 17:11:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=424
06/19/2022 17:11:09 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.3191489361702127 on epoch=424
06/19/2022 17:11:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.52 on epoch=429
06/19/2022 17:11:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=434
06/19/2022 17:11:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=439
06/19/2022 17:11:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=444
06/19/2022 17:11:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.51 on epoch=449
06/19/2022 17:11:17 - INFO - __main__ - Global step 900 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 17:11:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=454
06/19/2022 17:11:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=459
06/19/2022 17:11:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=464
06/19/2022 17:11:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=469
06/19/2022 17:11:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=474
06/19/2022 17:11:24 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 17:11:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=479
06/19/2022 17:11:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=484
06/19/2022 17:11:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=489
06/19/2022 17:11:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=494
06/19/2022 17:11:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=499
06/19/2022 17:11:32 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 17:11:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=504
06/19/2022 17:11:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.54 on epoch=509
06/19/2022 17:11:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=514
06/19/2022 17:11:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=519
06/19/2022 17:11:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=524
06/19/2022 17:11:40 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 17:11:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=529
06/19/2022 17:11:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.51 on epoch=534
06/19/2022 17:11:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=539
06/19/2022 17:11:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=544
06/19/2022 17:11:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=549
06/19/2022 17:11:47 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 17:11:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.48 on epoch=554
06/19/2022 17:11:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=559
06/19/2022 17:11:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.47 on epoch=564
06/19/2022 17:11:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=569
06/19/2022 17:11:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=574
06/19/2022 17:11:54 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 17:11:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=579
06/19/2022 17:11:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=584
06/19/2022 17:11:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=589
06/19/2022 17:12:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=594
06/19/2022 17:12:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.46 on epoch=599
06/19/2022 17:12:01 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 17:12:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=604
06/19/2022 17:12:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=609
06/19/2022 17:12:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.51 on epoch=614
06/19/2022 17:12:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.42 on epoch=619
06/19/2022 17:12:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=624
06/19/2022 17:12:08 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 17:12:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=629
06/19/2022 17:12:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=634
06/19/2022 17:12:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=639
06/19/2022 17:12:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=644
06/19/2022 17:12:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=649
06/19/2022 17:12:16 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 17:12:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=654
06/19/2022 17:12:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=659
06/19/2022 17:12:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=664
06/19/2022 17:12:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=669
06/19/2022 17:12:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=674
06/19/2022 17:12:23 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 17:12:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=679
06/19/2022 17:12:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=684
06/19/2022 17:12:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.44 on epoch=689
06/19/2022 17:12:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.50 on epoch=694
06/19/2022 17:12:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=699
06/19/2022 17:12:31 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 17:12:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=704
06/19/2022 17:12:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
06/19/2022 17:12:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=714
06/19/2022 17:12:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=719
06/19/2022 17:12:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.47 on epoch=724
06/19/2022 17:12:38 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 17:12:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=729
06/19/2022 17:12:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=734
06/19/2022 17:12:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=739
06/19/2022 17:12:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=744
06/19/2022 17:12:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.49 on epoch=749
06/19/2022 17:12:45 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 17:12:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=754
06/19/2022 17:12:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=759
06/19/2022 17:12:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=764
06/19/2022 17:12:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=769
06/19/2022 17:12:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=774
06/19/2022 17:12:52 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 17:12:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=779
06/19/2022 17:12:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.45 on epoch=784
06/19/2022 17:12:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=789
06/19/2022 17:12:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=794
06/19/2022 17:13:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=799
06/19/2022 17:13:00 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 17:13:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=804
06/19/2022 17:13:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=809
06/19/2022 17:13:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=814
06/19/2022 17:13:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=819
06/19/2022 17:13:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=824
06/19/2022 17:13:07 - INFO - __main__ - Global step 1650 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 17:13:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=829
06/19/2022 17:13:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=834
06/19/2022 17:13:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.39 on epoch=839
06/19/2022 17:13:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.44 on epoch=844
06/19/2022 17:13:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
06/19/2022 17:13:14 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 17:13:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=854
06/19/2022 17:13:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=859
06/19/2022 17:13:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.35 on epoch=864
06/19/2022 17:13:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.41 on epoch=869
06/19/2022 17:13:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=874
06/19/2022 17:13:21 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 17:13:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=879
06/19/2022 17:13:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=884
06/19/2022 17:13:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=889
06/19/2022 17:13:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=894
06/19/2022 17:13:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.45 on epoch=899
06/19/2022 17:13:28 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 17:13:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=904
06/19/2022 17:13:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=909
06/19/2022 17:13:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=914
06/19/2022 17:13:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.39 on epoch=919
06/19/2022 17:13:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=924
06/19/2022 17:13:36 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 17:13:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=929
06/19/2022 17:13:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=934
06/19/2022 17:13:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=939
06/19/2022 17:13:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=944
06/19/2022 17:13:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.38 on epoch=949
06/19/2022 17:13:43 - INFO - __main__ - Global step 1900 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 17:13:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.42 on epoch=954
06/19/2022 17:13:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
06/19/2022 17:13:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.43 on epoch=964
06/19/2022 17:13:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=969
06/19/2022 17:13:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=974
06/19/2022 17:13:51 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 17:13:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.47 on epoch=979
06/19/2022 17:13:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=984
06/19/2022 17:13:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=989
06/19/2022 17:13:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=994
06/19/2022 17:13:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=999
06/19/2022 17:13:58 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 17:13:58 - INFO - __main__ - save last model!
06/19/2022 17:13:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 17:13:58 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 17:13:58 - INFO - __main__ - Printing 3 examples
06/19/2022 17:13:58 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 17:13:58 - INFO - __main__ - ['0']
06/19/2022 17:13:58 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 17:13:58 - INFO - __main__ - ['1']
06/19/2022 17:13:58 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 17:13:58 - INFO - __main__ - ['1']
06/19/2022 17:13:58 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:13:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:13:58 - INFO - __main__ - Printing 3 examples
06/19/2022 17:13:58 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 17:13:58 - INFO - __main__ - ['1']
06/19/2022 17:13:58 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 17:13:58 - INFO - __main__ - ['1']
06/19/2022 17:13:58 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 17:13:58 - INFO - __main__ - ['1']
06/19/2022 17:13:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 17:13:58 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:13:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:13:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:13:58 - INFO - __main__ - Printing 3 examples
06/19/2022 17:13:58 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 17:13:58 - INFO - __main__ - ['1']
06/19/2022 17:13:58 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 17:13:58 - INFO - __main__ - ['1']
06/19/2022 17:13:58 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 17:13:58 - INFO - __main__ - ['1']
06/19/2022 17:13:58 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:13:58 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:13:58 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:14:02 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:14:04 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:14:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:14:04 - INFO - __main__ - Starting training!
06/19/2022 17:14:13 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 17:15:41 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_42_0.4_8_predictions.txt
06/19/2022 17:15:41 - INFO - __main__ - Classification-F1 on test data: 0.3067
06/19/2022 17:15:41 - INFO - __main__ - prefix=paws_16_42, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.30669902071236677
06/19/2022 17:15:41 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.3, bsz=8 ...
06/19/2022 17:15:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:15:42 - INFO - __main__ - Printing 3 examples
06/19/2022 17:15:42 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 17:15:42 - INFO - __main__ - ['1']
06/19/2022 17:15:42 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 17:15:42 - INFO - __main__ - ['1']
06/19/2022 17:15:42 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 17:15:42 - INFO - __main__ - ['1']
06/19/2022 17:15:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:15:42 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:15:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:15:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:15:42 - INFO - __main__ - Printing 3 examples
06/19/2022 17:15:42 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 17:15:42 - INFO - __main__ - ['1']
06/19/2022 17:15:42 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 17:15:42 - INFO - __main__ - ['1']
06/19/2022 17:15:42 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 17:15:42 - INFO - __main__ - ['1']
06/19/2022 17:15:42 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:15:42 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:15:42 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:15:49 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:15:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:15:49 - INFO - __main__ - Starting training!
06/19/2022 17:15:50 - INFO - __main__ - Step 10 Global step 10 Train loss 4.10 on epoch=4
06/19/2022 17:15:52 - INFO - __main__ - Step 20 Global step 20 Train loss 3.79 on epoch=9
06/19/2022 17:15:53 - INFO - __main__ - Step 30 Global step 30 Train loss 3.41 on epoch=14
06/19/2022 17:15:55 - INFO - __main__ - Step 40 Global step 40 Train loss 3.11 on epoch=19
06/19/2022 17:15:56 - INFO - __main__ - Step 50 Global step 50 Train loss 2.67 on epoch=24
06/19/2022 17:15:56 - INFO - __main__ - Global step 50 Train loss 3.41 Classification-F1 0.03333333333333333 on epoch=24
06/19/2022 17:15:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.03333333333333333 on epoch=24, global_step=50
06/19/2022 17:15:57 - INFO - __main__ - Step 60 Global step 60 Train loss 2.27 on epoch=29
06/19/2022 17:15:59 - INFO - __main__ - Step 70 Global step 70 Train loss 2.04 on epoch=34
06/19/2022 17:16:00 - INFO - __main__ - Step 80 Global step 80 Train loss 1.70 on epoch=39
06/19/2022 17:16:01 - INFO - __main__ - Step 90 Global step 90 Train loss 1.64 on epoch=44
06/19/2022 17:16:02 - INFO - __main__ - Step 100 Global step 100 Train loss 1.53 on epoch=49
06/19/2022 17:16:03 - INFO - __main__ - Global step 100 Train loss 1.84 Classification-F1 0.3073593073593074 on epoch=49
06/19/2022 17:16:03 - INFO - __main__ - Saving model with best Classification-F1: 0.03333333333333333 -> 0.3073593073593074 on epoch=49, global_step=100
06/19/2022 17:16:04 - INFO - __main__ - Step 110 Global step 110 Train loss 1.43 on epoch=54
06/19/2022 17:16:06 - INFO - __main__ - Step 120 Global step 120 Train loss 1.40 on epoch=59
06/19/2022 17:16:07 - INFO - __main__ - Step 130 Global step 130 Train loss 1.29 on epoch=64
06/19/2022 17:16:08 - INFO - __main__ - Step 140 Global step 140 Train loss 1.23 on epoch=69
06/19/2022 17:16:10 - INFO - __main__ - Step 150 Global step 150 Train loss 1.16 on epoch=74
06/19/2022 17:16:10 - INFO - __main__ - Global step 150 Train loss 1.30 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 17:16:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3073593073593074 -> 0.3333333333333333 on epoch=74, global_step=150
06/19/2022 17:16:11 - INFO - __main__ - Step 160 Global step 160 Train loss 1.16 on epoch=79
06/19/2022 17:16:13 - INFO - __main__ - Step 170 Global step 170 Train loss 1.00 on epoch=84
06/19/2022 17:16:14 - INFO - __main__ - Step 180 Global step 180 Train loss 1.10 on epoch=89
06/19/2022 17:16:15 - INFO - __main__ - Step 190 Global step 190 Train loss 1.08 on epoch=94
06/19/2022 17:16:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=99
06/19/2022 17:16:17 - INFO - __main__ - Global step 200 Train loss 1.06 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 17:16:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.93 on epoch=104
06/19/2022 17:16:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.82 on epoch=109
06/19/2022 17:16:21 - INFO - __main__ - Step 230 Global step 230 Train loss 1.00 on epoch=114
06/19/2022 17:16:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.92 on epoch=119
06/19/2022 17:16:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.81 on epoch=124
06/19/2022 17:16:24 - INFO - __main__ - Global step 250 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 17:16:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.90 on epoch=129
06/19/2022 17:16:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.82 on epoch=134
06/19/2022 17:16:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.80 on epoch=139
06/19/2022 17:16:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.71 on epoch=144
06/19/2022 17:16:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.82 on epoch=149
06/19/2022 17:16:31 - INFO - __main__ - Global step 300 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 17:16:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.78 on epoch=154
06/19/2022 17:16:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.73 on epoch=159
06/19/2022 17:16:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.78 on epoch=164
06/19/2022 17:16:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.74 on epoch=169
06/19/2022 17:16:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.74 on epoch=174
06/19/2022 17:16:38 - INFO - __main__ - Global step 350 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 17:16:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.71 on epoch=179
06/19/2022 17:16:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.63 on epoch=184
06/19/2022 17:16:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.65 on epoch=189
06/19/2022 17:16:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.66 on epoch=194
06/19/2022 17:16:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.67 on epoch=199
06/19/2022 17:16:45 - INFO - __main__ - Global step 400 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 17:16:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.76 on epoch=204
06/19/2022 17:16:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.58 on epoch=209
06/19/2022 17:16:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.70 on epoch=214
06/19/2022 17:16:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.67 on epoch=219
06/19/2022 17:16:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.56 on epoch=224
06/19/2022 17:16:52 - INFO - __main__ - Global step 450 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 17:16:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.63 on epoch=229
06/19/2022 17:16:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.62 on epoch=234
06/19/2022 17:16:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.59 on epoch=239
06/19/2022 17:16:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.65 on epoch=244
06/19/2022 17:16:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.63 on epoch=249
06/19/2022 17:16:59 - INFO - __main__ - Global step 500 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 17:17:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.59 on epoch=254
06/19/2022 17:17:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.52 on epoch=259
06/19/2022 17:17:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.56 on epoch=264
06/19/2022 17:17:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=269
06/19/2022 17:17:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=274
06/19/2022 17:17:05 - INFO - __main__ - Global step 550 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 17:17:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.59 on epoch=279
06/19/2022 17:17:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.62 on epoch=284
06/19/2022 17:17:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=289
06/19/2022 17:17:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.53 on epoch=294
06/19/2022 17:17:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.61 on epoch=299
06/19/2022 17:17:12 - INFO - __main__ - Global step 600 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 17:17:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.62 on epoch=304
06/19/2022 17:17:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.62 on epoch=309
06/19/2022 17:17:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.60 on epoch=314
06/19/2022 17:17:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=319
06/19/2022 17:17:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.56 on epoch=324
06/19/2022 17:17:18 - INFO - __main__ - Global step 650 Train loss 0.59 Classification-F1 0.3191489361702127 on epoch=324
06/19/2022 17:17:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.60 on epoch=329
06/19/2022 17:17:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.54 on epoch=334
06/19/2022 17:17:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.56 on epoch=339
06/19/2022 17:17:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.55 on epoch=344
06/19/2022 17:17:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.62 on epoch=349
06/19/2022 17:17:25 - INFO - __main__ - Global step 700 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 17:17:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.59 on epoch=354
06/19/2022 17:17:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.63 on epoch=359
06/19/2022 17:17:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.56 on epoch=364
06/19/2022 17:17:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.51 on epoch=369
06/19/2022 17:17:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=374
06/19/2022 17:17:31 - INFO - __main__ - Global step 750 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 17:17:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=379
06/19/2022 17:17:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.59 on epoch=384
06/19/2022 17:17:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.56 on epoch=389
06/19/2022 17:17:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=394
06/19/2022 17:17:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.57 on epoch=399
06/19/2022 17:17:39 - INFO - __main__ - Global step 800 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 17:17:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=404
06/19/2022 17:17:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=409
06/19/2022 17:17:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.55 on epoch=414
06/19/2022 17:17:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.54 on epoch=419
06/19/2022 17:17:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=424
06/19/2022 17:17:46 - INFO - __main__ - Global step 850 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 17:17:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.56 on epoch=429
06/19/2022 17:17:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.56 on epoch=434
06/19/2022 17:17:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.55 on epoch=439
06/19/2022 17:17:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.51 on epoch=444
06/19/2022 17:17:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.54 on epoch=449
06/19/2022 17:17:53 - INFO - __main__ - Global step 900 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 17:17:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.53 on epoch=454
06/19/2022 17:17:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=459
06/19/2022 17:17:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=464
06/19/2022 17:17:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.60 on epoch=469
06/19/2022 17:18:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=474
06/19/2022 17:18:00 - INFO - __main__ - Global step 950 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 17:18:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=479
06/19/2022 17:18:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.53 on epoch=484
06/19/2022 17:18:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.46 on epoch=489
06/19/2022 17:18:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=494
06/19/2022 17:18:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.51 on epoch=499
06/19/2022 17:18:06 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 17:18:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.54 on epoch=504
06/19/2022 17:18:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=509
06/19/2022 17:18:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=514
06/19/2022 17:18:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=519
06/19/2022 17:18:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.53 on epoch=524
06/19/2022 17:18:12 - INFO - __main__ - Global step 1050 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 17:18:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.54 on epoch=529
06/19/2022 17:18:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=534
06/19/2022 17:18:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=539
06/19/2022 17:18:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.49 on epoch=544
06/19/2022 17:18:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.51 on epoch=549
06/19/2022 17:18:20 - INFO - __main__ - Global step 1100 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 17:18:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.56 on epoch=554
06/19/2022 17:18:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=559
06/19/2022 17:18:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=564
06/19/2022 17:18:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.47 on epoch=569
06/19/2022 17:18:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=574
06/19/2022 17:18:27 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 17:18:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=579
06/19/2022 17:18:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=584
06/19/2022 17:18:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=589
06/19/2022 17:18:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=594
06/19/2022 17:18:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.46 on epoch=599
06/19/2022 17:18:35 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 17:18:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.52 on epoch=604
06/19/2022 17:18:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.47 on epoch=609
06/19/2022 17:18:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=614
06/19/2022 17:18:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=619
06/19/2022 17:18:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.51 on epoch=624
06/19/2022 17:18:43 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 17:18:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=629
06/19/2022 17:18:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.54 on epoch=634
06/19/2022 17:18:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=639
06/19/2022 17:18:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=644
06/19/2022 17:18:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=649
06/19/2022 17:18:51 - INFO - __main__ - Global step 1300 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 17:18:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=654
06/19/2022 17:18:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.56 on epoch=659
06/19/2022 17:18:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.51 on epoch=664
06/19/2022 17:18:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=669
06/19/2022 17:18:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.52 on epoch=674
06/19/2022 17:18:58 - INFO - __main__ - Global step 1350 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 17:19:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=679
06/19/2022 17:19:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=684
06/19/2022 17:19:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=689
06/19/2022 17:19:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=694
06/19/2022 17:19:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=699
06/19/2022 17:19:06 - INFO - __main__ - Global step 1400 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 17:19:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.45 on epoch=704
06/19/2022 17:19:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.52 on epoch=709
06/19/2022 17:19:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.46 on epoch=714
06/19/2022 17:19:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.47 on epoch=719
06/19/2022 17:19:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=724
06/19/2022 17:19:14 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 17:19:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.47 on epoch=729
06/19/2022 17:19:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.49 on epoch=734
06/19/2022 17:19:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.46 on epoch=739
06/19/2022 17:19:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.45 on epoch=744
06/19/2022 17:19:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=749
06/19/2022 17:19:22 - INFO - __main__ - Global step 1500 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 17:19:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=754
06/19/2022 17:19:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=759
06/19/2022 17:19:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.50 on epoch=764
06/19/2022 17:19:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=769
06/19/2022 17:19:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=774
06/19/2022 17:19:30 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 17:19:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=779
06/19/2022 17:19:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=784
06/19/2022 17:19:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.40 on epoch=789
06/19/2022 17:19:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=794
06/19/2022 17:19:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
06/19/2022 17:19:37 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 17:19:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=804
06/19/2022 17:19:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.48 on epoch=809
06/19/2022 17:19:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=814
06/19/2022 17:19:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=819
06/19/2022 17:19:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.48 on epoch=824
06/19/2022 17:19:44 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 17:19:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=829
06/19/2022 17:19:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.43 on epoch=834
06/19/2022 17:19:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
06/19/2022 17:19:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=844
06/19/2022 17:19:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.52 on epoch=849
06/19/2022 17:19:50 - INFO - __main__ - Global step 1700 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 17:19:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.44 on epoch=854
06/19/2022 17:19:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.45 on epoch=859
06/19/2022 17:19:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=864
06/19/2022 17:19:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=869
06/19/2022 17:19:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=874
06/19/2022 17:19:58 - INFO - __main__ - Global step 1750 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 17:19:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=879
06/19/2022 17:20:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.42 on epoch=884
06/19/2022 17:20:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=889
06/19/2022 17:20:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=894
06/19/2022 17:20:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=899
06/19/2022 17:20:05 - INFO - __main__ - Global step 1800 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 17:20:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=904
06/19/2022 17:20:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=909
06/19/2022 17:20:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=914
06/19/2022 17:20:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=919
06/19/2022 17:20:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.48 on epoch=924
06/19/2022 17:20:12 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.3992490613266583 on epoch=924
06/19/2022 17:20:12 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=924, global_step=1850
06/19/2022 17:20:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=929
06/19/2022 17:20:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=934
06/19/2022 17:20:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=939
06/19/2022 17:20:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.46 on epoch=944
06/19/2022 17:20:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
06/19/2022 17:20:19 - INFO - __main__ - Global step 1900 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 17:20:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
06/19/2022 17:20:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=959
06/19/2022 17:20:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.37 on epoch=964
06/19/2022 17:20:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=969
06/19/2022 17:20:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=974
06/19/2022 17:20:26 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 17:20:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=979
06/19/2022 17:20:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.45 on epoch=984
06/19/2022 17:20:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=989
06/19/2022 17:20:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=994
06/19/2022 17:20:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=999
06/19/2022 17:20:34 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 17:20:34 - INFO - __main__ - save last model!
06/19/2022 17:20:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 17:20:34 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 17:20:34 - INFO - __main__ - Printing 3 examples
06/19/2022 17:20:34 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 17:20:34 - INFO - __main__ - ['0']
06/19/2022 17:20:34 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 17:20:34 - INFO - __main__ - ['1']
06/19/2022 17:20:34 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 17:20:34 - INFO - __main__ - ['1']
06/19/2022 17:20:34 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:20:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:20:34 - INFO - __main__ - Printing 3 examples
06/19/2022 17:20:34 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 17:20:34 - INFO - __main__ - ['1']
06/19/2022 17:20:34 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 17:20:34 - INFO - __main__ - ['1']
06/19/2022 17:20:34 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 17:20:34 - INFO - __main__ - ['1']
06/19/2022 17:20:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 17:20:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:20:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:20:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:20:34 - INFO - __main__ - Printing 3 examples
06/19/2022 17:20:34 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 17:20:34 - INFO - __main__ - ['1']
06/19/2022 17:20:34 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 17:20:34 - INFO - __main__ - ['1']
06/19/2022 17:20:34 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 17:20:34 - INFO - __main__ - ['1']
06/19/2022 17:20:34 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:20:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:20:35 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:20:38 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:20:40 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:20:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:20:40 - INFO - __main__ - Starting training!
06/19/2022 17:20:45 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 17:22:08 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_42_0.3_8_predictions.txt
06/19/2022 17:22:08 - INFO - __main__ - Classification-F1 on test data: 0.3069
06/19/2022 17:22:09 - INFO - __main__ - prefix=paws_16_42, lr=0.3, bsz=8, dev_performance=0.3992490613266583, test_performance=0.30694971710417795
06/19/2022 17:22:09 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.2, bsz=8 ...
06/19/2022 17:22:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:22:09 - INFO - __main__ - Printing 3 examples
06/19/2022 17:22:09 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 17:22:09 - INFO - __main__ - ['1']
06/19/2022 17:22:09 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 17:22:09 - INFO - __main__ - ['1']
06/19/2022 17:22:09 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 17:22:09 - INFO - __main__ - ['1']
06/19/2022 17:22:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:22:09 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:22:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:22:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:22:10 - INFO - __main__ - Printing 3 examples
06/19/2022 17:22:10 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 17:22:10 - INFO - __main__ - ['1']
06/19/2022 17:22:10 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 17:22:10 - INFO - __main__ - ['1']
06/19/2022 17:22:10 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 17:22:10 - INFO - __main__ - ['1']
06/19/2022 17:22:10 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:22:10 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:22:10 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:22:15 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:22:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:22:16 - INFO - __main__ - Starting training!
06/19/2022 17:22:17 - INFO - __main__ - Step 10 Global step 10 Train loss 4.13 on epoch=4
06/19/2022 17:22:19 - INFO - __main__ - Step 20 Global step 20 Train loss 3.89 on epoch=9
06/19/2022 17:22:20 - INFO - __main__ - Step 30 Global step 30 Train loss 3.50 on epoch=14
06/19/2022 17:22:22 - INFO - __main__ - Step 40 Global step 40 Train loss 3.25 on epoch=19
06/19/2022 17:22:23 - INFO - __main__ - Step 50 Global step 50 Train loss 3.03 on epoch=24
06/19/2022 17:22:23 - INFO - __main__ - Global step 50 Train loss 3.56 Classification-F1 0.0 on epoch=24
06/19/2022 17:22:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 17:22:25 - INFO - __main__ - Step 60 Global step 60 Train loss 2.76 on epoch=29
06/19/2022 17:22:26 - INFO - __main__ - Step 70 Global step 70 Train loss 2.38 on epoch=34
06/19/2022 17:22:27 - INFO - __main__ - Step 80 Global step 80 Train loss 2.08 on epoch=39
06/19/2022 17:22:29 - INFO - __main__ - Step 90 Global step 90 Train loss 1.92 on epoch=44
06/19/2022 17:22:30 - INFO - __main__ - Step 100 Global step 100 Train loss 1.81 on epoch=49
06/19/2022 17:22:30 - INFO - __main__ - Global step 100 Train loss 2.19 Classification-F1 0.3333333333333333 on epoch=49
06/19/2022 17:22:30 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=49, global_step=100
06/19/2022 17:22:32 - INFO - __main__ - Step 110 Global step 110 Train loss 1.63 on epoch=54
06/19/2022 17:22:33 - INFO - __main__ - Step 120 Global step 120 Train loss 1.58 on epoch=59
06/19/2022 17:22:34 - INFO - __main__ - Step 130 Global step 130 Train loss 1.58 on epoch=64
06/19/2022 17:22:35 - INFO - __main__ - Step 140 Global step 140 Train loss 1.45 on epoch=69
06/19/2022 17:22:37 - INFO - __main__ - Step 150 Global step 150 Train loss 1.54 on epoch=74
06/19/2022 17:22:37 - INFO - __main__ - Global step 150 Train loss 1.55 Classification-F1 0.43529411764705883 on epoch=74
06/19/2022 17:22:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.43529411764705883 on epoch=74, global_step=150
06/19/2022 17:22:39 - INFO - __main__ - Step 160 Global step 160 Train loss 1.40 on epoch=79
06/19/2022 17:22:40 - INFO - __main__ - Step 170 Global step 170 Train loss 1.33 on epoch=84
06/19/2022 17:22:42 - INFO - __main__ - Step 180 Global step 180 Train loss 1.27 on epoch=89
06/19/2022 17:22:43 - INFO - __main__ - Step 190 Global step 190 Train loss 1.22 on epoch=94
06/19/2022 17:22:44 - INFO - __main__ - Step 200 Global step 200 Train loss 1.16 on epoch=99
06/19/2022 17:22:45 - INFO - __main__ - Global step 200 Train loss 1.28 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 17:22:46 - INFO - __main__ - Step 210 Global step 210 Train loss 1.19 on epoch=104
06/19/2022 17:22:47 - INFO - __main__ - Step 220 Global step 220 Train loss 1.14 on epoch=109
06/19/2022 17:22:49 - INFO - __main__ - Step 230 Global step 230 Train loss 1.20 on epoch=114
06/19/2022 17:22:50 - INFO - __main__ - Step 240 Global step 240 Train loss 1.11 on epoch=119
06/19/2022 17:22:52 - INFO - __main__ - Step 250 Global step 250 Train loss 1.07 on epoch=124
06/19/2022 17:22:52 - INFO - __main__ - Global step 250 Train loss 1.14 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 17:22:53 - INFO - __main__ - Step 260 Global step 260 Train loss 1.09 on epoch=129
06/19/2022 17:22:55 - INFO - __main__ - Step 270 Global step 270 Train loss 1.07 on epoch=134
06/19/2022 17:22:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.97 on epoch=139
06/19/2022 17:22:57 - INFO - __main__ - Step 290 Global step 290 Train loss 1.01 on epoch=144
06/19/2022 17:22:59 - INFO - __main__ - Step 300 Global step 300 Train loss 1.00 on epoch=149
06/19/2022 17:22:59 - INFO - __main__ - Global step 300 Train loss 1.03 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 17:23:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=154
06/19/2022 17:23:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.89 on epoch=159
06/19/2022 17:23:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.95 on epoch=164
06/19/2022 17:23:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.83 on epoch=169
06/19/2022 17:23:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.87 on epoch=174
06/19/2022 17:23:06 - INFO - __main__ - Global step 350 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 17:23:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.89 on epoch=179
06/19/2022 17:23:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.84 on epoch=184
06/19/2022 17:23:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.81 on epoch=189
06/19/2022 17:23:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.90 on epoch=194
06/19/2022 17:23:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.79 on epoch=199
06/19/2022 17:23:13 - INFO - __main__ - Global step 400 Train loss 0.84 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 17:23:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.83 on epoch=204
06/19/2022 17:23:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.84 on epoch=209
06/19/2022 17:23:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.92 on epoch=214
06/19/2022 17:23:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.79 on epoch=219
06/19/2022 17:23:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.72 on epoch=224
06/19/2022 17:23:20 - INFO - __main__ - Global step 450 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 17:23:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.75 on epoch=229
06/19/2022 17:23:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.76 on epoch=234
06/19/2022 17:23:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.80 on epoch=239
06/19/2022 17:23:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.83 on epoch=244
06/19/2022 17:23:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.77 on epoch=249
06/19/2022 17:23:27 - INFO - __main__ - Global step 500 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 17:23:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.72 on epoch=254
06/19/2022 17:23:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.73 on epoch=259
06/19/2022 17:23:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.72 on epoch=264
06/19/2022 17:23:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.68 on epoch=269
06/19/2022 17:23:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.70 on epoch=274
06/19/2022 17:23:34 - INFO - __main__ - Global step 550 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 17:23:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.72 on epoch=279
06/19/2022 17:23:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.70 on epoch=284
06/19/2022 17:23:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.64 on epoch=289
06/19/2022 17:23:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.61 on epoch=294
06/19/2022 17:23:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.77 on epoch=299
06/19/2022 17:23:40 - INFO - __main__ - Global step 600 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 17:23:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.70 on epoch=304
06/19/2022 17:23:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.74 on epoch=309
06/19/2022 17:23:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.69 on epoch=314
06/19/2022 17:23:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.71 on epoch=319
06/19/2022 17:23:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.64 on epoch=324
06/19/2022 17:23:47 - INFO - __main__ - Global step 650 Train loss 0.70 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 17:23:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.73 on epoch=329
06/19/2022 17:23:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.66 on epoch=334
06/19/2022 17:23:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.71 on epoch=339
06/19/2022 17:23:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.64 on epoch=344
06/19/2022 17:23:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.67 on epoch=349
06/19/2022 17:23:54 - INFO - __main__ - Global step 700 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 17:23:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.67 on epoch=354
06/19/2022 17:23:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.54 on epoch=359
06/19/2022 17:23:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.60 on epoch=364
06/19/2022 17:23:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.68 on epoch=369
06/19/2022 17:24:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.60 on epoch=374
06/19/2022 17:24:01 - INFO - __main__ - Global step 750 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 17:24:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.65 on epoch=379
06/19/2022 17:24:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.67 on epoch=384
06/19/2022 17:24:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.59 on epoch=389
06/19/2022 17:24:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.67 on epoch=394
06/19/2022 17:24:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.57 on epoch=399
06/19/2022 17:24:09 - INFO - __main__ - Global step 800 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 17:24:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.64 on epoch=404
06/19/2022 17:24:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.65 on epoch=409
06/19/2022 17:24:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.63 on epoch=414
06/19/2022 17:24:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.64 on epoch=419
06/19/2022 17:24:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.62 on epoch=424
06/19/2022 17:24:16 - INFO - __main__ - Global step 850 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 17:24:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.66 on epoch=429
06/19/2022 17:24:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.51 on epoch=434
06/19/2022 17:24:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.61 on epoch=439
06/19/2022 17:24:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.60 on epoch=444
06/19/2022 17:24:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.63 on epoch=449
06/19/2022 17:24:23 - INFO - __main__ - Global step 900 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 17:24:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.61 on epoch=454
06/19/2022 17:24:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.57 on epoch=459
06/19/2022 17:24:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.70 on epoch=464
06/19/2022 17:24:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=469
06/19/2022 17:24:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.58 on epoch=474
06/19/2022 17:24:31 - INFO - __main__ - Global step 950 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 17:24:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.60 on epoch=479
06/19/2022 17:24:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.66 on epoch=484
06/19/2022 17:24:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.65 on epoch=489
06/19/2022 17:24:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.54 on epoch=494
06/19/2022 17:24:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.53 on epoch=499
06/19/2022 17:24:38 - INFO - __main__ - Global step 1000 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 17:24:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.56 on epoch=504
06/19/2022 17:24:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.56 on epoch=509
06/19/2022 17:24:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.58 on epoch=514
06/19/2022 17:24:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.57 on epoch=519
06/19/2022 17:24:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.61 on epoch=524
06/19/2022 17:24:45 - INFO - __main__ - Global step 1050 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 17:24:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.53 on epoch=529
06/19/2022 17:24:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.59 on epoch=534
06/19/2022 17:24:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.54 on epoch=539
06/19/2022 17:24:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.53 on epoch=544
06/19/2022 17:24:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.50 on epoch=549
06/19/2022 17:24:52 - INFO - __main__ - Global step 1100 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 17:24:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.56 on epoch=554
06/19/2022 17:24:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.50 on epoch=559
06/19/2022 17:24:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.50 on epoch=564
06/19/2022 17:24:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=569
06/19/2022 17:24:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.56 on epoch=574
06/19/2022 17:24:59 - INFO - __main__ - Global step 1150 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 17:25:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=579
06/19/2022 17:25:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.48 on epoch=584
06/19/2022 17:25:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.62 on epoch=589
06/19/2022 17:25:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.57 on epoch=594
06/19/2022 17:25:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.55 on epoch=599
06/19/2022 17:25:06 - INFO - __main__ - Global step 1200 Train loss 0.54 Classification-F1 0.3191489361702127 on epoch=599
06/19/2022 17:25:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=604
06/19/2022 17:25:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.53 on epoch=609
06/19/2022 17:25:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.49 on epoch=614
06/19/2022 17:25:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.55 on epoch=619
06/19/2022 17:25:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=624
06/19/2022 17:25:13 - INFO - __main__ - Global step 1250 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 17:25:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.52 on epoch=629
06/19/2022 17:25:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.52 on epoch=634
06/19/2022 17:25:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.55 on epoch=639
06/19/2022 17:25:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.54 on epoch=644
06/19/2022 17:25:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.55 on epoch=649
06/19/2022 17:25:20 - INFO - __main__ - Global step 1300 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 17:25:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.51 on epoch=654
06/19/2022 17:25:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=659
06/19/2022 17:25:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.53 on epoch=664
06/19/2022 17:25:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.49 on epoch=669
06/19/2022 17:25:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.51 on epoch=674
06/19/2022 17:25:27 - INFO - __main__ - Global step 1350 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 17:25:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.54 on epoch=679
06/19/2022 17:25:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.50 on epoch=684
06/19/2022 17:25:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.56 on epoch=689
06/19/2022 17:25:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.56 on epoch=694
06/19/2022 17:25:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.49 on epoch=699
06/19/2022 17:25:34 - INFO - __main__ - Global step 1400 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 17:25:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.54 on epoch=704
06/19/2022 17:25:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.50 on epoch=709
06/19/2022 17:25:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.53 on epoch=714
06/19/2022 17:25:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=719
06/19/2022 17:25:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.52 on epoch=724
06/19/2022 17:25:41 - INFO - __main__ - Global step 1450 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 17:25:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.56 on epoch=729
06/19/2022 17:25:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.53 on epoch=734
06/19/2022 17:25:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.56 on epoch=739
06/19/2022 17:25:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.52 on epoch=744
06/19/2022 17:25:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.46 on epoch=749
06/19/2022 17:25:48 - INFO - __main__ - Global step 1500 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 17:25:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.47 on epoch=754
06/19/2022 17:25:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.57 on epoch=759
06/19/2022 17:25:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.52 on epoch=764
06/19/2022 17:25:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.55 on epoch=769
06/19/2022 17:25:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.48 on epoch=774
06/19/2022 17:25:55 - INFO - __main__ - Global step 1550 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 17:25:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.55 on epoch=779
06/19/2022 17:25:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.53 on epoch=784
06/19/2022 17:25:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=789
06/19/2022 17:26:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.47 on epoch=794
06/19/2022 17:26:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.49 on epoch=799
06/19/2022 17:26:02 - INFO - __main__ - Global step 1600 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 17:26:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=804
06/19/2022 17:26:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.52 on epoch=809
06/19/2022 17:26:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
06/19/2022 17:26:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=819
06/19/2022 17:26:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=824
06/19/2022 17:26:09 - INFO - __main__ - Global step 1650 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 17:26:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.43 on epoch=829
06/19/2022 17:26:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.48 on epoch=834
06/19/2022 17:26:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.50 on epoch=839
06/19/2022 17:26:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=844
06/19/2022 17:26:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.48 on epoch=849
06/19/2022 17:26:16 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 17:26:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.48 on epoch=854
06/19/2022 17:26:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.56 on epoch=859
06/19/2022 17:26:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.48 on epoch=864
06/19/2022 17:26:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=869
06/19/2022 17:26:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.55 on epoch=874
06/19/2022 17:26:24 - INFO - __main__ - Global step 1750 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 17:26:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.48 on epoch=879
06/19/2022 17:26:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.47 on epoch=884
06/19/2022 17:26:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.47 on epoch=889
06/19/2022 17:26:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.48 on epoch=894
06/19/2022 17:26:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=899
06/19/2022 17:26:31 - INFO - __main__ - Global step 1800 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 17:26:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.51 on epoch=904
06/19/2022 17:26:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.50 on epoch=909
06/19/2022 17:26:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.53 on epoch=914
06/19/2022 17:26:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.45 on epoch=919
06/19/2022 17:26:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.49 on epoch=924
06/19/2022 17:26:38 - INFO - __main__ - Global step 1850 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 17:26:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.49 on epoch=929
06/19/2022 17:26:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.46 on epoch=934
06/19/2022 17:26:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.47 on epoch=939
06/19/2022 17:26:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.48 on epoch=944
06/19/2022 17:26:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.49 on epoch=949
06/19/2022 17:26:46 - INFO - __main__ - Global step 1900 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 17:26:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.50 on epoch=954
06/19/2022 17:26:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=959
06/19/2022 17:26:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.51 on epoch=964
06/19/2022 17:26:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.55 on epoch=969
06/19/2022 17:26:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.47 on epoch=974
06/19/2022 17:26:53 - INFO - __main__ - Global step 1950 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 17:26:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.49 on epoch=979
06/19/2022 17:26:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.54 on epoch=984
06/19/2022 17:26:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=989
06/19/2022 17:26:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.44 on epoch=994
06/19/2022 17:26:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=999
06/19/2022 17:26:59 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 17:26:59 - INFO - __main__ - save last model!
06/19/2022 17:26:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 17:26:59 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 17:26:59 - INFO - __main__ - Printing 3 examples
06/19/2022 17:26:59 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 17:26:59 - INFO - __main__ - ['0']
06/19/2022 17:26:59 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 17:26:59 - INFO - __main__ - ['1']
06/19/2022 17:26:59 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 17:26:59 - INFO - __main__ - ['1']
06/19/2022 17:26:59 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:27:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:27:00 - INFO - __main__ - Printing 3 examples
06/19/2022 17:27:00 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 17:27:00 - INFO - __main__ - ['0']
06/19/2022 17:27:00 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 17:27:00 - INFO - __main__ - ['0']
06/19/2022 17:27:00 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 17:27:00 - INFO - __main__ - ['0']
06/19/2022 17:27:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 17:27:00 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:27:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:27:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:27:00 - INFO - __main__ - Printing 3 examples
06/19/2022 17:27:00 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 17:27:00 - INFO - __main__ - ['0']
06/19/2022 17:27:00 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 17:27:00 - INFO - __main__ - ['0']
06/19/2022 17:27:00 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 17:27:00 - INFO - __main__ - ['0']
06/19/2022 17:27:00 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:27:00 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:27:00 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:27:04 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:27:06 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:27:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:27:06 - INFO - __main__ - Starting training!
06/19/2022 17:27:11 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 17:28:34 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_42_0.2_8_predictions.txt
06/19/2022 17:28:35 - INFO - __main__ - Classification-F1 on test data: 0.3086
06/19/2022 17:28:35 - INFO - __main__ - prefix=paws_16_42, lr=0.2, bsz=8, dev_performance=0.43529411764705883, test_performance=0.3085883010907182
06/19/2022 17:28:35 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.5, bsz=8 ...
06/19/2022 17:28:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:28:36 - INFO - __main__ - Printing 3 examples
06/19/2022 17:28:36 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 17:28:36 - INFO - __main__ - ['0']
06/19/2022 17:28:36 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 17:28:36 - INFO - __main__ - ['0']
06/19/2022 17:28:36 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 17:28:36 - INFO - __main__ - ['0']
06/19/2022 17:28:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:28:36 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:28:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:28:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:28:36 - INFO - __main__ - Printing 3 examples
06/19/2022 17:28:36 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 17:28:36 - INFO - __main__ - ['0']
06/19/2022 17:28:36 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 17:28:36 - INFO - __main__ - ['0']
06/19/2022 17:28:36 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 17:28:36 - INFO - __main__ - ['0']
06/19/2022 17:28:36 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:28:36 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:28:36 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:28:41 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:28:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:28:41 - INFO - __main__ - Starting training!
06/19/2022 17:28:43 - INFO - __main__ - Step 10 Global step 10 Train loss 3.97 on epoch=4
06/19/2022 17:28:44 - INFO - __main__ - Step 20 Global step 20 Train loss 3.39 on epoch=9
06/19/2022 17:28:46 - INFO - __main__ - Step 30 Global step 30 Train loss 2.67 on epoch=14
06/19/2022 17:28:47 - INFO - __main__ - Step 40 Global step 40 Train loss 2.08 on epoch=19
06/19/2022 17:28:48 - INFO - __main__ - Step 50 Global step 50 Train loss 1.70 on epoch=24
06/19/2022 17:28:49 - INFO - __main__ - Global step 50 Train loss 2.76 Classification-F1 0.46843853820598 on epoch=24
06/19/2022 17:28:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.46843853820598 on epoch=24, global_step=50
06/19/2022 17:28:50 - INFO - __main__ - Step 60 Global step 60 Train loss 1.49 on epoch=29
06/19/2022 17:28:51 - INFO - __main__ - Step 70 Global step 70 Train loss 1.38 on epoch=34
06/19/2022 17:28:53 - INFO - __main__ - Step 80 Global step 80 Train loss 1.19 on epoch=39
06/19/2022 17:28:54 - INFO - __main__ - Step 90 Global step 90 Train loss 1.15 on epoch=44
06/19/2022 17:28:55 - INFO - __main__ - Step 100 Global step 100 Train loss 1.17 on epoch=49
06/19/2022 17:28:55 - INFO - __main__ - Global step 100 Train loss 1.28 Classification-F1 0.3333333333333333 on epoch=49
06/19/2022 17:28:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.99 on epoch=54
06/19/2022 17:28:58 - INFO - __main__ - Step 120 Global step 120 Train loss 1.00 on epoch=59
06/19/2022 17:28:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.94 on epoch=64
06/19/2022 17:29:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.91 on epoch=69
06/19/2022 17:29:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.90 on epoch=74
06/19/2022 17:29:02 - INFO - __main__ - Global step 150 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 17:29:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.88 on epoch=79
06/19/2022 17:29:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.87 on epoch=84
06/19/2022 17:29:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.76 on epoch=89
06/19/2022 17:29:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.76 on epoch=94
06/19/2022 17:29:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.61 on epoch=99
06/19/2022 17:29:10 - INFO - __main__ - Global step 200 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 17:29:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.74 on epoch=104
06/19/2022 17:29:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.76 on epoch=109
06/19/2022 17:29:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.55 on epoch=114
06/19/2022 17:29:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.72 on epoch=119
06/19/2022 17:29:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.63 on epoch=124
06/19/2022 17:29:17 - INFO - __main__ - Global step 250 Train loss 0.68 Classification-F1 0.3191489361702127 on epoch=124
06/19/2022 17:29:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.68 on epoch=129
06/19/2022 17:29:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.57 on epoch=134
06/19/2022 17:29:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.65 on epoch=139
06/19/2022 17:29:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.59 on epoch=144
06/19/2022 17:29:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.61 on epoch=149
06/19/2022 17:29:25 - INFO - __main__ - Global step 300 Train loss 0.62 Classification-F1 0.3816425120772947 on epoch=149
06/19/2022 17:29:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.64 on epoch=154
06/19/2022 17:29:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.54 on epoch=159
06/19/2022 17:29:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.62 on epoch=164
06/19/2022 17:29:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=169
06/19/2022 17:29:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.55 on epoch=174
06/19/2022 17:29:33 - INFO - __main__ - Global step 350 Train loss 0.57 Classification-F1 0.3816425120772947 on epoch=174
06/19/2022 17:29:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.58 on epoch=179
06/19/2022 17:29:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.55 on epoch=184
06/19/2022 17:29:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.60 on epoch=189
06/19/2022 17:29:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.58 on epoch=194
06/19/2022 17:29:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.58 on epoch=199
06/19/2022 17:29:40 - INFO - __main__ - Global step 400 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 17:29:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.60 on epoch=204
06/19/2022 17:29:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.55 on epoch=209
06/19/2022 17:29:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.58 on epoch=214
06/19/2022 17:29:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.59 on epoch=219
06/19/2022 17:29:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.49 on epoch=224
06/19/2022 17:29:48 - INFO - __main__ - Global step 450 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 17:29:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=229
06/19/2022 17:29:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.58 on epoch=234
06/19/2022 17:29:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.52 on epoch=239
06/19/2022 17:29:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.53 on epoch=244
06/19/2022 17:29:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.54 on epoch=249
06/19/2022 17:29:56 - INFO - __main__ - Global step 500 Train loss 0.56 Classification-F1 0.3992490613266583 on epoch=249
06/19/2022 17:29:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=254
06/19/2022 17:29:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=259
06/19/2022 17:30:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=264
06/19/2022 17:30:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=269
06/19/2022 17:30:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.49 on epoch=274
06/19/2022 17:30:03 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 17:30:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=279
06/19/2022 17:30:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=284
06/19/2022 17:30:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.53 on epoch=289
06/19/2022 17:30:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=294
06/19/2022 17:30:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=299
06/19/2022 17:30:09 - INFO - __main__ - Global step 600 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 17:30:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.50 on epoch=304
06/19/2022 17:30:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=309
06/19/2022 17:30:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.55 on epoch=314
06/19/2022 17:30:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=319
06/19/2022 17:30:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=324
06/19/2022 17:30:16 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 17:30:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=329
06/19/2022 17:30:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=334
06/19/2022 17:30:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=339
06/19/2022 17:30:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=344
06/19/2022 17:30:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=349
06/19/2022 17:30:24 - INFO - __main__ - Global step 700 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 17:30:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=354
06/19/2022 17:30:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=359
06/19/2022 17:30:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=364
06/19/2022 17:30:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.49 on epoch=369
06/19/2022 17:30:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=374
06/19/2022 17:30:33 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 17:30:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=379
06/19/2022 17:30:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=384
06/19/2022 17:30:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=389
06/19/2022 17:30:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=394
06/19/2022 17:30:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.46 on epoch=399
06/19/2022 17:30:41 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 17:30:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=404
06/19/2022 17:30:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=409
06/19/2022 17:30:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.49 on epoch=414
06/19/2022 17:30:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=419
06/19/2022 17:30:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=424
06/19/2022 17:30:48 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 17:30:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=429
06/19/2022 17:30:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.48 on epoch=434
06/19/2022 17:30:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=439
06/19/2022 17:30:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=444
06/19/2022 17:30:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=449
06/19/2022 17:30:56 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 17:30:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=454
06/19/2022 17:30:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=459
06/19/2022 17:31:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=464
06/19/2022 17:31:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.54 on epoch=469
06/19/2022 17:31:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=474
06/19/2022 17:31:03 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.4554554554554554 on epoch=474
06/19/2022 17:31:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=479
06/19/2022 17:31:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=484
06/19/2022 17:31:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=489
06/19/2022 17:31:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=494
06/19/2022 17:31:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=499
06/19/2022 17:31:11 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 17:31:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=504
06/19/2022 17:31:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=509
06/19/2022 17:31:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=514
06/19/2022 17:31:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=519
06/19/2022 17:31:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=524
06/19/2022 17:31:18 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 17:31:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=529
06/19/2022 17:31:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=534
06/19/2022 17:31:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=539
06/19/2022 17:31:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=544
06/19/2022 17:31:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=549
06/19/2022 17:31:26 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 17:31:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=554
06/19/2022 17:31:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=559
06/19/2022 17:31:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=564
06/19/2022 17:31:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=569
06/19/2022 17:31:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=574
06/19/2022 17:31:34 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.3191489361702127 on epoch=574
06/19/2022 17:31:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.42 on epoch=579
06/19/2022 17:31:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.41 on epoch=584
06/19/2022 17:31:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=589
06/19/2022 17:31:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=594
06/19/2022 17:31:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=599
06/19/2022 17:31:42 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.3816425120772947 on epoch=599
06/19/2022 17:31:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=604
06/19/2022 17:31:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.54 on epoch=609
06/19/2022 17:31:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=614
06/19/2022 17:31:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=619
06/19/2022 17:31:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=624
06/19/2022 17:31:49 - INFO - __main__ - Global step 1250 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 17:31:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=629
06/19/2022 17:31:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=634
06/19/2022 17:31:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=639
06/19/2022 17:31:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=644
06/19/2022 17:31:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=649
06/19/2022 17:31:57 - INFO - __main__ - Global step 1300 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 17:31:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=654
06/19/2022 17:31:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=659
06/19/2022 17:32:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=664
06/19/2022 17:32:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=669
06/19/2022 17:32:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=674
06/19/2022 17:32:04 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.3043478260869565 on epoch=674
06/19/2022 17:32:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=679
06/19/2022 17:32:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=684
06/19/2022 17:32:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=689
06/19/2022 17:32:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=694
06/19/2022 17:32:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=699
06/19/2022 17:32:13 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 17:32:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=704
06/19/2022 17:32:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=709
06/19/2022 17:32:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=714
06/19/2022 17:32:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=719
06/19/2022 17:32:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=724
06/19/2022 17:32:21 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 17:32:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.38 on epoch=729
06/19/2022 17:32:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=734
06/19/2022 17:32:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=739
06/19/2022 17:32:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=744
06/19/2022 17:32:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=749
06/19/2022 17:32:27 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 17:32:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=754
06/19/2022 17:32:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=759
06/19/2022 17:32:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=764
06/19/2022 17:32:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=769
06/19/2022 17:32:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=774
06/19/2022 17:32:34 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.3992490613266583 on epoch=774
06/19/2022 17:32:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=779
06/19/2022 17:32:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=784
06/19/2022 17:32:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=789
06/19/2022 17:32:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=794
06/19/2022 17:32:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=799
06/19/2022 17:32:41 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 17:32:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=804
06/19/2022 17:32:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=809
06/19/2022 17:32:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=814
06/19/2022 17:32:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=819
06/19/2022 17:32:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.36 on epoch=824
06/19/2022 17:32:49 - INFO - __main__ - Global step 1650 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 17:32:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=829
06/19/2022 17:32:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.46 on epoch=834
06/19/2022 17:32:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.36 on epoch=839
06/19/2022 17:32:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=844
06/19/2022 17:32:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=849
06/19/2022 17:32:57 - INFO - __main__ - Global step 1700 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 17:32:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=854
06/19/2022 17:33:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=859
06/19/2022 17:33:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=864
06/19/2022 17:33:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
06/19/2022 17:33:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=874
06/19/2022 17:33:05 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.39756367663344405 on epoch=874
06/19/2022 17:33:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.43 on epoch=879
06/19/2022 17:33:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=884
06/19/2022 17:33:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=889
06/19/2022 17:33:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=894
06/19/2022 17:33:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=899
06/19/2022 17:33:14 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 17:33:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=904
06/19/2022 17:33:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=909
06/19/2022 17:33:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
06/19/2022 17:33:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=919
06/19/2022 17:33:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=924
06/19/2022 17:33:22 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.4682306940371457 on epoch=924
06/19/2022 17:33:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.43 on epoch=929
06/19/2022 17:33:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=934
06/19/2022 17:33:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=939
06/19/2022 17:33:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.38 on epoch=944
06/19/2022 17:33:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=949
06/19/2022 17:33:30 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.3191489361702127 on epoch=949
06/19/2022 17:33:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=954
06/19/2022 17:33:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=959
06/19/2022 17:33:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.36 on epoch=964
06/19/2022 17:33:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=969
06/19/2022 17:33:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=974
06/19/2022 17:33:39 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 17:33:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=979
06/19/2022 17:33:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=984
06/19/2022 17:33:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.34 on epoch=989
06/19/2022 17:33:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=994
06/19/2022 17:33:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=999
06/19/2022 17:33:46 - INFO - __main__ - Global step 2000 Train loss 0.37 Classification-F1 0.49090909090909085 on epoch=999
06/19/2022 17:33:46 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.49090909090909085 on epoch=999, global_step=2000
06/19/2022 17:33:46 - INFO - __main__ - save last model!
06/19/2022 17:33:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 17:33:46 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 17:33:46 - INFO - __main__ - Printing 3 examples
06/19/2022 17:33:46 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 17:33:46 - INFO - __main__ - ['0']
06/19/2022 17:33:46 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 17:33:46 - INFO - __main__ - ['1']
06/19/2022 17:33:46 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 17:33:46 - INFO - __main__ - ['1']
06/19/2022 17:33:46 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:33:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:33:47 - INFO - __main__ - Printing 3 examples
06/19/2022 17:33:47 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 17:33:47 - INFO - __main__ - ['0']
06/19/2022 17:33:47 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 17:33:47 - INFO - __main__ - ['0']
06/19/2022 17:33:47 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 17:33:47 - INFO - __main__ - ['0']
06/19/2022 17:33:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 17:33:47 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:33:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:33:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:33:47 - INFO - __main__ - Printing 3 examples
06/19/2022 17:33:47 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 17:33:47 - INFO - __main__ - ['0']
06/19/2022 17:33:47 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 17:33:47 - INFO - __main__ - ['0']
06/19/2022 17:33:47 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 17:33:47 - INFO - __main__ - ['0']
06/19/2022 17:33:47 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:33:47 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:33:47 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:33:51 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:33:53 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:33:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:33:53 - INFO - __main__ - Starting training!
06/19/2022 17:34:01 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 17:35:34 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_87_0.5_8_predictions.txt
06/19/2022 17:35:34 - INFO - __main__ - Classification-F1 on test data: 0.4290
06/19/2022 17:35:34 - INFO - __main__ - prefix=paws_16_87, lr=0.5, bsz=8, dev_performance=0.49090909090909085, test_performance=0.42897322967659557
06/19/2022 17:35:34 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.4, bsz=8 ...
06/19/2022 17:35:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:35:35 - INFO - __main__ - Printing 3 examples
06/19/2022 17:35:35 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 17:35:35 - INFO - __main__ - ['0']
06/19/2022 17:35:35 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 17:35:35 - INFO - __main__ - ['0']
06/19/2022 17:35:35 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 17:35:35 - INFO - __main__ - ['0']
06/19/2022 17:35:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:35:35 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:35:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:35:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:35:35 - INFO - __main__ - Printing 3 examples
06/19/2022 17:35:35 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 17:35:35 - INFO - __main__ - ['0']
06/19/2022 17:35:35 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 17:35:35 - INFO - __main__ - ['0']
06/19/2022 17:35:35 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 17:35:35 - INFO - __main__ - ['0']
06/19/2022 17:35:35 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:35:35 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:35:35 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:35:40 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:35:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:35:41 - INFO - __main__ - Starting training!
06/19/2022 17:35:42 - INFO - __main__ - Step 10 Global step 10 Train loss 3.94 on epoch=4
06/19/2022 17:35:43 - INFO - __main__ - Step 20 Global step 20 Train loss 3.46 on epoch=9
06/19/2022 17:35:45 - INFO - __main__ - Step 30 Global step 30 Train loss 2.92 on epoch=14
06/19/2022 17:35:46 - INFO - __main__ - Step 40 Global step 40 Train loss 2.38 on epoch=19
06/19/2022 17:35:47 - INFO - __main__ - Step 50 Global step 50 Train loss 1.91 on epoch=24
06/19/2022 17:35:48 - INFO - __main__ - Global step 50 Train loss 2.92 Classification-F1 0.3333333333333333 on epoch=24
06/19/2022 17:35:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/19/2022 17:35:49 - INFO - __main__ - Step 60 Global step 60 Train loss 1.77 on epoch=29
06/19/2022 17:35:50 - INFO - __main__ - Step 70 Global step 70 Train loss 1.63 on epoch=34
06/19/2022 17:35:52 - INFO - __main__ - Step 80 Global step 80 Train loss 1.49 on epoch=39
06/19/2022 17:35:53 - INFO - __main__ - Step 90 Global step 90 Train loss 1.45 on epoch=44
06/19/2022 17:35:54 - INFO - __main__ - Step 100 Global step 100 Train loss 1.31 on epoch=49
06/19/2022 17:35:55 - INFO - __main__ - Global step 100 Train loss 1.53 Classification-F1 0.49090909090909085 on epoch=49
06/19/2022 17:35:55 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.49090909090909085 on epoch=49, global_step=100
06/19/2022 17:35:56 - INFO - __main__ - Step 110 Global step 110 Train loss 1.27 on epoch=54
06/19/2022 17:35:57 - INFO - __main__ - Step 120 Global step 120 Train loss 1.19 on epoch=59
06/19/2022 17:35:58 - INFO - __main__ - Step 130 Global step 130 Train loss 1.14 on epoch=64
06/19/2022 17:36:00 - INFO - __main__ - Step 140 Global step 140 Train loss 1.20 on epoch=69
06/19/2022 17:36:01 - INFO - __main__ - Step 150 Global step 150 Train loss 1.03 on epoch=74
06/19/2022 17:36:01 - INFO - __main__ - Global step 150 Train loss 1.16 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 17:36:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=79
06/19/2022 17:36:04 - INFO - __main__ - Step 170 Global step 170 Train loss 1.03 on epoch=84
06/19/2022 17:36:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.90 on epoch=89
06/19/2022 17:36:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.99 on epoch=94
06/19/2022 17:36:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.85 on epoch=99
06/19/2022 17:36:09 - INFO - __main__ - Global step 200 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 17:36:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=104
06/19/2022 17:36:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.95 on epoch=109
06/19/2022 17:36:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.85 on epoch=114
06/19/2022 17:36:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.79 on epoch=119
06/19/2022 17:36:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.83 on epoch=124
06/19/2022 17:36:16 - INFO - __main__ - Global step 250 Train loss 0.87 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 17:36:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.79 on epoch=129
06/19/2022 17:36:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.75 on epoch=134
06/19/2022 17:36:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.70 on epoch=139
06/19/2022 17:36:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.70 on epoch=144
06/19/2022 17:36:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.77 on epoch=149
06/19/2022 17:36:24 - INFO - __main__ - Global step 300 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 17:36:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.68 on epoch=154
06/19/2022 17:36:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.61 on epoch=159
06/19/2022 17:36:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.65 on epoch=164
06/19/2022 17:36:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.65 on epoch=169
06/19/2022 17:36:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.59 on epoch=174
06/19/2022 17:36:31 - INFO - __main__ - Global step 350 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 17:36:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.63 on epoch=179
06/19/2022 17:36:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.60 on epoch=184
06/19/2022 17:36:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.60 on epoch=189
06/19/2022 17:36:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.58 on epoch=194
06/19/2022 17:36:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.58 on epoch=199
06/19/2022 17:36:39 - INFO - __main__ - Global step 400 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 17:36:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.58 on epoch=204
06/19/2022 17:36:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.62 on epoch=209
06/19/2022 17:36:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.57 on epoch=214
06/19/2022 17:36:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.58 on epoch=219
06/19/2022 17:36:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.58 on epoch=224
06/19/2022 17:36:46 - INFO - __main__ - Global step 450 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 17:36:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=229
06/19/2022 17:36:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.58 on epoch=234
06/19/2022 17:36:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.56 on epoch=239
06/19/2022 17:36:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.59 on epoch=244
06/19/2022 17:36:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.64 on epoch=249
06/19/2022 17:36:55 - INFO - __main__ - Global step 500 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 17:36:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.61 on epoch=254
06/19/2022 17:36:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.55 on epoch=259
06/19/2022 17:36:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=264
06/19/2022 17:37:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=269
06/19/2022 17:37:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.54 on epoch=274
06/19/2022 17:37:02 - INFO - __main__ - Global step 550 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 17:37:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.60 on epoch=279
06/19/2022 17:37:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.49 on epoch=284
06/19/2022 17:37:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=289
06/19/2022 17:37:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=294
06/19/2022 17:37:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=299
06/19/2022 17:37:09 - INFO - __main__ - Global step 600 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 17:37:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=304
06/19/2022 17:37:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=309
06/19/2022 17:37:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=314
06/19/2022 17:37:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=319
06/19/2022 17:37:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=324
06/19/2022 17:37:16 - INFO - __main__ - Global step 650 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 17:37:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.58 on epoch=329
06/19/2022 17:37:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=334
06/19/2022 17:37:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.59 on epoch=339
06/19/2022 17:37:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=344
06/19/2022 17:37:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=349
06/19/2022 17:37:23 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 17:37:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=354
06/19/2022 17:37:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=359
06/19/2022 17:37:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=364
06/19/2022 17:37:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=369
06/19/2022 17:37:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.53 on epoch=374
06/19/2022 17:37:31 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 17:37:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=379
06/19/2022 17:37:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=384
06/19/2022 17:37:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=389
06/19/2022 17:37:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=394
06/19/2022 17:37:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=399
06/19/2022 17:37:39 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 17:37:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=404
06/19/2022 17:37:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=409
06/19/2022 17:37:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=414
06/19/2022 17:37:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=419
06/19/2022 17:37:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=424
06/19/2022 17:37:46 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 17:37:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=429
06/19/2022 17:37:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=434
06/19/2022 17:37:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=439
06/19/2022 17:37:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=444
06/19/2022 17:37:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=449
06/19/2022 17:37:54 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 17:37:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=454
06/19/2022 17:37:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=459
06/19/2022 17:37:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=464
06/19/2022 17:38:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=469
06/19/2022 17:38:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.51 on epoch=474
06/19/2022 17:38:02 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 17:38:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.50 on epoch=479
06/19/2022 17:38:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=484
06/19/2022 17:38:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=489
06/19/2022 17:38:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=494
06/19/2022 17:38:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=499
06/19/2022 17:38:09 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 17:38:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=504
06/19/2022 17:38:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=509
06/19/2022 17:38:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=514
06/19/2022 17:38:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=519
06/19/2022 17:38:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=524
06/19/2022 17:38:16 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 17:38:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=529
06/19/2022 17:38:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=534
06/19/2022 17:38:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=539
06/19/2022 17:38:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=544
06/19/2022 17:38:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=549
06/19/2022 17:38:23 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 17:38:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=554
06/19/2022 17:38:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=559
06/19/2022 17:38:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=564
06/19/2022 17:38:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=569
06/19/2022 17:38:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=574
06/19/2022 17:38:31 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 17:38:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=579
06/19/2022 17:38:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.41 on epoch=584
06/19/2022 17:38:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=589
06/19/2022 17:38:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=594
06/19/2022 17:38:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=599
06/19/2022 17:38:38 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 17:38:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=604
06/19/2022 17:38:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=609
06/19/2022 17:38:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.35 on epoch=614
06/19/2022 17:38:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=619
06/19/2022 17:38:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=624
06/19/2022 17:38:45 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 17:38:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=629
06/19/2022 17:38:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=634
06/19/2022 17:38:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=639
06/19/2022 17:38:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=644
06/19/2022 17:38:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=649
06/19/2022 17:38:52 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 17:38:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=654
06/19/2022 17:38:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=659
06/19/2022 17:38:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=664
06/19/2022 17:38:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.48 on epoch=669
06/19/2022 17:38:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=674
06/19/2022 17:38:59 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 17:39:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=679
06/19/2022 17:39:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=684
06/19/2022 17:39:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=689
06/19/2022 17:39:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=694
06/19/2022 17:39:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=699
06/19/2022 17:39:06 - INFO - __main__ - Global step 1400 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 17:39:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=704
06/19/2022 17:39:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=709
06/19/2022 17:39:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=714
06/19/2022 17:39:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=719
06/19/2022 17:39:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=724
06/19/2022 17:39:14 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 17:39:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=729
06/19/2022 17:39:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=734
06/19/2022 17:39:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.47 on epoch=739
06/19/2022 17:39:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=744
06/19/2022 17:39:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=749
06/19/2022 17:39:21 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 17:39:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=754
06/19/2022 17:39:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=759
06/19/2022 17:39:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=764
06/19/2022 17:39:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.44 on epoch=769
06/19/2022 17:39:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=774
06/19/2022 17:39:28 - INFO - __main__ - Global step 1550 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 17:39:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=779
06/19/2022 17:39:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=784
06/19/2022 17:39:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=789
06/19/2022 17:39:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=794
06/19/2022 17:39:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
06/19/2022 17:39:35 - INFO - __main__ - Global step 1600 Train loss 0.40 Classification-F1 0.4817813765182186 on epoch=799
06/19/2022 17:39:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=804
06/19/2022 17:39:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.41 on epoch=809
06/19/2022 17:39:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=814
06/19/2022 17:39:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=819
06/19/2022 17:39:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=824
06/19/2022 17:39:42 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 17:39:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=829
06/19/2022 17:39:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=834
06/19/2022 17:39:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
06/19/2022 17:39:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.32 on epoch=844
06/19/2022 17:39:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
06/19/2022 17:39:49 - INFO - __main__ - Global step 1700 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=849
06/19/2022 17:39:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=854
06/19/2022 17:39:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=859
06/19/2022 17:39:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=864
06/19/2022 17:39:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
06/19/2022 17:39:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.34 on epoch=874
06/19/2022 17:39:56 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 17:39:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=879
06/19/2022 17:39:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=884
06/19/2022 17:40:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=889
06/19/2022 17:40:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.33 on epoch=894
06/19/2022 17:40:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=899
06/19/2022 17:40:04 - INFO - __main__ - Global step 1800 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 17:40:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=904
06/19/2022 17:40:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=909
06/19/2022 17:40:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=914
06/19/2022 17:40:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=919
06/19/2022 17:40:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
06/19/2022 17:40:11 - INFO - __main__ - Global step 1850 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 17:40:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=929
06/19/2022 17:40:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=934
06/19/2022 17:40:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=939
06/19/2022 17:40:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=944
06/19/2022 17:40:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.38 on epoch=949
06/19/2022 17:40:18 - INFO - __main__ - Global step 1900 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 17:40:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=954
06/19/2022 17:40:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=959
06/19/2022 17:40:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=964
06/19/2022 17:40:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.33 on epoch=969
06/19/2022 17:40:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.35 on epoch=974
06/19/2022 17:40:26 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 17:40:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=979
06/19/2022 17:40:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
06/19/2022 17:40:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=989
06/19/2022 17:40:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.34 on epoch=994
06/19/2022 17:40:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=999
06/19/2022 17:40:33 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 17:40:33 - INFO - __main__ - save last model!
06/19/2022 17:40:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 17:40:33 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 17:40:33 - INFO - __main__ - Printing 3 examples
06/19/2022 17:40:33 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 17:40:33 - INFO - __main__ - ['0']
06/19/2022 17:40:33 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 17:40:33 - INFO - __main__ - ['1']
06/19/2022 17:40:33 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 17:40:33 - INFO - __main__ - ['1']
06/19/2022 17:40:33 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:40:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:40:34 - INFO - __main__ - Printing 3 examples
06/19/2022 17:40:34 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 17:40:34 - INFO - __main__ - ['0']
06/19/2022 17:40:34 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 17:40:34 - INFO - __main__ - ['0']
06/19/2022 17:40:34 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 17:40:34 - INFO - __main__ - ['0']
06/19/2022 17:40:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 17:40:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:40:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:40:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:40:34 - INFO - __main__ - Printing 3 examples
06/19/2022 17:40:34 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 17:40:34 - INFO - __main__ - ['0']
06/19/2022 17:40:34 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 17:40:34 - INFO - __main__ - ['0']
06/19/2022 17:40:34 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 17:40:34 - INFO - __main__ - ['0']
06/19/2022 17:40:34 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:40:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:40:34 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:40:38 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:40:41 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:40:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:40:41 - INFO - __main__ - Starting training!
06/19/2022 17:40:49 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 17:42:28 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_87_0.4_8_predictions.txt
06/19/2022 17:42:28 - INFO - __main__ - Classification-F1 on test data: 0.3084
06/19/2022 17:42:28 - INFO - __main__ - prefix=paws_16_87, lr=0.4, bsz=8, dev_performance=0.49090909090909085, test_performance=0.3083992491058873
06/19/2022 17:42:28 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.3, bsz=8 ...
06/19/2022 17:42:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:42:29 - INFO - __main__ - Printing 3 examples
06/19/2022 17:42:29 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 17:42:29 - INFO - __main__ - ['0']
06/19/2022 17:42:29 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 17:42:29 - INFO - __main__ - ['0']
06/19/2022 17:42:29 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 17:42:29 - INFO - __main__ - ['0']
06/19/2022 17:42:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:42:29 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:42:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:42:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:42:29 - INFO - __main__ - Printing 3 examples
06/19/2022 17:42:29 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 17:42:29 - INFO - __main__ - ['0']
06/19/2022 17:42:29 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 17:42:29 - INFO - __main__ - ['0']
06/19/2022 17:42:29 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 17:42:29 - INFO - __main__ - ['0']
06/19/2022 17:42:29 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:42:29 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:42:29 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:42:37 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:42:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:42:37 - INFO - __main__ - Starting training!
06/19/2022 17:42:38 - INFO - __main__ - Step 10 Global step 10 Train loss 3.89 on epoch=4
06/19/2022 17:42:40 - INFO - __main__ - Step 20 Global step 20 Train loss 3.62 on epoch=9
06/19/2022 17:42:41 - INFO - __main__ - Step 30 Global step 30 Train loss 3.08 on epoch=14
06/19/2022 17:42:43 - INFO - __main__ - Step 40 Global step 40 Train loss 2.70 on epoch=19
06/19/2022 17:42:44 - INFO - __main__ - Step 50 Global step 50 Train loss 2.21 on epoch=24
06/19/2022 17:42:45 - INFO - __main__ - Global step 50 Train loss 3.10 Classification-F1 0.21739130434782608 on epoch=24
06/19/2022 17:42:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.21739130434782608 on epoch=24, global_step=50
06/19/2022 17:42:46 - INFO - __main__ - Step 60 Global step 60 Train loss 2.01 on epoch=29
06/19/2022 17:42:48 - INFO - __main__ - Step 70 Global step 70 Train loss 1.83 on epoch=34
06/19/2022 17:42:49 - INFO - __main__ - Step 80 Global step 80 Train loss 1.59 on epoch=39
06/19/2022 17:42:50 - INFO - __main__ - Step 90 Global step 90 Train loss 1.48 on epoch=44
06/19/2022 17:42:51 - INFO - __main__ - Step 100 Global step 100 Train loss 1.35 on epoch=49
06/19/2022 17:42:52 - INFO - __main__ - Global step 100 Train loss 1.65 Classification-F1 0.4458874458874459 on epoch=49
06/19/2022 17:42:52 - INFO - __main__ - Saving model with best Classification-F1: 0.21739130434782608 -> 0.4458874458874459 on epoch=49, global_step=100
06/19/2022 17:42:53 - INFO - __main__ - Step 110 Global step 110 Train loss 1.39 on epoch=54
06/19/2022 17:42:55 - INFO - __main__ - Step 120 Global step 120 Train loss 1.32 on epoch=59
06/19/2022 17:42:57 - INFO - __main__ - Step 130 Global step 130 Train loss 1.37 on epoch=64
06/19/2022 17:42:58 - INFO - __main__ - Step 140 Global step 140 Train loss 1.15 on epoch=69
06/19/2022 17:42:59 - INFO - __main__ - Step 150 Global step 150 Train loss 1.16 on epoch=74
06/19/2022 17:43:00 - INFO - __main__ - Global step 150 Train loss 1.28 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 17:43:01 - INFO - __main__ - Step 160 Global step 160 Train loss 1.10 on epoch=79
06/19/2022 17:43:03 - INFO - __main__ - Step 170 Global step 170 Train loss 1.07 on epoch=84
06/19/2022 17:43:04 - INFO - __main__ - Step 180 Global step 180 Train loss 1.03 on epoch=89
06/19/2022 17:43:06 - INFO - __main__ - Step 190 Global step 190 Train loss 1.03 on epoch=94
06/19/2022 17:43:07 - INFO - __main__ - Step 200 Global step 200 Train loss 1.02 on epoch=99
06/19/2022 17:43:07 - INFO - __main__ - Global step 200 Train loss 1.05 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 17:43:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=104
06/19/2022 17:43:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=109
06/19/2022 17:43:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.94 on epoch=114
06/19/2022 17:43:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.90 on epoch=119
06/19/2022 17:43:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.78 on epoch=124
06/19/2022 17:43:15 - INFO - __main__ - Global step 250 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 17:43:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.79 on epoch=129
06/19/2022 17:43:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.80 on epoch=134
06/19/2022 17:43:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.69 on epoch=139
06/19/2022 17:43:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.66 on epoch=144
06/19/2022 17:43:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.79 on epoch=149
06/19/2022 17:43:22 - INFO - __main__ - Global step 300 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 17:43:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.72 on epoch=154
06/19/2022 17:43:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.68 on epoch=159
06/19/2022 17:43:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.73 on epoch=164
06/19/2022 17:43:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.66 on epoch=169
06/19/2022 17:43:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=174
06/19/2022 17:43:30 - INFO - __main__ - Global step 350 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 17:43:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.68 on epoch=179
06/19/2022 17:43:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.67 on epoch=184
06/19/2022 17:43:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.60 on epoch=189
06/19/2022 17:43:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.64 on epoch=194
06/19/2022 17:43:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.62 on epoch=199
06/19/2022 17:43:38 - INFO - __main__ - Global step 400 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 17:43:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.60 on epoch=204
06/19/2022 17:43:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.58 on epoch=209
06/19/2022 17:43:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.61 on epoch=214
06/19/2022 17:43:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.65 on epoch=219
06/19/2022 17:43:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.58 on epoch=224
06/19/2022 17:43:45 - INFO - __main__ - Global step 450 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 17:43:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.59 on epoch=229
06/19/2022 17:43:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.56 on epoch=234
06/19/2022 17:43:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.59 on epoch=239
06/19/2022 17:43:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.53 on epoch=244
06/19/2022 17:43:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.54 on epoch=249
06/19/2022 17:43:52 - INFO - __main__ - Global step 500 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 17:43:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=254
06/19/2022 17:43:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.62 on epoch=259
06/19/2022 17:43:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=264
06/19/2022 17:43:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.53 on epoch=269
06/19/2022 17:43:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.65 on epoch=274
06/19/2022 17:44:00 - INFO - __main__ - Global step 550 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 17:44:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.62 on epoch=279
06/19/2022 17:44:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.56 on epoch=284
06/19/2022 17:44:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.55 on epoch=289
06/19/2022 17:44:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.58 on epoch=294
06/19/2022 17:44:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=299
06/19/2022 17:44:07 - INFO - __main__ - Global step 600 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 17:44:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.56 on epoch=304
06/19/2022 17:44:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=309
06/19/2022 17:44:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.54 on epoch=314
06/19/2022 17:44:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.61 on epoch=319
06/19/2022 17:44:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=324
06/19/2022 17:44:15 - INFO - __main__ - Global step 650 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 17:44:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.54 on epoch=329
06/19/2022 17:44:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.53 on epoch=334
06/19/2022 17:44:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=339
06/19/2022 17:44:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=344
06/19/2022 17:44:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=349
06/19/2022 17:44:22 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.3992490613266583 on epoch=349
06/19/2022 17:44:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.51 on epoch=354
06/19/2022 17:44:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=359
06/19/2022 17:44:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=364
06/19/2022 17:44:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=369
06/19/2022 17:44:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.56 on epoch=374
06/19/2022 17:44:30 - INFO - __main__ - Global step 750 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 17:44:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=379
06/19/2022 17:44:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.52 on epoch=384
06/19/2022 17:44:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=389
06/19/2022 17:44:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.50 on epoch=394
06/19/2022 17:44:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.50 on epoch=399
06/19/2022 17:44:37 - INFO - __main__ - Global step 800 Train loss 0.49 Classification-F1 0.3992490613266583 on epoch=399
06/19/2022 17:44:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=404
06/19/2022 17:44:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=409
06/19/2022 17:44:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.49 on epoch=414
06/19/2022 17:44:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=419
06/19/2022 17:44:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=424
06/19/2022 17:44:45 - INFO - __main__ - Global step 850 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 17:44:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=429
06/19/2022 17:44:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.52 on epoch=434
06/19/2022 17:44:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=439
06/19/2022 17:44:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=444
06/19/2022 17:44:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.54 on epoch=449
06/19/2022 17:44:53 - INFO - __main__ - Global step 900 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 17:44:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.48 on epoch=454
06/19/2022 17:44:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.45 on epoch=459
06/19/2022 17:44:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=464
06/19/2022 17:44:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.49 on epoch=469
06/19/2022 17:44:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=474
06/19/2022 17:45:00 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 17:45:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=479
06/19/2022 17:45:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=484
06/19/2022 17:45:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=489
06/19/2022 17:45:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=494
06/19/2022 17:45:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=499
06/19/2022 17:45:07 - INFO - __main__ - Global step 1000 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 17:45:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=504
06/19/2022 17:45:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=509
06/19/2022 17:45:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=514
06/19/2022 17:45:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=519
06/19/2022 17:45:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.46 on epoch=524
06/19/2022 17:45:15 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 17:45:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=529
06/19/2022 17:45:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.47 on epoch=534
06/19/2022 17:45:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.59 on epoch=539
06/19/2022 17:45:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=544
06/19/2022 17:45:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=549
06/19/2022 17:45:23 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 17:45:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.49 on epoch=554
06/19/2022 17:45:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=559
06/19/2022 17:45:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.47 on epoch=564
06/19/2022 17:45:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=569
06/19/2022 17:45:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=574
06/19/2022 17:45:30 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 17:45:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=579
06/19/2022 17:45:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=584
06/19/2022 17:45:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.48 on epoch=589
06/19/2022 17:45:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=594
06/19/2022 17:45:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=599
06/19/2022 17:45:38 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 17:45:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=604
06/19/2022 17:45:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=609
06/19/2022 17:45:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=614
06/19/2022 17:45:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=619
06/19/2022 17:45:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=624
06/19/2022 17:45:46 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 17:45:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=629
06/19/2022 17:45:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=634
06/19/2022 17:45:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=639
06/19/2022 17:45:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=644
06/19/2022 17:45:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=649
06/19/2022 17:45:54 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.3191489361702127 on epoch=649
06/19/2022 17:45:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=654
06/19/2022 17:45:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=659
06/19/2022 17:45:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=664
06/19/2022 17:45:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.49 on epoch=669
06/19/2022 17:46:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=674
06/19/2022 17:46:01 - INFO - __main__ - Global step 1350 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 17:46:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.49 on epoch=679
06/19/2022 17:46:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=684
06/19/2022 17:46:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.46 on epoch=689
06/19/2022 17:46:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=694
06/19/2022 17:46:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=699
06/19/2022 17:46:08 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 17:46:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=704
06/19/2022 17:46:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=709
06/19/2022 17:46:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.39 on epoch=714
06/19/2022 17:46:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=719
06/19/2022 17:46:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.46 on epoch=724
06/19/2022 17:46:15 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 17:46:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.49 on epoch=729
06/19/2022 17:46:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=734
06/19/2022 17:46:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=739
06/19/2022 17:46:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=744
06/19/2022 17:46:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=749
06/19/2022 17:46:23 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 17:46:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=754
06/19/2022 17:46:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=759
06/19/2022 17:46:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.44 on epoch=764
06/19/2022 17:46:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=769
06/19/2022 17:46:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=774
06/19/2022 17:46:29 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 17:46:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=779
06/19/2022 17:46:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
06/19/2022 17:46:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=789
06/19/2022 17:46:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.47 on epoch=794
06/19/2022 17:46:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.41 on epoch=799
06/19/2022 17:46:36 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 17:46:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=804
06/19/2022 17:46:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.47 on epoch=809
06/19/2022 17:46:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=814
06/19/2022 17:46:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=819
06/19/2022 17:46:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.40 on epoch=824
06/19/2022 17:46:43 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 17:46:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=829
06/19/2022 17:46:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=834
06/19/2022 17:46:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
06/19/2022 17:46:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=844
06/19/2022 17:46:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
06/19/2022 17:46:50 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 17:46:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
06/19/2022 17:46:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=859
06/19/2022 17:46:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=864
06/19/2022 17:46:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
06/19/2022 17:46:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=874
06/19/2022 17:46:58 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 17:47:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=879
06/19/2022 17:47:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=884
06/19/2022 17:47:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=889
06/19/2022 17:47:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=894
06/19/2022 17:47:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=899
06/19/2022 17:47:06 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3191489361702127 on epoch=899
06/19/2022 17:47:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=904
06/19/2022 17:47:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.38 on epoch=909
06/19/2022 17:47:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=914
06/19/2022 17:47:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=919
06/19/2022 17:47:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=924
06/19/2022 17:47:14 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 17:47:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=929
06/19/2022 17:47:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=934
06/19/2022 17:47:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=939
06/19/2022 17:47:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
06/19/2022 17:47:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=949
06/19/2022 17:47:22 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.36374269005847953 on epoch=949
06/19/2022 17:47:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=954
06/19/2022 17:47:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
06/19/2022 17:47:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=964
06/19/2022 17:47:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=969
06/19/2022 17:47:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
06/19/2022 17:47:30 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 17:47:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=979
06/19/2022 17:47:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=984
06/19/2022 17:47:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=989
06/19/2022 17:47:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=994
06/19/2022 17:47:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=999
06/19/2022 17:47:38 - INFO - __main__ - Global step 2000 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 17:47:38 - INFO - __main__ - save last model!
06/19/2022 17:47:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 17:47:38 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 17:47:38 - INFO - __main__ - Printing 3 examples
06/19/2022 17:47:38 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 17:47:38 - INFO - __main__ - ['0']
06/19/2022 17:47:38 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 17:47:38 - INFO - __main__ - ['1']
06/19/2022 17:47:38 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 17:47:38 - INFO - __main__ - ['1']
06/19/2022 17:47:38 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:47:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:47:39 - INFO - __main__ - Printing 3 examples
06/19/2022 17:47:39 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 17:47:39 - INFO - __main__ - ['0']
06/19/2022 17:47:39 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 17:47:39 - INFO - __main__ - ['0']
06/19/2022 17:47:39 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 17:47:39 - INFO - __main__ - ['0']
06/19/2022 17:47:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 17:47:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:47:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:47:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:47:39 - INFO - __main__ - Printing 3 examples
06/19/2022 17:47:39 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 17:47:39 - INFO - __main__ - ['0']
06/19/2022 17:47:39 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 17:47:39 - INFO - __main__ - ['0']
06/19/2022 17:47:39 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 17:47:39 - INFO - __main__ - ['0']
06/19/2022 17:47:39 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:47:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:47:39 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:47:42 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:47:45 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:47:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:47:46 - INFO - __main__ - Starting training!
06/19/2022 17:47:50 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 17:49:17 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_87_0.3_8_predictions.txt
06/19/2022 17:49:17 - INFO - __main__ - Classification-F1 on test data: 0.3083
06/19/2022 17:49:17 - INFO - __main__ - prefix=paws_16_87, lr=0.3, bsz=8, dev_performance=0.4458874458874459, test_performance=0.30827047656450984
06/19/2022 17:49:17 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.2, bsz=8 ...
06/19/2022 17:49:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:49:18 - INFO - __main__ - Printing 3 examples
06/19/2022 17:49:18 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 17:49:18 - INFO - __main__ - ['0']
06/19/2022 17:49:18 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 17:49:18 - INFO - __main__ - ['0']
06/19/2022 17:49:18 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 17:49:18 - INFO - __main__ - ['0']
06/19/2022 17:49:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:49:18 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:49:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:49:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:49:18 - INFO - __main__ - Printing 3 examples
06/19/2022 17:49:18 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 17:49:18 - INFO - __main__ - ['0']
06/19/2022 17:49:18 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 17:49:18 - INFO - __main__ - ['0']
06/19/2022 17:49:18 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 17:49:18 - INFO - __main__ - ['0']
06/19/2022 17:49:18 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:49:18 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:49:18 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:49:24 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 17:49:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:49:24 - INFO - __main__ - Starting training!
06/19/2022 17:49:26 - INFO - __main__ - Step 10 Global step 10 Train loss 3.98 on epoch=4
06/19/2022 17:49:27 - INFO - __main__ - Step 20 Global step 20 Train loss 3.72 on epoch=9
06/19/2022 17:49:28 - INFO - __main__ - Step 30 Global step 30 Train loss 3.49 on epoch=14
06/19/2022 17:49:30 - INFO - __main__ - Step 40 Global step 40 Train loss 3.08 on epoch=19
06/19/2022 17:49:31 - INFO - __main__ - Step 50 Global step 50 Train loss 2.68 on epoch=24
06/19/2022 17:49:32 - INFO - __main__ - Global step 50 Train loss 3.39 Classification-F1 0.0 on epoch=24
06/19/2022 17:49:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 17:49:33 - INFO - __main__ - Step 60 Global step 60 Train loss 2.53 on epoch=29
06/19/2022 17:49:34 - INFO - __main__ - Step 70 Global step 70 Train loss 2.31 on epoch=34
06/19/2022 17:49:35 - INFO - __main__ - Step 80 Global step 80 Train loss 1.95 on epoch=39
06/19/2022 17:49:37 - INFO - __main__ - Step 90 Global step 90 Train loss 1.89 on epoch=44
06/19/2022 17:49:38 - INFO - __main__ - Step 100 Global step 100 Train loss 1.70 on epoch=49
06/19/2022 17:49:39 - INFO - __main__ - Global step 100 Train loss 2.08 Classification-F1 0.3333333333333333 on epoch=49
06/19/2022 17:49:39 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=49, global_step=100
06/19/2022 17:49:40 - INFO - __main__ - Step 110 Global step 110 Train loss 1.70 on epoch=54
06/19/2022 17:49:41 - INFO - __main__ - Step 120 Global step 120 Train loss 1.65 on epoch=59
06/19/2022 17:49:42 - INFO - __main__ - Step 130 Global step 130 Train loss 1.48 on epoch=64
06/19/2022 17:49:44 - INFO - __main__ - Step 140 Global step 140 Train loss 1.40 on epoch=69
06/19/2022 17:49:45 - INFO - __main__ - Step 150 Global step 150 Train loss 1.50 on epoch=74
06/19/2022 17:49:46 - INFO - __main__ - Global step 150 Train loss 1.55 Classification-F1 0.4285714285714286 on epoch=74
06/19/2022 17:49:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4285714285714286 on epoch=74, global_step=150
06/19/2022 17:49:47 - INFO - __main__ - Step 160 Global step 160 Train loss 1.36 on epoch=79
06/19/2022 17:49:49 - INFO - __main__ - Step 170 Global step 170 Train loss 1.37 on epoch=84
06/19/2022 17:49:50 - INFO - __main__ - Step 180 Global step 180 Train loss 1.34 on epoch=89
06/19/2022 17:49:52 - INFO - __main__ - Step 190 Global step 190 Train loss 1.17 on epoch=94
06/19/2022 17:49:53 - INFO - __main__ - Step 200 Global step 200 Train loss 1.14 on epoch=99
06/19/2022 17:49:54 - INFO - __main__ - Global step 200 Train loss 1.28 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 17:49:55 - INFO - __main__ - Step 210 Global step 210 Train loss 1.11 on epoch=104
06/19/2022 17:49:56 - INFO - __main__ - Step 220 Global step 220 Train loss 1.18 on epoch=109
06/19/2022 17:49:58 - INFO - __main__ - Step 230 Global step 230 Train loss 1.07 on epoch=114
06/19/2022 17:49:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.92 on epoch=119
06/19/2022 17:50:00 - INFO - __main__ - Step 250 Global step 250 Train loss 1.00 on epoch=124
06/19/2022 17:50:00 - INFO - __main__ - Global step 250 Train loss 1.06 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 17:50:02 - INFO - __main__ - Step 260 Global step 260 Train loss 1.02 on epoch=129
06/19/2022 17:50:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.95 on epoch=134
06/19/2022 17:50:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.95 on epoch=139
06/19/2022 17:50:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.93 on epoch=144
06/19/2022 17:50:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.98 on epoch=149
06/19/2022 17:50:08 - INFO - __main__ - Global step 300 Train loss 0.97 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 17:50:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.87 on epoch=154
06/19/2022 17:50:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.95 on epoch=159
06/19/2022 17:50:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.87 on epoch=164
06/19/2022 17:50:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.88 on epoch=169
06/19/2022 17:50:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.85 on epoch=174
06/19/2022 17:50:16 - INFO - __main__ - Global step 350 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 17:50:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.82 on epoch=179
06/19/2022 17:50:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.85 on epoch=184
06/19/2022 17:50:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.83 on epoch=189
06/19/2022 17:50:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.77 on epoch=194
06/19/2022 17:50:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.89 on epoch=199
06/19/2022 17:50:24 - INFO - __main__ - Global step 400 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 17:50:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.71 on epoch=204
06/19/2022 17:50:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.68 on epoch=209
06/19/2022 17:50:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.75 on epoch=214
06/19/2022 17:50:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.70 on epoch=219
06/19/2022 17:50:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.81 on epoch=224
06/19/2022 17:50:31 - INFO - __main__ - Global step 450 Train loss 0.73 Classification-F1 0.3992490613266583 on epoch=224
06/19/2022 17:50:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.74 on epoch=229
06/19/2022 17:50:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.67 on epoch=234
06/19/2022 17:50:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.71 on epoch=239
06/19/2022 17:50:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.68 on epoch=244
06/19/2022 17:50:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.63 on epoch=249
06/19/2022 17:50:39 - INFO - __main__ - Global step 500 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 17:50:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.69 on epoch=254
06/19/2022 17:50:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.63 on epoch=259
06/19/2022 17:50:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.70 on epoch=264
06/19/2022 17:50:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.75 on epoch=269
06/19/2022 17:50:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.68 on epoch=274
06/19/2022 17:50:46 - INFO - __main__ - Global step 550 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 17:50:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.63 on epoch=279
06/19/2022 17:50:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.63 on epoch=284
06/19/2022 17:50:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.63 on epoch=289
06/19/2022 17:50:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.67 on epoch=294
06/19/2022 17:50:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.62 on epoch=299
06/19/2022 17:50:52 - INFO - __main__ - Global step 600 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 17:50:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.70 on epoch=304
06/19/2022 17:50:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.69 on epoch=309
06/19/2022 17:50:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.65 on epoch=314
06/19/2022 17:50:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.65 on epoch=319
06/19/2022 17:51:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.61 on epoch=324
06/19/2022 17:51:00 - INFO - __main__ - Global step 650 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 17:51:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.59 on epoch=329
06/19/2022 17:51:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.60 on epoch=334
06/19/2022 17:51:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.68 on epoch=339
06/19/2022 17:51:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.55 on epoch=344
06/19/2022 17:51:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.70 on epoch=349
06/19/2022 17:51:07 - INFO - __main__ - Global step 700 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 17:51:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.58 on epoch=354
06/19/2022 17:51:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=359
06/19/2022 17:51:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.58 on epoch=364
06/19/2022 17:51:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.60 on epoch=369
06/19/2022 17:51:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=374
06/19/2022 17:51:14 - INFO - __main__ - Global step 750 Train loss 0.58 Classification-F1 0.3992490613266583 on epoch=374
06/19/2022 17:51:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.55 on epoch=379
06/19/2022 17:51:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.54 on epoch=384
06/19/2022 17:51:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.58 on epoch=389
06/19/2022 17:51:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.56 on epoch=394
06/19/2022 17:51:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.57 on epoch=399
06/19/2022 17:51:22 - INFO - __main__ - Global step 800 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 17:51:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.57 on epoch=404
06/19/2022 17:51:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.57 on epoch=409
06/19/2022 17:51:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.60 on epoch=414
06/19/2022 17:51:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.55 on epoch=419
06/19/2022 17:51:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.58 on epoch=424
06/19/2022 17:51:29 - INFO - __main__ - Global step 850 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 17:51:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.60 on epoch=429
06/19/2022 17:51:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.48 on epoch=434
06/19/2022 17:51:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.52 on epoch=439
06/19/2022 17:51:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.65 on epoch=444
06/19/2022 17:51:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.59 on epoch=449
06/19/2022 17:51:37 - INFO - __main__ - Global step 900 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 17:51:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.53 on epoch=454
06/19/2022 17:51:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=459
06/19/2022 17:51:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.53 on epoch=464
06/19/2022 17:51:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.53 on epoch=469
06/19/2022 17:51:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.54 on epoch=474
06/19/2022 17:51:44 - INFO - __main__ - Global step 950 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 17:51:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.62 on epoch=479
06/19/2022 17:51:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.51 on epoch=484
06/19/2022 17:51:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.51 on epoch=489
06/19/2022 17:51:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.57 on epoch=494
06/19/2022 17:51:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=499
06/19/2022 17:51:51 - INFO - __main__ - Global step 1000 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 17:51:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.50 on epoch=504
06/19/2022 17:51:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.53 on epoch=509
06/19/2022 17:51:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=514
06/19/2022 17:51:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.53 on epoch=519
06/19/2022 17:51:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.53 on epoch=524
06/19/2022 17:51:58 - INFO - __main__ - Global step 1050 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 17:51:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.52 on epoch=529
06/19/2022 17:52:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.49 on epoch=534
06/19/2022 17:52:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.62 on epoch=539
06/19/2022 17:52:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.49 on epoch=544
06/19/2022 17:52:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.53 on epoch=549
06/19/2022 17:52:06 - INFO - __main__ - Global step 1100 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 17:52:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=554
06/19/2022 17:52:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=559
06/19/2022 17:52:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.52 on epoch=564
06/19/2022 17:52:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.50 on epoch=569
06/19/2022 17:52:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=574
06/19/2022 17:52:13 - INFO - __main__ - Global step 1150 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 17:52:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.51 on epoch=579
06/19/2022 17:52:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.54 on epoch=584
06/19/2022 17:52:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=589
06/19/2022 17:52:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.52 on epoch=594
06/19/2022 17:52:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=599
06/19/2022 17:52:21 - INFO - __main__ - Global step 1200 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 17:52:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.50 on epoch=604
06/19/2022 17:52:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.52 on epoch=609
06/19/2022 17:52:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=614
06/19/2022 17:52:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.56 on epoch=619
06/19/2022 17:52:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=624
06/19/2022 17:52:29 - INFO - __main__ - Global step 1250 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 17:52:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=629
06/19/2022 17:52:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.50 on epoch=634
06/19/2022 17:52:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=639
06/19/2022 17:52:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=644
06/19/2022 17:52:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.69 on epoch=649
06/19/2022 17:52:36 - INFO - __main__ - Global step 1300 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 17:52:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=654
06/19/2022 17:52:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=659
06/19/2022 17:52:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=664
06/19/2022 17:52:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=669
06/19/2022 17:52:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=674
06/19/2022 17:52:43 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 17:52:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=679
06/19/2022 17:52:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.52 on epoch=684
06/19/2022 17:52:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=689
06/19/2022 17:52:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=694
06/19/2022 17:52:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=699
06/19/2022 17:52:51 - INFO - __main__ - Global step 1400 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 17:52:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.48 on epoch=704
06/19/2022 17:52:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=709
06/19/2022 17:52:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=714
06/19/2022 17:52:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=719
06/19/2022 17:52:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.47 on epoch=724
06/19/2022 17:52:58 - INFO - __main__ - Global step 1450 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 17:52:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=729
06/19/2022 17:53:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=734
06/19/2022 17:53:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.46 on epoch=739
06/19/2022 17:53:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=744
06/19/2022 17:53:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.51 on epoch=749
06/19/2022 17:53:05 - INFO - __main__ - Global step 1500 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 17:53:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.51 on epoch=754
06/19/2022 17:53:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=759
06/19/2022 17:53:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=764
06/19/2022 17:53:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.54 on epoch=769
06/19/2022 17:53:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=774
06/19/2022 17:53:13 - INFO - __main__ - Global step 1550 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 17:53:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=779
06/19/2022 17:53:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=784
06/19/2022 17:53:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.48 on epoch=789
06/19/2022 17:53:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.47 on epoch=794
06/19/2022 17:53:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.47 on epoch=799
06/19/2022 17:53:20 - INFO - __main__ - Global step 1600 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 17:53:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.49 on epoch=804
06/19/2022 17:53:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.46 on epoch=809
06/19/2022 17:53:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.45 on epoch=814
06/19/2022 17:53:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=819
06/19/2022 17:53:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.46 on epoch=824
06/19/2022 17:53:28 - INFO - __main__ - Global step 1650 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 17:53:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=829
06/19/2022 17:53:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=834
06/19/2022 17:53:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=839
06/19/2022 17:53:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=844
06/19/2022 17:53:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=849
06/19/2022 17:53:36 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 17:53:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.48 on epoch=854
06/19/2022 17:53:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=859
06/19/2022 17:53:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=864
06/19/2022 17:53:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.47 on epoch=869
06/19/2022 17:53:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=874
06/19/2022 17:53:44 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 17:53:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=879
06/19/2022 17:53:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=884
06/19/2022 17:53:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=889
06/19/2022 17:53:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=894
06/19/2022 17:53:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.48 on epoch=899
06/19/2022 17:53:52 - INFO - __main__ - Global step 1800 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 17:53:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.46 on epoch=904
06/19/2022 17:53:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=909
06/19/2022 17:53:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=914
06/19/2022 17:53:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=919
06/19/2022 17:54:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=924
06/19/2022 17:54:02 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 17:54:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.46 on epoch=929
06/19/2022 17:54:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=934
06/19/2022 17:54:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=939
06/19/2022 17:54:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.44 on epoch=944
06/19/2022 17:54:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=949
06/19/2022 17:54:10 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 17:54:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=954
06/19/2022 17:54:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=959
06/19/2022 17:54:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.48 on epoch=964
06/19/2022 17:54:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.47 on epoch=969
06/19/2022 17:54:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=974
06/19/2022 17:54:18 - INFO - __main__ - Global step 1950 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 17:54:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=979
06/19/2022 17:54:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=984
06/19/2022 17:54:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.47 on epoch=989
06/19/2022 17:54:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.44 on epoch=994
06/19/2022 17:54:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.37 on epoch=999
06/19/2022 17:54:25 - INFO - __main__ - Global step 2000 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 17:54:25 - INFO - __main__ - save last model!
06/19/2022 17:54:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 17:54:25 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 17:54:25 - INFO - __main__ - Printing 3 examples
06/19/2022 17:54:25 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 17:54:25 - INFO - __main__ - ['0']
06/19/2022 17:54:25 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 17:54:25 - INFO - __main__ - ['1']
06/19/2022 17:54:25 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 17:54:25 - INFO - __main__ - ['1']
06/19/2022 17:54:25 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:54:30 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:54:40 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 17:56:13 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_87_0.2_8_predictions.txt
06/19/2022 17:56:13 - INFO - __main__ - Classification-F1 on test data: 0.3066
06/19/2022 17:56:14 - INFO - __main__ - prefix=paws_16_87, lr=0.2, bsz=8, dev_performance=0.4285714285714286, test_performance=0.3066389322239556
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (87794): No such process
t5base para pt downstream
Task: glue-mrpc, Checkpoint: None, Identifier: T5-base-nopara2para
06/19/2022 17:56:19 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-nopara2para/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 17:56:19 - INFO - __main__ - models/T5-base-nopara2para/singletask-glue-mrpc
06/19/2022 17:56:19 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-nopara2para/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 17:56:19 - INFO - __main__ - models/T5-base-nopara2para/singletask-glue-mrpc
06/19/2022 17:56:20 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/19/2022 17:56:20 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/19/2022 17:56:20 - INFO - __main__ - args.device: cuda:0
06/19/2022 17:56:20 - INFO - __main__ - Using 2 gpus
06/19/2022 17:56:20 - INFO - __main__ - args.device: cuda:1
06/19/2022 17:56:20 - INFO - __main__ - Using 2 gpus
06/19/2022 17:56:20 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/19/2022 17:56:20 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/19/2022 17:56:25 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.5, bsz=8 ...
06/19/2022 17:56:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:56:26 - INFO - __main__ - Printing 3 examples
06/19/2022 17:56:26 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 17:56:26 - INFO - __main__ - ['not_equivalent']
06/19/2022 17:56:26 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 17:56:26 - INFO - __main__ - ['not_equivalent']
06/19/2022 17:56:26 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 17:56:26 - INFO - __main__ - ['not_equivalent']
06/19/2022 17:56:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:56:26 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:56:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:56:26 - INFO - __main__ - Printing 3 examples
06/19/2022 17:56:26 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 17:56:26 - INFO - __main__ - ['not_equivalent']
06/19/2022 17:56:26 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 17:56:26 - INFO - __main__ - ['not_equivalent']
06/19/2022 17:56:26 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 17:56:26 - INFO - __main__ - ['not_equivalent']
06/19/2022 17:56:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 17:56:26 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:56:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:56:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:56:26 - INFO - __main__ - Printing 3 examples
06/19/2022 17:56:26 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 17:56:26 - INFO - __main__ - ['not_equivalent']
06/19/2022 17:56:26 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 17:56:26 - INFO - __main__ - ['not_equivalent']
06/19/2022 17:56:26 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 17:56:26 - INFO - __main__ - ['not_equivalent']
06/19/2022 17:56:26 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:56:26 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:56:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 17:56:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 17:56:26 - INFO - __main__ - Printing 3 examples
06/19/2022 17:56:26 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 17:56:26 - INFO - __main__ - ['not_equivalent']
06/19/2022 17:56:26 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 17:56:26 - INFO - __main__ - ['not_equivalent']
06/19/2022 17:56:26 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 17:56:26 - INFO - __main__ - ['not_equivalent']
06/19/2022 17:56:26 - INFO - __main__ - Tokenizing Input ...
06/19/2022 17:56:26 - INFO - __main__ - Tokenizing Output ...
06/19/2022 17:56:26 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:56:26 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 17:56:32 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 17:56:32 - INFO - __main__ - task name: glue-mrpc
06/19/2022 17:56:32 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 17:56:32 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(1219, 784566), (205, 3175490), (3178, 338316), (2767, 377111), (2421, 410967), (1481, 640748), (924, 843157), (5916, 171503), (566, 946306), (5534, 158824), (4682, 222111), (360, 2496612), (333, 2670525), (958, 890939), (2537, 417981), (2569, 396692), (5644, 185115), (6480, 156027), (326, 2947559), (6313, 166539), (2123, 459189), (3488, 171592), (4602, 223532), (1752, 415773), (3177, 209086), (6847, 159087), (3213, 334916), (68, 12242894), (1729, 565725), (3341, 221693), (3798, 276026), (3041, 335480), (986, 581568), (2389, 425302), (3832, 279541), (1023, 843875), (725, 898187), (4260, 246836), (1139, 870475), (4205, 261446), (3702, 272314), (6894, 155616), (3001, 351252), (499, 1766405), (3731, 203441), (6478, 174299), (1287, 709613), (4382, 247856), (851, 1026262), (5946, 169484), (1409, 319874), (3527, 259912), (6373, 162161), (1640, 475640), (801, 1149556), (357, 1674058), (2402, 404464), (2536, 323855), (887, 1021213), (2386, 396013), (1103, 849279), (4237, 246742), (3032, 335423), (2922, 206677), (3889, 257596), (1683, 558497), (4149, 253554), (3634, 263727), (1955, 437347), (2891, 347678), (770, 1128621), (6298, 154205), (1904, 528496), (5956, 175083), (2943, 377167), (1220, 555589), (5103, 202299), (4313, 246924), (2734, 344156), (5735, 168028), (2404, 417090), (3626, 290198), (605, 1425064), (2324, 403572), (235, 2453612), (2733, 298680), (4166, 233368), (2422, 345969), (676, 1204693), (2272, 432795), (4569, 164817), (3285, 299538), (2136, 430918), (5166, 187635), (2040, 237038), (5037, 203882), (1556, 625505), (2836, 350538), (2079, 374647)]
initialize from c4
[(1219, 784566), (205, 3175490), (3178, 338316), (2767, 377111), (2421, 410967), (1481, 640748), (924, 843157), (5916, 171503), (566, 946306), (5534, 158824), (4682, 222111), (360, 2496612), (333, 2670525), (958, 890939), (2537, 417981), (2569, 396692), (5644, 185115), (6480, 156027), (326, 2947559), (6313, 166539), (2123, 459189), (3488, 171592), (4602, 223532), (1752, 415773), (3177, 209086), (6847, 159087), (3213, 334916), (68, 12242894), (1729, 565725), (3341, 221693), (3798, 276026), (3041, 335480), (986, 581568), (2389, 425302), (3832, 279541), (1023, 843875), (725, 898187), (4260, 246836), (1139, 870475), (4205, 261446), (3702, 272314), (6894, 155616), (3001, 351252), (499, 1766405), (3731, 203441), (6478, 174299), (1287, 709613), (4382, 247856), (851, 1026262), (5946, 169484), (1409, 319874), (3527, 259912), (6373, 162161), (1640, 475640), (801, 1149556), (357, 1674058), (2402, 404464), (2536, 323855), (887, 1021213), (2386, 396013), (1103, 849279), (4237, 246742), (3032, 335423), (2922, 206677), (3889, 257596), (1683, 558497), (4149, 253554), (3634, 263727), (1955, 437347), (2891, 347678), (770, 1128621), (6298, 154205), (1904, 528496), (5956, 175083), (2943, 377167), (1220, 555589), (5103, 202299), (4313, 246924), (2734, 344156), (5735, 168028), (2404, 417090), (3626, 290198), (605, 1425064), (2324, 403572), (235, 2453612), (2733, 298680), (4166, 233368), (2422, 345969), (676, 1204693), (2272, 432795), (4569, 164817), (3285, 299538), (2136, 430918), (5166, 187635), (2040, 237038), (5037, 203882), (1556, 625505), (2836, 350538), (2079, 374647)]
06/19/2022 17:56:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:56:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 17:56:32 - INFO - __main__ - Starting training!
06/19/2022 17:56:32 - INFO - __main__ - Starting training!
06/19/2022 17:56:34 - INFO - __main__ - Step 10 Global step 10 Train loss 5.15 on epoch=4
06/19/2022 17:56:36 - INFO - __main__ - Step 20 Global step 20 Train loss 1.77 on epoch=9
06/19/2022 17:56:37 - INFO - __main__ - Step 30 Global step 30 Train loss 1.32 on epoch=14
06/19/2022 17:56:38 - INFO - __main__ - Step 40 Global step 40 Train loss 3.77 on epoch=19
06/19/2022 17:56:40 - INFO - __main__ - Step 50 Global step 50 Train loss 5.90 on epoch=24
06/19/2022 17:56:50 - INFO - __main__ - Global step 50 Train loss 3.58 ACC 0.0 on epoch=24
06/19/2022 17:56:50 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 17:56:51 - INFO - __main__ - Step 60 Global step 60 Train loss 6.35 on epoch=29
06/19/2022 17:56:53 - INFO - __main__ - Step 70 Global step 70 Train loss 5.78 on epoch=34
06/19/2022 17:56:54 - INFO - __main__ - Step 80 Global step 80 Train loss 5.93 on epoch=39
06/19/2022 17:56:55 - INFO - __main__ - Step 90 Global step 90 Train loss 5.85 on epoch=44
06/19/2022 17:56:57 - INFO - __main__ - Step 100 Global step 100 Train loss 6.06 on epoch=49
06/19/2022 17:57:03 - INFO - __main__ - Global step 100 Train loss 6.00 ACC 0.0 on epoch=49
06/19/2022 17:57:04 - INFO - __main__ - Step 110 Global step 110 Train loss 5.99 on epoch=54
06/19/2022 17:57:06 - INFO - __main__ - Step 120 Global step 120 Train loss 6.17 on epoch=59
06/19/2022 17:57:07 - INFO - __main__ - Step 130 Global step 130 Train loss 6.21 on epoch=64
06/19/2022 17:57:08 - INFO - __main__ - Step 140 Global step 140 Train loss 6.27 on epoch=69
06/19/2022 17:57:09 - INFO - __main__ - Step 150 Global step 150 Train loss 6.08 on epoch=74
06/19/2022 17:57:20 - INFO - __main__ - Global step 150 Train loss 6.14 ACC 0.0 on epoch=74
06/19/2022 17:57:21 - INFO - __main__ - Step 160 Global step 160 Train loss 6.07 on epoch=79
06/19/2022 17:57:22 - INFO - __main__ - Step 170 Global step 170 Train loss 6.04 on epoch=84
06/19/2022 17:57:24 - INFO - __main__ - Step 180 Global step 180 Train loss 6.31 on epoch=89
06/19/2022 17:57:25 - INFO - __main__ - Step 190 Global step 190 Train loss 6.26 on epoch=94
06/19/2022 17:57:26 - INFO - __main__ - Step 200 Global step 200 Train loss 6.26 on epoch=99
06/19/2022 17:57:29 - INFO - __main__ - Global step 200 Train loss 6.19 ACC 0.0 on epoch=99
06/19/2022 17:57:30 - INFO - __main__ - Step 210 Global step 210 Train loss 6.40 on epoch=104
06/19/2022 17:57:32 - INFO - __main__ - Step 220 Global step 220 Train loss 6.27 on epoch=109
06/19/2022 17:57:33 - INFO - __main__ - Step 230 Global step 230 Train loss 6.29 on epoch=114
06/19/2022 17:57:34 - INFO - __main__ - Step 240 Global step 240 Train loss 6.28 on epoch=119
06/19/2022 17:57:36 - INFO - __main__ - Step 250 Global step 250 Train loss 6.30 on epoch=124
06/19/2022 17:57:44 - INFO - __main__ - Global step 250 Train loss 6.31 ACC 0.0 on epoch=124
06/19/2022 17:57:45 - INFO - __main__ - Step 260 Global step 260 Train loss 6.21 on epoch=129
06/19/2022 17:57:46 - INFO - __main__ - Step 270 Global step 270 Train loss 6.26 on epoch=134
06/19/2022 17:57:48 - INFO - __main__ - Step 280 Global step 280 Train loss 6.11 on epoch=139
06/19/2022 17:57:49 - INFO - __main__ - Step 290 Global step 290 Train loss 6.10 on epoch=144
06/19/2022 17:57:50 - INFO - __main__ - Step 300 Global step 300 Train loss 6.16 on epoch=149
06/19/2022 17:57:52 - INFO - __main__ - Global step 300 Train loss 6.17 ACC 0.0 on epoch=149
06/19/2022 17:57:53 - INFO - __main__ - Step 310 Global step 310 Train loss 6.19 on epoch=154
06/19/2022 17:57:55 - INFO - __main__ - Step 320 Global step 320 Train loss 6.14 on epoch=159
06/19/2022 17:57:56 - INFO - __main__ - Step 330 Global step 330 Train loss 6.11 on epoch=164
06/19/2022 17:57:57 - INFO - __main__ - Step 340 Global step 340 Train loss 6.15 on epoch=169
06/19/2022 17:57:59 - INFO - __main__ - Step 350 Global step 350 Train loss 6.29 on epoch=174
06/19/2022 17:58:06 - INFO - __main__ - Global step 350 Train loss 6.18 ACC 0.0 on epoch=174
06/19/2022 17:58:07 - INFO - __main__ - Step 360 Global step 360 Train loss 6.23 on epoch=179
06/19/2022 17:58:09 - INFO - __main__ - Step 370 Global step 370 Train loss 6.18 on epoch=184
06/19/2022 17:58:10 - INFO - __main__ - Step 380 Global step 380 Train loss 6.32 on epoch=189
06/19/2022 17:58:11 - INFO - __main__ - Step 390 Global step 390 Train loss 6.44 on epoch=194
06/19/2022 17:58:13 - INFO - __main__ - Step 400 Global step 400 Train loss 6.38 on epoch=199
06/19/2022 17:58:15 - INFO - __main__ - Global step 400 Train loss 6.31 ACC 0.0 on epoch=199
06/19/2022 17:58:17 - INFO - __main__ - Step 410 Global step 410 Train loss 6.39 on epoch=204
06/19/2022 17:58:18 - INFO - __main__ - Step 420 Global step 420 Train loss 6.38 on epoch=209
06/19/2022 17:58:19 - INFO - __main__ - Step 430 Global step 430 Train loss 6.42 on epoch=214
06/19/2022 17:58:21 - INFO - __main__ - Step 440 Global step 440 Train loss 6.44 on epoch=219
06/19/2022 17:58:22 - INFO - __main__ - Step 450 Global step 450 Train loss 6.46 on epoch=224
06/19/2022 17:58:28 - INFO - __main__ - Global step 450 Train loss 6.42 ACC 0.0 on epoch=224
06/19/2022 17:58:30 - INFO - __main__ - Step 460 Global step 460 Train loss 6.43 on epoch=229
06/19/2022 17:58:31 - INFO - __main__ - Step 470 Global step 470 Train loss 6.38 on epoch=234
06/19/2022 17:58:33 - INFO - __main__ - Step 480 Global step 480 Train loss 6.41 on epoch=239
06/19/2022 17:58:34 - INFO - __main__ - Step 490 Global step 490 Train loss 6.41 on epoch=244
06/19/2022 17:58:35 - INFO - __main__ - Step 500 Global step 500 Train loss 6.38 on epoch=249
06/19/2022 17:58:37 - INFO - __main__ - Global step 500 Train loss 6.40 ACC 0.0 on epoch=249
06/19/2022 17:58:39 - INFO - __main__ - Step 510 Global step 510 Train loss 6.28 on epoch=254
06/19/2022 17:58:40 - INFO - __main__ - Step 520 Global step 520 Train loss 6.36 on epoch=259
06/19/2022 17:58:42 - INFO - __main__ - Step 530 Global step 530 Train loss 6.35 on epoch=264
06/19/2022 17:58:43 - INFO - __main__ - Step 540 Global step 540 Train loss 6.30 on epoch=269
06/19/2022 17:58:44 - INFO - __main__ - Step 550 Global step 550 Train loss 6.31 on epoch=274
06/19/2022 17:58:56 - INFO - __main__ - Global step 550 Train loss 6.32 ACC 0.0 on epoch=274
06/19/2022 17:58:57 - INFO - __main__ - Step 560 Global step 560 Train loss 6.35 on epoch=279
06/19/2022 17:58:58 - INFO - __main__ - Step 570 Global step 570 Train loss 6.36 on epoch=284
06/19/2022 17:59:00 - INFO - __main__ - Step 580 Global step 580 Train loss 6.24 on epoch=289
06/19/2022 17:59:01 - INFO - __main__ - Step 590 Global step 590 Train loss 6.28 on epoch=294
06/19/2022 17:59:02 - INFO - __main__ - Step 600 Global step 600 Train loss 6.20 on epoch=299
06/19/2022 17:59:10 - INFO - __main__ - Global step 600 Train loss 6.29 ACC 0.0 on epoch=299
06/19/2022 17:59:11 - INFO - __main__ - Step 610 Global step 610 Train loss 6.21 on epoch=304
06/19/2022 17:59:12 - INFO - __main__ - Step 620 Global step 620 Train loss 6.25 on epoch=309
06/19/2022 17:59:13 - INFO - __main__ - Step 630 Global step 630 Train loss 6.15 on epoch=314
06/19/2022 17:59:15 - INFO - __main__ - Step 640 Global step 640 Train loss 6.18 on epoch=319
06/19/2022 17:59:16 - INFO - __main__ - Step 650 Global step 650 Train loss 6.05 on epoch=324
06/19/2022 17:59:18 - INFO - __main__ - Global step 650 Train loss 6.17 ACC 0.0 on epoch=324
06/19/2022 17:59:20 - INFO - __main__ - Step 660 Global step 660 Train loss 6.12 on epoch=329
06/19/2022 17:59:21 - INFO - __main__ - Step 670 Global step 670 Train loss 5.86 on epoch=334
06/19/2022 17:59:22 - INFO - __main__ - Step 680 Global step 680 Train loss 5.98 on epoch=339
06/19/2022 17:59:24 - INFO - __main__ - Step 690 Global step 690 Train loss 5.94 on epoch=344
06/19/2022 17:59:25 - INFO - __main__ - Step 700 Global step 700 Train loss 5.92 on epoch=349
06/19/2022 17:59:26 - INFO - __main__ - Global step 700 Train loss 5.97 ACC 0.0 on epoch=349
06/19/2022 17:59:27 - INFO - __main__ - Step 710 Global step 710 Train loss 6.03 on epoch=354
06/19/2022 17:59:29 - INFO - __main__ - Step 720 Global step 720 Train loss 5.98 on epoch=359
06/19/2022 17:59:30 - INFO - __main__ - Step 730 Global step 730 Train loss 5.86 on epoch=364
06/19/2022 17:59:31 - INFO - __main__ - Step 740 Global step 740 Train loss 6.17 on epoch=369
06/19/2022 17:59:33 - INFO - __main__ - Step 750 Global step 750 Train loss 6.27 on epoch=374
06/19/2022 17:59:36 - INFO - __main__ - Global step 750 Train loss 6.06 ACC 0.0 on epoch=374
06/19/2022 17:59:37 - INFO - __main__ - Step 760 Global step 760 Train loss 6.13 on epoch=379
06/19/2022 17:59:38 - INFO - __main__ - Step 770 Global step 770 Train loss 6.25 on epoch=384
06/19/2022 17:59:40 - INFO - __main__ - Step 780 Global step 780 Train loss 6.15 on epoch=389
06/19/2022 17:59:41 - INFO - __main__ - Step 790 Global step 790 Train loss 6.19 on epoch=394
06/19/2022 17:59:43 - INFO - __main__ - Step 800 Global step 800 Train loss 6.17 on epoch=399
06/19/2022 17:59:45 - INFO - __main__ - Global step 800 Train loss 6.18 ACC 0.0 on epoch=399
06/19/2022 17:59:46 - INFO - __main__ - Step 810 Global step 810 Train loss 6.09 on epoch=404
06/19/2022 17:59:47 - INFO - __main__ - Step 820 Global step 820 Train loss 6.16 on epoch=409
06/19/2022 17:59:48 - INFO - __main__ - Step 830 Global step 830 Train loss 6.22 on epoch=414
06/19/2022 17:59:50 - INFO - __main__ - Step 840 Global step 840 Train loss 6.19 on epoch=419
06/19/2022 17:59:51 - INFO - __main__ - Step 850 Global step 850 Train loss 6.17 on epoch=424
06/19/2022 17:59:53 - INFO - __main__ - Global step 850 Train loss 6.16 ACC 0.0 on epoch=424
06/19/2022 17:59:54 - INFO - __main__ - Step 860 Global step 860 Train loss 6.16 on epoch=429
06/19/2022 17:59:56 - INFO - __main__ - Step 870 Global step 870 Train loss 6.16 on epoch=434
06/19/2022 17:59:58 - INFO - __main__ - Step 880 Global step 880 Train loss 6.08 on epoch=439
06/19/2022 17:59:59 - INFO - __main__ - Step 890 Global step 890 Train loss 6.18 on epoch=444
06/19/2022 18:00:01 - INFO - __main__ - Step 900 Global step 900 Train loss 6.08 on epoch=449
06/19/2022 18:00:04 - INFO - __main__ - Global step 900 Train loss 6.13 ACC 0.0 on epoch=449
06/19/2022 18:00:06 - INFO - __main__ - Step 910 Global step 910 Train loss 6.04 on epoch=454
06/19/2022 18:00:07 - INFO - __main__ - Step 920 Global step 920 Train loss 6.07 on epoch=459
06/19/2022 18:00:09 - INFO - __main__ - Step 930 Global step 930 Train loss 6.03 on epoch=464
06/19/2022 18:00:10 - INFO - __main__ - Step 940 Global step 940 Train loss 6.05 on epoch=469
06/19/2022 18:00:11 - INFO - __main__ - Step 950 Global step 950 Train loss 5.97 on epoch=474
06/19/2022 18:00:14 - INFO - __main__ - Global step 950 Train loss 6.03 ACC 0.0 on epoch=474
06/19/2022 18:00:16 - INFO - __main__ - Step 960 Global step 960 Train loss 6.08 on epoch=479
06/19/2022 18:00:17 - INFO - __main__ - Step 970 Global step 970 Train loss 6.08 on epoch=484
06/19/2022 18:00:18 - INFO - __main__ - Step 980 Global step 980 Train loss 6.00 on epoch=489
06/19/2022 18:00:20 - INFO - __main__ - Step 990 Global step 990 Train loss 6.04 on epoch=494
06/19/2022 18:00:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 6.01 on epoch=499
06/19/2022 18:00:23 - INFO - __main__ - Global step 1000 Train loss 6.04 ACC 0.0 on epoch=499
06/19/2022 18:00:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 5.98 on epoch=504
06/19/2022 18:00:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 6.05 on epoch=509
06/19/2022 18:00:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 5.87 on epoch=514
06/19/2022 18:00:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 5.97 on epoch=519
06/19/2022 18:00:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 5.85 on epoch=524
06/19/2022 18:00:34 - INFO - __main__ - Global step 1050 Train loss 5.94 ACC 0.0 on epoch=524
06/19/2022 18:00:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 5.86 on epoch=529
06/19/2022 18:00:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 5.75 on epoch=534
06/19/2022 18:00:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 5.67 on epoch=539
06/19/2022 18:00:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 5.72 on epoch=544
06/19/2022 18:00:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 5.46 on epoch=549
06/19/2022 18:00:42 - INFO - __main__ - Global step 1100 Train loss 5.69 ACC 0.0 on epoch=549
06/19/2022 18:00:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 5.43 on epoch=554
06/19/2022 18:00:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 5.42 on epoch=559
06/19/2022 18:00:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 5.29 on epoch=564
06/19/2022 18:00:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 5.19 on epoch=569
06/19/2022 18:00:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 5.24 on epoch=574
06/19/2022 18:00:51 - INFO - __main__ - Global step 1150 Train loss 5.31 ACC 0.0 on epoch=574
06/19/2022 18:00:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 5.18 on epoch=579
06/19/2022 18:00:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 5.14 on epoch=584
06/19/2022 18:00:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 4.98 on epoch=589
06/19/2022 18:00:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 5.01 on epoch=594
06/19/2022 18:00:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 5.13 on epoch=599
06/19/2022 18:01:02 - INFO - __main__ - Global step 1200 Train loss 5.09 ACC 0.0 on epoch=599
06/19/2022 18:01:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 4.99 on epoch=604
06/19/2022 18:01:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 4.95 on epoch=609
06/19/2022 18:01:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 4.87 on epoch=614
06/19/2022 18:01:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 4.90 on epoch=619
06/19/2022 18:01:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 4.75 on epoch=624
06/19/2022 18:01:10 - INFO - __main__ - Global step 1250 Train loss 4.89 ACC 0.0 on epoch=624
06/19/2022 18:01:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 4.78 on epoch=629
06/19/2022 18:01:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 4.59 on epoch=634
06/19/2022 18:01:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 4.75 on epoch=639
06/19/2022 18:01:15 - INFO - __main__ - Step 1290 Global step 1290 Train loss 4.72 on epoch=644
06/19/2022 18:01:16 - INFO - __main__ - Step 1300 Global step 1300 Train loss 4.52 on epoch=649
06/19/2022 18:01:19 - INFO - __main__ - Global step 1300 Train loss 4.67 ACC 0.0 on epoch=649
06/19/2022 18:01:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 4.59 on epoch=654
06/19/2022 18:01:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 4.54 on epoch=659
06/19/2022 18:01:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 4.50 on epoch=664
06/19/2022 18:01:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 4.45 on epoch=669
06/19/2022 18:01:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 4.43 on epoch=674
06/19/2022 18:01:26 - INFO - __main__ - Global step 1350 Train loss 4.50 ACC 0.0 on epoch=674
06/19/2022 18:01:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 4.38 on epoch=679
06/19/2022 18:01:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 4.41 on epoch=684
06/19/2022 18:01:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 4.40 on epoch=689
06/19/2022 18:01:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 4.37 on epoch=694
06/19/2022 18:01:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 4.19 on epoch=699
06/19/2022 18:01:35 - INFO - __main__ - Global step 1400 Train loss 4.35 ACC 0.0 on epoch=699
06/19/2022 18:01:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 4.25 on epoch=704
06/19/2022 18:01:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 4.24 on epoch=709
06/19/2022 18:01:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 4.16 on epoch=714
06/19/2022 18:01:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 4.16 on epoch=719
06/19/2022 18:01:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 4.18 on epoch=724
06/19/2022 18:01:43 - INFO - __main__ - Global step 1450 Train loss 4.20 ACC 0.0 on epoch=724
06/19/2022 18:01:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 4.12 on epoch=729
06/19/2022 18:01:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 4.05 on epoch=734
06/19/2022 18:01:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 4.05 on epoch=739
06/19/2022 18:01:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 3.99 on epoch=744
06/19/2022 18:01:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 3.97 on epoch=749
06/19/2022 18:01:52 - INFO - __main__ - Global step 1500 Train loss 4.04 ACC 0.0 on epoch=749
06/19/2022 18:01:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 3.99 on epoch=754
06/19/2022 18:01:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 4.02 on epoch=759
06/19/2022 18:01:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 4.03 on epoch=764
06/19/2022 18:01:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 3.98 on epoch=769
06/19/2022 18:02:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 3.96 on epoch=774
06/19/2022 18:02:01 - INFO - __main__ - Global step 1550 Train loss 4.00 ACC 0.0 on epoch=774
06/19/2022 18:02:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 3.86 on epoch=779
06/19/2022 18:02:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 3.83 on epoch=784
06/19/2022 18:02:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 3.88 on epoch=789
06/19/2022 18:02:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 3.95 on epoch=794
06/19/2022 18:02:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 3.73 on epoch=799
06/19/2022 18:02:10 - INFO - __main__ - Global step 1600 Train loss 3.85 ACC 0.0 on epoch=799
06/19/2022 18:02:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 3.70 on epoch=804
06/19/2022 18:02:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 3.74 on epoch=809
06/19/2022 18:02:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 3.84 on epoch=814
06/19/2022 18:02:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 3.83 on epoch=819
06/19/2022 18:02:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 3.75 on epoch=824
06/19/2022 18:02:20 - INFO - __main__ - Global step 1650 Train loss 3.77 ACC 0.0 on epoch=824
06/19/2022 18:02:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 3.77 on epoch=829
06/19/2022 18:02:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 3.70 on epoch=834
06/19/2022 18:02:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 3.62 on epoch=839
06/19/2022 18:02:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 3.71 on epoch=844
06/19/2022 18:02:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 3.68 on epoch=849
06/19/2022 18:02:30 - INFO - __main__ - Global step 1700 Train loss 3.70 ACC 0.0 on epoch=849
06/19/2022 18:02:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 3.57 on epoch=854
06/19/2022 18:02:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 3.52 on epoch=859
06/19/2022 18:02:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 3.57 on epoch=864
06/19/2022 18:02:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 3.43 on epoch=869
06/19/2022 18:02:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 3.47 on epoch=874
06/19/2022 18:02:43 - INFO - __main__ - Global step 1750 Train loss 3.51 ACC 0.0 on epoch=874
06/19/2022 18:02:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 3.50 on epoch=879
06/19/2022 18:02:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 3.48 on epoch=884
06/19/2022 18:02:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 3.51 on epoch=889
06/19/2022 18:02:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 3.44 on epoch=894
06/19/2022 18:02:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 3.38 on epoch=899
06/19/2022 18:02:52 - INFO - __main__ - Global step 1800 Train loss 3.46 ACC 0.0 on epoch=899
06/19/2022 18:02:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 3.43 on epoch=904
06/19/2022 18:02:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 3.28 on epoch=909
06/19/2022 18:02:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 3.37 on epoch=914
06/19/2022 18:02:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 3.28 on epoch=919
06/19/2022 18:02:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 3.26 on epoch=924
06/19/2022 18:03:01 - INFO - __main__ - Global step 1850 Train loss 3.32 ACC 0.0 on epoch=924
06/19/2022 18:03:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 3.10 on epoch=929
06/19/2022 18:03:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 3.10 on epoch=934
06/19/2022 18:03:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 3.14 on epoch=939
06/19/2022 18:03:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 3.06 on epoch=944
06/19/2022 18:03:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 3.00 on epoch=949
06/19/2022 18:03:09 - INFO - __main__ - Global step 1900 Train loss 3.08 ACC 0.0 on epoch=949
06/19/2022 18:03:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 3.08 on epoch=954
06/19/2022 18:03:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 3.08 on epoch=959
06/19/2022 18:03:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 2.87 on epoch=964
06/19/2022 18:03:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 2.77 on epoch=969
06/19/2022 18:03:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 2.72 on epoch=974
06/19/2022 18:03:27 - INFO - __main__ - Global step 1950 Train loss 2.91 ACC 0.0 on epoch=974
06/19/2022 18:03:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 2.85 on epoch=979
06/19/2022 18:03:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 2.64 on epoch=984
06/19/2022 18:03:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 2.65 on epoch=989
06/19/2022 18:03:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 2.74 on epoch=994
06/19/2022 18:03:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 2.61 on epoch=999
06/19/2022 18:03:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:03:34 - INFO - __main__ - Printing 3 examples
06/19/2022 18:03:34 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 18:03:34 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:03:34 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 18:03:34 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:03:34 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 18:03:34 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:03:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 18:03:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:03:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:03:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:03:34 - INFO - __main__ - Printing 3 examples
06/19/2022 18:03:34 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 18:03:34 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:03:34 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 18:03:34 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:03:34 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 18:03:34 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:03:34 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:03:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:03:34 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:03:40 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:03:40 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(6579, 166537), (5880, 173333), (2252, 353035), (6881, 160252), (4773, 218071), (6404, 160401), (3371, 234531), (4187, 259804), (1925, 417452), (968, 645839), (2729, 183765), (5538, 189511), (987, 914447), (144, 1635131), (1079, 796417), (1653, 586845), (1731, 565272), (3424, 221936), (5680, 157495), (731, 1250799), (4298, 243616), (3238, 245783), (6415, 157560), (2406, 200208), (5775, 177401), (2721, 367851), (6309, 169174), (135, 7037199), (1214, 767509), (5962, 174066), (2867, 348102), (3209, 276527), (1189, 784250), (891, 319194), (5015, 214805), (1701, 571183), (3845, 206498), (26, 28035103), (2809, 352164), (5121, 187048), (1892, 508575), (3574, 184323), (996, 821524), (3263, 314166), (6363, 154270), (90, 459181), (4226, 250683), (1751, 560750), (5058, 173227), (5205, 176939), (7, 158399866), (6008, 156882), (3474, 290209), (3351, 190868), (225, 4092648), (1142, 782542), (154, 259007), (3278, 305705), (2637, 387504), (532, 1388213), (2597, 385302), (6237, 164778), (851, 1026262), (793, 959415), (2719, 191820), (788, 1153138), (4209, 175746), (1393, 703212), (1438, 688034), (5340, 197066), (3995, 170269), (1376, 549699), (2909, 350152), (5773, 177903), (6551, 154998), (4799, 221150), (2335, 431458), (6082, 173261), (1824, 455362), (3242, 301888), (4627, 234647), (3797, 250903), (4027, 213331), (6464, 181012), (5210, 207673), (1137, 734426), (2765, 373166), (2451, 408517), (708, 1240422), (3108, 278320), (243, 3854645), (4830, 159383), (5630, 169060), (2134, 401363), (4490, 159360), (2342, 415802), (80, 11392202), (758, 1131386), (652, 1360888)]
06/19/2022 18:03:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:03:41 - INFO - __main__ - Starting training!
06/19/2022 18:03:43 - INFO - __main__ - Global step 2000 Train loss 2.70 ACC 0.0 on epoch=999
06/19/2022 18:03:43 - INFO - __main__ - save last model!
06/19/2022 18:03:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 18:03:43 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 18:03:43 - INFO - __main__ - Printing 3 examples
06/19/2022 18:03:43 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 18:03:43 - INFO - __main__ - ['equivalent']
06/19/2022 18:03:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 18:03:43 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:03:43 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 18:03:43 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:03:43 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:03:44 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:03:44 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 18:05:40 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_100_0.5_8_predictions.txt
06/19/2022 18:05:40 - INFO - __main__ - ACC on test data: 0.0000
06/19/2022 18:05:40 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.5, bsz=8, dev_performance=0.0, test_performance=0.0
06/19/2022 18:05:40 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.4, bsz=8 ...
06/19/2022 18:05:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:05:41 - INFO - __main__ - Printing 3 examples
06/19/2022 18:05:41 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 18:05:41 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:05:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 18:05:41 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:05:41 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 18:05:41 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:05:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 18:05:41 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:05:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:05:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:05:41 - INFO - __main__ - Printing 3 examples
06/19/2022 18:05:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 18:05:41 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:05:41 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 18:05:41 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:05:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 18:05:41 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:05:41 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:05:41 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:05:41 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:05:47 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:05:47 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(6579, 166537), (5880, 173333), (2252, 353035), (6881, 160252), (4773, 218071), (6404, 160401), (3371, 234531), (4187, 259804), (1925, 417452), (968, 645839), (2729, 183765), (5538, 189511), (987, 914447), (144, 1635131), (1079, 796417), (1653, 586845), (1731, 565272), (3424, 221936), (5680, 157495), (731, 1250799), (4298, 243616), (3238, 245783), (6415, 157560), (2406, 200208), (5775, 177401), (2721, 367851), (6309, 169174), (135, 7037199), (1214, 767509), (5962, 174066), (2867, 348102), (3209, 276527), (1189, 784250), (891, 319194), (5015, 214805), (1701, 571183), (3845, 206498), (26, 28035103), (2809, 352164), (5121, 187048), (1892, 508575), (3574, 184323), (996, 821524), (3263, 314166), (6363, 154270), (90, 459181), (4226, 250683), (1751, 560750), (5058, 173227), (5205, 176939), (7, 158399866), (6008, 156882), (3474, 290209), (3351, 190868), (225, 4092648), (1142, 782542), (154, 259007), (3278, 305705), (2637, 387504), (532, 1388213), (2597, 385302), (6237, 164778), (851, 1026262), (793, 959415), (2719, 191820), (788, 1153138), (4209, 175746), (1393, 703212), (1438, 688034), (5340, 197066), (3995, 170269), (1376, 549699), (2909, 350152), (5773, 177903), (6551, 154998), (4799, 221150), (2335, 431458), (6082, 173261), (1824, 455362), (3242, 301888), (4627, 234647), (3797, 250903), (4027, 213331), (6464, 181012), (5210, 207673), (1137, 734426), (2765, 373166), (2451, 408517), (708, 1240422), (3108, 278320), (243, 3854645), (4830, 159383), (5630, 169060), (2134, 401363), (4490, 159360), (2342, 415802), (80, 11392202), (758, 1131386), (652, 1360888)]
06/19/2022 18:05:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:05:47 - INFO - __main__ - Starting training!
06/19/2022 18:05:49 - INFO - __main__ - Step 10 Global step 10 Train loss 5.80 on epoch=4
06/19/2022 18:05:51 - INFO - __main__ - Step 20 Global step 20 Train loss 2.88 on epoch=9
06/19/2022 18:05:52 - INFO - __main__ - Step 30 Global step 30 Train loss 1.27 on epoch=14
06/19/2022 18:05:54 - INFO - __main__ - Step 40 Global step 40 Train loss 0.85 on epoch=19
06/19/2022 18:05:55 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=24
06/19/2022 18:05:56 - INFO - __main__ - Global step 50 Train loss 2.25 ACC 0.5 on epoch=24
06/19/2022 18:05:56 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 18:05:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=29
06/19/2022 18:05:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=34
06/19/2022 18:06:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.31 on epoch=39
06/19/2022 18:06:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.38 on epoch=44
06/19/2022 18:06:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.33 on epoch=49
06/19/2022 18:06:03 - INFO - __main__ - Global step 100 Train loss 0.39 ACC 0.5 on epoch=49
06/19/2022 18:06:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.32 on epoch=54
06/19/2022 18:06:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
06/19/2022 18:06:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=64
06/19/2022 18:06:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=69
06/19/2022 18:06:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=74
06/19/2022 18:06:11 - INFO - __main__ - Global step 150 Train loss 0.30 ACC 0.5 on epoch=74
06/19/2022 18:06:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=79
06/19/2022 18:06:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
06/19/2022 18:06:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=89
06/19/2022 18:06:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=94
06/19/2022 18:06:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
06/19/2022 18:06:18 - INFO - __main__ - Global step 200 Train loss 0.27 ACC 0.5 on epoch=99
06/19/2022 18:06:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.25 on epoch=104
06/19/2022 18:06:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=109
06/19/2022 18:06:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=114
06/19/2022 18:06:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=119
06/19/2022 18:06:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=124
06/19/2022 18:06:25 - INFO - __main__ - Global step 250 Train loss 0.26 ACC 0.5625 on epoch=124
06/19/2022 18:06:25 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=124, global_step=250
06/19/2022 18:06:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.22 on epoch=129
06/19/2022 18:06:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
06/19/2022 18:06:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=139
06/19/2022 18:06:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=144
06/19/2022 18:06:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=149
06/19/2022 18:06:32 - INFO - __main__ - Global step 300 Train loss 0.25 ACC 0.5 on epoch=149
06/19/2022 18:06:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
06/19/2022 18:06:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.21 on epoch=159
06/19/2022 18:06:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=164
06/19/2022 18:06:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.18 on epoch=169
06/19/2022 18:06:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=174
06/19/2022 18:06:41 - INFO - __main__ - Global step 350 Train loss 0.23 ACC 0.59375 on epoch=174
06/19/2022 18:06:41 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=174, global_step=350
06/19/2022 18:06:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=179
06/19/2022 18:06:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.19 on epoch=184
06/19/2022 18:06:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
06/19/2022 18:06:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.19 on epoch=194
06/19/2022 18:06:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=199
06/19/2022 18:06:48 - INFO - __main__ - Global step 400 Train loss 0.22 ACC 0.65625 on epoch=199
06/19/2022 18:06:48 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=199, global_step=400
06/19/2022 18:06:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=204
06/19/2022 18:06:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
06/19/2022 18:06:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.20 on epoch=214
06/19/2022 18:06:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.17 on epoch=219
06/19/2022 18:06:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.20 on epoch=224
06/19/2022 18:06:55 - INFO - __main__ - Global step 450 Train loss 0.21 ACC 0.65625 on epoch=224
06/19/2022 18:06:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=229
06/19/2022 18:06:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=234
06/19/2022 18:06:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=239
06/19/2022 18:07:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=244
06/19/2022 18:07:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=249
06/19/2022 18:07:03 - INFO - __main__ - Global step 500 Train loss 0.21 ACC 0.53125 on epoch=249
06/19/2022 18:07:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.18 on epoch=254
06/19/2022 18:07:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.18 on epoch=259
06/19/2022 18:07:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.16 on epoch=264
06/19/2022 18:07:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=269
06/19/2022 18:07:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.15 on epoch=274
06/19/2022 18:07:10 - INFO - __main__ - Global step 550 Train loss 0.17 ACC 0.65625 on epoch=274
06/19/2022 18:07:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=279
06/19/2022 18:07:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=284
06/19/2022 18:07:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=289
06/19/2022 18:07:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=294
06/19/2022 18:07:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=299
06/19/2022 18:07:17 - INFO - __main__ - Global step 600 Train loss 0.16 ACC 0.5625 on epoch=299
06/19/2022 18:07:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=304
06/19/2022 18:07:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=309
06/19/2022 18:07:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=314
06/19/2022 18:07:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.14 on epoch=319
06/19/2022 18:07:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=324
06/19/2022 18:07:24 - INFO - __main__ - Global step 650 Train loss 0.17 ACC 0.65625 on epoch=324
06/19/2022 18:07:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=329
06/19/2022 18:07:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=334
06/19/2022 18:07:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=339
06/19/2022 18:07:30 - INFO - __main__ - Step 690 Global step 690 Train loss 1.38 on epoch=344
06/19/2022 18:07:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.53 on epoch=349
06/19/2022 18:07:31 - INFO - __main__ - Global step 700 Train loss 0.49 ACC 0.625 on epoch=349
06/19/2022 18:07:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.18 on epoch=354
06/19/2022 18:07:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=359
06/19/2022 18:07:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=364
06/19/2022 18:07:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=369
06/19/2022 18:07:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=374
06/19/2022 18:07:39 - INFO - __main__ - Global step 750 Train loss 0.16 ACC 0.625 on epoch=374
06/19/2022 18:07:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.15 on epoch=379
06/19/2022 18:07:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=384
06/19/2022 18:07:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=389
06/19/2022 18:07:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=394
06/19/2022 18:07:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.16 on epoch=399
06/19/2022 18:07:47 - INFO - __main__ - Global step 800 Train loss 0.14 ACC 0.65625 on epoch=399
06/19/2022 18:07:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=404
06/19/2022 18:07:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=409
06/19/2022 18:07:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=414
06/19/2022 18:07:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=419
06/19/2022 18:07:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=424
06/19/2022 18:07:55 - INFO - __main__ - Global step 850 Train loss 0.13 ACC 0.625 on epoch=424
06/19/2022 18:07:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=429
06/19/2022 18:07:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=434
06/19/2022 18:07:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=439
06/19/2022 18:08:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=444
06/19/2022 18:08:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=449
06/19/2022 18:08:02 - INFO - __main__ - Global step 900 Train loss 0.12 ACC 0.6875 on epoch=449
06/19/2022 18:08:02 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=449, global_step=900
06/19/2022 18:08:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=454
06/19/2022 18:08:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=459
06/19/2022 18:08:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=464
06/19/2022 18:08:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.15 on epoch=469
06/19/2022 18:08:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=474
06/19/2022 18:08:09 - INFO - __main__ - Global step 950 Train loss 0.14 ACC 0.625 on epoch=474
06/19/2022 18:08:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=479
06/19/2022 18:08:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=484
06/19/2022 18:08:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=489
06/19/2022 18:08:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.17 on epoch=494
06/19/2022 18:08:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=499
06/19/2022 18:08:16 - INFO - __main__ - Global step 1000 Train loss 0.13 ACC 0.625 on epoch=499
06/19/2022 18:08:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.12 on epoch=504
06/19/2022 18:08:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=509
06/19/2022 18:08:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=514
06/19/2022 18:08:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=519
06/19/2022 18:08:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=524
06/19/2022 18:08:24 - INFO - __main__ - Global step 1050 Train loss 0.13 ACC 0.65625 on epoch=524
06/19/2022 18:08:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=529
06/19/2022 18:08:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=534
06/19/2022 18:08:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=539
06/19/2022 18:08:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=544
06/19/2022 18:08:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=549
06/19/2022 18:08:31 - INFO - __main__ - Global step 1100 Train loss 0.11 ACC 0.6875 on epoch=549
06/19/2022 18:08:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=554
06/19/2022 18:08:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=559
06/19/2022 18:08:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=564
06/19/2022 18:08:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=569
06/19/2022 18:08:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.14 on epoch=574
06/19/2022 18:08:38 - INFO - __main__ - Global step 1150 Train loss 0.11 ACC 0.625 on epoch=574
06/19/2022 18:08:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=579
06/19/2022 18:08:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=584
06/19/2022 18:08:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=589
06/19/2022 18:08:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=594
06/19/2022 18:08:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=599
06/19/2022 18:08:47 - INFO - __main__ - Global step 1200 Train loss 0.11 ACC 0.65625 on epoch=599
06/19/2022 18:08:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.11 on epoch=604
06/19/2022 18:08:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=609
06/19/2022 18:08:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.10 on epoch=614
06/19/2022 18:08:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=619
06/19/2022 18:08:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=624
06/19/2022 18:08:54 - INFO - __main__ - Global step 1250 Train loss 0.11 ACC 0.65625 on epoch=624
06/19/2022 18:08:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.09 on epoch=629
06/19/2022 18:08:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=634
06/19/2022 18:08:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=639
06/19/2022 18:08:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.09 on epoch=644
06/19/2022 18:09:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=649
06/19/2022 18:09:01 - INFO - __main__ - Global step 1300 Train loss 0.09 ACC 0.5625 on epoch=649
06/19/2022 18:09:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=654
06/19/2022 18:09:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=659
06/19/2022 18:09:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=664
06/19/2022 18:09:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=669
06/19/2022 18:09:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=674
06/19/2022 18:09:09 - INFO - __main__ - Global step 1350 Train loss 0.08 ACC 0.59375 on epoch=674
06/19/2022 18:09:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=679
06/19/2022 18:09:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=684
06/19/2022 18:09:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=689
06/19/2022 18:09:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=694
06/19/2022 18:09:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=699
06/19/2022 18:09:17 - INFO - __main__ - Global step 1400 Train loss 0.09 ACC 0.5625 on epoch=699
06/19/2022 18:09:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=704
06/19/2022 18:09:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=709
06/19/2022 18:09:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=714
06/19/2022 18:09:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=719
06/19/2022 18:09:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=724
06/19/2022 18:09:24 - INFO - __main__ - Global step 1450 Train loss 0.06 ACC 0.625 on epoch=724
06/19/2022 18:09:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
06/19/2022 18:09:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=734
06/19/2022 18:09:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
06/19/2022 18:09:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=744
06/19/2022 18:09:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=749
06/19/2022 18:09:32 - INFO - __main__ - Global step 1500 Train loss 0.05 ACC 0.53125 on epoch=749
06/19/2022 18:09:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=754
06/19/2022 18:09:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=759
06/19/2022 18:09:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=764
06/19/2022 18:09:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=769
06/19/2022 18:09:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=774
06/19/2022 18:09:39 - INFO - __main__ - Global step 1550 Train loss 0.07 ACC 0.5625 on epoch=774
06/19/2022 18:09:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=779
06/19/2022 18:09:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=784
06/19/2022 18:09:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=789
06/19/2022 18:09:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=794
06/19/2022 18:09:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=799
06/19/2022 18:09:46 - INFO - __main__ - Global step 1600 Train loss 0.06 ACC 0.59375 on epoch=799
06/19/2022 18:09:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=804
06/19/2022 18:09:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
06/19/2022 18:09:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=814
06/19/2022 18:09:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
06/19/2022 18:09:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=824
06/19/2022 18:09:54 - INFO - __main__ - Global step 1650 Train loss 0.04 ACC 0.46875 on epoch=824
06/19/2022 18:09:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=829
06/19/2022 18:09:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
06/19/2022 18:09:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=839
06/19/2022 18:09:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=844
06/19/2022 18:10:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/19/2022 18:10:01 - INFO - __main__ - Global step 1700 Train loss 0.04 ACC 0.4375 on epoch=849
06/19/2022 18:10:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
06/19/2022 18:10:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
06/19/2022 18:10:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
06/19/2022 18:10:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
06/19/2022 18:10:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=874
06/19/2022 18:10:09 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.5625 on epoch=874
06/19/2022 18:10:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=879
06/19/2022 18:10:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=884
06/19/2022 18:10:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
06/19/2022 18:10:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
06/19/2022 18:10:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
06/19/2022 18:10:15 - INFO - __main__ - Global step 1800 Train loss 0.04 ACC 0.46875 on epoch=899
06/19/2022 18:10:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
06/19/2022 18:10:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=909
06/19/2022 18:10:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/19/2022 18:10:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/19/2022 18:10:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
06/19/2022 18:10:22 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.5 on epoch=924
06/19/2022 18:10:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/19/2022 18:10:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/19/2022 18:10:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
06/19/2022 18:10:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/19/2022 18:10:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
06/19/2022 18:10:30 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.5 on epoch=949
06/19/2022 18:10:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
06/19/2022 18:10:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/19/2022 18:10:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=964
06/19/2022 18:10:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/19/2022 18:10:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
06/19/2022 18:10:38 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.625 on epoch=974
06/19/2022 18:10:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/19/2022 18:10:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/19/2022 18:10:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/19/2022 18:10:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
06/19/2022 18:10:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/19/2022 18:10:46 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.5 on epoch=999
06/19/2022 18:10:46 - INFO - __main__ - save last model!
06/19/2022 18:10:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 18:10:46 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 18:10:46 - INFO - __main__ - Printing 3 examples
06/19/2022 18:10:46 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 18:10:46 - INFO - __main__ - ['equivalent']
06/19/2022 18:10:46 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 18:10:46 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:46 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 18:10:46 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:46 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:10:46 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:10:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:10:47 - INFO - __main__ - Printing 3 examples
06/19/2022 18:10:47 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 18:10:47 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:47 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 18:10:47 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:47 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 18:10:47 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 18:10:47 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:10:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:10:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:10:47 - INFO - __main__ - Printing 3 examples
06/19/2022 18:10:47 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 18:10:47 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:47 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 18:10:47 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:47 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 18:10:47 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:47 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:10:47 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:10:47 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:10:47 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 18:10:53 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:10:53 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(2594, 403711), (738, 1187017), (73, 2525281), (3583, 284375), (773, 1132610), (2099, 182268), (1757, 390014), (3023, 334230), (7161, 192247), (2231, 427234), (6082, 173261), (1461, 672164), (3573, 163617), (6209, 162555), (5259, 197913), (2697, 380850), (4225, 197872), (2319, 230472), (1994, 479967), (993, 886924), (1069, 868644), (4404, 216707), (4001, 264258), (4585, 221032), (4434, 228933), (5229, 200589), (629, 1463788), (1101, 860462), (661, 1317687), (3147, 232857), (2132, 277175), (1102, 798392), (2542, 371822), (2503, 477739), (2036, 480551), (5480, 174200), (4952, 209263), (397, 637384), (4682, 222111), (2030, 497571), (5234, 202177), (2665, 369855), (757, 1071575), (4370, 210982), (4895, 169946), (1085, 862457), (705, 1548305), (4508, 172897), (167, 5559739), (1112, 892303), (2193, 392132), (1396, 545283), (4374, 230866), (3385, 191872), (5816, 194070), (2358, 427447), (4191, 233374), (646, 1377071), (1790, 550995), (4264, 247591), (22, 38484130), (2415, 239286), (2392, 349952), (4190, 205854), (2833, 327549), (5032, 221170), (6353, 168404), (5324, 191543), (1518, 582487), (1894, 481117), (3228, 315556), (2265, 421208), (636, 1378362), (6548, 161733), (6098, 172422), (696, 1306625), (3452, 300052), (717, 1408825), (573, 1561543), (4562, 160268), (4277, 195942), (5640, 186094), (2551, 176659), (1249, 573784), (631, 1412370), (4905, 184294), (877, 1019947), (1758, 490726), (767, 1164520), (2558, 157757), (518, 1170336), (1534, 393440), (3774, 232302), (1269, 737181), (6699, 164021), (2888, 375431), (6496, 161755), (4973, 157902), (348, 1943901)]
06/19/2022 18:10:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:10:54 - INFO - __main__ - Starting training!
06/19/2022 18:10:55 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_100_0.4_8_predictions.txt
06/19/2022 18:10:55 - INFO - __main__ - ACC on test data: 0.6127
06/19/2022 18:10:55 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.4, bsz=8, dev_performance=0.6875, test_performance=0.6127450980392157
06/19/2022 18:10:55 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.3, bsz=8 ...
06/19/2022 18:10:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:10:56 - INFO - __main__ - Printing 3 examples
06/19/2022 18:10:56 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 18:10:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 18:10:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:56 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 18:10:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 18:10:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:10:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:10:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:10:56 - INFO - __main__ - Printing 3 examples
06/19/2022 18:10:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 18:10:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:56 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 18:10:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 18:10:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:10:56 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:10:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:10:56 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:11:02 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:11:02 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(2594, 403711), (738, 1187017), (73, 2525281), (3583, 284375), (773, 1132610), (2099, 182268), (1757, 390014), (3023, 334230), (7161, 192247), (2231, 427234), (6082, 173261), (1461, 672164), (3573, 163617), (6209, 162555), (5259, 197913), (2697, 380850), (4225, 197872), (2319, 230472), (1994, 479967), (993, 886924), (1069, 868644), (4404, 216707), (4001, 264258), (4585, 221032), (4434, 228933), (5229, 200589), (629, 1463788), (1101, 860462), (661, 1317687), (3147, 232857), (2132, 277175), (1102, 798392), (2542, 371822), (2503, 477739), (2036, 480551), (5480, 174200), (4952, 209263), (397, 637384), (4682, 222111), (2030, 497571), (5234, 202177), (2665, 369855), (757, 1071575), (4370, 210982), (4895, 169946), (1085, 862457), (705, 1548305), (4508, 172897), (167, 5559739), (1112, 892303), (2193, 392132), (1396, 545283), (4374, 230866), (3385, 191872), (5816, 194070), (2358, 427447), (4191, 233374), (646, 1377071), (1790, 550995), (4264, 247591), (22, 38484130), (2415, 239286), (2392, 349952), (4190, 205854), (2833, 327549), (5032, 221170), (6353, 168404), (5324, 191543), (1518, 582487), (1894, 481117), (3228, 315556), (2265, 421208), (636, 1378362), (6548, 161733), (6098, 172422), (696, 1306625), (3452, 300052), (717, 1408825), (573, 1561543), (4562, 160268), (4277, 195942), (5640, 186094), (2551, 176659), (1249, 573784), (631, 1412370), (4905, 184294), (877, 1019947), (1758, 490726), (767, 1164520), (2558, 157757), (518, 1170336), (1534, 393440), (3774, 232302), (1269, 737181), (6699, 164021), (2888, 375431), (6496, 161755), (4973, 157902), (348, 1943901)]
06/19/2022 18:11:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:11:02 - INFO - __main__ - Starting training!
06/19/2022 18:11:04 - INFO - __main__ - Step 10 Global step 10 Train loss 6.05 on epoch=4
06/19/2022 18:11:06 - INFO - __main__ - Step 20 Global step 20 Train loss 2.83 on epoch=9
06/19/2022 18:11:07 - INFO - __main__ - Step 30 Global step 30 Train loss 1.39 on epoch=14
06/19/2022 18:11:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.74 on epoch=19
06/19/2022 18:11:10 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=24
06/19/2022 18:11:11 - INFO - __main__ - Global step 50 Train loss 2.30 ACC 0.5 on epoch=24
06/19/2022 18:11:11 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 18:11:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=29
06/19/2022 18:11:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.39 on epoch=34
06/19/2022 18:11:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.62 on epoch=39
06/19/2022 18:11:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=44
06/19/2022 18:11:18 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=49
06/19/2022 18:11:18 - INFO - __main__ - Global step 100 Train loss 0.44 ACC 0.5 on epoch=49
06/19/2022 18:11:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.30 on epoch=54
06/19/2022 18:11:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.30 on epoch=59
06/19/2022 18:11:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.36 on epoch=64
06/19/2022 18:11:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.25 on epoch=69
06/19/2022 18:11:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=74
06/19/2022 18:11:27 - INFO - __main__ - Global step 150 Train loss 0.30 ACC 0.625 on epoch=74
06/19/2022 18:11:27 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.625 on epoch=74, global_step=150
06/19/2022 18:11:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.36 on epoch=79
06/19/2022 18:11:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.33 on epoch=84
06/19/2022 18:11:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=89
06/19/2022 18:11:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=94
06/19/2022 18:11:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
06/19/2022 18:11:34 - INFO - __main__ - Global step 200 Train loss 0.30 ACC 0.5625 on epoch=99
06/19/2022 18:11:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=104
06/19/2022 18:11:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.25 on epoch=109
06/19/2022 18:11:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=114
06/19/2022 18:11:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.29 on epoch=119
06/19/2022 18:11:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=124
06/19/2022 18:11:42 - INFO - __main__ - Global step 250 Train loss 0.25 ACC 0.46875 on epoch=124
06/19/2022 18:11:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
06/19/2022 18:11:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
06/19/2022 18:11:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=139
06/19/2022 18:11:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.27 on epoch=144
06/19/2022 18:11:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.21 on epoch=149
06/19/2022 18:11:49 - INFO - __main__ - Global step 300 Train loss 0.24 ACC 0.65625 on epoch=149
06/19/2022 18:11:49 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=149, global_step=300
06/19/2022 18:11:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
06/19/2022 18:11:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=159
06/19/2022 18:11:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=164
06/19/2022 18:11:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.21 on epoch=169
06/19/2022 18:11:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=174
06/19/2022 18:11:57 - INFO - __main__ - Global step 350 Train loss 0.23 ACC 0.6875 on epoch=174
06/19/2022 18:11:57 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=174, global_step=350
06/19/2022 18:11:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=179
06/19/2022 18:12:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.17 on epoch=184
06/19/2022 18:12:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=189
06/19/2022 18:12:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.19 on epoch=194
06/19/2022 18:12:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=199
06/19/2022 18:12:05 - INFO - __main__ - Global step 400 Train loss 0.20 ACC 0.53125 on epoch=199
06/19/2022 18:12:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.19 on epoch=204
06/19/2022 18:12:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=209
06/19/2022 18:12:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=214
06/19/2022 18:12:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.17 on epoch=219
06/19/2022 18:12:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=224
06/19/2022 18:12:13 - INFO - __main__ - Global step 450 Train loss 0.20 ACC 0.46875 on epoch=224
06/19/2022 18:12:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.19 on epoch=229
06/19/2022 18:12:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=234
06/19/2022 18:12:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=239
06/19/2022 18:12:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=244
06/19/2022 18:12:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=249
06/19/2022 18:12:20 - INFO - __main__ - Global step 500 Train loss 0.20 ACC 0.625 on epoch=249
06/19/2022 18:12:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=254
06/19/2022 18:12:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.17 on epoch=259
06/19/2022 18:12:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=264
06/19/2022 18:12:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=269
06/19/2022 18:12:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=274
06/19/2022 18:12:27 - INFO - __main__ - Global step 550 Train loss 0.20 ACC 0.65625 on epoch=274
06/19/2022 18:12:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=279
06/19/2022 18:12:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=284
06/19/2022 18:12:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=289
06/19/2022 18:12:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=294
06/19/2022 18:12:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=299
06/19/2022 18:12:35 - INFO - __main__ - Global step 600 Train loss 0.20 ACC 0.625 on epoch=299
06/19/2022 18:12:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=304
06/19/2022 18:12:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=309
06/19/2022 18:12:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=314
06/19/2022 18:12:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=319
06/19/2022 18:12:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=324
06/19/2022 18:12:43 - INFO - __main__ - Global step 650 Train loss 0.20 ACC 0.625 on epoch=324
06/19/2022 18:12:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=329
06/19/2022 18:12:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=334
06/19/2022 18:12:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=339
06/19/2022 18:12:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=344
06/19/2022 18:12:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=349
06/19/2022 18:12:51 - INFO - __main__ - Global step 700 Train loss 0.19 ACC 0.53125 on epoch=349
06/19/2022 18:12:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=354
06/19/2022 18:12:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=359
06/19/2022 18:12:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=364
06/19/2022 18:12:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=369
06/19/2022 18:12:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=374
06/19/2022 18:12:59 - INFO - __main__ - Global step 750 Train loss 0.16 ACC 0.53125 on epoch=374
06/19/2022 18:13:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.17 on epoch=379
06/19/2022 18:13:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.18 on epoch=384
06/19/2022 18:13:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=389
06/19/2022 18:13:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=394
06/19/2022 18:13:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=399
06/19/2022 18:13:06 - INFO - __main__ - Global step 800 Train loss 0.17 ACC 0.6875 on epoch=399
06/19/2022 18:13:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=404
06/19/2022 18:13:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.17 on epoch=409
06/19/2022 18:13:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=414
06/19/2022 18:13:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=419
06/19/2022 18:13:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=424
06/19/2022 18:13:13 - INFO - __main__ - Global step 850 Train loss 0.18 ACC 0.6875 on epoch=424
06/19/2022 18:13:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.17 on epoch=429
06/19/2022 18:13:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=434
06/19/2022 18:13:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=439
06/19/2022 18:13:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=444
06/19/2022 18:13:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=449
06/19/2022 18:13:21 - INFO - __main__ - Global step 900 Train loss 0.17 ACC 0.6875 on epoch=449
06/19/2022 18:13:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=454
06/19/2022 18:13:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.15 on epoch=459
06/19/2022 18:13:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.16 on epoch=464
06/19/2022 18:13:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.14 on epoch=469
06/19/2022 18:13:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=474
06/19/2022 18:13:28 - INFO - __main__ - Global step 950 Train loss 0.16 ACC 0.59375 on epoch=474
06/19/2022 18:13:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.49 on epoch=479
06/19/2022 18:13:31 - INFO - __main__ - Step 970 Global step 970 Train loss 1.52 on epoch=484
06/19/2022 18:13:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=489
06/19/2022 18:13:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=494
06/19/2022 18:13:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=499
06/19/2022 18:13:36 - INFO - __main__ - Global step 1000 Train loss 0.50 ACC 0.6875 on epoch=499
06/19/2022 18:13:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=504
06/19/2022 18:13:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.16 on epoch=509
06/19/2022 18:13:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=514
06/19/2022 18:13:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.14 on epoch=519
06/19/2022 18:13:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=524
06/19/2022 18:13:44 - INFO - __main__ - Global step 1050 Train loss 0.16 ACC 0.625 on epoch=524
06/19/2022 18:13:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=529
06/19/2022 18:13:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=534
06/19/2022 18:13:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.14 on epoch=539
06/19/2022 18:13:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=544
06/19/2022 18:13:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=549
06/19/2022 18:13:51 - INFO - __main__ - Global step 1100 Train loss 0.14 ACC 0.65625 on epoch=549
06/19/2022 18:13:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=554
06/19/2022 18:13:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=559
06/19/2022 18:13:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=564
06/19/2022 18:13:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=569
06/19/2022 18:13:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=574
06/19/2022 18:13:58 - INFO - __main__ - Global step 1150 Train loss 0.15 ACC 0.65625 on epoch=574
06/19/2022 18:14:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=579
06/19/2022 18:14:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=584
06/19/2022 18:14:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=589
06/19/2022 18:14:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=594
06/19/2022 18:14:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=599
06/19/2022 18:14:06 - INFO - __main__ - Global step 1200 Train loss 0.13 ACC 0.65625 on epoch=599
06/19/2022 18:14:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=604
06/19/2022 18:14:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=609
06/19/2022 18:14:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=614
06/19/2022 18:14:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=619
06/19/2022 18:14:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=624
06/19/2022 18:14:14 - INFO - __main__ - Global step 1250 Train loss 0.14 ACC 0.625 on epoch=624
06/19/2022 18:14:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=629
06/19/2022 18:14:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=634
06/19/2022 18:14:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=639
06/19/2022 18:14:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=644
06/19/2022 18:14:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=649
06/19/2022 18:14:22 - INFO - __main__ - Global step 1300 Train loss 0.13 ACC 0.59375 on epoch=649
06/19/2022 18:14:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=654
06/19/2022 18:14:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=659
06/19/2022 18:14:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=664
06/19/2022 18:14:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=669
06/19/2022 18:14:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.11 on epoch=674
06/19/2022 18:14:31 - INFO - __main__ - Global step 1350 Train loss 0.12 ACC 0.59375 on epoch=674
06/19/2022 18:14:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.13 on epoch=679
06/19/2022 18:14:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=684
06/19/2022 18:14:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=689
06/19/2022 18:14:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=694
06/19/2022 18:14:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=699
06/19/2022 18:14:39 - INFO - __main__ - Global step 1400 Train loss 0.11 ACC 0.625 on epoch=699
06/19/2022 18:14:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=704
06/19/2022 18:14:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=709
06/19/2022 18:14:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=714
06/19/2022 18:14:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=719
06/19/2022 18:14:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=724
06/19/2022 18:14:46 - INFO - __main__ - Global step 1450 Train loss 0.12 ACC 0.65625 on epoch=724
06/19/2022 18:14:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=729
06/19/2022 18:14:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=734
06/19/2022 18:14:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=739
06/19/2022 18:14:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=744
06/19/2022 18:14:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=749
06/19/2022 18:14:54 - INFO - __main__ - Global step 1500 Train loss 0.10 ACC 0.65625 on epoch=749
06/19/2022 18:14:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=754
06/19/2022 18:14:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=759
06/19/2022 18:14:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=764
06/19/2022 18:15:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=769
06/19/2022 18:15:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=774
06/19/2022 18:15:02 - INFO - __main__ - Global step 1550 Train loss 0.09 ACC 0.65625 on epoch=774
06/19/2022 18:15:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=779
06/19/2022 18:15:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=784
06/19/2022 18:15:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=789
06/19/2022 18:15:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=794
06/19/2022 18:15:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=799
06/19/2022 18:15:10 - INFO - __main__ - Global step 1600 Train loss 0.09 ACC 0.6875 on epoch=799
06/19/2022 18:15:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=804
06/19/2022 18:15:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=809
06/19/2022 18:15:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=814
06/19/2022 18:15:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=819
06/19/2022 18:15:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=824
06/19/2022 18:15:18 - INFO - __main__ - Global step 1650 Train loss 0.08 ACC 0.6875 on epoch=824
06/19/2022 18:15:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=829
06/19/2022 18:15:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=834
06/19/2022 18:15:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=839
06/19/2022 18:15:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=844
06/19/2022 18:15:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=849
06/19/2022 18:15:25 - INFO - __main__ - Global step 1700 Train loss 0.10 ACC 0.6875 on epoch=849
06/19/2022 18:15:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=854
06/19/2022 18:15:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.79 on epoch=859
06/19/2022 18:15:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=864
06/19/2022 18:15:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=869
06/19/2022 18:15:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=874
06/19/2022 18:15:33 - INFO - __main__ - Global step 1750 Train loss 0.22 ACC 0.625 on epoch=874
06/19/2022 18:15:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=879
06/19/2022 18:15:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=884
06/19/2022 18:15:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=889
06/19/2022 18:15:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=894
06/19/2022 18:15:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=899
06/19/2022 18:15:40 - INFO - __main__ - Global step 1800 Train loss 0.07 ACC 0.59375 on epoch=899
06/19/2022 18:15:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=904
06/19/2022 18:15:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=909
06/19/2022 18:15:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=914
06/19/2022 18:15:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=919
06/19/2022 18:15:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=924
06/19/2022 18:15:48 - INFO - __main__ - Global step 1850 Train loss 0.06 ACC 0.625 on epoch=924
06/19/2022 18:15:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=929
06/19/2022 18:15:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=934
06/19/2022 18:15:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=939
06/19/2022 18:15:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=944
06/19/2022 18:15:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=949
06/19/2022 18:15:56 - INFO - __main__ - Global step 1900 Train loss 0.06 ACC 0.65625 on epoch=949
06/19/2022 18:15:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=954
06/19/2022 18:15:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=959
06/19/2022 18:16:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=964
06/19/2022 18:16:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=969
06/19/2022 18:16:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=974
06/19/2022 18:16:04 - INFO - __main__ - Global step 1950 Train loss 0.05 ACC 0.65625 on epoch=974
06/19/2022 18:16:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
06/19/2022 18:16:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=984
06/19/2022 18:16:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=989
06/19/2022 18:16:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=994
06/19/2022 18:16:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=999
06/19/2022 18:16:11 - INFO - __main__ - Global step 2000 Train loss 0.05 ACC 0.65625 on epoch=999
06/19/2022 18:16:11 - INFO - __main__ - save last model!
06/19/2022 18:16:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 18:16:11 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 18:16:11 - INFO - __main__ - Printing 3 examples
06/19/2022 18:16:11 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 18:16:11 - INFO - __main__ - ['equivalent']
06/19/2022 18:16:11 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 18:16:11 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:11 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 18:16:11 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:11 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:16:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:16:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:16:12 - INFO - __main__ - Printing 3 examples
06/19/2022 18:16:12 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 18:16:12 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 18:16:12 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:12 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 18:16:12 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 18:16:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:16:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:16:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:16:12 - INFO - __main__ - Printing 3 examples
06/19/2022 18:16:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 18:16:12 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:12 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 18:16:12 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 18:16:12 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:12 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:16:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:16:12 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:16:12 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 18:16:18 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:16:18 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(907, 999819), (4328, 223698), (7511, 160346), (5954, 165280), (4511, 179764), (3531, 297895), (2852, 355056), (1714, 447470), (3237, 299613), (2554, 389221), (1462, 349360), (5120, 236973), (1511, 676741), (2846, 312704), (4848, 204967), (3155, 298023), (976, 1102538), (99, 8503604), (4345, 204361), (5077, 166084), (1077, 853741), (930, 1093140), (5712, 173941), (2441, 429114), (5696, 184838), (2392, 349952), (1363, 669374), (2856, 268051), (554, 1156528), (2131, 378214), (3105, 254084), (3315, 327974), (1735, 571995), (4183, 213298), (6331, 171986), (3243, 311061), (5189, 177490), (91, 9930046), (4320, 168819), (2577, 313511), (1273, 833860), (1466, 664308), (3001, 351252), (955, 764542), (1170, 817452), (6309, 169174), (986, 581568), (2381, 341017), (3127, 331271), (7594, 155434), (2456, 433793), (3716, 273699), (2024, 448261), (2411, 351989), (5529, 185095), (3351, 190868), (2717, 368013), (580, 1542140), (1019, 901860), (4585, 221032), (2887, 336279), (514, 1732295), (42, 24506569), (3827, 282190), (1368, 677011), (2835, 354320), (1751, 560750), (4764, 211573), (6349, 169524), (4554, 218517), (6817, 166736), (103, 8154869), (1142, 782542), (809, 1073970), (1651, 598254), (5782, 171468), (616, 2195072), (4145, 254483), (6957, 160765), (6212, 169413), (1519, 600961), (4894, 217353), (944, 693547), (124, 1842230), (2122, 305087), (3931, 257603), (453, 1935608), (4016, 262231), (1677, 434886), (2591, 370033), (953, 840707), (1022, 264436), (4487, 166978), (4374, 230866), (1499, 582683), (2605, 391810), (1675, 558878), (948, 512142), (4647, 228104)]
06/19/2022 18:16:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:16:19 - INFO - __main__ - Starting training!
06/19/2022 18:16:21 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_100_0.3_8_predictions.txt
06/19/2022 18:16:21 - INFO - __main__ - ACC on test data: 0.4632
06/19/2022 18:16:21 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.3, bsz=8, dev_performance=0.6875, test_performance=0.4632352941176471
06/19/2022 18:16:21 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.2, bsz=8 ...
06/19/2022 18:16:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:16:22 - INFO - __main__ - Printing 3 examples
06/19/2022 18:16:22 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 18:16:22 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:22 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 18:16:22 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:22 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 18:16:22 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 18:16:22 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:16:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:16:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:16:22 - INFO - __main__ - Printing 3 examples
06/19/2022 18:16:22 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 18:16:22 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:22 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 18:16:22 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:22 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 18:16:22 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:16:22 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:16:22 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:16:22 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:16:28 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:16:28 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(907, 999819), (4328, 223698), (7511, 160346), (5954, 165280), (4511, 179764), (3531, 297895), (2852, 355056), (1714, 447470), (3237, 299613), (2554, 389221), (1462, 349360), (5120, 236973), (1511, 676741), (2846, 312704), (4848, 204967), (3155, 298023), (976, 1102538), (99, 8503604), (4345, 204361), (5077, 166084), (1077, 853741), (930, 1093140), (5712, 173941), (2441, 429114), (5696, 184838), (2392, 349952), (1363, 669374), (2856, 268051), (554, 1156528), (2131, 378214), (3105, 254084), (3315, 327974), (1735, 571995), (4183, 213298), (6331, 171986), (3243, 311061), (5189, 177490), (91, 9930046), (4320, 168819), (2577, 313511), (1273, 833860), (1466, 664308), (3001, 351252), (955, 764542), (1170, 817452), (6309, 169174), (986, 581568), (2381, 341017), (3127, 331271), (7594, 155434), (2456, 433793), (3716, 273699), (2024, 448261), (2411, 351989), (5529, 185095), (3351, 190868), (2717, 368013), (580, 1542140), (1019, 901860), (4585, 221032), (2887, 336279), (514, 1732295), (42, 24506569), (3827, 282190), (1368, 677011), (2835, 354320), (1751, 560750), (4764, 211573), (6349, 169524), (4554, 218517), (6817, 166736), (103, 8154869), (1142, 782542), (809, 1073970), (1651, 598254), (5782, 171468), (616, 2195072), (4145, 254483), (6957, 160765), (6212, 169413), (1519, 600961), (4894, 217353), (944, 693547), (124, 1842230), (2122, 305087), (3931, 257603), (453, 1935608), (4016, 262231), (1677, 434886), (2591, 370033), (953, 840707), (1022, 264436), (4487, 166978), (4374, 230866), (1499, 582683), (2605, 391810), (1675, 558878), (948, 512142), (4647, 228104)]
06/19/2022 18:16:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:16:29 - INFO - __main__ - Starting training!
06/19/2022 18:16:31 - INFO - __main__ - Step 10 Global step 10 Train loss 6.05 on epoch=4
06/19/2022 18:16:32 - INFO - __main__ - Step 20 Global step 20 Train loss 4.14 on epoch=9
06/19/2022 18:16:33 - INFO - __main__ - Step 30 Global step 30 Train loss 2.67 on epoch=14
06/19/2022 18:16:35 - INFO - __main__ - Step 40 Global step 40 Train loss 1.74 on epoch=19
06/19/2022 18:16:36 - INFO - __main__ - Step 50 Global step 50 Train loss 1.59 on epoch=24
06/19/2022 18:16:37 - INFO - __main__ - Global step 50 Train loss 3.24 ACC 0.5 on epoch=24
06/19/2022 18:16:37 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 18:16:39 - INFO - __main__ - Step 60 Global step 60 Train loss 1.00 on epoch=29
06/19/2022 18:16:40 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=34
06/19/2022 18:16:41 - INFO - __main__ - Step 80 Global step 80 Train loss 0.74 on epoch=39
06/19/2022 18:16:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=44
06/19/2022 18:16:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.57 on epoch=49
06/19/2022 18:16:45 - INFO - __main__ - Global step 100 Train loss 0.76 ACC 0.40625 on epoch=49
06/19/2022 18:16:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=54
06/19/2022 18:16:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=59
06/19/2022 18:16:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=64
06/19/2022 18:16:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
06/19/2022 18:16:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
06/19/2022 18:16:53 - INFO - __main__ - Global step 150 Train loss 0.46 ACC 0.5 on epoch=74
06/19/2022 18:16:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.56 on epoch=79
06/19/2022 18:16:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=84
06/19/2022 18:16:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
06/19/2022 18:16:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
06/19/2022 18:17:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=99
06/19/2022 18:17:01 - INFO - __main__ - Global step 200 Train loss 0.41 ACC 0.5 on epoch=99
06/19/2022 18:17:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=104
06/19/2022 18:17:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=109
06/19/2022 18:17:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=114
06/19/2022 18:17:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=119
06/19/2022 18:17:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.30 on epoch=124
06/19/2022 18:17:09 - INFO - __main__ - Global step 250 Train loss 0.37 ACC 0.5 on epoch=124
06/19/2022 18:17:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.29 on epoch=129
06/19/2022 18:17:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=134
06/19/2022 18:17:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.31 on epoch=139
06/19/2022 18:17:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=144
06/19/2022 18:17:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=149
06/19/2022 18:17:16 - INFO - __main__ - Global step 300 Train loss 0.30 ACC 0.5 on epoch=149
06/19/2022 18:17:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.30 on epoch=154
06/19/2022 18:17:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
06/19/2022 18:17:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
06/19/2022 18:17:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=169
06/19/2022 18:17:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=174
06/19/2022 18:17:24 - INFO - __main__ - Global step 350 Train loss 0.27 ACC 0.5 on epoch=174
06/19/2022 18:17:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
06/19/2022 18:17:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=184
06/19/2022 18:17:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
06/19/2022 18:17:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.31 on epoch=194
06/19/2022 18:17:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=199
06/19/2022 18:17:31 - INFO - __main__ - Global step 400 Train loss 0.28 ACC 0.53125 on epoch=199
06/19/2022 18:17:31 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=199, global_step=400
06/19/2022 18:17:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
06/19/2022 18:17:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=209
06/19/2022 18:17:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=214
06/19/2022 18:17:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=219
06/19/2022 18:17:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/19/2022 18:17:38 - INFO - __main__ - Global step 450 Train loss 0.24 ACC 0.625 on epoch=224
06/19/2022 18:17:38 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.625 on epoch=224, global_step=450
06/19/2022 18:17:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.29 on epoch=229
06/19/2022 18:17:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=234
06/19/2022 18:17:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
06/19/2022 18:17:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
06/19/2022 18:17:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=249
06/19/2022 18:17:45 - INFO - __main__ - Global step 500 Train loss 0.26 ACC 0.5 on epoch=249
06/19/2022 18:17:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=254
06/19/2022 18:17:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=259
06/19/2022 18:17:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.29 on epoch=264
06/19/2022 18:17:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=269
06/19/2022 18:17:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=274
06/19/2022 18:17:53 - INFO - __main__ - Global step 550 Train loss 0.25 ACC 0.59375 on epoch=274
06/19/2022 18:17:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
06/19/2022 18:17:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=284
06/19/2022 18:17:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=289
06/19/2022 18:17:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=294
06/19/2022 18:18:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=299
06/19/2022 18:18:00 - INFO - __main__ - Global step 600 Train loss 0.23 ACC 0.59375 on epoch=299
06/19/2022 18:18:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=304
06/19/2022 18:18:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=309
06/19/2022 18:18:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=314
06/19/2022 18:18:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=319
06/19/2022 18:18:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=324
06/19/2022 18:18:09 - INFO - __main__ - Global step 650 Train loss 0.24 ACC 0.65625 on epoch=324
06/19/2022 18:18:09 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=324, global_step=650
06/19/2022 18:18:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=329
06/19/2022 18:18:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=334
06/19/2022 18:18:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=339
06/19/2022 18:18:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=344
06/19/2022 18:18:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=349
06/19/2022 18:18:16 - INFO - __main__ - Global step 700 Train loss 0.21 ACC 0.5625 on epoch=349
06/19/2022 18:18:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=354
06/19/2022 18:18:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=359
06/19/2022 18:18:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=364
06/19/2022 18:18:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=369
06/19/2022 18:18:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=374
06/19/2022 18:18:23 - INFO - __main__ - Global step 750 Train loss 0.21 ACC 0.5 on epoch=374
06/19/2022 18:18:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=379
06/19/2022 18:18:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=384
06/19/2022 18:18:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=389
06/19/2022 18:18:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=394
06/19/2022 18:18:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=399
06/19/2022 18:18:31 - INFO - __main__ - Global step 800 Train loss 0.21 ACC 0.5 on epoch=399
06/19/2022 18:18:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=404
06/19/2022 18:18:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=409
06/19/2022 18:18:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=414
06/19/2022 18:18:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=419
06/19/2022 18:18:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=424
06/19/2022 18:18:38 - INFO - __main__ - Global step 850 Train loss 0.22 ACC 0.5625 on epoch=424
06/19/2022 18:18:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=429
06/19/2022 18:18:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=434
06/19/2022 18:18:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=439
06/19/2022 18:18:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=444
06/19/2022 18:18:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=449
06/19/2022 18:18:46 - INFO - __main__ - Global step 900 Train loss 0.21 ACC 0.59375 on epoch=449
06/19/2022 18:18:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=454
06/19/2022 18:18:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=459
06/19/2022 18:18:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=464
06/19/2022 18:18:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=469
06/19/2022 18:18:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=474
06/19/2022 18:18:54 - INFO - __main__ - Global step 950 Train loss 0.19 ACC 0.59375 on epoch=474
06/19/2022 18:18:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=479
06/19/2022 18:18:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=484
06/19/2022 18:18:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=489
06/19/2022 18:18:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=494
06/19/2022 18:19:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=499
06/19/2022 18:19:01 - INFO - __main__ - Global step 1000 Train loss 0.18 ACC 0.5 on epoch=499
06/19/2022 18:19:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=504
06/19/2022 18:19:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=509
06/19/2022 18:19:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=514
06/19/2022 18:19:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=519
06/19/2022 18:19:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=524
06/19/2022 18:19:08 - INFO - __main__ - Global step 1050 Train loss 0.19 ACC 0.65625 on epoch=524
06/19/2022 18:19:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=529
06/19/2022 18:19:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=534
06/19/2022 18:19:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=539
06/19/2022 18:19:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=544
06/19/2022 18:19:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=549
06/19/2022 18:19:16 - INFO - __main__ - Global step 1100 Train loss 0.19 ACC 0.625 on epoch=549
06/19/2022 18:19:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=554
06/19/2022 18:19:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=559
06/19/2022 18:19:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=564
06/19/2022 18:19:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=569
06/19/2022 18:19:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.14 on epoch=574
06/19/2022 18:19:24 - INFO - __main__ - Global step 1150 Train loss 0.17 ACC 0.46875 on epoch=574
06/19/2022 18:19:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=579
06/19/2022 18:19:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=584
06/19/2022 18:19:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=589
06/19/2022 18:19:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=594
06/19/2022 18:19:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.15 on epoch=599
06/19/2022 18:19:31 - INFO - __main__ - Global step 1200 Train loss 0.18 ACC 0.59375 on epoch=599
06/19/2022 18:19:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=604
06/19/2022 18:19:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=609
06/19/2022 18:19:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=614
06/19/2022 18:19:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=619
06/19/2022 18:19:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.15 on epoch=624
06/19/2022 18:19:39 - INFO - __main__ - Global step 1250 Train loss 0.16 ACC 0.6875 on epoch=624
06/19/2022 18:19:39 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=624, global_step=1250
06/19/2022 18:19:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=629
06/19/2022 18:19:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=634
06/19/2022 18:19:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=639
06/19/2022 18:19:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=644
06/19/2022 18:19:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=649
06/19/2022 18:19:47 - INFO - __main__ - Global step 1300 Train loss 0.16 ACC 0.625 on epoch=649
06/19/2022 18:19:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=654
06/19/2022 18:19:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=659
06/19/2022 18:19:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=664
06/19/2022 18:19:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=669
06/19/2022 18:19:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=674
06/19/2022 18:19:54 - INFO - __main__ - Global step 1350 Train loss 0.15 ACC 0.6875 on epoch=674
06/19/2022 18:19:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.13 on epoch=679
06/19/2022 18:19:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.14 on epoch=684
06/19/2022 18:19:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=689
06/19/2022 18:19:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=694
06/19/2022 18:20:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.09 on epoch=699
06/19/2022 18:20:02 - INFO - __main__ - Global step 1400 Train loss 0.13 ACC 0.6875 on epoch=699
06/19/2022 18:20:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=704
06/19/2022 18:20:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=709
06/19/2022 18:20:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=714
06/19/2022 18:20:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=719
06/19/2022 18:20:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=724
06/19/2022 18:20:09 - INFO - __main__ - Global step 1450 Train loss 0.10 ACC 0.5625 on epoch=724
06/19/2022 18:20:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=729
06/19/2022 18:20:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=734
06/19/2022 18:20:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=739
06/19/2022 18:20:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=744
06/19/2022 18:20:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=749
06/19/2022 18:20:18 - INFO - __main__ - Global step 1500 Train loss 0.14 ACC 0.5625 on epoch=749
06/19/2022 18:20:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=754
06/19/2022 18:20:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=759
06/19/2022 18:20:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=764
06/19/2022 18:20:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=769
06/19/2022 18:20:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=774
06/19/2022 18:20:25 - INFO - __main__ - Global step 1550 Train loss 0.10 ACC 0.53125 on epoch=774
06/19/2022 18:20:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=779
06/19/2022 18:20:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=784
06/19/2022 18:20:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=789
06/19/2022 18:20:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=794
06/19/2022 18:20:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=799
06/19/2022 18:20:33 - INFO - __main__ - Global step 1600 Train loss 0.08 ACC 0.59375 on epoch=799
06/19/2022 18:20:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=804
06/19/2022 18:20:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=809
06/19/2022 18:20:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.10 on epoch=814
06/19/2022 18:20:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=819
06/19/2022 18:20:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=824
06/19/2022 18:20:40 - INFO - __main__ - Global step 1650 Train loss 0.08 ACC 0.53125 on epoch=824
06/19/2022 18:20:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=829
06/19/2022 18:20:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=834
06/19/2022 18:20:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=839
06/19/2022 18:20:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=844
06/19/2022 18:20:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.09 on epoch=849
06/19/2022 18:20:47 - INFO - __main__ - Global step 1700 Train loss 0.09 ACC 0.5625 on epoch=849
06/19/2022 18:20:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=854
06/19/2022 18:20:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=859
06/19/2022 18:20:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=864
06/19/2022 18:20:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=869
06/19/2022 18:20:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=874
06/19/2022 18:20:55 - INFO - __main__ - Global step 1750 Train loss 0.07 ACC 0.625 on epoch=874
06/19/2022 18:20:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=879
06/19/2022 18:20:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=884
06/19/2022 18:20:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=889
06/19/2022 18:21:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=894
06/19/2022 18:21:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=899
06/19/2022 18:21:02 - INFO - __main__ - Global step 1800 Train loss 0.06 ACC 0.625 on epoch=899
06/19/2022 18:21:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=904
06/19/2022 18:21:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=909
06/19/2022 18:21:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=914
06/19/2022 18:21:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=919
06/19/2022 18:21:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
06/19/2022 18:21:10 - INFO - __main__ - Global step 1850 Train loss 0.06 ACC 0.59375 on epoch=924
06/19/2022 18:21:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=929
06/19/2022 18:21:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=934
06/19/2022 18:21:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=939
06/19/2022 18:21:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=944
06/19/2022 18:21:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=949
06/19/2022 18:21:18 - INFO - __main__ - Global step 1900 Train loss 0.07 ACC 0.59375 on epoch=949
06/19/2022 18:21:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=954
06/19/2022 18:21:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=959
06/19/2022 18:21:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=964
06/19/2022 18:21:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=969
06/19/2022 18:21:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
06/19/2022 18:21:25 - INFO - __main__ - Global step 1950 Train loss 0.05 ACC 0.625 on epoch=974
06/19/2022 18:21:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=979
06/19/2022 18:21:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=984
06/19/2022 18:21:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=989
06/19/2022 18:21:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=994
06/19/2022 18:21:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=999
06/19/2022 18:21:33 - INFO - __main__ - Global step 2000 Train loss 0.05 ACC 0.59375 on epoch=999
06/19/2022 18:21:33 - INFO - __main__ - save last model!
06/19/2022 18:21:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 18:21:33 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 18:21:33 - INFO - __main__ - Printing 3 examples
06/19/2022 18:21:33 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 18:21:33 - INFO - __main__ - ['equivalent']
06/19/2022 18:21:33 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 18:21:33 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:21:33 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 18:21:33 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:21:33 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:21:33 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:21:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:21:33 - INFO - __main__ - Printing 3 examples
06/19/2022 18:21:33 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 18:21:33 - INFO - __main__ - ['equivalent']
06/19/2022 18:21:33 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 18:21:33 - INFO - __main__ - ['equivalent']
06/19/2022 18:21:33 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 18:21:33 - INFO - __main__ - ['equivalent']
06/19/2022 18:21:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 18:21:33 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:21:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:21:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:21:33 - INFO - __main__ - Printing 3 examples
06/19/2022 18:21:33 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 18:21:33 - INFO - __main__ - ['equivalent']
06/19/2022 18:21:33 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 18:21:33 - INFO - __main__ - ['equivalent']
06/19/2022 18:21:33 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 18:21:33 - INFO - __main__ - ['equivalent']
06/19/2022 18:21:33 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:21:33 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:21:33 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:21:33 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 18:21:39 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:21:39 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(210, 3214246), (956, 507463), (177, 283247), (2600, 228530), (2320, 373103), (2867, 348102), (1756, 567748), (1056, 811824), (3677, 244767), (451, 1996105), (4656, 199279), (2365, 411961), (2118, 458638), (4865, 202967), (3714, 267548), (2401, 307744), (2330, 404909), (2602, 411193), (281, 3369363), (2021, 474295), (4180, 234955), (3604, 286058), (3074, 334210), (799, 1149745), (2942, 333386), (3102, 266483), (6112, 183777), (4526, 234296), (6478, 174299), (1720, 283907), (4, 2855733), (1237, 765840), (2853, 354788), (1738, 508620), (6440, 161231), (2893, 349540), (429, 2065410), (1164, 807916), (6748, 157449), (4838, 214821), (3568, 270815), (3452, 300052), (4891, 214175), (3640, 155110), (2298, 183293), (1268, 763020), (3449, 243414), (6878, 162543), (1389, 480591), (2350, 363461), (530, 1727807), (4735, 214273), (24, 46921718), (3913, 180488), (5858, 173605), (2172, 463811), (3354, 258314), (3961, 216648), (778, 1141371), (3472, 284740), (3377, 299929), (1425, 711960), (3475, 312556), (2180, 184483), (2567, 304604), (355, 230108), (3404, 288313), (4344, 232964), (3047, 316651), (1753, 168938), (901, 693591), (2107, 476561), (4457, 223114), (4036, 247461), (1860, 522151), (4239, 164202), (3320, 312204), (1536, 231332), (4170, 170840), (8, 234707885), (2896, 309135), (3168, 326177), (2018, 434867), (3916, 217282), (6144, 161446), (6551, 154998), (3510, 291560), (5111, 201364), (4764, 211573), (3258, 211425), (2283, 428223), (6848, 183388), (4505, 197810), (1586, 532514), (715, 971516), (1935, 328964), (5835, 181763), (3170, 281056), (1020, 892057)]
06/19/2022 18:21:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:21:39 - INFO - __main__ - Starting training!
06/19/2022 18:21:42 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_100_0.2_8_predictions.txt
06/19/2022 18:21:42 - INFO - __main__ - ACC on test data: 0.4975
06/19/2022 18:21:42 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.2, bsz=8, dev_performance=0.6875, test_performance=0.49754901960784315
06/19/2022 18:21:42 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.5, bsz=8 ...
06/19/2022 18:21:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:21:43 - INFO - __main__ - Printing 3 examples
06/19/2022 18:21:43 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 18:21:43 - INFO - __main__ - ['equivalent']
06/19/2022 18:21:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 18:21:43 - INFO - __main__ - ['equivalent']
06/19/2022 18:21:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 18:21:43 - INFO - __main__ - ['equivalent']
06/19/2022 18:21:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 18:21:43 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:21:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:21:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:21:43 - INFO - __main__ - Printing 3 examples
06/19/2022 18:21:43 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 18:21:43 - INFO - __main__ - ['equivalent']
06/19/2022 18:21:43 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 18:21:43 - INFO - __main__ - ['equivalent']
06/19/2022 18:21:43 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 18:21:43 - INFO - __main__ - ['equivalent']
06/19/2022 18:21:43 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:21:43 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:21:43 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:21:49 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:21:49 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(210, 3214246), (956, 507463), (177, 283247), (2600, 228530), (2320, 373103), (2867, 348102), (1756, 567748), (1056, 811824), (3677, 244767), (451, 1996105), (4656, 199279), (2365, 411961), (2118, 458638), (4865, 202967), (3714, 267548), (2401, 307744), (2330, 404909), (2602, 411193), (281, 3369363), (2021, 474295), (4180, 234955), (3604, 286058), (3074, 334210), (799, 1149745), (2942, 333386), (3102, 266483), (6112, 183777), (4526, 234296), (6478, 174299), (1720, 283907), (4, 2855733), (1237, 765840), (2853, 354788), (1738, 508620), (6440, 161231), (2893, 349540), (429, 2065410), (1164, 807916), (6748, 157449), (4838, 214821), (3568, 270815), (3452, 300052), (4891, 214175), (3640, 155110), (2298, 183293), (1268, 763020), (3449, 243414), (6878, 162543), (1389, 480591), (2350, 363461), (530, 1727807), (4735, 214273), (24, 46921718), (3913, 180488), (5858, 173605), (2172, 463811), (3354, 258314), (3961, 216648), (778, 1141371), (3472, 284740), (3377, 299929), (1425, 711960), (3475, 312556), (2180, 184483), (2567, 304604), (355, 230108), (3404, 288313), (4344, 232964), (3047, 316651), (1753, 168938), (901, 693591), (2107, 476561), (4457, 223114), (4036, 247461), (1860, 522151), (4239, 164202), (3320, 312204), (1536, 231332), (4170, 170840), (8, 234707885), (2896, 309135), (3168, 326177), (2018, 434867), (3916, 217282), (6144, 161446), (6551, 154998), (3510, 291560), (5111, 201364), (4764, 211573), (3258, 211425), (2283, 428223), (6848, 183388), (4505, 197810), (1586, 532514), (715, 971516), (1935, 328964), (5835, 181763), (3170, 281056), (1020, 892057)]
06/19/2022 18:21:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:21:49 - INFO - __main__ - Starting training!
06/19/2022 18:21:51 - INFO - __main__ - Step 10 Global step 10 Train loss 5.32 on epoch=4
06/19/2022 18:21:52 - INFO - __main__ - Step 20 Global step 20 Train loss 1.55 on epoch=9
06/19/2022 18:21:53 - INFO - __main__ - Step 30 Global step 30 Train loss 0.92 on epoch=14
06/19/2022 18:21:54 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=19
06/19/2022 18:21:55 - INFO - __main__ - Step 50 Global step 50 Train loss 0.43 on epoch=24
06/19/2022 18:21:56 - INFO - __main__ - Global step 50 Train loss 1.75 ACC 0.5 on epoch=24
06/19/2022 18:21:56 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 18:21:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=29
06/19/2022 18:21:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.36 on epoch=34
06/19/2022 18:22:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.38 on epoch=39
06/19/2022 18:22:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.32 on epoch=44
06/19/2022 18:22:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=49
06/19/2022 18:22:03 - INFO - __main__ - Global step 100 Train loss 0.39 ACC 0.5 on epoch=49
06/19/2022 18:22:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.30 on epoch=54
06/19/2022 18:22:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
06/19/2022 18:22:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.34 on epoch=64
06/19/2022 18:22:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=69
06/19/2022 18:22:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.32 on epoch=74
06/19/2022 18:22:12 - INFO - __main__ - Global step 150 Train loss 0.32 ACC 0.5 on epoch=74
06/19/2022 18:22:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.33 on epoch=79
06/19/2022 18:22:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=84
06/19/2022 18:22:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=89
06/19/2022 18:22:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
06/19/2022 18:22:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.29 on epoch=99
06/19/2022 18:22:19 - INFO - __main__ - Global step 200 Train loss 0.29 ACC 0.5 on epoch=99
06/19/2022 18:22:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.29 on epoch=104
06/19/2022 18:22:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
06/19/2022 18:22:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=114
06/19/2022 18:22:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=119
06/19/2022 18:22:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=124
06/19/2022 18:22:26 - INFO - __main__ - Global step 250 Train loss 0.27 ACC 0.5 on epoch=124
06/19/2022 18:22:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
06/19/2022 18:22:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
06/19/2022 18:22:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.29 on epoch=139
06/19/2022 18:22:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
06/19/2022 18:22:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=149
06/19/2022 18:22:34 - INFO - __main__ - Global step 300 Train loss 0.26 ACC 0.5 on epoch=149
06/19/2022 18:22:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
06/19/2022 18:22:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.32 on epoch=159
06/19/2022 18:22:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
06/19/2022 18:22:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=169
06/19/2022 18:22:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=174
06/19/2022 18:22:41 - INFO - __main__ - Global step 350 Train loss 0.28 ACC 0.59375 on epoch=174
06/19/2022 18:22:41 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=174, global_step=350
06/19/2022 18:22:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=179
06/19/2022 18:22:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=184
06/19/2022 18:22:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=189
06/19/2022 18:22:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=194
06/19/2022 18:22:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=199
06/19/2022 18:22:49 - INFO - __main__ - Global step 400 Train loss 0.24 ACC 0.5 on epoch=199
06/19/2022 18:22:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
06/19/2022 18:22:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=209
06/19/2022 18:22:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=214
06/19/2022 18:22:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=219
06/19/2022 18:22:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.20 on epoch=224
06/19/2022 18:22:57 - INFO - __main__ - Global step 450 Train loss 0.23 ACC 0.5 on epoch=224
06/19/2022 18:22:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=229
06/19/2022 18:23:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=234
06/19/2022 18:23:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.15 on epoch=239
06/19/2022 18:23:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.16 on epoch=244
06/19/2022 18:23:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.17 on epoch=249
06/19/2022 18:23:04 - INFO - __main__ - Global step 500 Train loss 0.18 ACC 0.625 on epoch=249
06/19/2022 18:23:04 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=249, global_step=500
06/19/2022 18:23:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=254
06/19/2022 18:23:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=259
06/19/2022 18:23:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.14 on epoch=264
06/19/2022 18:23:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=269
06/19/2022 18:23:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.09 on epoch=274
06/19/2022 18:23:12 - INFO - __main__ - Global step 550 Train loss 0.16 ACC 0.46875 on epoch=274
06/19/2022 18:23:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.16 on epoch=279
06/19/2022 18:23:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.09 on epoch=284
06/19/2022 18:23:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.11 on epoch=289
06/19/2022 18:23:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=294
06/19/2022 18:23:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=299
06/19/2022 18:23:19 - INFO - __main__ - Global step 600 Train loss 0.14 ACC 0.65625 on epoch=299
06/19/2022 18:23:19 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=299, global_step=600
06/19/2022 18:23:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.11 on epoch=304
06/19/2022 18:23:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.09 on epoch=309
06/19/2022 18:23:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=314
06/19/2022 18:23:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.13 on epoch=319
06/19/2022 18:23:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=324
06/19/2022 18:23:27 - INFO - __main__ - Global step 650 Train loss 0.11 ACC 0.5625 on epoch=324
06/19/2022 18:23:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.09 on epoch=329
06/19/2022 18:23:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
06/19/2022 18:23:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=339
06/19/2022 18:23:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=344
06/19/2022 18:23:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=349
06/19/2022 18:23:34 - INFO - __main__ - Global step 700 Train loss 0.08 ACC 0.625 on epoch=349
06/19/2022 18:23:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
06/19/2022 18:23:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
06/19/2022 18:23:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=364
06/19/2022 18:23:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=369
06/19/2022 18:23:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=374
06/19/2022 18:23:42 - INFO - __main__ - Global step 750 Train loss 0.07 ACC 0.59375 on epoch=374
06/19/2022 18:23:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
06/19/2022 18:23:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
06/19/2022 18:23:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=389
06/19/2022 18:23:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=394
06/19/2022 18:23:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
06/19/2022 18:23:50 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.71875 on epoch=399
06/19/2022 18:23:50 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.71875 on epoch=399, global_step=800
06/19/2022 18:23:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
06/19/2022 18:23:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
06/19/2022 18:23:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=414
06/19/2022 18:23:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=419
06/19/2022 18:23:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/19/2022 18:23:57 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.65625 on epoch=424
06/19/2022 18:23:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
06/19/2022 18:24:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
06/19/2022 18:24:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
06/19/2022 18:24:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
06/19/2022 18:24:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/19/2022 18:24:05 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.59375 on epoch=449
06/19/2022 18:24:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
06/19/2022 18:24:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=459
06/19/2022 18:24:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
06/19/2022 18:24:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
06/19/2022 18:24:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
06/19/2022 18:24:12 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.65625 on epoch=474
06/19/2022 18:24:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
06/19/2022 18:24:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/19/2022 18:24:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
06/19/2022 18:24:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/19/2022 18:24:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/19/2022 18:24:20 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.625 on epoch=499
06/19/2022 18:24:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/19/2022 18:24:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
06/19/2022 18:24:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/19/2022 18:24:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/19/2022 18:24:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
06/19/2022 18:24:28 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.59375 on epoch=524
06/19/2022 18:24:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/19/2022 18:24:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
06/19/2022 18:24:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/19/2022 18:24:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/19/2022 18:24:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
06/19/2022 18:24:36 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.65625 on epoch=549
06/19/2022 18:24:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/19/2022 18:24:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/19/2022 18:24:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/19/2022 18:24:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/19/2022 18:24:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/19/2022 18:24:44 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.65625 on epoch=574
06/19/2022 18:24:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
06/19/2022 18:24:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/19/2022 18:24:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/19/2022 18:24:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/19/2022 18:24:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/19/2022 18:24:53 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.625 on epoch=599
06/19/2022 18:24:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/19/2022 18:24:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/19/2022 18:24:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/19/2022 18:24:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
06/19/2022 18:25:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
06/19/2022 18:25:01 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.6875 on epoch=624
06/19/2022 18:25:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/19/2022 18:25:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
06/19/2022 18:25:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/19/2022 18:25:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/19/2022 18:25:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/19/2022 18:25:08 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.65625 on epoch=649
06/19/2022 18:25:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
06/19/2022 18:25:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/19/2022 18:25:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/19/2022 18:25:14 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/19/2022 18:25:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/19/2022 18:25:16 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.6875 on epoch=674
06/19/2022 18:25:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/19/2022 18:25:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
06/19/2022 18:25:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/19/2022 18:25:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/19/2022 18:25:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=699
06/19/2022 18:25:23 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.65625 on epoch=699
06/19/2022 18:25:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/19/2022 18:25:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/19/2022 18:25:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/19/2022 18:25:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/19/2022 18:25:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/19/2022 18:25:31 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.6875 on epoch=724
06/19/2022 18:25:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/19/2022 18:25:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
06/19/2022 18:25:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/19/2022 18:25:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/19/2022 18:25:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/19/2022 18:25:39 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.6875 on epoch=749
06/19/2022 18:25:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/19/2022 18:25:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/19/2022 18:25:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/19/2022 18:25:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/19/2022 18:25:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/19/2022 18:25:46 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.65625 on epoch=774
06/19/2022 18:25:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
06/19/2022 18:25:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=784
06/19/2022 18:25:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/19/2022 18:25:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/19/2022 18:25:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/19/2022 18:25:53 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.625 on epoch=799
06/19/2022 18:25:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/19/2022 18:25:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/19/2022 18:25:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/19/2022 18:25:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/19/2022 18:26:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/19/2022 18:26:01 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.65625 on epoch=824
06/19/2022 18:26:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/19/2022 18:26:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/19/2022 18:26:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/19/2022 18:26:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/19/2022 18:26:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/19/2022 18:26:09 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.625 on epoch=849
06/19/2022 18:26:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/19/2022 18:26:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/19/2022 18:26:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/19/2022 18:26:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/19/2022 18:26:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/19/2022 18:26:17 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.65625 on epoch=874
06/19/2022 18:26:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/19/2022 18:26:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/19/2022 18:26:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/19/2022 18:26:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/19/2022 18:26:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/19/2022 18:26:25 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.59375 on epoch=899
06/19/2022 18:26:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/19/2022 18:26:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/19/2022 18:26:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/19/2022 18:26:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/19/2022 18:26:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=924
06/19/2022 18:26:33 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.59375 on epoch=924
06/19/2022 18:26:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/19/2022 18:26:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
06/19/2022 18:26:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/19/2022 18:26:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/19/2022 18:26:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/19/2022 18:26:41 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.65625 on epoch=949
06/19/2022 18:26:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/19/2022 18:26:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/19/2022 18:26:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/19/2022 18:26:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/19/2022 18:26:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/19/2022 18:26:48 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.625 on epoch=974
06/19/2022 18:26:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/19/2022 18:26:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/19/2022 18:26:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
06/19/2022 18:26:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/19/2022 18:26:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/19/2022 18:26:56 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.6875 on epoch=999
06/19/2022 18:26:56 - INFO - __main__ - save last model!
06/19/2022 18:26:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 18:26:56 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 18:26:56 - INFO - __main__ - Printing 3 examples
06/19/2022 18:26:56 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 18:26:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:26:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 18:26:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:26:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 18:26:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:26:56 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:26:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:26:56 - INFO - __main__ - Printing 3 examples
06/19/2022 18:26:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 18:26:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:26:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 18:26:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:26:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 18:26:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:26:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 18:26:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:26:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:26:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:26:57 - INFO - __main__ - Printing 3 examples
06/19/2022 18:26:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 18:26:57 - INFO - __main__ - ['equivalent']
06/19/2022 18:26:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 18:26:57 - INFO - __main__ - ['equivalent']
06/19/2022 18:26:57 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 18:26:57 - INFO - __main__ - ['equivalent']
06/19/2022 18:26:57 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:26:57 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:26:57 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:26:57 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:26:57 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 18:27:02 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:27:02 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(2502, 393505), (3485, 303196), (2451, 408517), (2124, 458893), (727, 605486), (159, 3283944), (550, 1652637), (2943, 377167), (4577, 197081), (536, 1096599), (4800, 205648), (3374, 227031), (4959, 162645), (2020, 470862), (3016, 244028), (6059, 189253), (3944, 234304), (2651, 378692), (1638, 604699), (62, 15842384), (1130, 821238), (4783, 219874), (1890, 417949), (1929, 517016), (5885, 180884), (5519, 201464), (568, 1556089), (6068, 167828), (2545, 370902), (1310, 731782), (4964, 205248), (1443, 666960), (4999, 201462), (5908, 176655), (4066, 167278), (6883, 157698), (3596, 297061), (3747, 211292), (5364, 185276), (4984, 218955), (4170, 170840), (5020, 210030), (1776, 567810), (5258, 197449), (4949, 208735), (2848, 356857), (2595, 374340), (2349, 335938), (5783, 180158), (5245, 192501), (2287, 388098), (2224, 338862), (3959, 212293), (852, 1044002), (2923, 327246), (1458, 394015), (1963, 341745), (3521, 280332), (2606, 294174), (7609, 173097), (830, 1012157), (968, 645839), (1487, 592557), (2198, 398890), (4674, 244421), (1627, 587044), (2068, 427366), (724, 1234015), (4657, 226628), (4614, 230206), (3467, 284152), (3304, 172257), (3291, 200786), (4441, 226147), (651, 1269510), (2214, 442372), (3497, 223253), (4716, 239808), (6497, 160316), (227, 4080344), (7555, 162623), (3924, 246414), (4666, 195723), (66, 13829035), (3645, 265964), (3236, 314080), (3229, 239354), (4804, 223950), (6162, 173710), (6040, 171279), (6152, 155746), (2428, 415536), (5503, 191100), (2374, 417354), (2949, 340763), (4836, 214321), (3385, 191872), (313, 2745242), (2206, 240868)]
06/19/2022 18:27:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:27:03 - INFO - __main__ - Starting training!
06/19/2022 18:27:06 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_13_0.5_8_predictions.txt
06/19/2022 18:27:06 - INFO - __main__ - ACC on test data: 0.5490
06/19/2022 18:27:06 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.5, bsz=8, dev_performance=0.71875, test_performance=0.5490196078431373
06/19/2022 18:27:06 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.4, bsz=8 ...
06/19/2022 18:27:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:27:07 - INFO - __main__ - Printing 3 examples
06/19/2022 18:27:07 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 18:27:07 - INFO - __main__ - ['equivalent']
06/19/2022 18:27:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 18:27:07 - INFO - __main__ - ['equivalent']
06/19/2022 18:27:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 18:27:07 - INFO - __main__ - ['equivalent']
06/19/2022 18:27:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 18:27:07 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:27:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:27:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:27:07 - INFO - __main__ - Printing 3 examples
06/19/2022 18:27:07 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 18:27:07 - INFO - __main__ - ['equivalent']
06/19/2022 18:27:07 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 18:27:07 - INFO - __main__ - ['equivalent']
06/19/2022 18:27:07 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 18:27:07 - INFO - __main__ - ['equivalent']
06/19/2022 18:27:07 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:27:07 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:27:07 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:27:13 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:27:13 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(2502, 393505), (3485, 303196), (2451, 408517), (2124, 458893), (727, 605486), (159, 3283944), (550, 1652637), (2943, 377167), (4577, 197081), (536, 1096599), (4800, 205648), (3374, 227031), (4959, 162645), (2020, 470862), (3016, 244028), (6059, 189253), (3944, 234304), (2651, 378692), (1638, 604699), (62, 15842384), (1130, 821238), (4783, 219874), (1890, 417949), (1929, 517016), (5885, 180884), (5519, 201464), (568, 1556089), (6068, 167828), (2545, 370902), (1310, 731782), (4964, 205248), (1443, 666960), (4999, 201462), (5908, 176655), (4066, 167278), (6883, 157698), (3596, 297061), (3747, 211292), (5364, 185276), (4984, 218955), (4170, 170840), (5020, 210030), (1776, 567810), (5258, 197449), (4949, 208735), (2848, 356857), (2595, 374340), (2349, 335938), (5783, 180158), (5245, 192501), (2287, 388098), (2224, 338862), (3959, 212293), (852, 1044002), (2923, 327246), (1458, 394015), (1963, 341745), (3521, 280332), (2606, 294174), (7609, 173097), (830, 1012157), (968, 645839), (1487, 592557), (2198, 398890), (4674, 244421), (1627, 587044), (2068, 427366), (724, 1234015), (4657, 226628), (4614, 230206), (3467, 284152), (3304, 172257), (3291, 200786), (4441, 226147), (651, 1269510), (2214, 442372), (3497, 223253), (4716, 239808), (6497, 160316), (227, 4080344), (7555, 162623), (3924, 246414), (4666, 195723), (66, 13829035), (3645, 265964), (3236, 314080), (3229, 239354), (4804, 223950), (6162, 173710), (6040, 171279), (6152, 155746), (2428, 415536), (5503, 191100), (2374, 417354), (2949, 340763), (4836, 214321), (3385, 191872), (313, 2745242), (2206, 240868)]
06/19/2022 18:27:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:27:14 - INFO - __main__ - Starting training!
06/19/2022 18:27:15 - INFO - __main__ - Step 10 Global step 10 Train loss 6.22 on epoch=4
06/19/2022 18:27:17 - INFO - __main__ - Step 20 Global step 20 Train loss 3.10 on epoch=9
06/19/2022 18:27:18 - INFO - __main__ - Step 30 Global step 30 Train loss 1.41 on epoch=14
06/19/2022 18:27:19 - INFO - __main__ - Step 40 Global step 40 Train loss 0.79 on epoch=19
06/19/2022 18:27:21 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=24
06/19/2022 18:27:21 - INFO - __main__ - Global step 50 Train loss 2.40 ACC 0.46875 on epoch=24
06/19/2022 18:27:21 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.46875 on epoch=24, global_step=50
06/19/2022 18:27:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=29
06/19/2022 18:27:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=34
06/19/2022 18:27:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=39
06/19/2022 18:27:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.73 on epoch=44
06/19/2022 18:27:28 - INFO - __main__ - Step 100 Global step 100 Train loss 1.35 on epoch=49
06/19/2022 18:27:29 - INFO - __main__ - Global step 100 Train loss 0.73 ACC 0.5 on epoch=49
06/19/2022 18:27:29 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=49, global_step=100
06/19/2022 18:27:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.41 on epoch=54
06/19/2022 18:27:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.37 on epoch=59
06/19/2022 18:27:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=64
06/19/2022 18:27:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=69
06/19/2022 18:27:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.33 on epoch=74
06/19/2022 18:27:37 - INFO - __main__ - Global step 150 Train loss 0.40 ACC 0.5 on epoch=74
06/19/2022 18:27:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=79
06/19/2022 18:27:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=84
06/19/2022 18:27:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.34 on epoch=89
06/19/2022 18:27:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=94
06/19/2022 18:27:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.31 on epoch=99
06/19/2022 18:27:45 - INFO - __main__ - Global step 200 Train loss 0.36 ACC 0.5 on epoch=99
06/19/2022 18:27:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.35 on epoch=104
06/19/2022 18:27:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
06/19/2022 18:27:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=114
06/19/2022 18:27:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=119
06/19/2022 18:27:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.30 on epoch=124
06/19/2022 18:27:53 - INFO - __main__ - Global step 250 Train loss 0.35 ACC 0.5 on epoch=124
06/19/2022 18:27:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.32 on epoch=129
06/19/2022 18:27:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.33 on epoch=134
06/19/2022 18:27:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.35 on epoch=139
06/19/2022 18:27:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=144
06/19/2022 18:27:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=149
06/19/2022 18:28:00 - INFO - __main__ - Global step 300 Train loss 0.35 ACC 0.5 on epoch=149
06/19/2022 18:28:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.30 on epoch=154
06/19/2022 18:28:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=159
06/19/2022 18:28:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
06/19/2022 18:28:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.32 on epoch=169
06/19/2022 18:28:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=174
06/19/2022 18:28:07 - INFO - __main__ - Global step 350 Train loss 0.30 ACC 0.5 on epoch=174
06/19/2022 18:28:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.31 on epoch=179
06/19/2022 18:28:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.32 on epoch=184
06/19/2022 18:28:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=189
06/19/2022 18:28:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.34 on epoch=194
06/19/2022 18:28:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.32 on epoch=199
06/19/2022 18:28:15 - INFO - __main__ - Global step 400 Train loss 0.32 ACC 0.5 on epoch=199
06/19/2022 18:28:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=204
06/19/2022 18:28:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.32 on epoch=209
06/19/2022 18:28:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.30 on epoch=214
06/19/2022 18:28:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=219
06/19/2022 18:28:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
06/19/2022 18:28:22 - INFO - __main__ - Global step 450 Train loss 0.29 ACC 0.5 on epoch=224
06/19/2022 18:28:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.30 on epoch=229
06/19/2022 18:28:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=234
06/19/2022 18:28:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=239
06/19/2022 18:28:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=244
06/19/2022 18:28:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=249
06/19/2022 18:28:29 - INFO - __main__ - Global step 500 Train loss 0.28 ACC 0.5 on epoch=249
06/19/2022 18:28:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.30 on epoch=254
06/19/2022 18:28:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
06/19/2022 18:28:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.30 on epoch=264
06/19/2022 18:28:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=269
06/19/2022 18:28:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=274
06/19/2022 18:28:37 - INFO - __main__ - Global step 550 Train loss 0.28 ACC 0.5 on epoch=274
06/19/2022 18:28:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=279
06/19/2022 18:28:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=284
06/19/2022 18:28:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.29 on epoch=289
06/19/2022 18:28:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=294
06/19/2022 18:28:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=299
06/19/2022 18:28:44 - INFO - __main__ - Global step 600 Train loss 0.29 ACC 0.5 on epoch=299
06/19/2022 18:28:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=304
06/19/2022 18:28:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=309
06/19/2022 18:28:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.29 on epoch=314
06/19/2022 18:28:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=319
06/19/2022 18:28:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=324
06/19/2022 18:28:52 - INFO - __main__ - Global step 650 Train loss 0.25 ACC 0.5 on epoch=324
06/19/2022 18:28:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=329
06/19/2022 18:28:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=334
06/19/2022 18:28:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=339
06/19/2022 18:28:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=344
06/19/2022 18:28:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=349
06/19/2022 18:28:59 - INFO - __main__ - Global step 700 Train loss 0.25 ACC 0.5 on epoch=349
06/19/2022 18:29:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=354
06/19/2022 18:29:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=359
06/19/2022 18:29:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=364
06/19/2022 18:29:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=369
06/19/2022 18:29:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=374
06/19/2022 18:29:06 - INFO - __main__ - Global step 750 Train loss 0.23 ACC 0.5 on epoch=374
06/19/2022 18:29:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=379
06/19/2022 18:29:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=384
06/19/2022 18:29:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=389
06/19/2022 18:29:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=394
06/19/2022 18:29:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=399
06/19/2022 18:29:14 - INFO - __main__ - Global step 800 Train loss 0.22 ACC 0.5 on epoch=399
06/19/2022 18:29:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=404
06/19/2022 18:29:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.29 on epoch=409
06/19/2022 18:29:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=414
06/19/2022 18:29:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=419
06/19/2022 18:29:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=424
06/19/2022 18:29:22 - INFO - __main__ - Global step 850 Train loss 0.21 ACC 0.4375 on epoch=424
06/19/2022 18:29:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=429
06/19/2022 18:29:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.19 on epoch=434
06/19/2022 18:29:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=439
06/19/2022 18:29:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=444
06/19/2022 18:29:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=449
06/19/2022 18:29:30 - INFO - __main__ - Global step 900 Train loss 0.19 ACC 0.5 on epoch=449
06/19/2022 18:29:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=454
06/19/2022 18:29:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=459
06/19/2022 18:29:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=464
06/19/2022 18:29:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=469
06/19/2022 18:29:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=474
06/19/2022 18:29:37 - INFO - __main__ - Global step 950 Train loss 0.18 ACC 0.5 on epoch=474
06/19/2022 18:29:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=479
06/19/2022 18:29:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.16 on epoch=484
06/19/2022 18:29:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=489
06/19/2022 18:29:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=494
06/19/2022 18:29:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.17 on epoch=499
06/19/2022 18:29:45 - INFO - __main__ - Global step 1000 Train loss 0.16 ACC 0.59375 on epoch=499
06/19/2022 18:29:45 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=499, global_step=1000
06/19/2022 18:29:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.12 on epoch=504
06/19/2022 18:29:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=509
06/19/2022 18:29:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=514
06/19/2022 18:29:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=519
06/19/2022 18:29:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=524
06/19/2022 18:29:54 - INFO - __main__ - Global step 1050 Train loss 0.14 ACC 0.46875 on epoch=524
06/19/2022 18:29:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=529
06/19/2022 18:29:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=534
06/19/2022 18:29:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=539
06/19/2022 18:30:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=544
06/19/2022 18:30:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.11 on epoch=549
06/19/2022 18:30:02 - INFO - __main__ - Global step 1100 Train loss 0.12 ACC 0.5 on epoch=549
06/19/2022 18:30:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=554
06/19/2022 18:30:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=559
06/19/2022 18:30:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=564
06/19/2022 18:30:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=569
06/19/2022 18:30:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=574
06/19/2022 18:30:10 - INFO - __main__ - Global step 1150 Train loss 0.15 ACC 0.46875 on epoch=574
06/19/2022 18:30:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=579
06/19/2022 18:30:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=584
06/19/2022 18:30:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.12 on epoch=589
06/19/2022 18:30:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=594
06/19/2022 18:30:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=599
06/19/2022 18:30:17 - INFO - __main__ - Global step 1200 Train loss 0.12 ACC 0.4375 on epoch=599
06/19/2022 18:30:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=604
06/19/2022 18:30:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=609
06/19/2022 18:30:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=614
06/19/2022 18:30:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=619
06/19/2022 18:30:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.15 on epoch=624
06/19/2022 18:30:24 - INFO - __main__ - Global step 1250 Train loss 0.10 ACC 0.5 on epoch=624
06/19/2022 18:30:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=629
06/19/2022 18:30:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=634
06/19/2022 18:30:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=639
06/19/2022 18:30:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=644
06/19/2022 18:30:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=649
06/19/2022 18:30:32 - INFO - __main__ - Global step 1300 Train loss 0.10 ACC 0.40625 on epoch=649
06/19/2022 18:30:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=654
06/19/2022 18:30:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=659
06/19/2022 18:30:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=664
06/19/2022 18:30:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
06/19/2022 18:30:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=674
06/19/2022 18:30:40 - INFO - __main__ - Global step 1350 Train loss 0.12 ACC 0.5 on epoch=674
06/19/2022 18:30:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=679
06/19/2022 18:30:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=684
06/19/2022 18:30:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=689
06/19/2022 18:30:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=694
06/19/2022 18:30:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=699
06/19/2022 18:30:48 - INFO - __main__ - Global step 1400 Train loss 0.09 ACC 0.5 on epoch=699
06/19/2022 18:30:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=704
06/19/2022 18:30:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=709
06/19/2022 18:30:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=714
06/19/2022 18:30:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=719
06/19/2022 18:30:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=724
06/19/2022 18:30:56 - INFO - __main__ - Global step 1450 Train loss 0.08 ACC 0.53125 on epoch=724
06/19/2022 18:30:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=729
06/19/2022 18:30:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=734
06/19/2022 18:31:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
06/19/2022 18:31:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=744
06/19/2022 18:31:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=749
06/19/2022 18:31:04 - INFO - __main__ - Global step 1500 Train loss 0.07 ACC 0.53125 on epoch=749
06/19/2022 18:31:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
06/19/2022 18:31:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
06/19/2022 18:31:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=764
06/19/2022 18:31:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
06/19/2022 18:31:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
06/19/2022 18:31:12 - INFO - __main__ - Global step 1550 Train loss 0.04 ACC 0.5 on epoch=774
06/19/2022 18:31:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=779
06/19/2022 18:31:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=784
06/19/2022 18:31:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=789
06/19/2022 18:31:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=794
06/19/2022 18:31:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
06/19/2022 18:31:20 - INFO - __main__ - Global step 1600 Train loss 0.06 ACC 0.46875 on epoch=799
06/19/2022 18:31:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
06/19/2022 18:31:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
06/19/2022 18:31:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/19/2022 18:31:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
06/19/2022 18:31:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=824
06/19/2022 18:31:28 - INFO - __main__ - Global step 1650 Train loss 0.04 ACC 0.53125 on epoch=824
06/19/2022 18:31:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=829
06/19/2022 18:31:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=834
06/19/2022 18:31:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/19/2022 18:31:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/19/2022 18:31:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
06/19/2022 18:31:36 - INFO - __main__ - Global step 1700 Train loss 0.03 ACC 0.4375 on epoch=849
06/19/2022 18:31:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
06/19/2022 18:31:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=859
06/19/2022 18:31:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
06/19/2022 18:31:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=869
06/19/2022 18:31:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=874
06/19/2022 18:31:44 - INFO - __main__ - Global step 1750 Train loss 0.04 ACC 0.59375 on epoch=874
06/19/2022 18:31:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
06/19/2022 18:31:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=884
06/19/2022 18:31:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=889
06/19/2022 18:31:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=894
06/19/2022 18:31:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=899
06/19/2022 18:31:51 - INFO - __main__ - Global step 1800 Train loss 0.04 ACC 0.5625 on epoch=899
06/19/2022 18:31:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
06/19/2022 18:31:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=909
06/19/2022 18:31:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/19/2022 18:31:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
06/19/2022 18:31:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=924
06/19/2022 18:31:59 - INFO - __main__ - Global step 1850 Train loss 0.03 ACC 0.53125 on epoch=924
06/19/2022 18:32:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=929
06/19/2022 18:32:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
06/19/2022 18:32:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=939
06/19/2022 18:32:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
06/19/2022 18:32:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=949
06/19/2022 18:32:07 - INFO - __main__ - Global step 1900 Train loss 0.04 ACC 0.71875 on epoch=949
06/19/2022 18:32:07 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.71875 on epoch=949, global_step=1900
06/19/2022 18:32:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/19/2022 18:32:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=959
06/19/2022 18:32:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/19/2022 18:32:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=969
06/19/2022 18:32:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.12 on epoch=974
06/19/2022 18:32:14 - INFO - __main__ - Global step 1950 Train loss 0.05 ACC 0.75 on epoch=974
06/19/2022 18:32:14 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.75 on epoch=974, global_step=1950
06/19/2022 18:32:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
06/19/2022 18:32:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=984
06/19/2022 18:32:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/19/2022 18:32:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
06/19/2022 18:32:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
06/19/2022 18:32:21 - INFO - __main__ - Global step 2000 Train loss 0.03 ACC 0.59375 on epoch=999
06/19/2022 18:32:21 - INFO - __main__ - save last model!
06/19/2022 18:32:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 18:32:21 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 18:32:21 - INFO - __main__ - Printing 3 examples
06/19/2022 18:32:21 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 18:32:21 - INFO - __main__ - ['equivalent']
06/19/2022 18:32:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 18:32:21 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:32:21 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 18:32:21 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:32:21 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:32:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:32:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:32:22 - INFO - __main__ - Printing 3 examples
06/19/2022 18:32:22 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 18:32:22 - INFO - __main__ - ['equivalent']
06/19/2022 18:32:22 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 18:32:22 - INFO - __main__ - ['equivalent']
06/19/2022 18:32:22 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 18:32:22 - INFO - __main__ - ['equivalent']
06/19/2022 18:32:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 18:32:22 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:32:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:32:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:32:22 - INFO - __main__ - Printing 3 examples
06/19/2022 18:32:22 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 18:32:22 - INFO - __main__ - ['equivalent']
06/19/2022 18:32:22 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 18:32:22 - INFO - __main__ - ['equivalent']
06/19/2022 18:32:22 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 18:32:22 - INFO - __main__ - ['equivalent']
06/19/2022 18:32:22 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:32:22 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:32:22 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:32:22 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 18:32:29 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:32:29 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(3666, 279856), (4279, 231910), (1376, 549699), (5099, 200518), (1480, 693138), (5956, 175083), (317, 2909017), (3859, 237931), (6537, 158593), (6422, 165641), (328, 2897449), (915, 980631), (6366, 217988), (1303, 658549), (5169, 202438), (1884, 502256), (568, 1556089), (1489, 355866), (4036, 247461), (2445, 287205), (2154, 432132), (5074, 206086), (3513, 287362), (2658, 279047), (2412, 334412), (2660, 222403), (2656, 367203), (882, 1004086), (4935, 199405), (115, 4101345), (5058, 173227), (593, 1511490), (3830, 267200), (2935, 409303), (726, 1159344), (464, 1931312), (262, 2562554), (2595, 374340), (2114, 458811), (231, 4022141), (1689, 588233), (2627, 389531), (1048, 702299), (5656, 197667), (1341, 767639), (4837, 165767), (2418, 420267), (5111, 201364), (2433, 361242), (3976, 254648), (1801, 537800), (3849, 155187), (6553, 154915), (1214, 767509), (1682, 553032), (3355, 302330), (1056, 811824), (5087, 161786), (196, 3104995), (2464, 301408), (7555, 162623), (3004, 249813), (4349, 236587), (1978, 460084), (448, 1064987), (5917, 158493), (2612, 380946), (1078, 844362), (2715, 311751), (5798, 156482), (1137, 734426), (6326, 165064), (456, 1871965), (3146, 269081), (1703, 175726), (4773, 218071), (3699, 253359), (746, 1156068), (5696, 184838), (3669, 275693), (165, 6245475), (3497, 223253), (4240, 190278), (1195, 825391), (400, 215271), (2938, 259506), (3095, 203292), (1433, 586613), (4400, 214423), (1995, 516280), (5706, 180057), (2953, 344008), (3396, 173723), (3557, 193090), (5002, 201395), (4365, 214420), (4936, 158408), (2855, 345487), (3008, 291537)]
06/19/2022 18:32:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:32:29 - INFO - __main__ - Starting training!
06/19/2022 18:32:31 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_13_0.4_8_predictions.txt
06/19/2022 18:32:31 - INFO - __main__ - ACC on test data: 0.4828
06/19/2022 18:32:31 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.4, bsz=8, dev_performance=0.75, test_performance=0.48284313725490197
06/19/2022 18:32:31 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.3, bsz=8 ...
06/19/2022 18:32:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:32:32 - INFO - __main__ - Printing 3 examples
06/19/2022 18:32:32 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 18:32:32 - INFO - __main__ - ['equivalent']
06/19/2022 18:32:32 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 18:32:32 - INFO - __main__ - ['equivalent']
06/19/2022 18:32:32 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 18:32:32 - INFO - __main__ - ['equivalent']
06/19/2022 18:32:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 18:32:32 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:32:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:32:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:32:32 - INFO - __main__ - Printing 3 examples
06/19/2022 18:32:32 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 18:32:32 - INFO - __main__ - ['equivalent']
06/19/2022 18:32:32 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 18:32:32 - INFO - __main__ - ['equivalent']
06/19/2022 18:32:32 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 18:32:32 - INFO - __main__ - ['equivalent']
06/19/2022 18:32:32 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:32:32 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:32:32 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:32:39 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:32:39 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(3666, 279856), (4279, 231910), (1376, 549699), (5099, 200518), (1480, 693138), (5956, 175083), (317, 2909017), (3859, 237931), (6537, 158593), (6422, 165641), (328, 2897449), (915, 980631), (6366, 217988), (1303, 658549), (5169, 202438), (1884, 502256), (568, 1556089), (1489, 355866), (4036, 247461), (2445, 287205), (2154, 432132), (5074, 206086), (3513, 287362), (2658, 279047), (2412, 334412), (2660, 222403), (2656, 367203), (882, 1004086), (4935, 199405), (115, 4101345), (5058, 173227), (593, 1511490), (3830, 267200), (2935, 409303), (726, 1159344), (464, 1931312), (262, 2562554), (2595, 374340), (2114, 458811), (231, 4022141), (1689, 588233), (2627, 389531), (1048, 702299), (5656, 197667), (1341, 767639), (4837, 165767), (2418, 420267), (5111, 201364), (2433, 361242), (3976, 254648), (1801, 537800), (3849, 155187), (6553, 154915), (1214, 767509), (1682, 553032), (3355, 302330), (1056, 811824), (5087, 161786), (196, 3104995), (2464, 301408), (7555, 162623), (3004, 249813), (4349, 236587), (1978, 460084), (448, 1064987), (5917, 158493), (2612, 380946), (1078, 844362), (2715, 311751), (5798, 156482), (1137, 734426), (6326, 165064), (456, 1871965), (3146, 269081), (1703, 175726), (4773, 218071), (3699, 253359), (746, 1156068), (5696, 184838), (3669, 275693), (165, 6245475), (3497, 223253), (4240, 190278), (1195, 825391), (400, 215271), (2938, 259506), (3095, 203292), (1433, 586613), (4400, 214423), (1995, 516280), (5706, 180057), (2953, 344008), (3396, 173723), (3557, 193090), (5002, 201395), (4365, 214420), (4936, 158408), (2855, 345487), (3008, 291537)]
06/19/2022 18:32:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:32:40 - INFO - __main__ - Starting training!
06/19/2022 18:32:41 - INFO - __main__ - Step 10 Global step 10 Train loss 6.42 on epoch=4
06/19/2022 18:32:43 - INFO - __main__ - Step 20 Global step 20 Train loss 3.88 on epoch=9
06/19/2022 18:32:44 - INFO - __main__ - Step 30 Global step 30 Train loss 2.14 on epoch=14
06/19/2022 18:32:46 - INFO - __main__ - Step 40 Global step 40 Train loss 1.19 on epoch=19
06/19/2022 18:32:47 - INFO - __main__ - Step 50 Global step 50 Train loss 0.68 on epoch=24
06/19/2022 18:32:48 - INFO - __main__ - Global step 50 Train loss 2.86 ACC 0.5 on epoch=24
06/19/2022 18:32:48 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 18:32:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=29
06/19/2022 18:32:50 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=34
06/19/2022 18:32:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=39
06/19/2022 18:32:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=44
06/19/2022 18:32:54 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=49
06/19/2022 18:32:55 - INFO - __main__ - Global step 100 Train loss 0.45 ACC 0.5 on epoch=49
06/19/2022 18:32:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.33 on epoch=54
06/19/2022 18:32:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=59
06/19/2022 18:32:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=64
06/19/2022 18:33:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.32 on epoch=69
06/19/2022 18:33:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.35 on epoch=74
06/19/2022 18:33:03 - INFO - __main__ - Global step 150 Train loss 0.37 ACC 0.53125 on epoch=74
06/19/2022 18:33:03 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=74, global_step=150
06/19/2022 18:33:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.29 on epoch=79
06/19/2022 18:33:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.33 on epoch=84
06/19/2022 18:33:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=89
06/19/2022 18:33:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.24 on epoch=94
06/19/2022 18:33:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=99
06/19/2022 18:33:11 - INFO - __main__ - Global step 200 Train loss 0.30 ACC 0.5 on epoch=99
06/19/2022 18:33:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.23 on epoch=104
06/19/2022 18:33:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=109
06/19/2022 18:33:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
06/19/2022 18:33:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.28 on epoch=119
06/19/2022 18:33:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=124
06/19/2022 18:33:18 - INFO - __main__ - Global step 250 Train loss 0.30 ACC 0.5 on epoch=124
06/19/2022 18:33:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.57 on epoch=129
06/19/2022 18:33:21 - INFO - __main__ - Step 270 Global step 270 Train loss 2.55 on epoch=134
06/19/2022 18:33:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.64 on epoch=139
06/19/2022 18:33:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=144
06/19/2022 18:33:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.28 on epoch=149
06/19/2022 18:33:26 - INFO - __main__ - Global step 300 Train loss 0.87 ACC 0.625 on epoch=149
06/19/2022 18:33:26 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.625 on epoch=149, global_step=300
06/19/2022 18:33:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=154
06/19/2022 18:33:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.30 on epoch=159
06/19/2022 18:33:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
06/19/2022 18:33:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=169
06/19/2022 18:33:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=174
06/19/2022 18:33:34 - INFO - __main__ - Global step 350 Train loss 0.28 ACC 0.625 on epoch=174
06/19/2022 18:33:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=179
06/19/2022 18:33:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.28 on epoch=184
06/19/2022 18:33:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=189
06/19/2022 18:33:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=194
06/19/2022 18:33:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=199
06/19/2022 18:33:41 - INFO - __main__ - Global step 400 Train loss 0.28 ACC 0.59375 on epoch=199
06/19/2022 18:33:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
06/19/2022 18:33:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=209
06/19/2022 18:33:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
06/19/2022 18:33:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=219
06/19/2022 18:33:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=224
06/19/2022 18:33:48 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.59375 on epoch=224
06/19/2022 18:33:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
06/19/2022 18:33:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=234
06/19/2022 18:33:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
06/19/2022 18:33:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=244
06/19/2022 18:33:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=249
06/19/2022 18:33:56 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.5625 on epoch=249
06/19/2022 18:33:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=254
06/19/2022 18:33:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=259
06/19/2022 18:33:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
06/19/2022 18:34:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=269
06/19/2022 18:34:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=274
06/19/2022 18:34:03 - INFO - __main__ - Global step 550 Train loss 0.27 ACC 0.5625 on epoch=274
06/19/2022 18:34:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=279
06/19/2022 18:34:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=284
06/19/2022 18:34:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=289
06/19/2022 18:34:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=294
06/19/2022 18:34:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=299
06/19/2022 18:34:10 - INFO - __main__ - Global step 600 Train loss 0.27 ACC 0.5625 on epoch=299
06/19/2022 18:34:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=304
06/19/2022 18:34:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=309
06/19/2022 18:34:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
06/19/2022 18:34:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=319
06/19/2022 18:34:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=324
06/19/2022 18:34:18 - INFO - __main__ - Global step 650 Train loss 0.22 ACC 0.5625 on epoch=324
06/19/2022 18:34:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=329
06/19/2022 18:34:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=334
06/19/2022 18:34:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=339
06/19/2022 18:34:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=344
06/19/2022 18:34:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.17 on epoch=349
06/19/2022 18:34:27 - INFO - __main__ - Global step 700 Train loss 0.19 ACC 0.46875 on epoch=349
06/19/2022 18:34:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=354
06/19/2022 18:34:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=359
06/19/2022 18:34:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=364
06/19/2022 18:34:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=369
06/19/2022 18:34:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=374
06/19/2022 18:34:34 - INFO - __main__ - Global step 750 Train loss 0.18 ACC 0.5 on epoch=374
06/19/2022 18:34:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=379
06/19/2022 18:34:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.16 on epoch=384
06/19/2022 18:34:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.14 on epoch=389
06/19/2022 18:34:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=394
06/19/2022 18:34:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=399
06/19/2022 18:34:42 - INFO - __main__ - Global step 800 Train loss 0.16 ACC 0.4375 on epoch=399
06/19/2022 18:34:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=404
06/19/2022 18:34:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=409
06/19/2022 18:34:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=414
06/19/2022 18:34:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=419
06/19/2022 18:34:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=424
06/19/2022 18:34:50 - INFO - __main__ - Global step 850 Train loss 0.14 ACC 0.34375 on epoch=424
06/19/2022 18:34:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=429
06/19/2022 18:34:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=434
06/19/2022 18:34:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=439
06/19/2022 18:34:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=444
06/19/2022 18:34:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=449
06/19/2022 18:34:57 - INFO - __main__ - Global step 900 Train loss 0.13 ACC 0.46875 on epoch=449
06/19/2022 18:34:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=454
06/19/2022 18:35:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.11 on epoch=459
06/19/2022 18:35:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=464
06/19/2022 18:35:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=469
06/19/2022 18:35:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=474
06/19/2022 18:35:04 - INFO - __main__ - Global step 950 Train loss 0.13 ACC 0.5625 on epoch=474
06/19/2022 18:35:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=479
06/19/2022 18:35:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=484
06/19/2022 18:35:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=489
06/19/2022 18:35:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=494
06/19/2022 18:35:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=499
06/19/2022 18:35:12 - INFO - __main__ - Global step 1000 Train loss 0.11 ACC 0.4375 on epoch=499
06/19/2022 18:35:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
06/19/2022 18:35:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=509
06/19/2022 18:35:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
06/19/2022 18:35:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=519
06/19/2022 18:35:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=524
06/19/2022 18:35:20 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.46875 on epoch=524
06/19/2022 18:35:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=529
06/19/2022 18:35:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=534
06/19/2022 18:35:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
06/19/2022 18:35:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=544
06/19/2022 18:35:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
06/19/2022 18:35:28 - INFO - __main__ - Global step 1100 Train loss 0.08 ACC 0.46875 on epoch=549
06/19/2022 18:35:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
06/19/2022 18:35:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
06/19/2022 18:35:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
06/19/2022 18:35:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=569
06/19/2022 18:35:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=574
06/19/2022 18:35:37 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.46875 on epoch=574
06/19/2022 18:35:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
06/19/2022 18:35:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=584
06/19/2022 18:35:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.11 on epoch=589
06/19/2022 18:35:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
06/19/2022 18:35:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
06/19/2022 18:35:44 - INFO - __main__ - Global step 1200 Train loss 0.07 ACC 0.4375 on epoch=599
06/19/2022 18:35:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
06/19/2022 18:35:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=609
06/19/2022 18:35:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
06/19/2022 18:35:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=619
06/19/2022 18:35:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=624
06/19/2022 18:35:52 - INFO - __main__ - Global step 1250 Train loss 0.05 ACC 0.4375 on epoch=624
06/19/2022 18:35:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
06/19/2022 18:35:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
06/19/2022 18:35:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
06/19/2022 18:35:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.24 on epoch=644
06/19/2022 18:35:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.13 on epoch=649
06/19/2022 18:36:00 - INFO - __main__ - Global step 1300 Train loss 0.10 ACC 0.46875 on epoch=649
06/19/2022 18:36:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=654
06/19/2022 18:36:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=659
06/19/2022 18:36:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=664
06/19/2022 18:36:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.17 on epoch=669
06/19/2022 18:36:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
06/19/2022 18:36:07 - INFO - __main__ - Global step 1350 Train loss 0.10 ACC 0.5 on epoch=674
06/19/2022 18:36:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=679
06/19/2022 18:36:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/19/2022 18:36:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=689
06/19/2022 18:36:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=694
06/19/2022 18:36:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/19/2022 18:36:16 - INFO - __main__ - Global step 1400 Train loss 0.06 ACC 0.5 on epoch=699
06/19/2022 18:36:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=704
06/19/2022 18:36:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=709
06/19/2022 18:36:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=714
06/19/2022 18:36:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=719
06/19/2022 18:36:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=724
06/19/2022 18:36:23 - INFO - __main__ - Global step 1450 Train loss 0.08 ACC 0.5 on epoch=724
06/19/2022 18:36:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
06/19/2022 18:36:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
06/19/2022 18:36:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
06/19/2022 18:36:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=744
06/19/2022 18:36:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=749
06/19/2022 18:36:31 - INFO - __main__ - Global step 1500 Train loss 0.05 ACC 0.53125 on epoch=749
06/19/2022 18:36:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
06/19/2022 18:36:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=759
06/19/2022 18:36:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/19/2022 18:36:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
06/19/2022 18:36:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=774
06/19/2022 18:36:39 - INFO - __main__ - Global step 1550 Train loss 0.04 ACC 0.4375 on epoch=774
06/19/2022 18:36:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=779
06/19/2022 18:36:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=784
06/19/2022 18:36:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
06/19/2022 18:36:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=794
06/19/2022 18:36:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.11 on epoch=799
06/19/2022 18:36:47 - INFO - __main__ - Global step 1600 Train loss 0.08 ACC 0.5 on epoch=799
06/19/2022 18:36:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=804
06/19/2022 18:36:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=809
06/19/2022 18:36:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
06/19/2022 18:36:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=819
06/19/2022 18:36:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
06/19/2022 18:36:55 - INFO - __main__ - Global step 1650 Train loss 0.04 ACC 0.5 on epoch=824
06/19/2022 18:36:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=829
06/19/2022 18:36:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
06/19/2022 18:36:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=839
06/19/2022 18:37:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/19/2022 18:37:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=849
06/19/2022 18:37:02 - INFO - __main__ - Global step 1700 Train loss 0.05 ACC 0.53125 on epoch=849
06/19/2022 18:37:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
06/19/2022 18:37:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=859
06/19/2022 18:37:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/19/2022 18:37:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
06/19/2022 18:37:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
06/19/2022 18:37:09 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.4375 on epoch=874
06/19/2022 18:37:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
06/19/2022 18:37:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=884
06/19/2022 18:37:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
06/19/2022 18:37:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=894
06/19/2022 18:37:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
06/19/2022 18:37:16 - INFO - __main__ - Global step 1800 Train loss 0.04 ACC 0.46875 on epoch=899
06/19/2022 18:37:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=904
06/19/2022 18:37:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=909
06/19/2022 18:37:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
06/19/2022 18:37:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=919
06/19/2022 18:37:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
06/19/2022 18:37:25 - INFO - __main__ - Global step 1850 Train loss 0.04 ACC 0.40625 on epoch=924
06/19/2022 18:37:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/19/2022 18:37:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
06/19/2022 18:37:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/19/2022 18:37:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/19/2022 18:37:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=949
06/19/2022 18:37:32 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.375 on epoch=949
06/19/2022 18:37:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
06/19/2022 18:37:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=959
06/19/2022 18:37:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/19/2022 18:37:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
06/19/2022 18:37:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=974
06/19/2022 18:37:40 - INFO - __main__ - Global step 1950 Train loss 0.03 ACC 0.40625 on epoch=974
06/19/2022 18:37:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=979
06/19/2022 18:37:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=984
06/19/2022 18:37:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/19/2022 18:37:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
06/19/2022 18:37:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=999
06/19/2022 18:37:48 - INFO - __main__ - Global step 2000 Train loss 0.05 ACC 0.5 on epoch=999
06/19/2022 18:37:48 - INFO - __main__ - save last model!
06/19/2022 18:37:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 18:37:48 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 18:37:48 - INFO - __main__ - Printing 3 examples
06/19/2022 18:37:48 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 18:37:48 - INFO - __main__ - ['equivalent']
06/19/2022 18:37:48 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 18:37:48 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:37:48 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 18:37:48 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:37:48 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:37:48 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:37:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:37:49 - INFO - __main__ - Printing 3 examples
06/19/2022 18:37:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 18:37:49 - INFO - __main__ - ['equivalent']
06/19/2022 18:37:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 18:37:49 - INFO - __main__ - ['equivalent']
06/19/2022 18:37:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 18:37:49 - INFO - __main__ - ['equivalent']
06/19/2022 18:37:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 18:37:49 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:37:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:37:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:37:49 - INFO - __main__ - Printing 3 examples
06/19/2022 18:37:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 18:37:49 - INFO - __main__ - ['equivalent']
06/19/2022 18:37:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 18:37:49 - INFO - __main__ - ['equivalent']
06/19/2022 18:37:49 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 18:37:49 - INFO - __main__ - ['equivalent']
06/19/2022 18:37:49 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:37:49 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:37:49 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:37:49 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 18:37:54 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:37:54 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(1074, 376286), (672, 948582), (3489, 333521), (6319, 208284), (2690, 378727), (4999, 201462), (4302, 163912), (4946, 154063), (4264, 247591), (3735, 279578), (286, 2755883), (6059, 189253), (3679, 289187), (1958, 502002), (4233, 191191), (2476, 431307), (3339, 264161), (2818, 358712), (3812, 276017), (2550, 332246), (6597, 157538), (1047, 336403), (5997, 168391), (96, 7894015), (5806, 181362), (2027, 478074), (793, 959415), (2480, 384879), (4374, 230866), (3351, 190868), (6096, 168646), (2144, 386724), (3519, 196508), (4992, 190172), (4493, 209311), (106, 4519798), (928, 894961), (1923, 318040), (2361, 413361), (4279, 231910), (900, 380561), (2466, 307035), (6330, 161028), (3939, 254546), (4505, 197810), (5847, 169158), (4974, 176708), (3742, 272355), (3643, 219730), (4895, 169946), (3087, 284156), (2977, 265914), (3683, 206457), (2772, 342861), (3413, 306732), (2721, 367851), (2431, 400094), (1327, 735060), (1602, 475410), (2733, 298680), (1228, 739170), (4132, 174353), (2023, 494514), (2503, 477739), (2299, 422736), (93, 192649), (2957, 336282), (6637, 158975), (1665, 179358), (4163, 157427), (1635, 329909), (1116, 852428), (2082, 471031), (3118, 315958), (2028, 404951), (4197, 260190), (956, 507463), (3505, 311150), (164, 5765797), (5956, 175083), (1305, 698278), (2879, 339182), (503, 1684486), (600, 1458762), (6854, 169166), (3202, 321048), (1048, 702299), (4621, 190154), (1087, 843077), (131, 6653166), (6292, 163131), (3412, 328424), (5201, 199644), (3769, 195111), (5001, 211928), (3879, 275863), (2016, 495776), (581, 1531361), (2912, 367106)]
06/19/2022 18:37:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:37:55 - INFO - __main__ - Starting training!
06/19/2022 18:37:58 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_13_0.3_8_predictions.txt
06/19/2022 18:37:58 - INFO - __main__ - ACC on test data: 0.6127
06/19/2022 18:37:58 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.3, bsz=8, dev_performance=0.625, test_performance=0.6127450980392157
06/19/2022 18:37:58 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.2, bsz=8 ...
06/19/2022 18:37:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:37:59 - INFO - __main__ - Printing 3 examples
06/19/2022 18:37:59 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 18:37:59 - INFO - __main__ - ['equivalent']
06/19/2022 18:37:59 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 18:37:59 - INFO - __main__ - ['equivalent']
06/19/2022 18:37:59 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 18:37:59 - INFO - __main__ - ['equivalent']
06/19/2022 18:37:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 18:37:59 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:37:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:37:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:37:59 - INFO - __main__ - Printing 3 examples
06/19/2022 18:37:59 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 18:37:59 - INFO - __main__ - ['equivalent']
06/19/2022 18:37:59 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 18:37:59 - INFO - __main__ - ['equivalent']
06/19/2022 18:37:59 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 18:37:59 - INFO - __main__ - ['equivalent']
06/19/2022 18:37:59 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:37:59 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:38:00 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:38:06 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:38:06 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(1074, 376286), (672, 948582), (3489, 333521), (6319, 208284), (2690, 378727), (4999, 201462), (4302, 163912), (4946, 154063), (4264, 247591), (3735, 279578), (286, 2755883), (6059, 189253), (3679, 289187), (1958, 502002), (4233, 191191), (2476, 431307), (3339, 264161), (2818, 358712), (3812, 276017), (2550, 332246), (6597, 157538), (1047, 336403), (5997, 168391), (96, 7894015), (5806, 181362), (2027, 478074), (793, 959415), (2480, 384879), (4374, 230866), (3351, 190868), (6096, 168646), (2144, 386724), (3519, 196508), (4992, 190172), (4493, 209311), (106, 4519798), (928, 894961), (1923, 318040), (2361, 413361), (4279, 231910), (900, 380561), (2466, 307035), (6330, 161028), (3939, 254546), (4505, 197810), (5847, 169158), (4974, 176708), (3742, 272355), (3643, 219730), (4895, 169946), (3087, 284156), (2977, 265914), (3683, 206457), (2772, 342861), (3413, 306732), (2721, 367851), (2431, 400094), (1327, 735060), (1602, 475410), (2733, 298680), (1228, 739170), (4132, 174353), (2023, 494514), (2503, 477739), (2299, 422736), (93, 192649), (2957, 336282), (6637, 158975), (1665, 179358), (4163, 157427), (1635, 329909), (1116, 852428), (2082, 471031), (3118, 315958), (2028, 404951), (4197, 260190), (956, 507463), (3505, 311150), (164, 5765797), (5956, 175083), (1305, 698278), (2879, 339182), (503, 1684486), (600, 1458762), (6854, 169166), (3202, 321048), (1048, 702299), (4621, 190154), (1087, 843077), (131, 6653166), (6292, 163131), (3412, 328424), (5201, 199644), (3769, 195111), (5001, 211928), (3879, 275863), (2016, 495776), (581, 1531361), (2912, 367106)]
06/19/2022 18:38:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:38:07 - INFO - __main__ - Starting training!
06/19/2022 18:38:08 - INFO - __main__ - Step 10 Global step 10 Train loss 6.48 on epoch=4
06/19/2022 18:38:10 - INFO - __main__ - Step 20 Global step 20 Train loss 4.20 on epoch=9
06/19/2022 18:38:11 - INFO - __main__ - Step 30 Global step 30 Train loss 2.36 on epoch=14
06/19/2022 18:38:13 - INFO - __main__ - Step 40 Global step 40 Train loss 1.45 on epoch=19
06/19/2022 18:38:14 - INFO - __main__ - Step 50 Global step 50 Train loss 0.99 on epoch=24
06/19/2022 18:38:15 - INFO - __main__ - Global step 50 Train loss 3.09 ACC 0.5 on epoch=24
06/19/2022 18:38:15 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 18:38:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.84 on epoch=29
06/19/2022 18:38:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.71 on epoch=34
06/19/2022 18:38:19 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=39
06/19/2022 18:38:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=44
06/19/2022 18:38:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=49
06/19/2022 18:38:23 - INFO - __main__ - Global step 100 Train loss 0.64 ACC 0.5 on epoch=49
06/19/2022 18:38:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=54
06/19/2022 18:38:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=59
06/19/2022 18:38:27 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=64
06/19/2022 18:38:29 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=69
06/19/2022 18:38:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=74
06/19/2022 18:38:31 - INFO - __main__ - Global step 150 Train loss 0.46 ACC 0.5 on epoch=74
06/19/2022 18:38:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.36 on epoch=79
06/19/2022 18:38:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=84
06/19/2022 18:38:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
06/19/2022 18:38:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=94
06/19/2022 18:38:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=99
06/19/2022 18:38:38 - INFO - __main__ - Global step 200 Train loss 0.41 ACC 0.5 on epoch=99
06/19/2022 18:38:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=104
06/19/2022 18:38:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=109
06/19/2022 18:38:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=114
06/19/2022 18:38:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=119
06/19/2022 18:38:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=124
06/19/2022 18:38:46 - INFO - __main__ - Global step 250 Train loss 0.40 ACC 0.5 on epoch=124
06/19/2022 18:38:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=129
06/19/2022 18:38:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=134
06/19/2022 18:38:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
06/19/2022 18:38:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
06/19/2022 18:38:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=149
06/19/2022 18:38:54 - INFO - __main__ - Global step 300 Train loss 0.36 ACC 0.5 on epoch=149
06/19/2022 18:38:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=154
06/19/2022 18:38:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=159
06/19/2022 18:38:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=164
06/19/2022 18:39:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=169
06/19/2022 18:39:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=174
06/19/2022 18:39:02 - INFO - __main__ - Global step 350 Train loss 0.43 ACC 0.5 on epoch=174
06/19/2022 18:39:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=179
06/19/2022 18:39:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=184
06/19/2022 18:39:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=189
06/19/2022 18:39:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=194
06/19/2022 18:39:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=199
06/19/2022 18:39:10 - INFO - __main__ - Global step 400 Train loss 0.36 ACC 0.5 on epoch=199
06/19/2022 18:39:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=204
06/19/2022 18:39:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.30 on epoch=209
06/19/2022 18:39:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=214
06/19/2022 18:39:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=219
06/19/2022 18:39:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=224
06/19/2022 18:39:17 - INFO - __main__ - Global step 450 Train loss 0.32 ACC 0.5 on epoch=224
06/19/2022 18:39:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=229
06/19/2022 18:39:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
06/19/2022 18:39:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=239
06/19/2022 18:39:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.33 on epoch=244
06/19/2022 18:39:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=249
06/19/2022 18:39:25 - INFO - __main__ - Global step 500 Train loss 0.31 ACC 0.5 on epoch=249
06/19/2022 18:39:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.28 on epoch=254
06/19/2022 18:39:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.28 on epoch=259
06/19/2022 18:39:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.30 on epoch=264
06/19/2022 18:39:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=269
06/19/2022 18:39:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.31 on epoch=274
06/19/2022 18:39:32 - INFO - __main__ - Global step 550 Train loss 0.29 ACC 0.5 on epoch=274
06/19/2022 18:39:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=279
06/19/2022 18:39:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.31 on epoch=284
06/19/2022 18:39:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=289
06/19/2022 18:39:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=294
06/19/2022 18:39:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=299
06/19/2022 18:39:40 - INFO - __main__ - Global step 600 Train loss 0.28 ACC 0.5 on epoch=299
06/19/2022 18:39:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=304
06/19/2022 18:39:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=309
06/19/2022 18:39:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.29 on epoch=314
06/19/2022 18:39:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=319
06/19/2022 18:39:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=324
06/19/2022 18:39:48 - INFO - __main__ - Global step 650 Train loss 0.28 ACC 0.5 on epoch=324
06/19/2022 18:39:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=329
06/19/2022 18:39:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=334
06/19/2022 18:39:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=339
06/19/2022 18:39:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=344
06/19/2022 18:39:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.31 on epoch=349
06/19/2022 18:39:56 - INFO - __main__ - Global step 700 Train loss 0.28 ACC 0.5 on epoch=349
06/19/2022 18:39:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=354
06/19/2022 18:39:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=359
06/19/2022 18:40:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.31 on epoch=364
06/19/2022 18:40:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=369
06/19/2022 18:40:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=374
06/19/2022 18:40:04 - INFO - __main__ - Global step 750 Train loss 0.29 ACC 0.53125 on epoch=374
06/19/2022 18:40:04 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=374, global_step=750
06/19/2022 18:40:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=379
06/19/2022 18:40:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.29 on epoch=384
06/19/2022 18:40:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
06/19/2022 18:40:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=394
06/19/2022 18:40:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=399
06/19/2022 18:40:12 - INFO - __main__ - Global step 800 Train loss 0.27 ACC 0.5 on epoch=399
06/19/2022 18:40:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=404
06/19/2022 18:40:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=409
06/19/2022 18:40:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=414
06/19/2022 18:40:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.29 on epoch=419
06/19/2022 18:40:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=424
06/19/2022 18:40:20 - INFO - __main__ - Global step 850 Train loss 0.26 ACC 0.5 on epoch=424
06/19/2022 18:40:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=429
06/19/2022 18:40:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=434
06/19/2022 18:40:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=439
06/19/2022 18:40:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=444
06/19/2022 18:40:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=449
06/19/2022 18:40:27 - INFO - __main__ - Global step 900 Train loss 0.27 ACC 0.625 on epoch=449
06/19/2022 18:40:27 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.625 on epoch=449, global_step=900
06/19/2022 18:40:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=454
06/19/2022 18:40:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.28 on epoch=459
06/19/2022 18:40:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=464
06/19/2022 18:40:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=469
06/19/2022 18:40:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=474
06/19/2022 18:40:35 - INFO - __main__ - Global step 950 Train loss 0.24 ACC 0.5 on epoch=474
06/19/2022 18:40:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=479
06/19/2022 18:40:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=484
06/19/2022 18:40:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=489
06/19/2022 18:40:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=494
06/19/2022 18:40:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=499
06/19/2022 18:40:41 - INFO - __main__ - Global step 1000 Train loss 0.25 ACC 0.5 on epoch=499
06/19/2022 18:40:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=504
06/19/2022 18:40:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=509
06/19/2022 18:40:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=514
06/19/2022 18:40:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=519
06/19/2022 18:40:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=524
06/19/2022 18:40:49 - INFO - __main__ - Global step 1050 Train loss 0.18 ACC 0.53125 on epoch=524
06/19/2022 18:40:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=529
06/19/2022 18:40:52 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=534
06/19/2022 18:40:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.17 on epoch=539
06/19/2022 18:40:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=544
06/19/2022 18:40:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=549
06/19/2022 18:40:56 - INFO - __main__ - Global step 1100 Train loss 0.16 ACC 0.46875 on epoch=549
06/19/2022 18:40:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=554
06/19/2022 18:40:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.26 on epoch=559
06/19/2022 18:41:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=564
06/19/2022 18:41:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=569
06/19/2022 18:41:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.14 on epoch=574
06/19/2022 18:41:04 - INFO - __main__ - Global step 1150 Train loss 0.20 ACC 0.59375 on epoch=574
06/19/2022 18:41:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=579
06/19/2022 18:41:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=584
06/19/2022 18:41:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=589
06/19/2022 18:41:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.11 on epoch=594
06/19/2022 18:41:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=599
06/19/2022 18:41:11 - INFO - __main__ - Global step 1200 Train loss 0.12 ACC 0.625 on epoch=599
06/19/2022 18:41:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=604
06/19/2022 18:41:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=609
06/19/2022 18:41:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=614
06/19/2022 18:41:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=619
06/19/2022 18:41:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=624
06/19/2022 18:41:18 - INFO - __main__ - Global step 1250 Train loss 0.09 ACC 0.625 on epoch=624
06/19/2022 18:41:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=629
06/19/2022 18:41:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=634
06/19/2022 18:41:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=639
06/19/2022 18:41:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=644
06/19/2022 18:41:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=649
06/19/2022 18:41:25 - INFO - __main__ - Global step 1300 Train loss 0.11 ACC 0.5625 on epoch=649
06/19/2022 18:41:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
06/19/2022 18:41:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.11 on epoch=659
06/19/2022 18:41:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=664
06/19/2022 18:41:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=669
06/19/2022 18:41:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.11 on epoch=674
06/19/2022 18:41:33 - INFO - __main__ - Global step 1350 Train loss 0.09 ACC 0.53125 on epoch=674
06/19/2022 18:41:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=679
06/19/2022 18:41:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=684
06/19/2022 18:41:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=689
06/19/2022 18:41:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=694
06/19/2022 18:41:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.13 on epoch=699
06/19/2022 18:41:40 - INFO - __main__ - Global step 1400 Train loss 0.08 ACC 0.5625 on epoch=699
06/19/2022 18:41:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=704
06/19/2022 18:41:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
06/19/2022 18:41:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=714
06/19/2022 18:41:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=719
06/19/2022 18:41:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=724
06/19/2022 18:41:47 - INFO - __main__ - Global step 1450 Train loss 0.06 ACC 0.53125 on epoch=724
06/19/2022 18:41:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=729
06/19/2022 18:41:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
06/19/2022 18:41:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=739
06/19/2022 18:41:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=744
06/19/2022 18:41:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=749
06/19/2022 18:41:55 - INFO - __main__ - Global step 1500 Train loss 0.06 ACC 0.625 on epoch=749
06/19/2022 18:41:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=754
06/19/2022 18:41:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=759
06/19/2022 18:41:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
06/19/2022 18:42:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/19/2022 18:42:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
06/19/2022 18:42:02 - INFO - __main__ - Global step 1550 Train loss 0.05 ACC 0.59375 on epoch=774
06/19/2022 18:42:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=779
06/19/2022 18:42:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
06/19/2022 18:42:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
06/19/2022 18:42:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=794
06/19/2022 18:42:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=799
06/19/2022 18:42:09 - INFO - __main__ - Global step 1600 Train loss 0.05 ACC 0.5625 on epoch=799
06/19/2022 18:42:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=804
06/19/2022 18:42:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=809
06/19/2022 18:42:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/19/2022 18:42:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
06/19/2022 18:42:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=824
06/19/2022 18:42:16 - INFO - __main__ - Global step 1650 Train loss 0.04 ACC 0.65625 on epoch=824
06/19/2022 18:42:16 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=824, global_step=1650
06/19/2022 18:42:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=829
06/19/2022 18:42:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
06/19/2022 18:42:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
06/19/2022 18:42:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=844
06/19/2022 18:42:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
06/19/2022 18:42:25 - INFO - __main__ - Global step 1700 Train loss 0.06 ACC 0.5625 on epoch=849
06/19/2022 18:42:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
06/19/2022 18:42:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/19/2022 18:42:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
06/19/2022 18:42:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
06/19/2022 18:42:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=874
06/19/2022 18:42:33 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.46875 on epoch=874
06/19/2022 18:42:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/19/2022 18:42:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
06/19/2022 18:42:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/19/2022 18:42:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/19/2022 18:42:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
06/19/2022 18:42:42 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.5625 on epoch=899
06/19/2022 18:42:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
06/19/2022 18:42:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
06/19/2022 18:42:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=914
06/19/2022 18:42:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/19/2022 18:42:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/19/2022 18:42:49 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.59375 on epoch=924
06/19/2022 18:42:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/19/2022 18:42:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/19/2022 18:42:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=939
06/19/2022 18:42:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=944
06/19/2022 18:42:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
06/19/2022 18:42:57 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.59375 on epoch=949
06/19/2022 18:42:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/19/2022 18:42:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=959
06/19/2022 18:43:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
06/19/2022 18:43:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/19/2022 18:43:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/19/2022 18:43:04 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.53125 on epoch=974
06/19/2022 18:43:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=979
06/19/2022 18:43:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/19/2022 18:43:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/19/2022 18:43:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=994
06/19/2022 18:43:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
06/19/2022 18:43:12 - INFO - __main__ - Global step 2000 Train loss 0.04 ACC 0.46875 on epoch=999
06/19/2022 18:43:12 - INFO - __main__ - save last model!
06/19/2022 18:43:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 18:43:12 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 18:43:12 - INFO - __main__ - Printing 3 examples
06/19/2022 18:43:12 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 18:43:12 - INFO - __main__ - ['equivalent']
06/19/2022 18:43:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 18:43:12 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:43:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 18:43:12 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:43:12 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:43:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:43:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:43:12 - INFO - __main__ - Printing 3 examples
06/19/2022 18:43:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 18:43:12 - INFO - __main__ - ['equivalent']
06/19/2022 18:43:12 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 18:43:12 - INFO - __main__ - ['equivalent']
06/19/2022 18:43:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 18:43:12 - INFO - __main__ - ['equivalent']
06/19/2022 18:43:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 18:43:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:43:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:43:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:43:12 - INFO - __main__ - Printing 3 examples
06/19/2022 18:43:12 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 18:43:12 - INFO - __main__ - ['equivalent']
06/19/2022 18:43:12 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 18:43:12 - INFO - __main__ - ['equivalent']
06/19/2022 18:43:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 18:43:12 - INFO - __main__ - ['equivalent']
06/19/2022 18:43:12 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:43:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:43:12 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:43:12 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 18:43:18 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:43:18 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(5492, 195474), (1068, 771470), (733, 1214890), (5682, 233612), (5433, 189971), (845, 1086618), (5661, 162439), (590, 1481555), (1953, 589554), (1631, 598174), (6364, 166204), (2896, 309135), (918, 967989), (2332, 372930), (1418, 745074), (6659, 167523), (4626, 225869), (3640, 155110), (6597, 157538), (2460, 406809), (5734, 179839), (3924, 246414), (4737, 208385), (4578, 211218), (2000, 315151), (6461, 159321), (4926, 217922), (2029, 307697), (6465, 164367), (297, 1324121), (6650, 154117), (1042, 857631), (2200, 439737), (2272, 432795), (2930, 350772), (937, 1007700), (1169, 574433), (1748, 386930), (855, 522337), (5413, 169514), (783, 1075853), (1717, 576145), (34, 32364675), (4420, 230091), (6446, 158041), (4775, 199538), (2501, 321680), (379, 2412996), (2253, 398597), (3266, 325487), (3227, 330589), (4913, 206391), (771, 1130105), (2510, 395690), (6723, 158917), (1736, 460906), (4783, 219874), (1221, 767154), (5963, 171669), (2824, 408492), (1449, 598263), (2875, 349182), (1492, 630896), (770, 1128621), (647, 1332269), (1773, 547210), (3278, 305705), (2558, 157757), (4239, 164202), (3065, 325101), (5452, 212734), (1344, 712448), (4852, 199901), (2783, 308812), (3551, 232925), (2940, 341076), (2169, 187157), (2795, 173107), (3463, 189435), (3876, 213357), (866, 1018574), (6803, 157013), (453, 1935608), (5275, 216061), (3008, 291537), (6894, 155616), (970, 369184), (298, 3090275), (324, 909673), (2594, 403711), (4477, 162791), (6539, 159792), (237, 3927936), (3372, 344743), (6209, 162555), (464, 1931312), (3535, 517891), (5859, 199324), (6157, 180769)]
06/19/2022 18:43:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:43:18 - INFO - __main__ - Starting training!
06/19/2022 18:43:22 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_13_0.2_8_predictions.txt
06/19/2022 18:43:22 - INFO - __main__ - ACC on test data: 0.3873
06/19/2022 18:43:22 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.2, bsz=8, dev_performance=0.65625, test_performance=0.3872549019607843
06/19/2022 18:43:22 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.5, bsz=8 ...
06/19/2022 18:43:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:43:23 - INFO - __main__ - Printing 3 examples
06/19/2022 18:43:23 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 18:43:23 - INFO - __main__ - ['equivalent']
06/19/2022 18:43:23 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 18:43:23 - INFO - __main__ - ['equivalent']
06/19/2022 18:43:23 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 18:43:23 - INFO - __main__ - ['equivalent']
06/19/2022 18:43:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 18:43:23 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:43:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:43:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:43:23 - INFO - __main__ - Printing 3 examples
06/19/2022 18:43:23 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 18:43:23 - INFO - __main__ - ['equivalent']
06/19/2022 18:43:23 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 18:43:23 - INFO - __main__ - ['equivalent']
06/19/2022 18:43:23 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 18:43:23 - INFO - __main__ - ['equivalent']
06/19/2022 18:43:23 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:43:23 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:43:23 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:43:30 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:43:30 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(5492, 195474), (1068, 771470), (733, 1214890), (5682, 233612), (5433, 189971), (845, 1086618), (5661, 162439), (590, 1481555), (1953, 589554), (1631, 598174), (6364, 166204), (2896, 309135), (918, 967989), (2332, 372930), (1418, 745074), (6659, 167523), (4626, 225869), (3640, 155110), (6597, 157538), (2460, 406809), (5734, 179839), (3924, 246414), (4737, 208385), (4578, 211218), (2000, 315151), (6461, 159321), (4926, 217922), (2029, 307697), (6465, 164367), (297, 1324121), (6650, 154117), (1042, 857631), (2200, 439737), (2272, 432795), (2930, 350772), (937, 1007700), (1169, 574433), (1748, 386930), (855, 522337), (5413, 169514), (783, 1075853), (1717, 576145), (34, 32364675), (4420, 230091), (6446, 158041), (4775, 199538), (2501, 321680), (379, 2412996), (2253, 398597), (3266, 325487), (3227, 330589), (4913, 206391), (771, 1130105), (2510, 395690), (6723, 158917), (1736, 460906), (4783, 219874), (1221, 767154), (5963, 171669), (2824, 408492), (1449, 598263), (2875, 349182), (1492, 630896), (770, 1128621), (647, 1332269), (1773, 547210), (3278, 305705), (2558, 157757), (4239, 164202), (3065, 325101), (5452, 212734), (1344, 712448), (4852, 199901), (2783, 308812), (3551, 232925), (2940, 341076), (2169, 187157), (2795, 173107), (3463, 189435), (3876, 213357), (866, 1018574), (6803, 157013), (453, 1935608), (5275, 216061), (3008, 291537), (6894, 155616), (970, 369184), (298, 3090275), (324, 909673), (2594, 403711), (4477, 162791), (6539, 159792), (237, 3927936), (3372, 344743), (6209, 162555), (464, 1931312), (3535, 517891), (5859, 199324), (6157, 180769)]
06/19/2022 18:43:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:43:31 - INFO - __main__ - Starting training!
06/19/2022 18:43:32 - INFO - __main__ - Step 10 Global step 10 Train loss 5.65 on epoch=4
06/19/2022 18:43:34 - INFO - __main__ - Step 20 Global step 20 Train loss 2.22 on epoch=9
06/19/2022 18:43:35 - INFO - __main__ - Step 30 Global step 30 Train loss 0.85 on epoch=14
06/19/2022 18:43:37 - INFO - __main__ - Step 40 Global step 40 Train loss 0.57 on epoch=19
06/19/2022 18:43:39 - INFO - __main__ - Step 50 Global step 50 Train loss 0.63 on epoch=24
06/19/2022 18:43:39 - INFO - __main__ - Global step 50 Train loss 1.98 ACC 0.4375 on epoch=24
06/19/2022 18:43:39 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.4375 on epoch=24, global_step=50
06/19/2022 18:43:41 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=29
06/19/2022 18:43:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=34
06/19/2022 18:43:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=39
06/19/2022 18:43:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=44
06/19/2022 18:43:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.35 on epoch=49
06/19/2022 18:43:47 - INFO - __main__ - Global step 100 Train loss 0.40 ACC 0.5 on epoch=49
06/19/2022 18:43:47 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=49, global_step=100
06/19/2022 18:43:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.29 on epoch=54
06/19/2022 18:43:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.30 on epoch=59
06/19/2022 18:43:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=64
06/19/2022 18:43:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.34 on epoch=69
06/19/2022 18:43:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=74
06/19/2022 18:43:55 - INFO - __main__ - Global step 150 Train loss 0.31 ACC 0.5 on epoch=74
06/19/2022 18:43:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.34 on epoch=79
06/19/2022 18:43:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.36 on epoch=84
06/19/2022 18:43:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=89
06/19/2022 18:44:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.31 on epoch=94
06/19/2022 18:44:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.28 on epoch=99
06/19/2022 18:44:03 - INFO - __main__ - Global step 200 Train loss 0.31 ACC 0.5 on epoch=99
06/19/2022 18:44:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
06/19/2022 18:44:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.30 on epoch=109
06/19/2022 18:44:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.32 on epoch=114
06/19/2022 18:44:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.31 on epoch=119
06/19/2022 18:44:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=124
06/19/2022 18:44:10 - INFO - __main__ - Global step 250 Train loss 0.29 ACC 0.5 on epoch=124
06/19/2022 18:44:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=129
06/19/2022 18:44:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
06/19/2022 18:44:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
06/19/2022 18:44:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
06/19/2022 18:44:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
06/19/2022 18:44:18 - INFO - __main__ - Global step 300 Train loss 0.27 ACC 0.53125 on epoch=149
06/19/2022 18:44:18 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=149, global_step=300
06/19/2022 18:44:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.30 on epoch=154
06/19/2022 18:44:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
06/19/2022 18:44:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
06/19/2022 18:44:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
06/19/2022 18:44:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.29 on epoch=174
06/19/2022 18:44:26 - INFO - __main__ - Global step 350 Train loss 0.27 ACC 0.5 on epoch=174
06/19/2022 18:44:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=179
06/19/2022 18:44:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=184
06/19/2022 18:44:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
06/19/2022 18:44:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=194
06/19/2022 18:44:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=199
06/19/2022 18:44:33 - INFO - __main__ - Global step 400 Train loss 0.28 ACC 0.46875 on epoch=199
06/19/2022 18:44:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
06/19/2022 18:44:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=209
06/19/2022 18:44:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=214
06/19/2022 18:44:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.29 on epoch=219
06/19/2022 18:44:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/19/2022 18:44:42 - INFO - __main__ - Global step 450 Train loss 0.27 ACC 0.5 on epoch=224
06/19/2022 18:44:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
06/19/2022 18:44:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=234
06/19/2022 18:44:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=239
06/19/2022 18:44:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=244
06/19/2022 18:44:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=249
06/19/2022 18:44:50 - INFO - __main__ - Global step 500 Train loss 0.26 ACC 0.5 on epoch=249
06/19/2022 18:44:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=254
06/19/2022 18:44:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=259
06/19/2022 18:44:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=264
06/19/2022 18:44:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=269
06/19/2022 18:44:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=274
06/19/2022 18:44:57 - INFO - __main__ - Global step 550 Train loss 0.22 ACC 0.625 on epoch=274
06/19/2022 18:44:57 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.625 on epoch=274, global_step=550
06/19/2022 18:44:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.97 on epoch=279
06/19/2022 18:44:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=284
06/19/2022 18:45:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=289
06/19/2022 18:45:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=294
06/19/2022 18:45:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=299
06/19/2022 18:45:04 - INFO - __main__ - Global step 600 Train loss 0.43 ACC 0.5 on epoch=299
06/19/2022 18:45:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=304
06/19/2022 18:45:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=309
06/19/2022 18:45:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
06/19/2022 18:45:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=319
06/19/2022 18:45:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=324
06/19/2022 18:45:12 - INFO - __main__ - Global step 650 Train loss 0.20 ACC 0.5625 on epoch=324
06/19/2022 18:45:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=329
06/19/2022 18:45:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=334
06/19/2022 18:45:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=339
06/19/2022 18:45:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=344
06/19/2022 18:45:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.81 on epoch=349
06/19/2022 18:45:21 - INFO - __main__ - Global step 700 Train loss 0.37 ACC 0.53125 on epoch=349
06/19/2022 18:45:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.93 on epoch=354
06/19/2022 18:45:23 - INFO - __main__ - Step 720 Global step 720 Train loss 1.12 on epoch=359
06/19/2022 18:45:25 - INFO - __main__ - Step 730 Global step 730 Train loss 1.46 on epoch=364
06/19/2022 18:45:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.83 on epoch=369
06/19/2022 18:45:27 - INFO - __main__ - Step 750 Global step 750 Train loss 1.60 on epoch=374
06/19/2022 18:45:28 - INFO - __main__ - Global step 750 Train loss 1.19 ACC 0.4375 on epoch=374
06/19/2022 18:45:30 - INFO - __main__ - Step 760 Global step 760 Train loss 1.16 on epoch=379
06/19/2022 18:45:31 - INFO - __main__ - Step 770 Global step 770 Train loss 1.38 on epoch=384
06/19/2022 18:45:33 - INFO - __main__ - Step 780 Global step 780 Train loss 1.64 on epoch=389
06/19/2022 18:45:34 - INFO - __main__ - Step 790 Global step 790 Train loss 1.13 on epoch=394
06/19/2022 18:45:35 - INFO - __main__ - Step 800 Global step 800 Train loss 1.56 on epoch=399
06/19/2022 18:45:36 - INFO - __main__ - Global step 800 Train loss 1.37 ACC 0.5 on epoch=399
06/19/2022 18:45:38 - INFO - __main__ - Step 810 Global step 810 Train loss 1.32 on epoch=404
06/19/2022 18:45:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.78 on epoch=409
06/19/2022 18:45:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.58 on epoch=414
06/19/2022 18:45:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=419
06/19/2022 18:45:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=424
06/19/2022 18:45:44 - INFO - __main__ - Global step 850 Train loss 0.70 ACC 0.5 on epoch=424
06/19/2022 18:45:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=429
06/19/2022 18:45:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=434
06/19/2022 18:45:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=439
06/19/2022 18:45:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.33 on epoch=444
06/19/2022 18:45:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=449
06/19/2022 18:45:52 - INFO - __main__ - Global step 900 Train loss 0.36 ACC 0.5 on epoch=449
06/19/2022 18:45:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=454
06/19/2022 18:45:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=459
06/19/2022 18:45:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=464
06/19/2022 18:45:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.29 on epoch=469
06/19/2022 18:45:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=474
06/19/2022 18:46:00 - INFO - __main__ - Global step 950 Train loss 0.29 ACC 0.53125 on epoch=474
06/19/2022 18:46:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=479
06/19/2022 18:46:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=484
06/19/2022 18:46:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=489
06/19/2022 18:46:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=494
06/19/2022 18:46:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=499
06/19/2022 18:46:07 - INFO - __main__ - Global step 1000 Train loss 0.26 ACC 0.5 on epoch=499
06/19/2022 18:46:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.27 on epoch=504
06/19/2022 18:46:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=509
06/19/2022 18:46:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=514
06/19/2022 18:46:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=519
06/19/2022 18:46:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=524
06/19/2022 18:46:14 - INFO - __main__ - Global step 1050 Train loss 0.26 ACC 0.53125 on epoch=524
06/19/2022 18:46:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.23 on epoch=529
06/19/2022 18:46:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=534
06/19/2022 18:46:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=539
06/19/2022 18:46:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=544
06/19/2022 18:46:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=549
06/19/2022 18:46:22 - INFO - __main__ - Global step 1100 Train loss 0.23 ACC 0.5 on epoch=549
06/19/2022 18:46:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=554
06/19/2022 18:46:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=559
06/19/2022 18:46:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=564
06/19/2022 18:46:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=569
06/19/2022 18:46:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=574
06/19/2022 18:46:29 - INFO - __main__ - Global step 1150 Train loss 0.23 ACC 0.53125 on epoch=574
06/19/2022 18:46:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=579
06/19/2022 18:46:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.27 on epoch=584
06/19/2022 18:46:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=589
06/19/2022 18:46:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=594
06/19/2022 18:46:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.26 on epoch=599
06/19/2022 18:46:37 - INFO - __main__ - Global step 1200 Train loss 0.23 ACC 0.53125 on epoch=599
06/19/2022 18:46:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=604
06/19/2022 18:46:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=609
06/19/2022 18:46:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=614
06/19/2022 18:46:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=619
06/19/2022 18:46:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=624
06/19/2022 18:46:45 - INFO - __main__ - Global step 1250 Train loss 0.24 ACC 0.5 on epoch=624
06/19/2022 18:46:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.22 on epoch=629
06/19/2022 18:46:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=634
06/19/2022 18:46:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=639
06/19/2022 18:46:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.24 on epoch=644
06/19/2022 18:46:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=649
06/19/2022 18:46:52 - INFO - __main__ - Global step 1300 Train loss 0.21 ACC 0.5 on epoch=649
06/19/2022 18:46:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=654
06/19/2022 18:46:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=659
06/19/2022 18:46:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=664
06/19/2022 18:46:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=669
06/19/2022 18:47:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=674
06/19/2022 18:47:00 - INFO - __main__ - Global step 1350 Train loss 0.20 ACC 0.46875 on epoch=674
06/19/2022 18:47:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=679
06/19/2022 18:47:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=684
06/19/2022 18:47:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=689
06/19/2022 18:47:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=694
06/19/2022 18:47:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=699
06/19/2022 18:47:08 - INFO - __main__ - Global step 1400 Train loss 0.21 ACC 0.46875 on epoch=699
06/19/2022 18:47:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=704
06/19/2022 18:47:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=709
06/19/2022 18:47:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.22 on epoch=714
06/19/2022 18:47:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=719
06/19/2022 18:47:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.23 on epoch=724
06/19/2022 18:47:16 - INFO - __main__ - Global step 1450 Train loss 0.22 ACC 0.5 on epoch=724
06/19/2022 18:47:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=729
06/19/2022 18:47:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=734
06/19/2022 18:47:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.17 on epoch=739
06/19/2022 18:47:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=744
06/19/2022 18:47:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=749
06/19/2022 18:47:25 - INFO - __main__ - Global step 1500 Train loss 0.20 ACC 0.46875 on epoch=749
06/19/2022 18:47:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.25 on epoch=754
06/19/2022 18:47:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=759
06/19/2022 18:47:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=764
06/19/2022 18:47:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=769
06/19/2022 18:47:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=774
06/19/2022 18:47:34 - INFO - __main__ - Global step 1550 Train loss 0.22 ACC 0.46875 on epoch=774
06/19/2022 18:47:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=779
06/19/2022 18:47:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.27 on epoch=784
06/19/2022 18:47:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=789
06/19/2022 18:47:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=794
06/19/2022 18:47:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.19 on epoch=799
06/19/2022 18:47:42 - INFO - __main__ - Global step 1600 Train loss 0.25 ACC 0.53125 on epoch=799
06/19/2022 18:47:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=804
06/19/2022 18:47:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.24 on epoch=809
06/19/2022 18:47:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=814
06/19/2022 18:47:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=819
06/19/2022 18:47:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=824
06/19/2022 18:47:49 - INFO - __main__ - Global step 1650 Train loss 0.20 ACC 0.5 on epoch=824
06/19/2022 18:47:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.20 on epoch=829
06/19/2022 18:47:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=834
06/19/2022 18:47:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=839
06/19/2022 18:47:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=844
06/19/2022 18:47:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=849
06/19/2022 18:47:57 - INFO - __main__ - Global step 1700 Train loss 0.20 ACC 0.4375 on epoch=849
06/19/2022 18:47:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=854
06/19/2022 18:48:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=859
06/19/2022 18:48:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=864
06/19/2022 18:48:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=869
06/19/2022 18:48:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=874
06/19/2022 18:48:05 - INFO - __main__ - Global step 1750 Train loss 0.21 ACC 0.46875 on epoch=874
06/19/2022 18:48:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=879
06/19/2022 18:48:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=884
06/19/2022 18:48:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=889
06/19/2022 18:48:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=894
06/19/2022 18:48:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=899
06/19/2022 18:48:13 - INFO - __main__ - Global step 1800 Train loss 0.19 ACC 0.46875 on epoch=899
06/19/2022 18:48:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=904
06/19/2022 18:48:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=909
06/19/2022 18:48:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=914
06/19/2022 18:48:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.20 on epoch=919
06/19/2022 18:48:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=924
06/19/2022 18:48:22 - INFO - __main__ - Global step 1850 Train loss 0.20 ACC 0.5 on epoch=924
06/19/2022 18:48:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=929
06/19/2022 18:48:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.16 on epoch=934
06/19/2022 18:48:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=939
06/19/2022 18:48:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=944
06/19/2022 18:48:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=949
06/19/2022 18:48:30 - INFO - __main__ - Global step 1900 Train loss 0.19 ACC 0.5 on epoch=949
06/19/2022 18:48:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=954
06/19/2022 18:48:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.22 on epoch=959
06/19/2022 18:48:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=964
06/19/2022 18:48:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=969
06/19/2022 18:48:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=974
06/19/2022 18:48:37 - INFO - __main__ - Global step 1950 Train loss 0.19 ACC 0.53125 on epoch=974
06/19/2022 18:48:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.23 on epoch=979
06/19/2022 18:48:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=984
06/19/2022 18:48:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=989
06/19/2022 18:48:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=994
06/19/2022 18:48:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=999
06/19/2022 18:48:45 - INFO - __main__ - Global step 2000 Train loss 0.21 ACC 0.5 on epoch=999
06/19/2022 18:48:45 - INFO - __main__ - save last model!
06/19/2022 18:48:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 18:48:45 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 18:48:45 - INFO - __main__ - Printing 3 examples
06/19/2022 18:48:45 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 18:48:45 - INFO - __main__ - ['equivalent']
06/19/2022 18:48:45 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 18:48:45 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:48:45 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 18:48:45 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:48:45 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:48:45 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:48:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:48:45 - INFO - __main__ - Printing 3 examples
06/19/2022 18:48:45 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 18:48:45 - INFO - __main__ - ['equivalent']
06/19/2022 18:48:45 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 18:48:45 - INFO - __main__ - ['equivalent']
06/19/2022 18:48:45 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 18:48:45 - INFO - __main__ - ['equivalent']
06/19/2022 18:48:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 18:48:45 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:48:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:48:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:48:45 - INFO - __main__ - Printing 3 examples
06/19/2022 18:48:45 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 18:48:45 - INFO - __main__ - ['equivalent']
06/19/2022 18:48:45 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 18:48:45 - INFO - __main__ - ['equivalent']
06/19/2022 18:48:45 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 18:48:45 - INFO - __main__ - ['equivalent']
06/19/2022 18:48:45 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:48:45 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:48:45 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:48:45 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 18:48:52 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:48:52 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(3258, 211425), (3213, 334916), (1909, 502837), (8857, 160161), (4836, 214321), (5433, 189971), (962, 911154), (4497, 199538), (1712, 268688), (355, 230108), (2271, 282308), (2560, 292543), (1090, 829527), (1212, 563029), (3426, 284907), (2600, 228530), (5100, 188863), (362, 325366), (4454, 233931), (5535, 170020), (424, 2133697), (3541, 206040), (1155, 940173), (2043, 299538), (2711, 367153), (3410, 290749), (1277, 761840), (4351, 231959), (4660, 182019), (16, 86594496), (2562, 172369), (5531, 202427), (4427, 227401), (531, 1463009), (1959, 488127), (6058, 180821), (3884, 259889), (4747, 187821), (5105, 211448), (577, 1526764), (2024, 448261), (2891, 347678), (6176, 170373), (526, 676575), (1744, 325562), (4183, 213298), (5308, 192463), (1310, 731782), (233, 2752414), (6865, 154321), (2409, 387991), (1429, 570527), (1618, 302520), (5642, 169703), (102, 5915536), (6195, 169481), (4890, 230195), (1039, 892616), (2451, 408517), (1293, 773200), (4327, 202585), (1248, 751368), (1705, 584257), (5563, 187825), (3381, 321221), (5918, 183921), (2786, 340142), (4285, 226300), (5872, 193577), (5250, 198049), (2692, 378744), (4659, 205143), (4173, 169588), (1469, 619260), (4321, 244144), (1921, 479667), (5352, 156643), (5286, 184140), (1472, 652757), (427, 1146202), (1047, 336403), (4463, 226544), (2403, 276759), (5209, 184306), (2787, 347383), (34, 32364675), (3033, 329937), (3275, 314100), (873, 159860), (6005, 161456), (4593, 190444), (1587, 599851), (3581, 209865), (4857, 173501), (4440, 192534), (2693, 271357), (2668, 283036), (5413, 169514), (6025, 171968)]
06/19/2022 18:48:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:48:53 - INFO - __main__ - Starting training!
06/19/2022 18:48:55 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_21_0.5_8_predictions.txt
06/19/2022 18:48:55 - INFO - __main__ - ACC on test data: 0.6005
06/19/2022 18:48:55 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.5, bsz=8, dev_performance=0.625, test_performance=0.6004901960784313
06/19/2022 18:48:55 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.4, bsz=8 ...
06/19/2022 18:48:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:48:56 - INFO - __main__ - Printing 3 examples
06/19/2022 18:48:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 18:48:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:48:56 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 18:48:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:48:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 18:48:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:48:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 18:48:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:48:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:48:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:48:56 - INFO - __main__ - Printing 3 examples
06/19/2022 18:48:56 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 18:48:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:48:56 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 18:48:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:48:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 18:48:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:48:56 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:48:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:48:56 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:49:02 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:49:02 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(3258, 211425), (3213, 334916), (1909, 502837), (8857, 160161), (4836, 214321), (5433, 189971), (962, 911154), (4497, 199538), (1712, 268688), (355, 230108), (2271, 282308), (2560, 292543), (1090, 829527), (1212, 563029), (3426, 284907), (2600, 228530), (5100, 188863), (362, 325366), (4454, 233931), (5535, 170020), (424, 2133697), (3541, 206040), (1155, 940173), (2043, 299538), (2711, 367153), (3410, 290749), (1277, 761840), (4351, 231959), (4660, 182019), (16, 86594496), (2562, 172369), (5531, 202427), (4427, 227401), (531, 1463009), (1959, 488127), (6058, 180821), (3884, 259889), (4747, 187821), (5105, 211448), (577, 1526764), (2024, 448261), (2891, 347678), (6176, 170373), (526, 676575), (1744, 325562), (4183, 213298), (5308, 192463), (1310, 731782), (233, 2752414), (6865, 154321), (2409, 387991), (1429, 570527), (1618, 302520), (5642, 169703), (102, 5915536), (6195, 169481), (4890, 230195), (1039, 892616), (2451, 408517), (1293, 773200), (4327, 202585), (1248, 751368), (1705, 584257), (5563, 187825), (3381, 321221), (5918, 183921), (2786, 340142), (4285, 226300), (5872, 193577), (5250, 198049), (2692, 378744), (4659, 205143), (4173, 169588), (1469, 619260), (4321, 244144), (1921, 479667), (5352, 156643), (5286, 184140), (1472, 652757), (427, 1146202), (1047, 336403), (4463, 226544), (2403, 276759), (5209, 184306), (2787, 347383), (34, 32364675), (3033, 329937), (3275, 314100), (873, 159860), (6005, 161456), (4593, 190444), (1587, 599851), (3581, 209865), (4857, 173501), (4440, 192534), (2693, 271357), (2668, 283036), (5413, 169514), (6025, 171968)]
06/19/2022 18:49:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:49:02 - INFO - __main__ - Starting training!
06/19/2022 18:49:04 - INFO - __main__ - Step 10 Global step 10 Train loss 5.99 on epoch=4
06/19/2022 18:49:06 - INFO - __main__ - Step 20 Global step 20 Train loss 2.52 on epoch=9
06/19/2022 18:49:07 - INFO - __main__ - Step 30 Global step 30 Train loss 1.17 on epoch=14
06/19/2022 18:49:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.78 on epoch=19
06/19/2022 18:49:10 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=24
06/19/2022 18:49:10 - INFO - __main__ - Global step 50 Train loss 2.21 ACC 0.5 on epoch=24
06/19/2022 18:49:10 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 18:49:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.47 on epoch=29
06/19/2022 18:49:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=34
06/19/2022 18:49:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=39
06/19/2022 18:49:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.35 on epoch=44
06/19/2022 18:49:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.34 on epoch=49
06/19/2022 18:49:17 - INFO - __main__ - Global step 100 Train loss 0.43 ACC 0.5 on epoch=49
06/19/2022 18:49:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=54
06/19/2022 18:49:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
06/19/2022 18:49:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.38 on epoch=64
06/19/2022 18:49:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.37 on epoch=69
06/19/2022 18:49:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.31 on epoch=74
06/19/2022 18:49:25 - INFO - __main__ - Global step 150 Train loss 0.35 ACC 0.5 on epoch=74
06/19/2022 18:49:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.31 on epoch=79
06/19/2022 18:49:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=84
06/19/2022 18:49:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.32 on epoch=89
06/19/2022 18:49:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.34 on epoch=94
06/19/2022 18:49:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.29 on epoch=99
06/19/2022 18:49:32 - INFO - __main__ - Global step 200 Train loss 0.31 ACC 0.5 on epoch=99
06/19/2022 18:49:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=104
06/19/2022 18:49:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=109
06/19/2022 18:49:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.34 on epoch=114
06/19/2022 18:49:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.31 on epoch=119
06/19/2022 18:49:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=124
06/19/2022 18:49:40 - INFO - __main__ - Global step 250 Train loss 0.32 ACC 0.5 on epoch=124
06/19/2022 18:49:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=129
06/19/2022 18:49:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
06/19/2022 18:49:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=139
06/19/2022 18:49:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=144
06/19/2022 18:49:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
06/19/2022 18:49:48 - INFO - __main__ - Global step 300 Train loss 0.28 ACC 0.5 on epoch=149
06/19/2022 18:49:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
06/19/2022 18:49:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=159
06/19/2022 18:49:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=164
06/19/2022 18:49:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
06/19/2022 18:49:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=174
06/19/2022 18:49:56 - INFO - __main__ - Global step 350 Train loss 0.29 ACC 0.5 on epoch=174
06/19/2022 18:49:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
06/19/2022 18:49:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.28 on epoch=184
06/19/2022 18:50:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
06/19/2022 18:50:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
06/19/2022 18:50:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=199
06/19/2022 18:50:04 - INFO - __main__ - Global step 400 Train loss 0.26 ACC 0.5 on epoch=199
06/19/2022 18:50:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=204
06/19/2022 18:50:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=209
06/19/2022 18:50:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=214
06/19/2022 18:50:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=219
06/19/2022 18:50:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/19/2022 18:50:12 - INFO - __main__ - Global step 450 Train loss 0.27 ACC 0.5 on epoch=224
06/19/2022 18:50:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=229
06/19/2022 18:50:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=234
06/19/2022 18:50:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=239
06/19/2022 18:50:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
06/19/2022 18:50:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=249
06/19/2022 18:50:19 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.5 on epoch=249
06/19/2022 18:50:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=254
06/19/2022 18:50:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=259
06/19/2022 18:50:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.31 on epoch=264
06/19/2022 18:50:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=269
06/19/2022 18:50:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=274
06/19/2022 18:50:26 - INFO - __main__ - Global step 550 Train loss 0.25 ACC 0.5 on epoch=274
06/19/2022 18:50:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=279
06/19/2022 18:50:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=284
06/19/2022 18:50:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=289
06/19/2022 18:50:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=294
06/19/2022 18:50:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
06/19/2022 18:50:34 - INFO - __main__ - Global step 600 Train loss 0.24 ACC 0.5625 on epoch=299
06/19/2022 18:50:34 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=299, global_step=600
06/19/2022 18:50:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=304
06/19/2022 18:50:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
06/19/2022 18:50:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=314
06/19/2022 18:50:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.28 on epoch=319
06/19/2022 18:50:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=324
06/19/2022 18:50:42 - INFO - __main__ - Global step 650 Train loss 0.25 ACC 0.625 on epoch=324
06/19/2022 18:50:42 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.625 on epoch=324, global_step=650
06/19/2022 18:50:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=329
06/19/2022 18:50:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=334
06/19/2022 18:50:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
06/19/2022 18:50:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=344
06/19/2022 18:50:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=349
06/19/2022 18:50:50 - INFO - __main__ - Global step 700 Train loss 0.23 ACC 0.5625 on epoch=349
06/19/2022 18:50:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=354
06/19/2022 18:50:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=359
06/19/2022 18:50:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=364
06/19/2022 18:50:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=369
06/19/2022 18:50:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=374
06/19/2022 18:50:57 - INFO - __main__ - Global step 750 Train loss 0.24 ACC 0.5 on epoch=374
06/19/2022 18:50:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
06/19/2022 18:50:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=384
06/19/2022 18:51:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
06/19/2022 18:51:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=394
06/19/2022 18:51:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=399
06/19/2022 18:51:05 - INFO - __main__ - Global step 800 Train loss 0.22 ACC 0.59375 on epoch=399
06/19/2022 18:51:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=404
06/19/2022 18:51:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=409
06/19/2022 18:51:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=414
06/19/2022 18:51:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=419
06/19/2022 18:51:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=424
06/19/2022 18:51:12 - INFO - __main__ - Global step 850 Train loss 0.24 ACC 0.53125 on epoch=424
06/19/2022 18:51:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=429
06/19/2022 18:51:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=434
06/19/2022 18:51:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=439
06/19/2022 18:51:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=444
06/19/2022 18:51:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=449
06/19/2022 18:51:21 - INFO - __main__ - Global step 900 Train loss 0.24 ACC 0.5 on epoch=449
06/19/2022 18:51:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.25 on epoch=454
06/19/2022 18:51:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=459
06/19/2022 18:51:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=464
06/19/2022 18:51:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=469
06/19/2022 18:51:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=474
06/19/2022 18:51:29 - INFO - __main__ - Global step 950 Train loss 0.22 ACC 0.5 on epoch=474
06/19/2022 18:51:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=479
06/19/2022 18:51:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=484
06/19/2022 18:51:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=489
06/19/2022 18:51:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.17 on epoch=494
06/19/2022 18:51:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=499
06/19/2022 18:51:37 - INFO - __main__ - Global step 1000 Train loss 0.18 ACC 0.5625 on epoch=499
06/19/2022 18:51:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=504
06/19/2022 18:51:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=509
06/19/2022 18:51:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=514
06/19/2022 18:51:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=519
06/19/2022 18:51:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=524
06/19/2022 18:51:45 - INFO - __main__ - Global step 1050 Train loss 0.18 ACC 0.65625 on epoch=524
06/19/2022 18:51:45 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=524, global_step=1050
06/19/2022 18:51:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=529
06/19/2022 18:51:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.15 on epoch=534
06/19/2022 18:51:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=539
06/19/2022 18:51:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=544
06/19/2022 18:51:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=549
06/19/2022 18:51:54 - INFO - __main__ - Global step 1100 Train loss 0.12 ACC 0.625 on epoch=549
06/19/2022 18:51:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=554
06/19/2022 18:51:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=559
06/19/2022 18:51:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=564
06/19/2022 18:51:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=569
06/19/2022 18:52:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=574
06/19/2022 18:52:01 - INFO - __main__ - Global step 1150 Train loss 0.10 ACC 0.6875 on epoch=574
06/19/2022 18:52:01 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=574, global_step=1150
06/19/2022 18:52:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=579
06/19/2022 18:52:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=584
06/19/2022 18:52:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.12 on epoch=589
06/19/2022 18:52:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=594
06/19/2022 18:52:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=599
06/19/2022 18:52:09 - INFO - __main__ - Global step 1200 Train loss 0.10 ACC 0.625 on epoch=599
06/19/2022 18:52:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
06/19/2022 18:52:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=609
06/19/2022 18:52:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=614
06/19/2022 18:52:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=619
06/19/2022 18:52:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=624
06/19/2022 18:52:17 - INFO - __main__ - Global step 1250 Train loss 0.05 ACC 0.6875 on epoch=624
06/19/2022 18:52:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
06/19/2022 18:52:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=634
06/19/2022 18:52:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=639
06/19/2022 18:52:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=644
06/19/2022 18:52:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=649
06/19/2022 18:52:24 - INFO - __main__ - Global step 1300 Train loss 0.06 ACC 0.5625 on epoch=649
06/19/2022 18:52:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
06/19/2022 18:52:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
06/19/2022 18:52:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
06/19/2022 18:52:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
06/19/2022 18:52:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
06/19/2022 18:52:31 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.6875 on epoch=674
06/19/2022 18:52:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
06/19/2022 18:52:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
06/19/2022 18:52:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/19/2022 18:52:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
06/19/2022 18:52:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
06/19/2022 18:52:39 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.65625 on epoch=699
06/19/2022 18:52:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=704
06/19/2022 18:52:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/19/2022 18:52:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/19/2022 18:52:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/19/2022 18:52:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/19/2022 18:52:47 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.6875 on epoch=724
06/19/2022 18:52:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
06/19/2022 18:52:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
06/19/2022 18:52:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
06/19/2022 18:52:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
06/19/2022 18:52:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/19/2022 18:52:55 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.59375 on epoch=749
06/19/2022 18:52:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
06/19/2022 18:52:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
06/19/2022 18:52:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
06/19/2022 18:53:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
06/19/2022 18:53:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/19/2022 18:53:02 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.59375 on epoch=774
06/19/2022 18:53:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/19/2022 18:53:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
06/19/2022 18:53:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/19/2022 18:53:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
06/19/2022 18:53:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/19/2022 18:53:10 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.6875 on epoch=799
06/19/2022 18:53:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/19/2022 18:53:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/19/2022 18:53:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=814
06/19/2022 18:53:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
06/19/2022 18:53:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
06/19/2022 18:53:19 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.5625 on epoch=824
06/19/2022 18:53:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/19/2022 18:53:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/19/2022 18:53:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/19/2022 18:53:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=844
06/19/2022 18:53:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=849
06/19/2022 18:53:27 - INFO - __main__ - Global step 1700 Train loss 0.04 ACC 0.625 on epoch=849
06/19/2022 18:53:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/19/2022 18:53:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
06/19/2022 18:53:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
06/19/2022 18:53:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/19/2022 18:53:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
06/19/2022 18:53:35 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.625 on epoch=874
06/19/2022 18:53:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/19/2022 18:53:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
06/19/2022 18:53:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
06/19/2022 18:53:41 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/19/2022 18:53:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/19/2022 18:53:43 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.71875 on epoch=899
06/19/2022 18:53:43 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=899, global_step=1800
06/19/2022 18:53:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
06/19/2022 18:53:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/19/2022 18:53:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/19/2022 18:53:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/19/2022 18:53:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/19/2022 18:53:51 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.625 on epoch=924
06/19/2022 18:53:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
06/19/2022 18:53:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/19/2022 18:53:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=939
06/19/2022 18:53:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/19/2022 18:53:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/19/2022 18:53:59 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.625 on epoch=949
06/19/2022 18:54:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/19/2022 18:54:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/19/2022 18:54:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
06/19/2022 18:54:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/19/2022 18:54:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/19/2022 18:54:07 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.59375 on epoch=974
06/19/2022 18:54:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/19/2022 18:54:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
06/19/2022 18:54:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/19/2022 18:54:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/19/2022 18:54:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/19/2022 18:54:15 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.625 on epoch=999
06/19/2022 18:54:15 - INFO - __main__ - save last model!
06/19/2022 18:54:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 18:54:15 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 18:54:15 - INFO - __main__ - Printing 3 examples
06/19/2022 18:54:15 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 18:54:15 - INFO - __main__ - ['equivalent']
06/19/2022 18:54:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 18:54:15 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:54:15 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 18:54:15 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:54:15 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:54:15 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:54:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:54:15 - INFO - __main__ - Printing 3 examples
06/19/2022 18:54:15 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 18:54:15 - INFO - __main__ - ['equivalent']
06/19/2022 18:54:15 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 18:54:15 - INFO - __main__ - ['equivalent']
06/19/2022 18:54:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 18:54:15 - INFO - __main__ - ['equivalent']
06/19/2022 18:54:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 18:54:15 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:54:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:54:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:54:15 - INFO - __main__ - Printing 3 examples
06/19/2022 18:54:15 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 18:54:15 - INFO - __main__ - ['equivalent']
06/19/2022 18:54:15 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 18:54:15 - INFO - __main__ - ['equivalent']
06/19/2022 18:54:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 18:54:15 - INFO - __main__ - ['equivalent']
06/19/2022 18:54:15 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:54:15 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:54:16 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:54:16 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 18:54:23 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:54:23 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(4269, 248302), (4800, 205648), (3565, 292071), (1941, 484885), (1808, 389120), (6473, 163516), (4674, 244421), (2681, 395660), (4879, 229125), (502, 1761063), (3151, 296309), (4225, 197872), (4109, 245907), (4346, 242679), (1663, 588937), (4271, 188919), (441, 2112542), (1048, 702299), (5459, 186372), (4777, 158874), (3616, 283413), (1107, 853041), (4877, 212046), (553, 856375), (5721, 178984), (6061, 205009), (107, 5404644), (1188, 621816), (3563, 229598), (687, 582989), (863, 1077354), (2740, 199631), (2909, 350152), (2766, 278103), (4119, 235907), (828, 1018934), (2652, 286006), (4174, 175228), (4237, 246742), (3347, 297641), (4212, 191151), (6161, 172774), (420, 2224081), (762, 1164855), (2948, 393735), (3214, 325872), (2028, 404951), (990, 919957), (5028, 214929), (1101, 860462), (1116, 852428), (4378, 210800), (1396, 545283), (3305, 313410), (313, 2745242), (541, 1667482), (3566, 290067), (643, 1424557), (3834, 319344), (3759, 261834), (3698, 250226), (3184, 216978), (1566, 615576), (1845, 378247), (5908, 176655), (4501, 228152), (6326, 165064), (603, 506191), (1842, 531497), (776, 518408), (893, 1024942), (6672, 154162), (3677, 244767), (2326, 385363), (5638, 188290), (6193, 160491), (1361, 625400), (2491, 397483), (4794, 202669), (2669, 364149), (2781, 203418), (2488, 362236), (99, 8503604), (5718, 201254), (3231, 325809), (5154, 171205), (1486, 571578), (829, 1087334), (4929, 211594), (3434, 270859), (4305, 159538), (3164, 313795), (4685, 220739), (2507, 369601), (3757, 205092), (2917, 311362), (2124, 458893), (3479, 243656), (5558, 193177)]
06/19/2022 18:54:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:54:23 - INFO - __main__ - Starting training!
06/19/2022 18:54:25 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_21_0.4_8_predictions.txt
06/19/2022 18:54:25 - INFO - __main__ - ACC on test data: 0.6422
06/19/2022 18:54:25 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.4, bsz=8, dev_performance=0.71875, test_performance=0.6421568627450981
06/19/2022 18:54:25 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.3, bsz=8 ...
06/19/2022 18:54:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:54:26 - INFO - __main__ - Printing 3 examples
06/19/2022 18:54:26 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 18:54:26 - INFO - __main__ - ['equivalent']
06/19/2022 18:54:26 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 18:54:26 - INFO - __main__ - ['equivalent']
06/19/2022 18:54:26 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 18:54:26 - INFO - __main__ - ['equivalent']
06/19/2022 18:54:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 18:54:26 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:54:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:54:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:54:26 - INFO - __main__ - Printing 3 examples
06/19/2022 18:54:26 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 18:54:26 - INFO - __main__ - ['equivalent']
06/19/2022 18:54:26 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 18:54:26 - INFO - __main__ - ['equivalent']
06/19/2022 18:54:26 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 18:54:26 - INFO - __main__ - ['equivalent']
06/19/2022 18:54:26 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:54:26 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:54:26 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:54:33 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:54:33 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(4269, 248302), (4800, 205648), (3565, 292071), (1941, 484885), (1808, 389120), (6473, 163516), (4674, 244421), (2681, 395660), (4879, 229125), (502, 1761063), (3151, 296309), (4225, 197872), (4109, 245907), (4346, 242679), (1663, 588937), (4271, 188919), (441, 2112542), (1048, 702299), (5459, 186372), (4777, 158874), (3616, 283413), (1107, 853041), (4877, 212046), (553, 856375), (5721, 178984), (6061, 205009), (107, 5404644), (1188, 621816), (3563, 229598), (687, 582989), (863, 1077354), (2740, 199631), (2909, 350152), (2766, 278103), (4119, 235907), (828, 1018934), (2652, 286006), (4174, 175228), (4237, 246742), (3347, 297641), (4212, 191151), (6161, 172774), (420, 2224081), (762, 1164855), (2948, 393735), (3214, 325872), (2028, 404951), (990, 919957), (5028, 214929), (1101, 860462), (1116, 852428), (4378, 210800), (1396, 545283), (3305, 313410), (313, 2745242), (541, 1667482), (3566, 290067), (643, 1424557), (3834, 319344), (3759, 261834), (3698, 250226), (3184, 216978), (1566, 615576), (1845, 378247), (5908, 176655), (4501, 228152), (6326, 165064), (603, 506191), (1842, 531497), (776, 518408), (893, 1024942), (6672, 154162), (3677, 244767), (2326, 385363), (5638, 188290), (6193, 160491), (1361, 625400), (2491, 397483), (4794, 202669), (2669, 364149), (2781, 203418), (2488, 362236), (99, 8503604), (5718, 201254), (3231, 325809), (5154, 171205), (1486, 571578), (829, 1087334), (4929, 211594), (3434, 270859), (4305, 159538), (3164, 313795), (4685, 220739), (2507, 369601), (3757, 205092), (2917, 311362), (2124, 458893), (3479, 243656), (5558, 193177)]
06/19/2022 18:54:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:54:33 - INFO - __main__ - Starting training!
06/19/2022 18:54:35 - INFO - __main__ - Step 10 Global step 10 Train loss 6.43 on epoch=4
06/19/2022 18:54:36 - INFO - __main__ - Step 20 Global step 20 Train loss 3.26 on epoch=9
06/19/2022 18:54:38 - INFO - __main__ - Step 30 Global step 30 Train loss 1.46 on epoch=14
06/19/2022 18:54:39 - INFO - __main__ - Step 40 Global step 40 Train loss 0.77 on epoch=19
06/19/2022 18:54:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=24
06/19/2022 18:54:42 - INFO - __main__ - Global step 50 Train loss 2.49 ACC 0.5 on epoch=24
06/19/2022 18:54:42 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 18:54:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=29
06/19/2022 18:54:44 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=34
06/19/2022 18:54:45 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=39
06/19/2022 18:54:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=44
06/19/2022 18:54:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.41 on epoch=49
06/19/2022 18:54:49 - INFO - __main__ - Global step 100 Train loss 0.42 ACC 0.5 on epoch=49
06/19/2022 18:54:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.35 on epoch=54
06/19/2022 18:54:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.34 on epoch=59
06/19/2022 18:54:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.36 on epoch=64
06/19/2022 18:54:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.31 on epoch=69
06/19/2022 18:54:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.34 on epoch=74
06/19/2022 18:54:57 - INFO - __main__ - Global step 150 Train loss 0.34 ACC 0.5 on epoch=74
06/19/2022 18:54:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.34 on epoch=79
06/19/2022 18:55:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=84
06/19/2022 18:55:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.33 on epoch=89
06/19/2022 18:55:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=94
06/19/2022 18:55:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=99
06/19/2022 18:55:04 - INFO - __main__ - Global step 200 Train loss 0.34 ACC 0.5 on epoch=99
06/19/2022 18:55:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.30 on epoch=104
06/19/2022 18:55:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.30 on epoch=109
06/19/2022 18:55:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
06/19/2022 18:55:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=119
06/19/2022 18:55:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=124
06/19/2022 18:55:12 - INFO - __main__ - Global step 250 Train loss 0.29 ACC 0.5 on epoch=124
06/19/2022 18:55:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.32 on epoch=129
06/19/2022 18:55:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.31 on epoch=134
06/19/2022 18:55:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.29 on epoch=139
06/19/2022 18:55:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.27 on epoch=144
06/19/2022 18:55:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=149
06/19/2022 18:55:19 - INFO - __main__ - Global step 300 Train loss 0.30 ACC 0.6875 on epoch=149
06/19/2022 18:55:19 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.6875 on epoch=149, global_step=300
06/19/2022 18:55:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
06/19/2022 18:55:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.29 on epoch=159
06/19/2022 18:55:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=164
06/19/2022 18:55:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
06/19/2022 18:55:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=174
06/19/2022 18:55:26 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.5 on epoch=174
06/19/2022 18:55:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=179
06/19/2022 18:55:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=184
06/19/2022 18:55:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
06/19/2022 18:55:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=194
06/19/2022 18:55:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
06/19/2022 18:55:34 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.5625 on epoch=199
06/19/2022 18:55:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=204
06/19/2022 18:55:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=209
06/19/2022 18:55:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.29 on epoch=214
06/19/2022 18:55:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=219
06/19/2022 18:55:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.29 on epoch=224
06/19/2022 18:55:42 - INFO - __main__ - Global step 450 Train loss 0.27 ACC 0.5 on epoch=224
06/19/2022 18:55:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
06/19/2022 18:55:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=234
06/19/2022 18:55:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=239
06/19/2022 18:55:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
06/19/2022 18:55:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=249
06/19/2022 18:55:49 - INFO - __main__ - Global step 500 Train loss 0.24 ACC 0.5 on epoch=249
06/19/2022 18:55:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=254
06/19/2022 18:55:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=259
06/19/2022 18:55:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
06/19/2022 18:55:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=269
06/19/2022 18:55:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
06/19/2022 18:55:57 - INFO - __main__ - Global step 550 Train loss 0.25 ACC 0.5 on epoch=274
06/19/2022 18:55:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=279
06/19/2022 18:56:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=284
06/19/2022 18:56:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=289
06/19/2022 18:56:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=294
06/19/2022 18:56:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=299
06/19/2022 18:56:05 - INFO - __main__ - Global step 600 Train loss 0.22 ACC 0.75 on epoch=299
06/19/2022 18:56:05 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.75 on epoch=299, global_step=600
06/19/2022 18:56:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=304
06/19/2022 18:56:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
06/19/2022 18:56:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=314
06/19/2022 18:56:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=319
06/19/2022 18:56:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=324
06/19/2022 18:56:13 - INFO - __main__ - Global step 650 Train loss 0.23 ACC 0.5 on epoch=324
06/19/2022 18:56:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=329
06/19/2022 18:56:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=334
06/19/2022 18:56:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=339
06/19/2022 18:56:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=344
06/19/2022 18:56:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=349
06/19/2022 18:56:21 - INFO - __main__ - Global step 700 Train loss 0.22 ACC 0.6875 on epoch=349
06/19/2022 18:56:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=354
06/19/2022 18:56:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=359
06/19/2022 18:56:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=364
06/19/2022 18:56:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=369
06/19/2022 18:56:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=374
06/19/2022 18:56:29 - INFO - __main__ - Global step 750 Train loss 0.18 ACC 0.53125 on epoch=374
06/19/2022 18:56:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=379
06/19/2022 18:56:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=384
06/19/2022 18:56:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
06/19/2022 18:56:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=394
06/19/2022 18:56:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=399
06/19/2022 18:56:36 - INFO - __main__ - Global step 800 Train loss 0.20 ACC 0.71875 on epoch=399
06/19/2022 18:56:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=404
06/19/2022 18:56:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=409
06/19/2022 18:56:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.98 on epoch=414
06/19/2022 18:56:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=419
06/19/2022 18:56:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=424
06/19/2022 18:56:44 - INFO - __main__ - Global step 850 Train loss 0.37 ACC 0.71875 on epoch=424
06/19/2022 18:56:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=429
06/19/2022 18:56:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=434
06/19/2022 18:56:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=439
06/19/2022 18:56:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=444
06/19/2022 18:56:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=449
06/19/2022 18:56:51 - INFO - __main__ - Global step 900 Train loss 0.16 ACC 0.6875 on epoch=449
06/19/2022 18:56:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=454
06/19/2022 18:56:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.15 on epoch=459
06/19/2022 18:56:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=464
06/19/2022 18:56:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=469
06/19/2022 18:56:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=474
06/19/2022 18:56:59 - INFO - __main__ - Global step 950 Train loss 0.13 ACC 0.65625 on epoch=474
06/19/2022 18:57:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=479
06/19/2022 18:57:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=484
06/19/2022 18:57:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.10 on epoch=489
06/19/2022 18:57:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
06/19/2022 18:57:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=499
06/19/2022 18:57:07 - INFO - __main__ - Global step 1000 Train loss 0.10 ACC 0.59375 on epoch=499
06/19/2022 18:57:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=504
06/19/2022 18:57:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=509
06/19/2022 18:57:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=514
06/19/2022 18:57:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=519
06/19/2022 18:57:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
06/19/2022 18:57:15 - INFO - __main__ - Global step 1050 Train loss 0.08 ACC 0.625 on epoch=524
06/19/2022 18:57:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=529
06/19/2022 18:57:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=534
06/19/2022 18:57:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=539
06/19/2022 18:57:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=544
06/19/2022 18:57:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
06/19/2022 18:57:23 - INFO - __main__ - Global step 1100 Train loss 0.08 ACC 0.65625 on epoch=549
06/19/2022 18:57:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
06/19/2022 18:57:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=559
06/19/2022 18:57:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=564
06/19/2022 18:57:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=569
06/19/2022 18:57:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=574
06/19/2022 18:57:31 - INFO - __main__ - Global step 1150 Train loss 0.07 ACC 0.625 on epoch=574
06/19/2022 18:57:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=579
06/19/2022 18:57:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=584
06/19/2022 18:57:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=589
06/19/2022 18:57:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=594
06/19/2022 18:57:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=599
06/19/2022 18:57:39 - INFO - __main__ - Global step 1200 Train loss 0.09 ACC 0.65625 on epoch=599
06/19/2022 18:57:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
06/19/2022 18:57:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=609
06/19/2022 18:57:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
06/19/2022 18:57:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=619
06/19/2022 18:57:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=624
06/19/2022 18:57:47 - INFO - __main__ - Global step 1250 Train loss 0.07 ACC 0.59375 on epoch=624
06/19/2022 18:57:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.09 on epoch=629
06/19/2022 18:57:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
06/19/2022 18:57:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=639
06/19/2022 18:57:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.09 on epoch=644
06/19/2022 18:57:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
06/19/2022 18:57:55 - INFO - __main__ - Global step 1300 Train loss 0.06 ACC 0.65625 on epoch=649
06/19/2022 18:57:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=654
06/19/2022 18:57:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=659
06/19/2022 18:57:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=664
06/19/2022 18:58:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
06/19/2022 18:58:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
06/19/2022 18:58:03 - INFO - __main__ - Global step 1350 Train loss 0.08 ACC 0.59375 on epoch=674
06/19/2022 18:58:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
06/19/2022 18:58:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=684
06/19/2022 18:58:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=689
06/19/2022 18:58:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=694
06/19/2022 18:58:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=699
06/19/2022 18:58:11 - INFO - __main__ - Global step 1400 Train loss 0.07 ACC 0.6875 on epoch=699
06/19/2022 18:58:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=704
06/19/2022 18:58:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=709
06/19/2022 18:58:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=714
06/19/2022 18:58:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=719
06/19/2022 18:58:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=724
06/19/2022 18:58:19 - INFO - __main__ - Global step 1450 Train loss 0.10 ACC 0.6875 on epoch=724
06/19/2022 18:58:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=729
06/19/2022 18:58:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=734
06/19/2022 18:58:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=739
06/19/2022 18:58:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=744
06/19/2022 18:58:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=749
06/19/2022 18:58:26 - INFO - __main__ - Global step 1500 Train loss 0.12 ACC 0.59375 on epoch=749
06/19/2022 18:58:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=754
06/19/2022 18:58:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=759
06/19/2022 18:58:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=764
06/19/2022 18:58:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
06/19/2022 18:58:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=774
06/19/2022 18:58:34 - INFO - __main__ - Global step 1550 Train loss 0.08 ACC 0.71875 on epoch=774
06/19/2022 18:58:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=779
06/19/2022 18:58:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=784
06/19/2022 18:58:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=789
06/19/2022 18:58:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=794
06/19/2022 18:58:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
06/19/2022 18:58:43 - INFO - __main__ - Global step 1600 Train loss 0.05 ACC 0.6875 on epoch=799
06/19/2022 18:58:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=804
06/19/2022 18:58:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
06/19/2022 18:58:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
06/19/2022 18:58:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=819
06/19/2022 18:58:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
06/19/2022 18:58:51 - INFO - __main__ - Global step 1650 Train loss 0.04 ACC 0.625 on epoch=824
06/19/2022 18:58:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
06/19/2022 18:58:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=834
06/19/2022 18:58:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=839
06/19/2022 18:58:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=844
06/19/2022 18:58:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
06/19/2022 18:58:59 - INFO - __main__ - Global step 1700 Train loss 0.03 ACC 0.65625 on epoch=849
06/19/2022 18:59:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/19/2022 18:59:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=859
06/19/2022 18:59:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
06/19/2022 18:59:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
06/19/2022 18:59:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/19/2022 18:59:06 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.65625 on epoch=874
06/19/2022 18:59:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/19/2022 18:59:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
06/19/2022 18:59:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=889
06/19/2022 18:59:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=894
06/19/2022 18:59:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
06/19/2022 18:59:14 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.625 on epoch=899
06/19/2022 18:59:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/19/2022 18:59:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
06/19/2022 18:59:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/19/2022 18:59:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/19/2022 18:59:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/19/2022 18:59:21 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.65625 on epoch=924
06/19/2022 18:59:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/19/2022 18:59:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
06/19/2022 18:59:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
06/19/2022 18:59:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=944
06/19/2022 18:59:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=949
06/19/2022 18:59:29 - INFO - __main__ - Global step 1900 Train loss 0.04 ACC 0.65625 on epoch=949
06/19/2022 18:59:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
06/19/2022 18:59:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/19/2022 18:59:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/19/2022 18:59:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/19/2022 18:59:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/19/2022 18:59:37 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.625 on epoch=974
06/19/2022 18:59:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/19/2022 18:59:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/19/2022 18:59:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
06/19/2022 18:59:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
06/19/2022 18:59:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=999
06/19/2022 18:59:45 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.59375 on epoch=999
06/19/2022 18:59:45 - INFO - __main__ - save last model!
06/19/2022 18:59:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 18:59:45 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 18:59:45 - INFO - __main__ - Printing 3 examples
06/19/2022 18:59:45 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 18:59:45 - INFO - __main__ - ['equivalent']
06/19/2022 18:59:45 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 18:59:45 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:59:45 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 18:59:45 - INFO - __main__ - ['not_equivalent']
06/19/2022 18:59:45 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:59:45 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:59:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:59:46 - INFO - __main__ - Printing 3 examples
06/19/2022 18:59:46 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 18:59:46 - INFO - __main__ - ['equivalent']
06/19/2022 18:59:46 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 18:59:46 - INFO - __main__ - ['equivalent']
06/19/2022 18:59:46 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 18:59:46 - INFO - __main__ - ['equivalent']
06/19/2022 18:59:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 18:59:46 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:59:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:59:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:59:46 - INFO - __main__ - Printing 3 examples
06/19/2022 18:59:46 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 18:59:46 - INFO - __main__ - ['equivalent']
06/19/2022 18:59:46 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 18:59:46 - INFO - __main__ - ['equivalent']
06/19/2022 18:59:46 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 18:59:46 - INFO - __main__ - ['equivalent']
06/19/2022 18:59:46 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:59:46 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:59:46 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 18:59:46 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 18:59:52 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 18:59:52 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(1130, 821238), (2505, 391017), (2260, 245889), (6412, 163495), (4205, 261446), (6219, 162939), (3217, 316619), (3119, 317140), (261, 3686702), (5120, 236973), (2851, 338896), (71, 9677051), (6326, 165064), (301, 1599306), (3882, 155103), (5676, 188429), (3248, 326955), (2134, 401363), (6997, 154928), (4199, 265295), (2620, 417765), (1389, 480591), (2756, 369138), (1502, 652657), (1040, 867513), (308, 1959584), (3116, 304867), (4158, 211917), (383, 2368094), (7228, 161576), (3889, 257596), (1691, 674454), (493, 921866), (2808, 317238), (2338, 287842), (4285, 226300), (1848, 517255), (2051, 455798), (1202, 672342), (5058, 173227), (3721, 256780), (6643, 176713), (4514, 186655), (2736, 340951), (1719, 551197), (2604, 359999), (2280, 193726), (3579, 277730), (1108, 766167), (3114, 200024), (809, 1073970), (1435, 636297), (2460, 406809), (5542, 236029), (6357, 167884), (3985, 256868), (983, 921089), (4083, 237800), (140, 5830325), (3001, 351252), (5545, 174101), (1427, 715870), (3541, 206040), (3701, 254712), (3026, 353651), (6881, 160252), (4294, 245995), (3221, 252473), (1037, 808814), (2657, 395294), (5030, 198426), (226, 3154509), (6398, 166527), (3438, 286245), (6733, 154076), (2351, 414849), (691, 1251430), (5163, 201510), (3068, 311224), (4456, 230159), (1295, 756208), (1024, 639303), (442, 1688276), (418, 2112116), (2777, 308767), (3180, 189715), (1323, 759069), (1015, 867172), (2948, 393735), (5589, 173942), (1303, 658549), (4033, 240888), (3845, 206498), (3783, 253073), (4871, 173063), (4531, 223921), (6297, 159697), (1656, 582760), (4657, 226628)]
06/19/2022 18:59:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 18:59:53 - INFO - __main__ - Starting training!
06/19/2022 18:59:55 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_21_0.3_8_predictions.txt
06/19/2022 18:59:55 - INFO - __main__ - ACC on test data: 0.6275
06/19/2022 18:59:55 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.3, bsz=8, dev_performance=0.75, test_performance=0.6274509803921569
06/19/2022 18:59:55 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.2, bsz=8 ...
06/19/2022 18:59:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:59:56 - INFO - __main__ - Printing 3 examples
06/19/2022 18:59:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 18:59:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:59:56 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 18:59:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:59:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 18:59:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:59:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 18:59:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:59:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 18:59:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 18:59:56 - INFO - __main__ - Printing 3 examples
06/19/2022 18:59:56 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 18:59:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:59:56 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 18:59:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:59:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 18:59:56 - INFO - __main__ - ['equivalent']
06/19/2022 18:59:56 - INFO - __main__ - Tokenizing Input ...
06/19/2022 18:59:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 18:59:56 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:00:01 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:00:01 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(1130, 821238), (2505, 391017), (2260, 245889), (6412, 163495), (4205, 261446), (6219, 162939), (3217, 316619), (3119, 317140), (261, 3686702), (5120, 236973), (2851, 338896), (71, 9677051), (6326, 165064), (301, 1599306), (3882, 155103), (5676, 188429), (3248, 326955), (2134, 401363), (6997, 154928), (4199, 265295), (2620, 417765), (1389, 480591), (2756, 369138), (1502, 652657), (1040, 867513), (308, 1959584), (3116, 304867), (4158, 211917), (383, 2368094), (7228, 161576), (3889, 257596), (1691, 674454), (493, 921866), (2808, 317238), (2338, 287842), (4285, 226300), (1848, 517255), (2051, 455798), (1202, 672342), (5058, 173227), (3721, 256780), (6643, 176713), (4514, 186655), (2736, 340951), (1719, 551197), (2604, 359999), (2280, 193726), (3579, 277730), (1108, 766167), (3114, 200024), (809, 1073970), (1435, 636297), (2460, 406809), (5542, 236029), (6357, 167884), (3985, 256868), (983, 921089), (4083, 237800), (140, 5830325), (3001, 351252), (5545, 174101), (1427, 715870), (3541, 206040), (3701, 254712), (3026, 353651), (6881, 160252), (4294, 245995), (3221, 252473), (1037, 808814), (2657, 395294), (5030, 198426), (226, 3154509), (6398, 166527), (3438, 286245), (6733, 154076), (2351, 414849), (691, 1251430), (5163, 201510), (3068, 311224), (4456, 230159), (1295, 756208), (1024, 639303), (442, 1688276), (418, 2112116), (2777, 308767), (3180, 189715), (1323, 759069), (1015, 867172), (2948, 393735), (5589, 173942), (1303, 658549), (4033, 240888), (3845, 206498), (3783, 253073), (4871, 173063), (4531, 223921), (6297, 159697), (1656, 582760), (4657, 226628)]
06/19/2022 19:00:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:00:02 - INFO - __main__ - Starting training!
06/19/2022 19:00:03 - INFO - __main__ - Step 10 Global step 10 Train loss 7.02 on epoch=4
06/19/2022 19:00:05 - INFO - __main__ - Step 20 Global step 20 Train loss 5.11 on epoch=9
06/19/2022 19:00:06 - INFO - __main__ - Step 30 Global step 30 Train loss 3.03 on epoch=14
06/19/2022 19:00:08 - INFO - __main__ - Step 40 Global step 40 Train loss 2.08 on epoch=19
06/19/2022 19:00:09 - INFO - __main__ - Step 50 Global step 50 Train loss 1.31 on epoch=24
06/19/2022 19:00:09 - INFO - __main__ - Global step 50 Train loss 3.71 ACC 0.46875 on epoch=24
06/19/2022 19:00:09 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.46875 on epoch=24, global_step=50
06/19/2022 19:00:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=29
06/19/2022 19:00:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.71 on epoch=34
06/19/2022 19:00:13 - INFO - __main__ - Step 80 Global step 80 Train loss 0.70 on epoch=39
06/19/2022 19:00:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=44
06/19/2022 19:00:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=49
06/19/2022 19:00:16 - INFO - __main__ - Global step 100 Train loss 0.68 ACC 0.5 on epoch=49
06/19/2022 19:00:16 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=49, global_step=100
06/19/2022 19:00:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
06/19/2022 19:00:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=59
06/19/2022 19:00:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=64
06/19/2022 19:00:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.40 on epoch=69
06/19/2022 19:00:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=74
06/19/2022 19:00:24 - INFO - __main__ - Global step 150 Train loss 0.44 ACC 0.4375 on epoch=74
06/19/2022 19:00:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=79
06/19/2022 19:00:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=84
06/19/2022 19:00:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
06/19/2022 19:00:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=94
06/19/2022 19:00:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=99
06/19/2022 19:00:31 - INFO - __main__ - Global step 200 Train loss 0.35 ACC 0.5625 on epoch=99
06/19/2022 19:00:31 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=99, global_step=200
06/19/2022 19:00:32 - INFO - __main__ - Step 210 Global step 210 Train loss 3.92 on epoch=104
06/19/2022 19:00:34 - INFO - __main__ - Step 220 Global step 220 Train loss 1.49 on epoch=109
06/19/2022 19:00:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=114
06/19/2022 19:00:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=119
06/19/2022 19:00:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=124
06/19/2022 19:00:39 - INFO - __main__ - Global step 250 Train loss 1.30 ACC 0.4375 on epoch=124
06/19/2022 19:00:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=129
06/19/2022 19:00:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=134
06/19/2022 19:00:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.29 on epoch=139
06/19/2022 19:00:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=144
06/19/2022 19:00:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.28 on epoch=149
06/19/2022 19:00:46 - INFO - __main__ - Global step 300 Train loss 0.30 ACC 0.5 on epoch=149
06/19/2022 19:00:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=154
06/19/2022 19:00:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=159
06/19/2022 19:00:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=164
06/19/2022 19:00:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=169
06/19/2022 19:00:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=174
06/19/2022 19:00:55 - INFO - __main__ - Global step 350 Train loss 0.33 ACC 0.4375 on epoch=174
06/19/2022 19:00:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=179
06/19/2022 19:00:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=184
06/19/2022 19:00:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=189
06/19/2022 19:01:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=194
06/19/2022 19:01:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=199
06/19/2022 19:01:02 - INFO - __main__ - Global step 400 Train loss 0.34 ACC 0.46875 on epoch=199
06/19/2022 19:01:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=204
06/19/2022 19:01:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=209
06/19/2022 19:01:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=214
06/19/2022 19:01:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.32 on epoch=219
06/19/2022 19:01:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=224
06/19/2022 19:01:11 - INFO - __main__ - Global step 450 Train loss 0.33 ACC 0.5 on epoch=224
06/19/2022 19:01:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.31 on epoch=229
06/19/2022 19:01:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=234
06/19/2022 19:01:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=239
06/19/2022 19:01:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=244
06/19/2022 19:01:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=249
06/19/2022 19:01:18 - INFO - __main__ - Global step 500 Train loss 0.29 ACC 0.5 on epoch=249
06/19/2022 19:01:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.32 on epoch=254
06/19/2022 19:01:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.29 on epoch=259
06/19/2022 19:01:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.28 on epoch=264
06/19/2022 19:01:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=269
06/19/2022 19:01:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=274
06/19/2022 19:01:27 - INFO - __main__ - Global step 550 Train loss 0.30 ACC 0.53125 on epoch=274
06/19/2022 19:01:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=279
06/19/2022 19:01:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=284
06/19/2022 19:01:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.31 on epoch=289
06/19/2022 19:01:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.29 on epoch=294
06/19/2022 19:01:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
06/19/2022 19:01:34 - INFO - __main__ - Global step 600 Train loss 0.28 ACC 0.5 on epoch=299
06/19/2022 19:01:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.34 on epoch=304
06/19/2022 19:01:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
06/19/2022 19:01:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.29 on epoch=314
06/19/2022 19:01:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=319
06/19/2022 19:01:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=324
06/19/2022 19:01:42 - INFO - __main__ - Global step 650 Train loss 0.29 ACC 0.5625 on epoch=324
06/19/2022 19:01:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=329
06/19/2022 19:01:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=334
06/19/2022 19:01:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.28 on epoch=339
06/19/2022 19:01:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=344
06/19/2022 19:01:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=349
06/19/2022 19:01:50 - INFO - __main__ - Global step 700 Train loss 0.28 ACC 0.5 on epoch=349
06/19/2022 19:01:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=354
06/19/2022 19:01:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=359
06/19/2022 19:01:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.28 on epoch=364
06/19/2022 19:01:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.27 on epoch=369
06/19/2022 19:01:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=374
06/19/2022 19:01:59 - INFO - __main__ - Global step 750 Train loss 0.29 ACC 0.5 on epoch=374
06/19/2022 19:02:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=379
06/19/2022 19:02:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=384
06/19/2022 19:02:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=389
06/19/2022 19:02:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=394
06/19/2022 19:02:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=399
06/19/2022 19:02:06 - INFO - __main__ - Global step 800 Train loss 0.28 ACC 0.5 on epoch=399
06/19/2022 19:02:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=404
06/19/2022 19:02:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=409
06/19/2022 19:02:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=414
06/19/2022 19:02:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=419
06/19/2022 19:02:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=424
06/19/2022 19:02:14 - INFO - __main__ - Global step 850 Train loss 0.28 ACC 0.53125 on epoch=424
06/19/2022 19:02:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=429
06/19/2022 19:02:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=434
06/19/2022 19:02:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.31 on epoch=439
06/19/2022 19:02:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.30 on epoch=444
06/19/2022 19:02:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=449
06/19/2022 19:02:22 - INFO - __main__ - Global step 900 Train loss 0.29 ACC 0.5 on epoch=449
06/19/2022 19:02:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=454
06/19/2022 19:02:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=459
06/19/2022 19:02:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.26 on epoch=464
06/19/2022 19:02:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=469
06/19/2022 19:02:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=474
06/19/2022 19:02:29 - INFO - __main__ - Global step 950 Train loss 0.25 ACC 0.46875 on epoch=474
06/19/2022 19:02:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=479
06/19/2022 19:02:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=484
06/19/2022 19:02:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.27 on epoch=489
06/19/2022 19:02:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.30 on epoch=494
06/19/2022 19:02:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=499
06/19/2022 19:02:38 - INFO - __main__ - Global step 1000 Train loss 0.27 ACC 0.5 on epoch=499
06/19/2022 19:02:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=504
06/19/2022 19:02:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.26 on epoch=509
06/19/2022 19:02:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=514
06/19/2022 19:02:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=519
06/19/2022 19:02:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=524
06/19/2022 19:02:46 - INFO - __main__ - Global step 1050 Train loss 0.26 ACC 0.53125 on epoch=524
06/19/2022 19:02:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=529
06/19/2022 19:02:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=534
06/19/2022 19:02:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=539
06/19/2022 19:02:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.29 on epoch=544
06/19/2022 19:02:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=549
06/19/2022 19:02:54 - INFO - __main__ - Global step 1100 Train loss 0.26 ACC 0.5 on epoch=549
06/19/2022 19:02:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.28 on epoch=554
06/19/2022 19:02:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=559
06/19/2022 19:02:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=564
06/19/2022 19:02:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=569
06/19/2022 19:03:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.25 on epoch=574
06/19/2022 19:03:02 - INFO - __main__ - Global step 1150 Train loss 0.26 ACC 0.46875 on epoch=574
06/19/2022 19:03:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.27 on epoch=579
06/19/2022 19:03:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=584
06/19/2022 19:03:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=589
06/19/2022 19:03:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=594
06/19/2022 19:03:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=599
06/19/2022 19:03:09 - INFO - __main__ - Global step 1200 Train loss 0.27 ACC 0.53125 on epoch=599
06/19/2022 19:03:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=604
06/19/2022 19:03:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=609
06/19/2022 19:03:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.25 on epoch=614
06/19/2022 19:03:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=619
06/19/2022 19:03:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=624
06/19/2022 19:03:16 - INFO - __main__ - Global step 1250 Train loss 0.24 ACC 0.5 on epoch=624
06/19/2022 19:03:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.25 on epoch=629
06/19/2022 19:03:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=634
06/19/2022 19:03:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.25 on epoch=639
06/19/2022 19:03:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=644
06/19/2022 19:03:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.27 on epoch=649
06/19/2022 19:03:24 - INFO - __main__ - Global step 1300 Train loss 0.27 ACC 0.625 on epoch=649
06/19/2022 19:03:24 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.625 on epoch=649, global_step=1300
06/19/2022 19:03:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=654
06/19/2022 19:03:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.26 on epoch=659
06/19/2022 19:03:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=664
06/19/2022 19:03:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=669
06/19/2022 19:03:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=674
06/19/2022 19:03:32 - INFO - __main__ - Global step 1350 Train loss 0.25 ACC 0.59375 on epoch=674
06/19/2022 19:03:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=679
06/19/2022 19:03:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=684
06/19/2022 19:03:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=689
06/19/2022 19:03:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=694
06/19/2022 19:03:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=699
06/19/2022 19:03:40 - INFO - __main__ - Global step 1400 Train loss 0.24 ACC 0.59375 on epoch=699
06/19/2022 19:03:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=704
06/19/2022 19:03:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.22 on epoch=709
06/19/2022 19:03:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.22 on epoch=714
06/19/2022 19:03:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=719
06/19/2022 19:03:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=724
06/19/2022 19:03:47 - INFO - __main__ - Global step 1450 Train loss 0.21 ACC 0.53125 on epoch=724
06/19/2022 19:03:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=729
06/19/2022 19:03:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.23 on epoch=734
06/19/2022 19:03:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=739
06/19/2022 19:03:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=744
06/19/2022 19:03:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.23 on epoch=749
06/19/2022 19:03:55 - INFO - __main__ - Global step 1500 Train loss 0.23 ACC 0.5 on epoch=749
06/19/2022 19:03:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.23 on epoch=754
06/19/2022 19:03:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=759
06/19/2022 19:04:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.24 on epoch=764
06/19/2022 19:04:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.26 on epoch=769
06/19/2022 19:04:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.24 on epoch=774
06/19/2022 19:04:04 - INFO - __main__ - Global step 1550 Train loss 0.24 ACC 0.59375 on epoch=774
06/19/2022 19:04:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.29 on epoch=779
06/19/2022 19:04:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=784
06/19/2022 19:04:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=789
06/19/2022 19:04:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=794
06/19/2022 19:04:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=799
06/19/2022 19:04:11 - INFO - __main__ - Global step 1600 Train loss 0.22 ACC 0.5625 on epoch=799
06/19/2022 19:04:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=804
06/19/2022 19:04:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=809
06/19/2022 19:04:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=814
06/19/2022 19:04:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.23 on epoch=819
06/19/2022 19:04:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=824
06/19/2022 19:04:19 - INFO - __main__ - Global step 1650 Train loss 0.21 ACC 0.65625 on epoch=824
06/19/2022 19:04:19 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=824, global_step=1650
06/19/2022 19:04:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.18 on epoch=829
06/19/2022 19:04:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=834
06/19/2022 19:04:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=839
06/19/2022 19:04:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=844
06/19/2022 19:04:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=849
06/19/2022 19:04:27 - INFO - __main__ - Global step 1700 Train loss 0.18 ACC 0.5 on epoch=849
06/19/2022 19:04:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=854
06/19/2022 19:04:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=859
06/19/2022 19:04:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=864
06/19/2022 19:04:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=869
06/19/2022 19:04:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=874
06/19/2022 19:04:35 - INFO - __main__ - Global step 1750 Train loss 0.17 ACC 0.53125 on epoch=874
06/19/2022 19:04:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=879
06/19/2022 19:04:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=884
06/19/2022 19:04:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=889
06/19/2022 19:04:41 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.16 on epoch=894
06/19/2022 19:04:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=899
06/19/2022 19:04:43 - INFO - __main__ - Global step 1800 Train loss 0.15 ACC 0.5 on epoch=899
06/19/2022 19:04:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=904
06/19/2022 19:04:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=909
06/19/2022 19:04:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=914
06/19/2022 19:04:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=919
06/19/2022 19:04:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=924
06/19/2022 19:04:51 - INFO - __main__ - Global step 1850 Train loss 0.15 ACC 0.625 on epoch=924
06/19/2022 19:04:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=929
06/19/2022 19:04:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=934
06/19/2022 19:04:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=939
06/19/2022 19:04:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.15 on epoch=944
06/19/2022 19:04:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=949
06/19/2022 19:04:58 - INFO - __main__ - Global step 1900 Train loss 0.14 ACC 0.5 on epoch=949
06/19/2022 19:04:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=954
06/19/2022 19:05:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=959
06/19/2022 19:05:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=964
06/19/2022 19:05:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=969
06/19/2022 19:05:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=974
06/19/2022 19:05:05 - INFO - __main__ - Global step 1950 Train loss 0.11 ACC 0.625 on epoch=974
06/19/2022 19:05:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=979
06/19/2022 19:05:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=984
06/19/2022 19:05:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=989
06/19/2022 19:05:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.11 on epoch=994
06/19/2022 19:05:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=999
06/19/2022 19:05:13 - INFO - __main__ - Global step 2000 Train loss 0.11 ACC 0.53125 on epoch=999
06/19/2022 19:05:13 - INFO - __main__ - save last model!
06/19/2022 19:05:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:05:13 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:05:13 - INFO - __main__ - Printing 3 examples
06/19/2022 19:05:13 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:05:13 - INFO - __main__ - ['equivalent']
06/19/2022 19:05:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:05:13 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:05:13 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:05:13 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:05:13 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:05:13 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:05:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:05:13 - INFO - __main__ - Printing 3 examples
06/19/2022 19:05:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 19:05:13 - INFO - __main__ - ['equivalent']
06/19/2022 19:05:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 19:05:13 - INFO - __main__ - ['equivalent']
06/19/2022 19:05:13 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 19:05:13 - INFO - __main__ - ['equivalent']
06/19/2022 19:05:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:05:13 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:05:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:05:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:05:13 - INFO - __main__ - Printing 3 examples
06/19/2022 19:05:13 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 19:05:13 - INFO - __main__ - ['equivalent']
06/19/2022 19:05:13 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 19:05:13 - INFO - __main__ - ['equivalent']
06/19/2022 19:05:13 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 19:05:13 - INFO - __main__ - ['equivalent']
06/19/2022 19:05:13 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:05:13 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:05:13 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:05:13 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:05:19 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:05:19 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(1096, 857963), (4738, 190584), (4049, 230165), (1793, 332226), (208, 2410751), (3699, 253359), (2204, 421709), (5094, 210937), (4813, 210454), (1976, 392476), (4110, 259070), (1053, 857202), (4273, 255574), (6171, 171690), (3084, 261725), (661, 1317687), (4284, 235257), (2728, 337315), (2019, 481757), (1308, 726600), (3541, 206040), (977, 908393), (2476, 431307), (4749, 157283), (241, 3838559), (705, 1548305), (3827, 282190), (2182, 379209), (1380, 703116), (2960, 165540), (2003, 445063), (726, 1159344), (5894, 169006), (1673, 440939), (6562, 159935), (1696, 423464), (2541, 395945), (2652, 286006), (1638, 604699), (6523, 157539), (37, 31055239), (2349, 335938), (1592, 619246), (1317, 681720), (4871, 173063), (2756, 369138), (1027, 519620), (1267, 795407), (298, 3090275), (1371, 673811), (168, 5553682), (3165, 262039), (1757, 390014), (7344, 159238), (2158, 290395), (174, 5233753), (927, 520773), (2859, 349629), (342, 1503107), (1275, 695113), (1919, 222828), (5458, 178957), (1128, 773025), (731, 1250799), (4162, 196307), (4849, 209310), (4187, 259804), (6546, 182629), (6114, 158081), (1172, 802296), (3273, 207524), (5776, 185572), (1572, 414616), (499, 1766405), (6166, 180142), (2715, 311751), (1907, 204437), (283, 2553263), (534, 1311231), (5351, 195051), (3431, 233202), (4676, 219209), (1200, 812133), (5216, 190231), (5182, 210925), (663, 1092211), (874, 1011784), (3106, 291674), (4556, 154640), (1161, 600418), (772, 1209132), (1135, 702740), (2306, 338371), (8857, 160161), (3772, 170716), (3627, 289143), (4234, 246090), (7216, 157191), (3465, 176815)]
06/19/2022 19:05:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:05:19 - INFO - __main__ - Starting training!
06/19/2022 19:05:21 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_21_0.2_8_predictions.txt
06/19/2022 19:05:21 - INFO - __main__ - ACC on test data: 0.6863
06/19/2022 19:05:21 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.2, bsz=8, dev_performance=0.65625, test_performance=0.6862745098039216
06/19/2022 19:05:21 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.5, bsz=8 ...
06/19/2022 19:05:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:05:22 - INFO - __main__ - Printing 3 examples
06/19/2022 19:05:22 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 19:05:22 - INFO - __main__ - ['equivalent']
06/19/2022 19:05:22 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 19:05:22 - INFO - __main__ - ['equivalent']
06/19/2022 19:05:22 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 19:05:22 - INFO - __main__ - ['equivalent']
06/19/2022 19:05:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:05:22 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:05:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:05:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:05:22 - INFO - __main__ - Printing 3 examples
06/19/2022 19:05:22 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 19:05:22 - INFO - __main__ - ['equivalent']
06/19/2022 19:05:22 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 19:05:22 - INFO - __main__ - ['equivalent']
06/19/2022 19:05:22 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 19:05:22 - INFO - __main__ - ['equivalent']
06/19/2022 19:05:22 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:05:22 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:05:22 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:05:28 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:05:28 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(1096, 857963), (4738, 190584), (4049, 230165), (1793, 332226), (208, 2410751), (3699, 253359), (2204, 421709), (5094, 210937), (4813, 210454), (1976, 392476), (4110, 259070), (1053, 857202), (4273, 255574), (6171, 171690), (3084, 261725), (661, 1317687), (4284, 235257), (2728, 337315), (2019, 481757), (1308, 726600), (3541, 206040), (977, 908393), (2476, 431307), (4749, 157283), (241, 3838559), (705, 1548305), (3827, 282190), (2182, 379209), (1380, 703116), (2960, 165540), (2003, 445063), (726, 1159344), (5894, 169006), (1673, 440939), (6562, 159935), (1696, 423464), (2541, 395945), (2652, 286006), (1638, 604699), (6523, 157539), (37, 31055239), (2349, 335938), (1592, 619246), (1317, 681720), (4871, 173063), (2756, 369138), (1027, 519620), (1267, 795407), (298, 3090275), (1371, 673811), (168, 5553682), (3165, 262039), (1757, 390014), (7344, 159238), (2158, 290395), (174, 5233753), (927, 520773), (2859, 349629), (342, 1503107), (1275, 695113), (1919, 222828), (5458, 178957), (1128, 773025), (731, 1250799), (4162, 196307), (4849, 209310), (4187, 259804), (6546, 182629), (6114, 158081), (1172, 802296), (3273, 207524), (5776, 185572), (1572, 414616), (499, 1766405), (6166, 180142), (2715, 311751), (1907, 204437), (283, 2553263), (534, 1311231), (5351, 195051), (3431, 233202), (4676, 219209), (1200, 812133), (5216, 190231), (5182, 210925), (663, 1092211), (874, 1011784), (3106, 291674), (4556, 154640), (1161, 600418), (772, 1209132), (1135, 702740), (2306, 338371), (8857, 160161), (3772, 170716), (3627, 289143), (4234, 246090), (7216, 157191), (3465, 176815)]
06/19/2022 19:05:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:05:28 - INFO - __main__ - Starting training!
06/19/2022 19:05:30 - INFO - __main__ - Step 10 Global step 10 Train loss 5.44 on epoch=4
06/19/2022 19:05:31 - INFO - __main__ - Step 20 Global step 20 Train loss 2.31 on epoch=9
06/19/2022 19:05:33 - INFO - __main__ - Step 30 Global step 30 Train loss 0.92 on epoch=14
06/19/2022 19:05:34 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=19
06/19/2022 19:05:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.56 on epoch=24
06/19/2022 19:05:36 - INFO - __main__ - Global step 50 Train loss 1.96 ACC 0.5 on epoch=24
06/19/2022 19:05:36 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 19:05:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=29
06/19/2022 19:05:39 - INFO - __main__ - Step 70 Global step 70 Train loss 0.33 on epoch=34
06/19/2022 19:05:41 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=39
06/19/2022 19:05:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.35 on epoch=44
06/19/2022 19:05:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.35 on epoch=49
06/19/2022 19:05:44 - INFO - __main__ - Global step 100 Train loss 0.35 ACC 0.5 on epoch=49
06/19/2022 19:05:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=54
06/19/2022 19:05:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
06/19/2022 19:05:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=64
06/19/2022 19:05:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.34 on epoch=69
06/19/2022 19:05:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.26 on epoch=74
06/19/2022 19:05:52 - INFO - __main__ - Global step 150 Train loss 0.32 ACC 0.5 on epoch=74
06/19/2022 19:05:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=79
06/19/2022 19:05:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
06/19/2022 19:05:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=89
06/19/2022 19:05:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=94
06/19/2022 19:06:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=99
06/19/2022 19:06:00 - INFO - __main__ - Global step 200 Train loss 0.28 ACC 0.5 on epoch=99
06/19/2022 19:06:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=104
06/19/2022 19:06:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=109
06/19/2022 19:06:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=114
06/19/2022 19:06:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.28 on epoch=119
06/19/2022 19:06:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=124
06/19/2022 19:06:09 - INFO - __main__ - Global step 250 Train loss 0.28 ACC 0.34375 on epoch=124
06/19/2022 19:06:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.29 on epoch=129
06/19/2022 19:06:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=134
06/19/2022 19:06:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=139
06/19/2022 19:06:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=144
06/19/2022 19:06:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=149
06/19/2022 19:06:17 - INFO - __main__ - Global step 300 Train loss 0.26 ACC 0.46875 on epoch=149
06/19/2022 19:06:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
06/19/2022 19:06:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=159
06/19/2022 19:06:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
06/19/2022 19:06:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
06/19/2022 19:06:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
06/19/2022 19:06:25 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.5 on epoch=174
06/19/2022 19:06:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=179
06/19/2022 19:06:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
06/19/2022 19:06:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=189
06/19/2022 19:06:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
06/19/2022 19:06:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=199
06/19/2022 19:06:33 - INFO - __main__ - Global step 400 Train loss 0.24 ACC 0.5 on epoch=199
06/19/2022 19:06:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=204
06/19/2022 19:06:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=209
06/19/2022 19:06:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=214
06/19/2022 19:06:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
06/19/2022 19:06:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
06/19/2022 19:06:41 - INFO - __main__ - Global step 450 Train loss 0.24 ACC 0.59375 on epoch=224
06/19/2022 19:06:41 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=224, global_step=450
06/19/2022 19:06:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=229
06/19/2022 19:06:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=234
06/19/2022 19:06:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
06/19/2022 19:06:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=244
06/19/2022 19:06:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=249
06/19/2022 19:06:48 - INFO - __main__ - Global step 500 Train loss 0.23 ACC 0.5 on epoch=249
06/19/2022 19:06:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
06/19/2022 19:06:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=259
06/19/2022 19:06:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=264
06/19/2022 19:06:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=269
06/19/2022 19:06:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=274
06/19/2022 19:06:56 - INFO - __main__ - Global step 550 Train loss 0.23 ACC 0.5 on epoch=274
06/19/2022 19:06:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=279
06/19/2022 19:06:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
06/19/2022 19:07:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=289
06/19/2022 19:07:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=294
06/19/2022 19:07:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=299
06/19/2022 19:07:04 - INFO - __main__ - Global step 600 Train loss 0.21 ACC 0.5 on epoch=299
06/19/2022 19:07:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=304
06/19/2022 19:07:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=309
06/19/2022 19:07:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=314
06/19/2022 19:07:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=319
06/19/2022 19:07:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=324
06/19/2022 19:07:10 - INFO - __main__ - Global step 650 Train loss 0.21 ACC 0.5 on epoch=324
06/19/2022 19:07:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=329
06/19/2022 19:07:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.27 on epoch=334
06/19/2022 19:07:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=339
06/19/2022 19:07:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=344
06/19/2022 19:07:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=349
06/19/2022 19:07:18 - INFO - __main__ - Global step 700 Train loss 0.25 ACC 0.5 on epoch=349
06/19/2022 19:07:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=354
06/19/2022 19:07:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=359
06/19/2022 19:07:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=364
06/19/2022 19:07:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=369
06/19/2022 19:07:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=374
06/19/2022 19:07:25 - INFO - __main__ - Global step 750 Train loss 0.21 ACC 0.46875 on epoch=374
06/19/2022 19:07:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=379
06/19/2022 19:07:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=384
06/19/2022 19:07:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=389
06/19/2022 19:07:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=394
06/19/2022 19:07:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.17 on epoch=399
06/19/2022 19:07:33 - INFO - __main__ - Global step 800 Train loss 0.17 ACC 0.53125 on epoch=399
06/19/2022 19:07:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=404
06/19/2022 19:07:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=409
06/19/2022 19:07:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=414
06/19/2022 19:07:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=419
06/19/2022 19:07:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=424
06/19/2022 19:07:41 - INFO - __main__ - Global step 850 Train loss 0.13 ACC 0.59375 on epoch=424
06/19/2022 19:07:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
06/19/2022 19:07:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=434
06/19/2022 19:07:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=439
06/19/2022 19:07:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=444
06/19/2022 19:07:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
06/19/2022 19:07:48 - INFO - __main__ - Global step 900 Train loss 0.07 ACC 0.5 on epoch=449
06/19/2022 19:07:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=454
06/19/2022 19:07:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=459
06/19/2022 19:07:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
06/19/2022 19:07:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=469
06/19/2022 19:07:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
06/19/2022 19:07:55 - INFO - __main__ - Global step 950 Train loss 0.06 ACC 0.59375 on epoch=474
06/19/2022 19:07:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
06/19/2022 19:07:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
06/19/2022 19:07:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
06/19/2022 19:08:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=494
06/19/2022 19:08:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=499
06/19/2022 19:08:03 - INFO - __main__ - Global step 1000 Train loss 0.06 ACC 0.53125 on epoch=499
06/19/2022 19:08:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
06/19/2022 19:08:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=509
06/19/2022 19:08:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/19/2022 19:08:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
06/19/2022 19:08:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/19/2022 19:08:10 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.53125 on epoch=524
06/19/2022 19:08:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/19/2022 19:08:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
06/19/2022 19:08:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/19/2022 19:08:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
06/19/2022 19:08:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/19/2022 19:08:18 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.5 on epoch=549
06/19/2022 19:08:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
06/19/2022 19:08:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=559
06/19/2022 19:08:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/19/2022 19:08:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
06/19/2022 19:08:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/19/2022 19:08:26 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.5 on epoch=574
06/19/2022 19:08:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
06/19/2022 19:08:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/19/2022 19:08:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=589
06/19/2022 19:08:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/19/2022 19:08:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/19/2022 19:08:34 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.5625 on epoch=599
06/19/2022 19:08:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
06/19/2022 19:08:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
06/19/2022 19:08:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/19/2022 19:08:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/19/2022 19:08:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/19/2022 19:08:41 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.46875 on epoch=624
06/19/2022 19:08:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/19/2022 19:08:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
06/19/2022 19:08:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=639
06/19/2022 19:08:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/19/2022 19:08:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/19/2022 19:08:48 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.5 on epoch=649
06/19/2022 19:08:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/19/2022 19:08:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/19/2022 19:08:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
06/19/2022 19:08:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/19/2022 19:08:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/19/2022 19:08:56 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.5 on epoch=674
06/19/2022 19:08:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/19/2022 19:09:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/19/2022 19:09:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/19/2022 19:09:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
06/19/2022 19:09:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/19/2022 19:09:05 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.46875 on epoch=699
06/19/2022 19:09:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/19/2022 19:09:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
06/19/2022 19:09:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/19/2022 19:09:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/19/2022 19:09:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/19/2022 19:09:12 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.46875 on epoch=724
06/19/2022 19:09:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/19/2022 19:09:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/19/2022 19:09:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/19/2022 19:09:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/19/2022 19:09:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/19/2022 19:09:20 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.1875 on epoch=749
06/19/2022 19:09:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/19/2022 19:09:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/19/2022 19:09:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/19/2022 19:09:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/19/2022 19:09:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/19/2022 19:09:28 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.375 on epoch=774
06/19/2022 19:09:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
06/19/2022 19:09:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/19/2022 19:09:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/19/2022 19:09:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/19/2022 19:09:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/19/2022 19:09:36 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.4375 on epoch=799
06/19/2022 19:09:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/19/2022 19:09:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/19/2022 19:09:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/19/2022 19:09:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/19/2022 19:09:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/19/2022 19:09:43 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.5 on epoch=824
06/19/2022 19:09:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/19/2022 19:09:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/19/2022 19:09:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/19/2022 19:09:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=844
06/19/2022 19:09:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=849
06/19/2022 19:09:52 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.46875 on epoch=849
06/19/2022 19:09:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/19/2022 19:09:55 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/19/2022 19:09:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/19/2022 19:09:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/19/2022 19:10:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/19/2022 19:10:01 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.4375 on epoch=874
06/19/2022 19:10:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/19/2022 19:10:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/19/2022 19:10:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/19/2022 19:10:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/19/2022 19:10:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/19/2022 19:10:09 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.53125 on epoch=899
06/19/2022 19:10:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/19/2022 19:10:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/19/2022 19:10:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/19/2022 19:10:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=919
06/19/2022 19:10:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=924
06/19/2022 19:10:16 - INFO - __main__ - Global step 1850 Train loss 0.04 ACC 0.4375 on epoch=924
06/19/2022 19:10:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/19/2022 19:10:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/19/2022 19:10:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/19/2022 19:10:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/19/2022 19:10:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
06/19/2022 19:10:24 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.4375 on epoch=949
06/19/2022 19:10:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/19/2022 19:10:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/19/2022 19:10:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/19/2022 19:10:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/19/2022 19:10:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/19/2022 19:10:31 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.4375 on epoch=974
06/19/2022 19:10:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
06/19/2022 19:10:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/19/2022 19:10:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/19/2022 19:10:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/19/2022 19:10:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/19/2022 19:10:39 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.46875 on epoch=999
06/19/2022 19:10:39 - INFO - __main__ - save last model!
06/19/2022 19:10:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:10:39 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:10:39 - INFO - __main__ - Printing 3 examples
06/19/2022 19:10:39 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:10:39 - INFO - __main__ - ['equivalent']
06/19/2022 19:10:39 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:10:39 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:10:39 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:10:39 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:10:39 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:10:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:10:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:10:39 - INFO - __main__ - Printing 3 examples
06/19/2022 19:10:39 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 19:10:39 - INFO - __main__ - ['equivalent']
06/19/2022 19:10:39 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 19:10:39 - INFO - __main__ - ['equivalent']
06/19/2022 19:10:39 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 19:10:39 - INFO - __main__ - ['equivalent']
06/19/2022 19:10:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:10:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:10:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:10:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:10:39 - INFO - __main__ - Printing 3 examples
06/19/2022 19:10:39 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 19:10:39 - INFO - __main__ - ['equivalent']
06/19/2022 19:10:39 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 19:10:39 - INFO - __main__ - ['equivalent']
06/19/2022 19:10:39 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 19:10:39 - INFO - __main__ - ['equivalent']
06/19/2022 19:10:39 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:10:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:10:39 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:10:40 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:10:46 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:10:46 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(1813, 317619), (3845, 206498), (5633, 185013), (3553, 155391), (3808, 217101), (1042, 857631), (1341, 767639), (5810, 169570), (2610, 426074), (3083, 217219), (2494, 207618), (2508, 403980), (3973, 227298), (3711, 277625), (5256, 206528), (4394, 234859), (3268, 226138), (1097, 881855), (3594, 300832), (4680, 218826), (2881, 363181), (3159, 250529), (1646, 587152), (5092, 197494), (751, 1191325), (962, 911154), (973, 965632), (1039, 892616), (4898, 216287), (1049, 871051), (3352, 251355), (1488, 681027), (6088, 168191), (389, 1329969), (1584, 159932), (6963, 166421), (3794, 166375), (2534, 284947), (1127, 724151), (4879, 229125), (2961, 264523), (4845, 221351), (596, 1528627), (3231, 325809), (5798, 156482), (2517, 300897), (3509, 266051), (1273, 833860), (5562, 162019), (3835, 184385), (2334, 430503), (1707, 582614), (3965, 193632), (1886, 409261), (1178, 811299), (3776, 266993), (4904, 168205), (4134, 255300), (1260, 767060), (3074, 334210), (7137, 163092), (2383, 406446), (5146, 217691), (5693, 166587), (4819, 168038), (310, 2990534), (6363, 154270), (3011, 346978), (313, 2745242), (1965, 505323), (2983, 340155), (2826, 304796), (3864, 276837), (3312, 266535), (63, 13320350), (1909, 502837), (1591, 624673), (6603, 165014), (4262, 233607), (761, 1146699), (1003, 631823), (331, 2589436), (1246, 907731), (5763, 176930), (2329, 425780), (4295, 249014), (4336, 223332), (3719, 275802), (1249, 573784), (3693, 253676), (3447, 301910), (3566, 290067), (2788, 164681), (6631, 157532), (909, 970948), (592, 1501058), (1645, 581504), (564, 1570044), (885, 1006302)]
06/19/2022 19:10:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:10:46 - INFO - __main__ - Starting training!
06/19/2022 19:10:49 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_42_0.5_8_predictions.txt
06/19/2022 19:10:49 - INFO - __main__ - ACC on test data: 0.4461
06/19/2022 19:10:49 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.5, bsz=8, dev_performance=0.59375, test_performance=0.44607843137254904
06/19/2022 19:10:49 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.4, bsz=8 ...
06/19/2022 19:10:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:10:50 - INFO - __main__ - Printing 3 examples
06/19/2022 19:10:50 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 19:10:50 - INFO - __main__ - ['equivalent']
06/19/2022 19:10:50 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 19:10:50 - INFO - __main__ - ['equivalent']
06/19/2022 19:10:50 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 19:10:50 - INFO - __main__ - ['equivalent']
06/19/2022 19:10:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:10:50 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:10:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:10:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:10:50 - INFO - __main__ - Printing 3 examples
06/19/2022 19:10:50 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 19:10:50 - INFO - __main__ - ['equivalent']
06/19/2022 19:10:50 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 19:10:50 - INFO - __main__ - ['equivalent']
06/19/2022 19:10:50 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 19:10:50 - INFO - __main__ - ['equivalent']
06/19/2022 19:10:50 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:10:50 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:10:50 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:10:56 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:10:56 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(1813, 317619), (3845, 206498), (5633, 185013), (3553, 155391), (3808, 217101), (1042, 857631), (1341, 767639), (5810, 169570), (2610, 426074), (3083, 217219), (2494, 207618), (2508, 403980), (3973, 227298), (3711, 277625), (5256, 206528), (4394, 234859), (3268, 226138), (1097, 881855), (3594, 300832), (4680, 218826), (2881, 363181), (3159, 250529), (1646, 587152), (5092, 197494), (751, 1191325), (962, 911154), (973, 965632), (1039, 892616), (4898, 216287), (1049, 871051), (3352, 251355), (1488, 681027), (6088, 168191), (389, 1329969), (1584, 159932), (6963, 166421), (3794, 166375), (2534, 284947), (1127, 724151), (4879, 229125), (2961, 264523), (4845, 221351), (596, 1528627), (3231, 325809), (5798, 156482), (2517, 300897), (3509, 266051), (1273, 833860), (5562, 162019), (3835, 184385), (2334, 430503), (1707, 582614), (3965, 193632), (1886, 409261), (1178, 811299), (3776, 266993), (4904, 168205), (4134, 255300), (1260, 767060), (3074, 334210), (7137, 163092), (2383, 406446), (5146, 217691), (5693, 166587), (4819, 168038), (310, 2990534), (6363, 154270), (3011, 346978), (313, 2745242), (1965, 505323), (2983, 340155), (2826, 304796), (3864, 276837), (3312, 266535), (63, 13320350), (1909, 502837), (1591, 624673), (6603, 165014), (4262, 233607), (761, 1146699), (1003, 631823), (331, 2589436), (1246, 907731), (5763, 176930), (2329, 425780), (4295, 249014), (4336, 223332), (3719, 275802), (1249, 573784), (3693, 253676), (3447, 301910), (3566, 290067), (2788, 164681), (6631, 157532), (909, 970948), (592, 1501058), (1645, 581504), (564, 1570044), (885, 1006302)]
06/19/2022 19:10:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:10:57 - INFO - __main__ - Starting training!
06/19/2022 19:10:58 - INFO - __main__ - Step 10 Global step 10 Train loss 5.75 on epoch=4
06/19/2022 19:11:00 - INFO - __main__ - Step 20 Global step 20 Train loss 3.29 on epoch=9
06/19/2022 19:11:02 - INFO - __main__ - Step 30 Global step 30 Train loss 2.36 on epoch=14
06/19/2022 19:11:03 - INFO - __main__ - Step 40 Global step 40 Train loss 3.16 on epoch=19
06/19/2022 19:11:05 - INFO - __main__ - Step 50 Global step 50 Train loss 1.28 on epoch=24
06/19/2022 19:11:05 - INFO - __main__ - Global step 50 Train loss 3.17 ACC 0.5 on epoch=24
06/19/2022 19:11:05 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 19:11:07 - INFO - __main__ - Step 60 Global step 60 Train loss 1.11 on epoch=29
06/19/2022 19:11:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.89 on epoch=34
06/19/2022 19:11:09 - INFO - __main__ - Step 80 Global step 80 Train loss 0.73 on epoch=39
06/19/2022 19:11:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.57 on epoch=44
06/19/2022 19:11:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=49
06/19/2022 19:11:12 - INFO - __main__ - Global step 100 Train loss 0.77 ACC 0.5 on epoch=49
06/19/2022 19:11:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=54
06/19/2022 19:11:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=59
06/19/2022 19:11:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=64
06/19/2022 19:11:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=69
06/19/2022 19:11:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.32 on epoch=74
06/19/2022 19:11:20 - INFO - __main__ - Global step 150 Train loss 0.44 ACC 0.375 on epoch=74
06/19/2022 19:11:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
06/19/2022 19:11:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=84
06/19/2022 19:11:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.33 on epoch=89
06/19/2022 19:11:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=94
06/19/2022 19:11:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=99
06/19/2022 19:11:27 - INFO - __main__ - Global step 200 Train loss 0.36 ACC 0.5 on epoch=99
06/19/2022 19:11:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=104
06/19/2022 19:11:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=109
06/19/2022 19:11:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.34 on epoch=114
06/19/2022 19:11:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.36 on epoch=119
06/19/2022 19:11:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=124
06/19/2022 19:11:34 - INFO - __main__ - Global step 250 Train loss 0.34 ACC 0.5 on epoch=124
06/19/2022 19:11:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.32 on epoch=129
06/19/2022 19:11:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=134
06/19/2022 19:11:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.30 on epoch=139
06/19/2022 19:11:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=144
06/19/2022 19:11:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=149
06/19/2022 19:11:42 - INFO - __main__ - Global step 300 Train loss 0.30 ACC 0.5 on epoch=149
06/19/2022 19:11:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.34 on epoch=154
06/19/2022 19:11:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=159
06/19/2022 19:11:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.30 on epoch=164
06/19/2022 19:11:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=169
06/19/2022 19:11:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=174
06/19/2022 19:11:49 - INFO - __main__ - Global step 350 Train loss 0.32 ACC 0.5 on epoch=174
06/19/2022 19:11:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.31 on epoch=179
06/19/2022 19:11:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=184
06/19/2022 19:11:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=189
06/19/2022 19:11:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=194
06/19/2022 19:11:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.31 on epoch=199
06/19/2022 19:11:57 - INFO - __main__ - Global step 400 Train loss 0.32 ACC 0.5 on epoch=199
06/19/2022 19:11:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=204
06/19/2022 19:12:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=209
06/19/2022 19:12:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=214
06/19/2022 19:12:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
06/19/2022 19:12:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
06/19/2022 19:12:04 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.5 on epoch=224
06/19/2022 19:12:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.29 on epoch=229
06/19/2022 19:12:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
06/19/2022 19:12:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=239
06/19/2022 19:12:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=244
06/19/2022 19:12:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=249
06/19/2022 19:12:12 - INFO - __main__ - Global step 500 Train loss 0.27 ACC 0.59375 on epoch=249
06/19/2022 19:12:12 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=249, global_step=500
06/19/2022 19:12:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=254
06/19/2022 19:12:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=259
06/19/2022 19:12:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=264
06/19/2022 19:12:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=269
06/19/2022 19:12:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=274
06/19/2022 19:12:20 - INFO - __main__ - Global step 550 Train loss 0.28 ACC 0.6875 on epoch=274
06/19/2022 19:12:20 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.6875 on epoch=274, global_step=550
06/19/2022 19:12:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.26 on epoch=279
06/19/2022 19:12:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=284
06/19/2022 19:12:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.30 on epoch=289
06/19/2022 19:12:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=294
06/19/2022 19:12:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=299
06/19/2022 19:12:27 - INFO - __main__ - Global step 600 Train loss 0.27 ACC 0.5 on epoch=299
06/19/2022 19:12:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=304
06/19/2022 19:12:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=309
06/19/2022 19:12:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=314
06/19/2022 19:12:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=319
06/19/2022 19:12:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=324
06/19/2022 19:12:35 - INFO - __main__ - Global step 650 Train loss 0.24 ACC 0.5 on epoch=324
06/19/2022 19:12:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=329
06/19/2022 19:12:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=334
06/19/2022 19:12:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=339
06/19/2022 19:12:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=344
06/19/2022 19:12:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=349
06/19/2022 19:12:44 - INFO - __main__ - Global step 700 Train loss 0.25 ACC 0.5625 on epoch=349
06/19/2022 19:12:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=354
06/19/2022 19:12:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=359
06/19/2022 19:12:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=364
06/19/2022 19:12:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=369
06/19/2022 19:12:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=374
06/19/2022 19:12:51 - INFO - __main__ - Global step 750 Train loss 0.23 ACC 0.5 on epoch=374
06/19/2022 19:12:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
06/19/2022 19:12:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=384
06/19/2022 19:12:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=389
06/19/2022 19:12:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=394
06/19/2022 19:12:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=399
06/19/2022 19:13:00 - INFO - __main__ - Global step 800 Train loss 0.22 ACC 0.5 on epoch=399
06/19/2022 19:13:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=404
06/19/2022 19:13:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=409
06/19/2022 19:13:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=414
06/19/2022 19:13:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=419
06/19/2022 19:13:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=424
06/19/2022 19:13:07 - INFO - __main__ - Global step 850 Train loss 0.23 ACC 0.5 on epoch=424
06/19/2022 19:13:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=429
06/19/2022 19:13:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.19 on epoch=434
06/19/2022 19:13:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=439
06/19/2022 19:13:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=444
06/19/2022 19:13:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=449
06/19/2022 19:13:15 - INFO - __main__ - Global step 900 Train loss 0.20 ACC 0.5 on epoch=449
06/19/2022 19:13:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=454
06/19/2022 19:13:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=459
06/19/2022 19:13:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=464
06/19/2022 19:13:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=469
06/19/2022 19:13:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=474
06/19/2022 19:13:22 - INFO - __main__ - Global step 950 Train loss 0.19 ACC 0.6875 on epoch=474
06/19/2022 19:13:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=479
06/19/2022 19:13:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=484
06/19/2022 19:13:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.10 on epoch=489
06/19/2022 19:13:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=494
06/19/2022 19:13:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=499
06/19/2022 19:13:30 - INFO - __main__ - Global step 1000 Train loss 0.13 ACC 0.4375 on epoch=499
06/19/2022 19:13:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.12 on epoch=504
06/19/2022 19:13:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=509
06/19/2022 19:13:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=514
06/19/2022 19:13:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
06/19/2022 19:13:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=524
06/19/2022 19:13:37 - INFO - __main__ - Global step 1050 Train loss 0.08 ACC 0.4375 on epoch=524
06/19/2022 19:13:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=529
06/19/2022 19:13:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
06/19/2022 19:13:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
06/19/2022 19:13:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=544
06/19/2022 19:13:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
06/19/2022 19:13:45 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.625 on epoch=549
06/19/2022 19:13:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
06/19/2022 19:13:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
06/19/2022 19:13:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=564
06/19/2022 19:13:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=569
06/19/2022 19:13:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=574
06/19/2022 19:13:53 - INFO - __main__ - Global step 1150 Train loss 0.11 ACC 0.5625 on epoch=574
06/19/2022 19:13:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.16 on epoch=579
06/19/2022 19:13:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=584
06/19/2022 19:13:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=589
06/19/2022 19:13:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
06/19/2022 19:13:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=599
06/19/2022 19:14:00 - INFO - __main__ - Global step 1200 Train loss 0.13 ACC 0.5 on epoch=599
06/19/2022 19:14:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=604
06/19/2022 19:14:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=609
06/19/2022 19:14:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/19/2022 19:14:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=619
06/19/2022 19:14:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
06/19/2022 19:14:07 - INFO - __main__ - Global step 1250 Train loss 0.04 ACC 0.5 on epoch=624
06/19/2022 19:14:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
06/19/2022 19:14:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
06/19/2022 19:14:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/19/2022 19:14:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/19/2022 19:14:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/19/2022 19:14:15 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.46875 on epoch=649
06/19/2022 19:14:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=654
06/19/2022 19:14:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
06/19/2022 19:14:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=664
06/19/2022 19:14:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
06/19/2022 19:14:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/19/2022 19:14:23 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.46875 on epoch=674
06/19/2022 19:14:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/19/2022 19:14:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/19/2022 19:14:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/19/2022 19:14:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
06/19/2022 19:14:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/19/2022 19:14:31 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.46875 on epoch=699
06/19/2022 19:14:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
06/19/2022 19:14:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
06/19/2022 19:14:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
06/19/2022 19:14:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
06/19/2022 19:14:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
06/19/2022 19:14:40 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.46875 on epoch=724
06/19/2022 19:14:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
06/19/2022 19:14:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
06/19/2022 19:14:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/19/2022 19:14:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
06/19/2022 19:14:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
06/19/2022 19:14:47 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.5 on epoch=749
06/19/2022 19:14:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
06/19/2022 19:14:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/19/2022 19:14:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/19/2022 19:14:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/19/2022 19:14:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/19/2022 19:14:55 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.4375 on epoch=774
06/19/2022 19:14:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
06/19/2022 19:14:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/19/2022 19:14:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/19/2022 19:15:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/19/2022 19:15:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
06/19/2022 19:15:03 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.4375 on epoch=799
06/19/2022 19:15:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/19/2022 19:15:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/19/2022 19:15:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/19/2022 19:15:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/19/2022 19:15:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
06/19/2022 19:15:11 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.40625 on epoch=824
06/19/2022 19:15:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/19/2022 19:15:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=834
06/19/2022 19:15:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/19/2022 19:15:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/19/2022 19:15:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/19/2022 19:15:19 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.375 on epoch=849
06/19/2022 19:15:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/19/2022 19:15:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/19/2022 19:15:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/19/2022 19:15:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/19/2022 19:15:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/19/2022 19:15:26 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.46875 on epoch=874
06/19/2022 19:15:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/19/2022 19:15:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
06/19/2022 19:15:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
06/19/2022 19:15:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/19/2022 19:15:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/19/2022 19:15:34 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.46875 on epoch=899
06/19/2022 19:15:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/19/2022 19:15:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/19/2022 19:15:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/19/2022 19:15:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/19/2022 19:15:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/19/2022 19:15:41 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.40625 on epoch=924
06/19/2022 19:15:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/19/2022 19:15:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/19/2022 19:15:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
06/19/2022 19:15:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
06/19/2022 19:15:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
06/19/2022 19:15:49 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.46875 on epoch=949
06/19/2022 19:15:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/19/2022 19:15:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/19/2022 19:15:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
06/19/2022 19:15:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/19/2022 19:15:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/19/2022 19:15:56 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.375 on epoch=974
06/19/2022 19:15:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/19/2022 19:16:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
06/19/2022 19:16:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/19/2022 19:16:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/19/2022 19:16:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/19/2022 19:16:05 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.46875 on epoch=999
06/19/2022 19:16:05 - INFO - __main__ - save last model!
06/19/2022 19:16:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:16:05 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:16:05 - INFO - __main__ - Printing 3 examples
06/19/2022 19:16:05 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:16:05 - INFO - __main__ - ['equivalent']
06/19/2022 19:16:05 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:16:05 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:16:05 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:16:05 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:16:05 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:16:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:16:05 - INFO - __main__ - Printing 3 examples
06/19/2022 19:16:05 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 19:16:05 - INFO - __main__ - ['equivalent']
06/19/2022 19:16:05 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 19:16:05 - INFO - __main__ - ['equivalent']
06/19/2022 19:16:05 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 19:16:05 - INFO - __main__ - ['equivalent']
06/19/2022 19:16:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:16:05 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:16:05 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:16:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:16:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:16:05 - INFO - __main__ - Printing 3 examples
06/19/2022 19:16:05 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 19:16:05 - INFO - __main__ - ['equivalent']
06/19/2022 19:16:05 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 19:16:05 - INFO - __main__ - ['equivalent']
06/19/2022 19:16:05 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 19:16:05 - INFO - __main__ - ['equivalent']
06/19/2022 19:16:05 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:16:05 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:16:05 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:16:06 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:16:12 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:16:12 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(2997, 341195), (3891, 211094), (4703, 220765), (3385, 191872), (6997, 154928), (5105, 211448), (5050, 227151), (1962, 340756), (2317, 424876), (5741, 183016), (1397, 772752), (2693, 271357), (3916, 217282), (749, 787508), (3977, 329839), (6888, 158291), (5324, 191543), (5650, 178209), (3786, 305013), (503, 1684486), (2428, 415536), (3205, 237602), (2434, 156656), (83, 1452381), (84, 10288303), (2125, 446994), (3298, 311752), (1445, 432249), (1501, 650320), (2478, 361778), (3376, 323744), (3438, 286245), (1269, 737181), (94, 9989518), (5727, 188354), (5008, 216986), (1879, 517099), (1384, 683777), (4335, 246709), (5620, 175844), (2134, 401363), (5446, 187212), (5199, 167310), (3883, 264299), (844, 1112441), (2255, 236375), (482, 1812745), (4903, 214208), (215, 4221120), (3095, 203292), (584, 1039586), (3073, 300611), (7555, 162623), (4112, 217671), (6337, 163252), (4048, 231896), (3063, 224634), (1108, 766167), (4724, 231722), (234, 3920679), (3046, 289396), (1906, 526589), (4947, 202998), (2744, 259869), (1155, 940173), (5069, 213802), (1162, 824379), (900, 380561), (4417, 214340), (5523, 159171), (4454, 233931), (5899, 179457), (739, 1029836), (5120, 236973), (2518, 303433), (4002, 277079), (2361, 413361), (1859, 282618), (1123, 537015), (816, 1059326), (6116, 183563), (1068, 771470), (3465, 176815), (4407, 183682), (2013, 472815), (4217, 267655), (3102, 266483), (1623, 603619), (2654, 392174), (953, 840707), (935, 612200), (2790, 361504), (1647, 462933), (1873, 523188), (6526, 155946), (1653, 586845), (757, 1071575), (1952, 512051), (3637, 189415)]
06/19/2022 19:16:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:16:13 - INFO - __main__ - Starting training!
06/19/2022 19:16:14 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_42_0.4_8_predictions.txt
06/19/2022 19:16:14 - INFO - __main__ - ACC on test data: 0.4583
06/19/2022 19:16:14 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.4, bsz=8, dev_performance=0.6875, test_performance=0.4583333333333333
06/19/2022 19:16:14 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.3, bsz=8 ...
06/19/2022 19:16:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:16:15 - INFO - __main__ - Printing 3 examples
06/19/2022 19:16:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 19:16:15 - INFO - __main__ - ['equivalent']
06/19/2022 19:16:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 19:16:15 - INFO - __main__ - ['equivalent']
06/19/2022 19:16:15 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 19:16:15 - INFO - __main__ - ['equivalent']
06/19/2022 19:16:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:16:15 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:16:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:16:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:16:15 - INFO - __main__ - Printing 3 examples
06/19/2022 19:16:15 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 19:16:15 - INFO - __main__ - ['equivalent']
06/19/2022 19:16:15 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 19:16:15 - INFO - __main__ - ['equivalent']
06/19/2022 19:16:15 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 19:16:15 - INFO - __main__ - ['equivalent']
06/19/2022 19:16:15 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:16:15 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:16:15 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:16:21 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:16:21 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(2997, 341195), (3891, 211094), (4703, 220765), (3385, 191872), (6997, 154928), (5105, 211448), (5050, 227151), (1962, 340756), (2317, 424876), (5741, 183016), (1397, 772752), (2693, 271357), (3916, 217282), (749, 787508), (3977, 329839), (6888, 158291), (5324, 191543), (5650, 178209), (3786, 305013), (503, 1684486), (2428, 415536), (3205, 237602), (2434, 156656), (83, 1452381), (84, 10288303), (2125, 446994), (3298, 311752), (1445, 432249), (1501, 650320), (2478, 361778), (3376, 323744), (3438, 286245), (1269, 737181), (94, 9989518), (5727, 188354), (5008, 216986), (1879, 517099), (1384, 683777), (4335, 246709), (5620, 175844), (2134, 401363), (5446, 187212), (5199, 167310), (3883, 264299), (844, 1112441), (2255, 236375), (482, 1812745), (4903, 214208), (215, 4221120), (3095, 203292), (584, 1039586), (3073, 300611), (7555, 162623), (4112, 217671), (6337, 163252), (4048, 231896), (3063, 224634), (1108, 766167), (4724, 231722), (234, 3920679), (3046, 289396), (1906, 526589), (4947, 202998), (2744, 259869), (1155, 940173), (5069, 213802), (1162, 824379), (900, 380561), (4417, 214340), (5523, 159171), (4454, 233931), (5899, 179457), (739, 1029836), (5120, 236973), (2518, 303433), (4002, 277079), (2361, 413361), (1859, 282618), (1123, 537015), (816, 1059326), (6116, 183563), (1068, 771470), (3465, 176815), (4407, 183682), (2013, 472815), (4217, 267655), (3102, 266483), (1623, 603619), (2654, 392174), (953, 840707), (935, 612200), (2790, 361504), (1647, 462933), (1873, 523188), (6526, 155946), (1653, 586845), (757, 1071575), (1952, 512051), (3637, 189415)]
06/19/2022 19:16:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:16:21 - INFO - __main__ - Starting training!
06/19/2022 19:16:23 - INFO - __main__ - Step 10 Global step 10 Train loss 6.11 on epoch=4
06/19/2022 19:16:25 - INFO - __main__ - Step 20 Global step 20 Train loss 3.00 on epoch=9
06/19/2022 19:16:26 - INFO - __main__ - Step 30 Global step 30 Train loss 1.91 on epoch=14
06/19/2022 19:16:27 - INFO - __main__ - Step 40 Global step 40 Train loss 1.14 on epoch=19
06/19/2022 19:16:29 - INFO - __main__ - Step 50 Global step 50 Train loss 0.69 on epoch=24
06/19/2022 19:16:30 - INFO - __main__ - Global step 50 Train loss 2.57 ACC 0.5 on epoch=24
06/19/2022 19:16:30 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 19:16:31 - INFO - __main__ - Step 60 Global step 60 Train loss 0.60 on epoch=29
06/19/2022 19:16:32 - INFO - __main__ - Step 70 Global step 70 Train loss 0.57 on epoch=34
06/19/2022 19:16:33 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=39
06/19/2022 19:16:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=44
06/19/2022 19:16:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.36 on epoch=49
06/19/2022 19:16:37 - INFO - __main__ - Global step 100 Train loss 0.49 ACC 0.5 on epoch=49
06/19/2022 19:16:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.35 on epoch=54
06/19/2022 19:16:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
06/19/2022 19:16:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=64
06/19/2022 19:16:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=69
06/19/2022 19:16:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
06/19/2022 19:16:44 - INFO - __main__ - Global step 150 Train loss 0.35 ACC 0.5 on epoch=74
06/19/2022 19:16:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=79
06/19/2022 19:16:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.36 on epoch=84
06/19/2022 19:16:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
06/19/2022 19:16:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=94
06/19/2022 19:16:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=99
06/19/2022 19:16:52 - INFO - __main__ - Global step 200 Train loss 0.37 ACC 0.5 on epoch=99
06/19/2022 19:16:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.32 on epoch=104
06/19/2022 19:16:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
06/19/2022 19:16:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=114
06/19/2022 19:16:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=119
06/19/2022 19:16:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.33 on epoch=124
06/19/2022 19:16:59 - INFO - __main__ - Global step 250 Train loss 0.32 ACC 0.5 on epoch=124
06/19/2022 19:17:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.32 on epoch=129
06/19/2022 19:17:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=134
06/19/2022 19:17:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.30 on epoch=139
06/19/2022 19:17:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
06/19/2022 19:17:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
06/19/2022 19:17:06 - INFO - __main__ - Global step 300 Train loss 0.29 ACC 0.5 on epoch=149
06/19/2022 19:17:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.33 on epoch=154
06/19/2022 19:17:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
06/19/2022 19:17:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=164
06/19/2022 19:17:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
06/19/2022 19:17:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
06/19/2022 19:17:13 - INFO - __main__ - Global step 350 Train loss 0.28 ACC 0.5 on epoch=174
06/19/2022 19:17:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.30 on epoch=179
06/19/2022 19:17:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
06/19/2022 19:17:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
06/19/2022 19:17:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=194
06/19/2022 19:17:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
06/19/2022 19:17:20 - INFO - __main__ - Global step 400 Train loss 0.27 ACC 0.5 on epoch=199
06/19/2022 19:17:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=204
06/19/2022 19:17:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=209
06/19/2022 19:17:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=214
06/19/2022 19:17:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=219
06/19/2022 19:17:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/19/2022 19:17:28 - INFO - __main__ - Global step 450 Train loss 0.27 ACC 0.5 on epoch=224
06/19/2022 19:17:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=229
06/19/2022 19:17:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
06/19/2022 19:17:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=239
06/19/2022 19:17:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=244
06/19/2022 19:17:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=249
06/19/2022 19:17:36 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.5 on epoch=249
06/19/2022 19:17:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=254
06/19/2022 19:17:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
06/19/2022 19:17:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
06/19/2022 19:17:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=269
06/19/2022 19:17:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=274
06/19/2022 19:17:44 - INFO - __main__ - Global step 550 Train loss 0.26 ACC 0.5 on epoch=274
06/19/2022 19:17:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=279
06/19/2022 19:17:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
06/19/2022 19:17:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=289
06/19/2022 19:17:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=294
06/19/2022 19:17:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=299
06/19/2022 19:17:51 - INFO - __main__ - Global step 600 Train loss 0.23 ACC 0.5 on epoch=299
06/19/2022 19:17:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=304
06/19/2022 19:17:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=309
06/19/2022 19:17:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=314
06/19/2022 19:17:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=319
06/19/2022 19:17:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=324
06/19/2022 19:18:00 - INFO - __main__ - Global step 650 Train loss 0.22 ACC 0.53125 on epoch=324
06/19/2022 19:18:00 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=324, global_step=650
06/19/2022 19:18:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=329
06/19/2022 19:18:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=334
06/19/2022 19:18:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=339
06/19/2022 19:18:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=344
06/19/2022 19:18:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=349
06/19/2022 19:18:08 - INFO - __main__ - Global step 700 Train loss 0.20 ACC 0.625 on epoch=349
06/19/2022 19:18:08 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.625 on epoch=349, global_step=700
06/19/2022 19:18:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.18 on epoch=354
06/19/2022 19:18:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=359
06/19/2022 19:18:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.18 on epoch=364
06/19/2022 19:18:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=369
06/19/2022 19:18:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=374
06/19/2022 19:18:16 - INFO - __main__ - Global step 750 Train loss 0.19 ACC 0.5 on epoch=374
06/19/2022 19:18:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=379
06/19/2022 19:18:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=384
06/19/2022 19:18:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=389
06/19/2022 19:18:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=394
06/19/2022 19:18:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.14 on epoch=399
06/19/2022 19:18:24 - INFO - __main__ - Global step 800 Train loss 0.19 ACC 0.5625 on epoch=399
06/19/2022 19:18:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=404
06/19/2022 19:18:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=409
06/19/2022 19:18:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=414
06/19/2022 19:18:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=419
06/19/2022 19:18:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=424
06/19/2022 19:18:32 - INFO - __main__ - Global step 850 Train loss 0.18 ACC 0.59375 on epoch=424
06/19/2022 19:18:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.16 on epoch=429
06/19/2022 19:18:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.17 on epoch=434
06/19/2022 19:18:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=439
06/19/2022 19:18:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=444
06/19/2022 19:18:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=449
06/19/2022 19:18:40 - INFO - __main__ - Global step 900 Train loss 0.15 ACC 0.59375 on epoch=449
06/19/2022 19:18:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=454
06/19/2022 19:18:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=459
06/19/2022 19:18:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=464
06/19/2022 19:18:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=469
06/19/2022 19:18:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=474
06/19/2022 19:18:48 - INFO - __main__ - Global step 950 Train loss 0.14 ACC 0.5625 on epoch=474
06/19/2022 19:18:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=479
06/19/2022 19:18:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=484
06/19/2022 19:18:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=489
06/19/2022 19:18:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=494
06/19/2022 19:18:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
06/19/2022 19:18:56 - INFO - __main__ - Global step 1000 Train loss 0.10 ACC 0.5625 on epoch=499
06/19/2022 19:18:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=504
06/19/2022 19:18:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=509
06/19/2022 19:19:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=514
06/19/2022 19:19:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=519
06/19/2022 19:19:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
06/19/2022 19:19:03 - INFO - __main__ - Global step 1050 Train loss 0.07 ACC 0.59375 on epoch=524
06/19/2022 19:19:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=529
06/19/2022 19:19:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=534
06/19/2022 19:19:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
06/19/2022 19:19:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=544
06/19/2022 19:19:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=549
06/19/2022 19:19:11 - INFO - __main__ - Global step 1100 Train loss 0.06 ACC 0.53125 on epoch=549
06/19/2022 19:19:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
06/19/2022 19:19:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=559
06/19/2022 19:19:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
06/19/2022 19:19:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
06/19/2022 19:19:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
06/19/2022 19:19:19 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.5 on epoch=574
06/19/2022 19:19:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/19/2022 19:19:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
06/19/2022 19:19:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
06/19/2022 19:19:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
06/19/2022 19:19:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
06/19/2022 19:19:26 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.4375 on epoch=599
06/19/2022 19:19:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
06/19/2022 19:19:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=609
06/19/2022 19:19:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
06/19/2022 19:19:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/19/2022 19:19:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=624
06/19/2022 19:19:33 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.21875 on epoch=624
06/19/2022 19:19:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=629
06/19/2022 19:19:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
06/19/2022 19:19:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/19/2022 19:19:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
06/19/2022 19:19:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
06/19/2022 19:19:40 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.1875 on epoch=649
06/19/2022 19:19:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/19/2022 19:19:43 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/19/2022 19:19:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/19/2022 19:19:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/19/2022 19:19:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/19/2022 19:19:48 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.5625 on epoch=674
06/19/2022 19:19:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/19/2022 19:19:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
06/19/2022 19:19:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/19/2022 19:19:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
06/19/2022 19:19:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/19/2022 19:19:55 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.3125 on epoch=699
06/19/2022 19:19:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=704
06/19/2022 19:19:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/19/2022 19:19:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/19/2022 19:20:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/19/2022 19:20:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
06/19/2022 19:20:02 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.375 on epoch=724
06/19/2022 19:20:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/19/2022 19:20:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
06/19/2022 19:20:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/19/2022 19:20:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/19/2022 19:20:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
06/19/2022 19:20:09 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.375 on epoch=749
06/19/2022 19:20:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/19/2022 19:20:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/19/2022 19:20:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/19/2022 19:20:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
06/19/2022 19:20:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/19/2022 19:20:16 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.46875 on epoch=774
06/19/2022 19:20:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/19/2022 19:20:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/19/2022 19:20:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=789
06/19/2022 19:20:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/19/2022 19:20:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/19/2022 19:20:22 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.4375 on epoch=799
06/19/2022 19:20:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/19/2022 19:20:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/19/2022 19:20:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/19/2022 19:20:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/19/2022 19:20:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
06/19/2022 19:20:29 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.46875 on epoch=824
06/19/2022 19:20:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/19/2022 19:20:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/19/2022 19:20:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/19/2022 19:20:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/19/2022 19:20:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/19/2022 19:20:36 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.5 on epoch=849
06/19/2022 19:20:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/19/2022 19:20:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/19/2022 19:20:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
06/19/2022 19:20:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/19/2022 19:20:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/19/2022 19:20:43 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.5 on epoch=874
06/19/2022 19:20:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/19/2022 19:20:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/19/2022 19:20:46 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/19/2022 19:20:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/19/2022 19:20:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/19/2022 19:20:50 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.40625 on epoch=899
06/19/2022 19:20:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/19/2022 19:20:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/19/2022 19:20:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/19/2022 19:20:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/19/2022 19:20:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/19/2022 19:20:57 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.40625 on epoch=924
06/19/2022 19:20:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/19/2022 19:20:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/19/2022 19:21:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/19/2022 19:21:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/19/2022 19:21:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/19/2022 19:21:03 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.53125 on epoch=949
06/19/2022 19:21:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/19/2022 19:21:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/19/2022 19:21:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/19/2022 19:21:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/19/2022 19:21:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/19/2022 19:21:10 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.5 on epoch=974
06/19/2022 19:21:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/19/2022 19:21:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/19/2022 19:21:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/19/2022 19:21:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/19/2022 19:21:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/19/2022 19:21:17 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.40625 on epoch=999
06/19/2022 19:21:17 - INFO - __main__ - save last model!
06/19/2022 19:21:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:21:17 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:21:17 - INFO - __main__ - Printing 3 examples
06/19/2022 19:21:17 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:21:17 - INFO - __main__ - ['equivalent']
06/19/2022 19:21:17 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:21:17 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:21:17 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:21:17 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:21:17 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:21:17 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:21:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:21:18 - INFO - __main__ - Printing 3 examples
06/19/2022 19:21:18 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 19:21:18 - INFO - __main__ - ['equivalent']
06/19/2022 19:21:18 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 19:21:18 - INFO - __main__ - ['equivalent']
06/19/2022 19:21:18 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 19:21:18 - INFO - __main__ - ['equivalent']
06/19/2022 19:21:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:21:18 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:21:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:21:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:21:18 - INFO - __main__ - Printing 3 examples
06/19/2022 19:21:18 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 19:21:18 - INFO - __main__ - ['equivalent']
06/19/2022 19:21:18 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 19:21:18 - INFO - __main__ - ['equivalent']
06/19/2022 19:21:18 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 19:21:18 - INFO - __main__ - ['equivalent']
06/19/2022 19:21:18 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:21:18 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:21:18 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:21:18 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:21:23 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:21:23 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(5086, 201716), (2701, 165860), (7228, 161576), (4782, 209172), (4369, 165583), (3567, 290752), (2487, 298264), (1475, 592201), (361, 212358), (634, 1165089), (5294, 199801), (3258, 211425), (3345, 310573), (2928, 338683), (5307, 158689), (382, 1423765), (3907, 265870), (3835, 184385), (797, 1085506), (3485, 303196), (5169, 202438), (3273, 207524), (428, 2074871), (460, 1411651), (4014, 254677), (3186, 326052), (838, 1056754), (983, 921089), (5130, 157985), (5461, 184577), (4375, 243717), (5071, 202122), (4009, 161333), (5894, 169006), (456, 1871965), (5095, 208743), (6224, 163528), (2005, 485196), (3547, 292304), (5140, 155426), (4241, 196978), (5528, 186850), (1487, 592557), (626, 1288292), (4737, 208385), (1009, 834880), (2773, 272911), (888, 977080), (5795, 185326), (1223, 524358), (455, 1972093), (2755, 373233), (3876, 213357), (4864, 212633), (5268, 179444), (4511, 179764), (6279, 154104), (1704, 569790), (1063, 258418), (4072, 251271), (3033, 329937), (4105, 241736), (1625, 230019), (2899, 278147), (6381, 157040), (556, 1506310), (3586, 281438), (671, 1208120), (2534, 284947), (992, 883589), (5961, 167603), (4307, 242447), (3022, 328805), (1708, 367245), (6246, 155887), (1616, 595713), (2271, 282308), (837, 1006426), (4624, 160645), (1513, 633559), (3097, 267345), (2518, 303433), (4263, 238773), (1601, 685019), (2558, 157757), (715, 971516), (3058, 304132), (4066, 167278), (4131, 248468), (3487, 286058), (1357, 690767), (5655, 178526), (1020, 892057), (3847, 221046), (5688, 184003), (2744, 259869), (122, 4321801), (2216, 259152), (3116, 304867)]
06/19/2022 19:21:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:21:24 - INFO - __main__ - Starting training!
06/19/2022 19:21:26 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_42_0.3_8_predictions.txt
06/19/2022 19:21:26 - INFO - __main__ - ACC on test data: 0.2941
06/19/2022 19:21:26 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.3, bsz=8, dev_performance=0.625, test_performance=0.29411764705882354
06/19/2022 19:21:26 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.2, bsz=8 ...
06/19/2022 19:21:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:21:27 - INFO - __main__ - Printing 3 examples
06/19/2022 19:21:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 19:21:27 - INFO - __main__ - ['equivalent']
06/19/2022 19:21:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 19:21:27 - INFO - __main__ - ['equivalent']
06/19/2022 19:21:27 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 19:21:27 - INFO - __main__ - ['equivalent']
06/19/2022 19:21:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:21:27 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:21:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:21:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:21:27 - INFO - __main__ - Printing 3 examples
06/19/2022 19:21:27 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 19:21:27 - INFO - __main__ - ['equivalent']
06/19/2022 19:21:27 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 19:21:27 - INFO - __main__ - ['equivalent']
06/19/2022 19:21:27 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 19:21:27 - INFO - __main__ - ['equivalent']
06/19/2022 19:21:27 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:21:27 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:21:27 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:21:33 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:21:33 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(5086, 201716), (2701, 165860), (7228, 161576), (4782, 209172), (4369, 165583), (3567, 290752), (2487, 298264), (1475, 592201), (361, 212358), (634, 1165089), (5294, 199801), (3258, 211425), (3345, 310573), (2928, 338683), (5307, 158689), (382, 1423765), (3907, 265870), (3835, 184385), (797, 1085506), (3485, 303196), (5169, 202438), (3273, 207524), (428, 2074871), (460, 1411651), (4014, 254677), (3186, 326052), (838, 1056754), (983, 921089), (5130, 157985), (5461, 184577), (4375, 243717), (5071, 202122), (4009, 161333), (5894, 169006), (456, 1871965), (5095, 208743), (6224, 163528), (2005, 485196), (3547, 292304), (5140, 155426), (4241, 196978), (5528, 186850), (1487, 592557), (626, 1288292), (4737, 208385), (1009, 834880), (2773, 272911), (888, 977080), (5795, 185326), (1223, 524358), (455, 1972093), (2755, 373233), (3876, 213357), (4864, 212633), (5268, 179444), (4511, 179764), (6279, 154104), (1704, 569790), (1063, 258418), (4072, 251271), (3033, 329937), (4105, 241736), (1625, 230019), (2899, 278147), (6381, 157040), (556, 1506310), (3586, 281438), (671, 1208120), (2534, 284947), (992, 883589), (5961, 167603), (4307, 242447), (3022, 328805), (1708, 367245), (6246, 155887), (1616, 595713), (2271, 282308), (837, 1006426), (4624, 160645), (1513, 633559), (3097, 267345), (2518, 303433), (4263, 238773), (1601, 685019), (2558, 157757), (715, 971516), (3058, 304132), (4066, 167278), (4131, 248468), (3487, 286058), (1357, 690767), (5655, 178526), (1020, 892057), (3847, 221046), (5688, 184003), (2744, 259869), (122, 4321801), (2216, 259152), (3116, 304867)]
06/19/2022 19:21:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:21:33 - INFO - __main__ - Starting training!
06/19/2022 19:21:35 - INFO - __main__ - Step 10 Global step 10 Train loss 6.67 on epoch=4
06/19/2022 19:21:36 - INFO - __main__ - Step 20 Global step 20 Train loss 4.39 on epoch=9
06/19/2022 19:21:37 - INFO - __main__ - Step 30 Global step 30 Train loss 3.05 on epoch=14
06/19/2022 19:21:38 - INFO - __main__ - Step 40 Global step 40 Train loss 2.02 on epoch=19
06/19/2022 19:21:40 - INFO - __main__ - Step 50 Global step 50 Train loss 1.41 on epoch=24
06/19/2022 19:21:40 - INFO - __main__ - Global step 50 Train loss 3.51 ACC 0.5 on epoch=24
06/19/2022 19:21:40 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 19:21:41 - INFO - __main__ - Step 60 Global step 60 Train loss 1.02 on epoch=29
06/19/2022 19:21:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.75 on epoch=34
06/19/2022 19:21:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.68 on epoch=39
06/19/2022 19:21:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.65 on epoch=44
06/19/2022 19:21:46 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=49
06/19/2022 19:21:47 - INFO - __main__ - Global step 100 Train loss 0.71 ACC 0.5 on epoch=49
06/19/2022 19:21:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=54
06/19/2022 19:21:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=59
06/19/2022 19:21:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.41 on epoch=64
06/19/2022 19:21:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
06/19/2022 19:21:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=74
06/19/2022 19:21:53 - INFO - __main__ - Global step 150 Train loss 0.45 ACC 0.5 on epoch=74
06/19/2022 19:21:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=79
06/19/2022 19:21:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
06/19/2022 19:21:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.34 on epoch=89
06/19/2022 19:21:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.36 on epoch=94
06/19/2022 19:21:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.31 on epoch=99
06/19/2022 19:22:00 - INFO - __main__ - Global step 200 Train loss 0.34 ACC 0.5 on epoch=99
06/19/2022 19:22:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=104
06/19/2022 19:22:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
06/19/2022 19:22:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=114
06/19/2022 19:22:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=119
06/19/2022 19:22:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=124
06/19/2022 19:22:06 - INFO - __main__ - Global step 250 Train loss 0.36 ACC 0.5 on epoch=124
06/19/2022 19:22:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=129
06/19/2022 19:22:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.31 on epoch=134
06/19/2022 19:22:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=139
06/19/2022 19:22:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=144
06/19/2022 19:22:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.35 on epoch=149
06/19/2022 19:22:13 - INFO - __main__ - Global step 300 Train loss 0.33 ACC 0.5 on epoch=149
06/19/2022 19:22:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=154
06/19/2022 19:22:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
06/19/2022 19:22:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=164
06/19/2022 19:22:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=169
06/19/2022 19:22:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=174
06/19/2022 19:22:19 - INFO - __main__ - Global step 350 Train loss 0.30 ACC 0.5 on epoch=174
06/19/2022 19:22:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
06/19/2022 19:22:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=184
06/19/2022 19:22:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=189
06/19/2022 19:22:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=194
06/19/2022 19:22:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=199
06/19/2022 19:22:26 - INFO - __main__ - Global step 400 Train loss 0.32 ACC 0.4375 on epoch=199
06/19/2022 19:22:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.29 on epoch=204
06/19/2022 19:22:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=209
06/19/2022 19:22:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=214
06/19/2022 19:22:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
06/19/2022 19:22:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=224
06/19/2022 19:22:32 - INFO - __main__ - Global step 450 Train loss 0.28 ACC 0.46875 on epoch=224
06/19/2022 19:22:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.30 on epoch=229
06/19/2022 19:22:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=234
06/19/2022 19:22:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=239
06/19/2022 19:22:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.33 on epoch=244
06/19/2022 19:22:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=249
06/19/2022 19:22:39 - INFO - __main__ - Global step 500 Train loss 0.31 ACC 0.5 on epoch=249
06/19/2022 19:22:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=254
06/19/2022 19:22:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=259
06/19/2022 19:22:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.35 on epoch=264
06/19/2022 19:22:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=269
06/19/2022 19:22:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=274
06/19/2022 19:22:46 - INFO - __main__ - Global step 550 Train loss 0.31 ACC 0.5 on epoch=274
06/19/2022 19:22:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=279
06/19/2022 19:22:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.29 on epoch=284
06/19/2022 19:22:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=289
06/19/2022 19:22:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
06/19/2022 19:22:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=299
06/19/2022 19:22:52 - INFO - __main__ - Global step 600 Train loss 0.29 ACC 0.5 on epoch=299
06/19/2022 19:22:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=304
06/19/2022 19:22:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=309
06/19/2022 19:22:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.33 on epoch=314
06/19/2022 19:22:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.28 on epoch=319
06/19/2022 19:22:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=324
06/19/2022 19:22:59 - INFO - __main__ - Global step 650 Train loss 0.28 ACC 0.5 on epoch=324
06/19/2022 19:23:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=329
06/19/2022 19:23:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=334
06/19/2022 19:23:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.29 on epoch=339
06/19/2022 19:23:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=344
06/19/2022 19:23:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=349
06/19/2022 19:23:05 - INFO - __main__ - Global step 700 Train loss 0.27 ACC 0.5 on epoch=349
06/19/2022 19:23:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=354
06/19/2022 19:23:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=359
06/19/2022 19:23:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=364
06/19/2022 19:23:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=369
06/19/2022 19:23:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=374
06/19/2022 19:23:12 - INFO - __main__ - Global step 750 Train loss 0.26 ACC 0.5 on epoch=374
06/19/2022 19:23:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=379
06/19/2022 19:23:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=384
06/19/2022 19:23:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=389
06/19/2022 19:23:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=394
06/19/2022 19:23:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=399
06/19/2022 19:23:18 - INFO - __main__ - Global step 800 Train loss 0.25 ACC 0.5 on epoch=399
06/19/2022 19:23:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=404
06/19/2022 19:23:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=409
06/19/2022 19:23:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=414
06/19/2022 19:23:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=419
06/19/2022 19:23:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=424
06/19/2022 19:23:25 - INFO - __main__ - Global step 850 Train loss 0.25 ACC 0.46875 on epoch=424
06/19/2022 19:23:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=429
06/19/2022 19:23:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=434
06/19/2022 19:23:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=439
06/19/2022 19:23:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=444
06/19/2022 19:23:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=449
06/19/2022 19:23:31 - INFO - __main__ - Global step 900 Train loss 0.25 ACC 0.5 on epoch=449
06/19/2022 19:23:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=454
06/19/2022 19:23:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=459
06/19/2022 19:23:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.27 on epoch=464
06/19/2022 19:23:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=469
06/19/2022 19:23:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=474
06/19/2022 19:23:38 - INFO - __main__ - Global step 950 Train loss 0.25 ACC 0.5 on epoch=474
06/19/2022 19:23:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=479
06/19/2022 19:23:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=484
06/19/2022 19:23:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=489
06/19/2022 19:23:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=494
06/19/2022 19:23:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=499
06/19/2022 19:23:44 - INFO - __main__ - Global step 1000 Train loss 0.23 ACC 0.46875 on epoch=499
06/19/2022 19:23:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=504
06/19/2022 19:23:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=509
06/19/2022 19:23:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=514
06/19/2022 19:23:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=519
06/19/2022 19:23:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=524
06/19/2022 19:23:51 - INFO - __main__ - Global step 1050 Train loss 0.23 ACC 0.5 on epoch=524
06/19/2022 19:23:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=529
06/19/2022 19:23:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=534
06/19/2022 19:23:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=539
06/19/2022 19:23:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=544
06/19/2022 19:23:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=549
06/19/2022 19:23:58 - INFO - __main__ - Global step 1100 Train loss 0.22 ACC 0.4375 on epoch=549
06/19/2022 19:23:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=554
06/19/2022 19:24:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=559
06/19/2022 19:24:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=564
06/19/2022 19:24:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=569
06/19/2022 19:24:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=574
06/19/2022 19:24:04 - INFO - __main__ - Global step 1150 Train loss 0.22 ACC 0.4375 on epoch=574
06/19/2022 19:24:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=579
06/19/2022 19:24:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=584
06/19/2022 19:24:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=589
06/19/2022 19:24:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=594
06/19/2022 19:24:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.15 on epoch=599
06/19/2022 19:24:11 - INFO - __main__ - Global step 1200 Train loss 0.17 ACC 0.46875 on epoch=599
06/19/2022 19:24:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=604
06/19/2022 19:24:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=609
06/19/2022 19:24:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=614
06/19/2022 19:24:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=619
06/19/2022 19:24:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=624
06/19/2022 19:24:17 - INFO - __main__ - Global step 1250 Train loss 0.23 ACC 0.53125 on epoch=624
06/19/2022 19:24:17 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=624, global_step=1250
06/19/2022 19:24:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=629
06/19/2022 19:24:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=634
06/19/2022 19:24:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=639
06/19/2022 19:24:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=644
06/19/2022 19:24:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=649
06/19/2022 19:24:24 - INFO - __main__ - Global step 1300 Train loss 0.18 ACC 0.5 on epoch=649
06/19/2022 19:24:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=654
06/19/2022 19:24:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=659
06/19/2022 19:24:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=664
06/19/2022 19:24:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=669
06/19/2022 19:24:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=674
06/19/2022 19:24:30 - INFO - __main__ - Global step 1350 Train loss 0.18 ACC 0.4375 on epoch=674
06/19/2022 19:24:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.21 on epoch=679
06/19/2022 19:24:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=684
06/19/2022 19:24:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=689
06/19/2022 19:24:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=694
06/19/2022 19:24:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=699
06/19/2022 19:24:37 - INFO - __main__ - Global step 1400 Train loss 0.17 ACC 0.46875 on epoch=699
06/19/2022 19:24:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=704
06/19/2022 19:24:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=709
06/19/2022 19:24:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=714
06/19/2022 19:24:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=719
06/19/2022 19:24:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=724
06/19/2022 19:24:44 - INFO - __main__ - Global step 1450 Train loss 0.15 ACC 0.46875 on epoch=724
06/19/2022 19:24:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=729
06/19/2022 19:24:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.12 on epoch=734
06/19/2022 19:24:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=739
06/19/2022 19:24:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=744
06/19/2022 19:24:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=749
06/19/2022 19:24:50 - INFO - __main__ - Global step 1500 Train loss 0.14 ACC 0.5625 on epoch=749
06/19/2022 19:24:50 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=749, global_step=1500
06/19/2022 19:24:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=754
06/19/2022 19:24:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=759
06/19/2022 19:24:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=764
06/19/2022 19:24:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=769
06/19/2022 19:24:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=774
06/19/2022 19:24:57 - INFO - __main__ - Global step 1550 Train loss 0.13 ACC 0.625 on epoch=774
06/19/2022 19:24:57 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.625 on epoch=774, global_step=1550
06/19/2022 19:24:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=779
06/19/2022 19:24:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=784
06/19/2022 19:25:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=789
06/19/2022 19:25:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=794
06/19/2022 19:25:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=799
06/19/2022 19:25:04 - INFO - __main__ - Global step 1600 Train loss 0.08 ACC 0.5625 on epoch=799
06/19/2022 19:25:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=804
06/19/2022 19:25:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=809
06/19/2022 19:25:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=814
06/19/2022 19:25:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=819
06/19/2022 19:25:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=824
06/19/2022 19:25:10 - INFO - __main__ - Global step 1650 Train loss 0.11 ACC 0.59375 on epoch=824
06/19/2022 19:25:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=829
06/19/2022 19:25:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=834
06/19/2022 19:25:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=839
06/19/2022 19:25:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=844
06/19/2022 19:25:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=849
06/19/2022 19:25:17 - INFO - __main__ - Global step 1700 Train loss 0.07 ACC 0.4375 on epoch=849
06/19/2022 19:25:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=854
06/19/2022 19:25:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=859
06/19/2022 19:25:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=864
06/19/2022 19:25:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=869
06/19/2022 19:25:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=874
06/19/2022 19:25:23 - INFO - __main__ - Global step 1750 Train loss 0.07 ACC 0.59375 on epoch=874
06/19/2022 19:25:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=879
06/19/2022 19:25:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=884
06/19/2022 19:25:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=889
06/19/2022 19:25:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=894
06/19/2022 19:25:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
06/19/2022 19:25:30 - INFO - __main__ - Global step 1800 Train loss 0.07 ACC 0.5625 on epoch=899
06/19/2022 19:25:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=904
06/19/2022 19:25:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=909
06/19/2022 19:25:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=914
06/19/2022 19:25:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=919
06/19/2022 19:25:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=924
06/19/2022 19:25:37 - INFO - __main__ - Global step 1850 Train loss 0.07 ACC 0.625 on epoch=924
06/19/2022 19:25:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=929
06/19/2022 19:25:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
06/19/2022 19:25:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
06/19/2022 19:25:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
06/19/2022 19:25:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
06/19/2022 19:25:43 - INFO - __main__ - Global step 1900 Train loss 0.04 ACC 0.65625 on epoch=949
06/19/2022 19:25:43 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=949, global_step=1900
06/19/2022 19:25:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
06/19/2022 19:25:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=959
06/19/2022 19:25:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
06/19/2022 19:25:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
06/19/2022 19:25:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=974
06/19/2022 19:25:50 - INFO - __main__ - Global step 1950 Train loss 0.04 ACC 0.65625 on epoch=974
06/19/2022 19:25:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=979
06/19/2022 19:25:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=984
06/19/2022 19:25:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=989
06/19/2022 19:25:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/19/2022 19:25:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
06/19/2022 19:25:57 - INFO - __main__ - Global step 2000 Train loss 0.03 ACC 0.625 on epoch=999
06/19/2022 19:25:57 - INFO - __main__ - save last model!
06/19/2022 19:25:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:25:57 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:25:57 - INFO - __main__ - Printing 3 examples
06/19/2022 19:25:57 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:25:57 - INFO - __main__ - ['equivalent']
06/19/2022 19:25:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:25:57 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:25:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:25:57 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:25:57 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:25:57 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:25:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:25:57 - INFO - __main__ - Printing 3 examples
06/19/2022 19:25:57 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 19:25:57 - INFO - __main__ - ['equivalent']
06/19/2022 19:25:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 19:25:57 - INFO - __main__ - ['equivalent']
06/19/2022 19:25:57 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 19:25:57 - INFO - __main__ - ['equivalent']
06/19/2022 19:25:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:25:57 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:25:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:25:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:25:57 - INFO - __main__ - Printing 3 examples
06/19/2022 19:25:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 19:25:57 - INFO - __main__ - ['equivalent']
06/19/2022 19:25:57 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 19:25:57 - INFO - __main__ - ['equivalent']
06/19/2022 19:25:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 19:25:57 - INFO - __main__ - ['equivalent']
06/19/2022 19:25:57 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:25:57 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:25:57 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:25:57 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:26:03 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:26:03 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(1865, 505831), (2321, 427255), (3814, 275126), (3534, 191669), (2081, 476380), (1981, 405777), (50, 649230), (1702, 582322), (711, 1051315), (3217, 316619), (1084, 848613), (3574, 184323), (6060, 173161), (5272, 178396), (435, 2072263), (3040, 279538), (526, 676575), (6578, 157228), (2318, 248450), (1722, 583639), (1355, 559231), (1871, 504548), (1783, 548358), (4188, 213409), (502, 1761063), (4879, 229125), (3904, 258285), (2677, 390904), (5781, 210606), (3315, 327974), (4782, 209172), (2493, 394839), (5849, 175119), (2287, 388098), (3134, 304431), (5281, 199800), (2250, 472469), (4245, 255196), (4542, 163807), (3537, 211928), (5169, 202438), (3127, 331271), (3826, 245516), (5640, 186094), (5055, 178040), (836, 224036), (1326, 560217), (1626, 458443), (6468, 155450), (968, 645839), (2507, 369601), (585, 1508230), (5330, 194338), (4073, 252987), (6168, 168840), (1087, 843077), (5017, 181544), (1244, 709702), (3060, 327552), (915, 980631), (1749, 563947), (575, 340830), (5285, 209039), (6026, 182603), (727, 605486), (3597, 213529), (1246, 907731), (2471, 412585), (4796, 207811), (3545, 267617), (315, 2942959), (4657, 226628), (582, 1484200), (4326, 236656), (6178, 186518), (4226, 250683), (2416, 392857), (4193, 242583), (885, 1006302), (4093, 250163), (2496, 408535), (337, 2812930), (3635, 295690), (1096, 857963), (3387, 280917), (1568, 614370), (1485, 648438), (429, 2065410), (3168, 326177), (2226, 198030), (1406, 645374), (4888, 209272), (10, 16409736), (791, 1024488), (223, 4176310), (2790, 361504), (1575, 424541), (1514, 597937), (6198, 170564)]
06/19/2022 19:26:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:26:03 - INFO - __main__ - Starting training!
06/19/2022 19:26:05 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_42_0.2_8_predictions.txt
06/19/2022 19:26:05 - INFO - __main__ - ACC on test data: 0.5809
06/19/2022 19:26:05 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.2, bsz=8, dev_performance=0.65625, test_performance=0.5808823529411765
06/19/2022 19:26:05 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.5, bsz=8 ...
06/19/2022 19:26:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:26:06 - INFO - __main__ - Printing 3 examples
06/19/2022 19:26:06 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 19:26:06 - INFO - __main__ - ['equivalent']
06/19/2022 19:26:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 19:26:06 - INFO - __main__ - ['equivalent']
06/19/2022 19:26:06 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 19:26:06 - INFO - __main__ - ['equivalent']
06/19/2022 19:26:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:26:06 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:26:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:26:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:26:06 - INFO - __main__ - Printing 3 examples
06/19/2022 19:26:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 19:26:06 - INFO - __main__ - ['equivalent']
06/19/2022 19:26:06 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 19:26:06 - INFO - __main__ - ['equivalent']
06/19/2022 19:26:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 19:26:06 - INFO - __main__ - ['equivalent']
06/19/2022 19:26:06 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:26:06 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:26:06 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:26:12 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:26:12 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(1865, 505831), (2321, 427255), (3814, 275126), (3534, 191669), (2081, 476380), (1981, 405777), (50, 649230), (1702, 582322), (711, 1051315), (3217, 316619), (1084, 848613), (3574, 184323), (6060, 173161), (5272, 178396), (435, 2072263), (3040, 279538), (526, 676575), (6578, 157228), (2318, 248450), (1722, 583639), (1355, 559231), (1871, 504548), (1783, 548358), (4188, 213409), (502, 1761063), (4879, 229125), (3904, 258285), (2677, 390904), (5781, 210606), (3315, 327974), (4782, 209172), (2493, 394839), (5849, 175119), (2287, 388098), (3134, 304431), (5281, 199800), (2250, 472469), (4245, 255196), (4542, 163807), (3537, 211928), (5169, 202438), (3127, 331271), (3826, 245516), (5640, 186094), (5055, 178040), (836, 224036), (1326, 560217), (1626, 458443), (6468, 155450), (968, 645839), (2507, 369601), (585, 1508230), (5330, 194338), (4073, 252987), (6168, 168840), (1087, 843077), (5017, 181544), (1244, 709702), (3060, 327552), (915, 980631), (1749, 563947), (575, 340830), (5285, 209039), (6026, 182603), (727, 605486), (3597, 213529), (1246, 907731), (2471, 412585), (4796, 207811), (3545, 267617), (315, 2942959), (4657, 226628), (582, 1484200), (4326, 236656), (6178, 186518), (4226, 250683), (2416, 392857), (4193, 242583), (885, 1006302), (4093, 250163), (2496, 408535), (337, 2812930), (3635, 295690), (1096, 857963), (3387, 280917), (1568, 614370), (1485, 648438), (429, 2065410), (3168, 326177), (2226, 198030), (1406, 645374), (4888, 209272), (10, 16409736), (791, 1024488), (223, 4176310), (2790, 361504), (1575, 424541), (1514, 597937), (6198, 170564)]
06/19/2022 19:26:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:26:12 - INFO - __main__ - Starting training!
06/19/2022 19:26:14 - INFO - __main__ - Step 10 Global step 10 Train loss 5.47 on epoch=4
06/19/2022 19:26:15 - INFO - __main__ - Step 20 Global step 20 Train loss 1.56 on epoch=9
06/19/2022 19:26:16 - INFO - __main__ - Step 30 Global step 30 Train loss 0.66 on epoch=14
06/19/2022 19:26:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=19
06/19/2022 19:26:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=24
06/19/2022 19:26:19 - INFO - __main__ - Global step 50 Train loss 1.75 ACC 0.5 on epoch=24
06/19/2022 19:26:19 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 19:26:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=29
06/19/2022 19:26:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=34
06/19/2022 19:26:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.38 on epoch=39
06/19/2022 19:26:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.33 on epoch=44
06/19/2022 19:26:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.35 on epoch=49
06/19/2022 19:26:26 - INFO - __main__ - Global step 100 Train loss 0.38 ACC 0.5 on epoch=49
06/19/2022 19:26:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.28 on epoch=54
06/19/2022 19:26:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=59
06/19/2022 19:26:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.31 on epoch=64
06/19/2022 19:26:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
06/19/2022 19:26:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=74
06/19/2022 19:26:33 - INFO - __main__ - Global step 150 Train loss 0.28 ACC 0.5 on epoch=74
06/19/2022 19:26:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.29 on epoch=79
06/19/2022 19:26:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.26 on epoch=84
06/19/2022 19:26:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=89
06/19/2022 19:26:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=94
06/19/2022 19:26:39 - INFO - __main__ - Step 200 Global step 200 Train loss 2.13 on epoch=99
06/19/2022 19:26:40 - INFO - __main__ - Global step 200 Train loss 0.67 ACC 0.28125 on epoch=99
06/19/2022 19:26:41 - INFO - __main__ - Step 210 Global step 210 Train loss 3.27 on epoch=104
06/19/2022 19:26:42 - INFO - __main__ - Step 220 Global step 220 Train loss 3.16 on epoch=109
06/19/2022 19:26:44 - INFO - __main__ - Step 230 Global step 230 Train loss 4.54 on epoch=114
06/19/2022 19:26:45 - INFO - __main__ - Step 240 Global step 240 Train loss 1.82 on epoch=119
06/19/2022 19:26:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.60 on epoch=124
06/19/2022 19:26:47 - INFO - __main__ - Global step 250 Train loss 2.68 ACC 0.5 on epoch=124
06/19/2022 19:26:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=129
06/19/2022 19:26:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.34 on epoch=134
06/19/2022 19:26:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=139
06/19/2022 19:26:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=144
06/19/2022 19:26:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
06/19/2022 19:26:54 - INFO - __main__ - Global step 300 Train loss 0.33 ACC 0.5 on epoch=149
06/19/2022 19:26:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
06/19/2022 19:26:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
06/19/2022 19:26:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
06/19/2022 19:26:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
06/19/2022 19:27:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=174
06/19/2022 19:27:00 - INFO - __main__ - Global step 350 Train loss 0.28 ACC 0.5 on epoch=174
06/19/2022 19:27:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
06/19/2022 19:27:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=184
06/19/2022 19:27:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=189
06/19/2022 19:27:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=194
06/19/2022 19:27:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=199
06/19/2022 19:27:07 - INFO - __main__ - Global step 400 Train loss 0.27 ACC 0.5 on epoch=199
06/19/2022 19:27:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
06/19/2022 19:27:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
06/19/2022 19:27:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=214
06/19/2022 19:27:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=219
06/19/2022 19:27:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
06/19/2022 19:27:14 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.5 on epoch=224
06/19/2022 19:27:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=229
06/19/2022 19:27:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.30 on epoch=234
06/19/2022 19:27:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
06/19/2022 19:27:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=244
06/19/2022 19:27:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=249
06/19/2022 19:27:20 - INFO - __main__ - Global step 500 Train loss 0.27 ACC 0.5 on epoch=249
06/19/2022 19:27:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=254
06/19/2022 19:27:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
06/19/2022 19:27:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
06/19/2022 19:27:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=269
06/19/2022 19:27:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=274
06/19/2022 19:27:27 - INFO - __main__ - Global step 550 Train loss 0.26 ACC 0.5 on epoch=274
06/19/2022 19:27:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.28 on epoch=279
06/19/2022 19:27:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=284
06/19/2022 19:27:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=289
06/19/2022 19:27:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
06/19/2022 19:27:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=299
06/19/2022 19:27:34 - INFO - __main__ - Global step 600 Train loss 0.27 ACC 0.5 on epoch=299
06/19/2022 19:27:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=304
06/19/2022 19:27:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.26 on epoch=309
06/19/2022 19:27:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=314
06/19/2022 19:27:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=319
06/19/2022 19:27:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=324
06/19/2022 19:27:41 - INFO - __main__ - Global step 650 Train loss 0.25 ACC 0.46875 on epoch=324
06/19/2022 19:27:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=329
06/19/2022 19:27:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.27 on epoch=334
06/19/2022 19:27:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=339
06/19/2022 19:27:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=344
06/19/2022 19:27:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.26 on epoch=349
06/19/2022 19:27:47 - INFO - __main__ - Global step 700 Train loss 0.26 ACC 0.5 on epoch=349
06/19/2022 19:27:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=354
06/19/2022 19:27:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.29 on epoch=359
06/19/2022 19:27:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=364
06/19/2022 19:27:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=369
06/19/2022 19:27:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=374
06/19/2022 19:27:54 - INFO - __main__ - Global step 750 Train loss 0.26 ACC 0.5 on epoch=374
06/19/2022 19:27:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=379
06/19/2022 19:27:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=384
06/19/2022 19:27:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
06/19/2022 19:27:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=394
06/19/2022 19:28:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=399
06/19/2022 19:28:01 - INFO - __main__ - Global step 800 Train loss 0.25 ACC 0.5 on epoch=399
06/19/2022 19:28:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=404
06/19/2022 19:28:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=409
06/19/2022 19:28:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=414
06/19/2022 19:28:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=419
06/19/2022 19:28:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=424
06/19/2022 19:28:08 - INFO - __main__ - Global step 850 Train loss 0.25 ACC 0.46875 on epoch=424
06/19/2022 19:28:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=429
06/19/2022 19:28:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=434
06/19/2022 19:28:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=439
06/19/2022 19:28:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=444
06/19/2022 19:28:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=449
06/19/2022 19:28:15 - INFO - __main__ - Global step 900 Train loss 0.24 ACC 0.40625 on epoch=449
06/19/2022 19:28:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=454
06/19/2022 19:28:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=459
06/19/2022 19:28:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=464
06/19/2022 19:28:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=469
06/19/2022 19:28:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=474
06/19/2022 19:28:21 - INFO - __main__ - Global step 950 Train loss 0.24 ACC 0.5 on epoch=474
06/19/2022 19:28:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=479
06/19/2022 19:28:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=484
06/19/2022 19:28:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=489
06/19/2022 19:28:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=494
06/19/2022 19:28:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=499
06/19/2022 19:28:28 - INFO - __main__ - Global step 1000 Train loss 0.24 ACC 0.46875 on epoch=499
06/19/2022 19:28:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=504
06/19/2022 19:28:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=509
06/19/2022 19:28:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=514
06/19/2022 19:28:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=519
06/19/2022 19:28:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=524
06/19/2022 19:28:35 - INFO - __main__ - Global step 1050 Train loss 0.23 ACC 0.46875 on epoch=524
06/19/2022 19:28:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=529
06/19/2022 19:28:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=534
06/19/2022 19:28:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=539
06/19/2022 19:28:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=544
06/19/2022 19:28:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=549
06/19/2022 19:28:42 - INFO - __main__ - Global step 1100 Train loss 0.24 ACC 0.46875 on epoch=549
06/19/2022 19:28:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=554
06/19/2022 19:28:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=559
06/19/2022 19:28:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=564
06/19/2022 19:28:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.28 on epoch=569
06/19/2022 19:28:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=574
06/19/2022 19:28:48 - INFO - __main__ - Global step 1150 Train loss 0.24 ACC 0.4375 on epoch=574
06/19/2022 19:28:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=579
06/19/2022 19:28:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=584
06/19/2022 19:28:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.26 on epoch=589
06/19/2022 19:28:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=594
06/19/2022 19:28:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=599
06/19/2022 19:28:55 - INFO - __main__ - Global step 1200 Train loss 0.25 ACC 0.5 on epoch=599
06/19/2022 19:28:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=604
06/19/2022 19:28:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=609
06/19/2022 19:28:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=614
06/19/2022 19:29:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.24 on epoch=619
06/19/2022 19:29:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=624
06/19/2022 19:29:02 - INFO - __main__ - Global step 1250 Train loss 0.23 ACC 0.40625 on epoch=624
06/19/2022 19:29:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=629
06/19/2022 19:29:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=634
06/19/2022 19:29:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=639
06/19/2022 19:29:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=644
06/19/2022 19:29:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=649
06/19/2022 19:29:08 - INFO - __main__ - Global step 1300 Train loss 0.23 ACC 0.4375 on epoch=649
06/19/2022 19:29:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=654
06/19/2022 19:29:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.26 on epoch=659
06/19/2022 19:29:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=664
06/19/2022 19:29:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=669
06/19/2022 19:29:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=674
06/19/2022 19:29:15 - INFO - __main__ - Global step 1350 Train loss 0.24 ACC 0.5 on epoch=674
06/19/2022 19:29:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=679
06/19/2022 19:29:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=684
06/19/2022 19:29:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=689
06/19/2022 19:29:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=694
06/19/2022 19:29:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=699
06/19/2022 19:29:22 - INFO - __main__ - Global step 1400 Train loss 0.23 ACC 0.4375 on epoch=699
06/19/2022 19:29:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=704
06/19/2022 19:29:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=709
06/19/2022 19:29:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.22 on epoch=714
06/19/2022 19:29:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=719
06/19/2022 19:29:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=724
06/19/2022 19:29:28 - INFO - __main__ - Global step 1450 Train loss 0.22 ACC 0.46875 on epoch=724
06/19/2022 19:29:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=729
06/19/2022 19:29:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=734
06/19/2022 19:29:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=739
06/19/2022 19:29:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=744
06/19/2022 19:29:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=749
06/19/2022 19:29:35 - INFO - __main__ - Global step 1500 Train loss 0.20 ACC 0.4375 on epoch=749
06/19/2022 19:29:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=754
06/19/2022 19:29:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=759
06/19/2022 19:29:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.25 on epoch=764
06/19/2022 19:29:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.21 on epoch=769
06/19/2022 19:29:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=774
06/19/2022 19:29:42 - INFO - __main__ - Global step 1550 Train loss 0.22 ACC 0.4375 on epoch=774
06/19/2022 19:29:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=779
06/19/2022 19:29:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=784
06/19/2022 19:29:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=789
06/19/2022 19:29:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=794
06/19/2022 19:29:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=799
06/19/2022 19:29:49 - INFO - __main__ - Global step 1600 Train loss 0.22 ACC 0.46875 on epoch=799
06/19/2022 19:29:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=804
06/19/2022 19:29:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=809
06/19/2022 19:29:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=814
06/19/2022 19:29:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=819
06/19/2022 19:29:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=824
06/19/2022 19:29:56 - INFO - __main__ - Global step 1650 Train loss 0.21 ACC 0.4375 on epoch=824
06/19/2022 19:29:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=829
06/19/2022 19:29:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=834
06/19/2022 19:30:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=839
06/19/2022 19:30:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.18 on epoch=844
06/19/2022 19:30:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=849
06/19/2022 19:30:03 - INFO - __main__ - Global step 1700 Train loss 0.20 ACC 0.4375 on epoch=849
06/19/2022 19:30:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=854
06/19/2022 19:30:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=859
06/19/2022 19:30:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=864
06/19/2022 19:30:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=869
06/19/2022 19:30:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=874
06/19/2022 19:30:09 - INFO - __main__ - Global step 1750 Train loss 0.21 ACC 0.4375 on epoch=874
06/19/2022 19:30:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=879
06/19/2022 19:30:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=884
06/19/2022 19:30:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=889
06/19/2022 19:30:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=894
06/19/2022 19:30:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=899
06/19/2022 19:30:16 - INFO - __main__ - Global step 1800 Train loss 0.19 ACC 0.4375 on epoch=899
06/19/2022 19:30:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=904
06/19/2022 19:30:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=909
06/19/2022 19:30:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=914
06/19/2022 19:30:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=919
06/19/2022 19:30:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=924
06/19/2022 19:30:23 - INFO - __main__ - Global step 1850 Train loss 0.18 ACC 0.46875 on epoch=924
06/19/2022 19:30:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=929
06/19/2022 19:30:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.16 on epoch=934
06/19/2022 19:30:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=939
06/19/2022 19:30:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=944
06/19/2022 19:30:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=949
06/19/2022 19:30:29 - INFO - __main__ - Global step 1900 Train loss 0.17 ACC 0.5 on epoch=949
06/19/2022 19:30:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=954
06/19/2022 19:30:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.23 on epoch=959
06/19/2022 19:30:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=964
06/19/2022 19:30:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=969
06/19/2022 19:30:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=974
06/19/2022 19:30:36 - INFO - __main__ - Global step 1950 Train loss 0.18 ACC 0.5 on epoch=974
06/19/2022 19:30:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=979
06/19/2022 19:30:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=984
06/19/2022 19:30:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=989
06/19/2022 19:30:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=994
06/19/2022 19:30:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=999
06/19/2022 19:30:43 - INFO - __main__ - Global step 2000 Train loss 0.13 ACC 0.5 on epoch=999
06/19/2022 19:30:43 - INFO - __main__ - save last model!
06/19/2022 19:30:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:30:43 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:30:43 - INFO - __main__ - Printing 3 examples
06/19/2022 19:30:43 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:30:43 - INFO - __main__ - ['equivalent']
06/19/2022 19:30:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:30:43 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:30:43 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:30:43 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:30:43 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:30:43 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:30:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:30:43 - INFO - __main__ - Printing 3 examples
06/19/2022 19:30:43 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 19:30:43 - INFO - __main__ - ['equivalent']
06/19/2022 19:30:43 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 19:30:43 - INFO - __main__ - ['equivalent']
06/19/2022 19:30:43 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 19:30:43 - INFO - __main__ - ['equivalent']
06/19/2022 19:30:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:30:43 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:30:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:30:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:30:43 - INFO - __main__ - Printing 3 examples
06/19/2022 19:30:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 19:30:43 - INFO - __main__ - ['equivalent']
06/19/2022 19:30:43 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 19:30:43 - INFO - __main__ - ['equivalent']
06/19/2022 19:30:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 19:30:43 - INFO - __main__ - ['equivalent']
06/19/2022 19:30:43 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:30:43 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:30:43 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:30:43 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:30:49 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:30:49 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(6865, 154321), (4323, 177841), (3628, 221135), (1187, 785998), (3266, 325487), (2667, 390774), (3388, 312116), (1364, 727815), (571, 1622848), (2285, 389634), (4336, 223332), (5136, 204994), (709, 1264316), (749, 787508), (5658, 187484), (4163, 157427), (6244, 174311), (186, 4844172), (5598, 181777), (6613, 162836), (2419, 382488), (1473, 623867), (3131, 321577), (4112, 217671), (18, 50033297), (3866, 265039), (2641, 386604), (6625, 163763), (3318, 225546), (1759, 488564), (793, 959415), (6063, 179543), (3989, 260528), (740, 1183405), (5458, 178957), (6086, 171829), (3835, 184385), (3574, 184323), (5225, 169030), (231, 4022141), (2415, 239286), (5235, 199452), (391, 1765194), (4805, 242141), (2147, 250994), (2675, 364936), (89, 4855953), (3886, 262677), (766, 1186765), (3399, 271540), (2069, 384965), (1093, 832892), (7836, 160922), (2668, 283036), (1443, 666960), (530, 1727807), (4199, 265295), (6855, 171299), (2766, 278103), (776, 518408), (4999, 201462), (5382, 195164), (1669, 500832), (1281, 662981), (700, 1256519), (5242, 204577), (423, 2128181), (3834, 319344), (2066, 452267), (483, 1751244), (2118, 458638), (481, 1838055), (2262, 298634), (3476, 303459), (5835, 181763), (4568, 235140), (5164, 172215), (4917, 197668), (2992, 365452), (416, 2166899), (2027, 478074), (3511, 327136), (3946, 262708), (3616, 283413), (2983, 340155), (1306, 712364), (3450, 255182), (4858, 213667), (2883, 234030), (361, 212358), (4043, 242414), (3615, 277585), (1942, 489032), (5455, 188626), (5530, 188311), (4273, 255574), (6126, 180684), (2671, 348278), (931, 995491)]
06/19/2022 19:30:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:30:49 - INFO - __main__ - Starting training!
06/19/2022 19:30:51 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_87_0.5_8_predictions.txt
06/19/2022 19:30:51 - INFO - __main__ - ACC on test data: 0.5784
06/19/2022 19:30:51 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.5, bsz=8, dev_performance=0.5, test_performance=0.5784313725490197
06/19/2022 19:30:51 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.4, bsz=8 ...
06/19/2022 19:30:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:30:52 - INFO - __main__ - Printing 3 examples
06/19/2022 19:30:52 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 19:30:52 - INFO - __main__ - ['equivalent']
06/19/2022 19:30:52 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 19:30:52 - INFO - __main__ - ['equivalent']
06/19/2022 19:30:52 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 19:30:52 - INFO - __main__ - ['equivalent']
06/19/2022 19:30:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:30:52 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:30:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:30:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:30:52 - INFO - __main__ - Printing 3 examples
06/19/2022 19:30:52 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 19:30:52 - INFO - __main__ - ['equivalent']
06/19/2022 19:30:52 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 19:30:52 - INFO - __main__ - ['equivalent']
06/19/2022 19:30:52 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 19:30:52 - INFO - __main__ - ['equivalent']
06/19/2022 19:30:52 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:30:52 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:30:52 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:30:58 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:30:58 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(6865, 154321), (4323, 177841), (3628, 221135), (1187, 785998), (3266, 325487), (2667, 390774), (3388, 312116), (1364, 727815), (571, 1622848), (2285, 389634), (4336, 223332), (5136, 204994), (709, 1264316), (749, 787508), (5658, 187484), (4163, 157427), (6244, 174311), (186, 4844172), (5598, 181777), (6613, 162836), (2419, 382488), (1473, 623867), (3131, 321577), (4112, 217671), (18, 50033297), (3866, 265039), (2641, 386604), (6625, 163763), (3318, 225546), (1759, 488564), (793, 959415), (6063, 179543), (3989, 260528), (740, 1183405), (5458, 178957), (6086, 171829), (3835, 184385), (3574, 184323), (5225, 169030), (231, 4022141), (2415, 239286), (5235, 199452), (391, 1765194), (4805, 242141), (2147, 250994), (2675, 364936), (89, 4855953), (3886, 262677), (766, 1186765), (3399, 271540), (2069, 384965), (1093, 832892), (7836, 160922), (2668, 283036), (1443, 666960), (530, 1727807), (4199, 265295), (6855, 171299), (2766, 278103), (776, 518408), (4999, 201462), (5382, 195164), (1669, 500832), (1281, 662981), (700, 1256519), (5242, 204577), (423, 2128181), (3834, 319344), (2066, 452267), (483, 1751244), (2118, 458638), (481, 1838055), (2262, 298634), (3476, 303459), (5835, 181763), (4568, 235140), (5164, 172215), (4917, 197668), (2992, 365452), (416, 2166899), (2027, 478074), (3511, 327136), (3946, 262708), (3616, 283413), (2983, 340155), (1306, 712364), (3450, 255182), (4858, 213667), (2883, 234030), (361, 212358), (4043, 242414), (3615, 277585), (1942, 489032), (5455, 188626), (5530, 188311), (4273, 255574), (6126, 180684), (2671, 348278), (931, 995491)]
06/19/2022 19:30:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:30:58 - INFO - __main__ - Starting training!
06/19/2022 19:31:00 - INFO - __main__ - Step 10 Global step 10 Train loss 6.27 on epoch=4
06/19/2022 19:31:01 - INFO - __main__ - Step 20 Global step 20 Train loss 3.55 on epoch=9
06/19/2022 19:31:02 - INFO - __main__ - Step 30 Global step 30 Train loss 2.01 on epoch=14
06/19/2022 19:31:03 - INFO - __main__ - Step 40 Global step 40 Train loss 0.99 on epoch=19
06/19/2022 19:31:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.61 on epoch=24
06/19/2022 19:31:05 - INFO - __main__ - Global step 50 Train loss 2.69 ACC 0.40625 on epoch=24
06/19/2022 19:31:05 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.40625 on epoch=24, global_step=50
06/19/2022 19:31:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.70 on epoch=29
06/19/2022 19:31:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=34
06/19/2022 19:31:09 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=39
06/19/2022 19:31:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.35 on epoch=44
06/19/2022 19:31:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=49
06/19/2022 19:31:12 - INFO - __main__ - Global step 100 Train loss 0.50 ACC 0.46875 on epoch=49
06/19/2022 19:31:12 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.46875 on epoch=49, global_step=100
06/19/2022 19:31:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=54
06/19/2022 19:31:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.32 on epoch=59
06/19/2022 19:31:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=64
06/19/2022 19:31:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=69
06/19/2022 19:31:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.31 on epoch=74
06/19/2022 19:31:19 - INFO - __main__ - Global step 150 Train loss 0.31 ACC 0.5 on epoch=74
06/19/2022 19:31:19 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=74, global_step=150
06/19/2022 19:31:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.29 on epoch=79
06/19/2022 19:31:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
06/19/2022 19:31:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.30 on epoch=89
06/19/2022 19:31:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.30 on epoch=94
06/19/2022 19:31:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.31 on epoch=99
06/19/2022 19:31:26 - INFO - __main__ - Global step 200 Train loss 0.30 ACC 0.5 on epoch=99
06/19/2022 19:31:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.35 on epoch=104
06/19/2022 19:31:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=109
06/19/2022 19:31:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=114
06/19/2022 19:31:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
06/19/2022 19:31:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=124
06/19/2022 19:31:32 - INFO - __main__ - Global step 250 Train loss 0.28 ACC 0.5 on epoch=124
06/19/2022 19:31:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.29 on epoch=129
06/19/2022 19:31:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.34 on epoch=134
06/19/2022 19:31:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=139
06/19/2022 19:31:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.30 on epoch=144
06/19/2022 19:31:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.31 on epoch=149
06/19/2022 19:31:39 - INFO - __main__ - Global step 300 Train loss 0.30 ACC 0.5 on epoch=149
06/19/2022 19:31:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
06/19/2022 19:31:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
06/19/2022 19:31:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
06/19/2022 19:31:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=169
06/19/2022 19:31:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=174
06/19/2022 19:31:46 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.46875 on epoch=174
06/19/2022 19:31:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
06/19/2022 19:31:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=184
06/19/2022 19:31:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=189
06/19/2022 19:31:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
06/19/2022 19:31:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=199
06/19/2022 19:31:53 - INFO - __main__ - Global step 400 Train loss 0.27 ACC 0.4375 on epoch=199
06/19/2022 19:31:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=204
06/19/2022 19:31:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=209
06/19/2022 19:31:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=214
06/19/2022 19:31:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
06/19/2022 19:31:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/19/2022 19:32:00 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.40625 on epoch=224
06/19/2022 19:32:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=229
06/19/2022 19:32:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=234
06/19/2022 19:32:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
06/19/2022 19:32:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
06/19/2022 19:32:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=249
06/19/2022 19:32:06 - INFO - __main__ - Global step 500 Train loss 0.24 ACC 0.375 on epoch=249
06/19/2022 19:32:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=254
06/19/2022 19:32:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=259
06/19/2022 19:32:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
06/19/2022 19:32:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=269
06/19/2022 19:32:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=274
06/19/2022 19:32:13 - INFO - __main__ - Global step 550 Train loss 0.21 ACC 0.59375 on epoch=274
06/19/2022 19:32:13 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=274, global_step=550
06/19/2022 19:32:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=279
06/19/2022 19:32:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.17 on epoch=284
06/19/2022 19:32:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=289
06/19/2022 19:32:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.17 on epoch=294
06/19/2022 19:32:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=299
06/19/2022 19:32:20 - INFO - __main__ - Global step 600 Train loss 0.19 ACC 0.40625 on epoch=299
06/19/2022 19:32:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=304
06/19/2022 19:32:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=309
06/19/2022 19:32:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
06/19/2022 19:32:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=319
06/19/2022 19:32:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.15 on epoch=324
06/19/2022 19:32:27 - INFO - __main__ - Global step 650 Train loss 0.16 ACC 0.40625 on epoch=324
06/19/2022 19:32:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=329
06/19/2022 19:32:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=334
06/19/2022 19:32:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=339
06/19/2022 19:32:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=344
06/19/2022 19:32:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=349
06/19/2022 19:32:34 - INFO - __main__ - Global step 700 Train loss 0.13 ACC 0.46875 on epoch=349
06/19/2022 19:32:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=354
06/19/2022 19:32:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=359
06/19/2022 19:32:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=364
06/19/2022 19:32:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=369
06/19/2022 19:32:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=374
06/19/2022 19:32:41 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.5 on epoch=374
06/19/2022 19:32:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.05 on epoch=379
06/19/2022 19:32:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
06/19/2022 19:32:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=389
06/19/2022 19:32:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
06/19/2022 19:32:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
06/19/2022 19:32:47 - INFO - __main__ - Global step 800 Train loss 0.04 ACC 0.53125 on epoch=399
06/19/2022 19:32:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
06/19/2022 19:32:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
06/19/2022 19:32:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
06/19/2022 19:32:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=419
06/19/2022 19:32:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
06/19/2022 19:32:54 - INFO - __main__ - Global step 850 Train loss 0.05 ACC 0.5625 on epoch=424
06/19/2022 19:32:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
06/19/2022 19:32:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/19/2022 19:32:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
06/19/2022 19:32:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
06/19/2022 19:33:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
06/19/2022 19:33:01 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.5 on epoch=449
06/19/2022 19:33:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/19/2022 19:33:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/19/2022 19:33:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/19/2022 19:33:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
06/19/2022 19:33:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
06/19/2022 19:33:08 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.5625 on epoch=474
06/19/2022 19:33:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/19/2022 19:33:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/19/2022 19:33:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/19/2022 19:33:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/19/2022 19:33:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/19/2022 19:33:15 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.5625 on epoch=499
06/19/2022 19:33:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/19/2022 19:33:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/19/2022 19:33:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/19/2022 19:33:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/19/2022 19:33:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/19/2022 19:33:21 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.5625 on epoch=524
06/19/2022 19:33:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/19/2022 19:33:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=534
06/19/2022 19:33:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/19/2022 19:33:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/19/2022 19:33:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/19/2022 19:33:28 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.53125 on epoch=549
06/19/2022 19:33:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/19/2022 19:33:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/19/2022 19:33:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/19/2022 19:33:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/19/2022 19:33:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/19/2022 19:33:35 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.625 on epoch=574
06/19/2022 19:33:35 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=574, global_step=1150
06/19/2022 19:33:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/19/2022 19:33:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=584
06/19/2022 19:33:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/19/2022 19:33:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/19/2022 19:33:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/19/2022 19:33:42 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.46875 on epoch=599
06/19/2022 19:33:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/19/2022 19:33:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/19/2022 19:33:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/19/2022 19:33:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/19/2022 19:33:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/19/2022 19:33:49 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.5625 on epoch=624
06/19/2022 19:33:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/19/2022 19:33:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
06/19/2022 19:33:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/19/2022 19:33:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/19/2022 19:33:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/19/2022 19:33:55 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.625 on epoch=649
06/19/2022 19:33:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
06/19/2022 19:33:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/19/2022 19:33:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/19/2022 19:34:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/19/2022 19:34:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/19/2022 19:34:02 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.53125 on epoch=674
06/19/2022 19:34:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/19/2022 19:34:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/19/2022 19:34:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/19/2022 19:34:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/19/2022 19:34:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/19/2022 19:34:09 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.59375 on epoch=699
06/19/2022 19:34:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/19/2022 19:34:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/19/2022 19:34:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/19/2022 19:34:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/19/2022 19:34:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/19/2022 19:34:16 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.5625 on epoch=724
06/19/2022 19:34:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/19/2022 19:34:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/19/2022 19:34:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/19/2022 19:34:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/19/2022 19:34:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/19/2022 19:34:23 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.5625 on epoch=749
06/19/2022 19:34:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/19/2022 19:34:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
06/19/2022 19:34:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/19/2022 19:34:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/19/2022 19:34:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/19/2022 19:34:29 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.59375 on epoch=774
06/19/2022 19:34:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/19/2022 19:34:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/19/2022 19:34:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/19/2022 19:34:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/19/2022 19:34:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
06/19/2022 19:34:36 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.5625 on epoch=799
06/19/2022 19:34:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/19/2022 19:34:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/19/2022 19:34:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/19/2022 19:34:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/19/2022 19:34:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
06/19/2022 19:34:43 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.5625 on epoch=824
06/19/2022 19:34:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/19/2022 19:34:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/19/2022 19:34:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/19/2022 19:34:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=844
06/19/2022 19:34:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/19/2022 19:34:50 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.5 on epoch=849
06/19/2022 19:34:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/19/2022 19:34:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/19/2022 19:34:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/19/2022 19:34:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/19/2022 19:34:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/19/2022 19:34:57 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.625 on epoch=874
06/19/2022 19:34:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/19/2022 19:34:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/19/2022 19:35:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/19/2022 19:35:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
06/19/2022 19:35:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/19/2022 19:35:04 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.5625 on epoch=899
06/19/2022 19:35:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/19/2022 19:35:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/19/2022 19:35:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/19/2022 19:35:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/19/2022 19:35:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/19/2022 19:35:11 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.53125 on epoch=924
06/19/2022 19:35:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/19/2022 19:35:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/19/2022 19:35:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/19/2022 19:35:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/19/2022 19:35:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/19/2022 19:35:17 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.53125 on epoch=949
06/19/2022 19:35:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/19/2022 19:35:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/19/2022 19:35:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/19/2022 19:35:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/19/2022 19:35:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/19/2022 19:35:24 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.59375 on epoch=974
06/19/2022 19:35:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/19/2022 19:35:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/19/2022 19:35:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/19/2022 19:35:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/19/2022 19:35:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/19/2022 19:35:31 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.59375 on epoch=999
06/19/2022 19:35:31 - INFO - __main__ - save last model!
06/19/2022 19:35:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:35:31 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:35:31 - INFO - __main__ - Printing 3 examples
06/19/2022 19:35:31 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:35:31 - INFO - __main__ - ['equivalent']
06/19/2022 19:35:31 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:35:31 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:35:31 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:35:31 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:35:31 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:35:31 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:35:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:35:31 - INFO - __main__ - Printing 3 examples
06/19/2022 19:35:31 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 19:35:31 - INFO - __main__ - ['equivalent']
06/19/2022 19:35:31 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 19:35:31 - INFO - __main__ - ['equivalent']
06/19/2022 19:35:31 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 19:35:31 - INFO - __main__ - ['equivalent']
06/19/2022 19:35:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:35:32 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:35:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:35:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:35:32 - INFO - __main__ - Printing 3 examples
06/19/2022 19:35:32 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 19:35:32 - INFO - __main__ - ['equivalent']
06/19/2022 19:35:32 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 19:35:32 - INFO - __main__ - ['equivalent']
06/19/2022 19:35:32 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 19:35:32 - INFO - __main__ - ['equivalent']
06/19/2022 19:35:32 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:35:32 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:35:32 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:35:32 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:35:37 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:35:37 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(3350, 220687), (893, 1024942), (5008, 216986), (3569, 155945), (1895, 505183), (6448, 171477), (3241, 318391), (929, 292427), (1087, 843077), (697, 1020385), (3240, 286466), (2247, 316743), (3160, 306799), (5020, 210030), (4688, 155891), (4117, 219390), (1585, 544408), (4944, 210752), (4061, 266094), (4431, 209722), (482, 1812745), (3620, 265063), (5097, 214762), (2224, 338862), (569, 1402094), (783, 1075853), (4357, 231267), (3841, 258710), (2528, 182651), (1731, 565272), (283, 2553263), (1562, 628829), (6743, 154852), (4183, 213298), (405, 2285563), (1048, 702299), (748, 1191082), (2090, 392646), (4006, 256579), (3806, 259517), (4364, 244427), (4569, 164817), (376, 2422082), (5140, 155426), (1659, 586057), (4023, 208736), (3750, 253148), (3668, 251835), (821, 1056303), (5473, 163136), (1485, 648438), (5236, 177258), (3899, 255931), (4358, 235267), (3492, 299542), (3232, 334015), (2591, 370033), (1293, 773200), (306, 3068322), (1737, 490367), (5564, 187660), (5897, 180972), (1954, 242093), (3794, 166375), (1038, 749777), (2691, 354421), (2292, 355522), (3657, 209918), (2372, 427055), (3366, 327251), (5341, 190097), (2086, 457719), (1349, 721864), (1303, 658549), (845, 1086618), (4833, 207472), (1676, 524121), (6029, 210280), (942, 942590), (2366, 294304), (3598, 269003), (694, 1218279), (6109, 170031), (6098, 172422), (2985, 322411), (2846, 312704), (1230, 573495), (1549, 521331), (3860, 259925), (5853, 183928), (2935, 409303), (1310, 731782), (2098, 455459), (1679, 644199), (2605, 391810), (3637, 189415), (310, 2990534), (4197, 260190), (4416, 169042)]
06/19/2022 19:35:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:35:38 - INFO - __main__ - Starting training!
06/19/2022 19:35:40 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_87_0.4_8_predictions.txt
06/19/2022 19:35:40 - INFO - __main__ - ACC on test data: 0.4804
06/19/2022 19:35:40 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.4, bsz=8, dev_performance=0.625, test_performance=0.4803921568627451
06/19/2022 19:35:40 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.3, bsz=8 ...
06/19/2022 19:35:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:35:41 - INFO - __main__ - Printing 3 examples
06/19/2022 19:35:41 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 19:35:41 - INFO - __main__ - ['equivalent']
06/19/2022 19:35:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 19:35:41 - INFO - __main__ - ['equivalent']
06/19/2022 19:35:41 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 19:35:41 - INFO - __main__ - ['equivalent']
06/19/2022 19:35:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:35:41 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:35:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:35:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:35:41 - INFO - __main__ - Printing 3 examples
06/19/2022 19:35:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 19:35:41 - INFO - __main__ - ['equivalent']
06/19/2022 19:35:41 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 19:35:41 - INFO - __main__ - ['equivalent']
06/19/2022 19:35:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 19:35:41 - INFO - __main__ - ['equivalent']
06/19/2022 19:35:41 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:35:41 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:35:41 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:35:46 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:35:46 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(3350, 220687), (893, 1024942), (5008, 216986), (3569, 155945), (1895, 505183), (6448, 171477), (3241, 318391), (929, 292427), (1087, 843077), (697, 1020385), (3240, 286466), (2247, 316743), (3160, 306799), (5020, 210030), (4688, 155891), (4117, 219390), (1585, 544408), (4944, 210752), (4061, 266094), (4431, 209722), (482, 1812745), (3620, 265063), (5097, 214762), (2224, 338862), (569, 1402094), (783, 1075853), (4357, 231267), (3841, 258710), (2528, 182651), (1731, 565272), (283, 2553263), (1562, 628829), (6743, 154852), (4183, 213298), (405, 2285563), (1048, 702299), (748, 1191082), (2090, 392646), (4006, 256579), (3806, 259517), (4364, 244427), (4569, 164817), (376, 2422082), (5140, 155426), (1659, 586057), (4023, 208736), (3750, 253148), (3668, 251835), (821, 1056303), (5473, 163136), (1485, 648438), (5236, 177258), (3899, 255931), (4358, 235267), (3492, 299542), (3232, 334015), (2591, 370033), (1293, 773200), (306, 3068322), (1737, 490367), (5564, 187660), (5897, 180972), (1954, 242093), (3794, 166375), (1038, 749777), (2691, 354421), (2292, 355522), (3657, 209918), (2372, 427055), (3366, 327251), (5341, 190097), (2086, 457719), (1349, 721864), (1303, 658549), (845, 1086618), (4833, 207472), (1676, 524121), (6029, 210280), (942, 942590), (2366, 294304), (3598, 269003), (694, 1218279), (6109, 170031), (6098, 172422), (2985, 322411), (2846, 312704), (1230, 573495), (1549, 521331), (3860, 259925), (5853, 183928), (2935, 409303), (1310, 731782), (2098, 455459), (1679, 644199), (2605, 391810), (3637, 189415), (310, 2990534), (4197, 260190), (4416, 169042)]
06/19/2022 19:35:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:35:46 - INFO - __main__ - Starting training!
06/19/2022 19:35:48 - INFO - __main__ - Step 10 Global step 10 Train loss 6.71 on epoch=4
06/19/2022 19:35:49 - INFO - __main__ - Step 20 Global step 20 Train loss 4.27 on epoch=9
06/19/2022 19:35:50 - INFO - __main__ - Step 30 Global step 30 Train loss 2.19 on epoch=14
06/19/2022 19:35:52 - INFO - __main__ - Step 40 Global step 40 Train loss 1.23 on epoch=19
06/19/2022 19:35:53 - INFO - __main__ - Step 50 Global step 50 Train loss 0.75 on epoch=24
06/19/2022 19:35:53 - INFO - __main__ - Global step 50 Train loss 3.03 ACC 0.4375 on epoch=24
06/19/2022 19:35:53 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.4375 on epoch=24, global_step=50
06/19/2022 19:35:55 - INFO - __main__ - Step 60 Global step 60 Train loss 0.64 on epoch=29
06/19/2022 19:35:56 - INFO - __main__ - Step 70 Global step 70 Train loss 0.57 on epoch=34
06/19/2022 19:35:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=39
06/19/2022 19:35:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.38 on epoch=44
06/19/2022 19:36:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=49
06/19/2022 19:36:00 - INFO - __main__ - Global step 100 Train loss 0.48 ACC 0.5 on epoch=49
06/19/2022 19:36:00 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=49, global_step=100
06/19/2022 19:36:02 - INFO - __main__ - Step 110 Global step 110 Train loss 0.31 on epoch=54
06/19/2022 19:36:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.35 on epoch=59
06/19/2022 19:36:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.33 on epoch=64
06/19/2022 19:36:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.40 on epoch=69
06/19/2022 19:36:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.33 on epoch=74
06/19/2022 19:36:07 - INFO - __main__ - Global step 150 Train loss 0.34 ACC 0.46875 on epoch=74
06/19/2022 19:36:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.35 on epoch=79
06/19/2022 19:36:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.35 on epoch=84
06/19/2022 19:36:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.33 on epoch=89
06/19/2022 19:36:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
06/19/2022 19:36:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
06/19/2022 19:36:14 - INFO - __main__ - Global step 200 Train loss 0.31 ACC 0.4375 on epoch=99
06/19/2022 19:36:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=104
06/19/2022 19:36:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
06/19/2022 19:36:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
06/19/2022 19:36:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=119
06/19/2022 19:36:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=124
06/19/2022 19:36:21 - INFO - __main__ - Global step 250 Train loss 0.31 ACC 0.40625 on epoch=124
06/19/2022 19:36:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.29 on epoch=129
06/19/2022 19:36:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.32 on epoch=134
06/19/2022 19:36:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=139
06/19/2022 19:36:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
06/19/2022 19:36:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
06/19/2022 19:36:28 - INFO - __main__ - Global step 300 Train loss 0.29 ACC 0.53125 on epoch=149
06/19/2022 19:36:28 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=149, global_step=300
06/19/2022 19:36:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
06/19/2022 19:36:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=159
06/19/2022 19:36:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
06/19/2022 19:36:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.31 on epoch=169
06/19/2022 19:36:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
06/19/2022 19:36:35 - INFO - __main__ - Global step 350 Train loss 0.27 ACC 0.5 on epoch=174
06/19/2022 19:36:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
06/19/2022 19:36:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
06/19/2022 19:36:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.32 on epoch=189
06/19/2022 19:36:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=194
06/19/2022 19:36:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=199
06/19/2022 19:36:41 - INFO - __main__ - Global step 400 Train loss 0.29 ACC 0.46875 on epoch=199
06/19/2022 19:36:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=204
06/19/2022 19:36:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=209
06/19/2022 19:36:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=214
06/19/2022 19:36:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
06/19/2022 19:36:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
06/19/2022 19:36:48 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.4375 on epoch=224
06/19/2022 19:36:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=229
06/19/2022 19:36:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=234
06/19/2022 19:36:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
06/19/2022 19:36:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=244
06/19/2022 19:36:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=249
06/19/2022 19:36:55 - INFO - __main__ - Global step 500 Train loss 0.28 ACC 0.4375 on epoch=249
06/19/2022 19:36:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=254
06/19/2022 19:36:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=259
06/19/2022 19:36:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=264
06/19/2022 19:37:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=269
06/19/2022 19:37:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=274
06/19/2022 19:37:02 - INFO - __main__ - Global step 550 Train loss 0.22 ACC 0.4375 on epoch=274
06/19/2022 19:37:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=279
06/19/2022 19:37:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=284
06/19/2022 19:37:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=289
06/19/2022 19:37:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=294
06/19/2022 19:37:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=299
06/19/2022 19:37:09 - INFO - __main__ - Global step 600 Train loss 0.24 ACC 0.40625 on epoch=299
06/19/2022 19:37:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=304
06/19/2022 19:37:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=309
06/19/2022 19:37:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=314
06/19/2022 19:37:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=319
06/19/2022 19:37:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=324
06/19/2022 19:37:15 - INFO - __main__ - Global step 650 Train loss 0.23 ACC 0.5 on epoch=324
06/19/2022 19:37:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=329
06/19/2022 19:37:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=334
06/19/2022 19:37:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=339
06/19/2022 19:37:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=344
06/19/2022 19:37:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=349
06/19/2022 19:37:22 - INFO - __main__ - Global step 700 Train loss 0.23 ACC 0.4375 on epoch=349
06/19/2022 19:37:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=354
06/19/2022 19:37:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=359
06/19/2022 19:37:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.18 on epoch=364
06/19/2022 19:37:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=369
06/19/2022 19:37:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.18 on epoch=374
06/19/2022 19:37:29 - INFO - __main__ - Global step 750 Train loss 0.19 ACC 0.40625 on epoch=374
06/19/2022 19:37:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
06/19/2022 19:37:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=384
06/19/2022 19:37:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=389
06/19/2022 19:37:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=394
06/19/2022 19:37:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=399
06/19/2022 19:37:36 - INFO - __main__ - Global step 800 Train loss 0.20 ACC 0.375 on epoch=399
06/19/2022 19:37:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=404
06/19/2022 19:37:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=409
06/19/2022 19:37:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=414
06/19/2022 19:37:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=419
06/19/2022 19:37:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=424
06/19/2022 19:37:43 - INFO - __main__ - Global step 850 Train loss 0.21 ACC 0.625 on epoch=424
06/19/2022 19:37:43 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.625 on epoch=424, global_step=850
06/19/2022 19:37:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=429
06/19/2022 19:37:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.15 on epoch=434
06/19/2022 19:37:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=439
06/19/2022 19:37:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=444
06/19/2022 19:37:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=449
06/19/2022 19:37:50 - INFO - __main__ - Global step 900 Train loss 0.17 ACC 0.53125 on epoch=449
06/19/2022 19:37:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=454
06/19/2022 19:37:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=459
06/19/2022 19:37:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.15 on epoch=464
06/19/2022 19:37:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=469
06/19/2022 19:37:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=474
06/19/2022 19:37:56 - INFO - __main__ - Global step 950 Train loss 0.16 ACC 0.4375 on epoch=474
06/19/2022 19:37:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=479
06/19/2022 19:37:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=484
06/19/2022 19:38:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=489
06/19/2022 19:38:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=494
06/19/2022 19:38:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=499
06/19/2022 19:38:03 - INFO - __main__ - Global step 1000 Train loss 0.12 ACC 0.46875 on epoch=499
06/19/2022 19:38:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.13 on epoch=504
06/19/2022 19:38:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=509
06/19/2022 19:38:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=514
06/19/2022 19:38:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=519
06/19/2022 19:38:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=524
06/19/2022 19:38:10 - INFO - __main__ - Global step 1050 Train loss 0.10 ACC 0.34375 on epoch=524
06/19/2022 19:38:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=529
06/19/2022 19:38:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=534
06/19/2022 19:38:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=539
06/19/2022 19:38:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
06/19/2022 19:38:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
06/19/2022 19:38:17 - INFO - __main__ - Global step 1100 Train loss 0.07 ACC 0.4375 on epoch=549
06/19/2022 19:38:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=554
06/19/2022 19:38:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=559
06/19/2022 19:38:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=564
06/19/2022 19:38:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=569
06/19/2022 19:38:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/19/2022 19:38:24 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.4375 on epoch=574
06/19/2022 19:38:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=579
06/19/2022 19:38:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/19/2022 19:38:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
06/19/2022 19:38:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/19/2022 19:38:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/19/2022 19:38:30 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.40625 on epoch=599
06/19/2022 19:38:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
06/19/2022 19:38:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
06/19/2022 19:38:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/19/2022 19:38:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/19/2022 19:38:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/19/2022 19:38:37 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.375 on epoch=624
06/19/2022 19:38:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
06/19/2022 19:38:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/19/2022 19:38:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/19/2022 19:38:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/19/2022 19:38:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/19/2022 19:38:44 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.40625 on epoch=649
06/19/2022 19:38:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/19/2022 19:38:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/19/2022 19:38:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/19/2022 19:38:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/19/2022 19:38:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/19/2022 19:38:51 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.46875 on epoch=674
06/19/2022 19:38:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/19/2022 19:38:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/19/2022 19:38:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/19/2022 19:38:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/19/2022 19:38:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=699
06/19/2022 19:38:58 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.4375 on epoch=699
06/19/2022 19:38:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/19/2022 19:39:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/19/2022 19:39:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/19/2022 19:39:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
06/19/2022 19:39:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/19/2022 19:39:05 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.40625 on epoch=724
06/19/2022 19:39:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/19/2022 19:39:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/19/2022 19:39:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=739
06/19/2022 19:39:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/19/2022 19:39:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/19/2022 19:39:11 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.4375 on epoch=749
06/19/2022 19:39:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/19/2022 19:39:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/19/2022 19:39:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/19/2022 19:39:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/19/2022 19:39:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/19/2022 19:39:18 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.4375 on epoch=774
06/19/2022 19:39:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/19/2022 19:39:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/19/2022 19:39:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/19/2022 19:39:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/19/2022 19:39:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/19/2022 19:39:25 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.375 on epoch=799
06/19/2022 19:39:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=804
06/19/2022 19:39:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/19/2022 19:39:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/19/2022 19:39:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/19/2022 19:39:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/19/2022 19:39:32 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.53125 on epoch=824
06/19/2022 19:39:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/19/2022 19:39:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/19/2022 19:39:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/19/2022 19:39:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/19/2022 19:39:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/19/2022 19:39:39 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.46875 on epoch=849
06/19/2022 19:39:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=854
06/19/2022 19:39:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/19/2022 19:39:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
06/19/2022 19:39:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/19/2022 19:39:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
06/19/2022 19:39:46 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.40625 on epoch=874
06/19/2022 19:39:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/19/2022 19:39:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/19/2022 19:39:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/19/2022 19:39:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/19/2022 19:39:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/19/2022 19:39:52 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.40625 on epoch=899
06/19/2022 19:39:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
06/19/2022 19:39:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/19/2022 19:39:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/19/2022 19:39:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/19/2022 19:39:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/19/2022 19:39:59 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.40625 on epoch=924
06/19/2022 19:40:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/19/2022 19:40:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/19/2022 19:40:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/19/2022 19:40:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/19/2022 19:40:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/19/2022 19:40:06 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.40625 on epoch=949
06/19/2022 19:40:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/19/2022 19:40:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/19/2022 19:40:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/19/2022 19:40:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/19/2022 19:40:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/19/2022 19:40:13 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.375 on epoch=974
06/19/2022 19:40:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/19/2022 19:40:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/19/2022 19:40:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/19/2022 19:40:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/19/2022 19:40:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/19/2022 19:40:20 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.375 on epoch=999
06/19/2022 19:40:20 - INFO - __main__ - save last model!
06/19/2022 19:40:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:40:20 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:40:20 - INFO - __main__ - Printing 3 examples
06/19/2022 19:40:20 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:40:20 - INFO - __main__ - ['equivalent']
06/19/2022 19:40:20 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:40:20 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:40:20 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:40:20 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:40:20 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:40:20 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:40:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:40:20 - INFO - __main__ - Printing 3 examples
06/19/2022 19:40:20 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 19:40:20 - INFO - __main__ - ['equivalent']
06/19/2022 19:40:20 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 19:40:20 - INFO - __main__ - ['equivalent']
06/19/2022 19:40:20 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 19:40:20 - INFO - __main__ - ['equivalent']
06/19/2022 19:40:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:40:20 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:40:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:40:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:40:21 - INFO - __main__ - Printing 3 examples
06/19/2022 19:40:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 19:40:21 - INFO - __main__ - ['equivalent']
06/19/2022 19:40:21 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 19:40:21 - INFO - __main__ - ['equivalent']
06/19/2022 19:40:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 19:40:21 - INFO - __main__ - ['equivalent']
06/19/2022 19:40:21 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:40:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:40:21 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:40:21 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:40:26 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:40:26 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(6261, 163483), (4632, 254637), (5099, 200518), (6349, 169524), (1329, 682367), (917, 952119), (772, 1209132), (2567, 304604), (4284, 235257), (3769, 195111), (74, 179054), (4879, 229125), (4142, 229917), (6316, 162881), (295, 1089836), (1232, 706951), (2628, 297222), (797, 1085506), (4949, 208735), (3291, 200786), (5623, 181105), (4050, 271976), (1438, 688034), (5810, 169570), (6476, 159449), (1937, 501940), (1253, 684766), (64, 215681), (5640, 186094), (617, 1442206), (1338, 712614), (5749, 180863), (1433, 586613), (2783, 308812), (1790, 550995), (883, 560479), (3516, 291319), (2213, 406980), (3854, 270252), (5624, 180780), (2128, 328887), (893, 1024942), (2724, 369166), (2096, 465293), (5642, 169703), (2851, 338896), (1051, 707945), (3345, 310573), (5726, 175237), (1020, 892057), (2121, 186176), (1583, 535374), (6537, 158593), (6440, 161231), (1722, 583639), (2088, 528184), (4540, 155419), (3665, 278672), (4772, 165920), (377, 1870362), (334, 2807606), (937, 1007700), (503, 1684486), (7444, 162492), (2377, 350752), (2191, 433246), (6516, 163489), (3318, 225546), (272, 2641523), (4633, 187899), (5782, 171468), (2931, 323269), (2917, 311362), (5191, 193394), (2767, 377111), (1536, 231332), (3234, 315011), (5256, 206528), (826, 1095264), (507, 1331640), (1486, 571578), (4696, 212488), (3943, 226298), (93, 192649), (5327, 201469), (2186, 447650), (3209, 276527), (6551, 154998), (1473, 623867), (3594, 300832), (3361, 294678), (1956, 270967), (4394, 234859), (1368, 677011), (3699, 253359), (5124, 181868), (3794, 166375), (1060, 823306), (3070, 294809)]
06/19/2022 19:40:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:40:27 - INFO - __main__ - Starting training!
06/19/2022 19:40:29 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_87_0.3_8_predictions.txt
06/19/2022 19:40:29 - INFO - __main__ - ACC on test data: 0.5098
06/19/2022 19:40:29 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.3, bsz=8, dev_performance=0.625, test_performance=0.5098039215686274
06/19/2022 19:40:29 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.2, bsz=8 ...
06/19/2022 19:40:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:40:30 - INFO - __main__ - Printing 3 examples
06/19/2022 19:40:30 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 19:40:30 - INFO - __main__ - ['equivalent']
06/19/2022 19:40:30 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 19:40:30 - INFO - __main__ - ['equivalent']
06/19/2022 19:40:30 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 19:40:30 - INFO - __main__ - ['equivalent']
06/19/2022 19:40:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:40:30 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:40:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:40:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:40:30 - INFO - __main__ - Printing 3 examples
06/19/2022 19:40:30 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 19:40:30 - INFO - __main__ - ['equivalent']
06/19/2022 19:40:30 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 19:40:30 - INFO - __main__ - ['equivalent']
06/19/2022 19:40:30 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 19:40:30 - INFO - __main__ - ['equivalent']
06/19/2022 19:40:30 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:40:30 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:40:30 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:40:35 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:40:35 - INFO - __main__ - task name: glue-mrpc
initialize from c4
[(6261, 163483), (4632, 254637), (5099, 200518), (6349, 169524), (1329, 682367), (917, 952119), (772, 1209132), (2567, 304604), (4284, 235257), (3769, 195111), (74, 179054), (4879, 229125), (4142, 229917), (6316, 162881), (295, 1089836), (1232, 706951), (2628, 297222), (797, 1085506), (4949, 208735), (3291, 200786), (5623, 181105), (4050, 271976), (1438, 688034), (5810, 169570), (6476, 159449), (1937, 501940), (1253, 684766), (64, 215681), (5640, 186094), (617, 1442206), (1338, 712614), (5749, 180863), (1433, 586613), (2783, 308812), (1790, 550995), (883, 560479), (3516, 291319), (2213, 406980), (3854, 270252), (5624, 180780), (2128, 328887), (893, 1024942), (2724, 369166), (2096, 465293), (5642, 169703), (2851, 338896), (1051, 707945), (3345, 310573), (5726, 175237), (1020, 892057), (2121, 186176), (1583, 535374), (6537, 158593), (6440, 161231), (1722, 583639), (2088, 528184), (4540, 155419), (3665, 278672), (4772, 165920), (377, 1870362), (334, 2807606), (937, 1007700), (503, 1684486), (7444, 162492), (2377, 350752), (2191, 433246), (6516, 163489), (3318, 225546), (272, 2641523), (4633, 187899), (5782, 171468), (2931, 323269), (2917, 311362), (5191, 193394), (2767, 377111), (1536, 231332), (3234, 315011), (5256, 206528), (826, 1095264), (507, 1331640), (1486, 571578), (4696, 212488), (3943, 226298), (93, 192649), (5327, 201469), (2186, 447650), (3209, 276527), (6551, 154998), (1473, 623867), (3594, 300832), (3361, 294678), (1956, 270967), (4394, 234859), (1368, 677011), (3699, 253359), (5124, 181868), (3794, 166375), (1060, 823306), (3070, 294809)]
06/19/2022 19:40:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:40:35 - INFO - __main__ - Starting training!
06/19/2022 19:40:37 - INFO - __main__ - Step 10 Global step 10 Train loss 7.17 on epoch=4
06/19/2022 19:40:38 - INFO - __main__ - Step 20 Global step 20 Train loss 5.45 on epoch=9
06/19/2022 19:40:39 - INFO - __main__ - Step 30 Global step 30 Train loss 4.31 on epoch=14
06/19/2022 19:40:41 - INFO - __main__ - Step 40 Global step 40 Train loss 2.88 on epoch=19
06/19/2022 19:40:42 - INFO - __main__ - Step 50 Global step 50 Train loss 1.65 on epoch=24
06/19/2022 19:40:42 - INFO - __main__ - Global step 50 Train loss 4.29 ACC 0.5 on epoch=24
06/19/2022 19:40:42 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/19/2022 19:40:43 - INFO - __main__ - Step 60 Global step 60 Train loss 1.20 on epoch=29
06/19/2022 19:40:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.79 on epoch=34
06/19/2022 19:40:46 - INFO - __main__ - Step 80 Global step 80 Train loss 0.66 on epoch=39
06/19/2022 19:40:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=44
06/19/2022 19:40:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=49
06/19/2022 19:40:49 - INFO - __main__ - Global step 100 Train loss 0.74 ACC 0.5 on epoch=49
06/19/2022 19:40:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=54
06/19/2022 19:40:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=59
06/19/2022 19:40:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=64
06/19/2022 19:40:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=69
06/19/2022 19:40:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=74
06/19/2022 19:40:56 - INFO - __main__ - Global step 150 Train loss 0.47 ACC 0.46875 on epoch=74
06/19/2022 19:40:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.36 on epoch=79
06/19/2022 19:40:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.35 on epoch=84
06/19/2022 19:41:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
06/19/2022 19:41:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
06/19/2022 19:41:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=99
06/19/2022 19:41:03 - INFO - __main__ - Global step 200 Train loss 0.36 ACC 0.46875 on epoch=99
06/19/2022 19:41:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.30 on epoch=104
06/19/2022 19:41:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.32 on epoch=109
06/19/2022 19:41:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=114
06/19/2022 19:41:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=119
06/19/2022 19:41:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=124
06/19/2022 19:41:10 - INFO - __main__ - Global step 250 Train loss 0.33 ACC 0.5 on epoch=124
06/19/2022 19:41:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.33 on epoch=129
06/19/2022 19:41:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=134
06/19/2022 19:41:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
06/19/2022 19:41:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
06/19/2022 19:41:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=149
06/19/2022 19:41:17 - INFO - __main__ - Global step 300 Train loss 0.29 ACC 0.5 on epoch=149
06/19/2022 19:41:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=154
06/19/2022 19:41:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=159
06/19/2022 19:41:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
06/19/2022 19:41:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
06/19/2022 19:41:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=174
06/19/2022 19:41:24 - INFO - __main__ - Global step 350 Train loss 0.29 ACC 0.40625 on epoch=174
06/19/2022 19:41:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.30 on epoch=179
06/19/2022 19:41:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=184
06/19/2022 19:41:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=189
06/19/2022 19:41:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
06/19/2022 19:41:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=199
06/19/2022 19:41:31 - INFO - __main__ - Global step 400 Train loss 0.27 ACC 0.5 on epoch=199
06/19/2022 19:41:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=204
06/19/2022 19:41:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=209
06/19/2022 19:41:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=214
06/19/2022 19:41:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=219
06/19/2022 19:41:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/19/2022 19:41:38 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.375 on epoch=224
06/19/2022 19:41:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
06/19/2022 19:41:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=234
06/19/2022 19:41:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
06/19/2022 19:41:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
06/19/2022 19:41:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=249
06/19/2022 19:41:45 - INFO - __main__ - Global step 500 Train loss 0.23 ACC 0.4375 on epoch=249
06/19/2022 19:41:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=254
06/19/2022 19:41:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=259
06/19/2022 19:41:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=264
06/19/2022 19:41:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=269
06/19/2022 19:41:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=274
06/19/2022 19:41:52 - INFO - __main__ - Global step 550 Train loss 0.21 ACC 0.46875 on epoch=274
06/19/2022 19:41:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=279
06/19/2022 19:41:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=284
06/19/2022 19:41:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=289
06/19/2022 19:41:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.12 on epoch=294
06/19/2022 19:41:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=299
06/19/2022 19:41:59 - INFO - __main__ - Global step 600 Train loss 0.17 ACC 0.4375 on epoch=299
06/19/2022 19:42:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=304
06/19/2022 19:42:01 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=309
06/19/2022 19:42:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=314
06/19/2022 19:42:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=319
06/19/2022 19:42:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=324
06/19/2022 19:42:05 - INFO - __main__ - Global step 650 Train loss 0.17 ACC 0.59375 on epoch=324
06/19/2022 19:42:06 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=324, global_step=650
06/19/2022 19:42:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=329
06/19/2022 19:42:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=334
06/19/2022 19:42:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=339
06/19/2022 19:42:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=344
06/19/2022 19:42:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.10 on epoch=349
06/19/2022 19:42:12 - INFO - __main__ - Global step 700 Train loss 0.13 ACC 0.625 on epoch=349
06/19/2022 19:42:13 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=349, global_step=700
06/19/2022 19:42:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=354
06/19/2022 19:42:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=359
06/19/2022 19:42:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=364
06/19/2022 19:42:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=369
06/19/2022 19:42:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=374
06/19/2022 19:42:19 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.625 on epoch=374
06/19/2022 19:42:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=379
06/19/2022 19:42:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=384
06/19/2022 19:42:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
06/19/2022 19:42:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=394
06/19/2022 19:42:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=399
06/19/2022 19:42:26 - INFO - __main__ - Global step 800 Train loss 0.09 ACC 0.53125 on epoch=399
06/19/2022 19:42:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
06/19/2022 19:42:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=409
06/19/2022 19:42:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=414
06/19/2022 19:42:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
06/19/2022 19:42:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=424
06/19/2022 19:42:33 - INFO - __main__ - Global step 850 Train loss 0.07 ACC 0.5625 on epoch=424
06/19/2022 19:42:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=429
06/19/2022 19:42:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=434
06/19/2022 19:42:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
06/19/2022 19:42:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=444
06/19/2022 19:42:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=449
06/19/2022 19:42:40 - INFO - __main__ - Global step 900 Train loss 0.13 ACC 0.53125 on epoch=449
06/19/2022 19:42:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.78 on epoch=454
06/19/2022 19:42:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.96 on epoch=459
06/19/2022 19:42:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
06/19/2022 19:42:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=469
06/19/2022 19:42:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
06/19/2022 19:42:47 - INFO - __main__ - Global step 950 Train loss 0.38 ACC 0.4375 on epoch=474
06/19/2022 19:42:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
06/19/2022 19:42:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
06/19/2022 19:42:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.08 on epoch=489
06/19/2022 19:42:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
06/19/2022 19:42:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
06/19/2022 19:42:54 - INFO - __main__ - Global step 1000 Train loss 0.06 ACC 0.53125 on epoch=499
06/19/2022 19:42:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
06/19/2022 19:42:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
06/19/2022 19:42:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
06/19/2022 19:42:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
06/19/2022 19:43:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
06/19/2022 19:43:01 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.5625 on epoch=524
06/19/2022 19:43:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
06/19/2022 19:43:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
06/19/2022 19:43:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=539
06/19/2022 19:43:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=544
06/19/2022 19:43:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
06/19/2022 19:43:08 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.4375 on epoch=549
06/19/2022 19:43:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=554
06/19/2022 19:43:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=559
06/19/2022 19:43:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=564
06/19/2022 19:43:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
06/19/2022 19:43:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=574
06/19/2022 19:43:15 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.5625 on epoch=574
06/19/2022 19:43:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/19/2022 19:43:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
06/19/2022 19:43:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
06/19/2022 19:43:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
06/19/2022 19:43:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
06/19/2022 19:43:22 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.53125 on epoch=599
06/19/2022 19:43:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
06/19/2022 19:43:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
06/19/2022 19:43:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
06/19/2022 19:43:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=619
06/19/2022 19:43:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
06/19/2022 19:43:29 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.5 on epoch=624
06/19/2022 19:43:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=629
06/19/2022 19:43:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
06/19/2022 19:43:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/19/2022 19:43:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
06/19/2022 19:43:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
06/19/2022 19:43:36 - INFO - __main__ - Global step 1300 Train loss 0.04 ACC 0.4375 on epoch=649
06/19/2022 19:43:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
06/19/2022 19:43:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=659
06/19/2022 19:43:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
06/19/2022 19:43:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
06/19/2022 19:43:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
06/19/2022 19:43:43 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.53125 on epoch=674
06/19/2022 19:43:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/19/2022 19:43:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/19/2022 19:43:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
06/19/2022 19:43:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
06/19/2022 19:43:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/19/2022 19:43:50 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.4375 on epoch=699
06/19/2022 19:43:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
06/19/2022 19:43:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
06/19/2022 19:43:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
06/19/2022 19:43:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
06/19/2022 19:43:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
06/19/2022 19:43:57 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.5 on epoch=724
06/19/2022 19:43:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=729
06/19/2022 19:44:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
06/19/2022 19:44:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/19/2022 19:44:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/19/2022 19:44:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/19/2022 19:44:04 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.59375 on epoch=749
06/19/2022 19:44:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/19/2022 19:44:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
06/19/2022 19:44:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
06/19/2022 19:44:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/19/2022 19:44:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/19/2022 19:44:11 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.59375 on epoch=774
06/19/2022 19:44:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=779
06/19/2022 19:44:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
06/19/2022 19:44:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
06/19/2022 19:44:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
06/19/2022 19:44:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=799
06/19/2022 19:44:18 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.4375 on epoch=799
06/19/2022 19:44:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
06/19/2022 19:44:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
06/19/2022 19:44:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/19/2022 19:44:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/19/2022 19:44:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
06/19/2022 19:44:25 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.5625 on epoch=824
06/19/2022 19:44:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/19/2022 19:44:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/19/2022 19:44:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
06/19/2022 19:44:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=844
06/19/2022 19:44:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
06/19/2022 19:44:32 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.5 on epoch=849
06/19/2022 19:44:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
06/19/2022 19:44:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/19/2022 19:44:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/19/2022 19:44:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=869
06/19/2022 19:44:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/19/2022 19:44:39 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.5625 on epoch=874
06/19/2022 19:44:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/19/2022 19:44:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=884
06/19/2022 19:44:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/19/2022 19:44:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/19/2022 19:44:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/19/2022 19:44:46 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.5625 on epoch=899
06/19/2022 19:44:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/19/2022 19:44:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/19/2022 19:44:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/19/2022 19:44:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/19/2022 19:44:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
06/19/2022 19:44:53 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.5625 on epoch=924
06/19/2022 19:44:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/19/2022 19:44:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
06/19/2022 19:44:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/19/2022 19:44:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
06/19/2022 19:45:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
06/19/2022 19:45:00 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.53125 on epoch=949
06/19/2022 19:45:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/19/2022 19:45:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
06/19/2022 19:45:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/19/2022 19:45:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/19/2022 19:45:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/19/2022 19:45:07 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.5 on epoch=974
06/19/2022 19:45:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
06/19/2022 19:45:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
06/19/2022 19:45:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/19/2022 19:45:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=994
06/19/2022 19:45:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/19/2022 19:45:14 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.5625 on epoch=999
06/19/2022 19:45:14 - INFO - __main__ - save last model!
06/19/2022 19:45:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:45:14 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:45:14 - INFO - __main__ - Printing 3 examples
06/19/2022 19:45:14 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:45:14 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:14 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:45:14 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:45:14 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:45:14 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:45:14 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:45:15 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:45:15 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:45:23 - INFO - __main__ - Saved prediction in models/T5-base-nopara2para/singletask-glue-mrpc/glue-mrpc_16_87_0.2_8_predictions.txt
06/19/2022 19:45:23 - INFO - __main__ - ACC on test data: 0.4853
06/19/2022 19:45:23 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.2, bsz=8, dev_performance=0.625, test_performance=0.4852941176470588
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (87889): No such process
Task: glue-qqp, Checkpoint: None, Identifier: T5-base-nopara2para
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt_nopara2para.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt_nopara2para.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/models/T5-base-nopara2para/singletask-glue-qqp/log.txt'
06/19/2022 19:45:28 - INFO - __main__ - Namespace(task_dir='data/glue-qqp/', task_name='glue-qqp', identifier='T5-base-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-nopara2para/singletask-glue-qqp', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 19:45:28 - INFO - __main__ - models/T5-base-nopara2para/singletask-glue-qqp
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'tune_hps_singletask_ddp_prompt_nopara2para.py', '--local_rank=1', '--task_dir', 'data/glue-qqp/', '--task_name', 'glue-qqp', '--identifier', 'T5-base-nopara2para', '--checkpoint', 'None', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-nopara2para/singletask-glue-qqp', '--cuda', '2,3', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', '--model', 'google/t5-v1_1-base', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 87894
Killing subprocess 87895
++++++++++++++++++++++++++++++
kill: (87901): No such process
Task: medical_questions_pairs, Checkpoint: None, Identifier: T5-base-nopara2para
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt_nopara2para.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt_nopara2para.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/models/T5-base-nopara2para/singletask-medical_questions_pairs/log.txt'
06/19/2022 19:45:32 - INFO - __main__ - Namespace(task_dir='data/medical_questions_pairs/', task_name='medical_questions_pairs', identifier='T5-base-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-nopara2para/singletask-medical_questions_pairs', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 19:45:32 - INFO - __main__ - models/T5-base-nopara2para/singletask-medical_questions_pairs
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'tune_hps_singletask_ddp_prompt_nopara2para.py', '--local_rank=1', '--task_dir', 'data/medical_questions_pairs/', '--task_name', 'medical_questions_pairs', '--identifier', 'T5-base-nopara2para', '--checkpoint', 'None', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-nopara2para/singletask-medical_questions_pairs', '--cuda', '2,3', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', '--model', 'google/t5-v1_1-base', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 87906
Killing subprocess 87907
++++++++++++++++++++++++++++++
kill: (87913): No such process
Task: paws, Checkpoint: None, Identifier: T5-base-nopara2para
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt_nopara2para.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt_nopara2para.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/models/T5-base-nopara2para/singletask-paws/log.txt'
06/19/2022 19:45:36 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-nopara2para/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 19:45:36 - INFO - __main__ - models/T5-base-nopara2para/singletask-paws
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'tune_hps_singletask_ddp_prompt_nopara2para.py', '--local_rank=1', '--task_dir', 'data/paws/', '--task_name', 'paws', '--identifier', 'T5-base-nopara2para', '--checkpoint', 'None', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-nopara2para/singletask-paws', '--cuda', '2,3', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', '--model', 'google/t5-v1_1-base', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 87918
Killing subprocess 87919
++++++++++++++++++++++++++++++
kill: (87925): No such process
t5base para maml upstream
06/19/2022 19:45:40 - INFO - __main__ - Namespace(train_dir='data', predict_dir='data', identifier='base', output_dir='models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base', do_train=True, do_predict=False, inner_bsz=2, inner_lr=3e-05, checkpoint=None, do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=64, num_beams=4, append_another_bos=False, train_batch_size=1, predict_batch_size=1, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=2, num_train_epochs=120.0, warmup_steps=360, total_steps=5000, wait_step=10000000000, verbose=False, eval_period=10, prefix='', debug=False, seed=42, custom_tasks_splits='./dataloader/custom_tasks_splits/train_nonparaphrase_classification_test_paraphrase.json', cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=-1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2')
06/19/2022 19:45:40 - INFO - __main__ - models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base
06/19/2022 19:45:40 - INFO - __main__ - args.device: cuda
06/19/2022 19:45:40 - INFO - __main__ - Using 1 gpus
06/19/2022 19:45:41 - INFO - __main__ - Training on the following tasks: ['ade_corpus_v2-classification', 'ag_news', 'amazon_polarity', 'anli', 'circa', 'climate_fever', 'dbpedia_14', 'discovery', 'emo', 'emotion', 'ethos-directed_vs_generalized', 'ethos-disability', 'ethos-gender', 'ethos-national_origin', 'ethos-race', 'ethos-religion', 'ethos-sexual_orientation', 'financial_phrasebank', 'glue-cola', 'glue-mnli', 'glue-qnli', 'glue-rte', 'glue-sst2', 'glue-wnli', 'google_wellformed_query', 'hate_speech18', 'hate_speech_offensive', 'hatexplain', 'health_fact', 'imdb', 'kilt_fever', 'liar', 'onestop_english', 'poem_sentiment', 'rotten_tomatoes', 'scicite', 'scitail', 'sick', 'sms_spam', 'superglue-cb', 'superglue-rte', 'superglue-wic', 'superglue-wsc', 'tab_fact', 'trec', 'trec-finegrained', 'tweet_eval-emoji', 'tweet_eval-emotion', 'tweet_eval-hate', 'tweet_eval-irony', 'tweet_eval-offensive', 'tweet_eval-sentiment', 'tweet_eval-stance_abortion', 'tweet_eval-stance_atheism', 'tweet_eval-stance_climate', 'tweet_eval-stance_feminist', 'tweet_eval-stance_hillary', 'wiki_qa', 'yahoo_answers_topics', 'yelp_polarity']
06/19/2022 19:45:44 - INFO - __main__ - Start tokenizing ... 300 instances
06/19/2022 19:45:44 - INFO - __main__ - Printing 3 examples
06/19/2022 19:45:44 - INFO - __main__ -  [ade_corpus_v2-classification] The treatment of FMF attacks in patients who cannot use colchicine is an important problem.
06/19/2022 19:45:44 - INFO - __main__ -  Not Related
06/19/2022 19:45:44 - INFO - __main__ -  [ade_corpus_v2-classification] The origin of NIS is outlined briefly and some fundamental clinical and experimental facts are presented, all of which stress the importance of the acute blockade of postsynaptic DA-ergic receptors.
06/19/2022 19:45:44 - INFO - __main__ -  Not Related
06/19/2022 19:45:44 - INFO - __main__ -  [ade_corpus_v2-classification] His AFP was initially 9828 microg/L and rapidly dropped to 5597 microg/L in ten days after oral sorafenib treatment.
06/19/2022 19:45:44 - INFO - __main__ -  Not Related
06/19/2022 19:45:44 - INFO - __main__ - Tokenizing Train Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:46:01 - INFO - __main__ - Tokenizing Train Output ...
06/19/2022 19:46:11 - INFO - __main__ - Tokenizing Dev Input ...
06/19/2022 19:46:28 - INFO - __main__ - Tokenizing Dev Output ...
06/19/2022 19:47:17 - INFO - __main__ - Loaded 300 examples from train data
06/19/2022 19:47:23 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 19:47:27 - INFO - __main__ - Starting training!
use RandomSampler
initialize from c4
[(1219, 784566), (205, 3175490), (3178, 338316), (2767, 377111), (2421, 410967), (1481, 640748), (924, 843157), (5916, 171503), (566, 946306), (5534, 158824), (4682, 222111), (360, 2496612), (333, 2670525), (958, 890939), (2537, 417981), (2569, 396692), (5644, 185115), (6480, 156027), (326, 2947559), (6313, 166539), (2123, 459189), (3488, 171592), (4602, 223532), (1752, 415773), (3177, 209086), (6847, 159087), (3213, 334916), (68, 12242894), (1729, 565725), (3341, 221693), (3798, 276026), (3041, 335480), (986, 581568), (2389, 425302), (3832, 279541), (1023, 843875), (725, 898187), (4260, 246836), (1139, 870475), (4205, 261446)]
Epoch 0:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 0:   0%|          | 1/300 [00:01<05:25,  1.09s/it]Epoch 0:   1%|          | 2/300 [00:01<03:51,  1.29it/s]Epoch 0:   1%|          | 3/300 [00:02<03:07,  1.58it/s]Epoch 0:   1%|▏         | 4/300 [00:02<02:52,  1.71it/s]Epoch 0:   2%|▏         | 5/300 [00:03<02:53,  1.70it/s]Epoch 0:   2%|▏         | 6/300 [00:03<02:51,  1.72it/s]Epoch 0:   2%|▏         | 7/300 [00:04<02:56,  1.66it/s]Epoch 0:   3%|▎         | 8/300 [00:04<02:44,  1.77it/s]Epoch 0:   3%|▎         | 9/300 [00:05<02:44,  1.77it/s]Epoch 0:   3%|▎         | 10/300 [00:06<02:44,  1.76it/s]Epoch 0:   4%|▎         | 11/300 [00:06<02:35,  1.86it/s]Epoch 0:   4%|▍         | 12/300 [00:07<02:37,  1.83it/s]Epoch 0:   4%|▍         | 13/300 [00:07<02:34,  1.86it/s]Epoch 0:   5%|▍         | 14/300 [00:08<02:40,  1.79it/s]Epoch 0:   5%|▌         | 15/300 [00:08<02:34,  1.85it/s]Epoch 0:   5%|▌         | 16/300 [00:09<02:38,  1.80it/s]Epoch 0:   6%|▌         | 17/300 [00:09<02:37,  1.79it/s]Epoch 0:   6%|▌         | 18/300 [00:10<02:30,  1.87it/s]Epoch 0:   6%|▋         | 19/300 [00:10<02:34,  1.82it/s]06/19/2022 19:47:39 - INFO - __main__ - global step: 10; train loss: 7.785744667053223; dev loss: 7.824021816253662
Epoch 0:   7%|▋         | 20/300 [00:11<02:27,  1.90it/s]Epoch 0:   7%|▋         | 21/300 [00:11<02:22,  1.96it/s]Epoch 0:   7%|▋         | 22/300 [00:12<02:19,  2.00it/s]Epoch 0:   8%|▊         | 23/300 [00:13<02:30,  1.84it/s]Epoch 0:   8%|▊         | 24/300 [00:13<02:24,  1.90it/s]Epoch 0:   8%|▊         | 25/300 [00:13<02:20,  1.96it/s]Epoch 0:   9%|▊         | 26/300 [00:14<02:17,  1.99it/s]Epoch 0:   9%|▉         | 27/300 [00:15<02:33,  1.78it/s]Epoch 0:   9%|▉         | 28/300 [00:15<02:26,  1.86it/s]Epoch 0:  10%|▉         | 29/300 [00:16<02:21,  1.92it/s]Epoch 0:  10%|█         | 30/300 [00:16<02:17,  1.96it/s]Epoch 0:  10%|█         | 31/300 [00:17<02:29,  1.80it/s]Epoch 0:  11%|█         | 32/300 [00:17<02:29,  1.80it/s]Epoch 0:  11%|█         | 33/300 [00:18<02:22,  1.88it/s]Epoch 0:  11%|█▏        | 34/300 [00:18<02:17,  1.93it/s]Epoch 0:  12%|█▏        | 35/300 [00:19<02:21,  1.87it/s]Epoch 0:  12%|█▏        | 36/300 [00:19<02:23,  1.84it/s]Epoch 0:  12%|█▏        | 37/300 [00:20<02:18,  1.91it/s]Epoch 0:  13%|█▎        | 38/300 [00:20<02:13,  1.96it/s]Epoch 0:  13%|█▎        | 39/300 [00:21<02:19,  1.87it/s]06/19/2022 19:47:49 - INFO - __main__ - global step: 20; train loss: 8.087076187133789; dev loss: 7.937912940979004
Epoch 0:  13%|█▎        | 40/300 [00:21<02:14,  1.94it/s]Epoch 0:  14%|█▎        | 41/300 [00:22<02:10,  1.98it/s]Epoch 0:  14%|█▍        | 42/300 [00:22<02:07,  2.02it/s]Epoch 0:  14%|█▍        | 43/300 [00:23<02:19,  1.84it/s]Epoch 0:  15%|█▍        | 44/300 [00:24<02:13,  1.91it/s]Epoch 0:  15%|█▌        | 45/300 [00:24<02:16,  1.86it/s]Epoch 0:  15%|█▌        | 46/300 [00:25<02:18,  1.83it/s]Epoch 0:  16%|█▌        | 47/300 [00:25<02:13,  1.90it/s]Epoch 0:  16%|█▌        | 48/300 [00:26<02:15,  1.85it/s]Epoch 0:  16%|█▋        | 49/300 [00:26<02:10,  1.92it/s]Epoch 0:  17%|█▋        | 50/300 [00:27<02:16,  1.83it/s]Epoch 0:  17%|█▋        | 51/300 [00:27<02:11,  1.90it/s]Epoch 0:  17%|█▋        | 52/300 [00:28<02:20,  1.77it/s]Epoch 0:  18%|█▊        | 53/300 [00:28<02:13,  1.86it/s]Epoch 0:  18%|█▊        | 54/300 [00:29<02:08,  1.92it/s]Epoch 0:  18%|█▊        | 55/300 [00:29<02:10,  1.87it/s]Epoch 0:  19%|█▊        | 56/300 [00:30<02:19,  1.75it/s]Epoch 0:  19%|█▉        | 57/300 [00:31<02:12,  1.84it/s]Epoch 0:  19%|█▉        | 58/300 [00:31<02:08,  1.89it/s]Epoch 0:  20%|█▉        | 59/300 [00:32<02:04,  1.94it/s]06/19/2022 19:48:00 - INFO - __main__ - global step: 30; train loss: 8.031521797180176; dev loss: 8.411349296569824
Epoch 0:  20%|██        | 60/300 [00:32<02:07,  1.88it/s]Epoch 0:  20%|██        | 61/300 [00:33<02:03,  1.93it/s]Epoch 0:  21%|██        | 62/300 [00:33<02:04,  1.92it/s]Epoch 0:  21%|██        | 63/300 [00:34<02:00,  1.97it/s]Epoch 0:  21%|██▏       | 64/300 [00:34<02:09,  1.82it/s]Epoch 0:  22%|██▏       | 65/300 [00:35<02:04,  1.89it/s]Epoch 0:  22%|██▏       | 66/300 [00:35<02:01,  1.93it/s]Epoch 0:  22%|██▏       | 67/300 [00:36<01:57,  1.97it/s]Epoch 0:  23%|██▎       | 68/300 [00:36<02:02,  1.89it/s]Epoch 0:  23%|██▎       | 69/300 [00:37<02:04,  1.85it/s]Epoch 0:  23%|██▎       | 70/300 [00:37<02:01,  1.89it/s]Epoch 0:  24%|██▎       | 71/300 [00:38<02:03,  1.85it/s]Epoch 0:  24%|██▍       | 72/300 [00:38<01:58,  1.92it/s]Epoch 0:  24%|██▍       | 73/300 [00:39<02:02,  1.86it/s]Epoch 0:  25%|██▍       | 74/300 [00:39<02:00,  1.88it/s]Epoch 0:  25%|██▌       | 75/300 [00:40<01:56,  1.94it/s]Epoch 0:  25%|██▌       | 76/300 [00:40<01:52,  1.99it/s]Epoch 0:  26%|██▌       | 77/300 [00:41<01:55,  1.92it/s]Epoch 0:  26%|██▌       | 78/300 [00:42<01:58,  1.87it/s]Epoch 0:  26%|██▋       | 79/300 [00:42<01:56,  1.89it/s]06/19/2022 19:48:10 - INFO - __main__ - global step: 40; train loss: 7.600039482116699; dev loss: 7.471343994140625
Epoch 0:  27%|██▋       | 80/300 [00:43<01:53,  1.95it/s]Epoch 0:  27%|██▋       | 81/300 [00:43<01:58,  1.85it/s]Epoch 0:  27%|██▋       | 82/300 [00:44<01:53,  1.92it/s]Epoch 0:  28%|██▊       | 83/300 [00:44<01:51,  1.94it/s]Epoch 0:  28%|██▊       | 84/300 [00:45<01:51,  1.94it/s]Epoch 0:  28%|██▊       | 85/300 [00:45<02:00,  1.79it/s]Epoch 0:  29%|██▊       | 86/300 [00:46<02:00,  1.78it/s]Epoch 0:  29%|██▉       | 87/300 [00:46<01:54,  1.86it/s]Epoch 0:  29%|██▉       | 88/300 [00:47<01:56,  1.82it/s]Epoch 0:  30%|██▉       | 89/300 [00:48<01:57,  1.80it/s]Epoch 0:  30%|███       | 90/300 [00:48<01:51,  1.88it/s]Epoch 0:  30%|███       | 91/300 [00:49<01:50,  1.90it/s]Epoch 0:  31%|███       | 92/300 [00:49<01:46,  1.95it/s]Epoch 0:  31%|███       | 93/300 [00:50<01:50,  1.88it/s]Epoch 0:  31%|███▏      | 94/300 [00:50<01:48,  1.91it/s]Epoch 0:  32%|███▏      | 95/300 [00:51<01:50,  1.86it/s]Epoch 0:  32%|███▏      | 96/300 [00:51<01:46,  1.92it/s]Epoch 0:  32%|███▏      | 97/300 [00:52<01:48,  1.86it/s]Epoch 0:  33%|███▎      | 98/300 [00:52<01:45,  1.92it/s]Epoch 0:  33%|███▎      | 99/300 [00:53<01:42,  1.97it/s]06/19/2022 19:48:21 - INFO - __main__ - global step: 50; train loss: 8.614636421203613; dev loss: 8.639716148376465
Epoch 0:  33%|███▎      | 100/300 [00:53<01:41,  1.97it/s]Epoch 0:  34%|███▎      | 101/300 [00:54<01:39,  2.01it/s]Epoch 0:  34%|███▍      | 102/300 [00:54<01:49,  1.80it/s]Epoch 0:  34%|███▍      | 103/300 [00:55<01:44,  1.88it/s]Epoch 0:  35%|███▍      | 104/300 [00:55<01:43,  1.90it/s]Epoch 0:  35%|███▌      | 105/300 [00:56<01:40,  1.95it/s]Epoch 0:  35%|███▌      | 106/300 [00:56<01:44,  1.85it/s]Epoch 0:  36%|███▌      | 107/300 [00:57<01:40,  1.92it/s]Epoch 0:  36%|███▌      | 108/300 [00:57<01:42,  1.87it/s]Epoch 0:  36%|███▋      | 109/300 [00:58<01:39,  1.92it/s]Epoch 0:  37%|███▋      | 110/300 [00:59<01:41,  1.87it/s]Epoch 0:  37%|███▋      | 111/300 [00:59<01:39,  1.91it/s]Epoch 0:  37%|███▋      | 112/300 [00:59<01:35,  1.96it/s]Epoch 0:  38%|███▊      | 113/300 [01:00<01:34,  1.99it/s]Epoch 0:  38%|███▊      | 114/300 [01:01<01:42,  1.82it/s]Epoch 0:  38%|███▊      | 115/300 [01:01<01:41,  1.83it/s]Epoch 0:  39%|███▊      | 116/300 [01:02<01:38,  1.87it/s]Epoch 0:  39%|███▉      | 117/300 [01:02<01:35,  1.92it/s]Epoch 0:  39%|███▉      | 118/300 [01:03<01:37,  1.86it/s]Epoch 0:  40%|███▉      | 119/300 [01:03<01:34,  1.91it/s]06/19/2022 19:48:31 - INFO - __main__ - global step: 60; train loss: 8.026849746704102; dev loss: 8.146833419799805
Epoch 0:  40%|████      | 120/300 [01:04<01:35,  1.89it/s]Epoch 0:  40%|████      | 121/300 [01:04<01:31,  1.95it/s]Epoch 0:  41%|████      | 122/300 [01:05<01:34,  1.88it/s]Epoch 0:  41%|████      | 123/300 [01:05<01:31,  1.93it/s]Epoch 0:  41%|████▏     | 124/300 [01:06<01:29,  1.98it/s]Epoch 0:  42%|████▏     | 125/300 [01:06<01:27,  2.01it/s]Epoch 0:  42%|████▏     | 126/300 [01:07<01:25,  2.03it/s]Epoch 0:  42%|████▏     | 127/300 [01:07<01:30,  1.91it/s]Epoch 0:  43%|████▎     | 128/300 [01:08<01:32,  1.86it/s]Epoch 0:  43%|████▎     | 129/300 [01:08<01:28,  1.92it/s]Epoch 0:  43%|████▎     | 130/300 [01:09<01:26,  1.97it/s]Epoch 0:  44%|████▎     | 131/300 [01:09<01:28,  1.90it/s]Epoch 0:  44%|████▍     | 132/300 [01:10<01:26,  1.95it/s]Epoch 0:  44%|████▍     | 133/300 [01:10<01:23,  1.99it/s]Epoch 0:  45%|████▍     | 134/300 [01:11<01:23,  2.00it/s]Epoch 0:  45%|████▌     | 135/300 [01:12<01:27,  1.89it/s]Epoch 0:  45%|████▌     | 136/300 [01:12<01:24,  1.94it/s]Epoch 0:  46%|████▌     | 137/300 [01:13<01:28,  1.85it/s]Epoch 0:  46%|████▌     | 138/300 [01:13<01:24,  1.92it/s]Epoch 0:  46%|████▋     | 139/300 [01:14<01:26,  1.86it/s]06/19/2022 19:48:42 - INFO - __main__ - global step: 70; train loss: 8.829087257385254; dev loss: 8.688482284545898
Epoch 0:  47%|████▋     | 140/300 [01:14<01:25,  1.88it/s]Epoch 0:  47%|████▋     | 141/300 [01:15<01:24,  1.87it/s]Epoch 0:  47%|████▋     | 142/300 [01:15<01:24,  1.88it/s]Epoch 0:  48%|████▊     | 143/300 [01:16<01:26,  1.82it/s]Epoch 0:  48%|████▊     | 144/300 [01:16<01:22,  1.89it/s]Epoch 0:  48%|████▊     | 145/300 [01:17<01:19,  1.94it/s]Epoch 0:  49%|████▊     | 146/300 [01:17<01:21,  1.88it/s]Epoch 0:  49%|████▉     | 147/300 [01:18<01:23,  1.83it/s]Epoch 0:  49%|████▉     | 148/300 [01:18<01:20,  1.90it/s]Epoch 0:  50%|████▉     | 149/300 [01:19<01:19,  1.90it/s]Epoch 0:  50%|█████     | 150/300 [01:19<01:20,  1.86it/s]Epoch 0:  50%|█████     | 151/300 [01:20<01:23,  1.79it/s]Epoch 0:  51%|█████     | 152/300 [01:21<01:19,  1.87it/s]Epoch 0:  51%|█████     | 153/300 [01:21<01:16,  1.93it/s]Epoch 0:  51%|█████▏    | 154/300 [01:22<01:14,  1.97it/s]Epoch 0:  52%|█████▏    | 155/300 [01:22<01:14,  1.94it/s]Epoch 0:  52%|█████▏    | 156/300 [01:23<01:21,  1.78it/s]Epoch 0:  52%|█████▏    | 157/300 [01:23<01:17,  1.84it/s]Epoch 0:  53%|█████▎    | 158/300 [01:24<01:18,  1.82it/s]Epoch 0:  53%|█████▎    | 159/300 [01:24<01:17,  1.83it/s]06/19/2022 19:48:53 - INFO - __main__ - global step: 80; train loss: 7.567385196685791; dev loss: 7.618015289306641
Epoch 0:  53%|█████▎    | 160/300 [01:25<01:17,  1.80it/s]Epoch 0:  54%|█████▎    | 161/300 [01:26<01:17,  1.79it/s]Epoch 0:  54%|█████▍    | 162/300 [01:26<01:13,  1.87it/s]Epoch 0:  54%|█████▍    | 163/300 [01:26<01:11,  1.93it/s]Epoch 0:  55%|█████▍    | 164/300 [01:27<01:12,  1.88it/s]Epoch 0:  55%|█████▌    | 165/300 [01:28<01:09,  1.93it/s]Epoch 0:  55%|█████▌    | 166/300 [01:28<01:12,  1.85it/s]Epoch 0:  56%|█████▌    | 167/300 [01:29<01:09,  1.91it/s]Epoch 0:  56%|█████▌    | 168/300 [01:29<01:11,  1.86it/s]Epoch 0:  56%|█████▋    | 169/300 [01:30<01:11,  1.83it/s]Epoch 0:  57%|█████▋    | 170/300 [01:30<01:09,  1.88it/s]Epoch 0:  57%|█████▋    | 171/300 [01:31<01:06,  1.94it/s]Epoch 0:  57%|█████▋    | 172/300 [01:31<01:08,  1.87it/s]Epoch 0:  58%|█████▊    | 173/300 [01:32<01:05,  1.94it/s]Epoch 0:  58%|█████▊    | 174/300 [01:32<01:04,  1.97it/s]Epoch 0:  58%|█████▊    | 175/300 [01:33<01:02,  2.00it/s]Epoch 0:  59%|█████▊    | 176/300 [01:33<01:05,  1.90it/s]Epoch 0:  59%|█████▉    | 177/300 [01:34<01:03,  1.95it/s]Epoch 0:  59%|█████▉    | 178/300 [01:34<01:01,  1.99it/s]Epoch 0:  60%|█████▉    | 179/300 [01:35<01:01,  1.97it/s]06/19/2022 19:49:03 - INFO - __main__ - global step: 90; train loss: 8.460450172424316; dev loss: 8.617193222045898
Epoch 0:  60%|██████    | 180/300 [01:35<00:59,  2.00it/s]Epoch 0:  60%|██████    | 181/300 [01:36<01:01,  1.92it/s]Epoch 0:  61%|██████    | 182/300 [01:36<01:02,  1.88it/s]Epoch 0:  61%|██████    | 183/300 [01:37<01:01,  1.91it/s]Epoch 0:  61%|██████▏   | 184/300 [01:37<00:59,  1.96it/s]Epoch 0:  62%|██████▏   | 185/300 [01:38<01:00,  1.90it/s]Epoch 0:  62%|██████▏   | 186/300 [01:38<00:58,  1.95it/s]Epoch 0:  62%|██████▏   | 187/300 [01:39<00:56,  1.99it/s]Epoch 0:  63%|██████▎   | 188/300 [01:39<00:55,  2.01it/s]Epoch 0:  63%|██████▎   | 189/300 [01:40<00:57,  1.93it/s]Epoch 0:  63%|██████▎   | 190/300 [01:40<00:56,  1.96it/s]Epoch 0:  64%|██████▎   | 191/300 [01:41<00:57,  1.90it/s]Epoch 0:  64%|██████▍   | 192/300 [01:42<00:58,  1.86it/s]Epoch 0:  64%|██████▍   | 193/300 [01:42<00:59,  1.79it/s]Epoch 0:  65%|██████▍   | 194/300 [01:43<00:56,  1.88it/s]Epoch 0:  65%|██████▌   | 195/300 [01:43<00:54,  1.93it/s]Epoch 0:  65%|██████▌   | 196/300 [01:44<00:52,  1.97it/s]Epoch 0:  66%|██████▌   | 197/300 [01:44<00:56,  1.82it/s]Epoch 0:  66%|██████▌   | 198/300 [01:45<00:54,  1.89it/s]Epoch 0:  66%|██████▋   | 199/300 [01:45<00:51,  1.94it/s]06/19/2022 19:49:13 - INFO - __main__ - global step: 100; train loss: 7.776520729064941; dev loss: 7.456794738769531
Epoch 0:  67%|██████▋   | 200/300 [01:46<00:50,  1.99it/s]Epoch 0:  67%|██████▋   | 201/300 [01:46<00:52,  1.90it/s]Epoch 0:  67%|██████▋   | 202/300 [01:47<00:50,  1.95it/s]Epoch 0:  68%|██████▊   | 203/300 [01:47<00:48,  1.98it/s]Epoch 0:  68%|██████▊   | 204/300 [01:48<00:50,  1.91it/s]Epoch 0:  68%|██████▊   | 205/300 [01:48<00:51,  1.85it/s]Epoch 0:  69%|██████▊   | 206/300 [01:49<00:51,  1.83it/s]Epoch 0:  69%|██████▉   | 207/300 [01:49<00:49,  1.89it/s]Epoch 0:  69%|██████▉   | 208/300 [01:50<00:47,  1.93it/s]Epoch 0:  70%|██████▉   | 209/300 [01:50<00:46,  1.97it/s]Epoch 0:  70%|███████   | 210/300 [01:51<00:48,  1.87it/s]Epoch 0:  70%|███████   | 211/300 [01:51<00:45,  1.94it/s]Epoch 0:  71%|███████   | 212/300 [01:52<00:45,  1.93it/s]Epoch 0:  71%|███████   | 213/300 [01:53<00:46,  1.89it/s]Epoch 0:  71%|███████▏  | 214/300 [01:53<00:46,  1.85it/s]Epoch 0:  72%|███████▏  | 215/300 [01:54<00:44,  1.91it/s]Epoch 0:  72%|███████▏  | 216/300 [01:54<00:43,  1.91it/s]Epoch 0:  72%|███████▏  | 217/300 [01:55<00:42,  1.96it/s]Epoch 0:  73%|███████▎  | 218/300 [01:55<00:43,  1.89it/s]Epoch 0:  73%|███████▎  | 219/300 [01:56<00:43,  1.85it/s]06/19/2022 19:49:24 - INFO - __main__ - global step: 110; train loss: 7.948652744293213; dev loss: 7.929149627685547
Epoch 0:  73%|███████▎  | 220/300 [01:56<00:41,  1.91it/s]Epoch 0:  74%|███████▎  | 221/300 [01:57<00:40,  1.94it/s]Epoch 0:  74%|███████▍  | 222/300 [01:57<00:44,  1.76it/s]Epoch 0:  74%|███████▍  | 223/300 [01:58<00:42,  1.81it/s]Epoch 0:  75%|███████▍  | 224/300 [01:58<00:40,  1.88it/s]Epoch 0:  75%|███████▌  | 225/300 [01:59<00:38,  1.94it/s]Epoch 0:  75%|███████▌  | 226/300 [01:59<00:39,  1.87it/s]Epoch 0:  76%|███████▌  | 227/300 [02:00<00:38,  1.90it/s]Epoch 0:  76%|███████▌  | 228/300 [02:01<00:38,  1.89it/s]Epoch 0:  76%|███████▋  | 229/300 [02:01<00:36,  1.92it/s]Epoch 0:  77%|███████▋  | 230/300 [02:02<00:37,  1.85it/s]Epoch 0:  77%|███████▋  | 231/300 [02:02<00:37,  1.82it/s]Epoch 0:  77%|███████▋  | 232/300 [02:03<00:35,  1.90it/s]Epoch 0:  78%|███████▊  | 233/300 [02:03<00:34,  1.94it/s]Epoch 0:  78%|███████▊  | 234/300 [02:04<00:33,  1.98it/s]Epoch 0:  78%|███████▊  | 235/300 [02:04<00:34,  1.90it/s]Epoch 0:  79%|███████▊  | 236/300 [02:05<00:33,  1.91it/s]Epoch 0:  79%|███████▉  | 237/300 [02:05<00:32,  1.95it/s]Epoch 0:  79%|███████▉  | 238/300 [02:06<00:31,  1.99it/s]Epoch 0:  80%|███████▉  | 239/300 [02:06<00:32,  1.89it/s]06/19/2022 19:49:35 - INFO - __main__ - global step: 120; train loss: 7.5937628746032715; dev loss: 7.6988677978515625
Epoch 0:  80%|████████  | 240/300 [02:07<00:32,  1.85it/s]Epoch 0:  80%|████████  | 241/300 [02:07<00:30,  1.92it/s]Epoch 0:  81%|████████  | 242/300 [02:08<00:30,  1.87it/s]Epoch 0:  81%|████████  | 243/300 [02:08<00:31,  1.83it/s]Epoch 0:  81%|████████▏ | 244/300 [02:09<00:29,  1.90it/s]Epoch 0:  82%|████████▏ | 245/300 [02:09<00:28,  1.90it/s]Epoch 0:  82%|████████▏ | 246/300 [02:10<00:27,  1.95it/s]Epoch 0:  82%|████████▏ | 247/300 [02:11<00:28,  1.87it/s]Epoch 0:  83%|████████▎ | 248/300 [02:11<00:26,  1.93it/s]Epoch 0:  83%|████████▎ | 249/300 [02:12<00:26,  1.95it/s]Epoch 0:  83%|████████▎ | 250/300 [02:12<00:25,  1.99it/s]Epoch 0:  84%|████████▎ | 251/300 [02:13<00:25,  1.92it/s]Epoch 0:  84%|████████▍ | 252/300 [02:13<00:24,  1.97it/s]Epoch 0:  84%|████████▍ | 253/300 [02:14<00:24,  1.95it/s]Epoch 0:  85%|████████▍ | 254/300 [02:14<00:23,  1.96it/s]Epoch 0:  85%|████████▌ | 255/300 [02:15<00:24,  1.85it/s]Epoch 0:  85%|████████▌ | 256/300 [02:15<00:24,  1.82it/s]Epoch 0:  86%|████████▌ | 257/300 [02:16<00:22,  1.88it/s]Epoch 0:  86%|████████▌ | 258/300 [02:16<00:22,  1.89it/s]Epoch 0:  86%|████████▋ | 259/300 [02:17<00:22,  1.82it/s]06/19/2022 19:49:45 - INFO - __main__ - global step: 130; train loss: 7.869990348815918; dev loss: 7.471831321716309
Epoch 0:  87%|████████▋ | 260/300 [02:17<00:22,  1.80it/s]Epoch 0:  87%|████████▋ | 261/300 [02:18<00:21,  1.79it/s]Epoch 0:  87%|████████▋ | 262/300 [02:19<00:20,  1.82it/s]Epoch 0:  88%|████████▊ | 263/300 [02:19<00:19,  1.85it/s]Epoch 0:  88%|████████▊ | 264/300 [02:20<00:20,  1.74it/s]Epoch 0:  88%|████████▊ | 265/300 [02:20<00:19,  1.79it/s]Epoch 0:  89%|████████▊ | 266/300 [02:21<00:18,  1.87it/s]Epoch 0:  89%|████████▉ | 267/300 [02:21<00:17,  1.94it/s]Epoch 0:  89%|████████▉ | 268/300 [02:22<00:17,  1.88it/s]Epoch 0:  90%|████████▉ | 269/300 [02:22<00:16,  1.93it/s]Epoch 0:  90%|█████████ | 270/300 [02:23<00:15,  1.98it/s]Epoch 0:  90%|█████████ | 271/300 [02:23<00:14,  1.95it/s]Epoch 0:  91%|█████████ | 272/300 [02:24<00:14,  1.89it/s]Epoch 0:  91%|█████████ | 273/300 [02:24<00:13,  1.93it/s]Epoch 0:  91%|█████████▏| 274/300 [02:25<00:13,  1.96it/s]Epoch 0:  92%|█████████▏| 275/300 [02:25<00:12,  1.94it/s]Epoch 0:  92%|█████████▏| 276/300 [02:26<00:12,  1.88it/s]Epoch 0:  92%|█████████▏| 277/300 [02:26<00:11,  1.93it/s]Epoch 0:  93%|█████████▎| 278/300 [02:27<00:11,  1.98it/s]Epoch 0:  93%|█████████▎| 279/300 [02:27<00:10,  1.97it/s]06/19/2022 19:49:56 - INFO - __main__ - global step: 140; train loss: 8.223700523376465; dev loss: 8.519979476928711
Epoch 0:  93%|█████████▎| 280/300 [02:28<00:10,  1.90it/s]Epoch 0:  94%|█████████▎| 281/300 [02:28<00:10,  1.86it/s]Epoch 0:  94%|█████████▍| 282/300 [02:29<00:09,  1.92it/s]Epoch 0:  94%|█████████▍| 283/300 [02:29<00:08,  1.91it/s]Epoch 0:  95%|█████████▍| 284/300 [02:30<00:08,  1.86it/s]Epoch 0:  95%|█████████▌| 285/300 [02:31<00:07,  1.88it/s]Epoch 0:  95%|█████████▌| 286/300 [02:31<00:07,  1.89it/s]Epoch 0:  96%|█████████▌| 287/300 [02:32<00:06,  1.94it/s]Epoch 0:  96%|█████████▌| 288/300 [02:32<00:06,  1.94it/s]Epoch 0:  96%|█████████▋| 289/300 [02:33<00:06,  1.83it/s]Epoch 0:  97%|█████████▋| 290/300 [02:33<00:05,  1.83it/s]Epoch 0:  97%|█████████▋| 291/300 [02:34<00:04,  1.90it/s]Epoch 0:  97%|█████████▋| 292/300 [02:34<00:04,  1.93it/s]Epoch 0:  98%|█████████▊| 293/300 [02:35<00:03,  1.86it/s]Epoch 0:  98%|█████████▊| 294/300 [02:35<00:03,  1.83it/s]Epoch 0:  98%|█████████▊| 295/300 [02:36<00:02,  1.91it/s]Epoch 0:  99%|█████████▊| 296/300 [02:36<00:02,  1.97it/s]Epoch 0:  99%|█████████▉| 297/300 [02:37<00:01,  1.91it/s]Epoch 0:  99%|█████████▉| 298/300 [02:37<00:01,  1.95it/s]Epoch 0: 100%|█████████▉| 299/300 [02:38<00:00,  2.00it/s]06/19/2022 19:50:06 - INFO - __main__ - global step: 150; train loss: 7.797752380371094; dev loss: 7.835096836090088
Epoch 0: 100%|██████████| 300/300 [02:38<00:00,  2.02it/s]Epoch 0: 100%|██████████| 300/300 [02:38<00:00,  1.89it/s]
Epoch 1:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 1:   0%|          | 1/300 [00:00<02:50,  1.75it/s]Epoch 1:   1%|          | 2/300 [00:01<02:33,  1.94it/s]Epoch 1:   1%|          | 3/300 [00:01<02:42,  1.83it/s]Epoch 1:   1%|▏         | 4/300 [00:02<02:34,  1.91it/s]Epoch 1:   2%|▏         | 5/300 [00:02<02:50,  1.73it/s]Epoch 1:   2%|▏         | 6/300 [00:03<02:39,  1.84it/s]Epoch 1:   2%|▏         | 7/300 [00:03<02:38,  1.85it/s]Epoch 1:   3%|▎         | 8/300 [00:04<02:32,  1.91it/s]Epoch 1:   3%|▎         | 9/300 [00:04<02:40,  1.81it/s]Epoch 1:   3%|▎         | 10/300 [00:05<02:32,  1.90it/s]Epoch 1:   4%|▎         | 11/300 [00:05<02:36,  1.85it/s]Epoch 1:   4%|▍         | 12/300 [00:06<02:38,  1.82it/s]Epoch 1:   4%|▍         | 13/300 [00:07<02:39,  1.80it/s]Epoch 1:   5%|▍         | 14/300 [00:07<02:40,  1.78it/s]Epoch 1:   5%|▌         | 15/300 [00:08<02:33,  1.86it/s]Epoch 1:   5%|▌         | 16/300 [00:08<02:33,  1.85it/s]Epoch 1:   6%|▌         | 17/300 [00:09<02:27,  1.92it/s]Epoch 1:   6%|▌         | 18/300 [00:09<02:32,  1.85it/s]Epoch 1:   6%|▋         | 19/300 [00:10<02:27,  1.91it/s]06/19/2022 19:50:17 - INFO - __main__ - global step: 160; train loss: 8.273641586303711; dev loss: 7.923128604888916
Epoch 1:   7%|▋         | 20/300 [00:10<02:23,  1.95it/s]Epoch 1:   7%|▋         | 21/300 [00:11<02:20,  1.99it/s]Epoch 1:   7%|▋         | 22/300 [00:11<02:25,  1.91it/s]Epoch 1:   8%|▊         | 23/300 [00:12<02:21,  1.96it/s]Epoch 1:   8%|▊         | 24/300 [00:12<02:18,  1.99it/s]Epoch 1:   8%|▊         | 25/300 [00:13<02:16,  2.02it/s]Epoch 1:   9%|▊         | 26/300 [00:13<02:23,  1.92it/s]Epoch 1:   9%|▉         | 27/300 [00:14<02:18,  1.97it/s]Epoch 1:   9%|▉         | 28/300 [00:14<02:17,  1.98it/s]Epoch 1:  10%|▉         | 29/300 [00:15<02:14,  2.01it/s]Epoch 1:  10%|█         | 30/300 [00:15<02:20,  1.92it/s]Epoch 1:  10%|█         | 31/300 [00:16<02:20,  1.92it/s]Epoch 1:  11%|█         | 32/300 [00:16<02:16,  1.96it/s]Epoch 1:  11%|█         | 33/300 [00:17<02:17,  1.94it/s]Epoch 1:  11%|█▏        | 34/300 [00:17<02:22,  1.87it/s]Epoch 1:  12%|█▏        | 35/300 [00:18<02:16,  1.94it/s]Epoch 1:  12%|█▏        | 36/300 [00:18<02:16,  1.93it/s]Epoch 1:  12%|█▏        | 37/300 [00:19<02:13,  1.97it/s]Epoch 1:  13%|█▎        | 38/300 [00:20<02:19,  1.88it/s]Epoch 1:  13%|█▎        | 39/300 [00:20<02:15,  1.93it/s]06/19/2022 19:50:27 - INFO - __main__ - global step: 170; train loss: 8.076417922973633; dev loss: 8.048284530639648
Epoch 1:  13%|█▎        | 40/300 [00:20<02:12,  1.97it/s]Epoch 1:  14%|█▎        | 41/300 [00:21<02:10,  1.98it/s]Epoch 1:  14%|█▍        | 42/300 [00:21<02:11,  1.97it/s]Epoch 1:  14%|█▍        | 43/300 [00:22<02:20,  1.84it/s]Epoch 1:  15%|█▍        | 44/300 [00:23<02:15,  1.89it/s]Epoch 1:  15%|█▌        | 45/300 [00:23<02:10,  1.95it/s]Epoch 1:  15%|█▌        | 46/300 [00:24<02:07,  1.99it/s]Epoch 1:  16%|█▌        | 47/300 [00:24<02:12,  1.91it/s]Epoch 1:  16%|█▌        | 48/300 [00:25<02:15,  1.86it/s]Epoch 1:  16%|█▋        | 49/300 [00:25<02:14,  1.87it/s]Epoch 1:  17%|█▋        | 50/300 [00:26<02:10,  1.92it/s]Epoch 1:  17%|█▋        | 51/300 [00:26<02:12,  1.88it/s]Epoch 1:  17%|█▋        | 52/300 [00:27<02:07,  1.94it/s]Epoch 1:  18%|█▊        | 53/300 [00:27<02:10,  1.89it/s]Epoch 1:  18%|█▊        | 54/300 [00:28<02:11,  1.88it/s]Epoch 1:  18%|█▊        | 55/300 [00:28<02:13,  1.84it/s]Epoch 1:  19%|█▊        | 56/300 [00:29<02:08,  1.90it/s]Epoch 1:  19%|█▉        | 57/300 [00:29<02:04,  1.95it/s]Epoch 1:  19%|█▉        | 58/300 [00:30<02:01,  1.99it/s]Epoch 1:  20%|█▉        | 59/300 [00:30<02:05,  1.92it/s]06/19/2022 19:50:37 - INFO - __main__ - global step: 180; train loss: 7.901821136474609; dev loss: 8.50146198272705
Epoch 1:  20%|██        | 60/300 [00:31<02:01,  1.97it/s]Epoch 1:  20%|██        | 61/300 [00:31<01:59,  2.01it/s]Epoch 1:  21%|██        | 62/300 [00:32<01:57,  2.03it/s]Epoch 1:  21%|██        | 63/300 [00:33<02:08,  1.84it/s]Epoch 1:  21%|██▏       | 64/300 [00:33<02:03,  1.91it/s]Epoch 1:  22%|██▏       | 65/300 [00:33<01:59,  1.97it/s]Epoch 1:  22%|██▏       | 66/300 [00:34<01:56,  2.01it/s]Epoch 1:  22%|██▏       | 67/300 [00:34<01:55,  2.01it/s]Epoch 1:  23%|██▎       | 68/300 [00:35<02:00,  1.92it/s]Epoch 1:  23%|██▎       | 69/300 [00:36<02:00,  1.92it/s]Epoch 1:  23%|██▎       | 70/300 [00:36<01:57,  1.96it/s]Epoch 1:  24%|██▎       | 71/300 [00:37<01:55,  1.98it/s]Epoch 1:  24%|██▍       | 72/300 [00:37<02:06,  1.80it/s]Epoch 1:  24%|██▍       | 73/300 [00:38<02:02,  1.86it/s]Epoch 1:  25%|██▍       | 74/300 [00:38<02:03,  1.83it/s]Epoch 1:  25%|██▌       | 75/300 [00:39<01:58,  1.90it/s]Epoch 1:  25%|██▌       | 76/300 [00:39<02:00,  1.86it/s]Epoch 1:  26%|██▌       | 77/300 [00:40<01:59,  1.87it/s]Epoch 1:  26%|██▌       | 78/300 [00:40<02:01,  1.83it/s]Epoch 1:  26%|██▋       | 79/300 [00:41<01:56,  1.90it/s]06/19/2022 19:50:48 - INFO - __main__ - global step: 190; train loss: 7.407129764556885; dev loss: 8.151780128479004
Epoch 1:  27%|██▋       | 80/300 [00:42<02:01,  1.80it/s]Epoch 1:  27%|██▋       | 81/300 [00:42<01:56,  1.88it/s]Epoch 1:  27%|██▋       | 82/300 [00:43<01:57,  1.85it/s]Epoch 1:  28%|██▊       | 83/300 [00:43<01:58,  1.83it/s]Epoch 1:  28%|██▊       | 84/300 [00:44<02:05,  1.72it/s]Epoch 1:  28%|██▊       | 85/300 [00:44<02:04,  1.73it/s]Epoch 1:  29%|██▊       | 86/300 [00:45<02:03,  1.74it/s]Epoch 1:  29%|██▉       | 87/300 [00:45<01:56,  1.82it/s]Epoch 1:  29%|██▉       | 88/300 [00:46<01:58,  1.79it/s]Epoch 1:  30%|██▉       | 89/300 [00:47<01:58,  1.78it/s]Epoch 1:  30%|███       | 90/300 [00:47<01:53,  1.85it/s]Epoch 1:  30%|███       | 91/300 [00:48<01:51,  1.87it/s]Epoch 1:  31%|███       | 92/300 [00:48<01:54,  1.82it/s]Epoch 1:  31%|███       | 93/300 [00:49<01:55,  1.79it/s]Epoch 1:  31%|███▏      | 94/300 [00:49<01:55,  1.78it/s]Epoch 1:  32%|███▏      | 95/300 [00:50<01:49,  1.87it/s]Epoch 1:  32%|███▏      | 96/300 [00:50<01:51,  1.83it/s]Epoch 1:  32%|███▏      | 97/300 [00:51<01:55,  1.76it/s]Epoch 1:  33%|███▎      | 98/300 [00:51<01:49,  1.85it/s]Epoch 1:  33%|███▎      | 99/300 [00:52<01:47,  1.87it/s]06/19/2022 19:50:59 - INFO - __main__ - global step: 200; train loss: 7.701902866363525; dev loss: 7.6174750328063965
Epoch 1:  33%|███▎      | 100/300 [00:52<01:43,  1.93it/s]Epoch 1:  34%|███▎      | 101/300 [00:53<01:45,  1.89it/s]Epoch 1:  34%|███▍      | 102/300 [00:54<01:44,  1.89it/s]Epoch 1:  34%|███▍      | 103/300 [00:54<01:40,  1.95it/s]Epoch 1:  35%|███▍      | 104/300 [00:55<01:41,  1.94it/s]Epoch 1:  35%|███▌      | 105/300 [00:55<01:46,  1.84it/s]Epoch 1:  35%|███▌      | 106/300 [00:56<01:42,  1.90it/s]Epoch 1:  36%|███▌      | 107/300 [00:56<01:38,  1.96it/s]Epoch 1:  36%|███▌      | 108/300 [00:57<01:36,  1.98it/s]Epoch 1:  36%|███▋      | 109/300 [00:57<01:42,  1.86it/s]Epoch 1:  37%|███▋      | 110/300 [00:58<01:40,  1.90it/s]Epoch 1:  37%|███▋      | 111/300 [00:58<01:36,  1.95it/s]Epoch 1:  37%|███▋      | 112/300 [00:59<01:34,  1.99it/s]Epoch 1:  38%|███▊      | 113/300 [00:59<01:38,  1.90it/s]Epoch 1:  38%|███▊      | 114/300 [01:00<01:36,  1.93it/s]Epoch 1:  38%|███▊      | 115/300 [01:00<01:33,  1.97it/s]Epoch 1:  39%|███▊      | 116/300 [01:01<01:38,  1.86it/s]Epoch 1:  39%|███▉      | 117/300 [01:01<01:40,  1.82it/s]Epoch 1:  39%|███▉      | 118/300 [01:02<01:35,  1.90it/s]Epoch 1:  40%|███▉      | 119/300 [01:02<01:37,  1.85it/s]06/19/2022 19:51:09 - INFO - __main__ - global step: 210; train loss: 7.395419120788574; dev loss: 7.4972734451293945
Epoch 1:  40%|████      | 120/300 [01:03<01:33,  1.92it/s]Epoch 1:  40%|████      | 121/300 [01:03<01:33,  1.92it/s]Epoch 1:  41%|████      | 122/300 [01:04<01:36,  1.84it/s]Epoch 1:  41%|████      | 123/300 [01:05<01:33,  1.89it/s]Epoch 1:  41%|████▏     | 124/300 [01:05<01:31,  1.92it/s]Epoch 1:  42%|████▏     | 125/300 [01:06<01:29,  1.95it/s]Epoch 1:  42%|████▏     | 126/300 [01:06<01:37,  1.78it/s]Epoch 1:  42%|████▏     | 127/300 [01:07<01:34,  1.83it/s]Epoch 1:  43%|████▎     | 128/300 [01:07<01:31,  1.88it/s]Epoch 1:  43%|████▎     | 129/300 [01:08<01:30,  1.89it/s]Epoch 1:  43%|████▎     | 130/300 [01:08<01:32,  1.84it/s]Epoch 1:  44%|████▎     | 131/300 [01:09<01:28,  1.90it/s]Epoch 1:  44%|████▍     | 132/300 [01:09<01:26,  1.95it/s]Epoch 1:  44%|████▍     | 133/300 [01:10<01:26,  1.93it/s]Epoch 1:  45%|████▍     | 134/300 [01:10<01:31,  1.81it/s]Epoch 1:  45%|████▌     | 135/300 [01:11<01:32,  1.78it/s]Epoch 1:  45%|████▌     | 136/300 [01:12<01:29,  1.84it/s]Epoch 1:  46%|████▌     | 137/300 [01:12<01:26,  1.89it/s]Epoch 1:  46%|████▌     | 138/300 [01:13<01:29,  1.82it/s]Epoch 1:  46%|████▋     | 139/300 [01:13<01:25,  1.88it/s]06/19/2022 19:51:20 - INFO - __main__ - global step: 220; train loss: 7.747069358825684; dev loss: 7.658982276916504
Epoch 1:  47%|████▋     | 140/300 [01:14<01:22,  1.93it/s]Epoch 1:  47%|████▋     | 141/300 [01:14<01:25,  1.86it/s]Epoch 1:  47%|████▋     | 142/300 [01:15<01:27,  1.81it/s]Epoch 1:  48%|████▊     | 143/300 [01:15<01:23,  1.88it/s]Epoch 1:  48%|████▊     | 144/300 [01:16<01:20,  1.93it/s]Epoch 1:  48%|████▊     | 145/300 [01:16<01:20,  1.92it/s]Epoch 1:  49%|████▊     | 146/300 [01:17<01:23,  1.84it/s]Epoch 1:  49%|████▉     | 147/300 [01:17<01:20,  1.91it/s]Epoch 1:  49%|████▉     | 148/300 [01:18<01:18,  1.94it/s]Epoch 1:  50%|████▉     | 149/300 [01:18<01:20,  1.88it/s]Epoch 1:  50%|█████     | 150/300 [01:19<01:21,  1.84it/s]Epoch 1:  50%|█████     | 151/300 [01:20<01:27,  1.70it/s]Epoch 1:  51%|█████     | 152/300 [01:20<01:27,  1.69it/s]Epoch 1:  51%|█████     | 153/300 [01:21<01:21,  1.80it/s]Epoch 1:  51%|█████▏    | 154/300 [01:21<01:18,  1.85it/s]Epoch 1:  52%|█████▏    | 155/300 [01:22<01:20,  1.81it/s]Epoch 1:  52%|█████▏    | 156/300 [01:22<01:18,  1.84it/s]Epoch 1:  52%|█████▏    | 157/300 [01:23<01:15,  1.89it/s]Epoch 1:  53%|█████▎    | 158/300 [01:23<01:13,  1.94it/s]Epoch 1:  53%|█████▎    | 159/300 [01:24<01:15,  1.86it/s]06/19/2022 19:51:31 - INFO - __main__ - global step: 230; train loss: 7.600401401519775; dev loss: 7.801268100738525
Epoch 1:  53%|█████▎    | 160/300 [01:24<01:12,  1.92it/s]Epoch 1:  54%|█████▎    | 161/300 [01:25<01:11,  1.95it/s]Epoch 1:  54%|█████▍    | 162/300 [01:25<01:09,  1.98it/s]Epoch 1:  54%|█████▍    | 163/300 [01:26<01:11,  1.92it/s]Epoch 1:  55%|█████▍    | 164/300 [01:26<01:09,  1.95it/s]Epoch 1:  55%|█████▌    | 165/300 [01:27<01:07,  1.99it/s]Epoch 1:  55%|█████▌    | 166/300 [01:27<01:06,  2.02it/s]Epoch 1:  56%|█████▌    | 167/300 [01:28<01:08,  1.94it/s]Epoch 1:  56%|█████▌    | 168/300 [01:28<01:08,  1.92it/s]Epoch 1:  56%|█████▋    | 169/300 [01:29<01:09,  1.88it/s]Epoch 1:  57%|█████▋    | 170/300 [01:30<01:10,  1.85it/s]Epoch 1:  57%|█████▋    | 171/300 [01:30<01:12,  1.77it/s]Epoch 1:  57%|█████▋    | 172/300 [01:31<01:08,  1.86it/s]Epoch 1:  58%|█████▊    | 173/300 [01:31<01:06,  1.91it/s]Epoch 1:  58%|█████▊    | 174/300 [01:32<01:04,  1.97it/s]Epoch 1:  58%|█████▊    | 175/300 [01:32<01:02,  2.01it/s]Epoch 1:  59%|█████▊    | 176/300 [01:33<01:06,  1.88it/s]Epoch 1:  59%|█████▉    | 177/300 [01:33<01:03,  1.95it/s]Epoch 1:  59%|█████▉    | 178/300 [01:34<01:01,  2.00it/s]Epoch 1:  60%|█████▉    | 179/300 [01:34<00:59,  2.04it/s]06/19/2022 19:51:41 - INFO - __main__ - global step: 240; train loss: 7.698510646820068; dev loss: 7.989853858947754
Epoch 1:  60%|██████    | 180/300 [01:35<01:02,  1.92it/s]Epoch 1:  60%|██████    | 181/300 [01:35<01:00,  1.98it/s]Epoch 1:  61%|██████    | 182/300 [01:36<00:58,  2.00it/s]Epoch 1:  61%|██████    | 183/300 [01:36<00:57,  2.04it/s]Epoch 1:  61%|██████▏   | 184/300 [01:37<01:00,  1.93it/s]Epoch 1:  62%|██████▏   | 185/300 [01:37<01:00,  1.91it/s]Epoch 1:  62%|██████▏   | 186/300 [01:38<00:59,  1.91it/s]Epoch 1:  62%|██████▏   | 187/300 [01:38<00:57,  1.97it/s]Epoch 1:  63%|██████▎   | 188/300 [01:39<01:00,  1.86it/s]Epoch 1:  63%|██████▎   | 189/300 [01:39<00:57,  1.92it/s]Epoch 1:  63%|██████▎   | 190/300 [01:40<00:58,  1.88it/s]Epoch 1:  64%|██████▎   | 191/300 [01:40<00:56,  1.94it/s]Epoch 1:  64%|██████▍   | 192/300 [01:41<00:58,  1.84it/s]Epoch 1:  64%|██████▍   | 193/300 [01:41<00:55,  1.92it/s]Epoch 1:  65%|██████▍   | 194/300 [01:42<00:56,  1.87it/s]Epoch 1:  65%|██████▌   | 195/300 [01:43<00:54,  1.93it/s]Epoch 1:  65%|██████▌   | 196/300 [01:43<00:55,  1.88it/s]Epoch 1:  66%|██████▌   | 197/300 [01:44<00:54,  1.88it/s]Epoch 1:  66%|██████▌   | 198/300 [01:44<00:54,  1.88it/s]Epoch 1:  66%|██████▋   | 199/300 [01:45<00:53,  1.90it/s]06/19/2022 19:51:52 - INFO - __main__ - global step: 250; train loss: 8.335285186767578; dev loss: 8.291464805603027
Epoch 1:  67%|██████▋   | 200/300 [01:45<00:53,  1.85it/s]Epoch 1:  67%|██████▋   | 201/300 [01:46<00:51,  1.93it/s]Epoch 1:  67%|██████▋   | 202/300 [01:46<00:49,  1.98it/s]Epoch 1:  68%|██████▊   | 203/300 [01:47<00:48,  2.02it/s]Epoch 1:  68%|██████▊   | 204/300 [01:47<00:49,  1.93it/s]Epoch 1:  68%|██████▊   | 205/300 [01:48<00:52,  1.82it/s]Epoch 1:  69%|██████▊   | 206/300 [01:48<00:49,  1.89it/s]Epoch 1:  69%|██████▉   | 207/300 [01:49<00:47,  1.95it/s]Epoch 1:  69%|██████▉   | 208/300 [01:49<00:46,  1.98it/s]Epoch 1:  70%|██████▉   | 209/300 [01:50<00:47,  1.91it/s]Epoch 1:  70%|███████   | 210/300 [01:50<00:45,  1.97it/s]Epoch 1:  70%|███████   | 211/300 [01:51<00:46,  1.90it/s]Epoch 1:  71%|███████   | 212/300 [01:51<00:45,  1.95it/s]Epoch 1:  71%|███████   | 213/300 [01:52<00:45,  1.90it/s]Epoch 1:  71%|███████▏  | 214/300 [01:52<00:44,  1.94it/s]Epoch 1:  72%|███████▏  | 215/300 [01:53<00:42,  1.99it/s]Epoch 1:  72%|███████▏  | 216/300 [01:54<00:45,  1.83it/s]Epoch 1:  72%|███████▏  | 217/300 [01:54<00:48,  1.72it/s]Epoch 1:  73%|███████▎  | 218/300 [01:55<00:47,  1.73it/s]Epoch 1:  73%|███████▎  | 219/300 [01:55<00:48,  1.66it/s]06/19/2022 19:52:03 - INFO - __main__ - global step: 260; train loss: 8.455194473266602; dev loss: 8.441169738769531
Epoch 1:  73%|███████▎  | 220/300 [01:56<00:48,  1.66it/s]Epoch 1:  74%|███████▎  | 221/300 [01:57<00:46,  1.69it/s]Epoch 1:  74%|███████▍  | 222/300 [01:57<00:43,  1.79it/s]Epoch 1:  74%|███████▍  | 223/300 [01:58<00:43,  1.79it/s]Epoch 1:  75%|███████▍  | 224/300 [01:58<00:41,  1.82it/s]Epoch 1:  75%|███████▌  | 225/300 [01:59<00:41,  1.80it/s]Epoch 1:  75%|███████▌  | 226/300 [01:59<00:41,  1.79it/s]Epoch 1:  76%|███████▌  | 227/300 [02:00<00:38,  1.87it/s]Epoch 1:  76%|███████▌  | 228/300 [02:00<00:37,  1.93it/s]Epoch 1:  76%|███████▋  | 229/300 [02:01<00:37,  1.88it/s]Epoch 1:  77%|███████▋  | 230/300 [02:02<00:40,  1.75it/s]Epoch 1:  77%|███████▋  | 231/300 [02:02<00:37,  1.84it/s]Epoch 1:  77%|███████▋  | 232/300 [02:02<00:35,  1.90it/s]Epoch 1:  78%|███████▊  | 233/300 [02:03<00:34,  1.96it/s]Epoch 1:  78%|███████▊  | 234/300 [02:04<00:34,  1.90it/s]Epoch 1:  78%|███████▊  | 235/300 [02:04<00:33,  1.96it/s]Epoch 1:  79%|███████▊  | 236/300 [02:04<00:32,  1.99it/s]Epoch 1:  79%|███████▉  | 237/300 [02:05<00:31,  2.03it/s]Epoch 1:  79%|███████▉  | 238/300 [02:06<00:32,  1.91it/s]Epoch 1:  80%|███████▉  | 239/300 [02:06<00:31,  1.96it/s]06/19/2022 19:52:13 - INFO - __main__ - global step: 270; train loss: 8.053293228149414; dev loss: 7.8505144119262695
Epoch 1:  80%|████████  | 240/300 [02:06<00:30,  1.99it/s]Epoch 1:  80%|████████  | 241/300 [02:07<00:30,  1.92it/s]Epoch 1:  81%|████████  | 242/300 [02:08<00:31,  1.82it/s]Epoch 1:  81%|████████  | 243/300 [02:08<00:30,  1.84it/s]Epoch 1:  81%|████████▏ | 244/300 [02:09<00:29,  1.92it/s]Epoch 1:  82%|████████▏ | 245/300 [02:09<00:29,  1.87it/s]Epoch 1:  82%|████████▏ | 246/300 [02:10<00:29,  1.83it/s]Epoch 1:  82%|████████▏ | 247/300 [02:10<00:27,  1.90it/s]Epoch 1:  83%|████████▎ | 248/300 [02:11<00:26,  1.96it/s]Epoch 1:  83%|████████▎ | 249/300 [02:11<00:25,  1.99it/s]Epoch 1:  83%|████████▎ | 250/300 [02:12<00:26,  1.91it/s]Epoch 1:  84%|████████▎ | 251/300 [02:12<00:25,  1.96it/s]Epoch 1:  84%|████████▍ | 252/300 [02:13<00:24,  1.99it/s]Epoch 1:  84%|████████▍ | 253/300 [02:13<00:23,  2.00it/s]Epoch 1:  85%|████████▍ | 254/300 [02:14<00:24,  1.92it/s]Epoch 1:  85%|████████▌ | 255/300 [02:14<00:22,  1.97it/s]Epoch 1:  85%|████████▌ | 256/300 [02:15<00:21,  2.00it/s]Epoch 1:  86%|████████▌ | 257/300 [02:15<00:22,  1.93it/s]Epoch 1:  86%|████████▌ | 258/300 [02:16<00:21,  1.97it/s]Epoch 1:  86%|████████▋ | 259/300 [02:16<00:22,  1.85it/s]06/19/2022 19:52:23 - INFO - __main__ - global step: 280; train loss: 8.58694839477539; dev loss: 8.356481552124023
Epoch 1:  87%|████████▋ | 260/300 [02:17<00:20,  1.91it/s]Epoch 1:  87%|████████▋ | 261/300 [02:17<00:19,  1.96it/s]Epoch 1:  87%|████████▋ | 262/300 [02:18<00:19,  1.99it/s]Epoch 1:  88%|████████▊ | 263/300 [02:19<00:20,  1.83it/s]Epoch 1:  88%|████████▊ | 264/300 [02:19<00:19,  1.89it/s]Epoch 1:  88%|████████▊ | 265/300 [02:20<00:18,  1.94it/s]Epoch 1:  89%|████████▊ | 266/300 [02:20<00:17,  1.98it/s]Epoch 1:  89%|████████▉ | 267/300 [02:21<00:17,  1.91it/s]Epoch 1:  89%|████████▉ | 268/300 [02:21<00:16,  1.96it/s]Epoch 1:  90%|████████▉ | 269/300 [02:22<00:15,  1.99it/s]Epoch 1:  90%|█████████ | 270/300 [02:22<00:15,  1.95it/s]Epoch 1:  90%|█████████ | 271/300 [02:23<00:15,  1.89it/s]Epoch 1:  91%|█████████ | 272/300 [02:23<00:15,  1.85it/s]Epoch 1:  91%|█████████ | 273/300 [02:24<00:14,  1.92it/s]Epoch 1:  91%|█████████▏| 274/300 [02:24<00:13,  1.92it/s]Epoch 1:  92%|█████████▏| 275/300 [02:25<00:13,  1.87it/s]Epoch 1:  92%|█████████▏| 276/300 [02:25<00:12,  1.93it/s]Epoch 1:  92%|█████████▏| 277/300 [02:26<00:11,  1.98it/s]Epoch 1:  93%|█████████▎| 278/300 [02:26<00:10,  2.01it/s]Epoch 1:  93%|█████████▎| 279/300 [02:27<00:10,  1.93it/s]06/19/2022 19:52:34 - INFO - __main__ - global step: 290; train loss: 8.072214126586914; dev loss: 8.370845794677734
Epoch 1:  93%|█████████▎| 280/300 [02:27<00:10,  1.97it/s]Epoch 1:  94%|█████████▎| 281/300 [02:28<00:09,  1.99it/s]Epoch 1:  94%|█████████▍| 282/300 [02:28<00:08,  2.01it/s]Epoch 1:  94%|█████████▍| 283/300 [02:29<00:08,  2.03it/s]Epoch 1:  95%|█████████▍| 284/300 [02:29<00:08,  1.92it/s]Epoch 1:  95%|█████████▌| 285/300 [02:30<00:07,  1.97it/s]Epoch 1:  95%|█████████▌| 286/300 [02:30<00:06,  2.00it/s]Epoch 1:  96%|█████████▌| 287/300 [02:31<00:06,  2.02it/s]Epoch 1:  96%|█████████▌| 288/300 [02:31<00:06,  1.88it/s]Epoch 1:  96%|█████████▋| 289/300 [02:32<00:05,  1.90it/s]Epoch 1:  97%|█████████▋| 290/300 [02:32<00:05,  1.90it/s]Epoch 1:  97%|█████████▋| 291/300 [02:33<00:04,  1.95it/s]Epoch 1:  97%|█████████▋| 292/300 [02:34<00:04,  1.80it/s]Epoch 1:  98%|█████████▊| 293/300 [02:34<00:03,  1.79it/s]Epoch 1:  98%|█████████▊| 294/300 [02:35<00:03,  1.81it/s]Epoch 1:  98%|█████████▊| 295/300 [02:35<00:02,  1.89it/s]Epoch 1:  99%|█████████▊| 296/300 [02:36<00:02,  1.84it/s]Epoch 1:  99%|█████████▉| 297/300 [02:36<00:01,  1.82it/s]Epoch 1:  99%|█████████▉| 298/300 [02:37<00:01,  1.89it/s]Epoch 1: 100%|█████████▉| 299/300 [02:37<00:00,  1.93it/s]06/19/2022 19:52:44 - INFO - __main__ - global step: 300; train loss: 7.609858512878418; dev loss: 7.838693141937256
Epoch 1: 100%|██████████| 300/300 [02:38<00:00,  1.84it/s]Epoch 1: 100%|██████████| 300/300 [02:38<00:00,  1.89it/s]
Epoch 2:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 2:   0%|          | 1/300 [00:00<02:22,  2.09it/s]Epoch 2:   1%|          | 2/300 [00:00<02:22,  2.09it/s]Epoch 2:   1%|          | 3/300 [00:01<02:22,  2.09it/s]Epoch 2:   1%|▏         | 4/300 [00:02<02:32,  1.94it/s]Epoch 2:   2%|▏         | 5/300 [00:02<02:28,  1.98it/s]Epoch 2:   2%|▏         | 6/300 [00:03<02:34,  1.91it/s]Epoch 2:   2%|▏         | 7/300 [00:03<02:29,  1.96it/s]Epoch 2:   3%|▎         | 8/300 [00:04<02:43,  1.78it/s]Epoch 2:   3%|▎         | 9/300 [00:04<02:35,  1.88it/s]Epoch 2:   3%|▎         | 10/300 [00:05<02:29,  1.94it/s]Epoch 2:   4%|▎         | 11/300 [00:05<02:29,  1.93it/s]Epoch 2:   4%|▍         | 12/300 [00:06<02:39,  1.81it/s]Epoch 2:   4%|▍         | 13/300 [00:07<02:50,  1.68it/s]Epoch 2:   5%|▍         | 14/300 [00:07<02:44,  1.73it/s]Epoch 2:   5%|▌         | 15/300 [00:08<02:39,  1.79it/s]Epoch 2:   5%|▌         | 16/300 [00:08<02:32,  1.86it/s]Epoch 2:   6%|▌         | 17/300 [00:09<02:35,  1.82it/s]Epoch 2:   6%|▌         | 18/300 [00:09<02:27,  1.91it/s]Epoch 2:   6%|▋         | 19/300 [00:10<02:22,  1.97it/s]06/19/2022 19:52:55 - INFO - __main__ - global step: 310; train loss: 7.885298728942871; dev loss: 7.76876974105835
Epoch 2:   7%|▋         | 20/300 [00:10<02:19,  2.00it/s]Epoch 2:   7%|▋         | 21/300 [00:11<02:24,  1.92it/s]Epoch 2:   7%|▋         | 22/300 [00:11<02:27,  1.88it/s]Epoch 2:   8%|▊         | 23/300 [00:12<02:30,  1.85it/s]Epoch 2:   8%|▊         | 24/300 [00:12<02:27,  1.87it/s]Epoch 2:   8%|▊         | 25/300 [00:13<02:31,  1.81it/s]Epoch 2:   9%|▊         | 26/300 [00:13<02:25,  1.88it/s]Epoch 2:   9%|▉         | 27/300 [00:14<02:21,  1.93it/s]Epoch 2:   9%|▉         | 28/300 [00:14<02:19,  1.96it/s]Epoch 2:  10%|▉         | 29/300 [00:15<02:23,  1.89it/s]Epoch 2:  10%|█         | 30/300 [00:15<02:18,  1.95it/s]Epoch 2:  10%|█         | 31/300 [00:16<02:19,  1.93it/s]Epoch 2:  11%|█         | 32/300 [00:16<02:15,  1.99it/s]Epoch 2:  11%|█         | 33/300 [00:17<02:23,  1.86it/s]Epoch 2:  11%|█▏        | 34/300 [00:17<02:18,  1.93it/s]Epoch 2:  12%|█▏        | 35/300 [00:18<02:13,  1.98it/s]Epoch 2:  12%|█▏        | 36/300 [00:18<02:14,  1.96it/s]Epoch 2:  12%|█▏        | 37/300 [00:19<02:18,  1.91it/s]Epoch 2:  13%|█▎        | 38/300 [00:20<02:21,  1.86it/s]Epoch 2:  13%|█▎        | 39/300 [00:20<02:22,  1.83it/s]06/19/2022 19:53:06 - INFO - __main__ - global step: 320; train loss: 7.888330936431885; dev loss: 7.9973931312561035
Epoch 2:  13%|█▎        | 40/300 [00:21<02:24,  1.80it/s]Epoch 2:  14%|█▎        | 41/300 [00:21<02:18,  1.88it/s]Epoch 2:  14%|█▍        | 42/300 [00:22<02:23,  1.80it/s]Epoch 2:  14%|█▍        | 43/300 [00:22<02:16,  1.88it/s]Epoch 2:  15%|█▍        | 44/300 [00:23<02:12,  1.93it/s]Epoch 2:  15%|█▌        | 45/300 [00:23<02:09,  1.96it/s]Epoch 2:  15%|█▌        | 46/300 [00:24<02:14,  1.88it/s]Epoch 2:  16%|█▌        | 47/300 [00:24<02:10,  1.94it/s]Epoch 2:  16%|█▌        | 48/300 [00:25<02:07,  1.98it/s]Epoch 2:  16%|█▋        | 49/300 [00:25<02:11,  1.91it/s]Epoch 2:  17%|█▋        | 50/300 [00:26<02:20,  1.78it/s]Epoch 2:  17%|█▋        | 51/300 [00:26<02:13,  1.87it/s]Epoch 2:  17%|█▋        | 52/300 [00:27<02:09,  1.91it/s]Epoch 2:  18%|█▊        | 53/300 [00:27<02:05,  1.97it/s]Epoch 2:  18%|█▊        | 54/300 [00:28<02:10,  1.89it/s]Epoch 2:  18%|█▊        | 55/300 [00:28<02:05,  1.95it/s]Epoch 2:  19%|█▊        | 56/300 [00:29<02:02,  1.99it/s]Epoch 2:  19%|█▉        | 57/300 [00:29<02:03,  1.96it/s]Epoch 2:  19%|█▉        | 58/300 [00:30<02:08,  1.89it/s]Epoch 2:  20%|█▉        | 59/300 [00:31<02:03,  1.95it/s]06/19/2022 19:53:16 - INFO - __main__ - global step: 330; train loss: 7.610125541687012; dev loss: 8.040542602539062
Epoch 2:  20%|██        | 60/300 [00:31<02:00,  1.99it/s]Epoch 2:  20%|██        | 61/300 [00:32<02:04,  1.92it/s]Epoch 2:  21%|██        | 62/300 [00:32<02:13,  1.78it/s]Epoch 2:  21%|██        | 63/300 [00:33<02:07,  1.86it/s]Epoch 2:  21%|██▏       | 64/300 [00:33<02:09,  1.83it/s]Epoch 2:  22%|██▏       | 65/300 [00:34<02:03,  1.90it/s]Epoch 2:  22%|██▏       | 66/300 [00:34<02:00,  1.95it/s]Epoch 2:  22%|██▏       | 67/300 [00:35<02:04,  1.88it/s]Epoch 2:  23%|██▎       | 68/300 [00:35<01:59,  1.94it/s]Epoch 2:  23%|██▎       | 69/300 [00:36<01:56,  1.98it/s]Epoch 2:  23%|██▎       | 70/300 [00:36<01:54,  2.01it/s]Epoch 2:  24%|██▎       | 71/300 [00:37<02:01,  1.89it/s]Epoch 2:  24%|██▍       | 72/300 [00:37<01:57,  1.94it/s]Epoch 2:  24%|██▍       | 73/300 [00:38<01:57,  1.92it/s]Epoch 2:  25%|██▍       | 74/300 [00:38<02:01,  1.86it/s]Epoch 2:  25%|██▌       | 75/300 [00:39<02:04,  1.81it/s]Epoch 2:  25%|██▌       | 76/300 [00:40<02:04,  1.79it/s]Epoch 2:  26%|██▌       | 77/300 [00:40<01:59,  1.86it/s]Epoch 2:  26%|██▌       | 78/300 [00:41<02:01,  1.83it/s]Epoch 2:  26%|██▋       | 79/300 [00:41<02:08,  1.72it/s]06/19/2022 19:53:27 - INFO - __main__ - global step: 340; train loss: 7.838835716247559; dev loss: 8.00422477722168
Epoch 2:  27%|██▋       | 80/300 [00:42<02:01,  1.81it/s]Epoch 2:  27%|██▋       | 81/300 [00:42<01:55,  1.89it/s]Epoch 2:  27%|██▋       | 82/300 [00:43<01:52,  1.93it/s]Epoch 2:  28%|██▊       | 83/300 [00:43<01:58,  1.83it/s]Epoch 2:  28%|██▊       | 84/300 [00:44<01:56,  1.86it/s]Epoch 2:  28%|██▊       | 85/300 [00:44<01:51,  1.93it/s]Epoch 2:  29%|██▊       | 86/300 [00:45<01:48,  1.97it/s]Epoch 2:  29%|██▉       | 87/300 [00:46<01:56,  1.82it/s]Epoch 2:  29%|██▉       | 88/300 [00:46<01:51,  1.90it/s]Epoch 2:  30%|██▉       | 89/300 [00:46<01:47,  1.96it/s]Epoch 2:  30%|███       | 90/300 [00:47<01:44,  2.00it/s]Epoch 2:  30%|███       | 91/300 [00:47<01:42,  2.04it/s]Epoch 2:  31%|███       | 92/300 [00:48<01:49,  1.90it/s]Epoch 2:  31%|███       | 93/300 [00:48<01:45,  1.96it/s]Epoch 2:  31%|███▏      | 94/300 [00:49<01:48,  1.91it/s]Epoch 2:  32%|███▏      | 95/300 [00:50<01:46,  1.92it/s]Epoch 2:  32%|███▏      | 96/300 [00:50<01:49,  1.86it/s]Epoch 2:  32%|███▏      | 97/300 [00:51<01:45,  1.92it/s]Epoch 2:  33%|███▎      | 98/300 [00:51<01:42,  1.97it/s]Epoch 2:  33%|███▎      | 99/300 [00:52<01:44,  1.93it/s]06/19/2022 19:53:37 - INFO - __main__ - global step: 350; train loss: 7.571376800537109; dev loss: 7.683268070220947
Epoch 2:  33%|███▎      | 100/300 [00:52<01:46,  1.88it/s]Epoch 2:  34%|███▎      | 101/300 [00:53<01:42,  1.94it/s]Epoch 2:  34%|███▍      | 102/300 [00:53<01:40,  1.97it/s]Epoch 2:  34%|███▍      | 103/300 [00:54<01:38,  2.01it/s]Epoch 2:  35%|███▍      | 104/300 [00:54<01:42,  1.92it/s]Epoch 2:  35%|███▌      | 105/300 [00:55<01:44,  1.86it/s]Epoch 2:  35%|███▌      | 106/300 [00:55<01:45,  1.83it/s]Epoch 2:  36%|███▌      | 107/300 [00:56<01:41,  1.91it/s]Epoch 2:  36%|███▌      | 108/300 [00:56<01:43,  1.86it/s]Epoch 2:  36%|███▋      | 109/300 [00:57<01:40,  1.91it/s]Epoch 2:  37%|███▋      | 110/300 [00:57<01:41,  1.87it/s]Epoch 2:  37%|███▋      | 111/300 [00:58<01:37,  1.93it/s]Epoch 2:  37%|███▋      | 112/300 [00:59<01:40,  1.87it/s]Epoch 2:  38%|███▊      | 113/300 [00:59<01:36,  1.94it/s]Epoch 2:  38%|███▊      | 114/300 [00:59<01:33,  1.99it/s]Epoch 2:  38%|███▊      | 115/300 [01:00<01:31,  2.03it/s]Epoch 2:  39%|███▊      | 116/300 [01:01<01:35,  1.93it/s]Epoch 2:  39%|███▉      | 117/300 [01:01<01:32,  1.98it/s]Epoch 2:  39%|███▉      | 118/300 [01:01<01:30,  2.02it/s]Epoch 2:  40%|███▉      | 119/300 [01:02<01:33,  1.95it/s]06/19/2022 19:53:47 - INFO - __main__ - global step: 360; train loss: 8.27670955657959; dev loss: 8.238856315612793
Epoch 2:  40%|████      | 120/300 [01:02<01:30,  2.00it/s]Epoch 2:  40%|████      | 121/300 [01:03<01:37,  1.83it/s]Epoch 2:  41%|████      | 122/300 [01:04<01:34,  1.88it/s]Epoch 2:  41%|████      | 123/300 [01:04<01:32,  1.92it/s]Epoch 2:  41%|████▏     | 124/300 [01:05<01:33,  1.87it/s]Epoch 2:  42%|████▏     | 125/300 [01:05<01:35,  1.84it/s]Epoch 2:  42%|████▏     | 126/300 [01:06<01:31,  1.91it/s]Epoch 2:  42%|████▏     | 127/300 [01:06<01:28,  1.96it/s]Epoch 2:  43%|████▎     | 128/300 [01:07<01:25,  2.00it/s]Epoch 2:  43%|████▎     | 129/300 [01:07<01:28,  1.92it/s]Epoch 2:  43%|████▎     | 130/300 [01:08<01:26,  1.96it/s]Epoch 2:  44%|████▎     | 131/300 [01:08<01:24,  2.00it/s]Epoch 2:  44%|████▍     | 132/300 [01:09<01:27,  1.93it/s]Epoch 2:  44%|████▍     | 133/300 [01:09<01:31,  1.83it/s]Epoch 2:  45%|████▍     | 134/300 [01:10<01:27,  1.90it/s]Epoch 2:  45%|████▌     | 135/300 [01:10<01:25,  1.92it/s]Epoch 2:  45%|████▌     | 136/300 [01:11<01:25,  1.93it/s]Epoch 2:  46%|████▌     | 137/300 [01:11<01:26,  1.87it/s]Epoch 2:  46%|████▌     | 138/300 [01:12<01:24,  1.92it/s]Epoch 2:  46%|████▋     | 139/300 [01:12<01:21,  1.97it/s]06/19/2022 19:53:58 - INFO - __main__ - global step: 370; train loss: 8.160150527954102; dev loss: 7.683879852294922
Epoch 2:  47%|████▋     | 140/300 [01:13<01:22,  1.93it/s]Epoch 2:  47%|████▋     | 141/300 [01:14<01:24,  1.88it/s]Epoch 2:  47%|████▋     | 142/300 [01:14<01:25,  1.85it/s]Epoch 2:  48%|████▊     | 143/300 [01:15<01:21,  1.92it/s]Epoch 2:  48%|████▊     | 144/300 [01:15<01:23,  1.88it/s]Epoch 2:  48%|████▊     | 145/300 [01:16<01:19,  1.94it/s]Epoch 2:  49%|████▊     | 146/300 [01:16<01:23,  1.83it/s]Epoch 2:  49%|████▉     | 147/300 [01:17<01:20,  1.91it/s]Epoch 2:  49%|████▉     | 148/300 [01:17<01:21,  1.87it/s]Epoch 2:  50%|████▉     | 149/300 [01:18<01:18,  1.92it/s]Epoch 2:  50%|█████     | 150/300 [01:18<01:20,  1.87it/s]Epoch 2:  50%|█████     | 151/300 [01:19<01:16,  1.94it/s]Epoch 2:  51%|█████     | 152/300 [01:19<01:16,  1.93it/s]Epoch 2:  51%|█████     | 153/300 [01:20<01:13,  1.99it/s]Epoch 2:  51%|█████▏    | 154/300 [01:20<01:20,  1.82it/s]Epoch 2:  52%|█████▏    | 155/300 [01:21<01:20,  1.80it/s]Epoch 2:  52%|█████▏    | 156/300 [01:21<01:16,  1.89it/s]Epoch 2:  52%|█████▏    | 157/300 [01:22<01:13,  1.95it/s]Epoch 2:  53%|█████▎    | 158/300 [01:23<01:15,  1.88it/s]Epoch 2:  53%|█████▎    | 159/300 [01:23<01:13,  1.93it/s]06/19/2022 19:54:09 - INFO - __main__ - global step: 380; train loss: 8.344537734985352; dev loss: 8.748617172241211
Epoch 2:  53%|█████▎    | 160/300 [01:24<01:16,  1.83it/s]Epoch 2:  54%|█████▎    | 161/300 [01:24<01:12,  1.91it/s]Epoch 2:  54%|█████▍    | 162/300 [01:25<01:18,  1.76it/s]Epoch 2:  54%|█████▍    | 163/300 [01:25<01:14,  1.83it/s]Epoch 2:  55%|█████▍    | 164/300 [01:26<01:12,  1.88it/s]Epoch 2:  55%|█████▌    | 165/300 [01:26<01:11,  1.89it/s]Epoch 2:  55%|█████▌    | 166/300 [01:27<01:12,  1.84it/s]Epoch 2:  56%|█████▌    | 167/300 [01:27<01:11,  1.87it/s]Epoch 2:  56%|█████▌    | 168/300 [01:28<01:08,  1.94it/s]Epoch 2:  56%|█████▋    | 169/300 [01:28<01:07,  1.93it/s]Epoch 2:  57%|█████▋    | 170/300 [01:29<01:09,  1.87it/s]Epoch 2:  57%|█████▋    | 171/300 [01:29<01:06,  1.94it/s]Epoch 2:  57%|█████▋    | 172/300 [01:30<01:04,  1.98it/s]Epoch 2:  58%|█████▊    | 173/300 [01:30<01:03,  2.01it/s]Epoch 2:  58%|█████▊    | 174/300 [01:31<01:03,  1.98it/s]Epoch 2:  58%|█████▊    | 175/300 [01:31<01:05,  1.90it/s]Epoch 2:  59%|█████▊    | 176/300 [01:32<01:03,  1.95it/s]Epoch 2:  59%|█████▉    | 177/300 [01:32<01:01,  1.99it/s]Epoch 2:  59%|█████▉    | 178/300 [01:33<01:00,  2.00it/s]Epoch 2:  60%|█████▉    | 179/300 [01:33<01:03,  1.92it/s]06/19/2022 19:54:19 - INFO - __main__ - global step: 390; train loss: 8.375528335571289; dev loss: 8.393415451049805
Epoch 2:  60%|██████    | 180/300 [01:34<01:04,  1.87it/s]Epoch 2:  60%|██████    | 181/300 [01:35<01:01,  1.93it/s]Epoch 2:  61%|██████    | 182/300 [01:35<00:59,  1.97it/s]Epoch 2:  61%|██████    | 183/300 [01:36<01:01,  1.89it/s]Epoch 2:  61%|██████▏   | 184/300 [01:36<00:59,  1.94it/s]Epoch 2:  62%|██████▏   | 185/300 [01:37<00:59,  1.94it/s]Epoch 2:  62%|██████▏   | 186/300 [01:37<00:57,  1.99it/s]Epoch 2:  62%|██████▏   | 187/300 [01:38<00:58,  1.92it/s]Epoch 2:  63%|██████▎   | 188/300 [01:38<00:56,  1.97it/s]Epoch 2:  63%|██████▎   | 189/300 [01:39<00:58,  1.91it/s]Epoch 2:  63%|██████▎   | 190/300 [01:39<00:57,  1.91it/s]Epoch 2:  64%|██████▎   | 191/300 [01:40<01:00,  1.81it/s]Epoch 2:  64%|██████▍   | 192/300 [01:40<00:58,  1.84it/s]Epoch 2:  64%|██████▍   | 193/300 [01:41<00:55,  1.91it/s]Epoch 2:  65%|██████▍   | 194/300 [01:41<00:54,  1.96it/s]Epoch 2:  65%|██████▌   | 195/300 [01:42<00:55,  1.89it/s]Epoch 2:  65%|██████▌   | 196/300 [01:42<00:53,  1.95it/s]Epoch 2:  66%|██████▌   | 197/300 [01:43<00:52,  1.96it/s]Epoch 2:  66%|██████▌   | 198/300 [01:43<00:52,  1.96it/s]Epoch 2:  66%|██████▋   | 199/300 [01:44<00:50,  1.99it/s]06/19/2022 19:54:29 - INFO - __main__ - global step: 400; train loss: 7.9515700340271; dev loss: 7.303198337554932
Epoch 2:  67%|██████▋   | 200/300 [01:45<00:54,  1.82it/s]Epoch 2:  67%|██████▋   | 201/300 [01:45<00:52,  1.89it/s]Epoch 2:  67%|██████▋   | 202/300 [01:46<00:52,  1.86it/s]Epoch 2:  68%|██████▊   | 203/300 [01:46<00:50,  1.90it/s]Epoch 2:  68%|██████▊   | 204/300 [01:47<00:53,  1.78it/s]Epoch 2:  68%|██████▊   | 205/300 [01:47<00:52,  1.82it/s]Epoch 2:  69%|██████▊   | 206/300 [01:48<00:49,  1.90it/s]Epoch 2:  69%|██████▉   | 207/300 [01:48<00:49,  1.90it/s]Epoch 2:  69%|██████▉   | 208/300 [01:49<00:51,  1.77it/s]Epoch 2:  70%|██████▉   | 209/300 [01:49<00:50,  1.82it/s]Epoch 2:  70%|███████   | 210/300 [01:50<00:47,  1.89it/s]Epoch 2:  70%|███████   | 211/300 [01:50<00:45,  1.95it/s]Epoch 2:  71%|███████   | 212/300 [01:51<00:47,  1.86it/s]Epoch 2:  71%|███████   | 213/300 [01:51<00:45,  1.93it/s]Epoch 2:  71%|███████▏  | 214/300 [01:52<00:45,  1.88it/s]Epoch 2:  72%|███████▏  | 215/300 [01:53<00:46,  1.85it/s]Epoch 2:  72%|███████▏  | 216/300 [01:53<00:46,  1.80it/s]Epoch 2:  72%|███████▏  | 217/300 [01:54<00:44,  1.88it/s]Epoch 2:  73%|███████▎  | 218/300 [01:54<00:44,  1.84it/s]Epoch 2:  73%|███████▎  | 219/300 [01:55<00:42,  1.90it/s]06/19/2022 19:54:40 - INFO - __main__ - global step: 410; train loss: 7.630319118499756; dev loss: 7.812464714050293
Epoch 2:  73%|███████▎  | 220/300 [01:55<00:43,  1.84it/s]Epoch 2:  74%|███████▎  | 221/300 [01:56<00:41,  1.91it/s]Epoch 2:  74%|███████▍  | 222/300 [01:56<00:39,  1.97it/s]Epoch 2:  74%|███████▍  | 223/300 [01:57<00:40,  1.90it/s]Epoch 2:  75%|███████▍  | 224/300 [01:57<00:41,  1.81it/s]Epoch 2:  75%|███████▌  | 225/300 [01:58<00:39,  1.89it/s]Epoch 2:  75%|███████▌  | 226/300 [01:58<00:38,  1.94it/s]Epoch 2:  76%|███████▌  | 227/300 [01:59<00:36,  1.99it/s]Epoch 2:  76%|███████▌  | 228/300 [01:59<00:37,  1.91it/s]Epoch 2:  76%|███████▋  | 229/300 [02:00<00:39,  1.82it/s]Epoch 2:  77%|███████▋  | 230/300 [02:00<00:37,  1.87it/s]Epoch 2:  77%|███████▋  | 231/300 [02:01<00:35,  1.93it/s]Epoch 2:  77%|███████▋  | 232/300 [02:01<00:34,  1.96it/s]Epoch 2:  78%|███████▊  | 233/300 [02:02<00:35,  1.89it/s]Epoch 2:  78%|███████▊  | 234/300 [02:03<00:33,  1.95it/s]Epoch 2:  78%|███████▊  | 235/300 [02:03<00:32,  1.98it/s]Epoch 2:  79%|███████▊  | 236/300 [02:03<00:31,  2.01it/s]Epoch 2:  79%|███████▉  | 237/300 [02:04<00:32,  1.93it/s]Epoch 2:  79%|███████▉  | 238/300 [02:05<00:31,  1.98it/s]Epoch 2:  80%|███████▉  | 239/300 [02:05<00:30,  2.00it/s]06/19/2022 19:54:50 - INFO - __main__ - global step: 420; train loss: 7.90817928314209; dev loss: 8.117392539978027
Epoch 2:  80%|████████  | 240/300 [02:06<00:30,  1.97it/s]Epoch 2:  80%|████████  | 241/300 [02:06<00:30,  1.90it/s]Epoch 2:  81%|████████  | 242/300 [02:07<00:29,  1.95it/s]Epoch 2:  81%|████████  | 243/300 [02:07<00:30,  1.89it/s]Epoch 2:  81%|████████▏ | 244/300 [02:08<00:30,  1.86it/s]Epoch 2:  82%|████████▏ | 245/300 [02:08<00:30,  1.82it/s]Epoch 2:  82%|████████▏ | 246/300 [02:09<00:28,  1.89it/s]Epoch 2:  82%|████████▏ | 247/300 [02:09<00:27,  1.94it/s]Epoch 2:  83%|████████▎ | 248/300 [02:10<00:26,  1.98it/s]Epoch 2:  83%|████████▎ | 249/300 [02:10<00:26,  1.90it/s]Epoch 2:  83%|████████▎ | 250/300 [02:11<00:25,  1.95it/s]Epoch 2:  84%|████████▎ | 251/300 [02:11<00:25,  1.94it/s]Epoch 2:  84%|████████▍ | 252/300 [02:12<00:24,  1.98it/s]Epoch 2:  84%|████████▍ | 253/300 [02:12<00:24,  1.92it/s]Epoch 2:  85%|████████▍ | 254/300 [02:13<00:24,  1.86it/s]Epoch 2:  85%|████████▌ | 255/300 [02:13<00:23,  1.92it/s]Epoch 2:  85%|████████▌ | 256/300 [02:14<00:23,  1.87it/s]Epoch 2:  86%|████████▌ | 257/300 [02:14<00:22,  1.93it/s]Epoch 2:  86%|████████▌ | 258/300 [02:15<00:22,  1.84it/s]Epoch 2:  86%|████████▋ | 259/300 [02:16<00:21,  1.91it/s]06/19/2022 19:55:01 - INFO - __main__ - global step: 430; train loss: 7.595591068267822; dev loss: 7.5755438804626465
Epoch 2:  87%|████████▋ | 260/300 [02:16<00:20,  1.95it/s]Epoch 2:  87%|████████▋ | 261/300 [02:16<00:19,  1.99it/s]Epoch 2:  87%|████████▋ | 262/300 [02:17<00:19,  1.92it/s]Epoch 2:  88%|████████▊ | 263/300 [02:18<00:18,  1.96it/s]Epoch 2:  88%|████████▊ | 264/300 [02:18<00:18,  1.94it/s]Epoch 2:  88%|████████▊ | 265/300 [02:19<00:17,  1.99it/s]Epoch 2:  89%|████████▊ | 266/300 [02:19<00:17,  1.92it/s]Epoch 2:  89%|████████▉ | 267/300 [02:20<00:16,  1.97it/s]Epoch 2:  89%|████████▉ | 268/300 [02:20<00:15,  2.01it/s]Epoch 2:  90%|████████▉ | 269/300 [02:21<00:15,  2.03it/s]Epoch 2:  90%|█████████ | 270/300 [02:21<00:15,  1.91it/s]Epoch 2:  90%|█████████ | 271/300 [02:22<00:14,  1.96it/s]Epoch 2:  91%|█████████ | 272/300 [02:22<00:14,  1.98it/s]Epoch 2:  91%|█████████ | 273/300 [02:23<00:13,  2.01it/s]Epoch 2:  91%|█████████▏| 274/300 [02:23<00:13,  1.93it/s]Epoch 2:  92%|█████████▏| 275/300 [02:24<00:12,  1.98it/s]Epoch 2:  92%|█████████▏| 276/300 [02:24<00:11,  2.00it/s]Epoch 2:  92%|█████████▏| 277/300 [02:25<00:11,  2.03it/s]Epoch 2:  93%|█████████▎| 278/300 [02:25<00:11,  1.93it/s]Epoch 2:  93%|█████████▎| 279/300 [02:26<00:10,  1.99it/s]06/19/2022 19:55:11 - INFO - __main__ - global step: 440; train loss: 8.106244087219238; dev loss: 8.025819778442383
Epoch 2:  93%|█████████▎| 280/300 [02:26<00:10,  1.96it/s]Epoch 2:  94%|█████████▎| 281/300 [02:27<00:09,  1.95it/s]Epoch 2:  94%|█████████▍| 282/300 [02:27<00:09,  1.98it/s]Epoch 2:  94%|█████████▍| 283/300 [02:28<00:08,  1.91it/s]Epoch 2:  95%|█████████▍| 284/300 [02:28<00:08,  1.86it/s]Epoch 2:  95%|█████████▌| 285/300 [02:29<00:07,  1.90it/s]Epoch 2:  95%|█████████▌| 286/300 [02:29<00:07,  1.92it/s]Epoch 2:  96%|█████████▌| 287/300 [02:30<00:07,  1.84it/s]Epoch 2:  96%|█████████▌| 288/300 [02:30<00:06,  1.91it/s]Epoch 2:  96%|█████████▋| 289/300 [02:31<00:05,  1.86it/s]Epoch 2:  97%|█████████▋| 290/300 [02:32<00:05,  1.82it/s]Epoch 2:  97%|█████████▋| 291/300 [02:32<00:05,  1.78it/s]Epoch 2:  97%|█████████▋| 292/300 [02:33<00:04,  1.82it/s]Epoch 2:  98%|█████████▊| 293/300 [02:33<00:03,  1.80it/s]Epoch 2:  98%|█████████▊| 294/300 [02:34<00:03,  1.79it/s]Epoch 2:  98%|█████████▊| 295/300 [02:34<00:02,  1.78it/s]Epoch 2:  99%|█████████▊| 296/300 [02:35<00:02,  1.86it/s]Epoch 2:  99%|█████████▉| 297/300 [02:35<00:01,  1.90it/s]Epoch 2:  99%|█████████▉| 298/300 [02:36<00:01,  1.95it/s]Epoch 2: 100%|█████████▉| 299/300 [02:36<00:00,  1.89it/s]06/19/2022 19:55:22 - INFO - __main__ - global step: 450; train loss: 7.927861213684082; dev loss: 7.635289192199707
Epoch 2: 100%|██████████| 300/300 [02:37<00:00,  1.95it/s]Epoch 2: 100%|██████████| 300/300 [02:37<00:00,  1.91it/s]
Epoch 3:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 3:   0%|          | 1/300 [00:00<02:50,  1.76it/s]Epoch 3:   1%|          | 2/300 [00:01<02:34,  1.93it/s]Epoch 3:   1%|          | 3/300 [00:01<02:47,  1.78it/s]Epoch 3:   1%|▏         | 4/300 [00:02<02:36,  1.89it/s]Epoch 3:   2%|▏         | 5/300 [00:02<02:30,  1.95it/s]Epoch 3:   2%|▏         | 6/300 [00:03<02:31,  1.94it/s]Epoch 3:   2%|▏         | 7/300 [00:03<02:28,  1.97it/s]Epoch 3:   3%|▎         | 8/300 [00:04<02:34,  1.89it/s]Epoch 3:   3%|▎         | 9/300 [00:04<02:37,  1.85it/s]Epoch 3:   3%|▎         | 10/300 [00:05<02:30,  1.92it/s]Epoch 3:   4%|▎         | 11/300 [00:05<02:26,  1.97it/s]Epoch 3:   4%|▍         | 12/300 [00:06<02:38,  1.82it/s]Epoch 3:   4%|▍         | 13/300 [00:06<02:32,  1.88it/s]Epoch 3:   5%|▍         | 14/300 [00:07<02:27,  1.94it/s]Epoch 3:   5%|▌         | 15/300 [00:07<02:23,  1.99it/s]Epoch 3:   5%|▌         | 16/300 [00:08<02:37,  1.80it/s]Epoch 3:   6%|▌         | 17/300 [00:09<02:33,  1.84it/s]Epoch 3:   6%|▌         | 18/300 [00:09<02:34,  1.82it/s]Epoch 3:   6%|▋         | 19/300 [00:10<02:27,  1.90it/s]06/19/2022 19:55:32 - INFO - __main__ - global step: 460; train loss: 8.072104454040527; dev loss: 8.145437240600586
Epoch 3:   7%|▋         | 20/300 [00:10<02:30,  1.86it/s]Epoch 3:   7%|▋         | 21/300 [00:11<02:28,  1.88it/s]Epoch 3:   7%|▋         | 22/300 [00:11<02:23,  1.94it/s]Epoch 3:   8%|▊         | 23/300 [00:12<02:22,  1.94it/s]Epoch 3:   8%|▊         | 24/300 [00:12<02:30,  1.83it/s]Epoch 3:   8%|▊         | 25/300 [00:13<02:24,  1.91it/s]Epoch 3:   9%|▊         | 26/300 [00:13<02:26,  1.87it/s]Epoch 3:   9%|▉         | 27/300 [00:14<02:21,  1.93it/s]Epoch 3:   9%|▉         | 28/300 [00:14<02:31,  1.79it/s]Epoch 3:  10%|▉         | 29/300 [00:15<02:28,  1.83it/s]Epoch 3:  10%|█         | 30/300 [00:15<02:22,  1.90it/s]Epoch 3:  10%|█         | 31/300 [00:16<02:17,  1.96it/s]Epoch 3:  11%|█         | 32/300 [00:16<02:21,  1.90it/s]Epoch 3:  11%|█         | 33/300 [00:17<02:23,  1.86it/s]Epoch 3:  11%|█▏        | 34/300 [00:18<02:22,  1.87it/s]Epoch 3:  12%|█▏        | 35/300 [00:18<02:17,  1.93it/s]Epoch 3:  12%|█▏        | 36/300 [00:18<02:13,  1.97it/s]Epoch 3:  12%|█▏        | 37/300 [00:19<02:19,  1.88it/s]Epoch 3:  13%|█▎        | 38/300 [00:20<02:22,  1.84it/s]Epoch 3:  13%|█▎        | 39/300 [00:20<02:16,  1.91it/s]06/19/2022 19:55:43 - INFO - __main__ - global step: 470; train loss: 7.564600467681885; dev loss: 7.793736457824707
Epoch 3:  13%|█▎        | 40/300 [00:21<02:12,  1.96it/s]Epoch 3:  14%|█▎        | 41/300 [00:21<02:17,  1.88it/s]Epoch 3:  14%|█▍        | 42/300 [00:22<02:12,  1.95it/s]Epoch 3:  14%|█▍        | 43/300 [00:22<02:12,  1.95it/s]Epoch 3:  15%|█▍        | 44/300 [00:23<02:08,  1.99it/s]Epoch 3:  15%|█▌        | 45/300 [00:23<02:13,  1.92it/s]Epoch 3:  15%|█▌        | 46/300 [00:24<02:09,  1.96it/s]Epoch 3:  16%|█▌        | 47/300 [00:24<02:06,  2.00it/s]Epoch 3:  16%|█▌        | 48/300 [00:25<02:04,  2.02it/s]Epoch 3:  16%|█▋        | 49/300 [00:25<02:09,  1.93it/s]Epoch 3:  17%|█▋        | 50/300 [00:26<02:12,  1.88it/s]Epoch 3:  17%|█▋        | 51/300 [00:26<02:07,  1.95it/s]Epoch 3:  17%|█▋        | 52/300 [00:27<02:04,  1.99it/s]Epoch 3:  18%|█▊        | 53/300 [00:27<02:09,  1.91it/s]Epoch 3:  18%|█▊        | 54/300 [00:28<02:05,  1.96it/s]Epoch 3:  18%|█▊        | 55/300 [00:28<02:10,  1.88it/s]Epoch 3:  19%|█▊        | 56/300 [00:29<02:05,  1.94it/s]Epoch 3:  19%|█▉        | 57/300 [00:29<02:08,  1.89it/s]Epoch 3:  19%|█▉        | 58/300 [00:30<02:04,  1.95it/s]Epoch 3:  20%|█▉        | 59/300 [00:30<02:03,  1.95it/s]06/19/2022 19:55:53 - INFO - __main__ - global step: 480; train loss: 7.980370998382568; dev loss: 7.8254804611206055
Epoch 3:  20%|██        | 60/300 [00:31<02:00,  1.99it/s]Epoch 3:  20%|██        | 61/300 [00:31<01:58,  2.02it/s]Epoch 3:  21%|██        | 62/300 [00:32<02:09,  1.83it/s]Epoch 3:  21%|██        | 63/300 [00:33<02:07,  1.86it/s]Epoch 3:  21%|██▏       | 64/300 [00:33<02:08,  1.83it/s]Epoch 3:  22%|██▏       | 65/300 [00:34<02:03,  1.90it/s]Epoch 3:  22%|██▏       | 66/300 [00:34<02:05,  1.86it/s]Epoch 3:  22%|██▏       | 67/300 [00:35<02:07,  1.83it/s]Epoch 3:  23%|██▎       | 68/300 [00:35<02:07,  1.82it/s]Epoch 3:  23%|██▎       | 69/300 [00:36<02:01,  1.90it/s]Epoch 3:  23%|██▎       | 70/300 [00:36<02:09,  1.78it/s]Epoch 3:  24%|██▎       | 71/300 [00:37<02:09,  1.77it/s]Epoch 3:  24%|██▍       | 72/300 [00:37<02:03,  1.85it/s]Epoch 3:  24%|██▍       | 73/300 [00:38<02:04,  1.83it/s]Epoch 3:  25%|██▍       | 74/300 [00:39<02:10,  1.73it/s]Epoch 3:  25%|██▌       | 75/300 [00:39<02:08,  1.75it/s]Epoch 3:  25%|██▌       | 76/300 [00:40<02:02,  1.84it/s]Epoch 3:  26%|██▌       | 77/300 [00:40<01:57,  1.90it/s]Epoch 3:  26%|██▌       | 78/300 [00:41<02:05,  1.77it/s]Epoch 3:  26%|██▋       | 79/300 [00:41<01:58,  1.86it/s]06/19/2022 19:56:04 - INFO - __main__ - global step: 490; train loss: 7.542109489440918; dev loss: 7.669300079345703
Epoch 3:  27%|██▋       | 80/300 [00:42<01:54,  1.92it/s]Epoch 3:  27%|██▋       | 81/300 [00:42<01:56,  1.88it/s]Epoch 3:  27%|██▋       | 82/300 [00:43<02:04,  1.75it/s]Epoch 3:  28%|██▊       | 83/300 [00:43<01:57,  1.85it/s]Epoch 3:  28%|██▊       | 84/300 [00:44<01:52,  1.93it/s]Epoch 3:  28%|██▊       | 85/300 [00:44<01:49,  1.97it/s]Epoch 3:  29%|██▊       | 86/300 [00:45<01:52,  1.90it/s]Epoch 3:  29%|██▉       | 87/300 [00:45<01:48,  1.96it/s]Epoch 3:  29%|██▉       | 88/300 [00:46<01:46,  1.99it/s]Epoch 3:  30%|██▉       | 89/300 [00:46<01:47,  1.97it/s]Epoch 3:  30%|███       | 90/300 [00:47<01:45,  2.00it/s]Epoch 3:  30%|███       | 91/300 [00:48<01:48,  1.92it/s]Epoch 3:  31%|███       | 92/300 [00:48<01:46,  1.96it/s]Epoch 3:  31%|███       | 93/300 [00:49<01:43,  1.99it/s]Epoch 3:  31%|███▏      | 94/300 [00:49<01:41,  2.02it/s]Epoch 3:  32%|███▏      | 95/300 [00:50<01:45,  1.94it/s]Epoch 3:  32%|███▏      | 96/300 [00:50<01:47,  1.89it/s]Epoch 3:  32%|███▏      | 97/300 [00:51<01:44,  1.94it/s]Epoch 3:  33%|███▎      | 98/300 [00:51<01:41,  1.99it/s]Epoch 3:  33%|███▎      | 99/300 [00:52<01:45,  1.90it/s]06/19/2022 19:56:14 - INFO - __main__ - global step: 500; train loss: 7.674355983734131; dev loss: 7.762701988220215
Epoch 3:  33%|███▎      | 100/300 [00:52<01:42,  1.96it/s]Epoch 3:  34%|███▎      | 101/300 [00:53<01:39,  2.00it/s]Epoch 3:  34%|███▍      | 102/300 [00:53<01:37,  2.03it/s]Epoch 3:  34%|███▍      | 103/300 [00:54<01:46,  1.84it/s]Epoch 3:  35%|███▍      | 104/300 [00:54<01:42,  1.92it/s]Epoch 3:  35%|███▌      | 105/300 [00:55<01:38,  1.97it/s]Epoch 3:  35%|███▌      | 106/300 [00:55<01:36,  2.01it/s]Epoch 3:  36%|███▌      | 107/300 [00:56<01:39,  1.93it/s]Epoch 3:  36%|███▌      | 108/300 [00:56<01:39,  1.93it/s]Epoch 3:  36%|███▋      | 109/300 [00:57<01:36,  1.98it/s]Epoch 3:  37%|███▋      | 110/300 [00:57<01:34,  2.01it/s]Epoch 3:  37%|███▋      | 111/300 [00:58<01:43,  1.83it/s]Epoch 3:  37%|███▋      | 112/300 [00:58<01:38,  1.91it/s]Epoch 3:  38%|███▊      | 113/300 [00:59<01:35,  1.96it/s]Epoch 3:  38%|███▊      | 114/300 [00:59<01:33,  2.00it/s]Epoch 3:  38%|███▊      | 115/300 [01:00<01:31,  2.03it/s]Epoch 3:  39%|███▊      | 116/300 [01:00<01:34,  1.94it/s]Epoch 3:  39%|███▉      | 117/300 [01:01<01:32,  1.98it/s]Epoch 3:  39%|███▉      | 118/300 [01:01<01:32,  1.96it/s]Epoch 3:  40%|███▉      | 119/300 [01:02<01:31,  1.99it/s]06/19/2022 19:56:25 - INFO - __main__ - global step: 510; train loss: 8.26481819152832; dev loss: 7.933213710784912
Epoch 3:  40%|████      | 120/300 [01:02<01:33,  1.92it/s]Epoch 3:  40%|████      | 121/300 [01:03<01:30,  1.98it/s]Epoch 3:  41%|████      | 122/300 [01:03<01:30,  1.96it/s]Epoch 3:  41%|████      | 123/300 [01:04<01:28,  1.99it/s]Epoch 3:  41%|████▏     | 124/300 [01:04<01:32,  1.89it/s]Epoch 3:  42%|████▏     | 125/300 [01:05<01:34,  1.86it/s]Epoch 3:  42%|████▏     | 126/300 [01:06<01:34,  1.84it/s]Epoch 3:  42%|████▏     | 127/300 [01:06<01:35,  1.81it/s]Epoch 3:  43%|████▎     | 128/300 [01:07<01:36,  1.78it/s]Epoch 3:  43%|████▎     | 129/300 [01:07<01:31,  1.86it/s]Epoch 3:  43%|████▎     | 130/300 [01:08<01:28,  1.92it/s]Epoch 3:  44%|████▎     | 131/300 [01:08<01:25,  1.97it/s]Epoch 3:  44%|████▍     | 132/300 [01:09<01:32,  1.81it/s]Epoch 3:  44%|████▍     | 133/300 [01:09<01:30,  1.85it/s]Epoch 3:  45%|████▍     | 134/300 [01:10<01:26,  1.92it/s]Epoch 3:  45%|████▌     | 135/300 [01:10<01:27,  1.88it/s]Epoch 3:  45%|████▌     | 136/300 [01:11<01:28,  1.85it/s]Epoch 3:  46%|████▌     | 137/300 [01:11<01:29,  1.83it/s]Epoch 3:  46%|████▌     | 138/300 [01:12<01:24,  1.91it/s]Epoch 3:  46%|████▋     | 139/300 [01:12<01:22,  1.96it/s]06/19/2022 19:56:35 - INFO - __main__ - global step: 520; train loss: 7.979351997375488; dev loss: 8.284287452697754
Epoch 3:  47%|████▋     | 140/300 [01:13<01:25,  1.87it/s]Epoch 3:  47%|████▋     | 141/300 [01:13<01:22,  1.93it/s]Epoch 3:  47%|████▋     | 142/300 [01:14<01:21,  1.93it/s]Epoch 3:  48%|████▊     | 143/300 [01:15<01:20,  1.96it/s]Epoch 3:  48%|████▊     | 144/300 [01:15<01:21,  1.91it/s]Epoch 3:  48%|████▊     | 145/300 [01:16<01:27,  1.78it/s]Epoch 3:  49%|████▊     | 146/300 [01:16<01:22,  1.87it/s]Epoch 3:  49%|████▉     | 147/300 [01:17<01:19,  1.93it/s]Epoch 3:  49%|████▉     | 148/300 [01:17<01:19,  1.92it/s]Epoch 3:  50%|████▉     | 149/300 [01:18<01:20,  1.86it/s]Epoch 3:  50%|█████     | 150/300 [01:18<01:21,  1.84it/s]Epoch 3:  50%|█████     | 151/300 [01:19<01:17,  1.91it/s]Epoch 3:  51%|█████     | 152/300 [01:19<01:14,  1.98it/s]Epoch 3:  51%|█████     | 153/300 [01:20<01:17,  1.88it/s]Epoch 3:  51%|█████▏    | 154/300 [01:20<01:15,  1.94it/s]Epoch 3:  52%|█████▏    | 155/300 [01:21<01:12,  1.99it/s]Epoch 3:  52%|█████▏    | 156/300 [01:21<01:11,  2.02it/s]Epoch 3:  52%|█████▏    | 157/300 [01:22<01:17,  1.85it/s]Epoch 3:  53%|█████▎    | 158/300 [01:22<01:14,  1.92it/s]Epoch 3:  53%|█████▎    | 159/300 [01:23<01:11,  1.96it/s]06/19/2022 19:56:46 - INFO - __main__ - global step: 530; train loss: 7.781861305236816; dev loss: 7.741183280944824
Epoch 3:  53%|█████▎    | 160/300 [01:23<01:14,  1.88it/s]Epoch 3:  54%|█████▎    | 161/300 [01:24<01:16,  1.83it/s]Epoch 3:  54%|█████▍    | 162/300 [01:25<01:12,  1.90it/s]Epoch 3:  54%|█████▍    | 163/300 [01:25<01:10,  1.95it/s]Epoch 3:  55%|█████▍    | 164/300 [01:25<01:08,  1.99it/s]Epoch 3:  55%|█████▌    | 165/300 [01:26<01:10,  1.92it/s]Epoch 3:  55%|█████▌    | 166/300 [01:27<01:08,  1.96it/s]Epoch 3:  56%|█████▌    | 167/300 [01:27<01:06,  2.00it/s]Epoch 3:  56%|█████▌    | 168/300 [01:27<01:05,  2.03it/s]Epoch 3:  56%|█████▋    | 169/300 [01:28<01:04,  2.02it/s]Epoch 3:  57%|█████▋    | 170/300 [01:29<01:10,  1.84it/s]Epoch 3:  57%|█████▋    | 171/300 [01:29<01:07,  1.91it/s]Epoch 3:  57%|█████▋    | 172/300 [01:30<01:05,  1.96it/s]Epoch 3:  58%|█████▊    | 173/300 [01:30<01:06,  1.90it/s]Epoch 3:  58%|█████▊    | 174/300 [01:31<01:07,  1.86it/s]Epoch 3:  58%|█████▊    | 175/300 [01:31<01:05,  1.92it/s]Epoch 3:  59%|█████▊    | 176/300 [01:32<01:03,  1.96it/s]Epoch 3:  59%|█████▉    | 177/300 [01:32<01:01,  2.00it/s]Epoch 3:  59%|█████▉    | 178/300 [01:33<01:06,  1.84it/s]Epoch 3:  60%|█████▉    | 179/300 [01:33<01:06,  1.82it/s]06/19/2022 19:56:56 - INFO - __main__ - global step: 540; train loss: 7.434427738189697; dev loss: 7.8107452392578125
Epoch 3:  60%|██████    | 180/300 [01:34<01:03,  1.90it/s]Epoch 3:  60%|██████    | 181/300 [01:34<01:02,  1.91it/s]Epoch 3:  61%|██████    | 182/300 [01:35<01:03,  1.85it/s]Epoch 3:  61%|██████    | 183/300 [01:35<01:02,  1.88it/s]Epoch 3:  61%|██████▏   | 184/300 [01:36<00:59,  1.93it/s]Epoch 3:  62%|██████▏   | 185/300 [01:36<00:57,  1.98it/s]Epoch 3:  62%|██████▏   | 186/300 [01:37<00:59,  1.91it/s]Epoch 3:  62%|██████▏   | 187/300 [01:38<01:00,  1.87it/s]Epoch 3:  63%|██████▎   | 188/300 [01:38<01:00,  1.84it/s]Epoch 3:  63%|██████▎   | 189/300 [01:39<00:58,  1.91it/s]Epoch 3:  63%|██████▎   | 190/300 [01:39<00:59,  1.85it/s]Epoch 3:  64%|██████▎   | 191/300 [01:40<00:56,  1.91it/s]Epoch 3:  64%|██████▍   | 192/300 [01:40<00:55,  1.96it/s]Epoch 3:  64%|██████▍   | 193/300 [01:41<00:53,  1.98it/s]Epoch 3:  65%|██████▍   | 194/300 [01:41<00:56,  1.89it/s]Epoch 3:  65%|██████▌   | 195/300 [01:42<00:54,  1.94it/s]Epoch 3:  65%|██████▌   | 196/300 [01:42<00:53,  1.93it/s]Epoch 3:  66%|██████▌   | 197/300 [01:43<00:53,  1.93it/s]Epoch 3:  66%|██████▌   | 198/300 [01:43<00:51,  1.98it/s]Epoch 3:  66%|██████▋   | 199/300 [01:44<00:52,  1.91it/s]06/19/2022 19:57:06 - INFO - __main__ - global step: 550; train loss: 8.263531684875488; dev loss: 8.24856948852539
Epoch 3:  67%|██████▋   | 200/300 [01:44<00:51,  1.95it/s]Epoch 3:  67%|██████▋   | 201/300 [01:45<00:49,  2.00it/s]Epoch 3:  67%|██████▋   | 202/300 [01:45<00:49,  1.98it/s]Epoch 3:  68%|██████▊   | 203/300 [01:46<00:50,  1.91it/s]Epoch 3:  68%|██████▊   | 204/300 [01:46<00:48,  1.96it/s]Epoch 3:  68%|██████▊   | 205/300 [01:47<00:47,  2.00it/s]Epoch 3:  69%|██████▊   | 206/300 [01:47<00:46,  2.00it/s]Epoch 3:  69%|██████▉   | 207/300 [01:48<00:48,  1.91it/s]Epoch 3:  69%|██████▉   | 208/300 [01:48<00:46,  1.97it/s]Epoch 3:  70%|██████▉   | 209/300 [01:49<00:46,  1.96it/s]Epoch 3:  70%|███████   | 210/300 [01:49<00:45,  2.00it/s]Epoch 3:  70%|███████   | 211/300 [01:50<00:47,  1.87it/s]Epoch 3:  71%|███████   | 212/300 [01:50<00:46,  1.89it/s]Epoch 3:  71%|███████   | 213/300 [01:51<00:44,  1.95it/s]Epoch 3:  71%|███████▏  | 214/300 [01:51<00:43,  1.98it/s]Epoch 3:  72%|███████▏  | 215/300 [01:52<00:44,  1.91it/s]Epoch 3:  72%|███████▏  | 216/300 [01:53<00:45,  1.86it/s]Epoch 3:  72%|███████▏  | 217/300 [01:53<00:43,  1.93it/s]Epoch 3:  73%|███████▎  | 218/300 [01:54<00:41,  1.97it/s]Epoch 3:  73%|███████▎  | 219/300 [01:54<00:43,  1.85it/s]06/19/2022 19:57:17 - INFO - __main__ - global step: 560; train loss: 7.853848934173584; dev loss: 8.441012382507324
Epoch 3:  73%|███████▎  | 220/300 [01:55<00:41,  1.91it/s]Epoch 3:  74%|███████▎  | 221/300 [01:55<00:39,  1.98it/s]Epoch 3:  74%|███████▍  | 222/300 [01:56<00:39,  1.95it/s]Epoch 3:  74%|███████▍  | 223/300 [01:56<00:38,  1.99it/s]Epoch 3:  75%|███████▍  | 224/300 [01:57<00:41,  1.82it/s]Epoch 3:  75%|███████▌  | 225/300 [01:57<00:39,  1.90it/s]Epoch 3:  75%|███████▌  | 226/300 [01:58<00:38,  1.91it/s]Epoch 3:  76%|███████▌  | 227/300 [01:58<00:39,  1.86it/s]Epoch 3:  76%|███████▌  | 228/300 [01:59<00:39,  1.83it/s]Epoch 3:  76%|███████▋  | 229/300 [01:59<00:37,  1.90it/s]Epoch 3:  77%|███████▋  | 230/300 [02:00<00:35,  1.95it/s]Epoch 3:  77%|███████▋  | 231/300 [02:00<00:36,  1.89it/s]Epoch 3:  77%|███████▋  | 232/300 [02:01<00:36,  1.85it/s]Epoch 3:  78%|███████▊  | 233/300 [02:01<00:34,  1.92it/s]Epoch 3:  78%|███████▊  | 234/300 [02:02<00:33,  1.97it/s]Epoch 3:  78%|███████▊  | 235/300 [02:02<00:34,  1.91it/s]Epoch 3:  79%|███████▊  | 236/300 [02:03<00:34,  1.85it/s]Epoch 3:  79%|███████▉  | 237/300 [02:04<00:33,  1.91it/s]Epoch 3:  79%|███████▉  | 238/300 [02:04<00:32,  1.92it/s]Epoch 3:  80%|███████▉  | 239/300 [02:05<00:32,  1.87it/s]06/19/2022 19:57:27 - INFO - __main__ - global step: 570; train loss: 8.34626579284668; dev loss: 8.739059448242188
Epoch 3:  80%|████████  | 240/300 [02:05<00:32,  1.82it/s]Epoch 3:  80%|████████  | 241/300 [02:06<00:32,  1.84it/s]Epoch 3:  81%|████████  | 242/300 [02:06<00:32,  1.81it/s]Epoch 3:  81%|████████  | 243/300 [02:07<00:30,  1.85it/s]Epoch 3:  81%|████████▏ | 244/300 [02:08<00:32,  1.73it/s]Epoch 3:  82%|████████▏ | 245/300 [02:08<00:31,  1.73it/s]Epoch 3:  82%|████████▏ | 246/300 [02:09<00:31,  1.74it/s]Epoch 3:  82%|████████▏ | 247/300 [02:09<00:29,  1.82it/s]Epoch 3:  83%|████████▎ | 248/300 [02:10<00:28,  1.80it/s]Epoch 3:  83%|████████▎ | 249/300 [02:10<00:27,  1.84it/s]Epoch 3:  83%|████████▎ | 250/300 [02:11<00:26,  1.86it/s]Epoch 3:  84%|████████▎ | 251/300 [02:11<00:25,  1.92it/s]Epoch 3:  84%|████████▍ | 252/300 [02:12<00:24,  1.97it/s]Epoch 3:  84%|████████▍ | 253/300 [02:12<00:24,  1.89it/s]Epoch 3:  85%|████████▍ | 254/300 [02:13<00:24,  1.85it/s]Epoch 3:  85%|████████▌ | 255/300 [02:13<00:23,  1.93it/s]Epoch 3:  85%|████████▌ | 256/300 [02:14<00:22,  1.99it/s]Epoch 3:  86%|████████▌ | 257/300 [02:14<00:22,  1.87it/s]Epoch 3:  86%|████████▌ | 258/300 [02:15<00:21,  1.94it/s]Epoch 3:  86%|████████▋ | 259/300 [02:15<00:20,  1.98it/s]06/19/2022 19:57:38 - INFO - __main__ - global step: 580; train loss: 8.069578170776367; dev loss: 7.7923264503479
Epoch 3:  87%|████████▋ | 260/300 [02:16<00:19,  2.00it/s]Epoch 3:  87%|████████▋ | 261/300 [02:16<00:20,  1.92it/s]Epoch 3:  87%|████████▋ | 262/300 [02:17<00:19,  1.96it/s]Epoch 3:  88%|████████▊ | 263/300 [02:17<00:18,  2.01it/s]Epoch 3:  88%|████████▊ | 264/300 [02:18<00:17,  2.02it/s]Epoch 3:  88%|████████▊ | 265/300 [02:18<00:18,  1.94it/s]Epoch 3:  89%|████████▊ | 266/300 [02:19<00:17,  1.99it/s]Epoch 3:  89%|████████▉ | 267/300 [02:19<00:17,  1.89it/s]Epoch 3:  89%|████████▉ | 268/300 [02:20<00:16,  1.95it/s]Epoch 3:  90%|████████▉ | 269/300 [02:21<00:16,  1.83it/s]Epoch 3:  90%|█████████ | 270/300 [02:21<00:16,  1.82it/s]Epoch 3:  90%|█████████ | 271/300 [02:22<00:15,  1.90it/s]Epoch 3:  91%|█████████ | 272/300 [02:22<00:14,  1.95it/s]Epoch 3:  91%|█████████ | 273/300 [02:23<00:14,  1.89it/s]Epoch 3:  91%|█████████▏| 274/300 [02:23<00:13,  1.94it/s]Epoch 3:  92%|█████████▏| 275/300 [02:24<00:12,  1.99it/s]Epoch 3:  92%|█████████▏| 276/300 [02:24<00:12,  1.96it/s]Epoch 3:  92%|█████████▏| 277/300 [02:25<00:12,  1.86it/s]Epoch 3:  93%|█████████▎| 278/300 [02:25<00:12,  1.75it/s]Epoch 3:  93%|█████████▎| 279/300 [02:26<00:11,  1.84it/s]06/19/2022 19:57:49 - INFO - __main__ - global step: 590; train loss: 7.945924282073975; dev loss: 7.692293643951416
Epoch 3:  93%|█████████▎| 280/300 [02:26<00:10,  1.91it/s]Epoch 3:  94%|█████████▎| 281/300 [02:27<00:09,  1.91it/s]Epoch 3:  94%|█████████▍| 282/300 [02:27<00:09,  1.84it/s]Epoch 3:  94%|█████████▍| 283/300 [02:28<00:09,  1.82it/s]Epoch 3:  95%|█████████▍| 284/300 [02:28<00:08,  1.90it/s]Epoch 3:  95%|█████████▌| 285/300 [02:29<00:07,  1.96it/s]Epoch 3:  95%|█████████▌| 286/300 [02:30<00:07,  1.87it/s]Epoch 3:  96%|█████████▌| 287/300 [02:30<00:06,  1.92it/s]Epoch 3:  96%|█████████▌| 288/300 [02:31<00:06,  1.96it/s]Epoch 3:  96%|█████████▋| 289/300 [02:31<00:05,  1.94it/s]Epoch 3:  97%|█████████▋| 290/300 [02:32<00:05,  1.88it/s]Epoch 3:  97%|█████████▋| 291/300 [02:32<00:04,  1.94it/s]Epoch 3:  97%|█████████▋| 292/300 [02:33<00:04,  1.98it/s]Epoch 3:  98%|█████████▊| 293/300 [02:33<00:03,  2.01it/s]Epoch 3:  98%|█████████▊| 294/300 [02:34<00:03,  1.92it/s]Epoch 3:  98%|█████████▊| 295/300 [02:34<00:02,  1.98it/s]Epoch 3:  99%|█████████▊| 296/300 [02:35<00:02,  1.96it/s]Epoch 3:  99%|█████████▉| 297/300 [02:35<00:01,  2.00it/s]Epoch 3:  99%|█████████▉| 298/300 [02:36<00:01,  1.91it/s]Epoch 3: 100%|█████████▉| 299/300 [02:36<00:00,  1.97it/s]06/19/2022 19:57:59 - INFO - __main__ - global step: 600; train loss: 7.879208564758301; dev loss: 7.88046407699585
Epoch 3: 100%|██████████| 300/300 [02:37<00:00,  2.00it/s]Epoch 3: 100%|██████████| 300/300 [02:37<00:00,  1.91it/s]
Epoch 4:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 4:   0%|          | 1/300 [00:00<02:24,  2.07it/s]Epoch 4:   1%|          | 2/300 [00:01<02:41,  1.85it/s]Epoch 4:   1%|          | 3/300 [00:01<02:35,  1.91it/s]Epoch 4:   1%|▏         | 4/300 [00:02<02:31,  1.96it/s]Epoch 4:   2%|▏         | 5/300 [00:02<02:27,  2.01it/s]Epoch 4:   2%|▏         | 6/300 [00:03<02:25,  2.02it/s]Epoch 4:   2%|▏         | 7/300 [00:03<02:32,  1.92it/s]Epoch 4:   3%|▎         | 8/300 [00:04<02:31,  1.92it/s]Epoch 4:   3%|▎         | 9/300 [00:04<02:27,  1.97it/s]Epoch 4:   3%|▎         | 10/300 [00:05<02:25,  2.00it/s]Epoch 4:   4%|▎         | 11/300 [00:05<02:38,  1.82it/s]Epoch 4:   4%|▍         | 12/300 [00:06<02:40,  1.80it/s]Epoch 4:   4%|▍         | 13/300 [00:06<02:32,  1.88it/s]Epoch 4:   5%|▍         | 14/300 [00:07<02:27,  1.93it/s]Epoch 4:   5%|▌         | 15/300 [00:07<02:35,  1.83it/s]Epoch 4:   5%|▌         | 16/300 [00:08<02:30,  1.88it/s]Epoch 4:   6%|▌         | 17/300 [00:08<02:35,  1.82it/s]Epoch 4:   6%|▌         | 18/300 [00:09<02:29,  1.88it/s]Epoch 4:   6%|▋         | 19/300 [00:10<02:41,  1.74it/s]06/19/2022 19:58:09 - INFO - __main__ - global step: 610; train loss: 7.467869758605957; dev loss: 7.685151100158691
Epoch 4:   7%|▋         | 20/300 [00:10<02:33,  1.82it/s]Epoch 4:   7%|▋         | 21/300 [00:11<02:28,  1.88it/s]Epoch 4:   7%|▋         | 22/300 [00:11<02:24,  1.93it/s]Epoch 4:   8%|▊         | 23/300 [00:12<02:30,  1.84it/s]Epoch 4:   8%|▊         | 24/300 [00:12<02:33,  1.80it/s]Epoch 4:   8%|▊         | 25/300 [00:13<02:27,  1.86it/s]Epoch 4:   9%|▊         | 26/300 [00:13<02:27,  1.86it/s]Epoch 4:   9%|▉         | 27/300 [00:14<02:31,  1.80it/s]Epoch 4:   9%|▉         | 28/300 [00:14<02:26,  1.86it/s]Epoch 4:  10%|▉         | 29/300 [00:15<02:22,  1.91it/s]Epoch 4:  10%|█         | 30/300 [00:15<02:18,  1.95it/s]Epoch 4:  10%|█         | 31/300 [00:16<02:19,  1.93it/s]Epoch 4:  11%|█         | 32/300 [00:17<02:24,  1.86it/s]Epoch 4:  11%|█         | 33/300 [00:17<02:19,  1.91it/s]Epoch 4:  11%|█▏        | 34/300 [00:17<02:16,  1.95it/s]Epoch 4:  12%|█▏        | 35/300 [00:18<02:13,  1.99it/s]Epoch 4:  12%|█▏        | 36/300 [00:19<02:24,  1.83it/s]Epoch 4:  12%|█▏        | 37/300 [00:19<02:18,  1.91it/s]Epoch 4:  13%|█▎        | 38/300 [00:20<02:13,  1.96it/s]Epoch 4:  13%|█▎        | 39/300 [00:20<02:17,  1.89it/s]06/19/2022 19:58:20 - INFO - __main__ - global step: 620; train loss: 8.243158340454102; dev loss: 7.785668849945068
Epoch 4:  13%|█▎        | 40/300 [00:21<02:21,  1.84it/s]Epoch 4:  14%|█▎        | 41/300 [00:21<02:15,  1.91it/s]Epoch 4:  14%|█▍        | 42/300 [00:22<02:13,  1.93it/s]Epoch 4:  14%|█▍        | 43/300 [00:22<02:09,  1.98it/s]Epoch 4:  15%|█▍        | 44/300 [00:23<02:14,  1.90it/s]Epoch 4:  15%|█▌        | 45/300 [00:23<02:10,  1.96it/s]Epoch 4:  15%|█▌        | 46/300 [00:24<02:07,  1.99it/s]Epoch 4:  16%|█▌        | 47/300 [00:24<02:05,  2.02it/s]Epoch 4:  16%|█▌        | 48/300 [00:25<02:10,  1.93it/s]Epoch 4:  16%|█▋        | 49/300 [00:25<02:07,  1.97it/s]Epoch 4:  17%|█▋        | 50/300 [00:26<02:04,  2.00it/s]Epoch 4:  17%|█▋        | 51/300 [00:26<02:02,  2.03it/s]Epoch 4:  17%|█▋        | 52/300 [00:27<02:09,  1.92it/s]Epoch 4:  18%|█▊        | 53/300 [00:27<02:08,  1.92it/s]Epoch 4:  18%|█▊        | 54/300 [00:28<02:04,  1.97it/s]Epoch 4:  18%|█▊        | 55/300 [00:28<02:01,  2.01it/s]Epoch 4:  19%|█▊        | 56/300 [00:29<02:06,  1.93it/s]Epoch 4:  19%|█▉        | 57/300 [00:29<02:03,  1.98it/s]Epoch 4:  19%|█▉        | 58/300 [00:30<02:00,  2.00it/s]Epoch 4:  20%|█▉        | 59/300 [00:30<01:58,  2.03it/s]06/19/2022 19:58:30 - INFO - __main__ - global step: 630; train loss: 7.7189483642578125; dev loss: 7.662240505218506
Epoch 4:  20%|██        | 60/300 [00:31<02:03,  1.94it/s]Epoch 4:  20%|██        | 61/300 [00:31<02:07,  1.88it/s]Epoch 4:  21%|██        | 62/300 [00:32<02:03,  1.92it/s]Epoch 4:  21%|██        | 63/300 [00:32<02:00,  1.97it/s]Epoch 4:  21%|██▏       | 64/300 [00:33<01:57,  2.00it/s]Epoch 4:  22%|██▏       | 65/300 [00:34<02:08,  1.83it/s]Epoch 4:  22%|██▏       | 66/300 [00:34<02:03,  1.90it/s]Epoch 4:  22%|██▏       | 67/300 [00:35<02:05,  1.86it/s]Epoch 4:  23%|██▎       | 68/300 [00:35<02:00,  1.92it/s]Epoch 4:  23%|██▎       | 69/300 [00:36<02:03,  1.87it/s]Epoch 4:  23%|██▎       | 70/300 [00:36<01:58,  1.94it/s]Epoch 4:  24%|██▎       | 71/300 [00:37<02:01,  1.88it/s]Epoch 4:  24%|██▍       | 72/300 [00:37<01:57,  1.94it/s]Epoch 4:  24%|██▍       | 73/300 [00:38<02:00,  1.88it/s]Epoch 4:  25%|██▍       | 74/300 [00:38<01:59,  1.89it/s]Epoch 4:  25%|██▌       | 75/300 [00:39<01:55,  1.95it/s]Epoch 4:  25%|██▌       | 76/300 [00:39<01:52,  1.99it/s]Epoch 4:  26%|██▌       | 77/300 [00:40<01:56,  1.91it/s]Epoch 4:  26%|██▌       | 78/300 [00:40<01:53,  1.96it/s]Epoch 4:  26%|██▋       | 79/300 [00:41<01:51,  1.99it/s]06/19/2022 19:58:41 - INFO - __main__ - global step: 640; train loss: 7.70339822769165; dev loss: 7.4850873947143555
Epoch 4:  27%|██▋       | 80/300 [00:41<01:49,  2.02it/s]Epoch 4:  27%|██▋       | 81/300 [00:42<01:55,  1.90it/s]Epoch 4:  27%|██▋       | 82/300 [00:42<01:54,  1.90it/s]Epoch 4:  28%|██▊       | 83/300 [00:43<01:55,  1.87it/s]Epoch 4:  28%|██▊       | 84/300 [00:43<01:51,  1.93it/s]Epoch 4:  28%|██▊       | 85/300 [00:44<01:48,  1.98it/s]Epoch 4:  29%|██▊       | 86/300 [00:44<01:52,  1.91it/s]Epoch 4:  29%|██▉       | 87/300 [00:45<01:48,  1.97it/s]Epoch 4:  29%|██▉       | 88/300 [00:45<01:45,  2.00it/s]Epoch 4:  30%|██▉       | 89/300 [00:46<01:46,  1.99it/s]Epoch 4:  30%|███       | 90/300 [00:46<01:49,  1.92it/s]Epoch 4:  30%|███       | 91/300 [00:47<01:46,  1.97it/s]Epoch 4:  31%|███       | 92/300 [00:47<01:43,  2.00it/s]Epoch 4:  31%|███       | 93/300 [00:48<01:41,  2.04it/s]Epoch 4:  31%|███▏      | 94/300 [00:48<01:45,  1.94it/s]Epoch 4:  32%|███▏      | 95/300 [00:49<01:43,  1.98it/s]Epoch 4:  32%|███▏      | 96/300 [00:49<01:41,  2.01it/s]Epoch 4:  32%|███▏      | 97/300 [00:50<01:39,  2.04it/s]Epoch 4:  33%|███▎      | 98/300 [00:50<01:45,  1.92it/s]Epoch 4:  33%|███▎      | 99/300 [00:51<01:41,  1.97it/s]06/19/2022 19:58:51 - INFO - __main__ - global step: 650; train loss: 7.793687343597412; dev loss: 7.944694519042969
Epoch 4:  33%|███▎      | 100/300 [00:51<01:39,  2.01it/s]Epoch 4:  34%|███▎      | 101/300 [00:52<01:38,  2.02it/s]Epoch 4:  34%|███▍      | 102/300 [00:53<01:48,  1.83it/s]Epoch 4:  34%|███▍      | 103/300 [00:53<01:44,  1.88it/s]Epoch 4:  35%|███▍      | 104/300 [00:54<01:41,  1.93it/s]Epoch 4:  35%|███▌      | 105/300 [00:54<01:38,  1.98it/s]Epoch 4:  35%|███▌      | 106/300 [00:55<01:44,  1.86it/s]Epoch 4:  36%|███▌      | 107/300 [00:55<01:39,  1.93it/s]Epoch 4:  36%|███▌      | 108/300 [00:56<01:45,  1.82it/s]Epoch 4:  36%|███▋      | 109/300 [00:56<01:42,  1.87it/s]Epoch 4:  37%|███▋      | 110/300 [00:57<01:45,  1.79it/s]Epoch 4:  37%|███▋      | 111/300 [00:57<01:40,  1.88it/s]Epoch 4:  37%|███▋      | 112/300 [00:58<01:37,  1.94it/s]Epoch 4:  38%|███▊      | 113/300 [00:58<01:34,  1.97it/s]Epoch 4:  38%|███▊      | 114/300 [00:59<01:32,  2.01it/s]Epoch 4:  38%|███▊      | 115/300 [00:59<01:36,  1.92it/s]Epoch 4:  39%|███▊      | 116/300 [01:00<01:33,  1.97it/s]Epoch 4:  39%|███▉      | 117/300 [01:00<01:31,  2.01it/s]Epoch 4:  39%|███▉      | 118/300 [01:01<01:29,  2.02it/s]Epoch 4:  40%|███▉      | 119/300 [01:01<01:33,  1.93it/s]06/19/2022 19:59:01 - INFO - __main__ - global step: 660; train loss: 8.267834663391113; dev loss: 8.257651329040527
Epoch 4:  40%|████      | 120/300 [01:02<01:33,  1.93it/s]Epoch 4:  40%|████      | 121/300 [01:02<01:30,  1.98it/s]Epoch 4:  41%|████      | 122/300 [01:03<01:29,  2.00it/s]Epoch 4:  41%|████      | 123/300 [01:03<01:34,  1.88it/s]Epoch 4:  41%|████▏     | 124/300 [01:04<01:30,  1.94it/s]Epoch 4:  42%|████▏     | 125/300 [01:04<01:32,  1.90it/s]Epoch 4:  42%|████▏     | 126/300 [01:05<01:29,  1.95it/s]Epoch 4:  42%|████▏     | 127/300 [01:06<01:33,  1.84it/s]Epoch 4:  43%|████▎     | 128/300 [01:06<01:32,  1.87it/s]Epoch 4:  43%|████▎     | 129/300 [01:07<01:30,  1.89it/s]Epoch 4:  43%|████▎     | 130/300 [01:07<01:27,  1.95it/s]Epoch 4:  44%|████▎     | 131/300 [01:08<01:29,  1.89it/s]Epoch 4:  44%|████▍     | 132/300 [01:08<01:30,  1.86it/s]Epoch 4:  44%|████▍     | 133/300 [01:09<01:27,  1.92it/s]Epoch 4:  45%|████▍     | 134/300 [01:09<01:26,  1.91it/s]Epoch 4:  45%|████▌     | 135/300 [01:10<01:28,  1.86it/s]Epoch 4:  45%|████▌     | 136/300 [01:10<01:24,  1.93it/s]Epoch 4:  46%|████▌     | 137/300 [01:11<01:22,  1.98it/s]Epoch 4:  46%|████▌     | 138/300 [01:11<01:24,  1.92it/s]Epoch 4:  46%|████▋     | 139/300 [01:12<01:22,  1.96it/s]06/19/2022 19:59:12 - INFO - __main__ - global step: 670; train loss: 8.226741790771484; dev loss: 8.324568748474121
Epoch 4:  47%|████▋     | 140/300 [01:12<01:25,  1.87it/s]Epoch 4:  47%|████▋     | 141/300 [01:13<01:22,  1.93it/s]Epoch 4:  47%|████▋     | 142/300 [01:13<01:20,  1.97it/s]Epoch 4:  48%|████▊     | 143/300 [01:14<01:18,  2.01it/s]Epoch 4:  48%|████▊     | 144/300 [01:14<01:24,  1.84it/s]Epoch 4:  48%|████▊     | 145/300 [01:15<01:20,  1.91it/s]Epoch 4:  49%|████▊     | 146/300 [01:15<01:18,  1.97it/s]Epoch 4:  49%|████▉     | 147/300 [01:16<01:20,  1.91it/s]Epoch 4:  49%|████▉     | 148/300 [01:16<01:22,  1.85it/s]Epoch 4:  50%|████▉     | 149/300 [01:17<01:22,  1.83it/s]Epoch 4:  50%|█████     | 150/300 [01:18<01:18,  1.90it/s]Epoch 4:  50%|█████     | 151/300 [01:18<01:16,  1.96it/s]Epoch 4:  51%|█████     | 152/300 [01:19<01:18,  1.89it/s]Epoch 4:  51%|█████     | 153/300 [01:19<01:19,  1.84it/s]Epoch 4:  51%|█████▏    | 154/300 [01:20<01:19,  1.83it/s]Epoch 4:  52%|█████▏    | 155/300 [01:20<01:16,  1.90it/s]Epoch 4:  52%|█████▏    | 156/300 [01:21<01:21,  1.77it/s]Epoch 4:  52%|█████▏    | 157/300 [01:21<01:16,  1.86it/s]Epoch 4:  53%|█████▎    | 158/300 [01:22<01:13,  1.93it/s]Epoch 4:  53%|█████▎    | 159/300 [01:22<01:11,  1.97it/s]06/19/2022 19:59:22 - INFO - __main__ - global step: 680; train loss: 8.431360244750977; dev loss: 8.932973861694336
Epoch 4:  53%|█████▎    | 160/300 [01:23<01:13,  1.91it/s]Epoch 4:  54%|█████▎    | 161/300 [01:23<01:10,  1.96it/s]Epoch 4:  54%|█████▍    | 162/300 [01:24<01:08,  2.00it/s]Epoch 4:  54%|█████▍    | 163/300 [01:24<01:07,  2.02it/s]Epoch 4:  55%|█████▍    | 164/300 [01:25<01:12,  1.88it/s]Epoch 4:  55%|█████▌    | 165/300 [01:25<01:09,  1.94it/s]Epoch 4:  55%|█████▌    | 166/300 [01:26<01:09,  1.94it/s]Epoch 4:  56%|█████▌    | 167/300 [01:26<01:07,  1.98it/s]Epoch 4:  56%|█████▌    | 168/300 [01:27<01:05,  2.01it/s]Epoch 4:  56%|█████▋    | 169/300 [01:27<01:10,  1.86it/s]Epoch 4:  57%|█████▋    | 170/300 [01:28<01:10,  1.84it/s]Epoch 4:  57%|█████▋    | 171/300 [01:29<01:07,  1.91it/s]Epoch 4:  57%|█████▋    | 172/300 [01:29<01:08,  1.87it/s]Epoch 4:  58%|█████▊    | 173/300 [01:30<01:12,  1.76it/s]Epoch 4:  58%|█████▊    | 174/300 [01:30<01:08,  1.84it/s]Epoch 4:  58%|█████▊    | 175/300 [01:31<01:05,  1.91it/s]Epoch 4:  59%|█████▊    | 176/300 [01:31<01:03,  1.96it/s]Epoch 4:  59%|█████▉    | 177/300 [01:32<01:04,  1.89it/s]Epoch 4:  59%|█████▉    | 178/300 [01:32<01:04,  1.90it/s]Epoch 4:  60%|█████▉    | 179/300 [01:33<01:01,  1.96it/s]06/19/2022 19:59:33 - INFO - __main__ - global step: 690; train loss: 8.179384231567383; dev loss: 8.185362815856934
Epoch 4:  60%|██████    | 180/300 [01:33<01:00,  1.99it/s]Epoch 4:  60%|██████    | 181/300 [01:34<01:03,  1.88it/s]Epoch 4:  61%|██████    | 182/300 [01:34<01:03,  1.86it/s]Epoch 4:  61%|██████    | 183/300 [01:35<01:00,  1.93it/s]Epoch 4:  61%|██████▏   | 184/300 [01:35<00:58,  1.97it/s]Epoch 4:  62%|██████▏   | 185/300 [01:36<01:00,  1.91it/s]Epoch 4:  62%|██████▏   | 186/300 [01:36<00:59,  1.92it/s]Epoch 4:  62%|██████▏   | 187/300 [01:37<01:00,  1.87it/s]Epoch 4:  63%|██████▎   | 188/300 [01:37<00:57,  1.94it/s]Epoch 4:  63%|██████▎   | 189/300 [01:38<00:59,  1.87it/s]Epoch 4:  63%|██████▎   | 190/300 [01:39<00:58,  1.88it/s]Epoch 4:  64%|██████▎   | 191/300 [01:39<00:57,  1.90it/s]Epoch 4:  64%|██████▍   | 192/300 [01:40<00:56,  1.91it/s]Epoch 4:  64%|██████▍   | 193/300 [01:40<00:57,  1.86it/s]Epoch 4:  65%|██████▍   | 194/300 [01:41<00:58,  1.81it/s]Epoch 4:  65%|██████▌   | 195/300 [01:41<00:58,  1.79it/s]Epoch 4:  65%|██████▌   | 196/300 [01:42<00:55,  1.87it/s]Epoch 4:  66%|██████▌   | 197/300 [01:42<00:55,  1.85it/s]Epoch 4:  66%|██████▌   | 198/300 [01:43<00:58,  1.74it/s]Epoch 4:  66%|██████▋   | 199/300 [01:43<00:55,  1.84it/s]06/19/2022 19:59:43 - INFO - __main__ - global step: 700; train loss: 7.990056037902832; dev loss: 7.742735385894775
Epoch 4:  67%|██████▋   | 200/300 [01:44<00:52,  1.91it/s]Epoch 4:  67%|██████▋   | 201/300 [01:44<00:50,  1.95it/s]Epoch 4:  67%|██████▋   | 202/300 [01:45<00:51,  1.89it/s]Epoch 4:  68%|██████▊   | 203/300 [01:45<00:49,  1.95it/s]Epoch 4:  68%|██████▊   | 204/300 [01:46<00:48,  2.00it/s]Epoch 4:  68%|██████▊   | 205/300 [01:46<00:46,  2.02it/s]Epoch 4:  69%|██████▊   | 206/300 [01:47<00:48,  1.94it/s]Epoch 4:  69%|██████▉   | 207/300 [01:47<00:46,  1.98it/s]Epoch 4:  69%|██████▉   | 208/300 [01:48<00:45,  2.02it/s]Epoch 4:  70%|██████▉   | 209/300 [01:48<00:44,  2.04it/s]Epoch 4:  70%|███████   | 210/300 [01:49<00:46,  1.92it/s]Epoch 4:  70%|███████   | 211/300 [01:49<00:45,  1.97it/s]Epoch 4:  71%|███████   | 212/300 [01:50<00:43,  2.01it/s]Epoch 4:  71%|███████   | 213/300 [01:50<00:42,  2.04it/s]Epoch 4:  71%|███████▏  | 214/300 [01:51<00:44,  1.95it/s]Epoch 4:  72%|███████▏  | 215/300 [01:52<00:43,  1.94it/s]Epoch 4:  72%|███████▏  | 216/300 [01:52<00:42,  1.99it/s]Epoch 4:  72%|███████▏  | 217/300 [01:53<00:43,  1.93it/s]Epoch 4:  73%|███████▎  | 218/300 [01:53<00:45,  1.79it/s]Epoch 4:  73%|███████▎  | 219/300 [01:54<00:43,  1.87it/s]06/19/2022 19:59:54 - INFO - __main__ - global step: 710; train loss: 7.506058692932129; dev loss: 7.903601169586182
Epoch 4:  73%|███████▎  | 220/300 [01:54<00:41,  1.93it/s]Epoch 4:  74%|███████▎  | 221/300 [01:55<00:40,  1.95it/s]Epoch 4:  74%|███████▍  | 222/300 [01:55<00:41,  1.90it/s]Epoch 4:  74%|███████▍  | 223/300 [01:56<00:41,  1.85it/s]Epoch 4:  75%|███████▍  | 224/300 [01:56<00:41,  1.81it/s]Epoch 4:  75%|███████▌  | 225/300 [01:57<00:40,  1.87it/s]Epoch 4:  75%|███████▌  | 226/300 [01:57<00:39,  1.89it/s]Epoch 4:  76%|███████▌  | 227/300 [01:58<00:41,  1.77it/s]Epoch 4:  76%|███████▌  | 228/300 [01:58<00:38,  1.85it/s]Epoch 4:  76%|███████▋  | 229/300 [01:59<00:36,  1.92it/s]Epoch 4:  77%|███████▋  | 230/300 [01:59<00:35,  1.97it/s]Epoch 4:  77%|███████▋  | 231/300 [02:00<00:36,  1.91it/s]Epoch 4:  77%|███████▋  | 232/300 [02:01<00:35,  1.94it/s]Epoch 4:  78%|███████▊  | 233/300 [02:01<00:35,  1.88it/s]Epoch 4:  78%|███████▊  | 234/300 [02:02<00:35,  1.84it/s]Epoch 4:  78%|███████▊  | 235/300 [02:02<00:36,  1.78it/s]Epoch 4:  79%|███████▊  | 236/300 [02:03<00:34,  1.86it/s]Epoch 4:  79%|███████▉  | 237/300 [02:03<00:32,  1.93it/s]Epoch 4:  79%|███████▉  | 238/300 [02:04<00:31,  1.98it/s]Epoch 4:  80%|███████▉  | 239/300 [02:04<00:31,  1.91it/s]06/19/2022 20:00:04 - INFO - __main__ - global step: 720; train loss: 8.46355152130127; dev loss: 8.270354270935059
Epoch 4:  80%|████████  | 240/300 [02:05<00:30,  1.97it/s]Epoch 4:  80%|████████  | 241/300 [02:05<00:29,  2.00it/s]Epoch 4:  81%|████████  | 242/300 [02:06<00:28,  2.02it/s]Epoch 4:  81%|████████  | 243/300 [02:06<00:29,  1.93it/s]Epoch 4:  81%|████████▏ | 244/300 [02:07<00:29,  1.89it/s]Epoch 4:  82%|████████▏ | 245/300 [02:07<00:28,  1.95it/s]Epoch 4:  82%|████████▏ | 246/300 [02:08<00:27,  1.99it/s]Epoch 4:  82%|████████▏ | 247/300 [02:08<00:26,  2.02it/s]Epoch 4:  83%|████████▎ | 248/300 [02:09<00:26,  1.93it/s]Epoch 4:  83%|████████▎ | 249/300 [02:09<00:25,  1.98it/s]Epoch 4:  83%|████████▎ | 250/300 [02:10<00:24,  2.01it/s]Epoch 4:  84%|████████▎ | 251/300 [02:10<00:24,  2.03it/s]Epoch 4:  84%|████████▍ | 252/300 [02:11<00:24,  1.94it/s]Epoch 4:  84%|████████▍ | 253/300 [02:11<00:23,  1.98it/s]Epoch 4:  85%|████████▍ | 254/300 [02:12<00:23,  1.96it/s]Epoch 4:  85%|████████▌ | 255/300 [02:12<00:22,  2.00it/s]Epoch 4:  85%|████████▌ | 256/300 [02:13<00:23,  1.88it/s]Epoch 4:  86%|████████▌ | 257/300 [02:13<00:23,  1.85it/s]Epoch 4:  86%|████████▌ | 258/300 [02:14<00:21,  1.91it/s]Epoch 4:  86%|████████▋ | 259/300 [02:14<00:20,  1.96it/s]06/19/2022 20:00:14 - INFO - __main__ - global step: 730; train loss: 8.319658279418945; dev loss: 8.143760681152344
Epoch 4:  87%|████████▋ | 260/300 [02:15<00:21,  1.89it/s]Epoch 4:  87%|████████▋ | 261/300 [02:16<00:20,  1.86it/s]Epoch 4:  87%|████████▋ | 262/300 [02:16<00:19,  1.93it/s]Epoch 4:  88%|████████▊ | 263/300 [02:17<00:18,  1.97it/s]Epoch 4:  88%|████████▊ | 264/300 [02:17<00:18,  1.90it/s]Epoch 4:  88%|████████▊ | 265/300 [02:18<00:18,  1.86it/s]Epoch 4:  89%|████████▊ | 266/300 [02:18<00:17,  1.93it/s]Epoch 4:  89%|████████▉ | 267/300 [02:19<00:16,  1.97it/s]Epoch 4:  89%|████████▉ | 268/300 [02:19<00:16,  1.91it/s]Epoch 4:  90%|████████▉ | 269/300 [02:20<00:16,  1.92it/s]Epoch 4:  90%|█████████ | 270/300 [02:20<00:15,  1.97it/s]Epoch 4:  90%|█████████ | 271/300 [02:21<00:14,  2.00it/s]Epoch 4:  91%|█████████ | 272/300 [02:21<00:15,  1.83it/s]Epoch 4:  91%|█████████ | 273/300 [02:22<00:14,  1.91it/s]Epoch 4:  91%|█████████▏| 274/300 [02:22<00:13,  1.96it/s]Epoch 4:  92%|█████████▏| 275/300 [02:23<00:12,  1.95it/s]Epoch 4:  92%|█████████▏| 276/300 [02:23<00:12,  1.99it/s]Epoch 4:  92%|█████████▏| 277/300 [02:24<00:12,  1.81it/s]Epoch 4:  93%|█████████▎| 278/300 [02:24<00:11,  1.86it/s]Epoch 4:  93%|█████████▎| 279/300 [02:25<00:10,  1.93it/s]06/19/2022 20:00:25 - INFO - __main__ - global step: 740; train loss: 8.091670036315918; dev loss: 8.422112464904785
Epoch 4:  93%|█████████▎| 280/300 [02:25<00:10,  1.88it/s]Epoch 4:  94%|█████████▎| 281/300 [02:26<00:10,  1.81it/s]Epoch 4:  94%|█████████▍| 282/300 [02:27<00:09,  1.81it/s]Epoch 4:  94%|█████████▍| 283/300 [02:27<00:09,  1.84it/s]Epoch 4:  95%|█████████▍| 284/300 [02:28<00:08,  1.82it/s]Epoch 4:  95%|█████████▌| 285/300 [02:28<00:08,  1.79it/s]Epoch 4:  95%|█████████▌| 286/300 [02:29<00:07,  1.78it/s]Epoch 4:  96%|█████████▌| 287/300 [02:29<00:07,  1.78it/s]Epoch 4:  96%|█████████▌| 288/300 [02:30<00:06,  1.86it/s]Epoch 4:  96%|█████████▋| 289/300 [02:31<00:06,  1.74it/s]Epoch 4:  97%|█████████▋| 290/300 [02:31<00:05,  1.79it/s]Epoch 4:  97%|█████████▋| 291/300 [02:32<00:04,  1.86it/s]Epoch 4:  97%|█████████▋| 292/300 [02:32<00:04,  1.86it/s]Epoch 4:  98%|█████████▊| 293/300 [02:33<00:03,  1.77it/s]Epoch 4:  98%|█████████▊| 294/300 [02:33<00:03,  1.78it/s]Epoch 4:  98%|█████████▊| 295/300 [02:34<00:02,  1.86it/s]Epoch 4:  99%|█████████▊| 296/300 [02:34<00:02,  1.92it/s]Epoch 4:  99%|█████████▉| 297/300 [02:35<00:01,  1.86it/s]Epoch 4:  99%|█████████▉| 298/300 [02:35<00:01,  1.92it/s]Epoch 4: 100%|█████████▉| 299/300 [02:36<00:00,  1.97it/s]06/19/2022 20:00:36 - INFO - __main__ - global step: 750; train loss: 8.060498237609863; dev loss: 8.435547828674316
Epoch 4: 100%|██████████| 300/300 [02:36<00:00,  1.91it/s]Epoch 4: 100%|██████████| 300/300 [02:36<00:00,  1.91it/s]
Epoch 5:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 5:   0%|          | 1/300 [00:00<02:21,  2.11it/s]Epoch 5:   1%|          | 2/300 [00:01<02:38,  1.88it/s]Epoch 5:   1%|          | 3/300 [00:01<02:30,  1.97it/s]Epoch 5:   1%|▏         | 4/300 [00:02<02:29,  1.99it/s]Epoch 5:   2%|▏         | 5/300 [00:02<02:30,  1.96it/s]Epoch 5:   2%|▏         | 6/300 [00:03<02:39,  1.84it/s]Epoch 5:   2%|▏         | 7/300 [00:03<02:32,  1.92it/s]Epoch 5:   3%|▎         | 8/300 [00:04<02:36,  1.87it/s]Epoch 5:   3%|▎         | 9/300 [00:04<02:38,  1.83it/s]Epoch 5:   3%|▎         | 10/300 [00:05<02:44,  1.77it/s]Epoch 5:   4%|▎         | 11/300 [00:05<02:39,  1.82it/s]Epoch 5:   4%|▍         | 12/300 [00:06<02:35,  1.85it/s]Epoch 5:   4%|▍         | 13/300 [00:06<02:29,  1.92it/s]Epoch 5:   5%|▍         | 14/300 [00:07<02:37,  1.82it/s]Epoch 5:   5%|▌         | 15/300 [00:07<02:30,  1.89it/s]Epoch 5:   5%|▌         | 16/300 [00:08<02:25,  1.96it/s]Epoch 5:   6%|▌         | 17/300 [00:08<02:25,  1.95it/s]Epoch 5:   6%|▌         | 18/300 [00:09<02:36,  1.80it/s]Epoch 5:   6%|▋         | 19/300 [00:10<02:36,  1.79it/s]06/19/2022 20:00:46 - INFO - __main__ - global step: 760; train loss: 7.817892551422119; dev loss: 7.343201637268066
Epoch 5:   7%|▋         | 20/300 [00:10<02:33,  1.83it/s]Epoch 5:   7%|▋         | 21/300 [00:11<02:34,  1.81it/s]Epoch 5:   7%|▋         | 22/300 [00:11<02:38,  1.75it/s]Epoch 5:   8%|▊         | 23/300 [00:12<02:34,  1.80it/s]Epoch 5:   8%|▊         | 24/300 [00:12<02:27,  1.87it/s]Epoch 5:   8%|▊         | 25/300 [00:13<02:29,  1.85it/s]Epoch 5:   9%|▊         | 26/300 [00:14<02:30,  1.82it/s]Epoch 5:   9%|▉         | 27/300 [00:14<02:24,  1.89it/s]Epoch 5:   9%|▉         | 28/300 [00:14<02:20,  1.94it/s]Epoch 5:  10%|▉         | 29/300 [00:15<02:18,  1.95it/s]Epoch 5:  10%|█         | 30/300 [00:16<02:19,  1.93it/s]Epoch 5:  10%|█         | 31/300 [00:16<02:30,  1.79it/s]Epoch 5:  11%|█         | 32/300 [00:17<02:23,  1.87it/s]Epoch 5:  11%|█         | 33/300 [00:17<02:18,  1.93it/s]Epoch 5:  11%|█▏        | 34/300 [00:18<02:14,  1.98it/s]Epoch 5:  12%|█▏        | 35/300 [00:18<02:18,  1.91it/s]Epoch 5:  12%|█▏        | 36/300 [00:19<02:14,  1.96it/s]Epoch 5:  12%|█▏        | 37/300 [00:19<02:15,  1.95it/s]Epoch 5:  13%|█▎        | 38/300 [00:20<02:12,  1.98it/s]Epoch 5:  13%|█▎        | 39/300 [00:20<02:20,  1.85it/s]06/19/2022 20:00:57 - INFO - __main__ - global step: 770; train loss: 7.878467559814453; dev loss: 8.021873474121094
Epoch 5:  13%|█▎        | 40/300 [00:21<02:18,  1.88it/s]Epoch 5:  14%|█▎        | 41/300 [00:21<02:16,  1.90it/s]Epoch 5:  14%|█▍        | 42/300 [00:22<02:12,  1.95it/s]Epoch 5:  14%|█▍        | 43/300 [00:22<02:18,  1.85it/s]Epoch 5:  15%|█▍        | 44/300 [00:23<02:13,  1.91it/s]Epoch 5:  15%|█▌        | 45/300 [00:23<02:10,  1.96it/s]Epoch 5:  15%|█▌        | 46/300 [00:24<02:07,  1.99it/s]Epoch 5:  16%|█▌        | 47/300 [00:24<02:11,  1.92it/s]Epoch 5:  16%|█▌        | 48/300 [00:25<02:09,  1.95it/s]Epoch 5:  16%|█▋        | 49/300 [00:25<02:07,  1.97it/s]Epoch 5:  17%|█▋        | 50/300 [00:26<02:05,  1.99it/s]Epoch 5:  17%|█▋        | 51/300 [00:26<02:10,  1.90it/s]Epoch 5:  17%|█▋        | 52/300 [00:27<02:06,  1.96it/s]Epoch 5:  18%|█▊        | 53/300 [00:27<02:03,  2.00it/s]Epoch 5:  18%|█▊        | 54/300 [00:28<02:02,  2.02it/s]Epoch 5:  18%|█▊        | 55/300 [00:28<02:00,  2.04it/s]Epoch 5:  19%|█▊        | 56/300 [00:29<02:11,  1.85it/s]Epoch 5:  19%|█▉        | 57/300 [00:30<02:06,  1.92it/s]Epoch 5:  19%|█▉        | 58/300 [00:30<02:02,  1.97it/s]Epoch 5:  20%|█▉        | 59/300 [00:30<02:00,  2.01it/s]06/19/2022 20:01:07 - INFO - __main__ - global step: 780; train loss: 8.285149574279785; dev loss: 8.657678604125977
Epoch 5:  20%|██        | 60/300 [00:31<02:05,  1.91it/s]Epoch 5:  20%|██        | 61/300 [00:32<02:01,  1.96it/s]Epoch 5:  21%|██        | 62/300 [00:32<01:59,  1.99it/s]Epoch 5:  21%|██        | 63/300 [00:33<01:58,  2.01it/s]Epoch 5:  21%|██▏       | 64/300 [00:33<02:08,  1.83it/s]Epoch 5:  22%|██▏       | 65/300 [00:34<02:04,  1.88it/s]Epoch 5:  22%|██▏       | 66/300 [00:34<02:00,  1.95it/s]Epoch 5:  22%|██▏       | 67/300 [00:35<01:58,  1.97it/s]Epoch 5:  23%|██▎       | 68/300 [00:35<02:02,  1.89it/s]Epoch 5:  23%|██▎       | 69/300 [00:36<01:59,  1.93it/s]Epoch 5:  23%|██▎       | 70/300 [00:36<01:56,  1.97it/s]Epoch 5:  24%|██▎       | 71/300 [00:37<02:00,  1.90it/s]Epoch 5:  24%|██▍       | 72/300 [00:37<02:05,  1.82it/s]Epoch 5:  24%|██▍       | 73/300 [00:38<02:00,  1.88it/s]Epoch 5:  25%|██▍       | 74/300 [00:38<01:56,  1.94it/s]Epoch 5:  25%|██▌       | 75/300 [00:39<01:54,  1.96it/s]Epoch 5:  25%|██▌       | 76/300 [00:39<01:58,  1.89it/s]Epoch 5:  26%|██▌       | 77/300 [00:40<01:55,  1.93it/s]Epoch 5:  26%|██▌       | 78/300 [00:40<01:52,  1.98it/s]Epoch 5:  26%|██▋       | 79/300 [00:41<01:49,  2.01it/s]06/19/2022 20:01:18 - INFO - __main__ - global step: 790; train loss: 7.934579372406006; dev loss: 8.192305564880371
Epoch 5:  27%|██▋       | 80/300 [00:41<01:54,  1.91it/s]Epoch 5:  27%|██▋       | 81/300 [00:42<01:51,  1.96it/s]Epoch 5:  27%|██▋       | 82/300 [00:43<01:57,  1.85it/s]Epoch 5:  28%|██▊       | 83/300 [00:43<01:53,  1.92it/s]Epoch 5:  28%|██▊       | 84/300 [00:43<01:49,  1.97it/s]Epoch 5:  28%|██▊       | 85/300 [00:44<01:53,  1.89it/s]Epoch 5:  29%|██▊       | 86/300 [00:45<01:55,  1.86it/s]Epoch 5:  29%|██▉       | 87/300 [00:45<01:55,  1.84it/s]Epoch 5:  29%|██▉       | 88/300 [00:46<01:56,  1.82it/s]Epoch 5:  30%|██▉       | 89/300 [00:46<01:57,  1.80it/s]Epoch 5:  30%|███       | 90/300 [00:47<01:51,  1.88it/s]Epoch 5:  30%|███       | 91/300 [00:47<01:48,  1.94it/s]Epoch 5:  31%|███       | 92/300 [00:48<01:50,  1.88it/s]Epoch 5:  31%|███       | 93/300 [00:48<01:52,  1.84it/s]Epoch 5:  31%|███▏      | 94/300 [00:49<01:48,  1.90it/s]Epoch 5:  32%|███▏      | 95/300 [00:49<01:44,  1.96it/s]Epoch 5:  32%|███▏      | 96/300 [00:50<01:42,  2.00it/s]Epoch 5:  32%|███▏      | 97/300 [00:50<01:45,  1.92it/s]Epoch 5:  33%|███▎      | 98/300 [00:51<01:45,  1.91it/s]Epoch 5:  33%|███▎      | 99/300 [00:51<01:43,  1.95it/s]06/19/2022 20:01:28 - INFO - __main__ - global step: 800; train loss: 7.701170444488525; dev loss: 7.755422115325928
Epoch 5:  33%|███▎      | 100/300 [00:52<01:45,  1.90it/s]Epoch 5:  34%|███▎      | 101/300 [00:53<01:48,  1.84it/s]Epoch 5:  34%|███▍      | 102/300 [00:53<01:43,  1.91it/s]Epoch 5:  34%|███▍      | 103/300 [00:54<01:40,  1.96it/s]Epoch 5:  35%|███▍      | 104/300 [00:54<01:40,  1.96it/s]Epoch 5:  35%|███▌      | 105/300 [00:55<01:47,  1.81it/s]Epoch 5:  35%|███▌      | 106/300 [00:55<01:47,  1.80it/s]Epoch 5:  36%|███▌      | 107/300 [00:56<01:48,  1.78it/s]Epoch 5:  36%|███▌      | 108/300 [00:56<01:43,  1.86it/s]Epoch 5:  36%|███▋      | 109/300 [00:57<01:39,  1.92it/s]Epoch 5:  37%|███▋      | 110/300 [00:57<01:42,  1.86it/s]Epoch 5:  37%|███▋      | 111/300 [00:58<01:38,  1.92it/s]Epoch 5:  37%|███▋      | 112/300 [00:58<01:40,  1.87it/s]Epoch 5:  38%|███▊      | 113/300 [00:59<01:37,  1.92it/s]Epoch 5:  38%|███▊      | 114/300 [00:59<01:39,  1.88it/s]Epoch 5:  38%|███▊      | 115/300 [01:00<01:35,  1.94it/s]Epoch 5:  39%|███▊      | 116/300 [01:00<01:32,  1.98it/s]Epoch 5:  39%|███▉      | 117/300 [01:01<01:31,  2.00it/s]Epoch 5:  39%|███▉      | 118/300 [01:01<01:34,  1.92it/s]Epoch 5:  40%|███▉      | 119/300 [01:02<01:32,  1.96it/s]06/19/2022 20:01:39 - INFO - __main__ - global step: 810; train loss: 8.026611328125; dev loss: 8.099515914916992
Epoch 5:  40%|████      | 120/300 [01:02<01:30,  1.99it/s]Epoch 5:  40%|████      | 121/300 [01:03<01:28,  2.02it/s]Epoch 5:  41%|████      | 122/300 [01:04<01:34,  1.89it/s]Epoch 5:  41%|████      | 123/300 [01:04<01:32,  1.91it/s]Epoch 5:  41%|████▏     | 124/300 [01:05<01:29,  1.96it/s]Epoch 5:  42%|████▏     | 125/300 [01:05<01:27,  2.00it/s]Epoch 5:  42%|████▏     | 126/300 [01:06<01:30,  1.92it/s]Epoch 5:  42%|████▏     | 127/300 [01:06<01:27,  1.97it/s]Epoch 5:  43%|████▎     | 128/300 [01:07<01:25,  2.00it/s]Epoch 5:  43%|████▎     | 129/300 [01:07<01:24,  2.03it/s]Epoch 5:  43%|████▎     | 130/300 [01:08<01:27,  1.94it/s]Epoch 5:  44%|████▎     | 131/300 [01:08<01:25,  1.98it/s]Epoch 5:  44%|████▍     | 132/300 [01:09<01:24,  2.00it/s]Epoch 5:  44%|████▍     | 133/300 [01:09<01:22,  2.01it/s]Epoch 5:  45%|████▍     | 134/300 [01:10<01:26,  1.92it/s]Epoch 5:  45%|████▌     | 135/300 [01:10<01:27,  1.88it/s]Epoch 5:  45%|████▌     | 136/300 [01:11<01:27,  1.88it/s]Epoch 5:  46%|████▌     | 137/300 [01:11<01:28,  1.85it/s]Epoch 5:  46%|████▌     | 138/300 [01:12<01:28,  1.82it/s]Epoch 5:  46%|████▋     | 139/300 [01:12<01:33,  1.72it/s]06/19/2022 20:01:49 - INFO - __main__ - global step: 820; train loss: 8.342379570007324; dev loss: 8.147533416748047
Epoch 5:  47%|████▋     | 140/300 [01:13<01:28,  1.81it/s]Epoch 5:  47%|████▋     | 141/300 [01:13<01:24,  1.89it/s]Epoch 5:  47%|████▋     | 142/300 [01:14<01:25,  1.84it/s]Epoch 5:  48%|████▊     | 143/300 [01:15<01:27,  1.79it/s]Epoch 5:  48%|████▊     | 144/300 [01:15<01:23,  1.87it/s]Epoch 5:  48%|████▊     | 145/300 [01:16<01:20,  1.93it/s]Epoch 5:  49%|████▊     | 146/300 [01:16<01:18,  1.97it/s]Epoch 5:  49%|████▉     | 147/300 [01:17<01:22,  1.85it/s]Epoch 5:  49%|████▉     | 148/300 [01:17<01:19,  1.91it/s]Epoch 5:  50%|████▉     | 149/300 [01:18<01:16,  1.97it/s]Epoch 5:  50%|█████     | 150/300 [01:18<01:15,  2.00it/s]Epoch 5:  50%|█████     | 151/300 [01:19<01:18,  1.91it/s]Epoch 5:  51%|█████     | 152/300 [01:19<01:15,  1.96it/s]Epoch 5:  51%|█████     | 153/300 [01:20<01:13,  2.00it/s]Epoch 5:  51%|█████▏    | 154/300 [01:20<01:12,  2.03it/s]Epoch 5:  52%|█████▏    | 155/300 [01:21<01:15,  1.92it/s]Epoch 5:  52%|█████▏    | 156/300 [01:21<01:13,  1.96it/s]Epoch 5:  52%|█████▏    | 157/300 [01:22<01:13,  1.94it/s]Epoch 5:  53%|█████▎    | 158/300 [01:22<01:15,  1.89it/s]Epoch 5:  53%|█████▎    | 159/300 [01:23<01:16,  1.85it/s]06/19/2022 20:02:00 - INFO - __main__ - global step: 830; train loss: 7.755417823791504; dev loss: 8.053085327148438
Epoch 5:  53%|█████▎    | 160/300 [01:23<01:13,  1.91it/s]Epoch 5:  54%|█████▎    | 161/300 [01:24<01:10,  1.96it/s]Epoch 5:  54%|█████▍    | 162/300 [01:24<01:09,  2.00it/s]Epoch 5:  54%|█████▍    | 163/300 [01:25<01:07,  2.02it/s]Epoch 5:  55%|█████▍    | 164/300 [01:25<01:14,  1.83it/s]Epoch 5:  55%|█████▌    | 165/300 [01:26<01:11,  1.90it/s]Epoch 5:  55%|█████▌    | 166/300 [01:26<01:08,  1.94it/s]Epoch 5:  56%|█████▌    | 167/300 [01:27<01:08,  1.93it/s]Epoch 5:  56%|█████▌    | 168/300 [01:27<01:10,  1.87it/s]Epoch 5:  56%|█████▋    | 169/300 [01:28<01:07,  1.93it/s]Epoch 5:  57%|█████▋    | 170/300 [01:29<01:09,  1.88it/s]Epoch 5:  57%|█████▋    | 171/300 [01:29<01:06,  1.94it/s]Epoch 5:  57%|█████▋    | 172/300 [01:30<01:09,  1.85it/s]Epoch 5:  58%|█████▊    | 173/300 [01:30<01:06,  1.92it/s]Epoch 5:  58%|█████▊    | 174/300 [01:31<01:04,  1.96it/s]Epoch 5:  58%|█████▊    | 175/300 [01:31<01:05,  1.90it/s]Epoch 5:  59%|█████▊    | 176/300 [01:32<01:08,  1.81it/s]Epoch 5:  59%|█████▉    | 177/300 [01:32<01:05,  1.88it/s]Epoch 5:  59%|█████▉    | 178/300 [01:33<01:03,  1.94it/s]Epoch 5:  60%|█████▉    | 179/300 [01:33<01:02,  1.93it/s]06/19/2022 20:02:10 - INFO - __main__ - global step: 840; train loss: 7.534285068511963; dev loss: 7.981978416442871
Epoch 5:  60%|██████    | 180/300 [01:34<01:04,  1.87it/s]Epoch 5:  60%|██████    | 181/300 [01:34<01:01,  1.93it/s]Epoch 5:  61%|██████    | 182/300 [01:35<00:59,  1.97it/s]Epoch 5:  61%|██████    | 183/300 [01:35<01:01,  1.90it/s]Epoch 5:  61%|██████▏   | 184/300 [01:36<01:02,  1.85it/s]Epoch 5:  62%|██████▏   | 185/300 [01:36<01:00,  1.91it/s]Epoch 5:  62%|██████▏   | 186/300 [01:37<00:58,  1.96it/s]Epoch 5:  62%|██████▏   | 187/300 [01:37<00:56,  2.00it/s]Epoch 5:  63%|██████▎   | 188/300 [01:38<01:01,  1.83it/s]Epoch 5:  63%|██████▎   | 189/300 [01:39<00:59,  1.86it/s]Epoch 5:  63%|██████▎   | 190/300 [01:39<00:57,  1.92it/s]Epoch 5:  64%|██████▎   | 191/300 [01:39<00:55,  1.97it/s]Epoch 5:  64%|██████▍   | 192/300 [01:40<00:56,  1.90it/s]Epoch 5:  64%|██████▍   | 193/300 [01:41<00:59,  1.81it/s]Epoch 5:  65%|██████▍   | 194/300 [01:41<00:56,  1.89it/s]Epoch 5:  65%|██████▌   | 195/300 [01:42<00:54,  1.94it/s]Epoch 5:  65%|██████▌   | 196/300 [01:42<00:52,  1.98it/s]Epoch 5:  66%|██████▌   | 197/300 [01:43<00:54,  1.90it/s]Epoch 5:  66%|██████▌   | 198/300 [01:43<00:52,  1.95it/s]Epoch 5:  66%|██████▋   | 199/300 [01:44<00:51,  1.94it/s]06/19/2022 20:02:20 - INFO - __main__ - global step: 850; train loss: 7.986174583435059; dev loss: 7.86467981338501
Epoch 5:  67%|██████▋   | 200/300 [01:44<00:52,  1.89it/s]Epoch 5:  67%|██████▋   | 201/300 [01:45<00:53,  1.84it/s]Epoch 5:  67%|██████▋   | 202/300 [01:45<00:52,  1.86it/s]Epoch 5:  68%|██████▊   | 203/300 [01:46<00:50,  1.91it/s]Epoch 5:  68%|██████▊   | 204/300 [01:46<00:49,  1.95it/s]Epoch 5:  68%|██████▊   | 205/300 [01:47<00:52,  1.79it/s]Epoch 5:  69%|██████▊   | 206/300 [01:48<00:51,  1.83it/s]Epoch 5:  69%|██████▉   | 207/300 [01:48<00:48,  1.91it/s]Epoch 5:  69%|██████▉   | 208/300 [01:48<00:47,  1.96it/s]Epoch 5:  70%|██████▉   | 209/300 [01:49<00:48,  1.88it/s]Epoch 5:  70%|███████   | 210/300 [01:50<00:46,  1.94it/s]Epoch 5:  70%|███████   | 211/300 [01:50<00:45,  1.98it/s]Epoch 5:  71%|███████   | 212/300 [01:51<00:44,  1.96it/s]Epoch 5:  71%|███████   | 213/300 [01:51<00:46,  1.87it/s]Epoch 5:  71%|███████▏  | 214/300 [01:52<00:44,  1.92it/s]Epoch 5:  72%|███████▏  | 215/300 [01:52<00:43,  1.96it/s]Epoch 5:  72%|███████▏  | 216/300 [01:53<00:42,  2.00it/s]Epoch 5:  72%|███████▏  | 217/300 [01:53<00:41,  2.02it/s]Epoch 5:  73%|███████▎  | 218/300 [01:54<00:44,  1.84it/s]Epoch 5:  73%|███████▎  | 219/300 [01:54<00:42,  1.91it/s]06/19/2022 20:02:31 - INFO - __main__ - global step: 860; train loss: 7.8697614669799805; dev loss: 8.118705749511719
Epoch 5:  73%|███████▎  | 220/300 [01:55<00:40,  1.96it/s]Epoch 5:  74%|███████▎  | 221/300 [01:55<00:39,  2.00it/s]Epoch 5:  74%|███████▍  | 222/300 [01:56<00:40,  1.91it/s]Epoch 5:  74%|███████▍  | 223/300 [01:56<00:39,  1.96it/s]Epoch 5:  75%|███████▍  | 224/300 [01:57<00:38,  1.95it/s]Epoch 5:  75%|███████▌  | 225/300 [01:57<00:37,  1.99it/s]Epoch 5:  75%|███████▌  | 226/300 [01:58<00:39,  1.87it/s]Epoch 5:  76%|███████▌  | 227/300 [01:58<00:38,  1.92it/s]Epoch 5:  76%|███████▌  | 228/300 [01:59<00:36,  1.96it/s]Epoch 5:  76%|███████▋  | 229/300 [01:59<00:35,  2.00it/s]Epoch 5:  77%|███████▋  | 230/300 [02:00<00:36,  1.93it/s]Epoch 5:  77%|███████▋  | 231/300 [02:00<00:35,  1.96it/s]Epoch 5:  77%|███████▋  | 232/300 [02:01<00:36,  1.89it/s]Epoch 5:  78%|███████▊  | 233/300 [02:01<00:34,  1.93it/s]Epoch 5:  78%|███████▊  | 234/300 [02:02<00:36,  1.82it/s]Epoch 5:  78%|███████▊  | 235/300 [02:02<00:34,  1.90it/s]Epoch 5:  79%|███████▊  | 236/300 [02:03<00:32,  1.95it/s]Epoch 5:  79%|███████▉  | 237/300 [02:03<00:31,  1.98it/s]Epoch 5:  79%|███████▉  | 238/300 [02:04<00:34,  1.82it/s]Epoch 5:  80%|███████▉  | 239/300 [02:05<00:32,  1.89it/s]06/19/2022 20:02:41 - INFO - __main__ - global step: 870; train loss: 8.306612014770508; dev loss: 8.245595932006836
Epoch 5:  80%|████████  | 240/300 [02:05<00:30,  1.94it/s]Epoch 5:  80%|████████  | 241/300 [02:06<00:30,  1.94it/s]Epoch 5:  81%|████████  | 242/300 [02:06<00:30,  1.88it/s]Epoch 5:  81%|████████  | 243/300 [02:07<00:30,  1.89it/s]Epoch 5:  81%|████████▏ | 244/300 [02:07<00:28,  1.94it/s]Epoch 5:  82%|████████▏ | 245/300 [02:08<00:27,  1.99it/s]Epoch 5:  82%|████████▏ | 246/300 [02:08<00:28,  1.92it/s]Epoch 5:  82%|████████▏ | 247/300 [02:09<00:29,  1.81it/s]Epoch 5:  83%|████████▎ | 248/300 [02:09<00:27,  1.89it/s]Epoch 5:  83%|████████▎ | 249/300 [02:10<00:26,  1.94it/s]Epoch 5:  83%|████████▎ | 250/300 [02:10<00:25,  1.93it/s]Epoch 5:  84%|████████▎ | 251/300 [02:11<00:26,  1.87it/s]Epoch 5:  84%|████████▍ | 252/300 [02:11<00:24,  1.94it/s]Epoch 5:  84%|████████▍ | 253/300 [02:12<00:23,  1.96it/s]Epoch 5:  85%|████████▍ | 254/300 [02:12<00:23,  1.99it/s]Epoch 5:  85%|████████▌ | 255/300 [02:13<00:23,  1.91it/s]Epoch 5:  85%|████████▌ | 256/300 [02:13<00:22,  1.96it/s]Epoch 5:  86%|████████▌ | 257/300 [02:14<00:22,  1.95it/s]Epoch 5:  86%|████████▌ | 258/300 [02:14<00:21,  1.99it/s]Epoch 5:  86%|████████▋ | 259/300 [02:15<00:21,  1.91it/s]06/19/2022 20:02:52 - INFO - __main__ - global step: 880; train loss: 8.577709197998047; dev loss: 8.147290229797363
Epoch 5:  87%|████████▋ | 260/300 [02:16<00:21,  1.86it/s]Epoch 5:  87%|████████▋ | 261/300 [02:16<00:20,  1.92it/s]Epoch 5:  87%|████████▋ | 262/300 [02:17<00:19,  1.92it/s]Epoch 5:  88%|████████▊ | 263/300 [02:17<00:20,  1.79it/s]Epoch 5:  88%|████████▊ | 264/300 [02:18<00:20,  1.79it/s]Epoch 5:  88%|████████▊ | 265/300 [02:18<00:18,  1.87it/s]Epoch 5:  89%|████████▊ | 266/300 [02:19<00:18,  1.89it/s]Epoch 5:  89%|████████▉ | 267/300 [02:19<00:17,  1.84it/s]Epoch 5:  89%|████████▉ | 268/300 [02:20<00:17,  1.85it/s]Epoch 5:  90%|████████▉ | 269/300 [02:20<00:16,  1.88it/s]Epoch 5:  90%|█████████ | 270/300 [02:21<00:15,  1.93it/s]Epoch 5:  90%|█████████ | 271/300 [02:21<00:14,  1.97it/s]Epoch 5:  91%|█████████ | 272/300 [02:22<00:15,  1.81it/s]Epoch 5:  91%|█████████ | 273/300 [02:22<00:14,  1.89it/s]Epoch 5:  91%|█████████▏| 274/300 [02:23<00:13,  1.91it/s]Epoch 5:  92%|█████████▏| 275/300 [02:23<00:12,  1.96it/s]Epoch 5:  92%|█████████▏| 276/300 [02:24<00:12,  1.90it/s]Epoch 5:  92%|█████████▏| 277/300 [02:24<00:11,  1.95it/s]Epoch 5:  93%|█████████▎| 278/300 [02:25<00:11,  1.99it/s]Epoch 5:  93%|█████████▎| 279/300 [02:26<00:10,  1.92it/s]06/19/2022 20:03:02 - INFO - __main__ - global step: 890; train loss: 7.963799953460693; dev loss: 8.090036392211914
Epoch 5:  93%|█████████▎| 280/300 [02:26<00:10,  1.85it/s]Epoch 5:  94%|█████████▎| 281/300 [02:27<00:09,  1.92it/s]Epoch 5:  94%|█████████▍| 282/300 [02:27<00:09,  1.92it/s]Epoch 5:  94%|█████████▍| 283/300 [02:28<00:08,  1.97it/s]Epoch 5:  95%|█████████▍| 284/300 [02:28<00:08,  1.90it/s]Epoch 5:  95%|█████████▌| 285/300 [02:29<00:07,  1.91it/s]Epoch 5:  95%|█████████▌| 286/300 [02:29<00:07,  1.96it/s]Epoch 5:  96%|█████████▌| 287/300 [02:30<00:06,  1.90it/s]Epoch 5:  96%|█████████▌| 288/300 [02:30<00:06,  1.85it/s]Epoch 5:  96%|█████████▋| 289/300 [02:31<00:05,  1.91it/s]Epoch 5:  97%|█████████▋| 290/300 [02:31<00:05,  1.96it/s]Epoch 5:  97%|█████████▋| 291/300 [02:32<00:04,  1.90it/s]Epoch 5:  97%|█████████▋| 292/300 [02:32<00:04,  1.81it/s]Epoch 5:  98%|█████████▊| 293/300 [02:33<00:03,  1.89it/s]Epoch 5:  98%|█████████▊| 294/300 [02:33<00:03,  1.86it/s]Epoch 5:  98%|█████████▊| 295/300 [02:34<00:02,  1.92it/s]Epoch 5:  99%|█████████▊| 296/300 [02:35<00:02,  1.81it/s]Epoch 5:  99%|█████████▉| 297/300 [02:35<00:01,  1.81it/s]Epoch 5:  99%|█████████▉| 298/300 [02:36<00:01,  1.88it/s]Epoch 5: 100%|█████████▉| 299/300 [02:36<00:00,  1.93it/s]06/19/2022 20:03:13 - INFO - __main__ - global step: 900; train loss: 8.425543785095215; dev loss: 8.137292861938477
Epoch 5: 100%|██████████| 300/300 [02:37<00:00,  1.97it/s]Epoch 5: 100%|██████████| 300/300 [02:37<00:00,  1.91it/s]
Epoch 6:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 6:   0%|          | 1/300 [00:00<02:51,  1.74it/s]Epoch 6:   1%|          | 2/300 [00:01<02:48,  1.77it/s]Epoch 6:   1%|          | 3/300 [00:01<02:35,  1.91it/s]Epoch 6:   1%|▏         | 4/300 [00:02<02:38,  1.86it/s]Epoch 6:   2%|▏         | 5/300 [00:02<02:41,  1.82it/s]Epoch 6:   2%|▏         | 6/300 [00:03<02:42,  1.81it/s]Epoch 6:   2%|▏         | 7/300 [00:03<02:39,  1.84it/s]Epoch 6:   3%|▎         | 8/300 [00:04<02:40,  1.82it/s]Epoch 6:   3%|▎         | 9/300 [00:04<02:41,  1.81it/s]Epoch 6:   3%|▎         | 10/300 [00:05<02:41,  1.80it/s]Epoch 6:   4%|▎         | 11/300 [00:05<02:35,  1.86it/s]Epoch 6:   4%|▍         | 12/300 [00:06<02:29,  1.93it/s]Epoch 6:   4%|▍         | 13/300 [00:07<02:33,  1.87it/s]Epoch 6:   5%|▍         | 14/300 [00:07<02:28,  1.93it/s]Epoch 6:   5%|▌         | 15/300 [00:08<02:32,  1.87it/s]Epoch 6:   5%|▌         | 16/300 [00:08<02:30,  1.88it/s]Epoch 6:   6%|▌         | 17/300 [00:09<02:37,  1.80it/s]Epoch 6:   6%|▌         | 18/300 [00:09<02:37,  1.80it/s]Epoch 6:   6%|▋         | 19/300 [00:10<02:36,  1.79it/s]06/19/2022 20:03:24 - INFO - __main__ - global step: 910; train loss: 8.067304611206055; dev loss: 8.525896072387695
Epoch 6:   7%|▋         | 20/300 [00:10<02:29,  1.87it/s]Epoch 6:   7%|▋         | 21/300 [00:11<02:32,  1.83it/s]Epoch 6:   7%|▋         | 22/300 [00:11<02:26,  1.90it/s]Epoch 6:   8%|▊         | 23/300 [00:12<02:28,  1.86it/s]Epoch 6:   8%|▊         | 24/300 [00:12<02:23,  1.93it/s]Epoch 6:   8%|▊         | 25/300 [00:13<02:19,  1.97it/s]Epoch 6:   9%|▊         | 26/300 [00:13<02:24,  1.89it/s]Epoch 6:   9%|▉         | 27/300 [00:14<02:20,  1.95it/s]Epoch 6:   9%|▉         | 28/300 [00:15<02:24,  1.88it/s]Epoch 6:  10%|▉         | 29/300 [00:15<02:20,  1.93it/s]Epoch 6:  10%|█         | 30/300 [00:16<02:24,  1.86it/s]Epoch 6:  10%|█         | 31/300 [00:16<02:19,  1.92it/s]Epoch 6:  11%|█         | 32/300 [00:17<02:16,  1.97it/s]Epoch 6:  11%|█         | 33/300 [00:17<02:14,  1.99it/s]Epoch 6:  11%|█▏        | 34/300 [00:18<02:19,  1.90it/s]Epoch 6:  12%|█▏        | 35/300 [00:18<02:15,  1.96it/s]Epoch 6:  12%|█▏        | 36/300 [00:19<02:12,  2.00it/s]Epoch 6:  12%|█▏        | 37/300 [00:19<02:09,  2.02it/s]Epoch 6:  13%|█▎        | 38/300 [00:20<02:18,  1.89it/s]Epoch 6:  13%|█▎        | 39/300 [00:20<02:17,  1.90it/s]06/19/2022 20:03:34 - INFO - __main__ - global step: 920; train loss: 7.995270729064941; dev loss: 8.233838081359863
Epoch 6:  13%|█▎        | 40/300 [00:21<02:13,  1.94it/s]Epoch 6:  14%|█▎        | 41/300 [00:21<02:13,  1.94it/s]Epoch 6:  14%|█▍        | 42/300 [00:22<02:17,  1.87it/s]Epoch 6:  14%|█▍        | 43/300 [00:22<02:19,  1.84it/s]Epoch 6:  15%|█▍        | 44/300 [00:23<02:13,  1.91it/s]Epoch 6:  15%|█▌        | 45/300 [00:23<02:09,  1.96it/s]Epoch 6:  15%|█▌        | 46/300 [00:24<02:17,  1.85it/s]Epoch 6:  16%|█▌        | 47/300 [00:24<02:12,  1.91it/s]Epoch 6:  16%|█▌        | 48/300 [00:25<02:08,  1.96it/s]Epoch 6:  16%|█▋        | 49/300 [00:25<02:06,  1.99it/s]Epoch 6:  17%|█▋        | 50/300 [00:26<02:13,  1.87it/s]Epoch 6:  17%|█▋        | 51/300 [00:26<02:09,  1.92it/s]Epoch 6:  17%|█▋        | 52/300 [00:27<02:08,  1.92it/s]Epoch 6:  18%|█▊        | 53/300 [00:28<02:09,  1.91it/s]Epoch 6:  18%|█▊        | 54/300 [00:28<02:05,  1.95it/s]Epoch 6:  18%|█▊        | 55/300 [00:29<02:09,  1.88it/s]Epoch 6:  19%|█▊        | 56/300 [00:29<02:05,  1.94it/s]Epoch 6:  19%|█▉        | 57/300 [00:30<02:03,  1.96it/s]Epoch 6:  19%|█▉        | 58/300 [00:30<02:06,  1.91it/s]Epoch 6:  20%|█▉        | 59/300 [00:31<02:09,  1.86it/s]06/19/2022 20:03:44 - INFO - __main__ - global step: 930; train loss: 7.842578887939453; dev loss: 7.761232852935791
Epoch 6:  20%|██        | 60/300 [00:31<02:10,  1.83it/s]Epoch 6:  20%|██        | 61/300 [00:32<02:05,  1.90it/s]Epoch 6:  21%|██        | 62/300 [00:32<02:02,  1.95it/s]Epoch 6:  21%|██        | 63/300 [00:33<02:05,  1.88it/s]Epoch 6:  21%|██▏       | 64/300 [00:33<02:07,  1.85it/s]Epoch 6:  22%|██▏       | 65/300 [00:34<02:02,  1.92it/s]Epoch 6:  22%|██▏       | 66/300 [00:34<01:58,  1.97it/s]Epoch 6:  22%|██▏       | 67/300 [00:35<02:02,  1.91it/s]Epoch 6:  23%|██▎       | 68/300 [00:35<01:58,  1.96it/s]Epoch 6:  23%|██▎       | 69/300 [00:36<02:00,  1.91it/s]Epoch 6:  23%|██▎       | 70/300 [00:36<01:57,  1.97it/s]Epoch 6:  24%|██▎       | 71/300 [00:37<02:00,  1.90it/s]Epoch 6:  24%|██▍       | 72/300 [00:37<01:56,  1.96it/s]Epoch 6:  24%|██▍       | 73/300 [00:38<01:53,  2.01it/s]Epoch 6:  25%|██▍       | 74/300 [00:38<01:51,  2.03it/s]Epoch 6:  25%|██▌       | 75/300 [00:39<01:56,  1.93it/s]Epoch 6:  25%|██▌       | 76/300 [00:39<01:58,  1.89it/s]Epoch 6:  26%|██▌       | 77/300 [00:40<02:00,  1.86it/s]Epoch 6:  26%|██▌       | 78/300 [00:41<01:55,  1.92it/s]Epoch 6:  26%|██▋       | 79/300 [00:41<01:54,  1.93it/s]06/19/2022 20:03:55 - INFO - __main__ - global step: 940; train loss: 8.231032371520996; dev loss: 7.957188606262207
Epoch 6:  27%|██▋       | 80/300 [00:42<01:58,  1.86it/s]Epoch 6:  27%|██▋       | 81/300 [00:42<01:53,  1.92it/s]Epoch 6:  27%|██▋       | 82/300 [00:43<01:50,  1.97it/s]Epoch 6:  28%|██▊       | 83/300 [00:43<01:50,  1.96it/s]Epoch 6:  28%|██▊       | 84/300 [00:44<01:54,  1.89it/s]Epoch 6:  28%|██▊       | 85/300 [00:44<01:50,  1.95it/s]Epoch 6:  29%|██▊       | 86/300 [00:45<01:53,  1.89it/s]Epoch 6:  29%|██▉       | 87/300 [00:45<01:49,  1.94it/s]Epoch 6:  29%|██▉       | 88/300 [00:46<01:55,  1.84it/s]Epoch 6:  30%|██▉       | 89/300 [00:46<01:52,  1.87it/s]Epoch 6:  30%|███       | 90/300 [00:47<01:49,  1.93it/s]Epoch 6:  30%|███       | 91/300 [00:47<01:51,  1.88it/s]Epoch 6:  31%|███       | 92/300 [00:48<01:53,  1.84it/s]Epoch 6:  31%|███       | 93/300 [00:48<01:48,  1.90it/s]Epoch 6:  31%|███▏      | 94/300 [00:49<01:47,  1.91it/s]Epoch 6:  32%|███▏      | 95/300 [00:49<01:44,  1.96it/s]Epoch 6:  32%|███▏      | 96/300 [00:50<01:48,  1.89it/s]Epoch 6:  32%|███▏      | 97/300 [00:51<01:48,  1.87it/s]Epoch 6:  33%|███▎      | 98/300 [00:51<01:44,  1.94it/s]Epoch 6:  33%|███▎      | 99/300 [00:51<01:41,  1.97it/s]06/19/2022 20:04:05 - INFO - __main__ - global step: 950; train loss: 7.624955177307129; dev loss: 7.774794578552246
Epoch 6:  33%|███▎      | 100/300 [00:52<01:45,  1.90it/s]Epoch 6:  34%|███▎      | 101/300 [00:53<01:43,  1.93it/s]Epoch 6:  34%|███▍      | 102/300 [00:53<01:40,  1.97it/s]Epoch 6:  34%|███▍      | 103/300 [00:54<01:38,  2.00it/s]Epoch 6:  35%|███▍      | 104/300 [00:54<01:42,  1.92it/s]Epoch 6:  35%|███▌      | 105/300 [00:55<01:44,  1.87it/s]Epoch 6:  35%|███▌      | 106/300 [00:55<01:40,  1.93it/s]Epoch 6:  36%|███▌      | 107/300 [00:56<01:38,  1.97it/s]Epoch 6:  36%|███▌      | 108/300 [00:56<01:35,  2.00it/s]Epoch 6:  36%|███▋      | 109/300 [00:57<01:41,  1.88it/s]Epoch 6:  37%|███▋      | 110/300 [00:57<01:37,  1.94it/s]Epoch 6:  37%|███▋      | 111/300 [00:58<01:37,  1.94it/s]Epoch 6:  37%|███▋      | 112/300 [00:58<01:34,  1.99it/s]Epoch 6:  38%|███▊      | 113/300 [00:59<01:39,  1.87it/s]Epoch 6:  38%|███▊      | 114/300 [00:59<01:36,  1.94it/s]Epoch 6:  38%|███▊      | 115/300 [01:00<01:33,  1.97it/s]Epoch 6:  39%|███▊      | 116/300 [01:00<01:31,  2.01it/s]Epoch 6:  39%|███▉      | 117/300 [01:01<01:35,  1.92it/s]Epoch 6:  39%|███▉      | 118/300 [01:01<01:36,  1.88it/s]Epoch 6:  40%|███▉      | 119/300 [01:02<01:32,  1.95it/s]06/19/2022 20:04:16 - INFO - __main__ - global step: 960; train loss: 8.487767219543457; dev loss: 8.449682235717773
Epoch 6:  40%|████      | 120/300 [01:02<01:30,  1.99it/s]Epoch 6:  40%|████      | 121/300 [01:03<01:37,  1.83it/s]Epoch 6:  41%|████      | 122/300 [01:03<01:33,  1.90it/s]Epoch 6:  41%|████      | 123/300 [01:04<01:35,  1.86it/s]Epoch 6:  41%|████▏     | 124/300 [01:04<01:31,  1.92it/s]Epoch 6:  42%|████▏     | 125/300 [01:05<01:37,  1.80it/s]Epoch 6:  42%|████▏     | 126/300 [01:06<01:37,  1.79it/s]Epoch 6:  42%|████▏     | 127/300 [01:06<01:32,  1.87it/s]Epoch 6:  43%|████▎     | 128/300 [01:07<01:33,  1.84it/s]Epoch 6:  43%|████▎     | 129/300 [01:07<01:34,  1.81it/s]Epoch 6:  43%|████▎     | 130/300 [01:08<01:29,  1.89it/s]Epoch 6:  44%|████▎     | 131/300 [01:08<01:26,  1.96it/s]Epoch 6:  44%|████▍     | 132/300 [01:09<01:28,  1.89it/s]Epoch 6:  44%|████▍     | 133/300 [01:09<01:25,  1.95it/s]Epoch 6:  45%|████▍     | 134/300 [01:10<01:29,  1.86it/s]Epoch 6:  45%|████▌     | 135/300 [01:10<01:25,  1.93it/s]Epoch 6:  45%|████▌     | 136/300 [01:11<01:23,  1.96it/s]Epoch 6:  46%|████▌     | 137/300 [01:11<01:25,  1.90it/s]Epoch 6:  46%|████▌     | 138/300 [01:12<01:26,  1.86it/s]Epoch 6:  46%|████▋     | 139/300 [01:12<01:23,  1.93it/s]06/19/2022 20:04:26 - INFO - __main__ - global step: 970; train loss: 8.055936813354492; dev loss: 7.838738918304443
Epoch 6:  47%|████▋     | 140/300 [01:13<01:23,  1.93it/s]Epoch 6:  47%|████▋     | 141/300 [01:13<01:20,  1.96it/s]Epoch 6:  47%|████▋     | 142/300 [01:14<01:25,  1.86it/s]Epoch 6:  48%|████▊     | 143/300 [01:15<01:21,  1.92it/s]Epoch 6:  48%|████▊     | 144/300 [01:15<01:19,  1.96it/s]Epoch 6:  48%|████▊     | 145/300 [01:15<01:17,  2.00it/s]Epoch 6:  49%|████▊     | 146/300 [01:16<01:20,  1.92it/s]Epoch 6:  49%|████▉     | 147/300 [01:17<01:19,  1.92it/s]Epoch 6:  49%|████▉     | 148/300 [01:17<01:17,  1.97it/s]Epoch 6:  50%|████▉     | 149/300 [01:18<01:19,  1.90it/s]Epoch 6:  50%|█████     | 150/300 [01:18<01:21,  1.85it/s]Epoch 6:  50%|█████     | 151/300 [01:19<01:17,  1.92it/s]Epoch 6:  51%|█████     | 152/300 [01:19<01:19,  1.87it/s]Epoch 6:  51%|█████     | 153/300 [01:20<01:15,  1.94it/s]Epoch 6:  51%|█████▏    | 154/300 [01:20<01:18,  1.86it/s]Epoch 6:  52%|█████▏    | 155/300 [01:21<01:15,  1.92it/s]Epoch 6:  52%|█████▏    | 156/300 [01:21<01:13,  1.97it/s]Epoch 6:  52%|█████▏    | 157/300 [01:22<01:14,  1.91it/s]Epoch 6:  53%|█████▎    | 158/300 [01:22<01:18,  1.81it/s]Epoch 6:  53%|█████▎    | 159/300 [01:23<01:14,  1.89it/s]06/19/2022 20:04:37 - INFO - __main__ - global step: 980; train loss: 8.13936996459961; dev loss: 8.276250839233398
Epoch 6:  53%|█████▎    | 160/300 [01:23<01:14,  1.89it/s]Epoch 6:  54%|█████▎    | 161/300 [01:24<01:11,  1.95it/s]Epoch 6:  54%|█████▍    | 162/300 [01:24<01:09,  1.99it/s]Epoch 6:  54%|█████▍    | 163/300 [01:25<01:11,  1.91it/s]Epoch 6:  55%|█████▍    | 164/300 [01:25<01:09,  1.96it/s]Epoch 6:  55%|█████▌    | 165/300 [01:26<01:07,  2.00it/s]Epoch 6:  55%|█████▌    | 166/300 [01:26<01:06,  2.03it/s]Epoch 6:  56%|█████▌    | 167/300 [01:27<01:09,  1.93it/s]Epoch 6:  56%|█████▌    | 168/300 [01:27<01:08,  1.92it/s]Epoch 6:  56%|█████▋    | 169/300 [01:28<01:06,  1.97it/s]Epoch 6:  57%|█████▋    | 170/300 [01:28<01:05,  1.99it/s]Epoch 6:  57%|█████▋    | 171/300 [01:29<01:07,  1.92it/s]Epoch 6:  57%|█████▋    | 172/300 [01:30<01:05,  1.96it/s]Epoch 6:  58%|█████▊    | 173/300 [01:30<01:03,  2.00it/s]Epoch 6:  58%|█████▊    | 174/300 [01:30<01:02,  2.01it/s]Epoch 6:  58%|█████▊    | 175/300 [01:31<01:05,  1.90it/s]Epoch 6:  59%|█████▊    | 176/300 [01:32<01:03,  1.95it/s]Epoch 6:  59%|█████▉    | 177/300 [01:32<01:05,  1.88it/s]Epoch 6:  59%|█████▉    | 178/300 [01:33<01:06,  1.83it/s]Epoch 6:  60%|█████▉    | 179/300 [01:33<01:10,  1.72it/s]06/19/2022 20:04:47 - INFO - __main__ - global step: 990; train loss: 8.275720596313477; dev loss: 8.229758262634277
Epoch 6:  60%|██████    | 180/300 [01:34<01:06,  1.81it/s]Epoch 6:  60%|██████    | 181/300 [01:34<01:03,  1.88it/s]Epoch 6:  61%|██████    | 182/300 [01:35<01:01,  1.93it/s]Epoch 6:  61%|██████    | 183/300 [01:35<01:04,  1.82it/s]Epoch 6:  61%|██████▏   | 184/300 [01:36<01:02,  1.86it/s]Epoch 6:  62%|██████▏   | 185/300 [01:36<00:59,  1.93it/s]Epoch 6:  62%|██████▏   | 186/300 [01:37<00:57,  1.97it/s]Epoch 6:  62%|██████▏   | 187/300 [01:37<00:59,  1.90it/s]Epoch 6:  63%|██████▎   | 188/300 [01:38<01:00,  1.85it/s]Epoch 6:  63%|██████▎   | 189/300 [01:39<01:00,  1.82it/s]Epoch 6:  63%|██████▎   | 190/300 [01:39<01:00,  1.81it/s]Epoch 6:  64%|██████▎   | 191/300 [01:40<01:00,  1.79it/s]Epoch 6:  64%|██████▍   | 192/300 [01:40<01:00,  1.77it/s]Epoch 6:  64%|██████▍   | 193/300 [01:41<01:00,  1.78it/s]Epoch 6:  65%|██████▍   | 194/300 [01:41<00:56,  1.86it/s]Epoch 6:  65%|██████▌   | 195/300 [01:42<00:55,  1.88it/s]Epoch 6:  65%|██████▌   | 196/300 [01:42<00:56,  1.85it/s]Epoch 6:  66%|██████▌   | 197/300 [01:43<00:54,  1.91it/s]Epoch 6:  66%|██████▌   | 198/300 [01:43<00:52,  1.94it/s]Epoch 6:  66%|██████▋   | 199/300 [01:44<00:52,  1.94it/s]06/19/2022 20:04:58 - INFO - __main__ - global step: 1000; train loss: 8.025960922241211; dev loss: 8.557807922363281
Epoch 6:  67%|██████▋   | 200/300 [01:45<00:53,  1.87it/s]Epoch 6:  67%|██████▋   | 201/300 [01:45<00:51,  1.93it/s]Epoch 6:  67%|██████▋   | 202/300 [01:45<00:49,  1.98it/s]Epoch 6:  68%|██████▊   | 203/300 [01:46<00:48,  2.01it/s]Epoch 6:  68%|██████▊   | 204/300 [01:47<00:49,  1.92it/s]Epoch 6:  68%|██████▊   | 205/300 [01:47<00:48,  1.97it/s]Epoch 6:  69%|██████▊   | 206/300 [01:48<00:47,  1.98it/s]Epoch 6:  69%|██████▉   | 207/300 [01:48<00:46,  2.01it/s]Epoch 6:  69%|██████▉   | 208/300 [01:49<00:48,  1.91it/s]Epoch 6:  70%|██████▉   | 209/300 [01:49<00:46,  1.96it/s]Epoch 6:  70%|███████   | 210/300 [01:50<00:47,  1.91it/s]Epoch 6:  70%|███████   | 211/300 [01:50<00:47,  1.87it/s]Epoch 6:  71%|███████   | 212/300 [01:51<00:48,  1.83it/s]Epoch 6:  71%|███████   | 213/300 [01:51<00:45,  1.90it/s]Epoch 6:  71%|███████▏  | 214/300 [01:52<00:43,  1.96it/s]Epoch 6:  72%|███████▏  | 215/300 [01:52<00:42,  1.99it/s]Epoch 6:  72%|███████▏  | 216/300 [01:53<00:41,  2.01it/s]Epoch 6:  72%|███████▏  | 217/300 [01:53<00:43,  1.93it/s]Epoch 6:  73%|███████▎  | 218/300 [01:54<00:41,  1.96it/s]Epoch 6:  73%|███████▎  | 219/300 [01:54<00:41,  1.95it/s]06/19/2022 20:05:08 - INFO - __main__ - global step: 1010; train loss: 7.652812957763672; dev loss: 7.5873284339904785
Epoch 6:  73%|███████▎  | 220/300 [01:55<00:40,  1.99it/s]Epoch 6:  74%|███████▎  | 221/300 [01:55<00:41,  1.91it/s]Epoch 6:  74%|███████▍  | 222/300 [01:56<00:40,  1.91it/s]Epoch 6:  74%|███████▍  | 223/300 [01:56<00:39,  1.96it/s]Epoch 6:  75%|███████▍  | 224/300 [01:57<00:37,  2.00it/s]Epoch 6:  75%|███████▌  | 225/300 [01:57<00:39,  1.92it/s]Epoch 6:  75%|███████▌  | 226/300 [01:58<00:37,  1.96it/s]Epoch 6:  76%|███████▌  | 227/300 [01:58<00:36,  2.00it/s]Epoch 6:  76%|███████▌  | 228/300 [01:59<00:35,  2.01it/s]Epoch 6:  76%|███████▋  | 229/300 [01:59<00:36,  1.93it/s]Epoch 6:  77%|███████▋  | 230/300 [02:00<00:35,  1.98it/s]Epoch 6:  77%|███████▋  | 231/300 [02:00<00:35,  1.96it/s]Epoch 6:  77%|███████▋  | 232/300 [02:01<00:35,  1.90it/s]Epoch 6:  78%|███████▊  | 233/300 [02:02<00:37,  1.77it/s]Epoch 6:  78%|███████▊  | 234/300 [02:02<00:35,  1.86it/s]Epoch 6:  78%|███████▊  | 235/300 [02:03<00:33,  1.92it/s]Epoch 6:  79%|███████▊  | 236/300 [02:03<00:32,  1.96it/s]Epoch 6:  79%|███████▉  | 237/300 [02:04<00:33,  1.89it/s]Epoch 6:  79%|███████▉  | 238/300 [02:04<00:31,  1.94it/s]Epoch 6:  80%|███████▉  | 239/300 [02:05<00:30,  1.98it/s]06/19/2022 20:05:18 - INFO - __main__ - global step: 1020; train loss: 8.006285667419434; dev loss: 7.845449924468994
Epoch 6:  80%|████████  | 240/300 [02:05<00:31,  1.91it/s]Epoch 6:  80%|████████  | 241/300 [02:06<00:31,  1.86it/s]Epoch 6:  81%|████████  | 242/300 [02:06<00:31,  1.83it/s]Epoch 6:  81%|████████  | 243/300 [02:07<00:30,  1.89it/s]Epoch 6:  81%|████████▏ | 244/300 [02:07<00:28,  1.94it/s]Epoch 6:  82%|████████▏ | 245/300 [02:08<00:27,  1.97it/s]Epoch 6:  82%|████████▏ | 246/300 [02:08<00:28,  1.91it/s]Epoch 6:  82%|████████▏ | 247/300 [02:09<00:27,  1.92it/s]Epoch 6:  83%|████████▎ | 248/300 [02:09<00:26,  1.97it/s]Epoch 6:  83%|████████▎ | 249/300 [02:10<00:25,  2.01it/s]Epoch 6:  83%|████████▎ | 250/300 [02:10<00:26,  1.92it/s]Epoch 6:  84%|████████▎ | 251/300 [02:11<00:26,  1.87it/s]Epoch 6:  84%|████████▍ | 252/300 [02:11<00:24,  1.93it/s]Epoch 6:  84%|████████▍ | 253/300 [02:12<00:23,  1.98it/s]Epoch 6:  85%|████████▍ | 254/300 [02:12<00:24,  1.90it/s]Epoch 6:  85%|████████▌ | 255/300 [02:13<00:23,  1.94it/s]Epoch 6:  85%|████████▌ | 256/300 [02:13<00:23,  1.90it/s]Epoch 6:  86%|████████▌ | 257/300 [02:14<00:21,  1.96it/s]Epoch 6:  86%|████████▌ | 258/300 [02:14<00:22,  1.90it/s]Epoch 6:  86%|████████▋ | 259/300 [02:15<00:21,  1.91it/s]06/19/2022 20:05:29 - INFO - __main__ - global step: 1030; train loss: 7.527628421783447; dev loss: 7.526989936828613
Epoch 6:  87%|████████▋ | 260/300 [02:16<00:20,  1.94it/s]Epoch 6:  87%|████████▋ | 261/300 [02:16<00:19,  1.98it/s]Epoch 6:  87%|████████▋ | 262/300 [02:17<00:19,  1.91it/s]Epoch 6:  88%|████████▊ | 263/300 [02:17<00:18,  1.96it/s]Epoch 6:  88%|████████▊ | 264/300 [02:18<00:18,  2.00it/s]Epoch 6:  88%|████████▊ | 265/300 [02:18<00:17,  2.03it/s]Epoch 6:  89%|████████▊ | 266/300 [02:19<00:17,  1.94it/s]Epoch 6:  89%|████████▉ | 267/300 [02:19<00:16,  1.98it/s]Epoch 6:  89%|████████▉ | 268/300 [02:20<00:16,  1.94it/s]Epoch 6:  90%|████████▉ | 269/300 [02:20<00:15,  1.98it/s]Epoch 6:  90%|█████████ | 270/300 [02:21<00:15,  2.00it/s]Epoch 6:  90%|█████████ | 271/300 [02:21<00:15,  1.92it/s]Epoch 6:  91%|█████████ | 272/300 [02:22<00:14,  1.95it/s]Epoch 6:  91%|█████████ | 273/300 [02:22<00:13,  2.00it/s]Epoch 6:  91%|█████████▏| 274/300 [02:23<00:12,  2.03it/s]Epoch 6:  92%|█████████▏| 275/300 [02:23<00:13,  1.89it/s]Epoch 6:  92%|█████████▏| 276/300 [02:24<00:12,  1.94it/s]Epoch 6:  92%|█████████▏| 277/300 [02:24<00:11,  1.98it/s]Epoch 6:  93%|█████████▎| 278/300 [02:25<00:10,  2.01it/s]Epoch 6:  93%|█████████▎| 279/300 [02:25<00:11,  1.89it/s]06/19/2022 20:05:39 - INFO - __main__ - global step: 1040; train loss: 8.036233901977539; dev loss: 8.137542724609375
Epoch 6:  93%|█████████▎| 280/300 [02:26<00:10,  1.94it/s]Epoch 6:  94%|█████████▎| 281/300 [02:26<00:09,  1.98it/s]Epoch 6:  94%|█████████▍| 282/300 [02:27<00:09,  1.99it/s]Epoch 6:  94%|█████████▍| 283/300 [02:27<00:09,  1.83it/s]Epoch 6:  95%|█████████▍| 284/300 [02:28<00:08,  1.88it/s]Epoch 6:  95%|█████████▌| 285/300 [02:28<00:07,  1.88it/s]Epoch 6:  95%|█████████▌| 286/300 [02:29<00:07,  1.85it/s]Epoch 6:  96%|█████████▌| 287/300 [02:29<00:07,  1.82it/s]Epoch 6:  96%|█████████▌| 288/300 [02:30<00:06,  1.81it/s]Epoch 6:  96%|█████████▋| 289/300 [02:31<00:06,  1.80it/s]Epoch 6:  97%|█████████▋| 290/300 [02:31<00:05,  1.81it/s]Epoch 6:  97%|█████████▋| 291/300 [02:32<00:05,  1.78it/s]Epoch 6:  97%|█████████▋| 292/300 [02:32<00:04,  1.87it/s]Epoch 6:  98%|█████████▊| 293/300 [02:33<00:03,  1.92it/s]Epoch 6:  98%|█████████▊| 294/300 [02:33<00:03,  1.97it/s]Epoch 6:  98%|█████████▊| 295/300 [02:34<00:02,  1.99it/s]Epoch 6:  99%|█████████▊| 296/300 [02:34<00:02,  1.91it/s]Epoch 6:  99%|█████████▉| 297/300 [02:35<00:01,  1.87it/s]Epoch 6:  99%|█████████▉| 298/300 [02:35<00:01,  1.84it/s]Epoch 6: 100%|█████████▉| 299/300 [02:36<00:00,  1.83it/s]06/19/2022 20:05:50 - INFO - __main__ - global step: 1050; train loss: 7.7541069984436035; dev loss: 7.7934980392456055
Epoch 6: 100%|██████████| 300/300 [02:36<00:00,  1.82it/s]Epoch 6: 100%|██████████| 300/300 [02:36<00:00,  1.91it/s]
Epoch 7:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 7:   0%|          | 1/300 [00:00<02:22,  2.09it/s]Epoch 7:   1%|          | 2/300 [00:00<02:21,  2.11it/s]Epoch 7:   1%|          | 3/300 [00:01<02:33,  1.94it/s]Epoch 7:   1%|▏         | 4/300 [00:02<02:41,  1.84it/s]Epoch 7:   2%|▏         | 5/300 [00:02<02:37,  1.88it/s]Epoch 7:   2%|▏         | 6/300 [00:03<02:35,  1.89it/s]Epoch 7:   2%|▏         | 7/300 [00:03<02:30,  1.94it/s]Epoch 7:   3%|▎         | 8/300 [00:04<02:36,  1.87it/s]Epoch 7:   3%|▎         | 9/300 [00:04<02:30,  1.93it/s]Epoch 7:   3%|▎         | 10/300 [00:05<02:26,  1.97it/s]Epoch 7:   4%|▎         | 11/300 [00:05<02:30,  1.92it/s]Epoch 7:   4%|▍         | 12/300 [00:06<02:38,  1.82it/s]Epoch 7:   4%|▍         | 13/300 [00:06<02:31,  1.90it/s]Epoch 7:   5%|▍         | 14/300 [00:07<02:26,  1.95it/s]Epoch 7:   5%|▌         | 15/300 [00:07<02:22,  2.00it/s]Epoch 7:   5%|▌         | 16/300 [00:08<02:28,  1.92it/s]Epoch 7:   6%|▌         | 17/300 [00:08<02:23,  1.97it/s]Epoch 7:   6%|▌         | 18/300 [00:09<02:21,  2.00it/s]Epoch 7:   6%|▋         | 19/300 [00:09<02:18,  2.02it/s]06/19/2022 20:06:00 - INFO - __main__ - global step: 1060; train loss: 7.401176452636719; dev loss: 7.711983680725098
Epoch 7:   7%|▋         | 20/300 [00:10<02:32,  1.84it/s]Epoch 7:   7%|▋         | 21/300 [00:10<02:33,  1.82it/s]Epoch 7:   7%|▋         | 22/300 [00:11<02:30,  1.85it/s]Epoch 7:   8%|▊         | 23/300 [00:12<02:25,  1.90it/s]Epoch 7:   8%|▊         | 24/300 [00:12<02:27,  1.87it/s]Epoch 7:   8%|▊         | 25/300 [00:13<02:37,  1.75it/s]Epoch 7:   9%|▊         | 26/300 [00:13<02:29,  1.83it/s]Epoch 7:   9%|▉         | 27/300 [00:14<02:23,  1.90it/s]Epoch 7:   9%|▉         | 28/300 [00:14<02:22,  1.91it/s]Epoch 7:  10%|▉         | 29/300 [00:15<02:25,  1.86it/s]Epoch 7:  10%|█         | 30/300 [00:15<02:19,  1.93it/s]Epoch 7:  10%|█         | 31/300 [00:16<02:15,  1.98it/s]Epoch 7:  11%|█         | 32/300 [00:16<02:12,  2.02it/s]Epoch 7:  11%|█         | 33/300 [00:17<02:18,  1.93it/s]Epoch 7:  11%|█▏        | 34/300 [00:17<02:14,  1.98it/s]Epoch 7:  12%|█▏        | 35/300 [00:18<02:11,  2.02it/s]Epoch 7:  12%|█▏        | 36/300 [00:18<02:16,  1.94it/s]Epoch 7:  12%|█▏        | 37/300 [00:19<02:25,  1.81it/s]Epoch 7:  13%|█▎        | 38/300 [00:19<02:25,  1.80it/s]Epoch 7:  13%|█▎        | 39/300 [00:20<02:19,  1.88it/s]06/19/2022 20:06:11 - INFO - __main__ - global step: 1070; train loss: 7.703439235687256; dev loss: 7.167718410491943
Epoch 7:  13%|█▎        | 40/300 [00:21<02:20,  1.85it/s]Epoch 7:  14%|█▎        | 41/300 [00:21<02:22,  1.81it/s]Epoch 7:  14%|█▍        | 42/300 [00:22<02:22,  1.81it/s]Epoch 7:  14%|█▍        | 43/300 [00:22<02:16,  1.89it/s]Epoch 7:  15%|█▍        | 44/300 [00:23<02:12,  1.93it/s]Epoch 7:  15%|█▌        | 45/300 [00:23<02:15,  1.88it/s]Epoch 7:  15%|█▌        | 46/300 [00:24<02:14,  1.88it/s]Epoch 7:  16%|█▌        | 47/300 [00:24<02:09,  1.95it/s]Epoch 7:  16%|█▌        | 48/300 [00:25<02:06,  1.99it/s]Epoch 7:  16%|█▋        | 49/300 [00:25<02:04,  2.02it/s]Epoch 7:  17%|█▋        | 50/300 [00:26<02:09,  1.92it/s]Epoch 7:  17%|█▋        | 51/300 [00:26<02:06,  1.97it/s]Epoch 7:  17%|█▋        | 52/300 [00:27<02:03,  2.01it/s]Epoch 7:  18%|█▊        | 53/300 [00:27<02:01,  2.04it/s]Epoch 7:  18%|█▊        | 54/300 [00:28<02:12,  1.86it/s]Epoch 7:  18%|█▊        | 55/300 [00:28<02:12,  1.84it/s]Epoch 7:  19%|█▊        | 56/300 [00:29<02:07,  1.91it/s]Epoch 7:  19%|█▉        | 57/300 [00:29<02:03,  1.96it/s]Epoch 7:  19%|█▉        | 58/300 [00:30<02:08,  1.89it/s]Epoch 7:  20%|█▉        | 59/300 [00:30<02:07,  1.89it/s]06/19/2022 20:06:21 - INFO - __main__ - global step: 1080; train loss: 8.283387184143066; dev loss: 8.503787994384766
Epoch 7:  20%|██        | 60/300 [00:31<02:06,  1.90it/s]Epoch 7:  20%|██        | 61/300 [00:31<02:02,  1.96it/s]Epoch 7:  21%|██        | 62/300 [00:32<02:09,  1.84it/s]Epoch 7:  21%|██        | 63/300 [00:32<02:04,  1.91it/s]Epoch 7:  21%|██▏       | 64/300 [00:33<02:00,  1.96it/s]Epoch 7:  22%|██▏       | 65/300 [00:33<01:58,  1.99it/s]Epoch 7:  22%|██▏       | 66/300 [00:34<02:02,  1.91it/s]Epoch 7:  22%|██▏       | 67/300 [00:35<02:05,  1.86it/s]Epoch 7:  23%|██▎       | 68/300 [00:35<02:00,  1.93it/s]Epoch 7:  23%|██▎       | 69/300 [00:36<01:57,  1.97it/s]Epoch 7:  23%|██▎       | 70/300 [00:36<02:06,  1.82it/s]Epoch 7:  24%|██▎       | 71/300 [00:37<02:02,  1.88it/s]Epoch 7:  24%|██▍       | 72/300 [00:37<01:58,  1.93it/s]Epoch 7:  24%|██▍       | 73/300 [00:38<01:54,  1.98it/s]Epoch 7:  25%|██▍       | 74/300 [00:38<02:01,  1.86it/s]Epoch 7:  25%|██▌       | 75/300 [00:39<01:56,  1.93it/s]Epoch 7:  25%|██▌       | 76/300 [00:39<01:53,  1.98it/s]Epoch 7:  26%|██▌       | 77/300 [00:40<01:54,  1.95it/s]Epoch 7:  26%|██▌       | 78/300 [00:40<01:51,  1.99it/s]Epoch 7:  26%|██▋       | 79/300 [00:41<01:56,  1.90it/s]06/19/2022 20:06:32 - INFO - __main__ - global step: 1090; train loss: 7.917905330657959; dev loss: 7.542339324951172
Epoch 7:  27%|██▋       | 80/300 [00:41<01:53,  1.95it/s]Epoch 7:  27%|██▋       | 81/300 [00:42<01:49,  1.99it/s]Epoch 7:  27%|██▋       | 82/300 [00:42<01:47,  2.02it/s]Epoch 7:  28%|██▊       | 83/300 [00:43<01:52,  1.93it/s]Epoch 7:  28%|██▊       | 84/300 [00:43<01:49,  1.97it/s]Epoch 7:  28%|██▊       | 85/300 [00:44<01:50,  1.95it/s]Epoch 7:  29%|██▊       | 86/300 [00:44<01:47,  1.98it/s]Epoch 7:  29%|██▉       | 87/300 [00:45<01:54,  1.86it/s]Epoch 7:  29%|██▉       | 88/300 [00:45<01:49,  1.93it/s]Epoch 7:  30%|██▉       | 89/300 [00:46<01:46,  1.98it/s]Epoch 7:  30%|███       | 90/300 [00:46<01:44,  2.01it/s]Epoch 7:  30%|███       | 91/300 [00:47<01:48,  1.93it/s]Epoch 7:  31%|███       | 92/300 [00:47<01:45,  1.97it/s]Epoch 7:  31%|███       | 93/300 [00:48<01:45,  1.96it/s]Epoch 7:  31%|███▏      | 94/300 [00:48<01:49,  1.89it/s]Epoch 7:  32%|███▏      | 95/300 [00:49<01:51,  1.85it/s]Epoch 7:  32%|███▏      | 96/300 [00:50<01:47,  1.90it/s]Epoch 7:  32%|███▏      | 97/300 [00:50<01:44,  1.95it/s]Epoch 7:  33%|███▎      | 98/300 [00:51<01:46,  1.89it/s]Epoch 7:  33%|███▎      | 99/300 [00:51<01:51,  1.80it/s]06/19/2022 20:06:42 - INFO - __main__ - global step: 1100; train loss: 8.448095321655273; dev loss: 8.039911270141602
Epoch 7:  33%|███▎      | 100/300 [00:52<01:46,  1.88it/s]Epoch 7:  34%|███▎      | 101/300 [00:52<01:42,  1.94it/s]Epoch 7:  34%|███▍      | 102/300 [00:53<01:40,  1.98it/s]Epoch 7:  34%|███▍      | 103/300 [00:53<01:37,  2.01it/s]Epoch 7:  35%|███▍      | 104/300 [00:54<01:41,  1.93it/s]Epoch 7:  35%|███▌      | 105/300 [00:54<01:38,  1.98it/s]Epoch 7:  35%|███▌      | 106/300 [00:55<01:36,  2.01it/s]Epoch 7:  36%|███▌      | 107/300 [00:55<01:35,  2.03it/s]Epoch 7:  36%|███▌      | 108/300 [00:56<01:38,  1.94it/s]Epoch 7:  36%|███▋      | 109/300 [00:56<01:36,  1.99it/s]Epoch 7:  37%|███▋      | 110/300 [00:57<01:36,  1.96it/s]Epoch 7:  37%|███▋      | 111/300 [00:57<01:34,  2.00it/s]Epoch 7:  37%|███▋      | 112/300 [00:58<01:38,  1.91it/s]Epoch 7:  38%|███▊      | 113/300 [00:58<01:37,  1.93it/s]Epoch 7:  38%|███▊      | 114/300 [00:59<01:36,  1.93it/s]Epoch 7:  38%|███▊      | 115/300 [00:59<01:33,  1.97it/s]Epoch 7:  39%|███▊      | 116/300 [01:00<01:41,  1.81it/s]Epoch 7:  39%|███▉      | 117/300 [01:00<01:36,  1.89it/s]Epoch 7:  39%|███▉      | 118/300 [01:01<01:37,  1.86it/s]Epoch 7:  40%|███▉      | 119/300 [01:02<01:38,  1.83it/s]06/19/2022 20:06:52 - INFO - __main__ - global step: 1110; train loss: 7.916604042053223; dev loss: 7.6424560546875
Epoch 7:  40%|████      | 120/300 [01:02<01:44,  1.73it/s]Epoch 7:  40%|████      | 121/300 [01:03<01:37,  1.83it/s]Epoch 7:  41%|████      | 122/300 [01:03<01:33,  1.90it/s]Epoch 7:  41%|████      | 123/300 [01:04<01:30,  1.96it/s]Epoch 7:  41%|████▏     | 124/300 [01:04<01:32,  1.90it/s]Epoch 7:  42%|████▏     | 125/300 [01:05<01:33,  1.86it/s]Epoch 7:  42%|████▏     | 126/300 [01:05<01:30,  1.93it/s]Epoch 7:  42%|████▏     | 127/300 [01:06<01:27,  1.97it/s]Epoch 7:  43%|████▎     | 128/300 [01:06<01:34,  1.81it/s]Epoch 7:  43%|████▎     | 129/300 [01:07<01:30,  1.89it/s]Epoch 7:  43%|████▎     | 130/300 [01:07<01:27,  1.94it/s]Epoch 7:  44%|████▎     | 131/300 [01:08<01:29,  1.90it/s]Epoch 7:  44%|████▍     | 132/300 [01:08<01:26,  1.95it/s]Epoch 7:  44%|████▍     | 133/300 [01:09<01:30,  1.84it/s]Epoch 7:  45%|████▍     | 134/300 [01:09<01:26,  1.91it/s]Epoch 7:  45%|████▌     | 135/300 [01:10<01:23,  1.97it/s]Epoch 7:  45%|████▌     | 136/300 [01:10<01:22,  2.00it/s]Epoch 7:  46%|████▌     | 137/300 [01:11<01:25,  1.91it/s]Epoch 7:  46%|████▌     | 138/300 [01:11<01:22,  1.96it/s]Epoch 7:  46%|████▋     | 139/300 [01:12<01:24,  1.90it/s]06/19/2022 20:07:03 - INFO - __main__ - global step: 1120; train loss: 7.975966453552246; dev loss: 7.804845333099365
Epoch 7:  47%|████▋     | 140/300 [01:13<01:25,  1.87it/s]Epoch 7:  47%|████▋     | 141/300 [01:13<01:26,  1.85it/s]Epoch 7:  47%|████▋     | 142/300 [01:14<01:23,  1.90it/s]Epoch 7:  48%|████▊     | 143/300 [01:14<01:21,  1.94it/s]Epoch 7:  48%|████▊     | 144/300 [01:15<01:18,  1.98it/s]Epoch 7:  48%|████▊     | 145/300 [01:15<01:21,  1.90it/s]Epoch 7:  49%|████▊     | 146/300 [01:16<01:18,  1.96it/s]Epoch 7:  49%|████▉     | 147/300 [01:16<01:16,  1.99it/s]Epoch 7:  49%|████▉     | 148/300 [01:17<01:16,  2.00it/s]Epoch 7:  50%|████▉     | 149/300 [01:17<01:18,  1.92it/s]Epoch 7:  50%|█████     | 150/300 [01:18<01:16,  1.95it/s]Epoch 7:  50%|█████     | 151/300 [01:18<01:15,  1.99it/s]Epoch 7:  51%|█████     | 152/300 [01:19<01:13,  2.02it/s]Epoch 7:  51%|█████     | 153/300 [01:19<01:16,  1.92it/s]Epoch 7:  51%|█████▏    | 154/300 [01:20<01:15,  1.94it/s]Epoch 7:  52%|█████▏    | 155/300 [01:20<01:14,  1.94it/s]Epoch 7:  52%|█████▏    | 156/300 [01:21<01:12,  1.99it/s]Epoch 7:  52%|█████▏    | 157/300 [01:21<01:11,  2.01it/s]Epoch 7:  53%|█████▎    | 158/300 [01:22<01:14,  1.92it/s]Epoch 7:  53%|█████▎    | 159/300 [01:22<01:15,  1.88it/s]06/19/2022 20:07:13 - INFO - __main__ - global step: 1130; train loss: 8.07479190826416; dev loss: 7.863650321960449
Epoch 7:  53%|█████▎    | 160/300 [01:23<01:12,  1.94it/s]Epoch 7:  54%|█████▎    | 161/300 [01:23<01:09,  1.99it/s]Epoch 7:  54%|█████▍    | 162/300 [01:24<01:13,  1.89it/s]Epoch 7:  54%|█████▍    | 163/300 [01:24<01:13,  1.85it/s]Epoch 7:  55%|█████▍    | 164/300 [01:25<01:12,  1.88it/s]Epoch 7:  55%|█████▌    | 165/300 [01:25<01:11,  1.89it/s]Epoch 7:  55%|█████▌    | 166/300 [01:26<01:15,  1.77it/s]Epoch 7:  56%|█████▌    | 167/300 [01:27<01:14,  1.78it/s]Epoch 7:  56%|█████▌    | 168/300 [01:27<01:10,  1.86it/s]Epoch 7:  56%|█████▋    | 169/300 [01:28<01:08,  1.91it/s]Epoch 7:  57%|█████▋    | 170/300 [01:28<01:09,  1.86it/s]Epoch 7:  57%|█████▋    | 171/300 [01:29<01:07,  1.90it/s]Epoch 7:  57%|█████▋    | 172/300 [01:29<01:05,  1.95it/s]Epoch 7:  58%|█████▊    | 173/300 [01:30<01:05,  1.93it/s]Epoch 7:  58%|█████▊    | 174/300 [01:30<01:07,  1.87it/s]Epoch 7:  58%|█████▊    | 175/300 [01:31<01:04,  1.93it/s]Epoch 7:  59%|█████▊    | 176/300 [01:31<01:03,  1.97it/s]Epoch 7:  59%|█████▉    | 177/300 [01:32<01:04,  1.90it/s]Epoch 7:  59%|█████▉    | 178/300 [01:32<01:06,  1.84it/s]Epoch 7:  60%|█████▉    | 179/300 [01:33<01:03,  1.90it/s]06/19/2022 20:07:24 - INFO - __main__ - global step: 1140; train loss: 7.524752616882324; dev loss: 7.6961669921875
Epoch 7:  60%|██████    | 180/300 [01:33<01:05,  1.83it/s]Epoch 7:  60%|██████    | 181/300 [01:34<01:02,  1.89it/s]Epoch 7:  61%|██████    | 182/300 [01:35<01:07,  1.75it/s]Epoch 7:  61%|██████    | 183/300 [01:35<01:03,  1.84it/s]Epoch 7:  61%|██████▏   | 184/300 [01:36<01:01,  1.88it/s]Epoch 7:  62%|██████▏   | 185/300 [01:36<00:59,  1.92it/s]Epoch 7:  62%|██████▏   | 186/300 [01:37<00:58,  1.95it/s]Epoch 7:  62%|██████▏   | 187/300 [01:37<01:00,  1.87it/s]Epoch 7:  63%|██████▎   | 188/300 [01:38<00:58,  1.91it/s]Epoch 7:  63%|██████▎   | 189/300 [01:38<00:57,  1.92it/s]Epoch 7:  63%|██████▎   | 190/300 [01:39<00:57,  1.90it/s]Epoch 7:  64%|██████▎   | 191/300 [01:39<00:59,  1.84it/s]Epoch 7:  64%|██████▍   | 192/300 [01:40<00:56,  1.90it/s]Epoch 7:  64%|██████▍   | 193/300 [01:40<00:56,  1.89it/s]Epoch 7:  65%|██████▍   | 194/300 [01:41<00:54,  1.94it/s]Epoch 7:  65%|██████▌   | 195/300 [01:41<00:56,  1.86it/s]Epoch 7:  65%|██████▌   | 196/300 [01:42<00:54,  1.91it/s]Epoch 7:  66%|██████▌   | 197/300 [01:42<00:53,  1.93it/s]Epoch 7:  66%|██████▌   | 198/300 [01:43<00:54,  1.87it/s]Epoch 7:  66%|██████▋   | 199/300 [01:44<00:56,  1.79it/s]06/19/2022 20:07:34 - INFO - __main__ - global step: 1150; train loss: 7.586556434631348; dev loss: 7.899743556976318
Epoch 7:  67%|██████▋   | 200/300 [01:44<00:53,  1.86it/s]Epoch 7:  67%|██████▋   | 201/300 [01:45<00:51,  1.92it/s]Epoch 7:  67%|██████▋   | 202/300 [01:45<00:49,  1.97it/s]Epoch 7:  68%|██████▊   | 203/300 [01:46<00:51,  1.89it/s]Epoch 7:  68%|██████▊   | 204/300 [01:46<00:49,  1.95it/s]Epoch 7:  68%|██████▊   | 205/300 [01:47<00:48,  1.97it/s]Epoch 7:  69%|██████▊   | 206/300 [01:47<00:49,  1.91it/s]Epoch 7:  69%|██████▉   | 207/300 [01:48<00:52,  1.78it/s]Epoch 7:  69%|██████▉   | 208/300 [01:48<00:49,  1.86it/s]Epoch 7:  70%|██████▉   | 209/300 [01:49<00:47,  1.92it/s]Epoch 7:  70%|███████   | 210/300 [01:49<00:45,  1.97it/s]Epoch 7:  70%|███████   | 211/300 [01:50<00:45,  1.95it/s]Epoch 7:  71%|███████   | 212/300 [01:50<00:46,  1.88it/s]Epoch 7:  71%|███████   | 213/300 [01:51<00:44,  1.93it/s]Epoch 7:  71%|███████▏  | 214/300 [01:51<00:45,  1.89it/s]Epoch 7:  72%|███████▏  | 215/300 [01:52<00:45,  1.85it/s]Epoch 7:  72%|███████▏  | 216/300 [01:53<00:46,  1.81it/s]Epoch 7:  72%|███████▏  | 217/300 [01:53<00:44,  1.88it/s]Epoch 7:  73%|███████▎  | 218/300 [01:54<00:42,  1.93it/s]Epoch 7:  73%|███████▎  | 219/300 [01:54<00:41,  1.97it/s]06/19/2022 20:07:45 - INFO - __main__ - global step: 1160; train loss: 8.297609329223633; dev loss: 8.233110427856445
Epoch 7:  73%|███████▎  | 220/300 [01:55<00:43,  1.82it/s]Epoch 7:  74%|███████▎  | 221/300 [01:55<00:41,  1.89it/s]Epoch 7:  74%|███████▍  | 222/300 [01:56<00:39,  1.95it/s]Epoch 7:  74%|███████▍  | 223/300 [01:56<00:39,  1.94it/s]Epoch 7:  75%|███████▍  | 224/300 [01:57<00:40,  1.87it/s]Epoch 7:  75%|███████▌  | 225/300 [01:57<00:38,  1.93it/s]Epoch 7:  75%|███████▌  | 226/300 [01:58<00:37,  1.98it/s]Epoch 7:  76%|███████▌  | 227/300 [01:58<00:36,  2.01it/s]Epoch 7:  76%|███████▌  | 228/300 [01:59<00:37,  1.93it/s]Epoch 7:  76%|███████▋  | 229/300 [01:59<00:35,  1.98it/s]Epoch 7:  77%|███████▋  | 230/300 [02:00<00:35,  1.96it/s]Epoch 7:  77%|███████▋  | 231/300 [02:00<00:35,  1.94it/s]Epoch 7:  77%|███████▋  | 232/300 [02:01<00:36,  1.87it/s]Epoch 7:  78%|███████▊  | 233/300 [02:01<00:34,  1.94it/s]Epoch 7:  78%|███████▊  | 234/300 [02:02<00:34,  1.93it/s]Epoch 7:  78%|███████▊  | 235/300 [02:02<00:33,  1.97it/s]Epoch 7:  79%|███████▊  | 236/300 [02:03<00:33,  1.89it/s]Epoch 7:  79%|███████▉  | 237/300 [02:03<00:32,  1.95it/s]Epoch 7:  79%|███████▉  | 238/300 [02:04<00:32,  1.89it/s]Epoch 7:  80%|███████▉  | 239/300 [02:04<00:31,  1.95it/s]06/19/2022 20:07:55 - INFO - __main__ - global step: 1170; train loss: 8.329370498657227; dev loss: 8.041872024536133
Epoch 7:  80%|████████  | 240/300 [02:05<00:30,  1.99it/s]Epoch 7:  80%|████████  | 241/300 [02:05<00:31,  1.90it/s]Epoch 7:  81%|████████  | 242/300 [02:06<00:29,  1.95it/s]Epoch 7:  81%|████████  | 243/300 [02:06<00:28,  1.99it/s]Epoch 7:  81%|████████▏ | 244/300 [02:07<00:27,  2.03it/s]Epoch 7:  82%|████████▏ | 245/300 [02:07<00:28,  1.94it/s]Epoch 7:  82%|████████▏ | 246/300 [02:08<00:27,  1.98it/s]Epoch 7:  82%|████████▏ | 247/300 [02:08<00:26,  2.02it/s]Epoch 7:  83%|████████▎ | 248/300 [02:09<00:26,  1.98it/s]Epoch 7:  83%|████████▎ | 249/300 [02:09<00:26,  1.91it/s]Epoch 7:  83%|████████▎ | 250/300 [02:10<00:25,  1.95it/s]Epoch 7:  84%|████████▎ | 251/300 [02:10<00:24,  1.98it/s]Epoch 7:  84%|████████▍ | 252/300 [02:11<00:25,  1.91it/s]Epoch 7:  84%|████████▍ | 253/300 [02:12<00:25,  1.86it/s]Epoch 7:  85%|████████▍ | 254/300 [02:12<00:23,  1.93it/s]Epoch 7:  85%|████████▌ | 255/300 [02:13<00:22,  1.97it/s]Epoch 7:  85%|████████▌ | 256/300 [02:13<00:22,  1.95it/s]Epoch 7:  86%|████████▌ | 257/300 [02:14<00:23,  1.80it/s]Epoch 7:  86%|████████▌ | 258/300 [02:14<00:22,  1.86it/s]Epoch 7:  86%|████████▋ | 259/300 [02:15<00:21,  1.91it/s]06/19/2022 20:08:05 - INFO - __main__ - global step: 1180; train loss: 7.646694183349609; dev loss: 7.809050559997559
Epoch 7:  87%|████████▋ | 260/300 [02:15<00:20,  1.96it/s]Epoch 7:  87%|████████▋ | 261/300 [02:16<00:20,  1.88it/s]Epoch 7:  87%|████████▋ | 262/300 [02:16<00:19,  1.93it/s]Epoch 7:  88%|████████▊ | 263/300 [02:17<00:19,  1.92it/s]Epoch 7:  88%|████████▊ | 264/300 [02:17<00:18,  1.92it/s]Epoch 7:  88%|████████▊ | 265/300 [02:18<00:18,  1.92it/s]Epoch 7:  89%|████████▊ | 266/300 [02:18<00:18,  1.86it/s]Epoch 7:  89%|████████▉ | 267/300 [02:19<00:17,  1.92it/s]Epoch 7:  89%|████████▉ | 268/300 [02:19<00:16,  1.97it/s]Epoch 7:  90%|████████▉ | 269/300 [02:20<00:15,  2.00it/s]Epoch 7:  90%|█████████ | 270/300 [02:20<00:16,  1.87it/s]Epoch 7:  90%|█████████ | 271/300 [02:21<00:15,  1.93it/s]Epoch 7:  91%|█████████ | 272/300 [02:21<00:14,  1.93it/s]Epoch 7:  91%|█████████ | 273/300 [02:22<00:13,  1.98it/s]Epoch 7:  91%|█████████▏| 274/300 [02:23<00:14,  1.82it/s]Epoch 7:  92%|█████████▏| 275/300 [02:23<00:13,  1.89it/s]Epoch 7:  92%|█████████▏| 276/300 [02:24<00:12,  1.85it/s]Epoch 7:  92%|█████████▏| 277/300 [02:24<00:11,  1.92it/s]Epoch 7:  93%|█████████▎| 278/300 [02:25<00:11,  1.85it/s]Epoch 7:  93%|█████████▎| 279/300 [02:25<00:11,  1.83it/s]06/19/2022 20:08:16 - INFO - __main__ - global step: 1190; train loss: 8.058427810668945; dev loss: 8.310836791992188
Epoch 7:  93%|█████████▎| 280/300 [02:26<00:11,  1.82it/s]Epoch 7:  94%|█████████▎| 281/300 [02:26<00:10,  1.85it/s]Epoch 7:  94%|█████████▍| 282/300 [02:27<00:09,  1.82it/s]Epoch 7:  94%|█████████▍| 283/300 [02:27<00:08,  1.89it/s]Epoch 7:  95%|█████████▍| 284/300 [02:28<00:08,  1.94it/s]Epoch 7:  95%|█████████▌| 285/300 [02:28<00:07,  1.89it/s]Epoch 7:  95%|█████████▌| 286/300 [02:29<00:07,  1.80it/s]Epoch 7:  96%|█████████▌| 287/300 [02:30<00:06,  1.88it/s]Epoch 7:  96%|█████████▌| 288/300 [02:30<00:06,  1.92it/s]Epoch 7:  96%|█████████▋| 289/300 [02:30<00:05,  1.97it/s]Epoch 7:  97%|█████████▋| 290/300 [02:31<00:05,  1.89it/s]Epoch 7:  97%|█████████▋| 291/300 [02:32<00:04,  1.93it/s]Epoch 7:  97%|█████████▋| 292/300 [02:32<00:04,  1.90it/s]Epoch 7:  98%|█████████▊| 293/300 [02:33<00:03,  1.94it/s]Epoch 7:  98%|█████████▊| 294/300 [02:33<00:03,  1.98it/s]Epoch 7:  98%|█████████▊| 295/300 [02:34<00:02,  1.90it/s]Epoch 7:  99%|█████████▊| 296/300 [02:34<00:02,  1.91it/s]Epoch 7:  99%|█████████▉| 297/300 [02:35<00:01,  1.96it/s]Epoch 7:  99%|█████████▉| 298/300 [02:35<00:01,  2.00it/s]Epoch 7: 100%|█████████▉| 299/300 [02:36<00:00,  1.84it/s]06/19/2022 20:08:27 - INFO - __main__ - global step: 1200; train loss: 7.524955749511719; dev loss: 7.221004486083984
Epoch 7: 100%|██████████| 300/300 [02:36<00:00,  1.80it/s]Epoch 7: 100%|██████████| 300/300 [02:36<00:00,  1.91it/s]
Epoch 8:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 8:   0%|          | 1/300 [00:00<02:26,  2.04it/s]Epoch 8:   1%|          | 2/300 [00:00<02:24,  2.07it/s]Epoch 8:   1%|          | 3/300 [00:01<02:38,  1.87it/s]Epoch 8:   1%|▏         | 4/300 [00:02<02:33,  1.93it/s]Epoch 8:   2%|▏         | 5/300 [00:02<02:36,  1.88it/s]Epoch 8:   2%|▏         | 6/300 [00:03<02:38,  1.85it/s]Epoch 8:   2%|▏         | 7/300 [00:03<02:41,  1.82it/s]Epoch 8:   3%|▎         | 8/300 [00:04<02:37,  1.85it/s]Epoch 8:   3%|▎         | 9/300 [00:04<02:35,  1.87it/s]Epoch 8:   3%|▎         | 10/300 [00:05<02:31,  1.91it/s]Epoch 8:   4%|▎         | 11/300 [00:05<02:35,  1.86it/s]Epoch 8:   4%|▍         | 12/300 [00:06<02:29,  1.92it/s]Epoch 8:   4%|▍         | 13/300 [00:06<02:25,  1.97it/s]Epoch 8:   5%|▍         | 14/300 [00:07<02:31,  1.89it/s]Epoch 8:   5%|▌         | 15/300 [00:07<02:34,  1.84it/s]Epoch 8:   5%|▌         | 16/300 [00:08<02:32,  1.86it/s]Epoch 8:   6%|▌         | 17/300 [00:08<02:27,  1.92it/s]Epoch 8:   6%|▌         | 18/300 [00:09<02:29,  1.88it/s]Epoch 8:   6%|▋         | 19/300 [00:10<02:24,  1.94it/s]06/19/2022 20:08:37 - INFO - __main__ - global step: 1210; train loss: 7.985090732574463; dev loss: 8.190752029418945
Epoch 8:   7%|▋         | 20/300 [00:10<02:29,  1.87it/s]Epoch 8:   7%|▋         | 21/300 [00:11<02:24,  1.93it/s]Epoch 8:   7%|▋         | 22/300 [00:11<02:21,  1.97it/s]Epoch 8:   8%|▊         | 23/300 [00:12<02:17,  2.01it/s]Epoch 8:   8%|▊         | 24/300 [00:12<02:22,  1.93it/s]Epoch 8:   8%|▊         | 25/300 [00:13<02:23,  1.92it/s]Epoch 8:   9%|▊         | 26/300 [00:13<02:19,  1.96it/s]Epoch 8:   9%|▉         | 27/300 [00:14<02:16,  2.00it/s]Epoch 8:   9%|▉         | 28/300 [00:14<02:21,  1.92it/s]Epoch 8:  10%|▉         | 29/300 [00:15<02:24,  1.87it/s]Epoch 8:  10%|█         | 30/300 [00:15<02:19,  1.94it/s]Epoch 8:  10%|█         | 31/300 [00:16<02:16,  1.98it/s]Epoch 8:  11%|█         | 32/300 [00:16<02:21,  1.90it/s]Epoch 8:  11%|█         | 33/300 [00:17<02:17,  1.94it/s]Epoch 8:  11%|█▏        | 34/300 [00:17<02:14,  1.98it/s]Epoch 8:  12%|█▏        | 35/300 [00:18<02:11,  2.02it/s]Epoch 8:  12%|█▏        | 36/300 [00:18<02:23,  1.83it/s]Epoch 8:  12%|█▏        | 37/300 [00:19<02:18,  1.90it/s]Epoch 8:  13%|█▎        | 38/300 [00:19<02:20,  1.86it/s]Epoch 8:  13%|█▎        | 39/300 [00:20<02:15,  1.93it/s]06/19/2022 20:08:48 - INFO - __main__ - global step: 1220; train loss: 8.158773422241211; dev loss: 8.196398735046387
Epoch 8:  13%|█▎        | 40/300 [00:20<02:18,  1.87it/s]Epoch 8:  14%|█▎        | 41/300 [00:21<02:13,  1.93it/s]Epoch 8:  14%|█▍        | 42/300 [00:21<02:11,  1.96it/s]Epoch 8:  14%|█▍        | 43/300 [00:22<02:09,  1.99it/s]Epoch 8:  15%|█▍        | 44/300 [00:22<02:14,  1.91it/s]Epoch 8:  15%|█▌        | 45/300 [00:23<02:10,  1.96it/s]Epoch 8:  15%|█▌        | 46/300 [00:24<02:13,  1.90it/s]Epoch 8:  16%|█▌        | 47/300 [00:24<02:16,  1.86it/s]Epoch 8:  16%|█▌        | 48/300 [00:25<02:11,  1.92it/s]Epoch 8:  16%|█▋        | 49/300 [00:25<02:20,  1.79it/s]Epoch 8:  17%|█▋        | 50/300 [00:26<02:16,  1.83it/s]Epoch 8:  17%|█▋        | 51/300 [00:26<02:14,  1.86it/s]Epoch 8:  17%|█▋        | 52/300 [00:27<02:15,  1.83it/s]Epoch 8:  18%|█▊        | 53/300 [00:27<02:16,  1.81it/s]Epoch 8:  18%|█▊        | 54/300 [00:28<02:13,  1.84it/s]Epoch 8:  18%|█▊        | 55/300 [00:28<02:08,  1.91it/s]Epoch 8:  19%|█▊        | 56/300 [00:29<02:06,  1.93it/s]Epoch 8:  19%|█▉        | 57/300 [00:30<02:15,  1.80it/s]Epoch 8:  19%|█▉        | 58/300 [00:30<02:09,  1.87it/s]Epoch 8:  20%|█▉        | 59/300 [00:31<02:07,  1.89it/s]06/19/2022 20:08:58 - INFO - __main__ - global step: 1230; train loss: 8.105825424194336; dev loss: 8.175817489624023
Epoch 8:  20%|██        | 60/300 [00:31<02:03,  1.94it/s]Epoch 8:  20%|██        | 61/300 [00:32<02:07,  1.87it/s]Epoch 8:  21%|██        | 62/300 [00:32<02:03,  1.93it/s]Epoch 8:  21%|██        | 63/300 [00:33<02:00,  1.97it/s]Epoch 8:  21%|██▏       | 64/300 [00:33<01:57,  2.00it/s]Epoch 8:  22%|██▏       | 65/300 [00:34<02:02,  1.92it/s]Epoch 8:  22%|██▏       | 66/300 [00:34<02:02,  1.91it/s]Epoch 8:  22%|██▏       | 67/300 [00:35<02:04,  1.87it/s]Epoch 8:  23%|██▎       | 68/300 [00:35<02:02,  1.89it/s]Epoch 8:  23%|██▎       | 69/300 [00:36<02:04,  1.85it/s]Epoch 8:  23%|██▎       | 70/300 [00:36<02:00,  1.92it/s]Epoch 8:  24%|██▎       | 71/300 [00:37<01:58,  1.93it/s]Epoch 8:  24%|██▍       | 72/300 [00:37<01:55,  1.98it/s]Epoch 8:  24%|██▍       | 73/300 [00:38<01:53,  2.01it/s]Epoch 8:  25%|██▍       | 74/300 [00:38<01:58,  1.91it/s]Epoch 8:  25%|██▌       | 75/300 [00:39<01:54,  1.96it/s]Epoch 8:  25%|██▌       | 76/300 [00:39<01:52,  1.99it/s]Epoch 8:  26%|██▌       | 77/300 [00:40<01:56,  1.91it/s]Epoch 8:  26%|██▌       | 78/300 [00:40<01:58,  1.87it/s]Epoch 8:  26%|██▋       | 79/300 [00:41<01:54,  1.93it/s]06/19/2022 20:09:08 - INFO - __main__ - global step: 1240; train loss: 7.906430721282959; dev loss: 7.510059356689453
Epoch 8:  27%|██▋       | 80/300 [00:41<01:51,  1.97it/s]Epoch 8:  27%|██▋       | 81/300 [00:42<01:49,  2.00it/s]Epoch 8:  27%|██▋       | 82/300 [00:42<01:53,  1.93it/s]Epoch 8:  28%|██▊       | 83/300 [00:43<01:53,  1.92it/s]Epoch 8:  28%|██▊       | 84/300 [00:43<01:49,  1.97it/s]Epoch 8:  28%|██▊       | 85/300 [00:44<01:46,  2.01it/s]Epoch 8:  29%|██▊       | 86/300 [00:44<01:53,  1.89it/s]Epoch 8:  29%|██▉       | 87/300 [00:45<01:50,  1.92it/s]Epoch 8:  29%|██▉       | 88/300 [00:46<01:52,  1.89it/s]Epoch 8:  30%|██▉       | 89/300 [00:46<01:48,  1.95it/s]Epoch 8:  30%|███       | 90/300 [00:47<01:56,  1.80it/s]Epoch 8:  30%|███       | 91/300 [00:47<01:51,  1.88it/s]Epoch 8:  31%|███       | 92/300 [00:48<01:52,  1.85it/s]Epoch 8:  31%|███       | 93/300 [00:48<01:47,  1.92it/s]Epoch 8:  31%|███▏      | 94/300 [00:49<01:50,  1.86it/s]Epoch 8:  32%|███▏      | 95/300 [00:49<01:46,  1.93it/s]Epoch 8:  32%|███▏      | 96/300 [00:50<01:44,  1.95it/s]Epoch 8:  32%|███▏      | 97/300 [00:50<01:46,  1.90it/s]Epoch 8:  33%|███▎      | 98/300 [00:51<01:50,  1.83it/s]Epoch 8:  33%|███▎      | 99/300 [00:51<01:45,  1.90it/s]06/19/2022 20:09:19 - INFO - __main__ - global step: 1250; train loss: 7.782599449157715; dev loss: 8.092660903930664
Epoch 8:  33%|███▎      | 100/300 [00:52<01:43,  1.94it/s]Epoch 8:  34%|███▎      | 101/300 [00:52<01:40,  1.98it/s]Epoch 8:  34%|███▍      | 102/300 [00:53<01:38,  2.02it/s]Epoch 8:  34%|███▍      | 103/300 [00:53<01:44,  1.89it/s]Epoch 8:  35%|███▍      | 104/300 [00:54<01:40,  1.94it/s]Epoch 8:  35%|███▌      | 105/300 [00:54<01:39,  1.96it/s]Epoch 8:  35%|███▌      | 106/300 [00:55<01:41,  1.91it/s]Epoch 8:  36%|███▌      | 107/300 [00:56<01:48,  1.78it/s]Epoch 8:  36%|███▌      | 108/300 [00:56<01:47,  1.79it/s]Epoch 8:  36%|███▋      | 109/300 [00:57<01:42,  1.87it/s]Epoch 8:  37%|███▋      | 110/300 [00:57<01:38,  1.93it/s]Epoch 8:  37%|███▋      | 111/300 [00:58<01:40,  1.88it/s]Epoch 8:  37%|███▋      | 112/300 [00:58<01:36,  1.94it/s]Epoch 8:  38%|███▊      | 113/300 [00:59<01:34,  1.99it/s]Epoch 8:  38%|███▊      | 114/300 [00:59<01:34,  1.97it/s]Epoch 8:  38%|███▊      | 115/300 [01:00<01:37,  1.90it/s]Epoch 8:  39%|███▊      | 116/300 [01:00<01:34,  1.96it/s]Epoch 8:  39%|███▉      | 117/300 [01:01<01:31,  2.00it/s]Epoch 8:  39%|███▉      | 118/300 [01:01<01:30,  2.02it/s]Epoch 8:  40%|███▉      | 119/300 [01:02<01:34,  1.92it/s]06/19/2022 20:09:29 - INFO - __main__ - global step: 1260; train loss: 7.743466377258301; dev loss: 7.876378059387207
Epoch 8:  40%|████      | 120/300 [01:02<01:31,  1.97it/s]Epoch 8:  40%|████      | 121/300 [01:03<01:28,  2.01it/s]Epoch 8:  41%|████      | 122/300 [01:03<01:27,  2.04it/s]Epoch 8:  41%|████      | 123/300 [01:04<01:35,  1.85it/s]Epoch 8:  41%|████▏     | 124/300 [01:04<01:31,  1.92it/s]Epoch 8:  42%|████▏     | 125/300 [01:05<01:29,  1.96it/s]Epoch 8:  42%|████▏     | 126/300 [01:05<01:26,  2.00it/s]Epoch 8:  42%|████▏     | 127/300 [01:06<01:25,  2.02it/s]Epoch 8:  43%|████▎     | 128/300 [01:06<01:29,  1.93it/s]Epoch 8:  43%|████▎     | 129/300 [01:07<01:27,  1.95it/s]Epoch 8:  43%|████▎     | 130/300 [01:07<01:25,  1.99it/s]Epoch 8:  44%|████▎     | 131/300 [01:08<01:24,  2.01it/s]Epoch 8:  44%|████▍     | 132/300 [01:08<01:27,  1.92it/s]Epoch 8:  44%|████▍     | 133/300 [01:09<01:27,  1.92it/s]Epoch 8:  45%|████▍     | 134/300 [01:09<01:26,  1.92it/s]Epoch 8:  45%|████▌     | 135/300 [01:10<01:28,  1.86it/s]Epoch 8:  45%|████▌     | 136/300 [01:11<01:29,  1.82it/s]Epoch 8:  46%|████▌     | 137/300 [01:11<01:30,  1.80it/s]Epoch 8:  46%|████▌     | 138/300 [01:12<01:30,  1.80it/s]Epoch 8:  46%|████▋     | 139/300 [01:12<01:30,  1.78it/s]06/19/2022 20:09:40 - INFO - __main__ - global step: 1270; train loss: 8.446819305419922; dev loss: 8.023660659790039
Epoch 8:  47%|████▋     | 140/300 [01:13<01:34,  1.70it/s]Epoch 8:  47%|████▋     | 141/300 [01:13<01:28,  1.80it/s]Epoch 8:  47%|████▋     | 142/300 [01:14<01:27,  1.80it/s]Epoch 8:  48%|████▊     | 143/300 [01:14<01:25,  1.83it/s]Epoch 8:  48%|████▊     | 144/300 [01:15<01:30,  1.73it/s]Epoch 8:  48%|████▊     | 145/300 [01:16<01:24,  1.82it/s]Epoch 8:  49%|████▊     | 146/300 [01:16<01:21,  1.90it/s]Epoch 8:  49%|████▉     | 147/300 [01:17<01:18,  1.96it/s]Epoch 8:  49%|████▉     | 148/300 [01:17<01:20,  1.89it/s]Epoch 8:  50%|████▉     | 149/300 [01:18<01:17,  1.95it/s]Epoch 8:  50%|█████     | 150/300 [01:18<01:15,  1.98it/s]Epoch 8:  50%|█████     | 151/300 [01:19<01:14,  2.00it/s]Epoch 8:  51%|█████     | 152/300 [01:19<01:17,  1.91it/s]Epoch 8:  51%|█████     | 153/300 [01:20<01:14,  1.96it/s]Epoch 8:  51%|█████▏    | 154/300 [01:20<01:13,  2.00it/s]Epoch 8:  52%|█████▏    | 155/300 [01:21<01:11,  2.02it/s]Epoch 8:  52%|█████▏    | 156/300 [01:21<01:10,  2.03it/s]Epoch 8:  52%|█████▏    | 157/300 [01:22<01:14,  1.92it/s]Epoch 8:  53%|█████▎    | 158/300 [01:22<01:12,  1.97it/s]Epoch 8:  53%|█████▎    | 159/300 [01:23<01:10,  2.00it/s]06/19/2022 20:09:50 - INFO - __main__ - global step: 1280; train loss: 7.715628147125244; dev loss: 7.960066318511963
Epoch 8:  53%|█████▎    | 160/300 [01:23<01:09,  2.02it/s]Epoch 8:  54%|█████▎    | 161/300 [01:24<01:12,  1.93it/s]Epoch 8:  54%|█████▍    | 162/300 [01:24<01:09,  1.98it/s]Epoch 8:  54%|█████▍    | 163/300 [01:25<01:08,  2.01it/s]Epoch 8:  55%|█████▍    | 164/300 [01:25<01:06,  2.03it/s]Epoch 8:  55%|█████▌    | 165/300 [01:26<01:09,  1.94it/s]Epoch 8:  55%|█████▌    | 166/300 [01:26<01:07,  1.98it/s]Epoch 8:  56%|█████▌    | 167/300 [01:27<01:05,  2.02it/s]Epoch 8:  56%|█████▌    | 168/300 [01:27<01:08,  1.94it/s]Epoch 8:  56%|█████▋    | 169/300 [01:28<01:10,  1.87it/s]Epoch 8:  57%|█████▋    | 170/300 [01:28<01:07,  1.93it/s]Epoch 8:  57%|█████▋    | 171/300 [01:29<01:06,  1.93it/s]Epoch 8:  57%|█████▋    | 172/300 [01:29<01:04,  1.97it/s]Epoch 8:  58%|█████▊    | 173/300 [01:30<01:07,  1.88it/s]Epoch 8:  58%|█████▊    | 174/300 [01:30<01:07,  1.85it/s]Epoch 8:  58%|█████▊    | 175/300 [01:31<01:06,  1.89it/s]Epoch 8:  59%|█████▊    | 176/300 [01:31<01:06,  1.86it/s]Epoch 8:  59%|█████▉    | 177/300 [01:32<01:07,  1.82it/s]Epoch 8:  59%|█████▉    | 178/300 [01:32<01:04,  1.89it/s]Epoch 8:  60%|█████▉    | 179/300 [01:33<01:02,  1.95it/s]06/19/2022 20:10:01 - INFO - __main__ - global step: 1290; train loss: 7.381156921386719; dev loss: 7.86669921875
Epoch 8:  60%|██████    | 180/300 [01:34<01:03,  1.89it/s]Epoch 8:  60%|██████    | 181/300 [01:34<01:01,  1.94it/s]Epoch 8:  61%|██████    | 182/300 [01:35<01:05,  1.80it/s]Epoch 8:  61%|██████    | 183/300 [01:35<01:03,  1.84it/s]Epoch 8:  61%|██████▏   | 184/300 [01:36<01:00,  1.90it/s]Epoch 8:  62%|██████▏   | 185/300 [01:36<00:58,  1.96it/s]Epoch 8:  62%|██████▏   | 186/300 [01:37<01:00,  1.88it/s]Epoch 8:  62%|██████▏   | 187/300 [01:37<00:58,  1.94it/s]Epoch 8:  63%|██████▎   | 188/300 [01:38<00:56,  1.99it/s]Epoch 8:  63%|██████▎   | 189/300 [01:38<00:54,  2.02it/s]Epoch 8:  63%|██████▎   | 190/300 [01:39<01:01,  1.80it/s]Epoch 8:  64%|██████▎   | 191/300 [01:39<00:59,  1.84it/s]Epoch 8:  64%|██████▍   | 192/300 [01:40<00:56,  1.91it/s]Epoch 8:  64%|██████▍   | 193/300 [01:40<00:55,  1.91it/s]Epoch 8:  65%|██████▍   | 194/300 [01:41<00:57,  1.86it/s]Epoch 8:  65%|██████▌   | 195/300 [01:41<00:57,  1.83it/s]Epoch 8:  65%|██████▌   | 196/300 [01:42<00:54,  1.90it/s]Epoch 8:  66%|██████▌   | 197/300 [01:42<00:52,  1.95it/s]Epoch 8:  66%|██████▌   | 198/300 [01:43<00:55,  1.85it/s]Epoch 8:  66%|██████▋   | 199/300 [01:44<00:52,  1.92it/s]06/19/2022 20:10:11 - INFO - __main__ - global step: 1300; train loss: 8.652575492858887; dev loss: 8.501863479614258
Epoch 8:  67%|██████▋   | 200/300 [01:44<00:51,  1.94it/s]Epoch 8:  67%|██████▋   | 201/300 [01:45<00:49,  1.98it/s]Epoch 8:  67%|██████▋   | 202/300 [01:45<00:53,  1.82it/s]Epoch 8:  68%|██████▊   | 203/300 [01:46<00:51,  1.89it/s]Epoch 8:  68%|██████▊   | 204/300 [01:46<00:49,  1.94it/s]Epoch 8:  68%|██████▊   | 205/300 [01:47<00:49,  1.94it/s]Epoch 8:  69%|██████▊   | 206/300 [01:47<00:52,  1.80it/s]Epoch 8:  69%|██████▉   | 207/300 [01:48<00:49,  1.87it/s]Epoch 8:  69%|██████▉   | 208/300 [01:48<00:47,  1.93it/s]Epoch 8:  70%|██████▉   | 209/300 [01:49<00:46,  1.97it/s]Epoch 8:  70%|███████   | 210/300 [01:49<00:47,  1.90it/s]Epoch 8:  70%|███████   | 211/300 [01:50<00:49,  1.80it/s]Epoch 8:  71%|███████   | 212/300 [01:50<00:46,  1.88it/s]Epoch 8:  71%|███████   | 213/300 [01:51<00:46,  1.85it/s]Epoch 8:  71%|███████▏  | 214/300 [01:52<00:46,  1.83it/s]Epoch 8:  72%|███████▏  | 215/300 [01:52<00:46,  1.82it/s]Epoch 8:  72%|███████▏  | 216/300 [01:53<00:44,  1.88it/s]Epoch 8:  72%|███████▏  | 217/300 [01:53<00:43,  1.91it/s]Epoch 8:  73%|███████▎  | 218/300 [01:54<00:41,  1.96it/s]Epoch 8:  73%|███████▎  | 219/300 [01:54<00:42,  1.90it/s]06/19/2022 20:10:22 - INFO - __main__ - global step: 1310; train loss: 7.931500434875488; dev loss: 7.7242889404296875
Epoch 8:  73%|███████▎  | 220/300 [01:55<00:41,  1.95it/s]Epoch 8:  74%|███████▎  | 221/300 [01:55<00:41,  1.91it/s]Epoch 8:  74%|███████▍  | 222/300 [01:56<00:40,  1.95it/s]Epoch 8:  74%|███████▍  | 223/300 [01:56<00:42,  1.80it/s]Epoch 8:  75%|███████▍  | 224/300 [01:57<00:41,  1.84it/s]Epoch 8:  75%|███████▌  | 225/300 [01:57<00:39,  1.91it/s]Epoch 8:  75%|███████▌  | 226/300 [01:58<00:38,  1.94it/s]Epoch 8:  76%|███████▌  | 227/300 [01:58<00:38,  1.87it/s]Epoch 8:  76%|███████▌  | 228/300 [01:59<00:37,  1.94it/s]Epoch 8:  76%|███████▋  | 229/300 [01:59<00:35,  1.99it/s]Epoch 8:  77%|███████▋  | 230/300 [02:00<00:34,  2.02it/s]Epoch 8:  77%|███████▋  | 231/300 [02:00<00:35,  1.93it/s]Epoch 8:  77%|███████▋  | 232/300 [02:01<00:34,  1.97it/s]Epoch 8:  78%|███████▊  | 233/300 [02:01<00:33,  1.98it/s]Epoch 8:  78%|███████▊  | 234/300 [02:02<00:33,  1.96it/s]Epoch 8:  78%|███████▊  | 235/300 [02:02<00:32,  2.00it/s]Epoch 8:  79%|███████▊  | 236/300 [02:03<00:35,  1.82it/s]Epoch 8:  79%|███████▉  | 237/300 [02:03<00:33,  1.90it/s]Epoch 8:  79%|███████▉  | 238/300 [02:04<00:31,  1.95it/s]Epoch 8:  80%|███████▉  | 239/300 [02:04<00:30,  2.00it/s]06/19/2022 20:10:32 - INFO - __main__ - global step: 1320; train loss: 7.828536033630371; dev loss: 7.8549370765686035
Epoch 8:  80%|████████  | 240/300 [02:05<00:31,  1.92it/s]Epoch 8:  80%|████████  | 241/300 [02:05<00:30,  1.97it/s]Epoch 8:  81%|████████  | 242/300 [02:06<00:29,  2.00it/s]Epoch 8:  81%|████████  | 243/300 [02:07<00:29,  1.92it/s]Epoch 8:  81%|████████▏ | 244/300 [02:07<00:29,  1.87it/s]Epoch 8:  82%|████████▏ | 245/300 [02:08<00:28,  1.94it/s]Epoch 8:  82%|████████▏ | 246/300 [02:08<00:27,  1.97it/s]Epoch 8:  82%|████████▏ | 247/300 [02:09<00:26,  1.99it/s]Epoch 8:  83%|████████▎ | 248/300 [02:09<00:27,  1.90it/s]Epoch 8:  83%|████████▎ | 249/300 [02:10<00:27,  1.87it/s]Epoch 8:  83%|████████▎ | 250/300 [02:10<00:27,  1.85it/s]Epoch 8:  84%|████████▎ | 251/300 [02:11<00:26,  1.82it/s]Epoch 8:  84%|████████▍ | 252/300 [02:11<00:27,  1.76it/s]Epoch 8:  84%|████████▍ | 253/300 [02:12<00:25,  1.84it/s]Epoch 8:  85%|████████▍ | 254/300 [02:12<00:24,  1.87it/s]Epoch 8:  85%|████████▌ | 255/300 [02:13<00:23,  1.93it/s]Epoch 8:  85%|████████▌ | 256/300 [02:14<00:24,  1.83it/s]Epoch 8:  86%|████████▌ | 257/300 [02:14<00:22,  1.90it/s]Epoch 8:  86%|████████▌ | 258/300 [02:14<00:21,  1.95it/s]Epoch 8:  86%|████████▋ | 259/300 [02:15<00:21,  1.91it/s]06/19/2022 20:10:43 - INFO - __main__ - global step: 1330; train loss: 7.483895301818848; dev loss: 7.6587018966674805
Epoch 8:  87%|████████▋ | 260/300 [02:16<00:21,  1.85it/s]Epoch 8:  87%|████████▋ | 261/300 [02:16<00:21,  1.83it/s]Epoch 8:  87%|████████▋ | 262/300 [02:17<00:20,  1.90it/s]Epoch 8:  88%|████████▊ | 263/300 [02:17<00:19,  1.90it/s]Epoch 8:  88%|████████▊ | 264/300 [02:18<00:18,  1.95it/s]Epoch 8:  88%|████████▊ | 265/300 [02:18<00:18,  1.88it/s]Epoch 8:  89%|████████▊ | 266/300 [02:19<00:17,  1.93it/s]Epoch 8:  89%|████████▉ | 267/300 [02:19<00:16,  1.97it/s]Epoch 8:  89%|████████▉ | 268/300 [02:20<00:16,  1.95it/s]Epoch 8:  90%|████████▉ | 269/300 [02:20<00:16,  1.90it/s]Epoch 8:  90%|█████████ | 270/300 [02:21<00:15,  1.94it/s]Epoch 8:  90%|█████████ | 271/300 [02:21<00:14,  1.98it/s]Epoch 8:  91%|█████████ | 272/300 [02:22<00:13,  2.01it/s]Epoch 8:  91%|█████████ | 273/300 [02:22<00:14,  1.91it/s]Epoch 8:  91%|█████████▏| 274/300 [02:23<00:13,  1.96it/s]Epoch 8:  92%|█████████▏| 275/300 [02:23<00:12,  1.95it/s]Epoch 8:  92%|█████████▏| 276/300 [02:24<00:12,  1.94it/s]Epoch 8:  92%|█████████▏| 277/300 [02:25<00:13,  1.77it/s]Epoch 8:  93%|█████████▎| 278/300 [02:25<00:11,  1.85it/s]Epoch 8:  93%|█████████▎| 279/300 [02:25<00:10,  1.92it/s]06/19/2022 20:10:53 - INFO - __main__ - global step: 1340; train loss: 7.910698890686035; dev loss: 7.922227382659912
Epoch 8:  93%|█████████▎| 280/300 [02:26<00:10,  1.88it/s]Epoch 8:  94%|█████████▎| 281/300 [02:27<00:10,  1.84it/s]Epoch 8:  94%|█████████▍| 282/300 [02:27<00:09,  1.88it/s]Epoch 8:  94%|█████████▍| 283/300 [02:28<00:08,  1.94it/s]Epoch 8:  95%|█████████▍| 284/300 [02:28<00:08,  1.89it/s]Epoch 8:  95%|█████████▌| 285/300 [02:29<00:08,  1.77it/s]Epoch 8:  95%|█████████▌| 286/300 [02:29<00:07,  1.86it/s]Epoch 8:  96%|█████████▌| 287/300 [02:30<00:06,  1.93it/s]Epoch 8:  96%|█████████▌| 288/300 [02:30<00:06,  1.97it/s]Epoch 8:  96%|█████████▋| 289/300 [02:31<00:05,  2.01it/s]Epoch 8:  97%|█████████▋| 290/300 [02:31<00:05,  1.88it/s]Epoch 8:  97%|█████████▋| 291/300 [02:32<00:04,  1.94it/s]Epoch 8:  97%|█████████▋| 292/300 [02:32<00:04,  1.98it/s]Epoch 8:  98%|█████████▊| 293/300 [02:33<00:03,  2.02it/s]Epoch 8:  98%|█████████▊| 294/300 [02:33<00:03,  1.94it/s]Epoch 8:  98%|█████████▊| 295/300 [02:34<00:02,  1.99it/s]Epoch 8:  99%|█████████▊| 296/300 [02:34<00:01,  2.02it/s]Epoch 8:  99%|█████████▉| 297/300 [02:35<00:01,  2.04it/s]Epoch 8:  99%|█████████▉| 298/300 [02:35<00:01,  1.86it/s]Epoch 8: 100%|█████████▉| 299/300 [02:36<00:00,  1.93it/s]06/19/2022 20:11:04 - INFO - __main__ - global step: 1350; train loss: 8.250638961791992; dev loss: 8.236165046691895
Epoch 8: 100%|██████████| 300/300 [02:36<00:00,  1.89it/s]Epoch 8: 100%|██████████| 300/300 [02:36<00:00,  1.91it/s]
Epoch 9:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 9:   0%|          | 1/300 [00:00<02:48,  1.78it/s]Epoch 9:   1%|          | 2/300 [00:01<02:48,  1.77it/s]Epoch 9:   1%|          | 3/300 [00:01<02:35,  1.91it/s]Epoch 9:   1%|▏         | 4/300 [00:02<02:30,  1.97it/s]Epoch 9:   2%|▏         | 5/300 [00:02<02:26,  2.01it/s]Epoch 9:   2%|▏         | 6/300 [00:03<02:34,  1.91it/s]Epoch 9:   2%|▏         | 7/300 [00:03<02:29,  1.96it/s]Epoch 9:   3%|▎         | 8/300 [00:04<02:26,  1.99it/s]Epoch 9:   3%|▎         | 9/300 [00:04<02:24,  2.02it/s]Epoch 9:   3%|▎         | 10/300 [00:05<02:36,  1.86it/s]Epoch 9:   4%|▎         | 11/300 [00:05<02:30,  1.93it/s]Epoch 9:   4%|▍         | 12/300 [00:06<02:33,  1.88it/s]Epoch 9:   4%|▍         | 13/300 [00:06<02:35,  1.85it/s]Epoch 9:   5%|▍         | 14/300 [00:07<02:37,  1.82it/s]Epoch 9:   5%|▌         | 15/300 [00:07<02:34,  1.85it/s]Epoch 9:   5%|▌         | 16/300 [00:08<02:29,  1.89it/s]Epoch 9:   6%|▌         | 17/300 [00:08<02:24,  1.95it/s]Epoch 9:   6%|▌         | 18/300 [00:09<02:22,  1.98it/s]Epoch 9:   6%|▋         | 19/300 [00:09<02:27,  1.90it/s]06/19/2022 20:11:14 - INFO - __main__ - global step: 1360; train loss: 7.706542015075684; dev loss: 7.841010093688965
Epoch 9:   7%|▋         | 20/300 [00:10<02:26,  1.91it/s]Epoch 9:   7%|▋         | 21/300 [00:10<02:22,  1.96it/s]Epoch 9:   7%|▋         | 22/300 [00:11<02:18,  2.00it/s]Epoch 9:   8%|▊         | 23/300 [00:11<02:23,  1.93it/s]Epoch 9:   8%|▊         | 24/300 [00:12<02:19,  1.98it/s]Epoch 9:   8%|▊         | 25/300 [00:12<02:16,  2.01it/s]Epoch 9:   9%|▊         | 26/300 [00:13<02:14,  2.04it/s]Epoch 9:   9%|▉         | 27/300 [00:13<02:19,  1.95it/s]Epoch 9:   9%|▉         | 28/300 [00:14<02:23,  1.90it/s]Epoch 9:  10%|▉         | 29/300 [00:15<02:25,  1.87it/s]Epoch 9:  10%|█         | 30/300 [00:15<02:26,  1.84it/s]Epoch 9:  10%|█         | 31/300 [00:16<02:28,  1.81it/s]Epoch 9:  11%|█         | 32/300 [00:16<02:22,  1.88it/s]Epoch 9:  11%|█         | 33/300 [00:17<02:25,  1.83it/s]Epoch 9:  11%|█▏        | 34/300 [00:17<02:19,  1.91it/s]Epoch 9:  12%|█▏        | 35/300 [00:18<02:26,  1.81it/s]Epoch 9:  12%|█▏        | 36/300 [00:18<02:20,  1.88it/s]Epoch 9:  12%|█▏        | 37/300 [00:19<02:16,  1.93it/s]Epoch 9:  13%|█▎        | 38/300 [00:19<02:13,  1.97it/s]Epoch 9:  13%|█▎        | 39/300 [00:20<02:17,  1.89it/s]06/19/2022 20:11:24 - INFO - __main__ - global step: 1370; train loss: 7.5587592124938965; dev loss: 7.481772422790527
Epoch 9:  13%|█▎        | 40/300 [00:20<02:13,  1.95it/s]Epoch 9:  14%|█▎        | 41/300 [00:21<02:10,  1.98it/s]Epoch 9:  14%|█▍        | 42/300 [00:21<02:14,  1.91it/s]Epoch 9:  14%|█▍        | 43/300 [00:22<02:18,  1.85it/s]Epoch 9:  15%|█▍        | 44/300 [00:23<02:21,  1.81it/s]Epoch 9:  15%|█▌        | 45/300 [00:23<02:14,  1.89it/s]Epoch 9:  15%|█▌        | 46/300 [00:24<02:16,  1.86it/s]Epoch 9:  16%|█▌        | 47/300 [00:24<02:12,  1.91it/s]Epoch 9:  16%|█▌        | 48/300 [00:25<02:15,  1.87it/s]Epoch 9:  16%|█▋        | 49/300 [00:25<02:13,  1.88it/s]Epoch 9:  17%|█▋        | 50/300 [00:26<02:08,  1.94it/s]Epoch 9:  17%|█▋        | 51/300 [00:26<02:05,  1.98it/s]Epoch 9:  17%|█▋        | 52/300 [00:27<02:09,  1.91it/s]Epoch 9:  18%|█▊        | 53/300 [00:27<02:11,  1.87it/s]Epoch 9:  18%|█▊        | 54/300 [00:28<02:09,  1.90it/s]Epoch 9:  18%|█▊        | 55/300 [00:28<02:05,  1.95it/s]Epoch 9:  19%|█▊        | 56/300 [00:29<02:09,  1.89it/s]Epoch 9:  19%|█▉        | 57/300 [00:29<02:05,  1.94it/s]Epoch 9:  19%|█▉        | 58/300 [00:30<02:04,  1.94it/s]Epoch 9:  20%|█▉        | 59/300 [00:30<02:01,  1.98it/s]06/19/2022 20:11:35 - INFO - __main__ - global step: 1380; train loss: 8.340643882751465; dev loss: 7.929978847503662
Epoch 9:  20%|██        | 60/300 [00:31<02:12,  1.82it/s]Epoch 9:  20%|██        | 61/300 [00:31<02:06,  1.89it/s]Epoch 9:  21%|██        | 62/300 [00:32<02:02,  1.94it/s]Epoch 9:  21%|██        | 63/300 [00:32<01:59,  1.98it/s]Epoch 9:  21%|██▏       | 64/300 [00:33<02:05,  1.88it/s]Epoch 9:  22%|██▏       | 65/300 [00:34<02:02,  1.91it/s]Epoch 9:  22%|██▏       | 66/300 [00:34<01:59,  1.96it/s]Epoch 9:  22%|██▏       | 67/300 [00:34<01:56,  1.99it/s]Epoch 9:  23%|██▎       | 68/300 [00:35<02:01,  1.91it/s]Epoch 9:  23%|██▎       | 69/300 [00:36<01:57,  1.96it/s]Epoch 9:  23%|██▎       | 70/300 [00:36<01:55,  1.99it/s]Epoch 9:  24%|██▎       | 71/300 [00:37<01:53,  2.01it/s]Epoch 9:  24%|██▍       | 72/300 [00:37<01:53,  2.02it/s]Epoch 9:  24%|██▍       | 73/300 [00:38<01:58,  1.91it/s]Epoch 9:  25%|██▍       | 74/300 [00:38<01:55,  1.96it/s]Epoch 9:  25%|██▌       | 75/300 [00:39<01:52,  2.00it/s]Epoch 9:  25%|██▌       | 76/300 [00:39<01:50,  2.02it/s]Epoch 9:  26%|██▌       | 77/300 [00:40<02:00,  1.84it/s]Epoch 9:  26%|██▌       | 78/300 [00:40<01:56,  1.91it/s]Epoch 9:  26%|██▋       | 79/300 [00:41<01:52,  1.96it/s]06/19/2022 20:11:45 - INFO - __main__ - global step: 1390; train loss: 8.027202606201172; dev loss: 7.932464599609375
Epoch 9:  27%|██▋       | 80/300 [00:41<01:51,  1.98it/s]Epoch 9:  27%|██▋       | 81/300 [00:42<01:54,  1.91it/s]Epoch 9:  27%|██▋       | 82/300 [00:42<01:51,  1.95it/s]Epoch 9:  28%|██▊       | 83/300 [00:43<01:49,  1.98it/s]Epoch 9:  28%|██▊       | 84/300 [00:43<01:47,  2.01it/s]Epoch 9:  28%|██▊       | 85/300 [00:44<01:51,  1.92it/s]Epoch 9:  29%|██▊       | 86/300 [00:44<01:48,  1.97it/s]Epoch 9:  29%|██▉       | 87/300 [00:45<01:52,  1.90it/s]Epoch 9:  29%|██▉       | 88/300 [00:45<01:48,  1.96it/s]Epoch 9:  30%|██▉       | 89/300 [00:46<01:51,  1.89it/s]Epoch 9:  30%|███       | 90/300 [00:46<01:48,  1.94it/s]Epoch 9:  30%|███       | 91/300 [00:47<01:50,  1.89it/s]Epoch 9:  31%|███       | 92/300 [00:47<01:52,  1.85it/s]Epoch 9:  31%|███       | 93/300 [00:48<01:53,  1.82it/s]Epoch 9:  31%|███▏      | 94/300 [00:48<01:49,  1.89it/s]Epoch 9:  32%|███▏      | 95/300 [00:49<01:50,  1.86it/s]Epoch 9:  32%|███▏      | 96/300 [00:50<01:46,  1.92it/s]Epoch 9:  32%|███▏      | 97/300 [00:50<01:45,  1.92it/s]Epoch 9:  33%|███▎      | 98/300 [00:51<01:50,  1.83it/s]Epoch 9:  33%|███▎      | 99/300 [00:51<01:50,  1.81it/s]06/19/2022 20:11:56 - INFO - __main__ - global step: 1400; train loss: 7.939278602600098; dev loss: 7.989483833312988
Epoch 9:  33%|███▎      | 100/300 [00:52<01:47,  1.87it/s]Epoch 9:  34%|███▎      | 101/300 [00:52<01:45,  1.88it/s]Epoch 9:  34%|███▍      | 102/300 [00:53<01:52,  1.77it/s]Epoch 9:  34%|███▍      | 103/300 [00:53<01:46,  1.86it/s]Epoch 9:  35%|███▍      | 104/300 [00:54<01:42,  1.91it/s]Epoch 9:  35%|███▌      | 105/300 [00:54<01:41,  1.92it/s]Epoch 9:  35%|███▌      | 106/300 [00:55<01:44,  1.85it/s]Epoch 9:  36%|███▌      | 107/300 [00:55<01:40,  1.91it/s]Epoch 9:  36%|███▌      | 108/300 [00:56<01:38,  1.96it/s]Epoch 9:  36%|███▋      | 109/300 [00:56<01:40,  1.90it/s]Epoch 9:  37%|███▋      | 110/300 [00:57<01:47,  1.77it/s]Epoch 9:  37%|███▋      | 111/300 [00:58<01:41,  1.86it/s]Epoch 9:  37%|███▋      | 112/300 [00:58<01:37,  1.92it/s]Epoch 9:  38%|███▊      | 113/300 [00:59<01:35,  1.96it/s]Epoch 9:  38%|███▊      | 114/300 [00:59<01:42,  1.81it/s]Epoch 9:  38%|███▊      | 115/300 [01:00<01:37,  1.90it/s]Epoch 9:  39%|███▊      | 116/300 [01:00<01:36,  1.90it/s]Epoch 9:  39%|███▉      | 117/300 [01:01<01:35,  1.91it/s]Epoch 9:  39%|███▉      | 118/300 [01:01<01:37,  1.86it/s]Epoch 9:  40%|███▉      | 119/300 [01:02<01:34,  1.92it/s]06/19/2022 20:12:06 - INFO - __main__ - global step: 1410; train loss: 7.8041205406188965; dev loss: 7.449827671051025
Epoch 9:  40%|████      | 120/300 [01:02<01:33,  1.92it/s]Epoch 9:  40%|████      | 121/300 [01:03<01:31,  1.96it/s]Epoch 9:  41%|████      | 122/300 [01:03<01:39,  1.79it/s]Epoch 9:  41%|████      | 123/300 [01:04<01:39,  1.78it/s]Epoch 9:  41%|████▏     | 124/300 [01:05<01:34,  1.87it/s]Epoch 9:  42%|████▏     | 125/300 [01:05<01:30,  1.93it/s]Epoch 9:  42%|████▏     | 126/300 [01:05<01:28,  1.97it/s]Epoch 9:  42%|████▏     | 127/300 [01:06<01:31,  1.89it/s]Epoch 9:  43%|████▎     | 128/300 [01:07<01:32,  1.85it/s]Epoch 9:  43%|████▎     | 129/300 [01:07<01:29,  1.92it/s]Epoch 9:  43%|████▎     | 130/300 [01:08<01:31,  1.87it/s]Epoch 9:  44%|████▎     | 131/300 [01:08<01:32,  1.83it/s]Epoch 9:  44%|████▍     | 132/300 [01:09<01:30,  1.85it/s]Epoch 9:  44%|████▍     | 133/300 [01:09<01:27,  1.90it/s]Epoch 9:  45%|████▍     | 134/300 [01:10<01:25,  1.95it/s]Epoch 9:  45%|████▌     | 135/300 [01:10<01:27,  1.89it/s]Epoch 9:  45%|████▌     | 136/300 [01:11<01:26,  1.90it/s]Epoch 9:  46%|████▌     | 137/300 [01:11<01:23,  1.96it/s]Epoch 9:  46%|████▌     | 138/300 [01:12<01:21,  2.00it/s]Epoch 9:  46%|████▋     | 139/300 [01:12<01:23,  1.92it/s]06/19/2022 20:12:17 - INFO - __main__ - global step: 1420; train loss: 7.592402458190918; dev loss: 8.212324142456055
Epoch 9:  47%|████▋     | 140/300 [01:13<01:21,  1.96it/s]Epoch 9:  47%|████▋     | 141/300 [01:13<01:19,  1.99it/s]Epoch 9:  47%|████▋     | 142/300 [01:14<01:22,  1.92it/s]Epoch 9:  48%|████▊     | 143/300 [01:14<01:24,  1.86it/s]Epoch 9:  48%|████▊     | 144/300 [01:15<01:24,  1.84it/s]Epoch 9:  48%|████▊     | 145/300 [01:15<01:21,  1.91it/s]Epoch 9:  49%|████▊     | 146/300 [01:16<01:19,  1.95it/s]Epoch 9:  49%|████▉     | 147/300 [01:17<01:23,  1.83it/s]Epoch 9:  49%|████▉     | 148/300 [01:17<01:20,  1.89it/s]Epoch 9:  50%|████▉     | 149/300 [01:18<01:18,  1.93it/s]Epoch 9:  50%|█████     | 150/300 [01:18<01:16,  1.96it/s]Epoch 9:  50%|█████     | 151/300 [01:19<01:15,  1.98it/s]Epoch 9:  51%|█████     | 152/300 [01:19<01:19,  1.86it/s]Epoch 9:  51%|█████     | 153/300 [01:20<01:16,  1.93it/s]Epoch 9:  51%|█████▏    | 154/300 [01:20<01:14,  1.97it/s]Epoch 9:  52%|█████▏    | 155/300 [01:21<01:13,  1.96it/s]Epoch 9:  52%|█████▏    | 156/300 [01:21<01:15,  1.90it/s]Epoch 9:  52%|█████▏    | 157/300 [01:22<01:13,  1.96it/s]Epoch 9:  53%|█████▎    | 158/300 [01:22<01:10,  2.01it/s]Epoch 9:  53%|█████▎    | 159/300 [01:23<01:13,  1.93it/s]06/19/2022 20:12:27 - INFO - __main__ - global step: 1430; train loss: 7.754578590393066; dev loss: 7.935662746429443
Epoch 9:  53%|█████▎    | 160/300 [01:23<01:18,  1.79it/s]Epoch 9:  54%|█████▎    | 161/300 [01:24<01:14,  1.87it/s]Epoch 9:  54%|█████▍    | 162/300 [01:24<01:12,  1.90it/s]Epoch 9:  54%|█████▍    | 163/300 [01:25<01:12,  1.89it/s]Epoch 9:  55%|█████▍    | 164/300 [01:26<01:15,  1.80it/s]Epoch 9:  55%|█████▌    | 165/300 [01:26<01:12,  1.86it/s]Epoch 9:  55%|█████▌    | 166/300 [01:26<01:09,  1.92it/s]Epoch 9:  56%|█████▌    | 167/300 [01:27<01:08,  1.94it/s]Epoch 9:  56%|█████▌    | 168/300 [01:28<01:10,  1.88it/s]Epoch 9:  56%|█████▋    | 169/300 [01:28<01:09,  1.89it/s]Epoch 9:  57%|█████▋    | 170/300 [01:29<01:09,  1.86it/s]Epoch 9:  57%|█████▋    | 171/300 [01:29<01:10,  1.84it/s]Epoch 9:  57%|█████▋    | 172/300 [01:30<01:10,  1.82it/s]Epoch 9:  58%|█████▊    | 173/300 [01:30<01:07,  1.89it/s]Epoch 9:  58%|█████▊    | 174/300 [01:31<01:04,  1.95it/s]Epoch 9:  58%|█████▊    | 175/300 [01:31<01:02,  1.99it/s]Epoch 9:  59%|█████▊    | 176/300 [01:32<01:05,  1.90it/s]Epoch 9:  59%|█████▉    | 177/300 [01:32<01:06,  1.86it/s]Epoch 9:  59%|█████▉    | 178/300 [01:33<01:05,  1.87it/s]Epoch 9:  60%|█████▉    | 179/300 [01:33<01:05,  1.84it/s]06/19/2022 20:12:38 - INFO - __main__ - global step: 1440; train loss: 8.505563735961914; dev loss: 8.616181373596191
Epoch 9:  60%|██████    | 180/300 [01:34<01:02,  1.91it/s]Epoch 9:  60%|██████    | 181/300 [01:34<01:04,  1.85it/s]Epoch 9:  61%|██████    | 182/300 [01:35<01:03,  1.87it/s]Epoch 9:  61%|██████    | 183/300 [01:35<01:00,  1.93it/s]Epoch 9:  61%|██████▏   | 184/300 [01:36<00:58,  1.97it/s]Epoch 9:  62%|██████▏   | 185/300 [01:37<01:00,  1.90it/s]Epoch 9:  62%|██████▏   | 186/300 [01:37<00:58,  1.96it/s]Epoch 9:  62%|██████▏   | 187/300 [01:37<00:56,  2.00it/s]Epoch 9:  63%|██████▎   | 188/300 [01:38<00:56,  1.97it/s]Epoch 9:  63%|██████▎   | 189/300 [01:39<00:59,  1.85it/s]Epoch 9:  63%|██████▎   | 190/300 [01:39<00:58,  1.89it/s]Epoch 9:  64%|██████▎   | 191/300 [01:40<00:56,  1.94it/s]Epoch 9:  64%|██████▍   | 192/300 [01:40<00:54,  1.98it/s]Epoch 9:  64%|██████▍   | 193/300 [01:41<00:57,  1.86it/s]Epoch 9:  65%|██████▍   | 194/300 [01:41<00:55,  1.91it/s]Epoch 9:  65%|██████▌   | 195/300 [01:42<00:54,  1.94it/s]Epoch 9:  65%|██████▌   | 196/300 [01:42<00:52,  1.97it/s]Epoch 9:  66%|██████▌   | 197/300 [01:43<00:55,  1.87it/s]Epoch 9:  66%|██████▌   | 198/300 [01:43<00:55,  1.84it/s]Epoch 9:  66%|██████▋   | 199/300 [01:44<00:53,  1.89it/s]06/19/2022 20:12:48 - INFO - __main__ - global step: 1450; train loss: 7.9070234298706055; dev loss: 7.364298343658447
Epoch 9:  67%|██████▋   | 200/300 [01:44<00:51,  1.94it/s]Epoch 9:  67%|██████▋   | 201/300 [01:45<00:55,  1.79it/s]Epoch 9:  67%|██████▋   | 202/300 [01:45<00:52,  1.87it/s]Epoch 9:  68%|██████▊   | 203/300 [01:46<00:50,  1.92it/s]Epoch 9:  68%|██████▊   | 204/300 [01:46<00:48,  1.97it/s]Epoch 9:  68%|██████▊   | 205/300 [01:47<00:47,  2.00it/s]Epoch 9:  69%|██████▊   | 206/300 [01:48<00:49,  1.89it/s]Epoch 9:  69%|██████▉   | 207/300 [01:48<00:48,  1.93it/s]Epoch 9:  69%|██████▉   | 208/300 [01:48<00:46,  1.98it/s]Epoch 9:  70%|██████▉   | 209/300 [01:49<00:45,  2.01it/s]Epoch 9:  70%|███████   | 210/300 [01:50<00:46,  1.93it/s]Epoch 9:  70%|███████   | 211/300 [01:50<00:45,  1.96it/s]Epoch 9:  71%|███████   | 212/300 [01:51<00:46,  1.90it/s]Epoch 9:  71%|███████   | 213/300 [01:51<00:44,  1.95it/s]Epoch 9:  71%|███████▏  | 214/300 [01:52<00:45,  1.88it/s]Epoch 9:  72%|███████▏  | 215/300 [01:52<00:44,  1.93it/s]Epoch 9:  72%|███████▏  | 216/300 [01:53<00:42,  1.97it/s]Epoch 9:  72%|███████▏  | 217/300 [01:53<00:41,  2.00it/s]Epoch 9:  73%|███████▎  | 218/300 [01:54<00:43,  1.90it/s]Epoch 9:  73%|███████▎  | 219/300 [01:54<00:42,  1.89it/s]06/19/2022 20:12:59 - INFO - __main__ - global step: 1460; train loss: 7.993966579437256; dev loss: 8.009231567382812
Epoch 9:  73%|███████▎  | 220/300 [01:55<00:42,  1.90it/s]Epoch 9:  74%|███████▎  | 221/300 [01:55<00:40,  1.96it/s]Epoch 9:  74%|███████▍  | 222/300 [01:56<00:42,  1.85it/s]Epoch 9:  74%|███████▍  | 223/300 [01:56<00:40,  1.92it/s]Epoch 9:  75%|███████▍  | 224/300 [01:57<00:38,  1.96it/s]Epoch 9:  75%|███████▌  | 225/300 [01:57<00:37,  2.00it/s]Epoch 9:  75%|███████▌  | 226/300 [01:58<00:38,  1.91it/s]Epoch 9:  76%|███████▌  | 227/300 [01:58<00:37,  1.96it/s]Epoch 9:  76%|███████▌  | 228/300 [01:59<00:37,  1.91it/s]Epoch 9:  76%|███████▋  | 229/300 [01:59<00:37,  1.88it/s]Epoch 9:  77%|███████▋  | 230/300 [02:00<00:38,  1.82it/s]Epoch 9:  77%|███████▋  | 231/300 [02:00<00:36,  1.89it/s]Epoch 9:  77%|███████▋  | 232/300 [02:01<00:34,  1.95it/s]Epoch 9:  78%|███████▊  | 233/300 [02:01<00:34,  1.94it/s]Epoch 9:  78%|███████▊  | 234/300 [02:02<00:34,  1.93it/s]Epoch 9:  78%|███████▊  | 235/300 [02:03<00:34,  1.86it/s]Epoch 9:  79%|███████▊  | 236/300 [02:03<00:33,  1.92it/s]Epoch 9:  79%|███████▉  | 237/300 [02:04<00:32,  1.97it/s]Epoch 9:  79%|███████▉  | 238/300 [02:04<00:31,  1.94it/s]Epoch 9:  80%|███████▉  | 239/300 [02:05<00:32,  1.85it/s]06/19/2022 20:13:09 - INFO - __main__ - global step: 1470; train loss: 8.102148056030273; dev loss: 8.100191116333008
Epoch 9:  80%|████████  | 240/300 [02:05<00:32,  1.87it/s]Epoch 9:  80%|████████  | 241/300 [02:06<00:32,  1.84it/s]Epoch 9:  81%|████████  | 242/300 [02:06<00:31,  1.87it/s]Epoch 9:  81%|████████  | 243/300 [02:07<00:30,  1.84it/s]Epoch 9:  81%|████████▏ | 244/300 [02:07<00:29,  1.91it/s]Epoch 9:  82%|████████▏ | 245/300 [02:08<00:28,  1.96it/s]Epoch 9:  82%|████████▏ | 246/300 [02:08<00:28,  1.88it/s]Epoch 9:  82%|████████▏ | 247/300 [02:09<00:28,  1.83it/s]Epoch 9:  83%|████████▎ | 248/300 [02:10<00:28,  1.81it/s]Epoch 9:  83%|████████▎ | 249/300 [02:10<00:26,  1.89it/s]Epoch 9:  83%|████████▎ | 250/300 [02:10<00:25,  1.95it/s]Epoch 9:  84%|████████▎ | 251/300 [02:11<00:25,  1.89it/s]Epoch 9:  84%|████████▍ | 252/300 [02:12<00:24,  1.94it/s]Epoch 9:  84%|████████▍ | 253/300 [02:12<00:24,  1.88it/s]Epoch 9:  85%|████████▍ | 254/300 [02:13<00:23,  1.94it/s]Epoch 9:  85%|████████▌ | 255/300 [02:13<00:24,  1.87it/s]Epoch 9:  85%|████████▌ | 256/300 [02:14<00:22,  1.94it/s]Epoch 9:  86%|████████▌ | 257/300 [02:14<00:21,  1.98it/s]Epoch 9:  86%|████████▌ | 258/300 [02:15<00:20,  2.02it/s]Epoch 9:  86%|████████▋ | 259/300 [02:15<00:21,  1.94it/s]06/19/2022 20:13:20 - INFO - __main__ - global step: 1480; train loss: 7.497141361236572; dev loss: 7.713925838470459
Epoch 9:  87%|████████▋ | 260/300 [02:16<00:21,  1.87it/s]Epoch 9:  87%|████████▋ | 261/300 [02:16<00:20,  1.94it/s]Epoch 9:  87%|████████▋ | 262/300 [02:17<00:19,  1.98it/s]Epoch 9:  88%|████████▊ | 263/300 [02:17<00:19,  1.91it/s]Epoch 9:  88%|████████▊ | 264/300 [02:18<00:19,  1.86it/s]Epoch 9:  88%|████████▊ | 265/300 [02:18<00:18,  1.92it/s]Epoch 9:  89%|████████▊ | 266/300 [02:19<00:17,  1.96it/s]Epoch 9:  89%|████████▉ | 267/300 [02:19<00:17,  1.91it/s]Epoch 9:  89%|████████▉ | 268/300 [02:20<00:17,  1.80it/s]Epoch 9:  90%|████████▉ | 269/300 [02:20<00:16,  1.84it/s]Epoch 9:  90%|█████████ | 270/300 [02:21<00:15,  1.90it/s]Epoch 9:  90%|█████████ | 271/300 [02:21<00:14,  1.96it/s]Epoch 9:  91%|█████████ | 272/300 [02:22<00:14,  1.89it/s]Epoch 9:  91%|█████████ | 273/300 [02:23<00:13,  1.94it/s]Epoch 9:  91%|█████████▏| 274/300 [02:23<00:13,  1.98it/s]Epoch 9:  92%|█████████▏| 275/300 [02:23<00:12,  2.02it/s]Epoch 9:  92%|█████████▏| 276/300 [02:24<00:13,  1.84it/s]Epoch 9:  92%|█████████▏| 277/300 [02:25<00:12,  1.91it/s]Epoch 9:  93%|█████████▎| 278/300 [02:25<00:11,  1.96it/s]Epoch 9:  93%|█████████▎| 279/300 [02:26<00:10,  2.00it/s]06/19/2022 20:13:30 - INFO - __main__ - global step: 1490; train loss: 7.8016204833984375; dev loss: 7.8748931884765625
Epoch 9:  93%|█████████▎| 280/300 [02:26<00:10,  1.90it/s]Epoch 9:  94%|█████████▎| 281/300 [02:27<00:09,  1.96it/s]Epoch 9:  94%|█████████▍| 282/300 [02:27<00:09,  1.99it/s]Epoch 9:  94%|█████████▍| 283/300 [02:28<00:08,  2.02it/s]Epoch 9:  95%|█████████▍| 284/300 [02:28<00:08,  1.92it/s]Epoch 9:  95%|█████████▌| 285/300 [02:29<00:07,  1.97it/s]Epoch 9:  95%|█████████▌| 286/300 [02:29<00:07,  1.90it/s]Epoch 9:  96%|█████████▌| 287/300 [02:30<00:07,  1.86it/s]Epoch 9:  96%|█████████▌| 288/300 [02:30<00:06,  1.88it/s]Epoch 9:  96%|█████████▋| 289/300 [02:31<00:05,  1.84it/s]Epoch 9:  97%|█████████▋| 290/300 [02:31<00:05,  1.90it/s]Epoch 9:  97%|█████████▋| 291/300 [02:32<00:04,  1.91it/s]Epoch 9:  97%|█████████▋| 292/300 [02:32<00:04,  1.96it/s]Epoch 9:  98%|█████████▊| 293/300 [02:33<00:03,  1.88it/s]Epoch 9:  98%|█████████▊| 294/300 [02:33<00:03,  1.94it/s]Epoch 9:  98%|█████████▊| 295/300 [02:34<00:02,  1.99it/s]Epoch 9:  99%|█████████▊| 296/300 [02:34<00:01,  2.02it/s]Epoch 9:  99%|█████████▉| 297/300 [02:35<00:01,  1.89it/s]Epoch 9:  99%|█████████▉| 298/300 [02:35<00:01,  1.96it/s]Epoch 9: 100%|█████████▉| 299/300 [02:36<00:00,  1.98it/s]06/19/2022 20:13:40 - INFO - __main__ - global step: 1500; train loss: 8.376189231872559; dev loss: 8.120258331298828
Epoch 9: 100%|██████████| 300/300 [02:36<00:00,  2.02it/s]Epoch 9: 100%|██████████| 300/300 [02:36<00:00,  1.91it/s]
Epoch 10:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 10:   0%|          | 1/300 [00:00<03:05,  1.61it/s]Epoch 10:   1%|          | 2/300 [00:01<02:46,  1.79it/s]Epoch 10:   1%|          | 3/300 [00:01<02:38,  1.87it/s]Epoch 10:   1%|▏         | 4/300 [00:02<02:35,  1.90it/s]Epoch 10:   2%|▏         | 5/300 [00:02<02:40,  1.84it/s]Epoch 10:   2%|▏         | 6/300 [00:03<02:35,  1.89it/s]Epoch 10:   2%|▏         | 7/300 [00:03<02:37,  1.86it/s]Epoch 10:   3%|▎         | 8/300 [00:04<02:32,  1.92it/s]Epoch 10:   3%|▎         | 9/300 [00:04<02:43,  1.78it/s]Epoch 10:   3%|▎         | 10/300 [00:05<02:35,  1.86it/s]Epoch 10:   4%|▎         | 11/300 [00:05<02:37,  1.84it/s]Epoch 10:   4%|▍         | 12/300 [00:06<02:30,  1.91it/s]Epoch 10:   4%|▍         | 13/300 [00:06<02:26,  1.96it/s]Epoch 10:   5%|▍         | 14/300 [00:07<02:32,  1.87it/s]Epoch 10:   5%|▌         | 15/300 [00:07<02:27,  1.94it/s]Epoch 10:   5%|▌         | 16/300 [00:08<02:23,  1.97it/s]Epoch 10:   6%|▌         | 17/300 [00:08<02:20,  2.02it/s]Epoch 10:   6%|▌         | 18/300 [00:09<02:25,  1.93it/s]Epoch 10:   6%|▋         | 19/300 [00:09<02:21,  1.99it/s]06/19/2022 20:13:51 - INFO - __main__ - global step: 1510; train loss: 8.087724685668945; dev loss: 7.826867580413818
Epoch 10:   7%|▋         | 20/300 [00:10<02:26,  1.92it/s]Epoch 10:   7%|▋         | 21/300 [00:11<02:25,  1.92it/s]Epoch 10:   7%|▋         | 22/300 [00:11<02:28,  1.87it/s]Epoch 10:   8%|▊         | 23/300 [00:12<02:24,  1.92it/s]Epoch 10:   8%|▊         | 24/300 [00:12<02:20,  1.96it/s]Epoch 10:   8%|▊         | 25/300 [00:13<02:18,  1.99it/s]Epoch 10:   9%|▊         | 26/300 [00:13<02:23,  1.91it/s]Epoch 10:   9%|▉         | 27/300 [00:14<02:19,  1.96it/s]Epoch 10:   9%|▉         | 28/300 [00:14<02:16,  1.99it/s]Epoch 10:  10%|▉         | 29/300 [00:15<02:14,  2.02it/s]Epoch 10:  10%|█         | 30/300 [00:15<02:20,  1.93it/s]Epoch 10:  10%|█         | 31/300 [00:16<02:22,  1.88it/s]Epoch 10:  11%|█         | 32/300 [00:16<02:18,  1.94it/s]Epoch 10:  11%|█         | 33/300 [00:17<02:14,  1.99it/s]Epoch 10:  11%|█▏        | 34/300 [00:17<02:24,  1.85it/s]Epoch 10:  12%|█▏        | 35/300 [00:18<02:18,  1.91it/s]Epoch 10:  12%|█▏        | 36/300 [00:18<02:14,  1.97it/s]Epoch 10:  12%|█▏        | 37/300 [00:19<02:11,  2.00it/s]Epoch 10:  13%|█▎        | 38/300 [00:19<02:16,  1.92it/s]Epoch 10:  13%|█▎        | 39/300 [00:20<02:12,  1.97it/s]06/19/2022 20:14:01 - INFO - __main__ - global step: 1520; train loss: 8.015361785888672; dev loss: 8.10403060913086
Epoch 10:  13%|█▎        | 40/300 [00:20<02:10,  1.99it/s]Epoch 10:  14%|█▎        | 41/300 [00:21<02:08,  2.02it/s]Epoch 10:  14%|█▍        | 42/300 [00:21<02:12,  1.94it/s]Epoch 10:  14%|█▍        | 43/300 [00:22<02:17,  1.87it/s]Epoch 10:  15%|█▍        | 44/300 [00:22<02:12,  1.94it/s]Epoch 10:  15%|█▌        | 45/300 [00:23<02:08,  1.99it/s]Epoch 10:  15%|█▌        | 46/300 [00:23<02:12,  1.91it/s]Epoch 10:  16%|█▌        | 47/300 [00:24<02:16,  1.85it/s]Epoch 10:  16%|█▌        | 48/300 [00:25<02:17,  1.83it/s]Epoch 10:  16%|█▋        | 49/300 [00:25<02:12,  1.89it/s]Epoch 10:  17%|█▋        | 50/300 [00:26<02:08,  1.95it/s]Epoch 10:  17%|█▋        | 51/300 [00:26<02:11,  1.89it/s]Epoch 10:  17%|█▋        | 52/300 [00:27<02:07,  1.95it/s]Epoch 10:  18%|█▊        | 53/300 [00:27<02:04,  1.99it/s]Epoch 10:  18%|█▊        | 54/300 [00:28<02:04,  1.97it/s]Epoch 10:  18%|█▊        | 55/300 [00:28<02:09,  1.89it/s]Epoch 10:  19%|█▊        | 56/300 [00:29<02:05,  1.95it/s]Epoch 10:  19%|█▉        | 57/300 [00:29<02:04,  1.96it/s]Epoch 10:  19%|█▉        | 58/300 [00:30<02:04,  1.95it/s]Epoch 10:  20%|█▉        | 59/300 [00:30<02:08,  1.88it/s]06/19/2022 20:14:12 - INFO - __main__ - global step: 1530; train loss: 7.87708044052124; dev loss: 7.314724922180176
Epoch 10:  20%|██        | 60/300 [00:31<02:04,  1.93it/s]Epoch 10:  20%|██        | 61/300 [00:31<02:02,  1.96it/s]Epoch 10:  21%|██        | 62/300 [00:32<01:59,  1.99it/s]Epoch 10:  21%|██        | 63/300 [00:32<02:03,  1.91it/s]Epoch 10:  21%|██▏       | 64/300 [00:33<02:02,  1.92it/s]Epoch 10:  22%|██▏       | 65/300 [00:33<01:59,  1.97it/s]Epoch 10:  22%|██▏       | 66/300 [00:34<01:56,  2.00it/s]Epoch 10:  22%|██▏       | 67/300 [00:34<01:54,  2.03it/s]Epoch 10:  23%|██▎       | 68/300 [00:35<02:05,  1.85it/s]Epoch 10:  23%|██▎       | 69/300 [00:35<02:03,  1.87it/s]Epoch 10:  23%|██▎       | 70/300 [00:36<01:58,  1.94it/s]Epoch 10:  24%|██▎       | 71/300 [00:36<02:01,  1.89it/s]Epoch 10:  24%|██▍       | 72/300 [00:37<02:09,  1.77it/s]Epoch 10:  24%|██▍       | 73/300 [00:38<02:02,  1.85it/s]Epoch 10:  25%|██▍       | 74/300 [00:38<02:00,  1.87it/s]Epoch 10:  25%|██▌       | 75/300 [00:39<01:56,  1.93it/s]Epoch 10:  25%|██▌       | 76/300 [00:39<01:59,  1.87it/s]Epoch 10:  26%|██▌       | 77/300 [00:40<01:58,  1.89it/s]Epoch 10:  26%|██▌       | 78/300 [00:40<01:53,  1.95it/s]Epoch 10:  26%|██▋       | 79/300 [00:41<01:50,  2.00it/s]06/19/2022 20:14:22 - INFO - __main__ - global step: 1540; train loss: 7.9409661293029785; dev loss: 8.197929382324219
Epoch 10:  27%|██▋       | 80/300 [00:41<01:57,  1.87it/s]Epoch 10:  27%|██▋       | 81/300 [00:42<01:53,  1.94it/s]Epoch 10:  27%|██▋       | 82/300 [00:42<01:51,  1.96it/s]Epoch 10:  28%|██▊       | 83/300 [00:43<01:49,  1.98it/s]Epoch 10:  28%|██▊       | 84/300 [00:43<01:55,  1.88it/s]Epoch 10:  28%|██▊       | 85/300 [00:44<01:51,  1.93it/s]Epoch 10:  29%|██▊       | 86/300 [00:44<01:49,  1.96it/s]Epoch 10:  29%|██▉       | 87/300 [00:45<01:47,  1.99it/s]Epoch 10:  29%|██▉       | 88/300 [00:45<01:51,  1.90it/s]Epoch 10:  30%|██▉       | 89/300 [00:46<01:48,  1.94it/s]Epoch 10:  30%|███       | 90/300 [00:46<01:46,  1.98it/s]Epoch 10:  30%|███       | 91/300 [00:47<01:49,  1.91it/s]Epoch 10:  31%|███       | 92/300 [00:48<01:57,  1.77it/s]Epoch 10:  31%|███       | 93/300 [00:48<01:52,  1.85it/s]Epoch 10:  31%|███▏      | 94/300 [00:49<01:53,  1.82it/s]Epoch 10:  32%|███▏      | 95/300 [00:49<01:54,  1.80it/s]Epoch 10:  32%|███▏      | 96/300 [00:50<01:49,  1.86it/s]Epoch 10:  32%|███▏      | 97/300 [00:50<01:54,  1.77it/s]Epoch 10:  33%|███▎      | 98/300 [00:51<01:52,  1.80it/s]Epoch 10:  33%|███▎      | 99/300 [00:51<01:47,  1.87it/s]06/19/2022 20:14:33 - INFO - __main__ - global step: 1550; train loss: 7.691130638122559; dev loss: 7.816407203674316
Epoch 10:  33%|███▎      | 100/300 [00:52<01:47,  1.85it/s]Epoch 10:  34%|███▎      | 101/300 [00:52<01:49,  1.82it/s]Epoch 10:  34%|███▍      | 102/300 [00:53<01:46,  1.87it/s]Epoch 10:  34%|███▍      | 103/300 [00:53<01:42,  1.91it/s]Epoch 10:  35%|███▍      | 104/300 [00:54<01:45,  1.86it/s]Epoch 10:  35%|███▌      | 105/300 [00:55<01:47,  1.81it/s]Epoch 10:  35%|███▌      | 106/300 [00:55<01:47,  1.80it/s]Epoch 10:  36%|███▌      | 107/300 [00:56<01:47,  1.80it/s]Epoch 10:  36%|███▌      | 108/300 [00:56<01:42,  1.88it/s]Epoch 10:  36%|███▋      | 109/300 [00:57<01:48,  1.76it/s]Epoch 10:  37%|███▋      | 110/300 [00:57<01:42,  1.85it/s]Epoch 10:  37%|███▋      | 111/300 [00:58<01:38,  1.92it/s]Epoch 10:  37%|███▋      | 112/300 [00:58<01:35,  1.96it/s]Epoch 10:  38%|███▊      | 113/300 [00:59<01:40,  1.87it/s]Epoch 10:  38%|███▊      | 114/300 [00:59<01:36,  1.93it/s]Epoch 10:  38%|███▊      | 115/300 [01:00<01:33,  1.98it/s]Epoch 10:  39%|███▊      | 116/300 [01:00<01:31,  2.02it/s]Epoch 10:  39%|███▉      | 117/300 [01:01<01:35,  1.92it/s]Epoch 10:  39%|███▉      | 118/300 [01:01<01:32,  1.98it/s]Epoch 10:  40%|███▉      | 119/300 [01:02<01:30,  2.01it/s]06/19/2022 20:14:43 - INFO - __main__ - global step: 1560; train loss: 8.253206253051758; dev loss: 7.960440158843994
Epoch 10:  40%|████      | 120/300 [01:02<01:28,  2.03it/s]Epoch 10:  40%|████      | 121/300 [01:03<01:27,  2.06it/s]Epoch 10:  41%|████      | 122/300 [01:03<01:33,  1.91it/s]Epoch 10:  41%|████      | 123/300 [01:04<01:34,  1.87it/s]Epoch 10:  41%|████▏     | 124/300 [01:04<01:31,  1.93it/s]Epoch 10:  42%|████▏     | 125/300 [01:05<01:28,  1.99it/s]Epoch 10:  42%|████▏     | 126/300 [01:05<01:30,  1.92it/s]Epoch 10:  42%|████▏     | 127/300 [01:06<01:27,  1.98it/s]Epoch 10:  43%|████▎     | 128/300 [01:06<01:25,  2.02it/s]Epoch 10:  43%|████▎     | 129/300 [01:07<01:25,  1.99it/s]Epoch 10:  43%|████▎     | 130/300 [01:07<01:28,  1.92it/s]Epoch 10:  44%|████▎     | 131/300 [01:08<01:25,  1.97it/s]Epoch 10:  44%|████▍     | 132/300 [01:08<01:23,  2.01it/s]Epoch 10:  44%|████▍     | 133/300 [01:09<01:22,  2.03it/s]Epoch 10:  45%|████▍     | 134/300 [01:09<01:25,  1.94it/s]Epoch 10:  45%|████▌     | 135/300 [01:10<01:23,  1.99it/s]Epoch 10:  45%|████▌     | 136/300 [01:10<01:21,  2.01it/s]Epoch 10:  46%|████▌     | 137/300 [01:11<01:19,  2.04it/s]Epoch 10:  46%|████▌     | 138/300 [01:11<01:23,  1.95it/s]Epoch 10:  46%|████▋     | 139/300 [01:12<01:20,  1.99it/s]06/19/2022 20:14:53 - INFO - __main__ - global step: 1570; train loss: 7.098604679107666; dev loss: 7.297664642333984
Epoch 10:  47%|████▋     | 140/300 [01:12<01:23,  1.91it/s]Epoch 10:  47%|████▋     | 141/300 [01:13<01:25,  1.87it/s]Epoch 10:  47%|████▋     | 142/300 [01:14<01:26,  1.83it/s]Epoch 10:  48%|████▊     | 143/300 [01:14<01:22,  1.89it/s]Epoch 10:  48%|████▊     | 144/300 [01:15<01:20,  1.95it/s]Epoch 10:  48%|████▊     | 145/300 [01:15<01:21,  1.90it/s]Epoch 10:  49%|████▊     | 146/300 [01:16<01:23,  1.85it/s]Epoch 10:  49%|████▉     | 147/300 [01:16<01:19,  1.91it/s]Epoch 10:  49%|████▉     | 148/300 [01:17<01:20,  1.88it/s]Epoch 10:  50%|████▉     | 149/300 [01:17<01:18,  1.92it/s]Epoch 10:  50%|█████     | 150/300 [01:18<01:16,  1.97it/s]Epoch 10:  50%|█████     | 151/300 [01:18<01:18,  1.89it/s]Epoch 10:  51%|█████     | 152/300 [01:19<01:16,  1.94it/s]Epoch 10:  51%|█████     | 153/300 [01:19<01:14,  1.97it/s]Epoch 10:  51%|█████▏    | 154/300 [01:20<01:12,  2.01it/s]Epoch 10:  52%|█████▏    | 155/300 [01:20<01:14,  1.94it/s]Epoch 10:  52%|█████▏    | 156/300 [01:21<01:12,  1.98it/s]Epoch 10:  52%|█████▏    | 157/300 [01:21<01:11,  2.00it/s]Epoch 10:  53%|█████▎    | 158/300 [01:22<01:11,  1.98it/s]Epoch 10:  53%|█████▎    | 159/300 [01:22<01:17,  1.82it/s]06/19/2022 20:15:04 - INFO - __main__ - global step: 1580; train loss: 7.775421142578125; dev loss: 7.675375461578369
Epoch 10:  53%|█████▎    | 160/300 [01:23<01:14,  1.87it/s]Epoch 10:  54%|█████▎    | 161/300 [01:23<01:11,  1.94it/s]Epoch 10:  54%|█████▍    | 162/300 [01:24<01:09,  1.97it/s]Epoch 10:  54%|█████▍    | 163/300 [01:24<01:11,  1.91it/s]Epoch 10:  55%|█████▍    | 164/300 [01:25<01:09,  1.96it/s]Epoch 10:  55%|█████▌    | 165/300 [01:25<01:07,  2.00it/s]Epoch 10:  55%|█████▌    | 166/300 [01:26<01:06,  2.01it/s]Epoch 10:  56%|█████▌    | 167/300 [01:26<01:08,  1.93it/s]Epoch 10:  56%|█████▌    | 168/300 [01:27<01:06,  1.98it/s]Epoch 10:  56%|█████▋    | 169/300 [01:28<01:08,  1.91it/s]Epoch 10:  57%|█████▋    | 170/300 [01:28<01:05,  1.97it/s]Epoch 10:  57%|█████▋    | 171/300 [01:29<01:11,  1.81it/s]Epoch 10:  57%|█████▋    | 172/300 [01:29<01:07,  1.89it/s]Epoch 10:  58%|█████▊    | 173/300 [01:30<01:07,  1.89it/s]Epoch 10:  58%|█████▊    | 174/300 [01:30<01:04,  1.95it/s]Epoch 10:  58%|█████▊    | 175/300 [01:31<01:02,  1.99it/s]Epoch 10:  59%|█████▊    | 176/300 [01:31<01:04,  1.91it/s]Epoch 10:  59%|█████▉    | 177/300 [01:32<01:02,  1.97it/s]Epoch 10:  59%|█████▉    | 178/300 [01:32<01:03,  1.91it/s]Epoch 10:  60%|█████▉    | 179/300 [01:33<01:01,  1.97it/s]06/19/2022 20:15:14 - INFO - __main__ - global step: 1590; train loss: 7.198700904846191; dev loss: 7.25247049331665
Epoch 10:  60%|██████    | 180/300 [01:33<01:03,  1.90it/s]Epoch 10:  60%|██████    | 181/300 [01:34<01:04,  1.85it/s]Epoch 10:  61%|██████    | 182/300 [01:34<01:01,  1.92it/s]Epoch 10:  61%|██████    | 183/300 [01:35<00:59,  1.98it/s]Epoch 10:  61%|██████▏   | 184/300 [01:35<01:03,  1.82it/s]Epoch 10:  62%|██████▏   | 185/300 [01:36<01:00,  1.89it/s]Epoch 10:  62%|██████▏   | 186/300 [01:36<00:59,  1.90it/s]Epoch 10:  62%|██████▏   | 187/300 [01:37<00:57,  1.96it/s]Epoch 10:  63%|██████▎   | 188/300 [01:37<00:59,  1.89it/s]Epoch 10:  63%|██████▎   | 189/300 [01:38<00:57,  1.94it/s]Epoch 10:  63%|██████▎   | 190/300 [01:38<00:55,  1.98it/s]Epoch 10:  64%|██████▎   | 191/300 [01:39<00:54,  2.00it/s]Epoch 10:  64%|██████▍   | 192/300 [01:40<00:57,  1.88it/s]Epoch 10:  64%|██████▍   | 193/300 [01:40<00:55,  1.94it/s]Epoch 10:  65%|██████▍   | 194/300 [01:41<00:56,  1.88it/s]Epoch 10:  65%|██████▌   | 195/300 [01:41<00:53,  1.95it/s]Epoch 10:  65%|██████▌   | 196/300 [01:42<00:55,  1.89it/s]Epoch 10:  66%|██████▌   | 197/300 [01:42<00:55,  1.86it/s]Epoch 10:  66%|██████▌   | 198/300 [01:43<00:55,  1.83it/s]Epoch 10:  66%|██████▋   | 199/300 [01:43<00:53,  1.90it/s]06/19/2022 20:15:25 - INFO - __main__ - global step: 1600; train loss: 7.986910820007324; dev loss: 8.125696182250977
Epoch 10:  67%|██████▋   | 200/300 [01:44<00:54,  1.84it/s]Epoch 10:  67%|██████▋   | 201/300 [01:44<00:51,  1.92it/s]Epoch 10:  67%|██████▋   | 202/300 [01:45<00:49,  1.97it/s]Epoch 10:  68%|██████▊   | 203/300 [01:45<00:48,  2.00it/s]Epoch 10:  68%|██████▊   | 204/300 [01:46<00:48,  1.98it/s]Epoch 10:  68%|██████▊   | 205/300 [01:46<00:52,  1.82it/s]Epoch 10:  69%|██████▊   | 206/300 [01:47<00:49,  1.90it/s]Epoch 10:  69%|██████▉   | 207/300 [01:47<00:47,  1.96it/s]Epoch 10:  69%|██████▉   | 208/300 [01:48<00:47,  1.96it/s]Epoch 10:  70%|██████▉   | 209/300 [01:48<00:49,  1.85it/s]Epoch 10:  70%|███████   | 210/300 [01:49<00:47,  1.91it/s]Epoch 10:  70%|███████   | 211/300 [01:49<00:45,  1.97it/s]Epoch 10:  71%|███████   | 212/300 [01:50<00:44,  2.00it/s]Epoch 10:  71%|███████   | 213/300 [01:50<00:45,  1.92it/s]Epoch 10:  71%|███████▏  | 214/300 [01:51<00:43,  1.97it/s]Epoch 10:  72%|███████▏  | 215/300 [01:51<00:44,  1.92it/s]Epoch 10:  72%|███████▏  | 216/300 [01:52<00:44,  1.87it/s]Epoch 10:  72%|███████▏  | 217/300 [01:53<00:47,  1.76it/s]Epoch 10:  73%|███████▎  | 218/300 [01:53<00:44,  1.85it/s]Epoch 10:  73%|███████▎  | 219/300 [01:54<00:43,  1.87it/s]06/19/2022 20:15:35 - INFO - __main__ - global step: 1610; train loss: 7.404273986816406; dev loss: 7.427127838134766
Epoch 10:  73%|███████▎  | 220/300 [01:54<00:42,  1.89it/s]Epoch 10:  74%|███████▎  | 221/300 [01:55<00:43,  1.83it/s]Epoch 10:  74%|███████▍  | 222/300 [01:55<00:40,  1.91it/s]Epoch 10:  74%|███████▍  | 223/300 [01:56<00:41,  1.87it/s]Epoch 10:  75%|███████▍  | 224/300 [01:56<00:39,  1.93it/s]Epoch 10:  75%|███████▌  | 225/300 [01:57<00:40,  1.87it/s]Epoch 10:  75%|███████▌  | 226/300 [01:57<00:38,  1.93it/s]Epoch 10:  76%|███████▌  | 227/300 [01:58<00:36,  1.98it/s]Epoch 10:  76%|███████▌  | 228/300 [01:58<00:36,  1.97it/s]Epoch 10:  76%|███████▋  | 229/300 [01:59<00:35,  1.99it/s]Epoch 10:  77%|███████▋  | 230/300 [01:59<00:38,  1.83it/s]Epoch 10:  77%|███████▋  | 231/300 [02:00<00:38,  1.81it/s]Epoch 10:  77%|███████▋  | 232/300 [02:01<00:36,  1.89it/s]Epoch 10:  78%|███████▊  | 233/300 [02:01<00:35,  1.91it/s]Epoch 10:  78%|███████▊  | 234/300 [02:02<00:35,  1.87it/s]Epoch 10:  78%|███████▊  | 235/300 [02:02<00:33,  1.92it/s]Epoch 10:  79%|███████▊  | 236/300 [02:03<00:32,  1.96it/s]Epoch 10:  79%|███████▉  | 237/300 [02:03<00:32,  1.91it/s]Epoch 10:  79%|███████▉  | 238/300 [02:04<00:33,  1.86it/s]Epoch 10:  80%|███████▉  | 239/300 [02:04<00:33,  1.85it/s]06/19/2022 20:15:46 - INFO - __main__ - global step: 1620; train loss: 7.3515944480896; dev loss: 7.865330696105957
Epoch 10:  80%|████████  | 240/300 [02:05<00:31,  1.91it/s]Epoch 10:  80%|████████  | 241/300 [02:05<00:31,  1.87it/s]Epoch 10:  81%|████████  | 242/300 [02:06<00:33,  1.75it/s]Epoch 10:  81%|████████  | 243/300 [02:06<00:30,  1.84it/s]Epoch 10:  81%|████████▏ | 244/300 [02:07<00:29,  1.89it/s]Epoch 10:  82%|████████▏ | 245/300 [02:07<00:28,  1.91it/s]Epoch 10:  82%|████████▏ | 246/300 [02:08<00:30,  1.78it/s]Epoch 10:  82%|████████▏ | 247/300 [02:09<00:29,  1.78it/s]Epoch 10:  83%|████████▎ | 248/300 [02:09<00:27,  1.87it/s]Epoch 10:  83%|████████▎ | 249/300 [02:10<00:26,  1.93it/s]Epoch 10:  83%|████████▎ | 250/300 [02:10<00:26,  1.88it/s]Epoch 10:  84%|████████▎ | 251/300 [02:11<00:25,  1.93it/s]Epoch 10:  84%|████████▍ | 252/300 [02:11<00:24,  1.96it/s]Epoch 10:  84%|████████▍ | 253/300 [02:12<00:23,  2.01it/s]Epoch 10:  85%|████████▍ | 254/300 [02:12<00:23,  1.92it/s]Epoch 10:  85%|████████▌ | 255/300 [02:13<00:23,  1.93it/s]Epoch 10:  85%|████████▌ | 256/300 [02:13<00:22,  1.96it/s]Epoch 10:  86%|████████▌ | 257/300 [02:14<00:21,  1.99it/s]Epoch 10:  86%|████████▌ | 258/300 [02:14<00:20,  2.02it/s]Epoch 10:  86%|████████▋ | 259/300 [02:15<00:21,  1.92it/s]06/19/2022 20:15:56 - INFO - __main__ - global step: 1630; train loss: 7.970558166503906; dev loss: 8.066122055053711
Epoch 10:  87%|████████▋ | 260/300 [02:15<00:20,  1.98it/s]Epoch 10:  87%|████████▋ | 261/300 [02:16<00:19,  1.99it/s]Epoch 10:  87%|████████▋ | 262/300 [02:16<00:19,  1.93it/s]Epoch 10:  88%|████████▊ | 263/300 [02:17<00:19,  1.88it/s]Epoch 10:  88%|████████▊ | 264/300 [02:17<00:18,  1.94it/s]Epoch 10:  88%|████████▊ | 265/300 [02:18<00:18,  1.89it/s]Epoch 10:  89%|████████▊ | 266/300 [02:18<00:17,  1.94it/s]Epoch 10:  89%|████████▉ | 267/300 [02:19<00:18,  1.81it/s]Epoch 10:  89%|████████▉ | 268/300 [02:19<00:16,  1.89it/s]Epoch 10:  90%|████████▉ | 269/300 [02:20<00:16,  1.86it/s]Epoch 10:  90%|█████████ | 270/300 [02:20<00:15,  1.92it/s]Epoch 10:  90%|█████████ | 271/300 [02:21<00:16,  1.79it/s]Epoch 10:  91%|█████████ | 272/300 [02:22<00:15,  1.87it/s]Epoch 10:  91%|█████████ | 273/300 [02:22<00:13,  1.94it/s]Epoch 10:  91%|█████████▏| 274/300 [02:23<00:13,  1.98it/s]Epoch 10:  92%|█████████▏| 275/300 [02:23<00:13,  1.90it/s]Epoch 10:  92%|█████████▏| 276/300 [02:24<00:12,  1.86it/s]Epoch 10:  92%|█████████▏| 277/300 [02:24<00:11,  1.93it/s]Epoch 10:  93%|█████████▎| 278/300 [02:25<00:11,  1.98it/s]Epoch 10:  93%|█████████▎| 279/300 [02:25<00:11,  1.90it/s]06/19/2022 20:16:07 - INFO - __main__ - global step: 1640; train loss: 8.515243530273438; dev loss: 8.434330940246582
Epoch 10:  93%|█████████▎| 280/300 [02:26<00:10,  1.90it/s]Epoch 10:  94%|█████████▎| 281/300 [02:26<00:10,  1.87it/s]Epoch 10:  94%|█████████▍| 282/300 [02:27<00:09,  1.89it/s]Epoch 10:  94%|█████████▍| 283/300 [02:27<00:08,  1.95it/s]Epoch 10:  95%|█████████▍| 284/300 [02:28<00:08,  1.87it/s]Epoch 10:  95%|█████████▌| 285/300 [02:28<00:07,  1.92it/s]Epoch 10:  95%|█████████▌| 286/300 [02:29<00:07,  1.98it/s]Epoch 10:  96%|█████████▌| 287/300 [02:29<00:06,  1.97it/s]Epoch 10:  96%|█████████▌| 288/300 [02:30<00:06,  1.88it/s]Epoch 10:  96%|█████████▋| 289/300 [02:30<00:05,  1.94it/s]Epoch 10:  97%|█████████▋| 290/300 [02:31<00:05,  1.94it/s]Epoch 10:  97%|█████████▋| 291/300 [02:31<00:04,  1.98it/s]Epoch 10:  97%|█████████▋| 292/300 [02:32<00:04,  1.82it/s]Epoch 10:  98%|█████████▊| 293/300 [02:33<00:03,  1.90it/s]Epoch 10:  98%|█████████▊| 294/300 [02:33<00:03,  1.96it/s]Epoch 10:  98%|█████████▊| 295/300 [02:34<00:02,  1.96it/s]Epoch 10:  99%|█████████▊| 296/300 [02:34<00:02,  1.90it/s]Epoch 10:  99%|█████████▉| 297/300 [02:35<00:01,  1.94it/s]Epoch 10:  99%|█████████▉| 298/300 [02:35<00:01,  1.89it/s]Epoch 10: 100%|█████████▉| 299/300 [02:36<00:00,  1.86it/s]06/19/2022 20:16:17 - INFO - __main__ - global step: 1650; train loss: 7.873291015625; dev loss: 7.179883003234863
Epoch 10: 100%|██████████| 300/300 [02:36<00:00,  1.83it/s]Epoch 10: 100%|██████████| 300/300 [02:36<00:00,  1.91it/s]
Epoch 11:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 11:   0%|          | 1/300 [00:00<02:50,  1.75it/s]Epoch 11:   1%|          | 2/300 [00:01<02:33,  1.94it/s]Epoch 11:   1%|          | 3/300 [00:01<02:38,  1.87it/s]Epoch 11:   1%|▏         | 4/300 [00:02<02:42,  1.82it/s]Epoch 11:   2%|▏         | 5/300 [00:02<02:42,  1.81it/s]Epoch 11:   2%|▏         | 6/300 [00:03<02:34,  1.90it/s]Epoch 11:   2%|▏         | 7/300 [00:03<02:33,  1.91it/s]Epoch 11:   3%|▎         | 8/300 [00:04<02:37,  1.86it/s]Epoch 11:   3%|▎         | 9/300 [00:04<02:31,  1.92it/s]Epoch 11:   3%|▎         | 10/300 [00:05<02:34,  1.88it/s]Epoch 11:   4%|▎         | 11/300 [00:05<02:29,  1.94it/s]Epoch 11:   4%|▍         | 12/300 [00:06<02:25,  1.98it/s]Epoch 11:   4%|▍         | 13/300 [00:06<02:34,  1.86it/s]Epoch 11:   5%|▍         | 14/300 [00:07<02:35,  1.83it/s]Epoch 11:   5%|▌         | 15/300 [00:07<02:29,  1.91it/s]Epoch 11:   5%|▌         | 16/300 [00:08<02:25,  1.96it/s]Epoch 11:   6%|▌         | 17/300 [00:09<02:32,  1.86it/s]Epoch 11:   6%|▌         | 18/300 [00:09<02:26,  1.92it/s]Epoch 11:   6%|▋         | 19/300 [00:09<02:22,  1.97it/s]06/19/2022 20:16:28 - INFO - __main__ - global step: 1660; train loss: 7.980101108551025; dev loss: 7.9426751136779785
Epoch 11:   7%|▋         | 20/300 [00:10<02:19,  2.01it/s]Epoch 11:   7%|▋         | 21/300 [00:11<02:25,  1.92it/s]Epoch 11:   7%|▋         | 22/300 [00:11<02:28,  1.88it/s]Epoch 11:   8%|▊         | 23/300 [00:12<02:23,  1.94it/s]Epoch 11:   8%|▊         | 24/300 [00:12<02:21,  1.95it/s]Epoch 11:   8%|▊         | 25/300 [00:13<02:25,  1.89it/s]Epoch 11:   9%|▊         | 26/300 [00:13<02:28,  1.85it/s]Epoch 11:   9%|▉         | 27/300 [00:14<02:26,  1.87it/s]Epoch 11:   9%|▉         | 28/300 [00:14<02:24,  1.88it/s]Epoch 11:  10%|▉         | 29/300 [00:15<02:31,  1.79it/s]Epoch 11:  10%|█         | 30/300 [00:15<02:23,  1.88it/s]Epoch 11:  10%|█         | 31/300 [00:16<02:18,  1.94it/s]Epoch 11:  11%|█         | 32/300 [00:16<02:15,  1.98it/s]Epoch 11:  11%|█         | 33/300 [00:17<02:19,  1.91it/s]Epoch 11:  11%|█▏        | 34/300 [00:17<02:21,  1.87it/s]Epoch 11:  12%|█▏        | 35/300 [00:18<02:19,  1.90it/s]Epoch 11:  12%|█▏        | 36/300 [00:18<02:15,  1.95it/s]Epoch 11:  12%|█▏        | 37/300 [00:19<02:12,  1.99it/s]Epoch 11:  13%|█▎        | 38/300 [00:19<02:17,  1.90it/s]Epoch 11:  13%|█▎        | 39/300 [00:20<02:12,  1.97it/s]06/19/2022 20:16:38 - INFO - __main__ - global step: 1670; train loss: 8.143704414367676; dev loss: 8.21238899230957
Epoch 11:  13%|█▎        | 40/300 [00:20<02:09,  2.01it/s]Epoch 11:  14%|█▎        | 41/300 [00:21<02:06,  2.04it/s]Epoch 11:  14%|█▍        | 42/300 [00:21<02:13,  1.94it/s]Epoch 11:  14%|█▍        | 43/300 [00:22<02:10,  1.97it/s]Epoch 11:  15%|█▍        | 44/300 [00:22<02:07,  2.01it/s]Epoch 11:  15%|█▌        | 45/300 [00:23<02:08,  1.98it/s]Epoch 11:  15%|█▌        | 46/300 [00:24<02:12,  1.91it/s]Epoch 11:  16%|█▌        | 47/300 [00:24<02:10,  1.93it/s]Epoch 11:  16%|█▌        | 48/300 [00:24<02:07,  1.98it/s]Epoch 11:  16%|█▋        | 49/300 [00:25<02:04,  2.01it/s]Epoch 11:  17%|█▋        | 50/300 [00:26<02:12,  1.88it/s]Epoch 11:  17%|█▋        | 51/300 [00:26<02:11,  1.89it/s]Epoch 11:  17%|█▋        | 52/300 [00:27<02:07,  1.95it/s]Epoch 11:  18%|█▊        | 53/300 [00:27<02:03,  1.99it/s]Epoch 11:  18%|█▊        | 54/300 [00:28<02:08,  1.92it/s]Epoch 11:  18%|█▊        | 55/300 [00:28<02:10,  1.88it/s]Epoch 11:  19%|█▊        | 56/300 [00:29<02:11,  1.85it/s]Epoch 11:  19%|█▉        | 57/300 [00:29<02:06,  1.92it/s]Epoch 11:  19%|█▉        | 58/300 [00:30<02:12,  1.83it/s]Epoch 11:  20%|█▉        | 59/300 [00:30<02:06,  1.90it/s]06/19/2022 20:16:48 - INFO - __main__ - global step: 1680; train loss: 7.300463676452637; dev loss: 7.547842502593994
Epoch 11:  20%|██        | 60/300 [00:31<02:02,  1.95it/s]Epoch 11:  20%|██        | 61/300 [00:31<02:05,  1.90it/s]Epoch 11:  21%|██        | 62/300 [00:32<02:08,  1.86it/s]Epoch 11:  21%|██        | 63/300 [00:32<02:04,  1.90it/s]Epoch 11:  21%|██▏       | 64/300 [00:33<02:00,  1.96it/s]Epoch 11:  22%|██▏       | 65/300 [00:33<01:57,  1.99it/s]Epoch 11:  22%|██▏       | 66/300 [00:34<01:55,  2.03it/s]Epoch 11:  22%|██▏       | 67/300 [00:34<02:03,  1.89it/s]Epoch 11:  23%|██▎       | 68/300 [00:35<01:58,  1.95it/s]Epoch 11:  23%|██▎       | 69/300 [00:35<01:55,  1.99it/s]Epoch 11:  23%|██▎       | 70/300 [00:36<01:56,  1.98it/s]Epoch 11:  24%|██▎       | 71/300 [00:36<01:59,  1.91it/s]Epoch 11:  24%|██▍       | 72/300 [00:37<01:56,  1.95it/s]Epoch 11:  24%|██▍       | 73/300 [00:37<01:53,  2.00it/s]Epoch 11:  25%|██▍       | 74/300 [00:38<01:51,  2.02it/s]Epoch 11:  25%|██▌       | 75/300 [00:39<01:57,  1.92it/s]Epoch 11:  25%|██▌       | 76/300 [00:39<01:54,  1.95it/s]Epoch 11:  26%|██▌       | 77/300 [00:39<01:52,  1.98it/s]Epoch 11:  26%|██▌       | 78/300 [00:40<01:52,  1.97it/s]Epoch 11:  26%|██▋       | 79/300 [00:41<01:56,  1.90it/s]06/19/2022 20:16:59 - INFO - __main__ - global step: 1690; train loss: 7.8440985679626465; dev loss: 8.065568923950195
Epoch 11:  27%|██▋       | 80/300 [00:41<01:55,  1.90it/s]Epoch 11:  27%|██▋       | 81/300 [00:42<01:55,  1.90it/s]Epoch 11:  27%|██▋       | 82/300 [00:42<01:57,  1.86it/s]Epoch 11:  28%|██▊       | 83/300 [00:43<01:59,  1.82it/s]Epoch 11:  28%|██▊       | 84/300 [00:43<01:54,  1.89it/s]Epoch 11:  28%|██▊       | 85/300 [00:44<01:50,  1.95it/s]Epoch 11:  29%|██▊       | 86/300 [00:44<01:47,  1.99it/s]Epoch 11:  29%|██▉       | 87/300 [00:45<01:51,  1.92it/s]Epoch 11:  29%|██▉       | 88/300 [00:45<01:48,  1.96it/s]Epoch 11:  30%|██▉       | 89/300 [00:46<01:45,  2.00it/s]Epoch 11:  30%|███       | 90/300 [00:46<01:46,  1.98it/s]Epoch 11:  30%|███       | 91/300 [00:47<01:49,  1.91it/s]Epoch 11:  31%|███       | 92/300 [00:47<01:57,  1.76it/s]Epoch 11:  31%|███       | 93/300 [00:48<01:55,  1.80it/s]Epoch 11:  31%|███▏      | 94/300 [00:49<01:52,  1.84it/s]Epoch 11:  32%|███▏      | 95/300 [00:49<01:49,  1.87it/s]Epoch 11:  32%|███▏      | 96/300 [00:50<01:51,  1.83it/s]Epoch 11:  32%|███▏      | 97/300 [00:50<01:46,  1.90it/s]Epoch 11:  33%|███▎      | 98/300 [00:51<01:42,  1.96it/s]Epoch 11:  33%|███▎      | 99/300 [00:51<01:41,  1.98it/s]06/19/2022 20:17:09 - INFO - __main__ - global step: 1700; train loss: 7.78963565826416; dev loss: 7.787212371826172
Epoch 11:  33%|███▎      | 100/300 [00:52<01:50,  1.81it/s]Epoch 11:  34%|███▎      | 101/300 [00:52<01:50,  1.81it/s]Epoch 11:  34%|███▍      | 102/300 [00:53<01:45,  1.88it/s]Epoch 11:  34%|███▍      | 103/300 [00:53<01:41,  1.93it/s]Epoch 11:  35%|███▍      | 104/300 [00:54<01:44,  1.88it/s]Epoch 11:  35%|███▌      | 105/300 [00:54<01:40,  1.93it/s]Epoch 11:  35%|███▌      | 106/300 [00:55<01:41,  1.92it/s]Epoch 11:  36%|███▌      | 107/300 [00:55<01:37,  1.97it/s]Epoch 11:  36%|███▌      | 108/300 [00:56<01:43,  1.85it/s]Epoch 11:  36%|███▋      | 109/300 [00:56<01:39,  1.91it/s]Epoch 11:  37%|███▋      | 110/300 [00:57<01:36,  1.96it/s]Epoch 11:  37%|███▋      | 111/300 [00:57<01:35,  1.99it/s]Epoch 11:  37%|███▋      | 112/300 [00:58<01:38,  1.90it/s]Epoch 11:  38%|███▊      | 113/300 [00:58<01:36,  1.94it/s]Epoch 11:  38%|███▊      | 114/300 [00:59<01:33,  1.98it/s]Epoch 11:  38%|███▊      | 115/300 [00:59<01:36,  1.91it/s]Epoch 11:  39%|███▊      | 116/300 [01:00<01:39,  1.86it/s]Epoch 11:  39%|███▉      | 117/300 [01:01<01:35,  1.92it/s]Epoch 11:  39%|███▉      | 118/300 [01:01<01:36,  1.88it/s]Epoch 11:  40%|███▉      | 119/300 [01:02<01:33,  1.94it/s]06/19/2022 20:17:20 - INFO - __main__ - global step: 1710; train loss: 8.016075134277344; dev loss: 7.691130638122559
Epoch 11:  40%|████      | 120/300 [01:02<01:31,  1.98it/s]Epoch 11:  40%|████      | 121/300 [01:03<01:33,  1.90it/s]Epoch 11:  41%|████      | 122/300 [01:03<01:30,  1.96it/s]Epoch 11:  41%|████      | 123/300 [01:04<01:32,  1.91it/s]Epoch 11:  41%|████▏     | 124/300 [01:04<01:29,  1.97it/s]Epoch 11:  42%|████▏     | 125/300 [01:05<01:31,  1.91it/s]Epoch 11:  42%|████▏     | 126/300 [01:05<01:29,  1.95it/s]Epoch 11:  42%|████▏     | 127/300 [01:06<01:31,  1.90it/s]Epoch 11:  43%|████▎     | 128/300 [01:06<01:30,  1.91it/s]Epoch 11:  43%|████▎     | 129/300 [01:07<01:31,  1.86it/s]Epoch 11:  43%|████▎     | 130/300 [01:07<01:28,  1.93it/s]Epoch 11:  44%|████▎     | 131/300 [01:08<01:29,  1.89it/s]Epoch 11:  44%|████▍     | 132/300 [01:08<01:26,  1.95it/s]Epoch 11:  44%|████▍     | 133/300 [01:09<01:28,  1.88it/s]Epoch 11:  45%|████▍     | 134/300 [01:09<01:25,  1.93it/s]Epoch 11:  45%|████▌     | 135/300 [01:10<01:27,  1.88it/s]Epoch 11:  45%|████▌     | 136/300 [01:10<01:28,  1.85it/s]Epoch 11:  46%|████▌     | 137/300 [01:11<01:29,  1.81it/s]Epoch 11:  46%|████▌     | 138/300 [01:12<01:27,  1.85it/s]Epoch 11:  46%|████▋     | 139/300 [01:12<01:23,  1.92it/s]06/19/2022 20:17:30 - INFO - __main__ - global step: 1720; train loss: 7.8774285316467285; dev loss: 7.392823696136475
Epoch 11:  47%|████▋     | 140/300 [01:13<01:21,  1.97it/s]Epoch 11:  47%|████▋     | 141/300 [01:13<01:24,  1.89it/s]Epoch 11:  47%|████▋     | 142/300 [01:14<01:21,  1.95it/s]Epoch 11:  48%|████▊     | 143/300 [01:14<01:22,  1.90it/s]Epoch 11:  48%|████▊     | 144/300 [01:15<01:19,  1.96it/s]Epoch 11:  48%|████▊     | 145/300 [01:15<01:17,  2.00it/s]Epoch 11:  49%|████▊     | 146/300 [01:16<01:22,  1.87it/s]Epoch 11:  49%|████▉     | 147/300 [01:16<01:19,  1.93it/s]Epoch 11:  49%|████▉     | 148/300 [01:17<01:16,  1.97it/s]Epoch 11:  50%|████▉     | 149/300 [01:17<01:18,  1.92it/s]Epoch 11:  50%|█████     | 150/300 [01:18<01:20,  1.87it/s]Epoch 11:  50%|█████     | 151/300 [01:18<01:17,  1.93it/s]Epoch 11:  51%|█████     | 152/300 [01:19<01:14,  1.98it/s]Epoch 11:  51%|█████     | 153/300 [01:19<01:13,  2.00it/s]Epoch 11:  51%|█████▏    | 154/300 [01:20<01:16,  1.91it/s]Epoch 11:  52%|█████▏    | 155/300 [01:20<01:14,  1.95it/s]Epoch 11:  52%|█████▏    | 156/300 [01:21<01:16,  1.89it/s]Epoch 11:  52%|█████▏    | 157/300 [01:21<01:13,  1.95it/s]Epoch 11:  53%|█████▎    | 158/300 [01:22<01:15,  1.88it/s]Epoch 11:  53%|█████▎    | 159/300 [01:22<01:12,  1.94it/s]06/19/2022 20:17:41 - INFO - __main__ - global step: 1730; train loss: 7.929080009460449; dev loss: 8.081812858581543
Epoch 11:  53%|█████▎    | 160/300 [01:23<01:12,  1.93it/s]Epoch 11:  54%|█████▎    | 161/300 [01:23<01:13,  1.88it/s]Epoch 11:  54%|█████▍    | 162/300 [01:24<01:14,  1.84it/s]Epoch 11:  54%|█████▍    | 163/300 [01:25<01:11,  1.91it/s]Epoch 11:  55%|█████▍    | 164/300 [01:25<01:12,  1.87it/s]Epoch 11:  55%|█████▌    | 165/300 [01:26<01:10,  1.93it/s]Epoch 11:  55%|█████▌    | 166/300 [01:26<01:11,  1.87it/s]Epoch 11:  56%|█████▌    | 167/300 [01:27<01:12,  1.84it/s]Epoch 11:  56%|█████▌    | 168/300 [01:27<01:09,  1.91it/s]Epoch 11:  56%|█████▋    | 169/300 [01:28<01:09,  1.87it/s]Epoch 11:  57%|█████▋    | 170/300 [01:28<01:10,  1.83it/s]Epoch 11:  57%|█████▋    | 171/300 [01:29<01:08,  1.89it/s]Epoch 11:  57%|█████▋    | 172/300 [01:29<01:05,  1.95it/s]Epoch 11:  58%|█████▊    | 173/300 [01:30<01:03,  2.00it/s]Epoch 11:  58%|█████▊    | 174/300 [01:30<01:02,  2.03it/s]Epoch 11:  58%|█████▊    | 175/300 [01:31<01:07,  1.84it/s]Epoch 11:  59%|█████▊    | 176/300 [01:31<01:04,  1.91it/s]Epoch 11:  59%|█████▉    | 177/300 [01:32<01:02,  1.95it/s]Epoch 11:  59%|█████▉    | 178/300 [01:32<01:01,  1.97it/s]Epoch 11:  60%|█████▉    | 179/300 [01:33<01:03,  1.91it/s]06/19/2022 20:17:51 - INFO - __main__ - global step: 1740; train loss: 8.052382469177246; dev loss: 8.156295776367188
Epoch 11:  60%|██████    | 180/300 [01:33<01:01,  1.96it/s]Epoch 11:  60%|██████    | 181/300 [01:34<00:59,  2.00it/s]Epoch 11:  61%|██████    | 182/300 [01:34<00:59,  1.98it/s]Epoch 11:  61%|██████    | 183/300 [01:35<01:01,  1.90it/s]Epoch 11:  61%|██████▏   | 184/300 [01:35<00:59,  1.95it/s]Epoch 11:  62%|██████▏   | 185/300 [01:36<00:57,  1.99it/s]Epoch 11:  62%|██████▏   | 186/300 [01:36<00:58,  1.96it/s]Epoch 11:  62%|██████▏   | 187/300 [01:37<01:02,  1.81it/s]Epoch 11:  63%|██████▎   | 188/300 [01:38<01:00,  1.85it/s]Epoch 11:  63%|██████▎   | 189/300 [01:38<00:58,  1.90it/s]Epoch 11:  63%|██████▎   | 190/300 [01:39<00:56,  1.94it/s]Epoch 11:  64%|██████▎   | 191/300 [01:39<00:57,  1.88it/s]Epoch 11:  64%|██████▍   | 192/300 [01:40<00:55,  1.93it/s]Epoch 11:  64%|██████▍   | 193/300 [01:40<00:56,  1.88it/s]Epoch 11:  65%|██████▍   | 194/300 [01:41<00:54,  1.94it/s]Epoch 11:  65%|██████▌   | 195/300 [01:41<00:55,  1.89it/s]Epoch 11:  65%|██████▌   | 196/300 [01:42<00:56,  1.85it/s]Epoch 11:  66%|██████▌   | 197/300 [01:42<00:54,  1.90it/s]Epoch 11:  66%|██████▌   | 198/300 [01:43<00:52,  1.95it/s]Epoch 11:  66%|██████▋   | 199/300 [01:43<00:53,  1.90it/s]06/19/2022 20:18:02 - INFO - __main__ - global step: 1750; train loss: 7.558546543121338; dev loss: 7.299900054931641
Epoch 11:  67%|██████▋   | 200/300 [01:44<00:55,  1.80it/s]Epoch 11:  67%|██████▋   | 201/300 [01:44<00:52,  1.88it/s]Epoch 11:  67%|██████▋   | 202/300 [01:45<00:51,  1.90it/s]Epoch 11:  68%|██████▊   | 203/300 [01:45<00:49,  1.95it/s]Epoch 11:  68%|██████▊   | 204/300 [01:46<00:53,  1.80it/s]Epoch 11:  68%|██████▊   | 205/300 [01:47<00:50,  1.89it/s]Epoch 11:  69%|██████▊   | 206/300 [01:47<00:48,  1.94it/s]Epoch 11:  69%|██████▉   | 207/300 [01:48<00:46,  1.99it/s]Epoch 11:  69%|██████▉   | 208/300 [01:48<00:48,  1.89it/s]Epoch 11:  70%|██████▉   | 209/300 [01:49<00:46,  1.95it/s]Epoch 11:  70%|███████   | 210/300 [01:49<00:47,  1.89it/s]Epoch 11:  70%|███████   | 211/300 [01:50<00:46,  1.91it/s]Epoch 11:  71%|███████   | 212/300 [01:50<00:47,  1.86it/s]Epoch 11:  71%|███████   | 213/300 [01:51<00:45,  1.92it/s]Epoch 11:  71%|███████▏  | 214/300 [01:51<00:44,  1.92it/s]Epoch 11:  72%|███████▏  | 215/300 [01:52<00:43,  1.97it/s]Epoch 11:  72%|███████▏  | 216/300 [01:52<00:44,  1.90it/s]Epoch 11:  72%|███████▏  | 217/300 [01:53<00:43,  1.89it/s]Epoch 11:  73%|███████▎  | 218/300 [01:53<00:42,  1.94it/s]Epoch 11:  73%|███████▎  | 219/300 [01:54<00:40,  1.98it/s]06/19/2022 20:18:12 - INFO - __main__ - global step: 1760; train loss: 7.838059902191162; dev loss: 7.564925193786621
Epoch 11:  73%|███████▎  | 220/300 [01:54<00:42,  1.86it/s]Epoch 11:  74%|███████▎  | 221/300 [01:55<00:42,  1.84it/s]Epoch 11:  74%|███████▍  | 222/300 [01:55<00:41,  1.89it/s]Epoch 11:  74%|███████▍  | 223/300 [01:56<00:39,  1.94it/s]Epoch 11:  75%|███████▍  | 224/300 [01:57<00:40,  1.87it/s]Epoch 11:  75%|███████▌  | 225/300 [01:57<00:38,  1.93it/s]Epoch 11:  75%|███████▌  | 226/300 [01:57<00:37,  1.97it/s]Epoch 11:  76%|███████▌  | 227/300 [01:58<00:36,  2.01it/s]Epoch 11:  76%|███████▌  | 228/300 [01:59<00:37,  1.93it/s]Epoch 11:  76%|███████▋  | 229/300 [01:59<00:37,  1.87it/s]Epoch 11:  77%|███████▋  | 230/300 [02:00<00:36,  1.93it/s]Epoch 11:  77%|███████▋  | 231/300 [02:00<00:36,  1.87it/s]Epoch 11:  77%|███████▋  | 232/300 [02:01<00:36,  1.85it/s]Epoch 11:  78%|███████▊  | 233/300 [02:01<00:36,  1.82it/s]Epoch 11:  78%|███████▊  | 234/300 [02:02<00:34,  1.90it/s]Epoch 11:  78%|███████▊  | 235/300 [02:02<00:33,  1.95it/s]Epoch 11:  79%|███████▊  | 236/300 [02:03<00:32,  2.00it/s]Epoch 11:  79%|███████▉  | 237/300 [02:03<00:32,  1.92it/s]Epoch 11:  79%|███████▉  | 238/300 [02:04<00:31,  1.97it/s]Epoch 11:  80%|███████▉  | 239/300 [02:04<00:30,  2.01it/s]06/19/2022 20:18:22 - INFO - __main__ - global step: 1770; train loss: 8.13487720489502; dev loss: 7.700295925140381
Epoch 11:  80%|████████  | 240/300 [02:05<00:29,  2.04it/s]Epoch 11:  80%|████████  | 241/300 [02:05<00:30,  1.94it/s]Epoch 11:  81%|████████  | 242/300 [02:06<00:29,  1.98it/s]Epoch 11:  81%|████████  | 243/300 [02:06<00:28,  2.01it/s]Epoch 11:  81%|████████▏ | 244/300 [02:07<00:27,  2.03it/s]Epoch 11:  82%|████████▏ | 245/300 [02:07<00:28,  1.91it/s]Epoch 11:  82%|████████▏ | 246/300 [02:08<00:27,  1.97it/s]Epoch 11:  82%|████████▏ | 247/300 [02:08<00:27,  1.94it/s]Epoch 11:  83%|████████▎ | 248/300 [02:09<00:26,  1.97it/s]Epoch 11:  83%|████████▎ | 249/300 [02:09<00:27,  1.88it/s]Epoch 11:  83%|████████▎ | 250/300 [02:10<00:27,  1.85it/s]Epoch 11:  84%|████████▎ | 251/300 [02:10<00:25,  1.89it/s]Epoch 11:  84%|████████▍ | 252/300 [02:11<00:24,  1.94it/s]Epoch 11:  84%|████████▍ | 253/300 [02:11<00:24,  1.89it/s]Epoch 11:  85%|████████▍ | 254/300 [02:12<00:26,  1.77it/s]Epoch 11:  85%|████████▌ | 255/300 [02:13<00:24,  1.85it/s]Epoch 11:  85%|████████▌ | 256/300 [02:13<00:23,  1.91it/s]Epoch 11:  86%|████████▌ | 257/300 [02:14<00:21,  1.96it/s]Epoch 11:  86%|████████▌ | 258/300 [02:14<00:23,  1.81it/s]Epoch 11:  86%|████████▋ | 259/300 [02:15<00:21,  1.88it/s]06/19/2022 20:18:33 - INFO - __main__ - global step: 1780; train loss: 7.757633209228516; dev loss: 7.764575958251953
Epoch 11:  87%|████████▋ | 260/300 [02:15<00:20,  1.94it/s]Epoch 11:  87%|████████▋ | 261/300 [02:16<00:19,  1.98it/s]Epoch 11:  87%|████████▋ | 262/300 [02:16<00:20,  1.82it/s]Epoch 11:  88%|████████▊ | 263/300 [02:17<00:19,  1.89it/s]Epoch 11:  88%|████████▊ | 264/300 [02:17<00:18,  1.93it/s]Epoch 11:  88%|████████▊ | 265/300 [02:18<00:17,  1.97it/s]Epoch 11:  89%|████████▊ | 266/300 [02:18<00:18,  1.86it/s]Epoch 11:  89%|████████▉ | 267/300 [02:19<00:17,  1.92it/s]Epoch 11:  89%|████████▉ | 268/300 [02:19<00:16,  1.91it/s]Epoch 11:  90%|████████▉ | 269/300 [02:20<00:15,  1.94it/s]Epoch 11:  90%|█████████ | 270/300 [02:21<00:16,  1.78it/s]Epoch 11:  90%|█████████ | 271/300 [02:21<00:15,  1.85it/s]Epoch 11:  91%|█████████ | 272/300 [02:22<00:14,  1.87it/s]Epoch 11:  91%|█████████ | 273/300 [02:22<00:14,  1.88it/s]Epoch 11:  91%|█████████▏| 274/300 [02:23<00:14,  1.84it/s]Epoch 11:  92%|█████████▏| 275/300 [02:23<00:13,  1.88it/s]Epoch 11:  92%|█████████▏| 276/300 [02:24<00:12,  1.94it/s]Epoch 11:  92%|█████████▏| 277/300 [02:24<00:11,  1.98it/s]Epoch 11:  93%|█████████▎| 278/300 [02:25<00:11,  1.90it/s]Epoch 11:  93%|█████████▎| 279/300 [02:25<00:10,  1.93it/s]06/19/2022 20:18:43 - INFO - __main__ - global step: 1790; train loss: 8.107217788696289; dev loss: 8.103450775146484
Epoch 11:  93%|█████████▎| 280/300 [02:26<00:10,  1.98it/s]Epoch 11:  94%|█████████▎| 281/300 [02:26<00:09,  2.00it/s]Epoch 11:  94%|█████████▍| 282/300 [02:27<00:09,  1.93it/s]Epoch 11:  94%|█████████▍| 283/300 [02:27<00:09,  1.83it/s]Epoch 11:  95%|█████████▍| 284/300 [02:28<00:08,  1.80it/s]Epoch 11:  95%|█████████▌| 285/300 [02:28<00:08,  1.86it/s]Epoch 11:  95%|█████████▌| 286/300 [02:29<00:07,  1.87it/s]Epoch 11:  96%|█████████▌| 287/300 [02:30<00:07,  1.82it/s]Epoch 11:  96%|█████████▌| 288/300 [02:30<00:06,  1.89it/s]Epoch 11:  96%|█████████▋| 289/300 [02:31<00:05,  1.93it/s]Epoch 11:  97%|█████████▋| 290/300 [02:31<00:05,  1.87it/s]Epoch 11:  97%|█████████▋| 291/300 [02:32<00:04,  1.84it/s]Epoch 11:  97%|█████████▋| 292/300 [02:32<00:04,  1.92it/s]Epoch 11:  98%|█████████▊| 293/300 [02:33<00:03,  1.97it/s]Epoch 11:  98%|█████████▊| 294/300 [02:33<00:03,  1.97it/s]Epoch 11:  98%|█████████▊| 295/300 [02:34<00:02,  1.90it/s]Epoch 11:  99%|█████████▊| 296/300 [02:34<00:02,  1.87it/s]Epoch 11:  99%|█████████▉| 297/300 [02:35<00:01,  1.94it/s]Epoch 11:  99%|█████████▉| 298/300 [02:35<00:01,  1.98it/s]Epoch 11: 100%|█████████▉| 299/300 [02:36<00:00,  1.86it/s]06/19/2022 20:18:54 - INFO - __main__ - global step: 1800; train loss: 6.8716888427734375; dev loss: 7.054859161376953
Epoch 11: 100%|██████████| 300/300 [02:36<00:00,  1.92it/s]Epoch 11: 100%|██████████| 300/300 [02:36<00:00,  1.91it/s]
Epoch 12:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 12:   0%|          | 1/300 [00:00<02:24,  2.07it/s]Epoch 12:   1%|          | 2/300 [00:00<02:23,  2.07it/s]Epoch 12:   1%|          | 3/300 [00:01<02:35,  1.92it/s]Epoch 12:   1%|▏         | 4/300 [00:02<02:38,  1.87it/s]Epoch 12:   2%|▏         | 5/300 [00:02<02:32,  1.94it/s]Epoch 12:   2%|▏         | 6/300 [00:03<02:28,  1.99it/s]Epoch 12:   2%|▏         | 7/300 [00:03<02:25,  2.02it/s]Epoch 12:   3%|▎         | 8/300 [00:04<02:35,  1.88it/s]Epoch 12:   3%|▎         | 9/300 [00:04<02:30,  1.94it/s]Epoch 12:   3%|▎         | 10/300 [00:05<02:26,  1.98it/s]Epoch 12:   4%|▎         | 11/300 [00:05<02:23,  2.02it/s]Epoch 12:   4%|▍         | 12/300 [00:06<02:29,  1.92it/s]Epoch 12:   4%|▍         | 13/300 [00:06<02:29,  1.92it/s]Epoch 12:   5%|▍         | 14/300 [00:07<02:24,  1.97it/s]Epoch 12:   5%|▌         | 15/300 [00:07<02:29,  1.90it/s]Epoch 12:   5%|▌         | 16/300 [00:08<02:36,  1.81it/s]Epoch 12:   6%|▌         | 17/300 [00:08<02:29,  1.89it/s]Epoch 12:   6%|▌         | 18/300 [00:09<02:24,  1.95it/s]Epoch 12:   6%|▋         | 19/300 [00:09<02:27,  1.90it/s]06/19/2022 20:19:04 - INFO - __main__ - global step: 1810; train loss: 7.732243537902832; dev loss: 7.539834022521973
Epoch 12:   7%|▋         | 20/300 [00:10<02:30,  1.85it/s]Epoch 12:   7%|▋         | 21/300 [00:10<02:25,  1.92it/s]Epoch 12:   7%|▋         | 22/300 [00:11<02:27,  1.88it/s]Epoch 12:   8%|▊         | 23/300 [00:11<02:22,  1.94it/s]Epoch 12:   8%|▊         | 24/300 [00:12<02:33,  1.80it/s]Epoch 12:   8%|▊         | 25/300 [00:13<02:33,  1.79it/s]Epoch 12:   9%|▊         | 26/300 [00:13<02:26,  1.87it/s]Epoch 12:   9%|▉         | 27/300 [00:14<02:21,  1.93it/s]Epoch 12:   9%|▉         | 28/300 [00:14<02:25,  1.87it/s]Epoch 12:  10%|▉         | 29/300 [00:15<02:20,  1.93it/s]Epoch 12:  10%|█         | 30/300 [00:15<02:17,  1.97it/s]Epoch 12:  10%|█         | 31/300 [00:16<02:14,  2.00it/s]Epoch 12:  11%|█         | 32/300 [00:16<02:20,  1.91it/s]Epoch 12:  11%|█         | 33/300 [00:17<02:15,  1.97it/s]Epoch 12:  11%|█▏        | 34/300 [00:17<02:12,  2.01it/s]Epoch 12:  12%|█▏        | 35/300 [00:18<02:10,  2.03it/s]Epoch 12:  12%|█▏        | 36/300 [00:18<02:08,  2.06it/s]Epoch 12:  12%|█▏        | 37/300 [00:19<02:14,  1.96it/s]Epoch 12:  13%|█▎        | 38/300 [00:19<02:12,  1.98it/s]Epoch 12:  13%|█▎        | 39/300 [00:20<02:09,  2.02it/s]06/19/2022 20:19:15 - INFO - __main__ - global step: 1820; train loss: 7.728979587554932; dev loss: 7.91912841796875
Epoch 12:  13%|█▎        | 40/300 [00:20<02:13,  1.94it/s]Epoch 12:  14%|█▎        | 41/300 [00:21<02:17,  1.88it/s]Epoch 12:  14%|█▍        | 42/300 [00:21<02:19,  1.85it/s]Epoch 12:  14%|█▍        | 43/300 [00:22<02:16,  1.88it/s]Epoch 12:  15%|█▍        | 44/300 [00:22<02:14,  1.90it/s]Epoch 12:  15%|█▌        | 45/300 [00:23<02:23,  1.78it/s]Epoch 12:  15%|█▌        | 46/300 [00:23<02:16,  1.86it/s]Epoch 12:  16%|█▌        | 47/300 [00:24<02:10,  1.93it/s]Epoch 12:  16%|█▌        | 48/300 [00:24<02:06,  1.99it/s]Epoch 12:  16%|█▋        | 49/300 [00:25<02:11,  1.91it/s]Epoch 12:  17%|█▋        | 50/300 [00:25<02:07,  1.96it/s]Epoch 12:  17%|█▋        | 51/300 [00:26<02:03,  2.01it/s]Epoch 12:  17%|█▋        | 52/300 [00:26<02:07,  1.94it/s]Epoch 12:  18%|█▊        | 53/300 [00:27<02:11,  1.88it/s]Epoch 12:  18%|█▊        | 54/300 [00:28<02:06,  1.94it/s]Epoch 12:  18%|█▊        | 55/300 [00:28<02:03,  1.99it/s]Epoch 12:  19%|█▊        | 56/300 [00:28<02:01,  2.01it/s]Epoch 12:  19%|█▉        | 57/300 [00:29<02:05,  1.94it/s]Epoch 12:  19%|█▉        | 58/300 [00:30<02:02,  1.98it/s]Epoch 12:  20%|█▉        | 59/300 [00:30<02:05,  1.92it/s]06/19/2022 20:19:25 - INFO - __main__ - global step: 1830; train loss: 7.644331455230713; dev loss: 7.816178798675537
Epoch 12:  20%|██        | 60/300 [00:31<02:02,  1.95it/s]Epoch 12:  20%|██        | 61/300 [00:31<02:03,  1.94it/s]Epoch 12:  21%|██        | 62/300 [00:32<02:06,  1.87it/s]Epoch 12:  21%|██        | 63/300 [00:32<02:02,  1.93it/s]Epoch 12:  21%|██▏       | 64/300 [00:33<02:02,  1.93it/s]Epoch 12:  22%|██▏       | 65/300 [00:33<01:59,  1.97it/s]Epoch 12:  22%|██▏       | 66/300 [00:34<02:03,  1.90it/s]Epoch 12:  22%|██▏       | 67/300 [00:34<02:05,  1.86it/s]Epoch 12:  23%|██▎       | 68/300 [00:35<02:00,  1.92it/s]Epoch 12:  23%|██▎       | 69/300 [00:35<01:57,  1.96it/s]Epoch 12:  23%|██▎       | 70/300 [00:36<02:01,  1.89it/s]Epoch 12:  24%|██▎       | 71/300 [00:36<01:57,  1.95it/s]Epoch 12:  24%|██▍       | 72/300 [00:37<01:54,  1.99it/s]Epoch 12:  24%|██▍       | 73/300 [00:37<01:52,  2.03it/s]Epoch 12:  25%|██▍       | 74/300 [00:38<01:56,  1.94it/s]Epoch 12:  25%|██▌       | 75/300 [00:38<01:59,  1.88it/s]Epoch 12:  25%|██▌       | 76/300 [00:39<01:55,  1.94it/s]Epoch 12:  26%|██▌       | 77/300 [00:39<01:52,  1.98it/s]Epoch 12:  26%|██▌       | 78/300 [00:40<01:55,  1.92it/s]Epoch 12:  26%|██▋       | 79/300 [00:40<01:52,  1.97it/s]06/19/2022 20:19:35 - INFO - __main__ - global step: 1840; train loss: 7.2549872398376465; dev loss: 7.278495788574219
Epoch 12:  27%|██▋       | 80/300 [00:41<01:55,  1.91it/s]Epoch 12:  27%|██▋       | 81/300 [00:41<01:56,  1.88it/s]Epoch 12:  27%|██▋       | 82/300 [00:42<02:01,  1.80it/s]Epoch 12:  28%|██▊       | 83/300 [00:43<01:58,  1.83it/s]Epoch 12:  28%|██▊       | 84/300 [00:43<01:55,  1.86it/s]Epoch 12:  28%|██▊       | 85/300 [00:44<01:51,  1.93it/s]Epoch 12:  29%|██▊       | 86/300 [00:44<01:54,  1.87it/s]Epoch 12:  29%|██▉       | 87/300 [00:45<01:50,  1.93it/s]Epoch 12:  29%|██▉       | 88/300 [00:45<01:52,  1.88it/s]Epoch 12:  30%|██▉       | 89/300 [00:46<01:49,  1.93it/s]Epoch 12:  30%|███       | 90/300 [00:46<01:48,  1.94it/s]Epoch 12:  30%|███       | 91/300 [00:47<01:56,  1.80it/s]Epoch 12:  31%|███       | 92/300 [00:47<01:50,  1.87it/s]Epoch 12:  31%|███       | 93/300 [00:48<01:46,  1.94it/s]Epoch 12:  31%|███▏      | 94/300 [00:48<01:43,  1.98it/s]Epoch 12:  32%|███▏      | 95/300 [00:49<01:46,  1.92it/s]Epoch 12:  32%|███▏      | 96/300 [00:49<01:49,  1.87it/s]Epoch 12:  32%|███▏      | 97/300 [00:50<01:44,  1.94it/s]Epoch 12:  33%|███▎      | 98/300 [00:50<01:41,  1.98it/s]Epoch 12:  33%|███▎      | 99/300 [00:51<01:44,  1.92it/s]06/19/2022 20:19:46 - INFO - __main__ - global step: 1850; train loss: 7.853537559509277; dev loss: 7.891735076904297
Epoch 12:  33%|███▎      | 100/300 [00:51<01:44,  1.92it/s]Epoch 12:  34%|███▎      | 101/300 [00:52<01:41,  1.97it/s]Epoch 12:  34%|███▍      | 102/300 [00:53<01:43,  1.91it/s]Epoch 12:  34%|███▍      | 103/300 [00:53<01:48,  1.82it/s]Epoch 12:  35%|███▍      | 104/300 [00:54<01:44,  1.88it/s]Epoch 12:  35%|███▌      | 105/300 [00:54<01:41,  1.93it/s]Epoch 12:  35%|███▌      | 106/300 [00:55<01:38,  1.97it/s]Epoch 12:  36%|███▌      | 107/300 [00:55<01:46,  1.81it/s]Epoch 12:  36%|███▌      | 108/300 [00:56<01:46,  1.80it/s]Epoch 12:  36%|███▋      | 109/300 [00:56<01:41,  1.88it/s]Epoch 12:  37%|███▋      | 110/300 [00:57<01:38,  1.93it/s]Epoch 12:  37%|███▋      | 111/300 [00:57<01:40,  1.87it/s]Epoch 12:  37%|███▋      | 112/300 [00:58<01:36,  1.94it/s]Epoch 12:  38%|███▊      | 113/300 [00:58<01:33,  1.99it/s]Epoch 12:  38%|███▊      | 114/300 [00:59<01:31,  2.02it/s]Epoch 12:  38%|███▊      | 115/300 [00:59<01:30,  2.05it/s]Epoch 12:  39%|███▊      | 116/300 [01:00<01:39,  1.84it/s]Epoch 12:  39%|███▉      | 117/300 [01:00<01:36,  1.90it/s]Epoch 12:  39%|███▉      | 118/300 [01:01<01:33,  1.96it/s]Epoch 12:  40%|███▉      | 119/300 [01:01<01:30,  2.00it/s]06/19/2022 20:19:56 - INFO - __main__ - global step: 1860; train loss: 8.137629508972168; dev loss: 7.83533239364624
Epoch 12:  40%|████      | 120/300 [01:02<01:33,  1.92it/s]Epoch 12:  40%|████      | 121/300 [01:02<01:30,  1.97it/s]Epoch 12:  41%|████      | 122/300 [01:03<01:28,  2.01it/s]Epoch 12:  41%|████      | 123/300 [01:03<01:26,  2.03it/s]Epoch 12:  41%|████▏     | 124/300 [01:04<01:30,  1.94it/s]Epoch 12:  42%|████▏     | 125/300 [01:04<01:27,  1.99it/s]Epoch 12:  42%|████▏     | 126/300 [01:05<01:25,  2.02it/s]Epoch 12:  42%|████▏     | 127/300 [01:05<01:24,  2.04it/s]Epoch 12:  43%|████▎     | 128/300 [01:06<01:28,  1.94it/s]Epoch 12:  43%|████▎     | 129/300 [01:06<01:30,  1.89it/s]Epoch 12:  43%|████▎     | 130/300 [01:07<01:26,  1.96it/s]Epoch 12:  44%|████▎     | 131/300 [01:07<01:26,  1.96it/s]Epoch 12:  44%|████▍     | 132/300 [01:08<01:32,  1.81it/s]Epoch 12:  44%|████▍     | 133/300 [01:09<01:28,  1.88it/s]Epoch 12:  45%|████▍     | 134/300 [01:09<01:25,  1.93it/s]Epoch 12:  45%|████▌     | 135/300 [01:10<01:23,  1.98it/s]Epoch 12:  45%|████▌     | 136/300 [01:10<01:27,  1.88it/s]Epoch 12:  46%|████▌     | 137/300 [01:11<01:25,  1.91it/s]Epoch 12:  46%|████▌     | 138/300 [01:11<01:22,  1.96it/s]Epoch 12:  46%|████▋     | 139/300 [01:12<01:22,  1.95it/s]06/19/2022 20:20:07 - INFO - __main__ - global step: 1870; train loss: 7.8398003578186035; dev loss: 7.495461463928223
Epoch 12:  47%|████▋     | 140/300 [01:12<01:24,  1.89it/s]Epoch 12:  47%|████▋     | 141/300 [01:13<01:21,  1.95it/s]Epoch 12:  47%|████▋     | 142/300 [01:13<01:19,  1.98it/s]Epoch 12:  48%|████▊     | 143/300 [01:14<01:18,  2.01it/s]Epoch 12:  48%|████▊     | 144/300 [01:14<01:17,  2.01it/s]Epoch 12:  48%|████▊     | 145/300 [01:15<01:20,  1.92it/s]Epoch 12:  49%|████▊     | 146/300 [01:15<01:18,  1.97it/s]Epoch 12:  49%|████▉     | 147/300 [01:16<01:18,  1.95it/s]Epoch 12:  49%|████▉     | 148/300 [01:16<01:19,  1.90it/s]Epoch 12:  50%|████▉     | 149/300 [01:17<01:21,  1.86it/s]Epoch 12:  50%|█████     | 150/300 [01:17<01:18,  1.91it/s]Epoch 12:  50%|█████     | 151/300 [01:18<01:16,  1.95it/s]Epoch 12:  51%|█████     | 152/300 [01:18<01:18,  1.89it/s]Epoch 12:  51%|█████     | 153/300 [01:19<01:19,  1.85it/s]Epoch 12:  51%|█████▏    | 154/300 [01:20<01:20,  1.82it/s]Epoch 12:  52%|█████▏    | 155/300 [01:20<01:16,  1.89it/s]Epoch 12:  52%|█████▏    | 156/300 [01:21<01:16,  1.89it/s]Epoch 12:  52%|█████▏    | 157/300 [01:21<01:17,  1.85it/s]Epoch 12:  53%|█████▎    | 158/300 [01:22<01:14,  1.91it/s]Epoch 12:  53%|█████▎    | 159/300 [01:22<01:11,  1.97it/s]06/19/2022 20:20:17 - INFO - __main__ - global step: 1880; train loss: 8.097132682800293; dev loss: 8.268559455871582
Epoch 12:  53%|█████▎    | 160/300 [01:23<01:13,  1.91it/s]Epoch 12:  54%|█████▎    | 161/300 [01:23<01:14,  1.85it/s]Epoch 12:  54%|█████▍    | 162/300 [01:24<01:11,  1.92it/s]Epoch 12:  54%|█████▍    | 163/300 [01:24<01:09,  1.97it/s]Epoch 12:  55%|█████▍    | 164/300 [01:25<01:09,  1.97it/s]Epoch 12:  55%|█████▌    | 165/300 [01:25<01:11,  1.89it/s]Epoch 12:  55%|█████▌    | 166/300 [01:26<01:09,  1.93it/s]Epoch 12:  56%|█████▌    | 167/300 [01:26<01:07,  1.97it/s]Epoch 12:  56%|█████▌    | 168/300 [01:27<01:07,  1.97it/s]Epoch 12:  56%|█████▋    | 169/300 [01:27<01:08,  1.91it/s]Epoch 12:  57%|█████▋    | 170/300 [01:28<01:09,  1.86it/s]Epoch 12:  57%|█████▋    | 171/300 [01:28<01:10,  1.84it/s]Epoch 12:  57%|█████▋    | 172/300 [01:29<01:06,  1.91it/s]Epoch 12:  58%|█████▊    | 173/300 [01:29<01:05,  1.95it/s]Epoch 12:  58%|█████▊    | 174/300 [01:30<01:06,  1.89it/s]Epoch 12:  58%|█████▊    | 175/300 [01:30<01:04,  1.95it/s]Epoch 12:  59%|█████▊    | 176/300 [01:31<01:03,  1.94it/s]Epoch 12:  59%|█████▉    | 177/300 [01:31<01:03,  1.94it/s]Epoch 12:  59%|█████▉    | 178/300 [01:32<01:05,  1.87it/s]Epoch 12:  60%|█████▉    | 179/300 [01:33<01:02,  1.93it/s]06/19/2022 20:20:27 - INFO - __main__ - global step: 1890; train loss: 7.6435346603393555; dev loss: 7.913015842437744
Epoch 12:  60%|██████    | 180/300 [01:33<01:00,  1.97it/s]Epoch 12:  60%|██████    | 181/300 [01:33<00:59,  2.01it/s]Epoch 12:  61%|██████    | 182/300 [01:34<01:01,  1.92it/s]Epoch 12:  61%|██████    | 183/300 [01:35<00:59,  1.97it/s]Epoch 12:  61%|██████▏   | 184/300 [01:35<00:59,  1.96it/s]Epoch 12:  62%|██████▏   | 185/300 [01:36<01:00,  1.90it/s]Epoch 12:  62%|██████▏   | 186/300 [01:36<01:04,  1.77it/s]Epoch 12:  62%|██████▏   | 187/300 [01:37<01:03,  1.78it/s]Epoch 12:  63%|██████▎   | 188/300 [01:37<00:59,  1.87it/s]Epoch 12:  63%|██████▎   | 189/300 [01:38<00:57,  1.94it/s]Epoch 12:  63%|██████▎   | 190/300 [01:38<00:58,  1.88it/s]Epoch 12:  64%|██████▎   | 191/300 [01:39<00:58,  1.86it/s]Epoch 12:  64%|██████▍   | 192/300 [01:39<00:55,  1.93it/s]Epoch 12:  64%|██████▍   | 193/300 [01:40<00:54,  1.98it/s]Epoch 12:  65%|██████▍   | 194/300 [01:40<00:55,  1.91it/s]Epoch 12:  65%|██████▌   | 195/300 [01:41<00:55,  1.88it/s]Epoch 12:  65%|██████▌   | 196/300 [01:41<00:53,  1.95it/s]Epoch 12:  66%|██████▌   | 197/300 [01:42<00:51,  2.00it/s]Epoch 12:  66%|██████▌   | 198/300 [01:42<00:50,  2.03it/s]Epoch 12:  66%|██████▋   | 199/300 [01:43<00:54,  1.85it/s]06/19/2022 20:20:38 - INFO - __main__ - global step: 1900; train loss: 7.9460625648498535; dev loss: 7.685040473937988
Epoch 12:  67%|██████▋   | 200/300 [01:43<00:52,  1.91it/s]Epoch 12:  67%|██████▋   | 201/300 [01:44<00:51,  1.92it/s]Epoch 12:  67%|██████▋   | 202/300 [01:44<00:49,  1.97it/s]Epoch 12:  68%|██████▊   | 203/300 [01:45<00:50,  1.91it/s]Epoch 12:  68%|██████▊   | 204/300 [01:46<00:49,  1.95it/s]Epoch 12:  68%|██████▊   | 205/300 [01:46<00:47,  1.99it/s]Epoch 12:  69%|██████▊   | 206/300 [01:46<00:46,  2.03it/s]Epoch 12:  69%|██████▉   | 207/300 [01:47<00:48,  1.90it/s]Epoch 12:  69%|██████▉   | 208/300 [01:48<00:49,  1.86it/s]Epoch 12:  70%|██████▉   | 209/300 [01:48<00:47,  1.92it/s]Epoch 12:  70%|███████   | 210/300 [01:49<00:45,  1.97it/s]Epoch 12:  70%|███████   | 211/300 [01:49<00:48,  1.82it/s]Epoch 12:  71%|███████   | 212/300 [01:50<00:47,  1.86it/s]Epoch 12:  71%|███████   | 213/300 [01:50<00:45,  1.92it/s]Epoch 12:  71%|███████▏  | 214/300 [01:51<00:43,  1.98it/s]Epoch 12:  72%|███████▏  | 215/300 [01:51<00:44,  1.89it/s]Epoch 12:  72%|███████▏  | 216/300 [01:52<00:43,  1.95it/s]Epoch 12:  72%|███████▏  | 217/300 [01:52<00:41,  2.00it/s]Epoch 12:  73%|███████▎  | 218/300 [01:53<00:40,  2.03it/s]Epoch 12:  73%|███████▎  | 219/300 [01:53<00:41,  1.94it/s]06/19/2022 20:20:48 - INFO - __main__ - global step: 1910; train loss: 7.732187747955322; dev loss: 8.124585151672363
Epoch 12:  73%|███████▎  | 220/300 [01:54<00:40,  1.98it/s]Epoch 12:  74%|███████▎  | 221/300 [01:54<00:39,  2.02it/s]Epoch 12:  74%|███████▍  | 222/300 [01:55<00:38,  2.04it/s]Epoch 12:  74%|███████▍  | 223/300 [01:55<00:37,  2.06it/s]Epoch 12:  75%|███████▍  | 224/300 [01:56<00:38,  1.95it/s]Epoch 12:  75%|███████▌  | 225/300 [01:56<00:39,  1.90it/s]Epoch 12:  75%|███████▌  | 226/300 [01:57<00:37,  1.96it/s]Epoch 12:  76%|███████▌  | 227/300 [01:57<00:36,  2.00it/s]Epoch 12:  76%|███████▌  | 228/300 [01:58<00:37,  1.92it/s]Epoch 12:  76%|███████▋  | 229/300 [01:58<00:36,  1.97it/s]Epoch 12:  77%|███████▋  | 230/300 [01:59<00:35,  1.96it/s]Epoch 12:  77%|███████▋  | 231/300 [01:59<00:34,  2.00it/s]Epoch 12:  77%|███████▋  | 232/300 [02:00<00:37,  1.83it/s]Epoch 12:  78%|███████▊  | 233/300 [02:00<00:35,  1.90it/s]Epoch 12:  78%|███████▊  | 234/300 [02:01<00:33,  1.96it/s]Epoch 12:  78%|███████▊  | 235/300 [02:01<00:32,  2.01it/s]Epoch 12:  79%|███████▊  | 236/300 [02:02<00:33,  1.94it/s]Epoch 12:  79%|███████▉  | 237/300 [02:02<00:32,  1.94it/s]Epoch 12:  79%|███████▉  | 238/300 [02:03<00:31,  1.99it/s]Epoch 12:  80%|███████▉  | 239/300 [02:03<00:30,  2.02it/s]06/19/2022 20:20:58 - INFO - __main__ - global step: 1920; train loss: 7.565741539001465; dev loss: 7.422184944152832
Epoch 12:  80%|████████  | 240/300 [02:04<00:31,  1.94it/s]Epoch 12:  80%|████████  | 241/300 [02:05<00:30,  1.93it/s]Epoch 12:  81%|████████  | 242/300 [02:05<00:29,  1.98it/s]Epoch 12:  81%|████████  | 243/300 [02:05<00:28,  1.99it/s]Epoch 12:  81%|████████▏ | 244/300 [02:06<00:29,  1.92it/s]Epoch 12:  82%|████████▏ | 245/300 [02:07<00:28,  1.92it/s]Epoch 12:  82%|████████▏ | 246/300 [02:07<00:27,  1.96it/s]Epoch 12:  82%|████████▏ | 247/300 [02:08<00:27,  1.90it/s]Epoch 12:  83%|████████▎ | 248/300 [02:08<00:28,  1.82it/s]Epoch 12:  83%|████████▎ | 249/300 [02:09<00:28,  1.81it/s]Epoch 12:  83%|████████▎ | 250/300 [02:09<00:26,  1.88it/s]Epoch 12:  84%|████████▎ | 251/300 [02:10<00:26,  1.85it/s]Epoch 12:  84%|████████▍ | 252/300 [02:10<00:25,  1.91it/s]Epoch 12:  84%|████████▍ | 253/300 [02:11<00:26,  1.78it/s]Epoch 12:  85%|████████▍ | 254/300 [02:11<00:24,  1.86it/s]Epoch 12:  85%|████████▌ | 255/300 [02:12<00:23,  1.93it/s]Epoch 12:  85%|████████▌ | 256/300 [02:12<00:22,  1.96it/s]Epoch 12:  86%|████████▌ | 257/300 [02:13<00:22,  1.90it/s]Epoch 12:  86%|████████▌ | 258/300 [02:13<00:21,  1.95it/s]Epoch 12:  86%|████████▋ | 259/300 [02:14<00:21,  1.95it/s]06/19/2022 20:21:09 - INFO - __main__ - global step: 1930; train loss: 7.6186628341674805; dev loss: 7.579604148864746
Epoch 12:  87%|████████▋ | 260/300 [02:14<00:20,  1.99it/s]Epoch 12:  87%|████████▋ | 261/300 [02:15<00:20,  1.92it/s]Epoch 12:  87%|████████▋ | 262/300 [02:15<00:19,  1.97it/s]Epoch 12:  88%|████████▊ | 263/300 [02:16<00:18,  1.96it/s]Epoch 12:  88%|████████▊ | 264/300 [02:17<00:18,  1.90it/s]Epoch 12:  88%|████████▊ | 265/300 [02:17<00:19,  1.78it/s]Epoch 12:  89%|████████▊ | 266/300 [02:18<00:18,  1.83it/s]Epoch 12:  89%|████████▉ | 267/300 [02:18<00:17,  1.90it/s]Epoch 12:  89%|████████▉ | 268/300 [02:19<00:16,  1.95it/s]Epoch 12:  90%|████████▉ | 269/300 [02:19<00:16,  1.89it/s]Epoch 12:  90%|█████████ | 270/300 [02:20<00:15,  1.96it/s]Epoch 12:  90%|█████████ | 271/300 [02:20<00:14,  1.96it/s]Epoch 12:  91%|█████████ | 272/300 [02:21<00:14,  1.99it/s]Epoch 12:  91%|█████████ | 273/300 [02:21<00:14,  1.90it/s]Epoch 12:  91%|█████████▏| 274/300 [02:22<00:13,  1.95it/s]Epoch 12:  92%|█████████▏| 275/300 [02:22<00:12,  2.00it/s]Epoch 12:  92%|█████████▏| 276/300 [02:23<00:12,  1.99it/s]Epoch 12:  92%|█████████▏| 277/300 [02:23<00:11,  2.01it/s]Epoch 12:  93%|█████████▎| 278/300 [02:24<00:11,  1.93it/s]Epoch 12:  93%|█████████▎| 279/300 [02:24<00:10,  1.98it/s]06/19/2022 20:21:19 - INFO - __main__ - global step: 1940; train loss: 7.946031093597412; dev loss: 8.092198371887207
Epoch 12:  93%|█████████▎| 280/300 [02:25<00:10,  1.99it/s]Epoch 12:  94%|█████████▎| 281/300 [02:25<00:09,  2.02it/s]Epoch 12:  94%|█████████▍| 282/300 [02:26<00:09,  1.85it/s]Epoch 12:  94%|█████████▍| 283/300 [02:26<00:09,  1.84it/s]Epoch 12:  95%|█████████▍| 284/300 [02:27<00:08,  1.90it/s]Epoch 12:  95%|█████████▌| 285/300 [02:27<00:07,  1.95it/s]Epoch 12:  95%|█████████▌| 286/300 [02:28<00:07,  1.89it/s]Epoch 12:  96%|█████████▌| 287/300 [02:28<00:06,  1.92it/s]Epoch 12:  96%|█████████▌| 288/300 [02:29<00:06,  1.97it/s]Epoch 12:  96%|█████████▋| 289/300 [02:29<00:05,  2.00it/s]Epoch 12:  97%|█████████▋| 290/300 [02:30<00:05,  1.92it/s]Epoch 12:  97%|█████████▋| 291/300 [02:31<00:04,  1.93it/s]Epoch 12:  97%|█████████▋| 292/300 [02:31<00:04,  1.99it/s]Epoch 12:  98%|█████████▊| 293/300 [02:31<00:03,  2.02it/s]Epoch 12:  98%|█████████▊| 294/300 [02:32<00:03,  1.93it/s]Epoch 12:  98%|█████████▊| 295/300 [02:33<00:02,  1.89it/s]Epoch 12:  99%|█████████▊| 296/300 [02:33<00:02,  1.85it/s]Epoch 12:  99%|█████████▉| 297/300 [02:34<00:01,  1.83it/s]Epoch 12:  99%|█████████▉| 298/300 [02:34<00:01,  1.80it/s]Epoch 12: 100%|█████████▉| 299/300 [02:35<00:00,  1.88it/s]06/19/2022 20:21:30 - INFO - __main__ - global step: 1950; train loss: 7.583268642425537; dev loss: 7.563668727874756
Epoch 12: 100%|██████████| 300/300 [02:35<00:00,  1.89it/s]Epoch 12: 100%|██████████| 300/300 [02:35<00:00,  1.93it/s]
Epoch 13:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 13:   0%|          | 1/300 [00:00<02:35,  1.93it/s]Epoch 13:   1%|          | 2/300 [00:01<02:58,  1.67it/s]Epoch 13:   1%|          | 3/300 [00:01<02:40,  1.86it/s]Epoch 13:   1%|▏         | 4/300 [00:02<02:33,  1.93it/s]Epoch 13:   2%|▏         | 5/300 [00:02<02:37,  1.87it/s]Epoch 13:   2%|▏         | 6/300 [00:03<02:31,  1.94it/s]Epoch 13:   2%|▏         | 7/300 [00:03<02:38,  1.85it/s]Epoch 13:   3%|▎         | 8/300 [00:04<02:32,  1.91it/s]Epoch 13:   3%|▎         | 9/300 [00:04<02:27,  1.97it/s]Epoch 13:   3%|▎         | 10/300 [00:05<02:24,  2.00it/s]Epoch 13:   4%|▎         | 11/300 [00:05<02:36,  1.84it/s]Epoch 13:   4%|▍         | 12/300 [00:06<02:30,  1.91it/s]Epoch 13:   4%|▍         | 13/300 [00:06<02:26,  1.96it/s]Epoch 13:   5%|▍         | 14/300 [00:07<02:22,  2.00it/s]Epoch 13:   5%|▌         | 15/300 [00:07<02:35,  1.83it/s]Epoch 13:   5%|▌         | 16/300 [00:08<02:32,  1.86it/s]Epoch 13:   6%|▌         | 17/300 [00:08<02:26,  1.93it/s]Epoch 13:   6%|▌         | 18/300 [00:09<02:22,  1.98it/s]Epoch 13:   6%|▋         | 19/300 [00:09<02:28,  1.89it/s]06/19/2022 20:21:40 - INFO - __main__ - global step: 1960; train loss: 7.233646392822266; dev loss: 7.091780185699463
Epoch 13:   7%|▋         | 20/300 [00:10<02:25,  1.92it/s]Epoch 13:   7%|▋         | 21/300 [00:10<02:20,  1.98it/s]Epoch 13:   7%|▋         | 22/300 [00:11<02:25,  1.91it/s]Epoch 13:   8%|▊         | 23/300 [00:12<02:28,  1.86it/s]Epoch 13:   8%|▊         | 24/300 [00:12<02:26,  1.89it/s]Epoch 13:   8%|▊         | 25/300 [00:13<02:21,  1.95it/s]Epoch 13:   9%|▊         | 26/300 [00:13<02:17,  2.00it/s]Epoch 13:   9%|▉         | 27/300 [00:14<02:25,  1.87it/s]Epoch 13:   9%|▉         | 28/300 [00:14<02:21,  1.93it/s]Epoch 13:  10%|▉         | 29/300 [00:15<02:16,  1.98it/s]Epoch 13:  10%|█         | 30/300 [00:15<02:14,  2.01it/s]Epoch 13:  10%|█         | 31/300 [00:16<02:15,  1.99it/s]Epoch 13:  11%|█         | 32/300 [00:16<02:27,  1.82it/s]Epoch 13:  11%|█         | 33/300 [00:17<02:20,  1.90it/s]Epoch 13:  11%|█▏        | 34/300 [00:17<02:19,  1.90it/s]Epoch 13:  12%|█▏        | 35/300 [00:18<02:16,  1.94it/s]Epoch 13:  12%|█▏        | 36/300 [00:18<02:23,  1.84it/s]Epoch 13:  12%|█▏        | 37/300 [00:19<02:18,  1.90it/s]Epoch 13:  13%|█▎        | 38/300 [00:19<02:13,  1.97it/s]Epoch 13:  13%|█▎        | 39/300 [00:20<02:10,  2.01it/s]06/19/2022 20:21:51 - INFO - __main__ - global step: 1970; train loss: 7.814456939697266; dev loss: 8.1908597946167
Epoch 13:  13%|█▎        | 40/300 [00:20<02:14,  1.93it/s]Epoch 13:  14%|█▎        | 41/300 [00:21<02:11,  1.97it/s]Epoch 13:  14%|█▍        | 42/300 [00:21<02:08,  2.01it/s]Epoch 13:  14%|█▍        | 43/300 [00:22<02:12,  1.94it/s]Epoch 13:  15%|█▍        | 44/300 [00:22<02:16,  1.88it/s]Epoch 13:  15%|█▌        | 45/300 [00:23<02:11,  1.94it/s]Epoch 13:  15%|█▌        | 46/300 [00:23<02:14,  1.89it/s]Epoch 13:  16%|█▌        | 47/300 [00:24<02:10,  1.94it/s]Epoch 13:  16%|█▌        | 48/300 [00:25<02:14,  1.87it/s]Epoch 13:  16%|█▋        | 49/300 [00:25<02:12,  1.89it/s]Epoch 13:  17%|█▋        | 50/300 [00:26<02:14,  1.86it/s]Epoch 13:  17%|█▋        | 51/300 [00:26<02:08,  1.93it/s]Epoch 13:  17%|█▋        | 52/300 [00:27<02:12,  1.87it/s]Epoch 13:  18%|█▊        | 53/300 [00:27<02:09,  1.90it/s]Epoch 13:  18%|█▊        | 54/300 [00:28<02:11,  1.86it/s]Epoch 13:  18%|█▊        | 55/300 [00:28<02:06,  1.93it/s]Epoch 13:  19%|█▊        | 56/300 [00:29<02:12,  1.84it/s]Epoch 13:  19%|█▉        | 57/300 [00:29<02:06,  1.92it/s]Epoch 13:  19%|█▉        | 58/300 [00:30<02:03,  1.96it/s]Epoch 13:  20%|█▉        | 59/300 [00:30<02:00,  1.99it/s]06/19/2022 20:22:01 - INFO - __main__ - global step: 1980; train loss: 8.41718578338623; dev loss: 8.160310745239258
Epoch 13:  20%|██        | 60/300 [00:31<01:58,  2.03it/s]Epoch 13:  20%|██        | 61/300 [00:31<02:03,  1.94it/s]Epoch 13:  21%|██        | 62/300 [00:32<02:00,  1.98it/s]Epoch 13:  21%|██        | 63/300 [00:32<01:57,  2.01it/s]Epoch 13:  21%|██▏       | 64/300 [00:33<01:56,  2.03it/s]Epoch 13:  22%|██▏       | 65/300 [00:33<02:01,  1.94it/s]Epoch 13:  22%|██▏       | 66/300 [00:34<01:58,  1.98it/s]Epoch 13:  22%|██▏       | 67/300 [00:34<01:56,  2.01it/s]Epoch 13:  23%|██▎       | 68/300 [00:35<01:54,  2.03it/s]Epoch 13:  23%|██▎       | 69/300 [00:35<01:58,  1.95it/s]Epoch 13:  23%|██▎       | 70/300 [00:36<01:55,  1.99it/s]Epoch 13:  24%|██▎       | 71/300 [00:36<01:53,  2.02it/s]Epoch 13:  24%|██▍       | 72/300 [00:37<01:52,  2.03it/s]Epoch 13:  24%|██▍       | 73/300 [00:37<01:56,  1.94it/s]Epoch 13:  25%|██▍       | 74/300 [00:38<01:55,  1.96it/s]Epoch 13:  25%|██▌       | 75/300 [00:38<01:54,  1.97it/s]Epoch 13:  25%|██▌       | 76/300 [00:39<01:51,  2.01it/s]Epoch 13:  26%|██▌       | 77/300 [00:39<02:01,  1.84it/s]Epoch 13:  26%|██▌       | 78/300 [00:40<01:56,  1.91it/s]Epoch 13:  26%|██▋       | 79/300 [00:40<01:52,  1.96it/s]06/19/2022 20:22:11 - INFO - __main__ - global step: 1990; train loss: 7.905417442321777; dev loss: 7.878395080566406
Epoch 13:  27%|██▋       | 80/300 [00:41<01:55,  1.91it/s]Epoch 13:  27%|██▋       | 81/300 [00:42<01:58,  1.84it/s]Epoch 13:  27%|██▋       | 82/300 [00:42<01:59,  1.82it/s]Epoch 13:  28%|██▊       | 83/300 [00:43<01:54,  1.90it/s]Epoch 13:  28%|██▊       | 84/300 [00:43<01:50,  1.95it/s]Epoch 13:  28%|██▊       | 85/300 [00:44<01:47,  1.99it/s]Epoch 13:  29%|██▊       | 86/300 [00:44<01:54,  1.86it/s]Epoch 13:  29%|██▉       | 87/300 [00:45<01:56,  1.83it/s]Epoch 13:  29%|██▉       | 88/300 [00:45<01:53,  1.87it/s]Epoch 13:  30%|██▉       | 89/300 [00:46<01:55,  1.83it/s]Epoch 13:  30%|███       | 90/300 [00:46<01:55,  1.81it/s]Epoch 13:  30%|███       | 91/300 [00:47<01:50,  1.88it/s]Epoch 13:  31%|███       | 92/300 [00:47<01:46,  1.95it/s]Epoch 13:  31%|███       | 93/300 [00:48<01:44,  1.99it/s]Epoch 13:  31%|███▏      | 94/300 [00:48<01:48,  1.90it/s]Epoch 13:  32%|███▏      | 95/300 [00:49<01:49,  1.87it/s]Epoch 13:  32%|███▏      | 96/300 [00:49<01:46,  1.92it/s]Epoch 13:  32%|███▏      | 97/300 [00:50<01:42,  1.98it/s]Epoch 13:  33%|███▎      | 98/300 [00:50<01:48,  1.86it/s]Epoch 13:  33%|███▎      | 99/300 [00:51<01:45,  1.91it/s]06/19/2022 20:22:22 - INFO - __main__ - global step: 2000; train loss: 8.026734352111816; dev loss: 8.117294311523438
Epoch 13:  33%|███▎      | 100/300 [00:52<01:48,  1.85it/s]Epoch 13:  34%|███▎      | 101/300 [00:52<01:44,  1.91it/s]Epoch 13:  34%|███▍      | 102/300 [00:53<01:49,  1.80it/s]Epoch 13:  34%|███▍      | 103/300 [00:53<01:45,  1.87it/s]Epoch 13:  35%|███▍      | 104/300 [00:54<01:42,  1.92it/s]Epoch 13:  35%|███▌      | 105/300 [00:54<01:44,  1.87it/s]Epoch 13:  35%|███▌      | 106/300 [00:55<01:48,  1.78it/s]Epoch 13:  36%|███▌      | 107/300 [00:55<01:44,  1.85it/s]Epoch 13:  36%|███▌      | 108/300 [00:56<01:45,  1.83it/s]Epoch 13:  36%|███▋      | 109/300 [00:56<01:41,  1.87it/s]Epoch 13:  37%|███▋      | 110/300 [00:57<01:49,  1.74it/s]Epoch 13:  37%|███▋      | 111/300 [00:58<01:43,  1.83it/s]Epoch 13:  37%|███▋      | 112/300 [00:58<01:39,  1.89it/s]Epoch 13:  38%|███▊      | 113/300 [00:59<01:37,  1.93it/s]Epoch 13:  38%|███▊      | 114/300 [00:59<01:35,  1.95it/s]Epoch 13:  38%|███▊      | 115/300 [01:00<01:39,  1.86it/s]Epoch 13:  39%|███▊      | 116/300 [01:00<01:40,  1.83it/s]Epoch 13:  39%|███▉      | 117/300 [01:01<01:36,  1.90it/s]Epoch 13:  39%|███▉      | 118/300 [01:01<01:33,  1.94it/s]Epoch 13:  40%|███▉      | 119/300 [01:02<01:36,  1.87it/s]06/19/2022 20:22:33 - INFO - __main__ - global step: 2010; train loss: 7.741359710693359; dev loss: 7.7794342041015625
Epoch 13:  40%|████      | 120/300 [01:02<01:35,  1.88it/s]Epoch 13:  40%|████      | 121/300 [01:03<01:34,  1.89it/s]Epoch 13:  41%|████      | 122/300 [01:03<01:32,  1.93it/s]Epoch 13:  41%|████      | 123/300 [01:04<01:35,  1.86it/s]Epoch 13:  41%|████▏     | 124/300 [01:04<01:31,  1.92it/s]Epoch 13:  42%|████▏     | 125/300 [01:05<01:29,  1.95it/s]Epoch 13:  42%|████▏     | 126/300 [01:05<01:31,  1.90it/s]Epoch 13:  42%|████▏     | 127/300 [01:06<01:33,  1.85it/s]Epoch 13:  43%|████▎     | 128/300 [01:06<01:30,  1.91it/s]Epoch 13:  43%|████▎     | 129/300 [01:07<01:30,  1.90it/s]Epoch 13:  43%|████▎     | 130/300 [01:07<01:27,  1.95it/s]Epoch 13:  44%|████▎     | 131/300 [01:08<01:30,  1.88it/s]Epoch 13:  44%|████▍     | 132/300 [01:09<01:26,  1.94it/s]Epoch 13:  44%|████▍     | 133/300 [01:09<01:24,  1.98it/s]Epoch 13:  45%|████▍     | 134/300 [01:10<01:26,  1.92it/s]Epoch 13:  45%|████▌     | 135/300 [01:10<01:32,  1.78it/s]Epoch 13:  45%|████▌     | 136/300 [01:11<01:29,  1.83it/s]Epoch 13:  46%|████▌     | 137/300 [01:11<01:25,  1.90it/s]Epoch 13:  46%|████▌     | 138/300 [01:12<01:24,  1.91it/s]Epoch 13:  46%|████▋     | 139/300 [01:12<01:21,  1.97it/s]06/19/2022 20:22:43 - INFO - __main__ - global step: 2020; train loss: 7.592714786529541; dev loss: 7.876628875732422
Epoch 13:  47%|████▋     | 140/300 [01:13<01:26,  1.85it/s]Epoch 13:  47%|████▋     | 141/300 [01:13<01:22,  1.93it/s]Epoch 13:  47%|████▋     | 142/300 [01:14<01:19,  1.98it/s]Epoch 13:  48%|████▊     | 143/300 [01:14<01:18,  2.01it/s]Epoch 13:  48%|████▊     | 144/300 [01:15<01:21,  1.93it/s]Epoch 13:  48%|████▊     | 145/300 [01:15<01:18,  1.98it/s]Epoch 13:  49%|████▊     | 146/300 [01:16<01:17,  1.98it/s]Epoch 13:  49%|████▉     | 147/300 [01:16<01:19,  1.93it/s]Epoch 13:  49%|████▉     | 148/300 [01:17<01:24,  1.80it/s]Epoch 13:  50%|████▉     | 149/300 [01:18<01:23,  1.80it/s]Epoch 13:  50%|█████     | 150/300 [01:18<01:21,  1.83it/s]Epoch 13:  50%|█████     | 151/300 [01:19<01:20,  1.85it/s]Epoch 13:  51%|█████     | 152/300 [01:19<01:24,  1.75it/s]Epoch 13:  51%|█████     | 153/300 [01:20<01:22,  1.78it/s]Epoch 13:  51%|█████▏    | 154/300 [01:20<01:22,  1.78it/s]Epoch 13:  52%|█████▏    | 155/300 [01:21<01:19,  1.83it/s]Epoch 13:  52%|█████▏    | 156/300 [01:22<01:24,  1.71it/s]Epoch 13:  52%|█████▏    | 157/300 [01:22<01:24,  1.70it/s]Epoch 13:  53%|█████▎    | 158/300 [01:23<01:21,  1.74it/s]Epoch 13:  53%|█████▎    | 159/300 [01:23<01:20,  1.76it/s]06/19/2022 20:22:54 - INFO - __main__ - global step: 2030; train loss: 8.165773391723633; dev loss: 7.987717628479004
Epoch 13:  53%|█████▎    | 160/300 [01:24<01:26,  1.61it/s]Epoch 13:  54%|█████▎    | 161/300 [01:25<01:23,  1.67it/s]Epoch 13:  54%|█████▍    | 162/300 [01:25<01:19,  1.73it/s]Epoch 13:  54%|█████▍    | 163/300 [01:26<01:16,  1.79it/s]Epoch 13:  55%|█████▍    | 164/300 [01:26<01:20,  1.69it/s]Epoch 13:  55%|█████▌    | 165/300 [01:27<01:16,  1.76it/s]Epoch 13:  55%|█████▌    | 166/300 [01:27<01:13,  1.82it/s]Epoch 13:  56%|█████▌    | 167/300 [01:28<01:13,  1.81it/s]Epoch 13:  56%|█████▌    | 168/300 [01:28<01:12,  1.83it/s]Epoch 13:  56%|█████▋    | 169/300 [01:29<01:18,  1.66it/s]Epoch 13:  57%|█████▋    | 170/300 [01:30<01:14,  1.74it/s]Epoch 13:  57%|█████▋    | 171/300 [01:30<01:11,  1.80it/s]Epoch 13:  57%|█████▋    | 172/300 [01:31<01:11,  1.80it/s]Epoch 13:  58%|█████▊    | 173/300 [01:31<01:15,  1.69it/s]Epoch 13:  58%|█████▊    | 174/300 [01:32<01:11,  1.76it/s]Epoch 13:  58%|█████▊    | 175/300 [01:32<01:09,  1.80it/s]Epoch 13:  59%|█████▊    | 176/300 [01:33<01:06,  1.85it/s]Epoch 13:  59%|█████▉    | 177/300 [01:33<01:09,  1.76it/s]Epoch 13:  59%|█████▉    | 178/300 [01:34<01:07,  1.81it/s]Epoch 13:  60%|█████▉    | 179/300 [01:35<01:05,  1.85it/s]06/19/2022 20:23:05 - INFO - __main__ - global step: 2040; train loss: 7.9210028648376465; dev loss: 7.771693229675293
Epoch 13:  60%|██████    | 180/300 [01:35<01:05,  1.84it/s]Epoch 13:  60%|██████    | 181/300 [01:36<01:08,  1.74it/s]Epoch 13:  61%|██████    | 182/300 [01:36<01:07,  1.76it/s]Epoch 13:  61%|██████    | 183/300 [01:37<01:04,  1.82it/s]Epoch 13:  61%|██████▏   | 184/300 [01:37<01:01,  1.87it/s]Epoch 13:  62%|██████▏   | 185/300 [01:38<01:04,  1.78it/s]Epoch 13:  62%|██████▏   | 186/300 [01:38<01:01,  1.84it/s]Epoch 13:  62%|██████▏   | 187/300 [01:39<00:59,  1.88it/s]Epoch 13:  63%|██████▎   | 188/300 [01:39<00:58,  1.91it/s]Epoch 13:  63%|██████▎   | 189/300 [01:40<01:01,  1.80it/s]Epoch 13:  63%|██████▎   | 190/300 [01:41<00:59,  1.86it/s]Epoch 13:  64%|██████▎   | 191/300 [01:41<00:57,  1.88it/s]Epoch 13:  64%|██████▍   | 192/300 [01:42<00:56,  1.91it/s]Epoch 13:  64%|██████▍   | 193/300 [01:42<00:55,  1.94it/s]Epoch 13:  65%|██████▍   | 194/300 [01:43<00:58,  1.83it/s]Epoch 13:  65%|██████▌   | 195/300 [01:43<00:56,  1.87it/s]Epoch 13:  65%|██████▌   | 196/300 [01:44<00:54,  1.91it/s]Epoch 13:  66%|██████▌   | 197/300 [01:44<00:53,  1.93it/s]Epoch 13:  66%|██████▌   | 198/300 [01:45<00:56,  1.80it/s]Epoch 13:  66%|██████▋   | 199/300 [01:45<00:54,  1.86it/s]06/19/2022 20:23:16 - INFO - __main__ - global step: 2050; train loss: 8.20387077331543; dev loss: 8.308727264404297
Epoch 13:  67%|██████▋   | 200/300 [01:46<00:53,  1.87it/s]Epoch 13:  67%|██████▋   | 201/300 [01:46<00:51,  1.91it/s]Epoch 13:  67%|██████▋   | 202/300 [01:47<00:55,  1.77it/s]Epoch 13:  68%|██████▊   | 203/300 [01:48<00:52,  1.83it/s]Epoch 13:  68%|██████▊   | 204/300 [01:48<00:51,  1.86it/s]Epoch 13:  68%|██████▊   | 205/300 [01:49<00:49,  1.90it/s]Epoch 13:  69%|██████▊   | 206/300 [01:49<00:52,  1.81it/s]Epoch 13:  69%|██████▉   | 207/300 [01:50<00:50,  1.86it/s]Epoch 13:  69%|██████▉   | 208/300 [01:50<00:48,  1.91it/s]Epoch 13:  70%|██████▉   | 209/300 [01:51<00:46,  1.95it/s]Epoch 13:  70%|███████   | 210/300 [01:51<00:49,  1.83it/s]Epoch 13:  70%|███████   | 211/300 [01:52<00:47,  1.89it/s]Epoch 13:  71%|███████   | 212/300 [01:52<00:47,  1.87it/s]Epoch 13:  71%|███████   | 213/300 [01:53<00:47,  1.84it/s]Epoch 13:  71%|███████▏  | 214/300 [01:53<00:49,  1.75it/s]Epoch 13:  72%|███████▏  | 215/300 [01:54<00:46,  1.82it/s]Epoch 13:  72%|███████▏  | 216/300 [01:54<00:45,  1.86it/s]Epoch 13:  72%|███████▏  | 217/300 [01:55<00:43,  1.89it/s]Epoch 13:  73%|███████▎  | 218/300 [01:56<00:45,  1.78it/s]Epoch 13:  73%|███████▎  | 219/300 [01:56<00:44,  1.81it/s]06/19/2022 20:23:27 - INFO - __main__ - global step: 2060; train loss: 7.670814514160156; dev loss: 7.686177730560303
Epoch 13:  73%|███████▎  | 220/300 [01:57<00:43,  1.85it/s]Epoch 13:  74%|███████▎  | 221/300 [01:57<00:41,  1.90it/s]Epoch 13:  74%|███████▍  | 222/300 [01:58<00:40,  1.91it/s]Epoch 13:  74%|███████▍  | 223/300 [01:58<00:42,  1.81it/s]Epoch 13:  75%|███████▍  | 224/300 [01:59<00:41,  1.84it/s]Epoch 13:  75%|███████▌  | 225/300 [01:59<00:41,  1.82it/s]Epoch 13:  75%|███████▌  | 226/300 [02:00<00:39,  1.87it/s]Epoch 13:  76%|███████▌  | 227/300 [02:01<00:40,  1.79it/s]Epoch 13:  76%|███████▌  | 228/300 [02:01<00:39,  1.81it/s]Epoch 13:  76%|███████▋  | 229/300 [02:02<00:38,  1.84it/s]Epoch 13:  77%|███████▋  | 230/300 [02:02<00:37,  1.89it/s]Epoch 13:  77%|███████▋  | 231/300 [02:03<00:38,  1.79it/s]Epoch 13:  77%|███████▋  | 232/300 [02:03<00:36,  1.85it/s]Epoch 13:  78%|███████▊  | 233/300 [02:04<00:36,  1.85it/s]Epoch 13:  78%|███████▊  | 234/300 [02:04<00:34,  1.89it/s]Epoch 13:  78%|███████▊  | 235/300 [02:05<00:36,  1.80it/s]Epoch 13:  79%|███████▊  | 236/300 [02:05<00:34,  1.86it/s]Epoch 13:  79%|███████▉  | 237/300 [02:06<00:33,  1.90it/s]Epoch 13:  79%|███████▉  | 238/300 [02:06<00:32,  1.94it/s]Epoch 13:  80%|███████▉  | 239/300 [02:07<00:33,  1.82it/s]06/19/2022 20:23:38 - INFO - __main__ - global step: 2070; train loss: 7.750648498535156; dev loss: 7.904886722564697
Epoch 13:  80%|████████  | 240/300 [02:07<00:32,  1.85it/s]Epoch 13:  80%|████████  | 241/300 [02:08<00:31,  1.90it/s]Epoch 13:  81%|████████  | 242/300 [02:08<00:29,  1.93it/s]Epoch 13:  81%|████████  | 243/300 [02:09<00:31,  1.78it/s]Epoch 13:  81%|████████▏ | 244/300 [02:10<00:30,  1.84it/s]Epoch 13:  82%|████████▏ | 245/300 [02:10<00:29,  1.88it/s]Epoch 13:  82%|████████▏ | 246/300 [02:11<00:28,  1.89it/s]Epoch 13:  82%|████████▏ | 247/300 [02:11<00:27,  1.93it/s]Epoch 13:  83%|████████▎ | 248/300 [02:12<00:28,  1.83it/s]Epoch 13:  83%|████████▎ | 249/300 [02:12<00:27,  1.88it/s]Epoch 13:  83%|████████▎ | 250/300 [02:13<00:26,  1.92it/s]Epoch 13:  84%|████████▎ | 251/300 [02:13<00:25,  1.95it/s]Epoch 13:  84%|████████▍ | 252/300 [02:14<00:26,  1.84it/s]Epoch 13:  84%|████████▍ | 253/300 [02:14<00:25,  1.88it/s]Epoch 13:  85%|████████▍ | 254/300 [02:15<00:24,  1.91it/s]Epoch 13:  85%|████████▌ | 255/300 [02:15<00:23,  1.94it/s]Epoch 13:  85%|████████▌ | 256/300 [02:16<00:23,  1.83it/s]Epoch 13:  86%|████████▌ | 257/300 [02:17<00:23,  1.81it/s]Epoch 13:  86%|████████▌ | 258/300 [02:17<00:22,  1.86it/s]Epoch 13:  86%|████████▋ | 259/300 [02:18<00:21,  1.91it/s]06/19/2022 20:23:49 - INFO - __main__ - global step: 2080; train loss: 7.601728916168213; dev loss: 7.29995059967041
Epoch 13:  87%|████████▋ | 260/300 [02:18<00:22,  1.74it/s]Epoch 13:  87%|████████▋ | 261/300 [02:19<00:22,  1.76it/s]Epoch 13:  87%|████████▋ | 262/300 [02:19<00:21,  1.73it/s]Epoch 13:  88%|████████▊ | 263/300 [02:20<00:21,  1.69it/s]Epoch 13:  88%|████████▊ | 264/300 [02:21<00:23,  1.56it/s]Epoch 13:  88%|████████▊ | 265/300 [02:21<00:22,  1.59it/s]Epoch 13:  89%|████████▊ | 266/300 [02:22<00:20,  1.62it/s]Epoch 13:  89%|████████▉ | 267/300 [02:23<00:19,  1.65it/s]Epoch 13:  89%|████████▉ | 268/300 [02:23<00:20,  1.57it/s]Epoch 13:  90%|████████▉ | 269/300 [02:24<00:19,  1.63it/s]Epoch 13:  90%|█████████ | 270/300 [02:24<00:18,  1.64it/s]Epoch 13:  90%|█████████ | 271/300 [02:25<00:17,  1.65it/s]Epoch 13:  91%|█████████ | 272/300 [02:26<00:17,  1.59it/s]Epoch 13:  91%|█████████ | 273/300 [02:26<00:15,  1.70it/s]Epoch 13:  91%|█████████▏| 274/300 [02:27<00:14,  1.78it/s]Epoch 13:  92%|█████████▏| 275/300 [02:27<00:13,  1.81it/s]Epoch 13:  92%|█████████▏| 276/300 [02:28<00:13,  1.81it/s]Epoch 13:  92%|█████████▏| 277/300 [02:28<00:13,  1.74it/s]Epoch 13:  93%|█████████▎| 278/300 [02:29<00:12,  1.82it/s]Epoch 13:  93%|█████████▎| 279/300 [02:29<00:11,  1.87it/s]06/19/2022 20:24:00 - INFO - __main__ - global step: 2090; train loss: 8.072639465332031; dev loss: 7.774327754974365
Epoch 13:  93%|█████████▎| 280/300 [02:30<00:10,  1.90it/s]Epoch 13:  94%|█████████▎| 281/300 [02:31<00:10,  1.77it/s]Epoch 13:  94%|█████████▍| 282/300 [02:31<00:10,  1.78it/s]Epoch 13:  94%|█████████▍| 283/300 [02:32<00:09,  1.85it/s]Epoch 13:  95%|█████████▍| 284/300 [02:32<00:08,  1.84it/s]Epoch 13:  95%|█████████▌| 285/300 [02:33<00:08,  1.77it/s]Epoch 13:  95%|█████████▌| 286/300 [02:33<00:07,  1.85it/s]Epoch 13:  96%|█████████▌| 287/300 [02:34<00:06,  1.92it/s]Epoch 13:  96%|█████████▌| 288/300 [02:34<00:06,  1.96it/s]Epoch 13:  96%|█████████▋| 289/300 [02:35<00:05,  1.89it/s]Epoch 13:  97%|█████████▋| 290/300 [02:35<00:05,  1.95it/s]Epoch 13:  97%|█████████▋| 291/300 [02:36<00:04,  1.99it/s]Epoch 13:  97%|█████████▋| 292/300 [02:36<00:03,  2.02it/s]Epoch 13:  98%|█████████▊| 293/300 [02:37<00:03,  1.93it/s]Epoch 13:  98%|█████████▊| 294/300 [02:37<00:03,  1.97it/s]Epoch 13:  98%|█████████▊| 295/300 [02:38<00:02,  1.90it/s]Epoch 13:  99%|█████████▊| 296/300 [02:38<00:02,  1.90it/s]Epoch 13:  99%|█████████▉| 297/300 [02:39<00:01,  1.85it/s]Epoch 13:  99%|█████████▉| 298/300 [02:39<00:01,  1.88it/s]Epoch 13: 100%|█████████▉| 299/300 [02:40<00:00,  1.94it/s]06/19/2022 20:24:11 - INFO - __main__ - global step: 2100; train loss: 7.765181064605713; dev loss: 7.519500732421875
Epoch 13: 100%|██████████| 300/300 [02:40<00:00,  1.98it/s]Epoch 13: 100%|██████████| 300/300 [02:40<00:00,  1.86it/s]
Epoch 14:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 14:   0%|          | 1/300 [00:00<02:23,  2.08it/s]Epoch 14:   1%|          | 2/300 [00:01<02:48,  1.76it/s]Epoch 14:   1%|          | 3/300 [00:01<02:46,  1.78it/s]Epoch 14:   1%|▏         | 4/300 [00:02<02:42,  1.82it/s]Epoch 14:   2%|▏         | 5/300 [00:02<02:42,  1.81it/s]Epoch 14:   2%|▏         | 6/300 [00:03<02:43,  1.80it/s]Epoch 14:   2%|▏         | 7/300 [00:03<02:38,  1.85it/s]Epoch 14:   3%|▎         | 8/300 [00:04<02:33,  1.90it/s]Epoch 14:   3%|▎         | 9/300 [00:04<02:28,  1.96it/s]Epoch 14:   3%|▎         | 10/300 [00:05<02:33,  1.89it/s]Epoch 14:   4%|▎         | 11/300 [00:05<02:32,  1.90it/s]Epoch 14:   4%|▍         | 12/300 [00:06<02:27,  1.95it/s]Epoch 14:   4%|▍         | 13/300 [00:06<02:23,  2.00it/s]Epoch 14:   5%|▍         | 14/300 [00:07<02:36,  1.82it/s]Epoch 14:   5%|▌         | 15/300 [00:07<02:30,  1.89it/s]Epoch 14:   5%|▌         | 16/300 [00:08<02:25,  1.95it/s]Epoch 14:   6%|▌         | 17/300 [00:08<02:21,  2.00it/s]Epoch 14:   6%|▌         | 18/300 [00:09<02:31,  1.86it/s]Epoch 14:   6%|▋         | 19/300 [00:10<02:25,  1.92it/s]06/19/2022 20:24:21 - INFO - __main__ - global step: 2110; train loss: 7.588281154632568; dev loss: 8.028597831726074
Epoch 14:   7%|▋         | 20/300 [00:10<02:21,  1.97it/s]Epoch 14:   7%|▋         | 21/300 [00:10<02:19,  2.01it/s]Epoch 14:   7%|▋         | 22/300 [00:11<02:24,  1.93it/s]Epoch 14:   8%|▊         | 23/300 [00:12<02:25,  1.91it/s]Epoch 14:   8%|▊         | 24/300 [00:12<02:21,  1.96it/s]Epoch 14:   8%|▊         | 25/300 [00:13<02:18,  1.99it/s]Epoch 14:   9%|▊         | 26/300 [00:13<02:23,  1.90it/s]Epoch 14:   9%|▉         | 27/300 [00:14<02:24,  1.90it/s]Epoch 14:   9%|▉         | 28/300 [00:14<02:19,  1.95it/s]Epoch 14:  10%|▉         | 29/300 [00:15<02:16,  1.99it/s]Epoch 14:  10%|█         | 30/300 [00:15<02:13,  2.02it/s]Epoch 14:  10%|█         | 31/300 [00:16<02:20,  1.92it/s]Epoch 14:  11%|█         | 32/300 [00:16<02:22,  1.88it/s]Epoch 14:  11%|█         | 33/300 [00:17<02:17,  1.94it/s]Epoch 14:  11%|█▏        | 34/300 [00:17<02:14,  1.98it/s]Epoch 14:  12%|█▏        | 35/300 [00:18<02:18,  1.91it/s]Epoch 14:  12%|█▏        | 36/300 [00:18<02:14,  1.97it/s]Epoch 14:  12%|█▏        | 37/300 [00:19<02:10,  2.01it/s]Epoch 14:  13%|█▎        | 38/300 [00:19<02:09,  2.03it/s]Epoch 14:  13%|█▎        | 39/300 [00:20<02:14,  1.94it/s]06/19/2022 20:24:31 - INFO - __main__ - global step: 2120; train loss: 7.800343990325928; dev loss: 7.391103267669678
Epoch 14:  13%|█▎        | 40/300 [00:20<02:10,  1.99it/s]Epoch 14:  14%|█▎        | 41/300 [00:21<02:08,  2.01it/s]Epoch 14:  14%|█▍        | 42/300 [00:21<02:06,  2.03it/s]Epoch 14:  14%|█▍        | 43/300 [00:22<02:19,  1.85it/s]Epoch 14:  15%|█▍        | 44/300 [00:22<02:14,  1.91it/s]Epoch 14:  15%|█▌        | 45/300 [00:23<02:12,  1.93it/s]Epoch 14:  15%|█▌        | 46/300 [00:23<02:14,  1.88it/s]Epoch 14:  16%|█▌        | 47/300 [00:24<02:18,  1.83it/s]Epoch 14:  16%|█▌        | 48/300 [00:25<02:18,  1.82it/s]Epoch 14:  16%|█▋        | 49/300 [00:25<02:12,  1.90it/s]Epoch 14:  17%|█▋        | 50/300 [00:25<02:08,  1.94it/s]Epoch 14:  17%|█▋        | 51/300 [00:26<02:12,  1.88it/s]Epoch 14:  17%|█▋        | 52/300 [00:27<02:08,  1.94it/s]Epoch 14:  18%|█▊        | 53/300 [00:27<02:05,  1.97it/s]Epoch 14:  18%|█▊        | 54/300 [00:28<02:06,  1.95it/s]Epoch 14:  18%|█▊        | 55/300 [00:28<02:03,  1.98it/s]Epoch 14:  19%|█▊        | 56/300 [00:29<02:08,  1.90it/s]Epoch 14:  19%|█▉        | 57/300 [00:29<02:04,  1.95it/s]Epoch 14:  19%|█▉        | 58/300 [00:30<02:05,  1.93it/s]Epoch 14:  20%|█▉        | 59/300 [00:30<02:04,  1.94it/s]06/19/2022 20:24:42 - INFO - __main__ - global step: 2130; train loss: 7.825707912445068; dev loss: 8.148500442504883
Epoch 14:  20%|██        | 60/300 [00:31<02:07,  1.88it/s]Epoch 14:  20%|██        | 61/300 [00:31<02:03,  1.94it/s]Epoch 14:  21%|██        | 62/300 [00:32<02:00,  1.98it/s]Epoch 14:  21%|██        | 63/300 [00:32<02:00,  1.96it/s]Epoch 14:  21%|██▏       | 64/300 [00:33<02:13,  1.77it/s]Epoch 14:  22%|██▏       | 65/300 [00:33<02:06,  1.85it/s]Epoch 14:  22%|██▏       | 66/300 [00:34<02:02,  1.91it/s]Epoch 14:  22%|██▏       | 67/300 [00:34<01:59,  1.96it/s]Epoch 14:  23%|██▎       | 68/300 [00:35<02:06,  1.84it/s]Epoch 14:  23%|██▎       | 69/300 [00:35<02:04,  1.86it/s]Epoch 14:  23%|██▎       | 70/300 [00:36<01:59,  1.93it/s]Epoch 14:  24%|██▎       | 71/300 [00:36<01:56,  1.97it/s]Epoch 14:  24%|██▍       | 72/300 [00:37<02:00,  1.90it/s]Epoch 14:  24%|██▍       | 73/300 [00:38<02:02,  1.85it/s]Epoch 14:  25%|██▍       | 74/300 [00:38<01:57,  1.92it/s]Epoch 14:  25%|██▌       | 75/300 [00:39<01:54,  1.96it/s]Epoch 14:  25%|██▌       | 76/300 [00:39<01:58,  1.89it/s]Epoch 14:  26%|██▌       | 77/300 [00:40<01:55,  1.93it/s]Epoch 14:  26%|██▌       | 78/300 [00:40<01:52,  1.97it/s]Epoch 14:  26%|██▋       | 79/300 [00:41<01:49,  2.01it/s]06/19/2022 20:24:52 - INFO - __main__ - global step: 2140; train loss: 7.719961643218994; dev loss: 7.389690399169922
Epoch 14:  27%|██▋       | 80/300 [00:41<01:55,  1.91it/s]Epoch 14:  27%|██▋       | 81/300 [00:42<01:51,  1.96it/s]Epoch 14:  27%|██▋       | 82/300 [00:42<01:49,  1.99it/s]Epoch 14:  28%|██▊       | 83/300 [00:43<01:47,  2.01it/s]Epoch 14:  28%|██▊       | 84/300 [00:43<01:52,  1.93it/s]Epoch 14:  28%|██▊       | 85/300 [00:44<01:55,  1.87it/s]Epoch 14:  29%|██▊       | 86/300 [00:44<01:51,  1.92it/s]Epoch 14:  29%|██▉       | 87/300 [00:45<01:49,  1.94it/s]Epoch 14:  29%|██▉       | 88/300 [00:45<01:47,  1.98it/s]Epoch 14:  30%|██▉       | 89/300 [00:46<01:50,  1.90it/s]Epoch 14:  30%|███       | 90/300 [00:46<01:52,  1.86it/s]Epoch 14:  30%|███       | 91/300 [00:47<01:48,  1.92it/s]Epoch 14:  31%|███       | 92/300 [00:47<01:45,  1.97it/s]Epoch 14:  31%|███       | 93/300 [00:48<01:49,  1.90it/s]Epoch 14:  31%|███▏      | 94/300 [00:48<01:45,  1.95it/s]Epoch 14:  32%|███▏      | 95/300 [00:49<01:43,  1.97it/s]Epoch 14:  32%|███▏      | 96/300 [00:49<01:42,  1.99it/s]Epoch 14:  32%|███▏      | 97/300 [00:50<01:47,  1.89it/s]Epoch 14:  33%|███▎      | 98/300 [00:51<01:49,  1.85it/s]Epoch 14:  33%|███▎      | 99/300 [00:51<01:45,  1.90it/s]06/19/2022 20:25:03 - INFO - __main__ - global step: 2150; train loss: 8.408719062805176; dev loss: 8.429078102111816
Epoch 14:  33%|███▎      | 100/300 [00:51<01:43,  1.93it/s]Epoch 14:  34%|███▎      | 101/300 [00:52<01:46,  1.87it/s]Epoch 14:  34%|███▍      | 102/300 [00:53<01:42,  1.94it/s]Epoch 14:  34%|███▍      | 103/300 [00:53<01:39,  1.99it/s]Epoch 14:  35%|███▍      | 104/300 [00:53<01:37,  2.02it/s]Epoch 14:  35%|███▌      | 105/300 [00:54<01:45,  1.84it/s]Epoch 14:  35%|███▌      | 106/300 [00:55<01:42,  1.89it/s]Epoch 14:  36%|███▌      | 107/300 [00:55<01:38,  1.96it/s]Epoch 14:  36%|███▌      | 108/300 [00:56<01:41,  1.90it/s]Epoch 14:  36%|███▋      | 109/300 [00:56<01:40,  1.91it/s]Epoch 14:  37%|███▋      | 110/300 [00:57<01:42,  1.86it/s]Epoch 14:  37%|███▋      | 111/300 [00:57<01:43,  1.83it/s]Epoch 14:  37%|███▋      | 112/300 [00:58<01:41,  1.84it/s]Epoch 14:  38%|███▊      | 113/300 [00:58<01:38,  1.91it/s]Epoch 14:  38%|███▊      | 114/300 [00:59<01:39,  1.86it/s]Epoch 14:  38%|███▊      | 115/300 [00:59<01:36,  1.93it/s]Epoch 14:  39%|███▊      | 116/300 [01:00<01:36,  1.91it/s]Epoch 14:  39%|███▉      | 117/300 [01:00<01:33,  1.96it/s]Epoch 14:  39%|███▉      | 118/300 [01:01<01:40,  1.81it/s]Epoch 14:  40%|███▉      | 119/300 [01:02<01:36,  1.88it/s]06/19/2022 20:25:13 - INFO - __main__ - global step: 2160; train loss: 7.8332624435424805; dev loss: 7.924173831939697
Epoch 14:  40%|████      | 120/300 [01:02<01:35,  1.89it/s]Epoch 14:  40%|████      | 121/300 [01:03<01:34,  1.90it/s]Epoch 14:  41%|████      | 122/300 [01:03<01:35,  1.86it/s]Epoch 14:  41%|████      | 123/300 [01:04<01:32,  1.91it/s]Epoch 14:  41%|████▏     | 124/300 [01:04<01:29,  1.96it/s]Epoch 14:  42%|████▏     | 125/300 [01:05<01:28,  1.98it/s]Epoch 14:  42%|████▏     | 126/300 [01:05<01:31,  1.90it/s]Epoch 14:  42%|████▏     | 127/300 [01:06<01:31,  1.90it/s]Epoch 14:  43%|████▎     | 128/300 [01:06<01:28,  1.94it/s]Epoch 14:  43%|████▎     | 129/300 [01:07<01:26,  1.97it/s]Epoch 14:  43%|████▎     | 130/300 [01:07<01:29,  1.90it/s]Epoch 14:  44%|████▎     | 131/300 [01:08<01:26,  1.95it/s]Epoch 14:  44%|████▍     | 132/300 [01:08<01:24,  2.00it/s]Epoch 14:  44%|████▍     | 133/300 [01:09<01:22,  2.02it/s]Epoch 14:  45%|████▍     | 134/300 [01:09<01:26,  1.92it/s]Epoch 14:  45%|████▌     | 135/300 [01:10<01:27,  1.88it/s]Epoch 14:  45%|████▌     | 136/300 [01:10<01:28,  1.85it/s]Epoch 14:  46%|████▌     | 137/300 [01:11<01:24,  1.92it/s]Epoch 14:  46%|████▌     | 138/300 [01:11<01:24,  1.92it/s]Epoch 14:  46%|████▋     | 139/300 [01:12<01:25,  1.88it/s]06/19/2022 20:25:24 - INFO - __main__ - global step: 2170; train loss: 7.134521484375; dev loss: 7.599381923675537
Epoch 14:  47%|████▋     | 140/300 [01:13<01:26,  1.84it/s]Epoch 14:  47%|████▋     | 141/300 [01:13<01:22,  1.92it/s]Epoch 14:  47%|████▋     | 142/300 [01:13<01:20,  1.97it/s]Epoch 14:  48%|████▊     | 143/300 [01:14<01:22,  1.90it/s]Epoch 14:  48%|████▊     | 144/300 [01:15<01:19,  1.95it/s]Epoch 14:  48%|████▊     | 145/300 [01:15<01:19,  1.94it/s]Epoch 14:  49%|████▊     | 146/300 [01:16<01:17,  1.98it/s]Epoch 14:  49%|████▉     | 147/300 [01:16<01:20,  1.91it/s]Epoch 14:  49%|████▉     | 148/300 [01:17<01:19,  1.92it/s]Epoch 14:  50%|████▉     | 149/300 [01:17<01:16,  1.97it/s]Epoch 14:  50%|█████     | 150/300 [01:18<01:15,  2.00it/s]Epoch 14:  50%|█████     | 151/300 [01:18<01:21,  1.82it/s]Epoch 14:  51%|█████     | 152/300 [01:19<01:21,  1.81it/s]Epoch 14:  51%|█████     | 153/300 [01:19<01:21,  1.80it/s]Epoch 14:  51%|█████▏    | 154/300 [01:20<01:21,  1.79it/s]Epoch 14:  52%|█████▏    | 155/300 [01:21<01:25,  1.70it/s]Epoch 14:  52%|█████▏    | 156/300 [01:21<01:19,  1.80it/s]Epoch 14:  52%|█████▏    | 157/300 [01:22<01:16,  1.88it/s]Epoch 14:  53%|█████▎    | 158/300 [01:22<01:17,  1.84it/s]Epoch 14:  53%|█████▎    | 159/300 [01:23<01:18,  1.80it/s]06/19/2022 20:25:34 - INFO - __main__ - global step: 2180; train loss: 7.611844539642334; dev loss: 7.4499335289001465
Epoch 14:  53%|█████▎    | 160/300 [01:23<01:14,  1.88it/s]Epoch 14:  54%|█████▎    | 161/300 [01:24<01:11,  1.94it/s]Epoch 14:  54%|█████▍    | 162/300 [01:24<01:10,  1.96it/s]Epoch 14:  54%|█████▍    | 163/300 [01:25<01:10,  1.95it/s]Epoch 14:  55%|█████▍    | 164/300 [01:25<01:13,  1.85it/s]Epoch 14:  55%|█████▌    | 165/300 [01:26<01:10,  1.90it/s]Epoch 14:  55%|█████▌    | 166/300 [01:26<01:08,  1.96it/s]Epoch 14:  56%|█████▌    | 167/300 [01:27<01:06,  2.00it/s]Epoch 14:  56%|█████▌    | 168/300 [01:27<01:12,  1.82it/s]Epoch 14:  56%|█████▋    | 169/300 [01:28<01:09,  1.89it/s]Epoch 14:  57%|█████▋    | 170/300 [01:28<01:06,  1.94it/s]Epoch 14:  57%|█████▋    | 171/300 [01:29<01:05,  1.97it/s]Epoch 14:  57%|█████▋    | 172/300 [01:29<01:10,  1.82it/s]Epoch 14:  58%|█████▊    | 173/300 [01:30<01:10,  1.81it/s]Epoch 14:  58%|█████▊    | 174/300 [01:30<01:06,  1.89it/s]Epoch 14:  58%|█████▊    | 175/300 [01:31<01:07,  1.86it/s]Epoch 14:  59%|█████▊    | 176/300 [01:32<01:09,  1.79it/s]Epoch 14:  59%|█████▉    | 177/300 [01:32<01:05,  1.87it/s]Epoch 14:  59%|█████▉    | 178/300 [01:33<01:02,  1.94it/s]Epoch 14:  60%|█████▉    | 179/300 [01:33<01:04,  1.89it/s]06/19/2022 20:25:45 - INFO - __main__ - global step: 2190; train loss: 7.2471184730529785; dev loss: 7.504805564880371
Epoch 14:  60%|██████    | 180/300 [01:34<01:05,  1.83it/s]Epoch 14:  60%|██████    | 181/300 [01:34<01:02,  1.91it/s]Epoch 14:  61%|██████    | 182/300 [01:35<01:00,  1.94it/s]Epoch 14:  61%|██████    | 183/300 [01:35<01:00,  1.93it/s]Epoch 14:  61%|██████▏   | 184/300 [01:36<01:04,  1.79it/s]Epoch 14:  62%|██████▏   | 185/300 [01:36<01:01,  1.87it/s]Epoch 14:  62%|██████▏   | 186/300 [01:37<01:00,  1.88it/s]Epoch 14:  62%|██████▏   | 187/300 [01:37<00:58,  1.94it/s]Epoch 14:  63%|██████▎   | 188/300 [01:38<01:02,  1.79it/s]Epoch 14:  63%|██████▎   | 189/300 [01:39<00:59,  1.87it/s]Epoch 14:  63%|██████▎   | 190/300 [01:39<00:58,  1.88it/s]Epoch 14:  64%|██████▎   | 191/300 [01:40<00:58,  1.86it/s]Epoch 14:  64%|██████▍   | 192/300 [01:40<00:56,  1.92it/s]Epoch 14:  64%|██████▍   | 193/300 [01:41<00:57,  1.87it/s]Epoch 14:  65%|██████▍   | 194/300 [01:41<00:55,  1.92it/s]Epoch 14:  65%|██████▌   | 195/300 [01:42<00:53,  1.95it/s]Epoch 14:  65%|██████▌   | 196/300 [01:42<00:55,  1.89it/s]Epoch 14:  66%|██████▌   | 197/300 [01:43<00:56,  1.84it/s]Epoch 14:  66%|██████▌   | 198/300 [01:43<00:53,  1.91it/s]Epoch 14:  66%|██████▋   | 199/300 [01:44<00:51,  1.96it/s]06/19/2022 20:25:55 - INFO - __main__ - global step: 2200; train loss: 8.470664024353027; dev loss: 8.16816234588623
Epoch 14:  67%|██████▋   | 200/300 [01:44<00:50,  1.99it/s]Epoch 14:  67%|██████▋   | 201/300 [01:45<00:51,  1.92it/s]Epoch 14:  67%|██████▋   | 202/300 [01:45<00:49,  1.96it/s]Epoch 14:  68%|██████▊   | 203/300 [01:46<00:48,  2.00it/s]Epoch 14:  68%|██████▊   | 204/300 [01:46<00:49,  1.93it/s]Epoch 14:  68%|██████▊   | 205/300 [01:47<00:50,  1.86it/s]Epoch 14:  69%|██████▊   | 206/300 [01:47<00:51,  1.84it/s]Epoch 14:  69%|██████▉   | 207/300 [01:48<00:48,  1.91it/s]Epoch 14:  69%|██████▉   | 208/300 [01:48<00:46,  1.96it/s]Epoch 14:  70%|██████▉   | 209/300 [01:49<00:48,  1.89it/s]Epoch 14:  70%|███████   | 210/300 [01:49<00:46,  1.95it/s]Epoch 14:  70%|███████   | 211/300 [01:50<00:44,  2.00it/s]Epoch 14:  71%|███████   | 212/300 [01:50<00:43,  2.02it/s]Epoch 14:  71%|███████   | 213/300 [01:51<00:45,  1.93it/s]Epoch 14:  71%|███████▏  | 214/300 [01:51<00:43,  1.97it/s]Epoch 14:  72%|███████▏  | 215/300 [01:52<00:44,  1.91it/s]Epoch 14:  72%|███████▏  | 216/300 [01:53<00:44,  1.91it/s]Epoch 14:  72%|███████▏  | 217/300 [01:53<00:43,  1.92it/s]Epoch 14:  73%|███████▎  | 218/300 [01:54<00:44,  1.86it/s]Epoch 14:  73%|███████▎  | 219/300 [01:54<00:42,  1.91it/s]06/19/2022 20:26:06 - INFO - __main__ - global step: 2210; train loss: 7.336524963378906; dev loss: 7.9593505859375
Epoch 14:  73%|███████▎  | 220/300 [01:55<00:41,  1.95it/s]Epoch 14:  74%|███████▎  | 221/300 [01:55<00:39,  1.99it/s]Epoch 14:  74%|███████▍  | 222/300 [01:56<00:40,  1.91it/s]Epoch 14:  74%|███████▍  | 223/300 [01:56<00:39,  1.96it/s]Epoch 14:  75%|███████▍  | 224/300 [01:57<00:38,  1.95it/s]Epoch 14:  75%|███████▌  | 225/300 [01:57<00:37,  1.98it/s]Epoch 14:  75%|███████▌  | 226/300 [01:58<00:38,  1.90it/s]Epoch 14:  76%|███████▌  | 227/300 [01:58<00:39,  1.85it/s]Epoch 14:  76%|███████▌  | 228/300 [01:59<00:38,  1.87it/s]Epoch 14:  76%|███████▋  | 229/300 [01:59<00:36,  1.93it/s]Epoch 14:  77%|███████▋  | 230/300 [02:00<00:38,  1.83it/s]Epoch 14:  77%|███████▋  | 231/300 [02:00<00:37,  1.82it/s]Epoch 14:  77%|███████▋  | 232/300 [02:01<00:35,  1.89it/s]Epoch 14:  78%|███████▊  | 233/300 [02:01<00:34,  1.95it/s]Epoch 14:  78%|███████▊  | 234/300 [02:02<00:35,  1.89it/s]Epoch 14:  78%|███████▊  | 235/300 [02:02<00:33,  1.95it/s]Epoch 14:  79%|███████▊  | 236/300 [02:03<00:32,  1.99it/s]Epoch 14:  79%|███████▉  | 237/300 [02:03<00:32,  1.93it/s]Epoch 14:  79%|███████▉  | 238/300 [02:04<00:33,  1.83it/s]Epoch 14:  80%|███████▉  | 239/300 [02:05<00:32,  1.85it/s]06/19/2022 20:26:16 - INFO - __main__ - global step: 2220; train loss: 8.379226684570312; dev loss: 8.453997611999512
Epoch 14:  80%|████████  | 240/300 [02:05<00:31,  1.92it/s]Epoch 14:  80%|████████  | 241/300 [02:06<00:31,  1.88it/s]Epoch 14:  81%|████████  | 242/300 [02:06<00:31,  1.83it/s]Epoch 14:  81%|████████  | 243/300 [02:07<00:29,  1.91it/s]Epoch 14:  81%|████████▏ | 244/300 [02:07<00:29,  1.92it/s]Epoch 14:  82%|████████▏ | 245/300 [02:08<00:27,  1.97it/s]Epoch 14:  82%|████████▏ | 246/300 [02:08<00:26,  2.00it/s]Epoch 14:  82%|████████▏ | 247/300 [02:09<00:27,  1.92it/s]Epoch 14:  83%|████████▎ | 248/300 [02:09<00:26,  1.96it/s]Epoch 14:  83%|████████▎ | 249/300 [02:10<00:25,  1.99it/s]Epoch 14:  83%|████████▎ | 250/300 [02:10<00:26,  1.89it/s]Epoch 14:  84%|████████▎ | 251/300 [02:11<00:27,  1.79it/s]Epoch 14:  84%|████████▍ | 252/300 [02:11<00:25,  1.87it/s]Epoch 14:  84%|████████▍ | 253/300 [02:12<00:25,  1.85it/s]Epoch 14:  85%|████████▍ | 254/300 [02:13<00:25,  1.83it/s]Epoch 14:  85%|████████▌ | 255/300 [02:13<00:24,  1.80it/s]Epoch 14:  85%|████████▌ | 256/300 [02:14<00:23,  1.88it/s]Epoch 14:  86%|████████▌ | 257/300 [02:14<00:22,  1.94it/s]Epoch 14:  86%|████████▌ | 258/300 [02:15<00:21,  1.98it/s]Epoch 14:  86%|████████▋ | 259/300 [02:15<00:21,  1.91it/s]06/19/2022 20:26:27 - INFO - __main__ - global step: 2230; train loss: 7.734195709228516; dev loss: 7.426192283630371
Epoch 14:  87%|████████▋ | 260/300 [02:16<00:20,  1.93it/s]Epoch 14:  87%|████████▋ | 261/300 [02:16<00:20,  1.89it/s]Epoch 14:  87%|████████▋ | 262/300 [02:17<00:20,  1.85it/s]Epoch 14:  88%|████████▊ | 263/300 [02:17<00:20,  1.79it/s]Epoch 14:  88%|████████▊ | 264/300 [02:18<00:19,  1.87it/s]Epoch 14:  88%|████████▊ | 265/300 [02:18<00:18,  1.93it/s]Epoch 14:  89%|████████▊ | 266/300 [02:19<00:17,  1.97it/s]Epoch 14:  89%|████████▉ | 267/300 [02:19<00:17,  1.85it/s]Epoch 14:  89%|████████▉ | 268/300 [02:20<00:16,  1.90it/s]Epoch 14:  90%|████████▉ | 269/300 [02:20<00:15,  1.96it/s]Epoch 14:  90%|█████████ | 270/300 [02:21<00:15,  1.95it/s]Epoch 14:  90%|█████████ | 271/300 [02:21<00:14,  1.98it/s]Epoch 14:  91%|█████████ | 272/300 [02:22<00:14,  1.90it/s]Epoch 14:  91%|█████████ | 273/300 [02:22<00:13,  1.95it/s]Epoch 14:  91%|█████████▏| 274/300 [02:23<00:13,  1.99it/s]Epoch 14:  92%|█████████▏| 275/300 [02:23<00:12,  2.02it/s]Epoch 14:  92%|█████████▏| 276/300 [02:24<00:13,  1.85it/s]Epoch 14:  92%|█████████▏| 277/300 [02:25<00:12,  1.91it/s]Epoch 14:  93%|█████████▎| 278/300 [02:25<00:11,  1.97it/s]Epoch 14:  93%|█████████▎| 279/300 [02:26<00:10,  1.91it/s]06/19/2022 20:26:37 - INFO - __main__ - global step: 2240; train loss: 7.6349945068359375; dev loss: 7.409360408782959
Epoch 14:  93%|█████████▎| 280/300 [02:26<00:10,  1.86it/s]Epoch 14:  94%|█████████▎| 281/300 [02:27<00:09,  1.92it/s]Epoch 14:  94%|█████████▍| 282/300 [02:27<00:09,  1.96it/s]Epoch 14:  94%|█████████▍| 283/300 [02:28<00:08,  1.91it/s]Epoch 14:  95%|█████████▍| 284/300 [02:28<00:09,  1.75it/s]Epoch 14:  95%|█████████▌| 285/300 [02:29<00:08,  1.84it/s]Epoch 14:  95%|█████████▌| 286/300 [02:29<00:07,  1.92it/s]Epoch 14:  96%|█████████▌| 287/300 [02:30<00:06,  1.95it/s]Epoch 14:  96%|█████████▌| 288/300 [02:30<00:06,  1.88it/s]Epoch 14:  96%|█████████▋| 289/300 [02:31<00:05,  1.92it/s]Epoch 14:  97%|█████████▋| 290/300 [02:31<00:05,  1.96it/s]Epoch 14:  97%|█████████▋| 291/300 [02:32<00:04,  1.97it/s]Epoch 14:  97%|█████████▋| 292/300 [02:32<00:04,  1.86it/s]Epoch 14:  98%|█████████▊| 293/300 [02:33<00:03,  1.83it/s]Epoch 14:  98%|█████████▊| 294/300 [02:33<00:03,  1.90it/s]Epoch 14:  98%|█████████▊| 295/300 [02:34<00:02,  1.90it/s]Epoch 14:  99%|█████████▊| 296/300 [02:35<00:02,  1.77it/s]Epoch 14:  99%|█████████▉| 297/300 [02:35<00:01,  1.86it/s]Epoch 14:  99%|█████████▉| 298/300 [02:36<00:01,  1.87it/s]Epoch 14: 100%|█████████▉| 299/300 [02:36<00:00,  1.89it/s]06/19/2022 20:26:48 - INFO - __main__ - global step: 2250; train loss: 7.773341178894043; dev loss: 7.586761474609375
Epoch 14: 100%|██████████| 300/300 [02:37<00:00,  1.94it/s]Epoch 14: 100%|██████████| 300/300 [02:37<00:00,  1.91it/s]
Epoch 15:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 15:   0%|          | 1/300 [00:00<02:52,  1.73it/s]Epoch 15:   1%|          | 2/300 [00:01<02:49,  1.76it/s]Epoch 15:   1%|          | 3/300 [00:01<02:36,  1.90it/s]Epoch 15:   1%|▏         | 4/300 [00:02<02:41,  1.83it/s]Epoch 15:   2%|▏         | 5/300 [00:02<02:43,  1.80it/s]Epoch 15:   2%|▏         | 6/300 [00:03<02:39,  1.84it/s]Epoch 15:   2%|▏         | 7/300 [00:03<02:35,  1.88it/s]Epoch 15:   3%|▎         | 8/300 [00:04<02:30,  1.94it/s]Epoch 15:   3%|▎         | 9/300 [00:04<02:34,  1.88it/s]Epoch 15:   3%|▎         | 10/300 [00:05<02:37,  1.85it/s]Epoch 15:   4%|▎         | 11/300 [00:05<02:38,  1.82it/s]Epoch 15:   4%|▍         | 12/300 [00:06<02:32,  1.89it/s]Epoch 15:   4%|▍         | 13/300 [00:07<02:35,  1.85it/s]Epoch 15:   5%|▍         | 14/300 [00:07<02:28,  1.92it/s]Epoch 15:   5%|▌         | 15/300 [00:07<02:24,  1.98it/s]Epoch 15:   5%|▌         | 16/300 [00:08<02:21,  2.01it/s]Epoch 15:   6%|▌         | 17/300 [00:09<02:30,  1.87it/s]Epoch 15:   6%|▌         | 18/300 [00:09<02:26,  1.93it/s]Epoch 15:   6%|▋         | 19/300 [00:10<02:22,  1.98it/s]06/19/2022 20:26:58 - INFO - __main__ - global step: 2260; train loss: 7.427276611328125; dev loss: 7.557913780212402
Epoch 15:   7%|▋         | 20/300 [00:10<02:19,  2.00it/s]Epoch 15:   7%|▋         | 21/300 [00:11<02:32,  1.83it/s]Epoch 15:   7%|▋         | 22/300 [00:11<02:26,  1.90it/s]Epoch 15:   8%|▊         | 23/300 [00:12<02:28,  1.87it/s]Epoch 15:   8%|▊         | 24/300 [00:12<02:22,  1.93it/s]Epoch 15:   8%|▊         | 25/300 [00:13<02:26,  1.88it/s]Epoch 15:   9%|▊         | 26/300 [00:13<02:35,  1.76it/s]Epoch 15:   9%|▉         | 27/300 [00:14<02:27,  1.85it/s]Epoch 15:   9%|▉         | 28/300 [00:14<02:22,  1.91it/s]Epoch 15:  10%|▉         | 29/300 [00:15<02:18,  1.95it/s]Epoch 15:  10%|█         | 30/300 [00:15<02:26,  1.85it/s]Epoch 15:  10%|█         | 31/300 [00:16<02:20,  1.92it/s]Epoch 15:  11%|█         | 32/300 [00:16<02:16,  1.97it/s]Epoch 15:  11%|█         | 33/300 [00:17<02:13,  2.00it/s]Epoch 15:  11%|█▏        | 34/300 [00:17<02:18,  1.92it/s]Epoch 15:  12%|█▏        | 35/300 [00:18<02:21,  1.87it/s]Epoch 15:  12%|█▏        | 36/300 [00:19<02:16,  1.93it/s]Epoch 15:  12%|█▏        | 37/300 [00:19<02:12,  1.98it/s]Epoch 15:  13%|█▎        | 38/300 [00:20<02:23,  1.82it/s]Epoch 15:  13%|█▎        | 39/300 [00:20<02:17,  1.90it/s]06/19/2022 20:27:09 - INFO - __main__ - global step: 2270; train loss: 6.96450138092041; dev loss: 6.960605621337891
Epoch 15:  13%|█▎        | 40/300 [00:21<02:16,  1.91it/s]Epoch 15:  14%|█▎        | 41/300 [00:21<02:12,  1.96it/s]Epoch 15:  14%|█▍        | 42/300 [00:22<02:15,  1.90it/s]Epoch 15:  14%|█▍        | 43/300 [00:22<02:11,  1.95it/s]Epoch 15:  15%|█▍        | 44/300 [00:23<02:15,  1.89it/s]Epoch 15:  15%|█▌        | 45/300 [00:23<02:11,  1.94it/s]Epoch 15:  15%|█▌        | 46/300 [00:24<02:15,  1.87it/s]Epoch 15:  16%|█▌        | 47/300 [00:24<02:10,  1.94it/s]Epoch 15:  16%|█▌        | 48/300 [00:25<02:06,  1.99it/s]Epoch 15:  16%|█▋        | 49/300 [00:25<02:04,  2.02it/s]Epoch 15:  17%|█▋        | 50/300 [00:26<02:10,  1.92it/s]Epoch 15:  17%|█▋        | 51/300 [00:26<02:07,  1.96it/s]Epoch 15:  17%|█▋        | 52/300 [00:27<02:10,  1.91it/s]Epoch 15:  18%|█▊        | 53/300 [00:27<02:09,  1.91it/s]Epoch 15:  18%|█▊        | 54/300 [00:28<02:05,  1.95it/s]Epoch 15:  18%|█▊        | 55/300 [00:28<02:10,  1.88it/s]Epoch 15:  19%|█▊        | 56/300 [00:29<02:06,  1.93it/s]Epoch 15:  19%|█▉        | 57/300 [00:29<02:02,  1.98it/s]Epoch 15:  19%|█▉        | 58/300 [00:30<02:00,  2.01it/s]Epoch 15:  20%|█▉        | 59/300 [00:30<02:04,  1.93it/s]06/19/2022 20:27:19 - INFO - __main__ - global step: 2280; train loss: 7.812310218811035; dev loss: 7.777005195617676
Epoch 15:  20%|██        | 60/300 [00:31<02:02,  1.97it/s]Epoch 15:  20%|██        | 61/300 [00:31<01:59,  1.99it/s]Epoch 15:  21%|██        | 62/300 [00:32<02:00,  1.97it/s]Epoch 15:  21%|██        | 63/300 [00:32<02:06,  1.88it/s]Epoch 15:  21%|██▏       | 64/300 [00:33<02:01,  1.94it/s]Epoch 15:  22%|██▏       | 65/300 [00:34<02:04,  1.89it/s]Epoch 15:  22%|██▏       | 66/300 [00:34<02:00,  1.95it/s]Epoch 15:  22%|██▏       | 67/300 [00:35<02:03,  1.88it/s]Epoch 15:  23%|██▎       | 68/300 [00:35<01:59,  1.94it/s]Epoch 15:  23%|██▎       | 69/300 [00:36<01:56,  1.98it/s]Epoch 15:  23%|██▎       | 70/300 [00:36<01:54,  2.01it/s]Epoch 15:  24%|██▎       | 71/300 [00:37<01:59,  1.92it/s]Epoch 15:  24%|██▍       | 72/300 [00:37<02:01,  1.88it/s]Epoch 15:  24%|██▍       | 73/300 [00:38<01:59,  1.89it/s]Epoch 15:  25%|██▍       | 74/300 [00:38<01:56,  1.94it/s]Epoch 15:  25%|██▌       | 75/300 [00:39<02:00,  1.87it/s]Epoch 15:  25%|██▌       | 76/300 [00:39<01:56,  1.93it/s]Epoch 15:  26%|██▌       | 77/300 [00:40<01:52,  1.98it/s]Epoch 15:  26%|██▌       | 78/300 [00:40<01:51,  1.99it/s]Epoch 15:  26%|██▋       | 79/300 [00:41<01:52,  1.97it/s]06/19/2022 20:27:30 - INFO - __main__ - global step: 2290; train loss: 7.809266567230225; dev loss: 7.6646728515625
Epoch 15:  27%|██▋       | 80/300 [00:41<01:59,  1.84it/s]Epoch 15:  27%|██▋       | 81/300 [00:42<01:54,  1.91it/s]Epoch 15:  27%|██▋       | 82/300 [00:42<01:51,  1.96it/s]Epoch 15:  28%|██▊       | 83/300 [00:43<01:51,  1.94it/s]Epoch 15:  28%|██▊       | 84/300 [00:43<01:53,  1.90it/s]Epoch 15:  28%|██▊       | 85/300 [00:44<01:52,  1.92it/s]Epoch 15:  29%|██▊       | 86/300 [00:44<01:49,  1.96it/s]Epoch 15:  29%|██▉       | 87/300 [00:45<01:46,  1.99it/s]Epoch 15:  29%|██▉       | 88/300 [00:45<01:51,  1.90it/s]Epoch 15:  30%|██▉       | 89/300 [00:46<01:48,  1.95it/s]Epoch 15:  30%|███       | 90/300 [00:46<01:46,  1.98it/s]Epoch 15:  30%|███       | 91/300 [00:47<01:43,  2.01it/s]Epoch 15:  31%|███       | 92/300 [00:48<01:52,  1.86it/s]Epoch 15:  31%|███       | 93/300 [00:48<01:48,  1.92it/s]Epoch 15:  31%|███▏      | 94/300 [00:49<01:50,  1.86it/s]Epoch 15:  32%|███▏      | 95/300 [00:49<01:51,  1.83it/s]Epoch 15:  32%|███▏      | 96/300 [00:50<02:03,  1.66it/s]Epoch 15:  32%|███▏      | 97/300 [00:50<01:54,  1.77it/s]Epoch 15:  33%|███▎      | 98/300 [00:51<01:49,  1.84it/s]Epoch 15:  33%|███▎      | 99/300 [00:51<01:45,  1.90it/s]06/19/2022 20:27:40 - INFO - __main__ - global step: 2300; train loss: 8.046141624450684; dev loss: 8.164365768432617
Epoch 15:  33%|███▎      | 100/300 [00:52<01:49,  1.82it/s]Epoch 15:  34%|███▎      | 101/300 [00:52<01:46,  1.86it/s]Epoch 15:  34%|███▍      | 102/300 [00:53<01:43,  1.92it/s]Epoch 15:  34%|███▍      | 103/300 [00:53<01:40,  1.96it/s]Epoch 15:  35%|███▍      | 104/300 [00:54<01:43,  1.89it/s]Epoch 15:  35%|███▌      | 105/300 [00:54<01:40,  1.93it/s]Epoch 15:  35%|███▌      | 106/300 [00:55<01:37,  1.98it/s]Epoch 15:  36%|███▌      | 107/300 [00:56<01:40,  1.92it/s]Epoch 15:  36%|███▌      | 108/300 [00:56<01:37,  1.96it/s]Epoch 15:  36%|███▋      | 109/300 [00:57<01:41,  1.89it/s]Epoch 15:  37%|███▋      | 110/300 [00:57<01:40,  1.89it/s]Epoch 15:  37%|███▋      | 111/300 [00:58<01:36,  1.95it/s]Epoch 15:  37%|███▋      | 112/300 [00:58<01:34,  2.00it/s]Epoch 15:  38%|███▊      | 113/300 [00:59<01:42,  1.82it/s]Epoch 15:  38%|███▊      | 114/300 [00:59<01:38,  1.89it/s]Epoch 15:  38%|███▊      | 115/300 [01:00<01:34,  1.95it/s]Epoch 15:  39%|███▊      | 116/300 [01:00<01:32,  2.00it/s]Epoch 15:  39%|███▉      | 117/300 [01:01<01:34,  1.93it/s]Epoch 15:  39%|███▉      | 118/300 [01:01<01:37,  1.87it/s]Epoch 15:  40%|███▉      | 119/300 [01:02<01:33,  1.93it/s]06/19/2022 20:27:51 - INFO - __main__ - global step: 2310; train loss: 7.2977399826049805; dev loss: 7.372498989105225
Epoch 15:  40%|████      | 120/300 [01:02<01:35,  1.88it/s]Epoch 15:  40%|████      | 121/300 [01:03<01:39,  1.80it/s]Epoch 15:  41%|████      | 122/300 [01:03<01:37,  1.84it/s]Epoch 15:  41%|████      | 123/300 [01:04<01:32,  1.91it/s]Epoch 15:  41%|████▏     | 124/300 [01:04<01:30,  1.95it/s]Epoch 15:  42%|████▏     | 125/300 [01:05<01:37,  1.80it/s]Epoch 15:  42%|████▏     | 126/300 [01:06<01:36,  1.80it/s]Epoch 15:  42%|████▏     | 127/300 [01:06<01:32,  1.88it/s]Epoch 15:  43%|████▎     | 128/300 [01:07<01:28,  1.93it/s]Epoch 15:  43%|████▎     | 129/300 [01:07<01:33,  1.83it/s]Epoch 15:  43%|████▎     | 130/300 [01:08<01:29,  1.90it/s]Epoch 15:  44%|████▎     | 131/300 [01:08<01:26,  1.95it/s]Epoch 15:  44%|████▍     | 132/300 [01:09<01:24,  1.99it/s]Epoch 15:  44%|████▍     | 133/300 [01:09<01:26,  1.92it/s]Epoch 15:  45%|████▍     | 134/300 [01:10<01:29,  1.86it/s]Epoch 15:  45%|████▌     | 135/300 [01:10<01:25,  1.93it/s]Epoch 15:  45%|████▌     | 136/300 [01:11<01:22,  1.98it/s]Epoch 15:  46%|████▌     | 137/300 [01:11<01:24,  1.92it/s]Epoch 15:  46%|████▌     | 138/300 [01:12<01:27,  1.86it/s]Epoch 15:  46%|████▋     | 139/300 [01:12<01:24,  1.92it/s]06/19/2022 20:28:01 - INFO - __main__ - global step: 2320; train loss: 7.766980171203613; dev loss: 7.585446357727051
Epoch 15:  47%|████▋     | 140/300 [01:13<01:21,  1.96it/s]Epoch 15:  47%|████▋     | 141/300 [01:13<01:19,  2.00it/s]Epoch 15:  47%|████▋     | 142/300 [01:14<01:24,  1.87it/s]Epoch 15:  48%|████▊     | 143/300 [01:14<01:21,  1.93it/s]Epoch 15:  48%|████▊     | 144/300 [01:15<01:21,  1.92it/s]Epoch 15:  48%|████▊     | 145/300 [01:15<01:21,  1.91it/s]Epoch 15:  49%|████▊     | 146/300 [01:16<01:22,  1.86it/s]Epoch 15:  49%|████▉     | 147/300 [01:17<01:19,  1.91it/s]Epoch 15:  49%|████▉     | 148/300 [01:17<01:17,  1.95it/s]Epoch 15:  50%|████▉     | 149/300 [01:18<01:19,  1.90it/s]Epoch 15:  50%|█████     | 150/300 [01:18<01:21,  1.84it/s]Epoch 15:  50%|█████     | 151/300 [01:19<01:17,  1.91it/s]Epoch 15:  51%|█████     | 152/300 [01:19<01:15,  1.96it/s]Epoch 15:  51%|█████     | 153/300 [01:20<01:13,  2.00it/s]Epoch 15:  51%|█████▏    | 154/300 [01:20<01:19,  1.83it/s]Epoch 15:  52%|█████▏    | 155/300 [01:21<01:15,  1.91it/s]Epoch 15:  52%|█████▏    | 156/300 [01:21<01:13,  1.96it/s]Epoch 15:  52%|█████▏    | 157/300 [01:22<01:11,  1.99it/s]Epoch 15:  53%|█████▎    | 158/300 [01:22<01:18,  1.81it/s]Epoch 15:  53%|█████▎    | 159/300 [01:23<01:14,  1.89it/s]06/19/2022 20:28:12 - INFO - __main__ - global step: 2330; train loss: 7.994304656982422; dev loss: 7.954444885253906
Epoch 15:  53%|█████▎    | 160/300 [01:23<01:13,  1.90it/s]Epoch 15:  54%|█████▎    | 161/300 [01:24<01:11,  1.94it/s]Epoch 15:  54%|█████▍    | 162/300 [01:24<01:09,  1.98it/s]Epoch 15:  54%|█████▍    | 163/300 [01:25<01:12,  1.90it/s]Epoch 15:  55%|█████▍    | 164/300 [01:25<01:12,  1.86it/s]Epoch 15:  55%|█████▌    | 165/300 [01:26<01:11,  1.88it/s]Epoch 15:  55%|█████▌    | 166/300 [01:26<01:08,  1.94it/s]Epoch 15:  56%|█████▌    | 167/300 [01:27<01:12,  1.84it/s]Epoch 15:  56%|█████▌    | 168/300 [01:28<01:10,  1.86it/s]Epoch 15:  56%|█████▋    | 169/300 [01:28<01:08,  1.92it/s]Epoch 15:  57%|█████▋    | 170/300 [01:29<01:05,  1.97it/s]Epoch 15:  57%|█████▋    | 171/300 [01:29<01:07,  1.91it/s]Epoch 15:  57%|█████▋    | 172/300 [01:30<01:08,  1.87it/s]Epoch 15:  58%|█████▊    | 173/300 [01:30<01:05,  1.94it/s]Epoch 15:  58%|█████▊    | 174/300 [01:31<01:06,  1.89it/s]Epoch 15:  58%|█████▊    | 175/300 [01:31<01:07,  1.85it/s]Epoch 15:  59%|█████▊    | 176/300 [01:32<01:04,  1.91it/s]Epoch 15:  59%|█████▉    | 177/300 [01:32<01:02,  1.97it/s]Epoch 15:  59%|█████▉    | 178/300 [01:33<01:00,  2.00it/s]Epoch 15:  60%|█████▉    | 179/300 [01:33<01:02,  1.92it/s]06/19/2022 20:28:22 - INFO - __main__ - global step: 2340; train loss: 7.3610358238220215; dev loss: 7.369374752044678
Epoch 15:  60%|██████    | 180/300 [01:34<01:01,  1.95it/s]Epoch 15:  60%|██████    | 181/300 [01:34<01:01,  1.95it/s]Epoch 15:  61%|██████    | 182/300 [01:35<01:02,  1.90it/s]Epoch 15:  61%|██████    | 183/300 [01:35<01:05,  1.77it/s]Epoch 15:  61%|██████▏   | 184/300 [01:36<01:03,  1.82it/s]Epoch 15:  62%|██████▏   | 185/300 [01:36<01:00,  1.89it/s]Epoch 15:  62%|██████▏   | 186/300 [01:37<00:58,  1.94it/s]Epoch 15:  62%|██████▏   | 187/300 [01:37<00:56,  1.99it/s]Epoch 15:  63%|██████▎   | 188/300 [01:38<00:58,  1.91it/s]Epoch 15:  63%|██████▎   | 189/300 [01:38<00:56,  1.96it/s]Epoch 15:  63%|██████▎   | 190/300 [01:39<00:56,  1.95it/s]Epoch 15:  64%|██████▎   | 191/300 [01:40<00:56,  1.94it/s]Epoch 15:  64%|██████▍   | 192/300 [01:40<00:59,  1.83it/s]Epoch 15:  64%|██████▍   | 193/300 [01:41<00:56,  1.90it/s]Epoch 15:  65%|██████▍   | 194/300 [01:41<00:57,  1.83it/s]Epoch 15:  65%|██████▌   | 195/300 [01:42<00:59,  1.77it/s]Epoch 15:  65%|██████▌   | 196/300 [01:42<01:02,  1.67it/s]Epoch 15:  66%|██████▌   | 197/300 [01:43<01:01,  1.66it/s]Epoch 15:  66%|██████▌   | 198/300 [01:44<01:01,  1.65it/s]Epoch 15:  66%|██████▋   | 199/300 [01:44<01:01,  1.64it/s]06/19/2022 20:28:33 - INFO - __main__ - global step: 2350; train loss: 7.845727443695068; dev loss: 7.8015618324279785
Epoch 15:  67%|██████▋   | 200/300 [01:45<01:03,  1.58it/s]Epoch 15:  67%|██████▋   | 201/300 [01:45<00:58,  1.70it/s]Epoch 15:  67%|██████▋   | 202/300 [01:46<00:54,  1.81it/s]Epoch 15:  68%|██████▊   | 203/300 [01:46<00:51,  1.88it/s]Epoch 15:  68%|██████▊   | 204/300 [01:47<00:52,  1.83it/s]Epoch 15:  68%|██████▊   | 205/300 [01:48<00:50,  1.86it/s]Epoch 15:  69%|██████▊   | 206/300 [01:48<00:51,  1.81it/s]Epoch 15:  69%|██████▉   | 207/300 [01:49<00:49,  1.89it/s]Epoch 15:  69%|██████▉   | 208/300 [01:49<00:51,  1.77it/s]Epoch 15:  70%|██████▉   | 209/300 [01:50<00:50,  1.82it/s]Epoch 15:  70%|███████   | 210/300 [01:50<00:50,  1.80it/s]Epoch 15:  70%|███████   | 211/300 [01:51<00:47,  1.87it/s]Epoch 15:  71%|███████   | 212/300 [01:51<00:47,  1.84it/s]Epoch 15:  71%|███████   | 213/300 [01:52<00:47,  1.81it/s]Epoch 15:  71%|███████▏  | 214/300 [01:52<00:46,  1.84it/s]Epoch 15:  72%|███████▏  | 215/300 [01:53<00:44,  1.91it/s]Epoch 15:  72%|███████▏  | 216/300 [01:53<00:42,  1.97it/s]Epoch 15:  72%|███████▏  | 217/300 [01:54<00:43,  1.90it/s]Epoch 15:  73%|███████▎  | 218/300 [01:54<00:42,  1.95it/s]Epoch 15:  73%|███████▎  | 219/300 [01:55<00:40,  1.99it/s]06/19/2022 20:28:44 - INFO - __main__ - global step: 2360; train loss: 7.953893184661865; dev loss: 7.618295192718506
Epoch 15:  73%|███████▎  | 220/300 [01:55<00:39,  2.00it/s]Epoch 15:  74%|███████▎  | 221/300 [01:56<00:42,  1.87it/s]Epoch 15:  74%|███████▍  | 222/300 [01:57<00:40,  1.93it/s]Epoch 15:  74%|███████▍  | 223/300 [01:57<00:38,  1.98it/s]Epoch 15:  75%|███████▍  | 224/300 [01:58<00:39,  1.92it/s]Epoch 15:  75%|███████▌  | 225/300 [01:58<00:40,  1.87it/s]Epoch 15:  75%|███████▌  | 226/300 [01:59<00:38,  1.94it/s]Epoch 15:  76%|███████▌  | 227/300 [01:59<00:37,  1.97it/s]Epoch 15:  76%|███████▌  | 228/300 [02:00<00:36,  2.00it/s]Epoch 15:  76%|███████▋  | 229/300 [02:00<00:38,  1.87it/s]Epoch 15:  77%|███████▋  | 230/300 [02:01<00:37,  1.85it/s]Epoch 15:  77%|███████▋  | 231/300 [02:01<00:36,  1.91it/s]Epoch 15:  77%|███████▋  | 232/300 [02:02<00:34,  1.96it/s]Epoch 15:  78%|███████▊  | 233/300 [02:02<00:35,  1.88it/s]Epoch 15:  78%|███████▊  | 234/300 [02:03<00:35,  1.84it/s]Epoch 15:  78%|███████▊  | 235/300 [02:03<00:33,  1.92it/s]Epoch 15:  79%|███████▊  | 236/300 [02:04<00:32,  1.97it/s]Epoch 15:  79%|███████▉  | 237/300 [02:04<00:33,  1.89it/s]Epoch 15:  79%|███████▉  | 238/300 [02:05<00:31,  1.94it/s]Epoch 15:  80%|███████▉  | 239/300 [02:05<00:32,  1.89it/s]06/19/2022 20:28:54 - INFO - __main__ - global step: 2370; train loss: 7.826530456542969; dev loss: 7.6644392013549805
Epoch 15:  80%|████████  | 240/300 [02:06<00:32,  1.86it/s]Epoch 15:  80%|████████  | 241/300 [02:06<00:30,  1.93it/s]Epoch 15:  81%|████████  | 242/300 [02:07<00:32,  1.79it/s]Epoch 15:  81%|████████  | 243/300 [02:08<00:30,  1.87it/s]Epoch 15:  81%|████████▏ | 244/300 [02:08<00:29,  1.93it/s]Epoch 15:  82%|████████▏ | 245/300 [02:09<00:29,  1.89it/s]Epoch 15:  82%|████████▏ | 246/300 [02:09<00:29,  1.86it/s]Epoch 15:  82%|████████▏ | 247/300 [02:10<00:28,  1.83it/s]Epoch 15:  83%|████████▎ | 248/300 [02:10<00:27,  1.91it/s]Epoch 15:  83%|████████▎ | 249/300 [02:11<00:26,  1.96it/s]Epoch 15:  83%|████████▎ | 250/300 [02:11<00:26,  1.89it/s]Epoch 15:  84%|████████▎ | 251/300 [02:12<00:25,  1.93it/s]Epoch 15:  84%|████████▍ | 252/300 [02:12<00:24,  1.97it/s]Epoch 15:  84%|████████▍ | 253/300 [02:13<00:24,  1.96it/s]Epoch 15:  85%|████████▍ | 254/300 [02:13<00:24,  1.89it/s]Epoch 15:  85%|████████▌ | 255/300 [02:14<00:23,  1.95it/s]Epoch 15:  85%|████████▌ | 256/300 [02:14<00:22,  2.00it/s]Epoch 15:  86%|████████▌ | 257/300 [02:15<00:21,  2.03it/s]Epoch 15:  86%|████████▌ | 258/300 [02:15<00:22,  1.84it/s]Epoch 15:  86%|████████▋ | 259/300 [02:16<00:21,  1.91it/s]06/19/2022 20:29:05 - INFO - __main__ - global step: 2380; train loss: 8.312609672546387; dev loss: 8.389852523803711
Epoch 15:  87%|████████▋ | 260/300 [02:16<00:20,  1.96it/s]Epoch 15:  87%|████████▋ | 261/300 [02:17<00:19,  2.00it/s]Epoch 15:  87%|████████▋ | 262/300 [02:17<00:19,  1.92it/s]Epoch 15:  88%|████████▊ | 263/300 [02:18<00:18,  1.97it/s]Epoch 15:  88%|████████▊ | 264/300 [02:18<00:17,  2.00it/s]Epoch 15:  88%|████████▊ | 265/300 [02:19<00:17,  2.03it/s]Epoch 15:  89%|████████▊ | 266/300 [02:19<00:17,  1.95it/s]Epoch 15:  89%|████████▉ | 267/300 [02:20<00:16,  1.99it/s]Epoch 15:  89%|████████▉ | 268/300 [02:20<00:15,  2.01it/s]Epoch 15:  90%|████████▉ | 269/300 [02:21<00:15,  2.03it/s]Epoch 15:  90%|█████████ | 270/300 [02:21<00:14,  2.05it/s]Epoch 15:  90%|█████████ | 271/300 [02:22<00:15,  1.90it/s]Epoch 15:  91%|█████████ | 272/300 [02:22<00:14,  1.96it/s]Epoch 15:  91%|█████████ | 273/300 [02:23<00:14,  1.90it/s]Epoch 15:  91%|█████████▏| 274/300 [02:23<00:13,  1.96it/s]Epoch 15:  92%|█████████▏| 275/300 [02:24<00:13,  1.90it/s]Epoch 15:  92%|█████████▏| 276/300 [02:25<00:12,  1.86it/s]Epoch 15:  92%|█████████▏| 277/300 [02:25<00:11,  1.92it/s]Epoch 15:  93%|█████████▎| 278/300 [02:26<00:11,  1.97it/s]Epoch 15:  93%|█████████▎| 279/300 [02:26<00:11,  1.81it/s]06/19/2022 20:29:15 - INFO - __main__ - global step: 2390; train loss: 8.267032623291016; dev loss: 7.712117671966553
Epoch 15:  93%|█████████▎| 280/300 [02:27<00:10,  1.89it/s]Epoch 15:  94%|█████████▎| 281/300 [02:27<00:09,  1.95it/s]Epoch 15:  94%|█████████▍| 282/300 [02:28<00:09,  1.89it/s]Epoch 15:  94%|█████████▍| 283/300 [02:28<00:09,  1.85it/s]Epoch 15:  95%|█████████▍| 284/300 [02:29<00:08,  1.91it/s]Epoch 15:  95%|█████████▌| 285/300 [02:29<00:07,  1.95it/s]Epoch 15:  95%|█████████▌| 286/300 [02:30<00:07,  1.90it/s]Epoch 15:  96%|█████████▌| 287/300 [02:30<00:06,  1.86it/s]Epoch 15:  96%|█████████▌| 288/300 [02:31<00:06,  1.92it/s]Epoch 15:  96%|█████████▋| 289/300 [02:31<00:05,  1.92it/s]Epoch 15:  97%|█████████▋| 290/300 [02:32<00:05,  1.87it/s]Epoch 15:  97%|█████████▋| 291/300 [02:33<00:04,  1.83it/s]Epoch 15:  97%|█████████▋| 292/300 [02:33<00:04,  1.91it/s]Epoch 15:  98%|█████████▊| 293/300 [02:34<00:03,  1.92it/s]Epoch 15:  98%|█████████▊| 294/300 [02:34<00:03,  1.97it/s]Epoch 15:  98%|█████████▊| 295/300 [02:34<00:02,  2.00it/s]Epoch 15:  99%|█████████▊| 296/300 [02:35<00:02,  1.83it/s]Epoch 15:  99%|█████████▉| 297/300 [02:36<00:01,  1.90it/s]Epoch 15:  99%|█████████▉| 298/300 [02:36<00:01,  1.96it/s]Epoch 15: 100%|█████████▉| 299/300 [02:37<00:00,  1.95it/s]06/19/2022 20:29:26 - INFO - __main__ - global step: 2400; train loss: 7.616626739501953; dev loss: 7.757384300231934
Epoch 15: 100%|██████████| 300/300 [02:37<00:00,  1.88it/s]Epoch 15: 100%|██████████| 300/300 [02:37<00:00,  1.90it/s]
Epoch 16:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 16:   0%|          | 1/300 [00:00<02:23,  2.09it/s]Epoch 16:   1%|          | 2/300 [00:00<02:22,  2.09it/s]Epoch 16:   1%|          | 3/300 [00:01<02:22,  2.09it/s]Epoch 16:   1%|▏         | 4/300 [00:02<02:33,  1.93it/s]Epoch 16:   2%|▏         | 5/300 [00:02<02:37,  1.87it/s]Epoch 16:   2%|▏         | 6/300 [00:03<02:37,  1.87it/s]Epoch 16:   2%|▏         | 7/300 [00:03<02:30,  1.95it/s]Epoch 16:   3%|▎         | 8/300 [00:04<02:35,  1.88it/s]Epoch 16:   3%|▎         | 9/300 [00:04<02:38,  1.84it/s]Epoch 16:   3%|▎         | 10/300 [00:05<02:33,  1.89it/s]Epoch 16:   4%|▎         | 11/300 [00:05<02:28,  1.95it/s]Epoch 16:   4%|▍         | 12/300 [00:06<02:33,  1.88it/s]Epoch 16:   4%|▍         | 13/300 [00:06<02:27,  1.95it/s]Epoch 16:   5%|▍         | 14/300 [00:07<02:30,  1.90it/s]Epoch 16:   5%|▌         | 15/300 [00:07<02:25,  1.96it/s]Epoch 16:   5%|▌         | 16/300 [00:08<02:31,  1.87it/s]Epoch 16:   6%|▌         | 17/300 [00:08<02:26,  1.93it/s]Epoch 16:   6%|▌         | 18/300 [00:09<02:22,  1.98it/s]Epoch 16:   6%|▋         | 19/300 [00:09<02:26,  1.91it/s]06/19/2022 20:29:36 - INFO - __main__ - global step: 2410; train loss: 7.909318447113037; dev loss: 7.897535800933838
Epoch 16:   7%|▋         | 20/300 [00:10<02:30,  1.86it/s]Epoch 16:   7%|▋         | 21/300 [00:10<02:24,  1.92it/s]Epoch 16:   7%|▋         | 22/300 [00:11<02:23,  1.94it/s]Epoch 16:   8%|▊         | 23/300 [00:12<02:26,  1.89it/s]Epoch 16:   8%|▊         | 24/300 [00:12<02:22,  1.94it/s]Epoch 16:   8%|▊         | 25/300 [00:13<02:26,  1.88it/s]Epoch 16:   9%|▊         | 26/300 [00:13<02:22,  1.93it/s]Epoch 16:   9%|▉         | 27/300 [00:14<02:18,  1.98it/s]Epoch 16:   9%|▉         | 28/300 [00:14<02:22,  1.91it/s]Epoch 16:  10%|▉         | 29/300 [00:15<02:31,  1.78it/s]Epoch 16:  10%|█         | 30/300 [00:15<02:28,  1.82it/s]Epoch 16:  10%|█         | 31/300 [00:16<02:23,  1.87it/s]Epoch 16:  11%|█         | 32/300 [00:16<02:18,  1.93it/s]Epoch 16:  11%|█         | 33/300 [00:17<02:26,  1.83it/s]Epoch 16:  11%|█▏        | 34/300 [00:17<02:20,  1.89it/s]Epoch 16:  12%|█▏        | 35/300 [00:18<02:22,  1.86it/s]Epoch 16:  12%|█▏        | 36/300 [00:18<02:17,  1.92it/s]Epoch 16:  12%|█▏        | 37/300 [00:19<02:21,  1.85it/s]Epoch 16:  13%|█▎        | 38/300 [00:19<02:16,  1.92it/s]Epoch 16:  13%|█▎        | 39/300 [00:20<02:13,  1.96it/s]06/19/2022 20:29:46 - INFO - __main__ - global step: 2420; train loss: 7.1809892654418945; dev loss: 7.351693153381348
Epoch 16:  13%|█▎        | 40/300 [00:20<02:11,  1.98it/s]Epoch 16:  14%|█▎        | 41/300 [00:21<02:18,  1.87it/s]Epoch 16:  14%|█▍        | 42/300 [00:21<02:13,  1.93it/s]Epoch 16:  14%|█▍        | 43/300 [00:22<02:13,  1.93it/s]Epoch 16:  15%|█▍        | 44/300 [00:22<02:09,  1.97it/s]Epoch 16:  15%|█▌        | 45/300 [00:23<02:14,  1.90it/s]Epoch 16:  15%|█▌        | 46/300 [00:24<02:10,  1.95it/s]Epoch 16:  16%|█▌        | 47/300 [00:24<02:09,  1.95it/s]Epoch 16:  16%|█▌        | 48/300 [00:25<02:06,  1.99it/s]Epoch 16:  16%|█▋        | 49/300 [00:25<02:04,  2.01it/s]Epoch 16:  17%|█▋        | 50/300 [00:26<02:10,  1.91it/s]Epoch 16:  17%|█▋        | 51/300 [00:26<02:07,  1.96it/s]Epoch 16:  17%|█▋        | 52/300 [00:27<02:04,  1.99it/s]Epoch 16:  18%|█▊        | 53/300 [00:27<02:03,  2.00it/s]Epoch 16:  18%|█▊        | 54/300 [00:28<02:07,  1.92it/s]Epoch 16:  18%|█▊        | 55/300 [00:28<02:11,  1.86it/s]Epoch 16:  19%|█▊        | 56/300 [00:29<02:07,  1.92it/s]Epoch 16:  19%|█▉        | 57/300 [00:29<02:03,  1.97it/s]Epoch 16:  19%|█▉        | 58/300 [00:30<02:13,  1.81it/s]Epoch 16:  20%|█▉        | 59/300 [00:30<02:14,  1.80it/s]06/19/2022 20:29:57 - INFO - __main__ - global step: 2430; train loss: 7.793374538421631; dev loss: 7.820040225982666
Epoch 16:  20%|██        | 60/300 [00:31<02:08,  1.87it/s]Epoch 16:  20%|██        | 61/300 [00:31<02:03,  1.93it/s]Epoch 16:  21%|██        | 62/300 [00:32<02:12,  1.80it/s]Epoch 16:  21%|██        | 63/300 [00:32<02:06,  1.88it/s]Epoch 16:  21%|██▏       | 64/300 [00:33<02:02,  1.93it/s]Epoch 16:  22%|██▏       | 65/300 [00:33<01:58,  1.98it/s]Epoch 16:  22%|██▏       | 66/300 [00:34<02:04,  1.88it/s]Epoch 16:  22%|██▏       | 67/300 [00:35<02:00,  1.94it/s]Epoch 16:  23%|██▎       | 68/300 [00:35<01:57,  1.98it/s]Epoch 16:  23%|██▎       | 69/300 [00:36<02:00,  1.91it/s]Epoch 16:  23%|██▎       | 70/300 [00:36<02:09,  1.78it/s]Epoch 16:  24%|██▎       | 71/300 [00:37<02:03,  1.86it/s]Epoch 16:  24%|██▍       | 72/300 [00:37<02:03,  1.84it/s]Epoch 16:  24%|██▍       | 73/300 [00:38<02:00,  1.88it/s]Epoch 16:  25%|██▍       | 74/300 [00:38<02:03,  1.83it/s]Epoch 16:  25%|██▌       | 75/300 [00:39<02:01,  1.86it/s]Epoch 16:  25%|██▌       | 76/300 [00:39<01:56,  1.92it/s]Epoch 16:  26%|██▌       | 77/300 [00:40<01:56,  1.92it/s]Epoch 16:  26%|██▌       | 78/300 [00:40<01:58,  1.87it/s]Epoch 16:  26%|██▋       | 79/300 [00:41<02:00,  1.83it/s]06/19/2022 20:30:07 - INFO - __main__ - global step: 2440; train loss: 7.44922399520874; dev loss: 7.560949802398682
Epoch 16:  27%|██▋       | 80/300 [00:41<01:55,  1.91it/s]Epoch 16:  27%|██▋       | 81/300 [00:42<01:52,  1.95it/s]Epoch 16:  27%|██▋       | 82/300 [00:42<01:49,  1.98it/s]Epoch 16:  28%|██▊       | 83/300 [00:43<01:53,  1.91it/s]Epoch 16:  28%|██▊       | 84/300 [00:43<01:50,  1.96it/s]Epoch 16:  28%|██▊       | 85/300 [00:44<01:49,  1.97it/s]Epoch 16:  29%|██▊       | 86/300 [00:44<01:46,  2.00it/s]Epoch 16:  29%|██▉       | 87/300 [00:45<01:50,  1.92it/s]Epoch 16:  29%|██▉       | 88/300 [00:46<01:53,  1.87it/s]Epoch 16:  30%|██▉       | 89/300 [00:46<01:49,  1.93it/s]Epoch 16:  30%|███       | 90/300 [00:47<01:52,  1.87it/s]Epoch 16:  30%|███       | 91/300 [00:47<01:58,  1.76it/s]Epoch 16:  31%|███       | 92/300 [00:48<01:52,  1.84it/s]Epoch 16:  31%|███       | 93/300 [00:48<01:47,  1.92it/s]Epoch 16:  31%|███▏      | 94/300 [00:49<01:44,  1.97it/s]Epoch 16:  32%|███▏      | 95/300 [00:49<01:52,  1.81it/s]Epoch 16:  32%|███▏      | 96/300 [00:50<01:53,  1.80it/s]Epoch 16:  32%|███▏      | 97/300 [00:50<01:48,  1.88it/s]Epoch 16:  33%|███▎      | 98/300 [00:51<01:44,  1.94it/s]Epoch 16:  33%|███▎      | 99/300 [00:51<01:47,  1.87it/s]06/19/2022 20:30:18 - INFO - __main__ - global step: 2450; train loss: 7.199307918548584; dev loss: 7.569143772125244
Epoch 16:  33%|███▎      | 100/300 [00:52<01:43,  1.93it/s]Epoch 16:  34%|███▎      | 101/300 [00:53<01:45,  1.89it/s]Epoch 16:  34%|███▍      | 102/300 [00:53<01:44,  1.90it/s]Epoch 16:  34%|███▍      | 103/300 [00:54<01:41,  1.94it/s]Epoch 16:  35%|███▍      | 104/300 [00:54<01:45,  1.86it/s]Epoch 16:  35%|███▌      | 105/300 [00:55<01:41,  1.93it/s]Epoch 16:  35%|███▌      | 106/300 [00:55<01:38,  1.98it/s]Epoch 16:  36%|███▌      | 107/300 [00:56<01:36,  2.01it/s]Epoch 16:  36%|███▌      | 108/300 [00:56<01:39,  1.92it/s]Epoch 16:  36%|███▋      | 109/300 [00:57<01:36,  1.98it/s]Epoch 16:  37%|███▋      | 110/300 [00:57<01:34,  2.00it/s]Epoch 16:  37%|███▋      | 111/300 [00:58<01:33,  2.02it/s]Epoch 16:  37%|███▋      | 112/300 [00:58<01:38,  1.91it/s]Epoch 16:  38%|███▊      | 113/300 [00:59<01:34,  1.97it/s]Epoch 16:  38%|███▊      | 114/300 [00:59<01:37,  1.91it/s]Epoch 16:  38%|███▊      | 115/300 [01:00<01:39,  1.85it/s]Epoch 16:  39%|███▊      | 116/300 [01:00<01:45,  1.75it/s]Epoch 16:  39%|███▉      | 117/300 [01:01<01:39,  1.84it/s]Epoch 16:  39%|███▉      | 118/300 [01:01<01:35,  1.90it/s]Epoch 16:  40%|███▉      | 119/300 [01:02<01:32,  1.96it/s]06/19/2022 20:30:28 - INFO - __main__ - global step: 2460; train loss: 7.465368747711182; dev loss: 7.804560661315918
Epoch 16:  40%|████      | 120/300 [01:02<01:37,  1.85it/s]Epoch 16:  40%|████      | 121/300 [01:03<01:33,  1.92it/s]Epoch 16:  41%|████      | 122/300 [01:03<01:30,  1.96it/s]Epoch 16:  41%|████      | 123/300 [01:04<01:28,  2.00it/s]Epoch 16:  41%|████▏     | 124/300 [01:04<01:31,  1.92it/s]Epoch 16:  42%|████▏     | 125/300 [01:05<01:31,  1.92it/s]Epoch 16:  42%|████▏     | 126/300 [01:06<01:32,  1.88it/s]Epoch 16:  42%|████▏     | 127/300 [01:06<01:29,  1.94it/s]Epoch 16:  43%|████▎     | 128/300 [01:07<01:32,  1.87it/s]Epoch 16:  43%|████▎     | 129/300 [01:07<01:28,  1.93it/s]Epoch 16:  43%|████▎     | 130/300 [01:08<01:27,  1.95it/s]Epoch 16:  44%|████▎     | 131/300 [01:08<01:26,  1.95it/s]Epoch 16:  44%|████▍     | 132/300 [01:09<01:24,  1.99it/s]Epoch 16:  44%|████▍     | 133/300 [01:09<01:27,  1.91it/s]Epoch 16:  45%|████▍     | 134/300 [01:10<01:24,  1.96it/s]Epoch 16:  45%|████▌     | 135/300 [01:10<01:26,  1.91it/s]Epoch 16:  45%|████▌     | 136/300 [01:11<01:23,  1.96it/s]Epoch 16:  46%|████▌     | 137/300 [01:11<01:26,  1.89it/s]Epoch 16:  46%|████▌     | 138/300 [01:12<01:27,  1.86it/s]Epoch 16:  46%|████▋     | 139/300 [01:12<01:25,  1.88it/s]06/19/2022 20:30:39 - INFO - __main__ - global step: 2470; train loss: 7.659028053283691; dev loss: 7.70553731918335
Epoch 16:  47%|████▋     | 140/300 [01:13<01:22,  1.93it/s]Epoch 16:  47%|████▋     | 141/300 [01:13<01:24,  1.88it/s]Epoch 16:  47%|████▋     | 142/300 [01:14<01:21,  1.94it/s]Epoch 16:  48%|████▊     | 143/300 [01:14<01:22,  1.89it/s]Epoch 16:  48%|████▊     | 144/300 [01:15<01:19,  1.95it/s]Epoch 16:  48%|████▊     | 145/300 [01:15<01:21,  1.89it/s]Epoch 16:  49%|████▊     | 146/300 [01:16<01:19,  1.95it/s]Epoch 16:  49%|████▉     | 147/300 [01:16<01:16,  2.00it/s]Epoch 16:  49%|████▉     | 148/300 [01:17<01:15,  2.02it/s]Epoch 16:  50%|████▉     | 149/300 [01:17<01:19,  1.91it/s]Epoch 16:  50%|█████     | 150/300 [01:18<01:16,  1.95it/s]Epoch 16:  50%|█████     | 151/300 [01:18<01:14,  1.99it/s]Epoch 16:  51%|█████     | 152/300 [01:19<01:13,  2.01it/s]Epoch 16:  51%|█████     | 153/300 [01:20<01:18,  1.87it/s]Epoch 16:  51%|█████▏    | 154/300 [01:20<01:16,  1.92it/s]Epoch 16:  52%|█████▏    | 155/300 [01:20<01:13,  1.96it/s]Epoch 16:  52%|█████▏    | 156/300 [01:21<01:13,  1.96it/s]Epoch 16:  52%|█████▏    | 157/300 [01:21<01:11,  1.99it/s]Epoch 16:  53%|█████▎    | 158/300 [01:22<01:14,  1.91it/s]Epoch 16:  53%|█████▎    | 159/300 [01:23<01:12,  1.95it/s]06/19/2022 20:30:49 - INFO - __main__ - global step: 2480; train loss: 7.654658317565918; dev loss: 7.456655025482178
Epoch 16:  53%|█████▎    | 160/300 [01:23<01:10,  1.99it/s]Epoch 16:  54%|█████▎    | 161/300 [01:23<01:08,  2.03it/s]Epoch 16:  54%|█████▍    | 162/300 [01:24<01:14,  1.84it/s]Epoch 16:  54%|█████▍    | 163/300 [01:25<01:13,  1.87it/s]Epoch 16:  55%|█████▍    | 164/300 [01:25<01:10,  1.93it/s]Epoch 16:  55%|█████▌    | 165/300 [01:26<01:11,  1.89it/s]Epoch 16:  55%|█████▌    | 166/300 [01:26<01:13,  1.83it/s]Epoch 16:  56%|█████▌    | 167/300 [01:27<01:09,  1.91it/s]Epoch 16:  56%|█████▌    | 168/300 [01:27<01:07,  1.95it/s]Epoch 16:  56%|█████▋    | 169/300 [01:28<01:06,  1.98it/s]Epoch 16:  57%|█████▋    | 170/300 [01:28<01:08,  1.90it/s]Epoch 16:  57%|█████▋    | 171/300 [01:29<01:06,  1.94it/s]Epoch 16:  57%|█████▋    | 172/300 [01:29<01:06,  1.93it/s]Epoch 16:  58%|█████▊    | 173/300 [01:30<01:04,  1.96it/s]Epoch 16:  58%|█████▊    | 174/300 [01:30<01:07,  1.87it/s]Epoch 16:  58%|█████▊    | 175/300 [01:31<01:04,  1.94it/s]Epoch 16:  59%|█████▊    | 176/300 [01:31<01:04,  1.93it/s]Epoch 16:  59%|█████▉    | 177/300 [01:32<01:03,  1.95it/s]Epoch 16:  59%|█████▉    | 178/300 [01:33<01:06,  1.85it/s]Epoch 16:  60%|█████▉    | 179/300 [01:33<01:03,  1.92it/s]06/19/2022 20:31:00 - INFO - __main__ - global step: 2490; train loss: 7.80057430267334; dev loss: 7.767476558685303
Epoch 16:  60%|██████    | 180/300 [01:33<01:01,  1.94it/s]Epoch 16:  60%|██████    | 181/300 [01:34<01:01,  1.93it/s]Epoch 16:  61%|██████    | 182/300 [01:35<01:03,  1.87it/s]Epoch 16:  61%|██████    | 183/300 [01:35<01:00,  1.93it/s]Epoch 16:  61%|██████▏   | 184/300 [01:36<00:59,  1.97it/s]Epoch 16:  62%|██████▏   | 185/300 [01:36<00:59,  1.94it/s]Epoch 16:  62%|██████▏   | 186/300 [01:37<00:57,  1.99it/s]Epoch 16:  62%|██████▏   | 187/300 [01:37<00:58,  1.92it/s]Epoch 16:  63%|██████▎   | 188/300 [01:38<00:59,  1.88it/s]Epoch 16:  63%|██████▎   | 189/300 [01:38<00:58,  1.89it/s]Epoch 16:  63%|██████▎   | 190/300 [01:39<00:56,  1.95it/s]Epoch 16:  64%|██████▎   | 191/300 [01:39<00:57,  1.89it/s]Epoch 16:  64%|██████▍   | 192/300 [01:40<00:55,  1.93it/s]Epoch 16:  64%|██████▍   | 193/300 [01:40<00:54,  1.95it/s]Epoch 16:  65%|██████▍   | 194/300 [01:41<00:53,  1.98it/s]Epoch 16:  65%|██████▌   | 195/300 [01:41<00:56,  1.86it/s]Epoch 16:  65%|██████▌   | 196/300 [01:42<00:54,  1.93it/s]Epoch 16:  66%|██████▌   | 197/300 [01:42<00:52,  1.97it/s]Epoch 16:  66%|██████▌   | 198/300 [01:43<00:51,  2.00it/s]Epoch 16:  66%|██████▋   | 199/300 [01:43<00:52,  1.92it/s]06/19/2022 20:31:10 - INFO - __main__ - global step: 2500; train loss: 7.556858062744141; dev loss: 7.914956092834473
Epoch 16:  67%|██████▋   | 200/300 [01:44<00:52,  1.92it/s]Epoch 16:  67%|██████▋   | 201/300 [01:44<00:50,  1.97it/s]Epoch 16:  67%|██████▋   | 202/300 [01:45<00:49,  1.98it/s]Epoch 16:  68%|██████▊   | 203/300 [01:45<00:51,  1.90it/s]Epoch 16:  68%|██████▊   | 204/300 [01:46<00:50,  1.91it/s]Epoch 16:  68%|██████▊   | 205/300 [01:46<00:48,  1.97it/s]Epoch 16:  69%|██████▊   | 206/300 [01:47<00:47,  1.99it/s]Epoch 16:  69%|██████▉   | 207/300 [01:47<00:48,  1.91it/s]Epoch 16:  69%|██████▉   | 208/300 [01:48<00:47,  1.95it/s]Epoch 16:  70%|██████▉   | 209/300 [01:48<00:45,  2.00it/s]Epoch 16:  70%|███████   | 210/300 [01:49<00:46,  1.93it/s]Epoch 16:  70%|███████   | 211/300 [01:49<00:45,  1.97it/s]Epoch 16:  71%|███████   | 212/300 [01:50<00:46,  1.90it/s]Epoch 16:  71%|███████   | 213/300 [01:51<00:44,  1.94it/s]Epoch 16:  71%|███████▏  | 214/300 [01:51<00:43,  1.98it/s]Epoch 16:  72%|███████▏  | 215/300 [01:52<00:42,  1.99it/s]Epoch 16:  72%|███████▏  | 216/300 [01:52<00:43,  1.92it/s]Epoch 16:  72%|███████▏  | 217/300 [01:53<00:42,  1.95it/s]Epoch 16:  73%|███████▎  | 218/300 [01:53<00:41,  2.00it/s]Epoch 16:  73%|███████▎  | 219/300 [01:54<00:40,  2.02it/s]06/19/2022 20:31:20 - INFO - __main__ - global step: 2510; train loss: 7.754383087158203; dev loss: 7.9645843505859375
Epoch 16:  73%|███████▎  | 220/300 [01:54<00:41,  1.92it/s]Epoch 16:  74%|███████▎  | 221/300 [01:55<00:41,  1.92it/s]Epoch 16:  74%|███████▍  | 222/300 [01:55<00:39,  1.97it/s]Epoch 16:  74%|███████▍  | 223/300 [01:56<00:38,  1.99it/s]Epoch 16:  75%|███████▍  | 224/300 [01:56<00:40,  1.87it/s]Epoch 16:  75%|███████▌  | 225/300 [01:57<00:39,  1.89it/s]Epoch 16:  75%|███████▌  | 226/300 [01:57<00:40,  1.85it/s]Epoch 16:  76%|███████▌  | 227/300 [01:58<00:38,  1.91it/s]Epoch 16:  76%|███████▌  | 228/300 [01:58<00:40,  1.78it/s]Epoch 16:  76%|███████▋  | 229/300 [01:59<00:38,  1.86it/s]Epoch 16:  77%|███████▋  | 230/300 [01:59<00:37,  1.89it/s]Epoch 16:  77%|███████▋  | 231/300 [02:00<00:35,  1.95it/s]Epoch 16:  77%|███████▋  | 232/300 [02:01<00:37,  1.82it/s]Epoch 16:  78%|███████▊  | 233/300 [02:01<00:35,  1.90it/s]Epoch 16:  78%|███████▊  | 234/300 [02:02<00:35,  1.87it/s]Epoch 16:  78%|███████▊  | 235/300 [02:02<00:35,  1.85it/s]Epoch 16:  79%|███████▊  | 236/300 [02:03<00:35,  1.82it/s]Epoch 16:  79%|███████▉  | 237/300 [02:03<00:34,  1.85it/s]Epoch 16:  79%|███████▉  | 238/300 [02:04<00:32,  1.91it/s]Epoch 16:  80%|███████▉  | 239/300 [02:04<00:32,  1.87it/s]06/19/2022 20:31:31 - INFO - __main__ - global step: 2520; train loss: 7.746277809143066; dev loss: 7.676242828369141
Epoch 16:  80%|████████  | 240/300 [02:05<00:31,  1.92it/s]Epoch 16:  80%|████████  | 241/300 [02:05<00:31,  1.86it/s]Epoch 16:  81%|████████  | 242/300 [02:06<00:31,  1.83it/s]Epoch 16:  81%|████████  | 243/300 [02:06<00:31,  1.82it/s]Epoch 16:  81%|████████▏ | 244/300 [02:07<00:31,  1.80it/s]Epoch 16:  82%|████████▏ | 245/300 [02:08<00:30,  1.79it/s]Epoch 16:  82%|████████▏ | 246/300 [02:08<00:28,  1.87it/s]Epoch 16:  82%|████████▏ | 247/300 [02:09<00:27,  1.93it/s]Epoch 16:  83%|████████▎ | 248/300 [02:09<00:27,  1.88it/s]Epoch 16:  83%|████████▎ | 249/300 [02:10<00:27,  1.85it/s]Epoch 16:  83%|████████▎ | 250/300 [02:10<00:26,  1.92it/s]Epoch 16:  84%|████████▎ | 251/300 [02:11<00:24,  1.97it/s]Epoch 16:  84%|████████▍ | 252/300 [02:11<00:24,  1.95it/s]Epoch 16:  84%|████████▍ | 253/300 [02:12<00:25,  1.81it/s]Epoch 16:  85%|████████▍ | 254/300 [02:12<00:24,  1.88it/s]Epoch 16:  85%|████████▌ | 255/300 [02:13<00:23,  1.94it/s]Epoch 16:  85%|████████▌ | 256/300 [02:13<00:22,  1.98it/s]Epoch 16:  86%|████████▌ | 257/300 [02:14<00:23,  1.81it/s]Epoch 16:  86%|████████▌ | 258/300 [02:14<00:23,  1.80it/s]Epoch 16:  86%|████████▋ | 259/300 [02:15<00:21,  1.88it/s]06/19/2022 20:31:41 - INFO - __main__ - global step: 2530; train loss: 7.994242191314697; dev loss: 7.952866554260254
Epoch 16:  87%|████████▋ | 260/300 [02:15<00:21,  1.89it/s]Epoch 16:  87%|████████▋ | 261/300 [02:16<00:21,  1.84it/s]Epoch 16:  87%|████████▋ | 262/300 [02:16<00:19,  1.91it/s]Epoch 16:  88%|████████▊ | 263/300 [02:17<00:18,  1.95it/s]Epoch 16:  88%|████████▊ | 264/300 [02:17<00:18,  1.99it/s]Epoch 16:  88%|████████▊ | 265/300 [02:18<00:17,  2.02it/s]Epoch 16:  89%|████████▊ | 266/300 [02:19<00:18,  1.85it/s]Epoch 16:  89%|████████▉ | 267/300 [02:19<00:17,  1.91it/s]Epoch 16:  89%|████████▉ | 268/300 [02:20<00:16,  1.96it/s]Epoch 16:  90%|████████▉ | 269/300 [02:20<00:15,  1.98it/s]Epoch 16:  90%|█████████ | 270/300 [02:21<00:16,  1.83it/s]Epoch 16:  90%|█████████ | 271/300 [02:21<00:15,  1.90it/s]Epoch 16:  91%|█████████ | 272/300 [02:22<00:14,  1.95it/s]Epoch 16:  91%|█████████ | 273/300 [02:22<00:13,  1.94it/s]Epoch 16:  91%|█████████▏| 274/300 [02:23<00:14,  1.84it/s]Epoch 16:  92%|█████████▏| 275/300 [02:23<00:13,  1.91it/s]Epoch 16:  92%|█████████▏| 276/300 [02:24<00:12,  1.95it/s]Epoch 16:  92%|█████████▏| 277/300 [02:24<00:11,  2.00it/s]Epoch 16:  93%|█████████▎| 278/300 [02:25<00:11,  1.92it/s]Epoch 16:  93%|█████████▎| 279/300 [02:25<00:10,  1.98it/s]06/19/2022 20:31:52 - INFO - __main__ - global step: 2540; train loss: 7.659089088439941; dev loss: 7.693219184875488
Epoch 16:  93%|█████████▎| 280/300 [02:26<00:09,  2.00it/s]Epoch 16:  94%|█████████▎| 281/300 [02:26<00:09,  2.03it/s]Epoch 16:  94%|█████████▍| 282/300 [02:27<00:09,  1.93it/s]Epoch 16:  94%|█████████▍| 283/300 [02:27<00:08,  1.97it/s]Epoch 16:  95%|█████████▍| 284/300 [02:28<00:07,  2.00it/s]Epoch 16:  95%|█████████▌| 285/300 [02:28<00:07,  1.93it/s]Epoch 16:  95%|█████████▌| 286/300 [02:29<00:07,  1.83it/s]Epoch 16:  96%|█████████▌| 287/300 [02:29<00:07,  1.82it/s]Epoch 16:  96%|█████████▌| 288/300 [02:30<00:06,  1.89it/s]Epoch 16:  96%|█████████▋| 289/300 [02:30<00:05,  1.95it/s]Epoch 16:  97%|█████████▋| 290/300 [02:31<00:05,  1.87it/s]Epoch 16:  97%|█████████▋| 291/300 [02:31<00:04,  1.94it/s]Epoch 16:  97%|█████████▋| 292/300 [02:32<00:04,  1.89it/s]Epoch 16:  98%|█████████▊| 293/300 [02:33<00:03,  1.95it/s]Epoch 16:  98%|█████████▊| 294/300 [02:33<00:03,  1.98it/s]Epoch 16:  98%|█████████▊| 295/300 [02:34<00:02,  1.91it/s]Epoch 16:  99%|█████████▊| 296/300 [02:34<00:02,  1.95it/s]Epoch 16:  99%|█████████▉| 297/300 [02:35<00:01,  1.99it/s]Epoch 16:  99%|█████████▉| 298/300 [02:35<00:00,  2.02it/s]Epoch 16: 100%|█████████▉| 299/300 [02:36<00:00,  1.84it/s]06/19/2022 20:32:02 - INFO - __main__ - global step: 2550; train loss: 7.3935089111328125; dev loss: 7.817242622375488
Epoch 16: 100%|██████████| 300/300 [02:36<00:00,  1.90it/s]Epoch 16: 100%|██████████| 300/300 [02:36<00:00,  1.92it/s]
Epoch 17:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 17:   0%|          | 1/300 [00:00<02:23,  2.09it/s]Epoch 17:   1%|          | 2/300 [00:00<02:23,  2.08it/s]Epoch 17:   1%|          | 3/300 [00:01<02:42,  1.83it/s]Epoch 17:   1%|▏         | 4/300 [00:02<02:33,  1.93it/s]Epoch 17:   2%|▏         | 5/300 [00:02<02:27,  1.99it/s]Epoch 17:   2%|▏         | 6/300 [00:03<02:33,  1.91it/s]Epoch 17:   2%|▏         | 7/300 [00:03<02:38,  1.84it/s]Epoch 17:   3%|▎         | 8/300 [00:04<02:33,  1.91it/s]Epoch 17:   3%|▎         | 9/300 [00:04<02:29,  1.95it/s]Epoch 17:   3%|▎         | 10/300 [00:05<02:26,  1.98it/s]Epoch 17:   4%|▎         | 11/300 [00:05<02:36,  1.85it/s]Epoch 17:   4%|▍         | 12/300 [00:06<02:30,  1.92it/s]Epoch 17:   4%|▍         | 13/300 [00:06<02:25,  1.97it/s]Epoch 17:   5%|▍         | 14/300 [00:07<02:30,  1.90it/s]Epoch 17:   5%|▌         | 15/300 [00:07<02:36,  1.82it/s]Epoch 17:   5%|▌         | 16/300 [00:08<02:30,  1.88it/s]Epoch 17:   6%|▌         | 17/300 [00:08<02:26,  1.93it/s]Epoch 17:   6%|▌         | 18/300 [00:09<02:22,  1.98it/s]Epoch 17:   6%|▋         | 19/300 [00:09<02:25,  1.93it/s]06/19/2022 20:32:13 - INFO - __main__ - global step: 2560; train loss: 7.58678674697876; dev loss: 7.679732322692871
Epoch 17:   7%|▋         | 20/300 [00:10<02:29,  1.87it/s]Epoch 17:   7%|▋         | 21/300 [00:11<02:32,  1.83it/s]Epoch 17:   7%|▋         | 22/300 [00:11<02:29,  1.86it/s]Epoch 17:   8%|▊         | 23/300 [00:12<02:24,  1.91it/s]Epoch 17:   8%|▊         | 24/300 [00:12<02:32,  1.81it/s]Epoch 17:   8%|▊         | 25/300 [00:13<02:33,  1.80it/s]Epoch 17:   9%|▊         | 26/300 [00:13<02:30,  1.82it/s]Epoch 17:   9%|▉         | 27/300 [00:14<02:24,  1.89it/s]Epoch 17:   9%|▉         | 28/300 [00:14<02:29,  1.82it/s]Epoch 17:  10%|▉         | 29/300 [00:15<02:30,  1.80it/s]Epoch 17:  10%|█         | 30/300 [00:15<02:23,  1.88it/s]Epoch 17:  10%|█         | 31/300 [00:16<02:19,  1.93it/s]Epoch 17:  11%|█         | 32/300 [00:16<02:26,  1.83it/s]Epoch 17:  11%|█         | 33/300 [00:17<02:24,  1.85it/s]Epoch 17:  11%|█▏        | 34/300 [00:17<02:18,  1.92it/s]Epoch 17:  12%|█▏        | 35/300 [00:18<02:15,  1.95it/s]Epoch 17:  12%|█▏        | 36/300 [00:19<02:20,  1.89it/s]Epoch 17:  12%|█▏        | 37/300 [00:19<02:15,  1.94it/s]Epoch 17:  13%|█▎        | 38/300 [00:20<02:13,  1.96it/s]Epoch 17:  13%|█▎        | 39/300 [00:20<02:17,  1.90it/s]06/19/2022 20:32:23 - INFO - __main__ - global step: 2570; train loss: 6.942282676696777; dev loss: 6.826598167419434
Epoch 17:  13%|█▎        | 40/300 [00:21<02:21,  1.84it/s]Epoch 17:  14%|█▎        | 41/300 [00:21<02:21,  1.83it/s]Epoch 17:  14%|█▍        | 42/300 [00:22<02:15,  1.90it/s]Epoch 17:  14%|█▍        | 43/300 [00:22<02:15,  1.90it/s]Epoch 17:  15%|█▍        | 44/300 [00:23<02:19,  1.84it/s]Epoch 17:  15%|█▌        | 45/300 [00:23<02:13,  1.91it/s]Epoch 17:  15%|█▌        | 46/300 [00:24<02:09,  1.96it/s]Epoch 17:  16%|█▌        | 47/300 [00:24<02:06,  1.99it/s]Epoch 17:  16%|█▌        | 48/300 [00:25<02:04,  2.02it/s]Epoch 17:  16%|█▋        | 49/300 [00:25<02:16,  1.84it/s]Epoch 17:  17%|█▋        | 50/300 [00:26<02:11,  1.90it/s]Epoch 17:  17%|█▋        | 51/300 [00:26<02:07,  1.95it/s]Epoch 17:  17%|█▋        | 52/300 [00:27<02:07,  1.94it/s]Epoch 17:  18%|█▊        | 53/300 [00:27<02:11,  1.88it/s]Epoch 17:  18%|█▊        | 54/300 [00:28<02:12,  1.86it/s]Epoch 17:  18%|█▊        | 55/300 [00:28<02:07,  1.92it/s]Epoch 17:  19%|█▊        | 56/300 [00:29<02:07,  1.92it/s]Epoch 17:  19%|█▉        | 57/300 [00:30<02:10,  1.87it/s]Epoch 17:  19%|█▉        | 58/300 [00:30<02:05,  1.92it/s]Epoch 17:  20%|█▉        | 59/300 [00:31<02:05,  1.92it/s]06/19/2022 20:32:34 - INFO - __main__ - global step: 2580; train loss: 7.7504425048828125; dev loss: 7.797024726867676
Epoch 17:  20%|██        | 60/300 [00:31<02:01,  1.97it/s]Epoch 17:  20%|██        | 61/300 [00:32<02:06,  1.89it/s]Epoch 17:  21%|██        | 62/300 [00:32<02:02,  1.94it/s]Epoch 17:  21%|██        | 63/300 [00:33<02:05,  1.89it/s]Epoch 17:  21%|██▏       | 64/300 [00:33<02:01,  1.95it/s]Epoch 17:  22%|██▏       | 65/300 [00:34<02:09,  1.81it/s]Epoch 17:  22%|██▏       | 66/300 [00:34<02:10,  1.80it/s]Epoch 17:  22%|██▏       | 67/300 [00:35<02:06,  1.84it/s]Epoch 17:  23%|██▎       | 68/300 [00:35<02:01,  1.92it/s]Epoch 17:  23%|██▎       | 69/300 [00:36<02:03,  1.88it/s]Epoch 17:  23%|██▎       | 70/300 [00:36<02:02,  1.87it/s]Epoch 17:  24%|██▎       | 71/300 [00:37<01:59,  1.92it/s]Epoch 17:  24%|██▍       | 72/300 [00:37<01:56,  1.96it/s]Epoch 17:  24%|██▍       | 73/300 [00:38<01:59,  1.90it/s]Epoch 17:  25%|██▍       | 74/300 [00:39<02:02,  1.85it/s]Epoch 17:  25%|██▌       | 75/300 [00:39<01:59,  1.88it/s]Epoch 17:  25%|██▌       | 76/300 [00:40<01:55,  1.94it/s]Epoch 17:  26%|██▌       | 77/300 [00:40<01:52,  1.99it/s]Epoch 17:  26%|██▌       | 78/300 [00:41<01:58,  1.87it/s]Epoch 17:  26%|██▋       | 79/300 [00:41<01:55,  1.91it/s]06/19/2022 20:32:44 - INFO - __main__ - global step: 2590; train loss: 8.088720321655273; dev loss: 8.021431922912598
Epoch 17:  27%|██▋       | 80/300 [00:42<01:52,  1.96it/s]Epoch 17:  27%|██▋       | 81/300 [00:42<01:49,  2.00it/s]Epoch 17:  27%|██▋       | 82/300 [00:43<01:53,  1.92it/s]Epoch 17:  28%|██▊       | 83/300 [00:43<01:49,  1.98it/s]Epoch 17:  28%|██▊       | 84/300 [00:44<01:47,  2.01it/s]Epoch 17:  28%|██▊       | 85/300 [00:44<01:45,  2.04it/s]Epoch 17:  29%|██▊       | 86/300 [00:45<01:49,  1.95it/s]Epoch 17:  29%|██▉       | 87/300 [00:45<01:49,  1.94it/s]Epoch 17:  29%|██▉       | 88/300 [00:46<01:52,  1.89it/s]Epoch 17:  30%|██▉       | 89/300 [00:46<01:48,  1.94it/s]Epoch 17:  30%|███       | 90/300 [00:47<01:51,  1.88it/s]Epoch 17:  30%|███       | 91/300 [00:47<01:48,  1.93it/s]Epoch 17:  31%|███       | 92/300 [00:48<01:45,  1.97it/s]Epoch 17:  31%|███       | 93/300 [00:48<01:48,  1.91it/s]Epoch 17:  31%|███▏      | 94/300 [00:49<01:50,  1.86it/s]Epoch 17:  32%|███▏      | 95/300 [00:49<01:46,  1.93it/s]Epoch 17:  32%|███▏      | 96/300 [00:50<01:48,  1.88it/s]Epoch 17:  32%|███▏      | 97/300 [00:50<01:44,  1.94it/s]Epoch 17:  33%|███▎      | 98/300 [00:51<01:48,  1.87it/s]Epoch 17:  33%|███▎      | 99/300 [00:52<01:48,  1.85it/s]06/19/2022 20:32:55 - INFO - __main__ - global step: 2600; train loss: 7.674884796142578; dev loss: 7.274628639221191
Epoch 17:  33%|███▎      | 100/300 [00:52<01:45,  1.90it/s]Epoch 17:  34%|███▎      | 101/300 [00:53<01:43,  1.91it/s]Epoch 17:  34%|███▍      | 102/300 [00:53<01:41,  1.96it/s]Epoch 17:  34%|███▍      | 103/300 [00:54<01:44,  1.89it/s]Epoch 17:  35%|███▍      | 104/300 [00:54<01:41,  1.92it/s]Epoch 17:  35%|███▌      | 105/300 [00:55<01:43,  1.88it/s]Epoch 17:  35%|███▌      | 106/300 [00:55<01:40,  1.92it/s]Epoch 17:  36%|███▌      | 107/300 [00:56<01:43,  1.87it/s]Epoch 17:  36%|███▌      | 108/300 [00:56<01:39,  1.92it/s]Epoch 17:  36%|███▋      | 109/300 [00:57<01:36,  1.97it/s]Epoch 17:  37%|███▋      | 110/300 [00:57<01:39,  1.91it/s]Epoch 17:  37%|███▋      | 111/300 [00:58<01:41,  1.85it/s]Epoch 17:  37%|███▋      | 112/300 [00:58<01:38,  1.91it/s]Epoch 17:  38%|███▊      | 113/300 [00:59<01:37,  1.91it/s]Epoch 17:  38%|███▊      | 114/300 [00:59<01:34,  1.96it/s]Epoch 17:  38%|███▊      | 115/300 [01:00<01:41,  1.82it/s]Epoch 17:  39%|███▊      | 116/300 [01:00<01:39,  1.85it/s]Epoch 17:  39%|███▉      | 117/300 [01:01<01:35,  1.92it/s]Epoch 17:  39%|███▉      | 118/300 [01:01<01:32,  1.97it/s]Epoch 17:  40%|███▉      | 119/300 [01:02<01:35,  1.90it/s]06/19/2022 20:33:05 - INFO - __main__ - global step: 2610; train loss: 7.927668571472168; dev loss: 8.189338684082031
Epoch 17:  40%|████      | 120/300 [01:03<01:36,  1.86it/s]Epoch 17:  40%|████      | 121/300 [01:03<01:34,  1.89it/s]Epoch 17:  41%|████      | 122/300 [01:04<01:35,  1.86it/s]Epoch 17:  41%|████      | 123/300 [01:04<01:37,  1.82it/s]Epoch 17:  41%|████▏     | 124/300 [01:05<01:33,  1.89it/s]Epoch 17:  42%|████▏     | 125/300 [01:05<01:30,  1.93it/s]Epoch 17:  42%|████▏     | 126/300 [01:06<01:28,  1.98it/s]Epoch 17:  42%|████▏     | 127/300 [01:06<01:26,  2.00it/s]Epoch 17:  43%|████▎     | 128/300 [01:07<01:30,  1.91it/s]Epoch 17:  43%|████▎     | 129/300 [01:07<01:31,  1.86it/s]Epoch 17:  43%|████▎     | 130/300 [01:08<01:28,  1.92it/s]Epoch 17:  44%|████▎     | 131/300 [01:08<01:25,  1.97it/s]Epoch 17:  44%|████▍     | 132/300 [01:09<01:28,  1.90it/s]Epoch 17:  44%|████▍     | 133/300 [01:09<01:25,  1.95it/s]Epoch 17:  45%|████▍     | 134/300 [01:10<01:25,  1.94it/s]Epoch 17:  45%|████▌     | 135/300 [01:10<01:23,  1.98it/s]Epoch 17:  45%|████▌     | 136/300 [01:11<01:27,  1.87it/s]Epoch 17:  46%|████▌     | 137/300 [01:11<01:24,  1.93it/s]Epoch 17:  46%|████▌     | 138/300 [01:12<01:22,  1.97it/s]Epoch 17:  46%|████▋     | 139/300 [01:12<01:20,  2.01it/s]06/19/2022 20:33:16 - INFO - __main__ - global step: 2620; train loss: 7.7973313331604; dev loss: 7.520294189453125
Epoch 17:  47%|████▋     | 140/300 [01:13<01:24,  1.88it/s]Epoch 17:  47%|████▋     | 141/300 [01:13<01:23,  1.90it/s]Epoch 17:  47%|████▋     | 142/300 [01:14<01:21,  1.95it/s]Epoch 17:  48%|████▊     | 143/300 [01:14<01:18,  1.99it/s]Epoch 17:  48%|████▊     | 144/300 [01:15<01:21,  1.92it/s]Epoch 17:  48%|████▊     | 145/300 [01:15<01:18,  1.97it/s]Epoch 17:  49%|████▊     | 146/300 [01:16<01:19,  1.94it/s]Epoch 17:  49%|████▉     | 147/300 [01:17<01:19,  1.93it/s]Epoch 17:  49%|████▉     | 148/300 [01:17<01:21,  1.86it/s]Epoch 17:  50%|████▉     | 149/300 [01:18<01:20,  1.88it/s]Epoch 17:  50%|█████     | 150/300 [01:18<01:17,  1.94it/s]Epoch 17:  50%|█████     | 151/300 [01:19<01:15,  1.98it/s]Epoch 17:  51%|█████     | 152/300 [01:19<01:19,  1.85it/s]Epoch 17:  51%|█████     | 153/300 [01:20<01:16,  1.92it/s]Epoch 17:  51%|█████▏    | 154/300 [01:20<01:14,  1.96it/s]Epoch 17:  52%|█████▏    | 155/300 [01:21<01:12,  1.99it/s]Epoch 17:  52%|█████▏    | 156/300 [01:21<01:11,  2.01it/s]Epoch 17:  52%|█████▏    | 157/300 [01:22<01:14,  1.92it/s]Epoch 17:  53%|█████▎    | 158/300 [01:22<01:16,  1.85it/s]Epoch 17:  53%|█████▎    | 159/300 [01:23<01:15,  1.88it/s]06/19/2022 20:33:26 - INFO - __main__ - global step: 2630; train loss: 7.4907941818237305; dev loss: 7.608727931976318
Epoch 17:  53%|█████▎    | 160/300 [01:23<01:12,  1.93it/s]Epoch 17:  54%|█████▎    | 161/300 [01:24<01:17,  1.80it/s]Epoch 17:  54%|█████▍    | 162/300 [01:24<01:13,  1.87it/s]Epoch 17:  54%|█████▍    | 163/300 [01:25<01:11,  1.93it/s]Epoch 17:  55%|█████▍    | 164/300 [01:25<01:09,  1.96it/s]Epoch 17:  55%|█████▌    | 165/300 [01:26<01:11,  1.89it/s]Epoch 17:  55%|█████▌    | 166/300 [01:26<01:10,  1.90it/s]Epoch 17:  56%|█████▌    | 167/300 [01:27<01:07,  1.96it/s]Epoch 17:  56%|█████▌    | 168/300 [01:27<01:05,  2.00it/s]Epoch 17:  56%|█████▋    | 169/300 [01:28<01:08,  1.92it/s]Epoch 17:  57%|█████▋    | 170/300 [01:28<01:05,  1.98it/s]Epoch 17:  57%|█████▋    | 171/300 [01:29<01:04,  2.01it/s]Epoch 17:  57%|█████▋    | 172/300 [01:29<01:03,  2.03it/s]Epoch 17:  58%|█████▊    | 173/300 [01:30<01:05,  1.93it/s]Epoch 17:  58%|█████▊    | 174/300 [01:30<01:03,  1.98it/s]Epoch 17:  58%|█████▊    | 175/300 [01:31<01:02,  2.01it/s]Epoch 17:  59%|█████▊    | 176/300 [01:31<01:02,  1.98it/s]Epoch 17:  59%|█████▉    | 177/300 [01:32<01:04,  1.90it/s]Epoch 17:  59%|█████▉    | 178/300 [01:33<01:02,  1.96it/s]Epoch 17:  60%|█████▉    | 179/300 [01:33<01:00,  1.99it/s]06/19/2022 20:33:36 - INFO - __main__ - global step: 2640; train loss: 7.340550899505615; dev loss: 6.9279375076293945
Epoch 17:  60%|██████    | 180/300 [01:34<00:59,  2.02it/s]Epoch 17:  60%|██████    | 181/300 [01:34<01:01,  1.93it/s]Epoch 17:  61%|██████    | 182/300 [01:35<01:06,  1.79it/s]Epoch 17:  61%|██████    | 183/300 [01:35<01:03,  1.83it/s]Epoch 17:  61%|██████▏   | 184/300 [01:36<01:00,  1.91it/s]Epoch 17:  62%|██████▏   | 185/300 [01:36<00:58,  1.96it/s]Epoch 17:  62%|██████▏   | 186/300 [01:37<01:02,  1.82it/s]Epoch 17:  62%|██████▏   | 187/300 [01:37<00:59,  1.89it/s]Epoch 17:  63%|██████▎   | 188/300 [01:38<00:57,  1.95it/s]Epoch 17:  63%|██████▎   | 189/300 [01:38<00:57,  1.94it/s]Epoch 17:  63%|██████▎   | 190/300 [01:39<00:58,  1.89it/s]Epoch 17:  64%|██████▎   | 191/300 [01:39<00:56,  1.93it/s]Epoch 17:  64%|██████▍   | 192/300 [01:40<00:54,  1.98it/s]Epoch 17:  64%|██████▍   | 193/300 [01:40<00:53,  2.00it/s]Epoch 17:  65%|██████▍   | 194/300 [01:41<00:55,  1.91it/s]Epoch 17:  65%|██████▌   | 195/300 [01:41<00:56,  1.87it/s]Epoch 17:  65%|██████▌   | 196/300 [01:42<00:53,  1.93it/s]Epoch 17:  66%|██████▌   | 197/300 [01:42<00:52,  1.97it/s]Epoch 17:  66%|██████▌   | 198/300 [01:43<00:53,  1.89it/s]Epoch 17:  66%|██████▋   | 199/300 [01:43<00:51,  1.95it/s]06/19/2022 20:33:47 - INFO - __main__ - global step: 2650; train loss: 7.93491268157959; dev loss: 8.08574390411377
Epoch 17:  67%|██████▋   | 200/300 [01:44<00:50,  1.98it/s]Epoch 17:  67%|██████▋   | 201/300 [01:44<00:49,  2.01it/s]Epoch 17:  67%|██████▋   | 202/300 [01:45<00:51,  1.92it/s]Epoch 17:  68%|██████▊   | 203/300 [01:45<00:49,  1.98it/s]Epoch 17:  68%|██████▊   | 204/300 [01:46<00:48,  1.99it/s]Epoch 17:  68%|██████▊   | 205/300 [01:46<00:46,  2.03it/s]Epoch 17:  69%|██████▊   | 206/300 [01:47<00:50,  1.85it/s]Epoch 17:  69%|██████▉   | 207/300 [01:48<00:48,  1.92it/s]Epoch 17:  69%|██████▉   | 208/300 [01:48<00:46,  1.97it/s]Epoch 17:  70%|██████▉   | 209/300 [01:49<00:47,  1.91it/s]Epoch 17:  70%|███████   | 210/300 [01:49<00:45,  1.97it/s]Epoch 17:  70%|███████   | 211/300 [01:50<00:47,  1.89it/s]Epoch 17:  71%|███████   | 212/300 [01:50<00:45,  1.95it/s]Epoch 17:  71%|███████   | 213/300 [01:51<00:43,  1.98it/s]Epoch 17:  71%|███████▏  | 214/300 [01:51<00:43,  1.98it/s]Epoch 17:  72%|███████▏  | 215/300 [01:52<00:44,  1.90it/s]Epoch 17:  72%|███████▏  | 216/300 [01:52<00:45,  1.86it/s]Epoch 17:  72%|███████▏  | 217/300 [01:53<00:42,  1.93it/s]Epoch 17:  73%|███████▎  | 218/300 [01:53<00:41,  1.98it/s]Epoch 17:  73%|███████▎  | 219/300 [01:54<00:42,  1.91it/s]06/19/2022 20:33:57 - INFO - __main__ - global step: 2660; train loss: 8.304144859313965; dev loss: 7.843201637268066
Epoch 17:  73%|███████▎  | 220/300 [01:54<00:40,  1.95it/s]Epoch 17:  74%|███████▎  | 221/300 [01:55<00:39,  2.00it/s]Epoch 17:  74%|███████▍  | 222/300 [01:55<00:38,  2.03it/s]Epoch 17:  74%|███████▍  | 223/300 [01:56<00:41,  1.85it/s]Epoch 17:  75%|███████▍  | 224/300 [01:56<00:41,  1.84it/s]Epoch 17:  75%|███████▌  | 225/300 [01:57<00:39,  1.91it/s]Epoch 17:  75%|███████▌  | 226/300 [01:57<00:37,  1.96it/s]Epoch 17:  76%|███████▌  | 227/300 [01:58<00:38,  1.89it/s]Epoch 17:  76%|███████▌  | 228/300 [01:58<00:37,  1.94it/s]Epoch 17:  76%|███████▋  | 229/300 [01:59<00:35,  1.98it/s]Epoch 17:  77%|███████▋  | 230/300 [01:59<00:34,  2.00it/s]Epoch 17:  77%|███████▋  | 231/300 [02:00<00:35,  1.92it/s]Epoch 17:  77%|███████▋  | 232/300 [02:00<00:34,  1.96it/s]Epoch 17:  78%|███████▊  | 233/300 [02:01<00:33,  2.00it/s]Epoch 17:  78%|███████▊  | 234/300 [02:01<00:32,  2.03it/s]Epoch 17:  78%|███████▊  | 235/300 [02:02<00:31,  2.05it/s]Epoch 17:  79%|███████▊  | 236/300 [02:03<00:34,  1.86it/s]Epoch 17:  79%|███████▉  | 237/300 [02:03<00:33,  1.88it/s]Epoch 17:  79%|███████▉  | 238/300 [02:04<00:32,  1.93it/s]Epoch 17:  80%|███████▉  | 239/300 [02:04<00:30,  1.99it/s]06/19/2022 20:34:07 - INFO - __main__ - global step: 2670; train loss: 7.698429107666016; dev loss: 7.838369846343994
Epoch 17:  80%|████████  | 240/300 [02:05<00:31,  1.91it/s]Epoch 17:  80%|████████  | 241/300 [02:05<00:30,  1.96it/s]Epoch 17:  81%|████████  | 242/300 [02:06<00:30,  1.88it/s]Epoch 17:  81%|████████  | 243/300 [02:06<00:29,  1.93it/s]Epoch 17:  81%|████████▏ | 244/300 [02:07<00:29,  1.87it/s]Epoch 17:  82%|████████▏ | 245/300 [02:07<00:28,  1.93it/s]Epoch 17:  82%|████████▏ | 246/300 [02:08<00:27,  1.98it/s]Epoch 17:  82%|████████▏ | 247/300 [02:08<00:26,  2.00it/s]Epoch 17:  83%|████████▎ | 248/300 [02:09<00:27,  1.92it/s]Epoch 17:  83%|████████▎ | 249/300 [02:09<00:27,  1.87it/s]Epoch 17:  83%|████████▎ | 250/300 [02:10<00:25,  1.93it/s]Epoch 17:  84%|████████▎ | 251/300 [02:10<00:24,  1.97it/s]Epoch 17:  84%|████████▍ | 252/300 [02:11<00:25,  1.90it/s]Epoch 17:  84%|████████▍ | 253/300 [02:11<00:24,  1.95it/s]Epoch 17:  85%|████████▍ | 254/300 [02:12<00:23,  2.00it/s]Epoch 17:  85%|████████▌ | 255/300 [02:12<00:22,  2.03it/s]Epoch 17:  85%|████████▌ | 256/300 [02:13<00:22,  1.94it/s]Epoch 17:  86%|████████▌ | 257/300 [02:13<00:21,  1.97it/s]Epoch 17:  86%|████████▌ | 258/300 [02:14<00:21,  1.91it/s]Epoch 17:  86%|████████▋ | 259/300 [02:14<00:20,  1.96it/s]06/19/2022 20:34:18 - INFO - __main__ - global step: 2680; train loss: 7.6189117431640625; dev loss: 7.857862949371338
Epoch 17:  87%|████████▋ | 260/300 [02:15<00:22,  1.80it/s]Epoch 17:  87%|████████▋ | 261/300 [02:15<00:20,  1.88it/s]Epoch 17:  87%|████████▋ | 262/300 [02:16<00:20,  1.88it/s]Epoch 17:  88%|████████▊ | 263/300 [02:17<00:19,  1.93it/s]Epoch 17:  88%|████████▊ | 264/300 [02:17<00:18,  1.96it/s]Epoch 17:  88%|████████▊ | 265/300 [02:18<00:18,  1.88it/s]Epoch 17:  89%|████████▊ | 266/300 [02:18<00:17,  1.89it/s]Epoch 17:  89%|████████▉ | 267/300 [02:19<00:17,  1.86it/s]Epoch 17:  89%|████████▉ | 268/300 [02:19<00:16,  1.92it/s]Epoch 17:  90%|████████▉ | 269/300 [02:20<00:16,  1.88it/s]Epoch 17:  90%|█████████ | 270/300 [02:20<00:15,  1.92it/s]Epoch 17:  90%|█████████ | 271/300 [02:21<00:14,  1.96it/s]Epoch 17:  91%|█████████ | 272/300 [02:21<00:14,  2.00it/s]Epoch 17:  91%|█████████ | 273/300 [02:22<00:14,  1.92it/s]Epoch 17:  91%|█████████▏| 274/300 [02:22<00:13,  1.97it/s]Epoch 17:  92%|█████████▏| 275/300 [02:23<00:12,  2.01it/s]Epoch 17:  92%|█████████▏| 276/300 [02:23<00:11,  2.03it/s]Epoch 17:  92%|█████████▏| 277/300 [02:24<00:11,  1.94it/s]Epoch 17:  93%|█████████▎| 278/300 [02:24<00:11,  1.96it/s]Epoch 17:  93%|█████████▎| 279/300 [02:25<00:11,  1.90it/s]06/19/2022 20:34:28 - INFO - __main__ - global step: 2690; train loss: 7.972405910491943; dev loss: 7.896613121032715
Epoch 17:  93%|█████████▎| 280/300 [02:25<00:10,  1.85it/s]Epoch 17:  94%|█████████▎| 281/300 [02:26<00:10,  1.74it/s]Epoch 17:  94%|█████████▍| 282/300 [02:27<00:09,  1.83it/s]Epoch 17:  94%|█████████▍| 283/300 [02:27<00:09,  1.87it/s]Epoch 17:  95%|█████████▍| 284/300 [02:27<00:08,  1.92it/s]Epoch 17:  95%|█████████▌| 285/300 [02:28<00:08,  1.86it/s]Epoch 17:  95%|█████████▌| 286/300 [02:29<00:07,  1.88it/s]Epoch 17:  96%|█████████▌| 287/300 [02:29<00:06,  1.94it/s]Epoch 17:  96%|█████████▌| 288/300 [02:30<00:06,  1.93it/s]Epoch 17:  96%|█████████▋| 289/300 [02:30<00:05,  1.97it/s]Epoch 17:  97%|█████████▋| 290/300 [02:31<00:05,  1.89it/s]Epoch 17:  97%|█████████▋| 291/300 [02:31<00:04,  1.95it/s]Epoch 17:  97%|█████████▋| 292/300 [02:32<00:04,  1.90it/s]Epoch 17:  98%|█████████▊| 293/300 [02:32<00:03,  1.95it/s]Epoch 17:  98%|█████████▊| 294/300 [02:33<00:03,  1.88it/s]Epoch 17:  98%|█████████▊| 295/300 [02:33<00:02,  1.94it/s]Epoch 17:  99%|█████████▊| 296/300 [02:34<00:02,  1.99it/s]Epoch 17:  99%|█████████▉| 297/300 [02:34<00:01,  1.93it/s]Epoch 17:  99%|█████████▉| 298/300 [02:35<00:01,  1.88it/s]Epoch 17: 100%|█████████▉| 299/300 [02:35<00:00,  1.95it/s]06/19/2022 20:34:39 - INFO - __main__ - global step: 2700; train loss: 8.212430953979492; dev loss: 8.03860092163086
Epoch 17: 100%|██████████| 300/300 [02:36<00:00,  1.89it/s]Epoch 17: 100%|██████████| 300/300 [02:36<00:00,  1.92it/s]
Epoch 18:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 18:   0%|          | 1/300 [00:00<02:19,  2.14it/s]Epoch 18:   1%|          | 2/300 [00:01<02:52,  1.73it/s]Epoch 18:   1%|          | 3/300 [00:01<02:38,  1.87it/s]Epoch 18:   1%|▏         | 4/300 [00:02<02:41,  1.84it/s]Epoch 18:   2%|▏         | 5/300 [00:02<02:33,  1.93it/s]Epoch 18:   2%|▏         | 6/300 [00:03<02:37,  1.86it/s]Epoch 18:   2%|▏         | 7/300 [00:03<02:31,  1.93it/s]Epoch 18:   3%|▎         | 8/300 [00:04<02:36,  1.87it/s]Epoch 18:   3%|▎         | 9/300 [00:04<02:31,  1.92it/s]Epoch 18:   3%|▎         | 10/300 [00:05<02:35,  1.86it/s]Epoch 18:   4%|▎         | 11/300 [00:05<02:30,  1.92it/s]Epoch 18:   4%|▍         | 12/300 [00:06<02:26,  1.96it/s]Epoch 18:   4%|▍         | 13/300 [00:06<02:27,  1.95it/s]Epoch 18:   5%|▍         | 14/300 [00:07<02:35,  1.84it/s]Epoch 18:   5%|▌         | 15/300 [00:07<02:29,  1.90it/s]Epoch 18:   5%|▌         | 16/300 [00:08<02:24,  1.96it/s]Epoch 18:   6%|▌         | 17/300 [00:08<02:28,  1.91it/s]Epoch 18:   6%|▌         | 18/300 [00:09<02:24,  1.95it/s]Epoch 18:   6%|▋         | 19/300 [00:10<02:36,  1.80it/s]06/19/2022 20:34:49 - INFO - __main__ - global step: 2710; train loss: 7.477665901184082; dev loss: 7.161108493804932
Epoch 18:   7%|▋         | 20/300 [00:10<02:29,  1.88it/s]Epoch 18:   7%|▋         | 21/300 [00:11<02:32,  1.83it/s]Epoch 18:   7%|▋         | 22/300 [00:11<02:26,  1.90it/s]Epoch 18:   8%|▊         | 23/300 [00:12<02:29,  1.85it/s]Epoch 18:   8%|▊         | 24/300 [00:12<02:25,  1.90it/s]Epoch 18:   8%|▊         | 25/300 [00:13<02:24,  1.91it/s]Epoch 18:   9%|▊         | 26/300 [00:13<02:20,  1.95it/s]Epoch 18:   9%|▉         | 27/300 [00:14<02:24,  1.89it/s]Epoch 18:   9%|▉         | 28/300 [00:14<02:26,  1.85it/s]Epoch 18:  10%|▉         | 29/300 [00:15<02:22,  1.91it/s]Epoch 18:  10%|█         | 30/300 [00:15<02:18,  1.95it/s]Epoch 18:  10%|█         | 31/300 [00:16<02:23,  1.87it/s]Epoch 18:  11%|█         | 32/300 [00:16<02:18,  1.93it/s]Epoch 18:  11%|█         | 33/300 [00:17<02:14,  1.99it/s]Epoch 18:  11%|█▏        | 34/300 [00:17<02:15,  1.97it/s]Epoch 18:  12%|█▏        | 35/300 [00:18<02:19,  1.90it/s]Epoch 18:  12%|█▏        | 36/300 [00:18<02:18,  1.91it/s]Epoch 18:  12%|█▏        | 37/300 [00:19<02:15,  1.93it/s]Epoch 18:  13%|█▎        | 38/300 [00:20<02:19,  1.88it/s]Epoch 18:  13%|█▎        | 39/300 [00:20<02:25,  1.80it/s]06/19/2022 20:35:00 - INFO - __main__ - global step: 2720; train loss: 7.686210632324219; dev loss: 7.694114685058594
Epoch 18:  13%|█▎        | 40/300 [00:21<02:25,  1.79it/s]Epoch 18:  14%|█▎        | 41/300 [00:21<02:18,  1.87it/s]Epoch 18:  14%|█▍        | 42/300 [00:22<02:13,  1.93it/s]Epoch 18:  14%|█▍        | 43/300 [00:22<02:11,  1.96it/s]Epoch 18:  15%|█▍        | 44/300 [00:23<02:19,  1.84it/s]Epoch 18:  15%|█▌        | 45/300 [00:23<02:13,  1.91it/s]Epoch 18:  15%|█▌        | 46/300 [00:24<02:10,  1.95it/s]Epoch 18:  16%|█▌        | 47/300 [00:24<02:06,  2.00it/s]Epoch 18:  16%|█▌        | 48/300 [00:25<02:14,  1.88it/s]Epoch 18:  16%|█▋        | 49/300 [00:25<02:09,  1.94it/s]Epoch 18:  17%|█▋        | 50/300 [00:26<02:06,  1.98it/s]Epoch 18:  17%|█▋        | 51/300 [00:26<02:02,  2.02it/s]Epoch 18:  17%|█▋        | 52/300 [00:27<02:14,  1.85it/s]Epoch 18:  18%|█▊        | 53/300 [00:27<02:08,  1.92it/s]Epoch 18:  18%|█▊        | 54/300 [00:28<02:04,  1.97it/s]Epoch 18:  18%|█▊        | 55/300 [00:28<02:01,  2.02it/s]Epoch 18:  19%|█▊        | 56/300 [00:29<02:12,  1.85it/s]Epoch 18:  19%|█▉        | 57/300 [00:30<02:13,  1.82it/s]Epoch 18:  19%|█▉        | 58/300 [00:30<02:11,  1.85it/s]Epoch 18:  20%|█▉        | 59/300 [00:31<02:08,  1.87it/s]06/19/2022 20:35:10 - INFO - __main__ - global step: 2730; train loss: 7.822184085845947; dev loss: 7.459641456604004
Epoch 18:  20%|██        | 60/300 [00:31<02:10,  1.84it/s]Epoch 18:  20%|██        | 61/300 [00:32<02:05,  1.91it/s]Epoch 18:  21%|██        | 62/300 [00:32<02:07,  1.87it/s]Epoch 18:  21%|██        | 63/300 [00:33<02:04,  1.90it/s]Epoch 18:  21%|██▏       | 64/300 [00:33<02:07,  1.85it/s]Epoch 18:  22%|██▏       | 65/300 [00:34<02:03,  1.91it/s]Epoch 18:  22%|██▏       | 66/300 [00:34<01:59,  1.96it/s]Epoch 18:  22%|██▏       | 67/300 [00:35<01:58,  1.96it/s]Epoch 18:  23%|██▎       | 68/300 [00:35<02:02,  1.89it/s]Epoch 18:  23%|██▎       | 69/300 [00:36<01:59,  1.94it/s]Epoch 18:  23%|██▎       | 70/300 [00:36<02:01,  1.89it/s]Epoch 18:  24%|██▎       | 71/300 [00:37<01:57,  1.95it/s]Epoch 18:  24%|██▍       | 72/300 [00:37<01:57,  1.94it/s]Epoch 18:  24%|██▍       | 73/300 [00:38<02:00,  1.88it/s]Epoch 18:  25%|██▍       | 74/300 [00:38<01:56,  1.94it/s]Epoch 18:  25%|██▌       | 75/300 [00:39<01:53,  1.98it/s]Epoch 18:  25%|██▌       | 76/300 [00:39<01:51,  2.02it/s]Epoch 18:  26%|██▌       | 77/300 [00:40<01:56,  1.92it/s]Epoch 18:  26%|██▌       | 78/300 [00:40<01:52,  1.97it/s]Epoch 18:  26%|██▋       | 79/300 [00:41<01:55,  1.91it/s]06/19/2022 20:35:20 - INFO - __main__ - global step: 2740; train loss: 7.620455741882324; dev loss: 7.803328514099121
Epoch 18:  27%|██▋       | 80/300 [00:41<01:52,  1.95it/s]Epoch 18:  27%|██▋       | 81/300 [00:42<01:58,  1.84it/s]Epoch 18:  27%|██▋       | 82/300 [00:43<01:54,  1.90it/s]Epoch 18:  28%|██▊       | 83/300 [00:43<01:51,  1.95it/s]Epoch 18:  28%|██▊       | 84/300 [00:44<01:51,  1.94it/s]Epoch 18:  28%|██▊       | 85/300 [00:44<01:54,  1.88it/s]Epoch 18:  29%|██▊       | 86/300 [00:45<01:50,  1.93it/s]Epoch 18:  29%|██▉       | 87/300 [00:45<01:48,  1.97it/s]Epoch 18:  29%|██▉       | 88/300 [00:46<01:48,  1.95it/s]Epoch 18:  30%|██▉       | 89/300 [00:46<01:52,  1.88it/s]Epoch 18:  30%|███       | 90/300 [00:47<01:50,  1.89it/s]Epoch 18:  30%|███       | 91/300 [00:47<01:47,  1.94it/s]Epoch 18:  31%|███       | 92/300 [00:48<01:44,  1.98it/s]Epoch 18:  31%|███       | 93/300 [00:48<01:48,  1.91it/s]Epoch 18:  31%|███▏      | 94/300 [00:49<01:45,  1.96it/s]Epoch 18:  32%|███▏      | 95/300 [00:49<01:42,  1.99it/s]Epoch 18:  32%|███▏      | 96/300 [00:50<01:41,  2.01it/s]Epoch 18:  32%|███▏      | 97/300 [00:50<01:40,  2.02it/s]Epoch 18:  33%|███▎      | 98/300 [00:51<01:45,  1.92it/s]Epoch 18:  33%|███▎      | 99/300 [00:51<01:44,  1.93it/s]06/19/2022 20:35:31 - INFO - __main__ - global step: 2750; train loss: 7.525259971618652; dev loss: 7.284796237945557
Epoch 18:  33%|███▎      | 100/300 [00:52<01:42,  1.94it/s]Epoch 18:  34%|███▎      | 101/300 [00:52<01:40,  1.99it/s]Epoch 18:  34%|███▍      | 102/300 [00:53<01:44,  1.90it/s]Epoch 18:  34%|███▍      | 103/300 [00:53<01:41,  1.94it/s]Epoch 18:  35%|███▍      | 104/300 [00:54<01:38,  1.99it/s]Epoch 18:  35%|███▌      | 105/300 [00:54<01:39,  1.96it/s]Epoch 18:  35%|███▌      | 106/300 [00:55<01:43,  1.87it/s]Epoch 18:  36%|███▌      | 107/300 [00:55<01:40,  1.92it/s]Epoch 18:  36%|███▌      | 108/300 [00:56<01:37,  1.97it/s]Epoch 18:  36%|███▋      | 109/300 [00:56<01:35,  2.00it/s]Epoch 18:  37%|███▋      | 110/300 [00:57<01:39,  1.92it/s]Epoch 18:  37%|███▋      | 111/300 [00:57<01:41,  1.87it/s]Epoch 18:  37%|███▋      | 112/300 [00:58<01:37,  1.93it/s]Epoch 18:  38%|███▊      | 113/300 [00:58<01:34,  1.98it/s]Epoch 18:  38%|███▊      | 114/300 [00:59<01:42,  1.82it/s]Epoch 18:  38%|███▊      | 115/300 [01:00<01:37,  1.90it/s]Epoch 18:  39%|███▊      | 116/300 [01:00<01:34,  1.95it/s]Epoch 18:  39%|███▉      | 117/300 [01:01<01:33,  1.96it/s]Epoch 18:  39%|███▉      | 118/300 [01:01<01:40,  1.81it/s]Epoch 18:  40%|███▉      | 119/300 [01:02<01:36,  1.88it/s]06/19/2022 20:35:41 - INFO - __main__ - global step: 2760; train loss: 7.588785648345947; dev loss: 7.356899261474609
Epoch 18:  40%|████      | 120/300 [01:02<01:32,  1.94it/s]Epoch 18:  40%|████      | 121/300 [01:03<01:34,  1.89it/s]Epoch 18:  41%|████      | 122/300 [01:03<01:36,  1.85it/s]Epoch 18:  41%|████      | 123/300 [01:04<01:34,  1.87it/s]Epoch 18:  41%|████▏     | 124/300 [01:04<01:31,  1.93it/s]Epoch 18:  42%|████▏     | 125/300 [01:05<01:28,  1.97it/s]Epoch 18:  42%|████▏     | 126/300 [01:05<01:29,  1.95it/s]Epoch 18:  42%|████▏     | 127/300 [01:06<01:32,  1.88it/s]Epoch 18:  43%|████▎     | 128/300 [01:06<01:28,  1.93it/s]Epoch 18:  43%|████▎     | 129/300 [01:07<01:30,  1.88it/s]Epoch 18:  43%|████▎     | 130/300 [01:07<01:28,  1.93it/s]Epoch 18:  44%|████▎     | 131/300 [01:08<01:34,  1.78it/s]Epoch 18:  44%|████▍     | 132/300 [01:09<01:30,  1.86it/s]Epoch 18:  44%|████▍     | 133/300 [01:09<01:29,  1.86it/s]Epoch 18:  45%|████▍     | 134/300 [01:10<01:26,  1.92it/s]Epoch 18:  45%|████▌     | 135/300 [01:10<01:32,  1.78it/s]Epoch 18:  45%|████▌     | 136/300 [01:11<01:27,  1.86it/s]Epoch 18:  46%|████▌     | 137/300 [01:11<01:28,  1.84it/s]Epoch 18:  46%|████▌     | 138/300 [01:12<01:27,  1.86it/s]Epoch 18:  46%|████▋     | 139/300 [01:12<01:28,  1.83it/s]06/19/2022 20:35:52 - INFO - __main__ - global step: 2770; train loss: 8.308340072631836; dev loss: 8.078226089477539
Epoch 18:  47%|████▋     | 140/300 [01:13<01:26,  1.85it/s]Epoch 18:  47%|████▋     | 141/300 [01:13<01:22,  1.92it/s]Epoch 18:  47%|████▋     | 142/300 [01:14<01:22,  1.92it/s]Epoch 18:  48%|████▊     | 143/300 [01:14<01:24,  1.86it/s]Epoch 18:  48%|████▊     | 144/300 [01:15<01:23,  1.87it/s]Epoch 18:  48%|████▊     | 145/300 [01:15<01:20,  1.93it/s]Epoch 18:  49%|████▊     | 146/300 [01:16<01:21,  1.89it/s]Epoch 18:  49%|████▉     | 147/300 [01:17<01:24,  1.80it/s]Epoch 18:  49%|████▉     | 148/300 [01:17<01:20,  1.88it/s]Epoch 18:  50%|████▉     | 149/300 [01:18<01:17,  1.94it/s]Epoch 18:  50%|█████     | 150/300 [01:18<01:17,  1.93it/s]Epoch 18:  50%|█████     | 151/300 [01:19<01:19,  1.88it/s]Epoch 18:  51%|█████     | 152/300 [01:19<01:24,  1.76it/s]Epoch 18:  51%|█████     | 153/300 [01:20<01:19,  1.84it/s]Epoch 18:  51%|█████▏    | 154/300 [01:20<01:16,  1.91it/s]Epoch 18:  52%|█████▏    | 155/300 [01:21<01:17,  1.86it/s]Epoch 18:  52%|█████▏    | 156/300 [01:21<01:18,  1.84it/s]Epoch 18:  52%|█████▏    | 157/300 [01:22<01:18,  1.82it/s]Epoch 18:  53%|█████▎    | 158/300 [01:23<01:18,  1.81it/s]Epoch 18:  53%|█████▎    | 159/300 [01:23<01:16,  1.84it/s]06/19/2022 20:36:03 - INFO - __main__ - global step: 2780; train loss: 7.348090171813965; dev loss: 7.529979705810547
Epoch 18:  53%|█████▎    | 160/300 [01:24<01:17,  1.81it/s]Epoch 18:  54%|█████▎    | 161/300 [01:24<01:13,  1.88it/s]Epoch 18:  54%|█████▍    | 162/300 [01:25<01:11,  1.92it/s]Epoch 18:  54%|█████▍    | 163/300 [01:25<01:12,  1.88it/s]Epoch 18:  55%|█████▍    | 164/300 [01:26<01:14,  1.84it/s]Epoch 18:  55%|█████▌    | 165/300 [01:26<01:12,  1.86it/s]Epoch 18:  55%|█████▌    | 166/300 [01:27<01:12,  1.84it/s]Epoch 18:  56%|█████▌    | 167/300 [01:27<01:11,  1.85it/s]Epoch 18:  56%|█████▌    | 168/300 [01:28<01:12,  1.82it/s]Epoch 18:  56%|█████▋    | 169/300 [01:28<01:09,  1.89it/s]Epoch 18:  57%|█████▋    | 170/300 [01:29<01:07,  1.94it/s]Epoch 18:  57%|█████▋    | 171/300 [01:29<01:05,  1.98it/s]Epoch 18:  57%|█████▋    | 172/300 [01:30<01:07,  1.91it/s]Epoch 18:  58%|█████▊    | 173/300 [01:30<01:04,  1.96it/s]Epoch 18:  58%|█████▊    | 174/300 [01:31<01:03,  2.00it/s]Epoch 18:  58%|█████▊    | 175/300 [01:31<01:01,  2.03it/s]Epoch 18:  59%|█████▊    | 176/300 [01:32<01:04,  1.93it/s]Epoch 18:  59%|█████▉    | 177/300 [01:32<01:03,  1.93it/s]Epoch 18:  59%|█████▉    | 178/300 [01:33<01:03,  1.92it/s]Epoch 18:  60%|█████▉    | 179/300 [01:33<01:01,  1.97it/s]06/19/2022 20:36:13 - INFO - __main__ - global step: 2790; train loss: 8.128925323486328; dev loss: 8.303223609924316
Epoch 18:  60%|██████    | 180/300 [01:34<01:01,  1.94it/s]Epoch 18:  60%|██████    | 181/300 [01:35<01:05,  1.82it/s]Epoch 18:  61%|██████    | 182/300 [01:35<01:02,  1.89it/s]Epoch 18:  61%|██████    | 183/300 [01:36<01:00,  1.94it/s]Epoch 18:  61%|██████▏   | 184/300 [01:36<00:58,  1.99it/s]Epoch 18:  62%|██████▏   | 185/300 [01:37<01:01,  1.88it/s]Epoch 18:  62%|██████▏   | 186/300 [01:37<00:59,  1.93it/s]Epoch 18:  62%|██████▏   | 187/300 [01:38<00:57,  1.97it/s]Epoch 18:  63%|██████▎   | 188/300 [01:38<00:55,  2.00it/s]Epoch 18:  63%|██████▎   | 189/300 [01:39<00:58,  1.91it/s]Epoch 18:  63%|██████▎   | 190/300 [01:39<00:57,  1.93it/s]Epoch 18:  64%|██████▎   | 191/300 [01:40<00:57,  1.89it/s]Epoch 18:  64%|██████▍   | 192/300 [01:40<00:55,  1.94it/s]Epoch 18:  64%|██████▍   | 193/300 [01:41<00:57,  1.87it/s]Epoch 18:  65%|██████▍   | 194/300 [01:41<00:54,  1.93it/s]Epoch 18:  65%|██████▌   | 195/300 [01:42<00:54,  1.94it/s]Epoch 18:  65%|██████▌   | 196/300 [01:42<00:53,  1.95it/s]Epoch 18:  66%|██████▌   | 197/300 [01:43<00:57,  1.80it/s]Epoch 18:  66%|██████▌   | 198/300 [01:43<00:54,  1.87it/s]Epoch 18:  66%|██████▋   | 199/300 [01:44<00:52,  1.93it/s]06/19/2022 20:36:23 - INFO - __main__ - global step: 2800; train loss: 8.123010635375977; dev loss: 8.322786331176758
Epoch 18:  67%|██████▋   | 200/300 [01:44<00:50,  1.96it/s]Epoch 18:  67%|██████▋   | 201/300 [01:45<00:52,  1.89it/s]Epoch 18:  67%|██████▋   | 202/300 [01:46<00:52,  1.86it/s]Epoch 18:  68%|██████▊   | 203/300 [01:46<00:51,  1.88it/s]Epoch 18:  68%|██████▊   | 204/300 [01:47<00:50,  1.92it/s]Epoch 18:  68%|██████▊   | 205/300 [01:47<00:49,  1.92it/s]Epoch 18:  69%|██████▊   | 206/300 [01:48<00:50,  1.86it/s]Epoch 18:  69%|██████▉   | 207/300 [01:48<00:49,  1.88it/s]Epoch 18:  69%|██████▉   | 208/300 [01:49<00:48,  1.89it/s]Epoch 18:  70%|██████▉   | 209/300 [01:49<00:46,  1.95it/s]Epoch 18:  70%|███████   | 210/300 [01:50<00:48,  1.87it/s]Epoch 18:  70%|███████   | 211/300 [01:50<00:45,  1.94it/s]Epoch 18:  71%|███████   | 212/300 [01:51<00:44,  1.98it/s]Epoch 18:  71%|███████   | 213/300 [01:51<00:43,  2.01it/s]Epoch 18:  71%|███████▏  | 214/300 [01:52<00:44,  1.92it/s]Epoch 18:  72%|███████▏  | 215/300 [01:52<00:45,  1.88it/s]Epoch 18:  72%|███████▏  | 216/300 [01:53<00:43,  1.94it/s]Epoch 18:  72%|███████▏  | 217/300 [01:53<00:41,  1.98it/s]Epoch 18:  73%|███████▎  | 218/300 [01:54<00:42,  1.91it/s]Epoch 18:  73%|███████▎  | 219/300 [01:54<00:41,  1.96it/s]06/19/2022 20:36:34 - INFO - __main__ - global step: 2810; train loss: 8.196009635925293; dev loss: 7.924513339996338
Epoch 18:  73%|███████▎  | 220/300 [01:55<00:42,  1.89it/s]Epoch 18:  74%|███████▎  | 221/300 [01:55<00:41,  1.90it/s]Epoch 18:  74%|███████▍  | 222/300 [01:56<00:42,  1.85it/s]Epoch 18:  74%|███████▍  | 223/300 [01:57<00:40,  1.91it/s]Epoch 18:  75%|███████▍  | 224/300 [01:57<00:38,  1.95it/s]Epoch 18:  75%|███████▌  | 225/300 [01:57<00:37,  1.99it/s]Epoch 18:  75%|███████▌  | 226/300 [01:58<00:39,  1.90it/s]Epoch 18:  76%|███████▌  | 227/300 [01:59<00:37,  1.94it/s]Epoch 18:  76%|███████▌  | 228/300 [01:59<00:37,  1.93it/s]Epoch 18:  76%|███████▋  | 229/300 [02:00<00:36,  1.97it/s]Epoch 18:  77%|███████▋  | 230/300 [02:00<00:36,  1.89it/s]Epoch 18:  77%|███████▋  | 231/300 [02:01<00:37,  1.86it/s]Epoch 18:  77%|███████▋  | 232/300 [02:01<00:35,  1.92it/s]Epoch 18:  78%|███████▊  | 233/300 [02:02<00:33,  1.97it/s]Epoch 18:  78%|███████▊  | 234/300 [02:02<00:33,  1.96it/s]Epoch 18:  78%|███████▊  | 235/300 [02:03<00:34,  1.89it/s]Epoch 18:  79%|███████▊  | 236/300 [02:03<00:32,  1.95it/s]Epoch 18:  79%|███████▉  | 237/300 [02:04<00:31,  1.99it/s]Epoch 18:  79%|███████▉  | 238/300 [02:04<00:31,  1.97it/s]Epoch 18:  80%|███████▉  | 239/300 [02:05<00:32,  1.90it/s]06/19/2022 20:36:44 - INFO - __main__ - global step: 2820; train loss: 8.099306106567383; dev loss: 8.015623092651367
Epoch 18:  80%|████████  | 240/300 [02:05<00:30,  1.95it/s]Epoch 18:  80%|████████  | 241/300 [02:06<00:29,  1.99it/s]Epoch 18:  81%|████████  | 242/300 [02:06<00:28,  2.03it/s]Epoch 18:  81%|████████  | 243/300 [02:07<00:29,  1.93it/s]Epoch 18:  81%|████████▏ | 244/300 [02:07<00:29,  1.93it/s]Epoch 18:  82%|████████▏ | 245/300 [02:08<00:27,  1.98it/s]Epoch 18:  82%|████████▏ | 246/300 [02:08<00:26,  2.01it/s]Epoch 18:  82%|████████▏ | 247/300 [02:09<00:27,  1.89it/s]Epoch 18:  83%|████████▎ | 248/300 [02:09<00:26,  1.95it/s]Epoch 18:  83%|████████▎ | 249/300 [02:10<00:26,  1.96it/s]Epoch 18:  83%|████████▎ | 250/300 [02:10<00:25,  1.99it/s]Epoch 18:  84%|████████▎ | 251/300 [02:11<00:25,  1.91it/s]Epoch 18:  84%|████████▍ | 252/300 [02:11<00:24,  1.96it/s]Epoch 18:  84%|████████▍ | 253/300 [02:12<00:23,  2.00it/s]Epoch 18:  85%|████████▍ | 254/300 [02:12<00:22,  2.02it/s]Epoch 18:  85%|████████▌ | 255/300 [02:13<00:23,  1.93it/s]Epoch 18:  85%|████████▌ | 256/300 [02:13<00:22,  1.98it/s]Epoch 18:  86%|████████▌ | 257/300 [02:14<00:21,  2.00it/s]Epoch 18:  86%|████████▌ | 258/300 [02:14<00:20,  2.01it/s]Epoch 18:  86%|████████▋ | 259/300 [02:15<00:21,  1.93it/s]06/19/2022 20:36:55 - INFO - __main__ - global step: 2830; train loss: 7.3839874267578125; dev loss: 7.665435791015625
Epoch 18:  87%|████████▋ | 260/300 [02:15<00:21,  1.87it/s]Epoch 18:  87%|████████▋ | 261/300 [02:16<00:20,  1.93it/s]Epoch 18:  87%|████████▋ | 262/300 [02:16<00:19,  1.97it/s]Epoch 18:  88%|████████▊ | 263/300 [02:17<00:18,  1.99it/s]Epoch 18:  88%|████████▊ | 264/300 [02:18<00:18,  1.91it/s]Epoch 18:  88%|████████▊ | 265/300 [02:18<00:17,  1.97it/s]Epoch 18:  89%|████████▊ | 266/300 [02:18<00:16,  2.01it/s]Epoch 18:  89%|████████▉ | 267/300 [02:19<00:16,  2.02it/s]Epoch 18:  89%|████████▉ | 268/300 [02:20<00:16,  1.93it/s]Epoch 18:  90%|████████▉ | 269/300 [02:20<00:15,  1.94it/s]Epoch 18:  90%|█████████ | 270/300 [02:21<00:15,  1.93it/s]Epoch 18:  90%|█████████ | 271/300 [02:21<00:14,  1.97it/s]Epoch 18:  91%|█████████ | 272/300 [02:22<00:14,  1.90it/s]Epoch 18:  91%|█████████ | 273/300 [02:22<00:14,  1.86it/s]Epoch 18:  91%|█████████▏| 274/300 [02:23<00:13,  1.92it/s]Epoch 18:  92%|█████████▏| 275/300 [02:23<00:12,  1.97it/s]Epoch 18:  92%|█████████▏| 276/300 [02:24<00:12,  1.89it/s]Epoch 18:  92%|█████████▏| 277/300 [02:24<00:11,  1.95it/s]Epoch 18:  93%|█████████▎| 278/300 [02:25<00:11,  1.99it/s]Epoch 18:  93%|█████████▎| 279/300 [02:25<00:10,  1.92it/s]06/19/2022 20:37:05 - INFO - __main__ - global step: 2840; train loss: 7.724825859069824; dev loss: 8.349364280700684
Epoch 18:  93%|█████████▎| 280/300 [02:26<00:10,  1.86it/s]Epoch 18:  94%|█████████▎| 281/300 [02:26<00:10,  1.84it/s]Epoch 18:  94%|█████████▍| 282/300 [02:27<00:09,  1.91it/s]Epoch 18:  94%|█████████▍| 283/300 [02:27<00:09,  1.87it/s]Epoch 18:  95%|█████████▍| 284/300 [02:28<00:08,  1.82it/s]Epoch 18:  95%|█████████▌| 285/300 [02:28<00:07,  1.89it/s]Epoch 18:  95%|█████████▌| 286/300 [02:29<00:07,  1.86it/s]Epoch 18:  96%|█████████▌| 287/300 [02:30<00:06,  1.92it/s]Epoch 18:  96%|█████████▌| 288/300 [02:30<00:06,  1.92it/s]Epoch 18:  96%|█████████▋| 289/300 [02:31<00:05,  1.86it/s]Epoch 18:  97%|█████████▋| 290/300 [02:31<00:05,  1.91it/s]Epoch 18:  97%|█████████▋| 291/300 [02:32<00:04,  1.96it/s]Epoch 18:  97%|█████████▋| 292/300 [02:32<00:04,  1.90it/s]Epoch 18:  98%|█████████▊| 293/300 [02:33<00:03,  1.85it/s]Epoch 18:  98%|█████████▊| 294/300 [02:33<00:03,  1.87it/s]Epoch 18:  98%|█████████▊| 295/300 [02:34<00:02,  1.94it/s]Epoch 18:  99%|█████████▊| 296/300 [02:34<00:02,  1.98it/s]Epoch 18:  99%|█████████▉| 297/300 [02:35<00:01,  1.82it/s]Epoch 18:  99%|█████████▉| 298/300 [02:35<00:01,  1.89it/s]Epoch 18: 100%|█████████▉| 299/300 [02:36<00:00,  1.86it/s]06/19/2022 20:37:15 - INFO - __main__ - global step: 2850; train loss: 7.2244367599487305; dev loss: 7.169730186462402
Epoch 18: 100%|██████████| 300/300 [02:36<00:00,  1.93it/s]Epoch 18: 100%|██████████| 300/300 [02:36<00:00,  1.91it/s]
Epoch 19:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 19:   0%|          | 1/300 [00:00<02:51,  1.74it/s]Epoch 19:   1%|          | 2/300 [00:01<02:35,  1.92it/s]Epoch 19:   1%|          | 3/300 [00:01<02:30,  1.98it/s]Epoch 19:   1%|▏         | 4/300 [00:02<02:26,  2.02it/s]Epoch 19:   2%|▏         | 5/300 [00:02<02:35,  1.90it/s]Epoch 19:   2%|▏         | 6/300 [00:03<02:30,  1.95it/s]Epoch 19:   2%|▏         | 7/300 [00:03<02:27,  1.99it/s]Epoch 19:   3%|▎         | 8/300 [00:04<02:24,  2.02it/s]Epoch 19:   3%|▎         | 9/300 [00:04<02:31,  1.92it/s]Epoch 19:   3%|▎         | 10/300 [00:05<02:27,  1.97it/s]Epoch 19:   4%|▎         | 11/300 [00:05<02:26,  1.97it/s]Epoch 19:   4%|▍         | 12/300 [00:06<02:24,  1.99it/s]Epoch 19:   4%|▍         | 13/300 [00:06<02:22,  2.02it/s]Epoch 19:   5%|▍         | 14/300 [00:07<02:35,  1.84it/s]Epoch 19:   5%|▌         | 15/300 [00:07<02:29,  1.90it/s]Epoch 19:   5%|▌         | 16/300 [00:08<02:25,  1.95it/s]Epoch 19:   6%|▌         | 17/300 [00:08<02:25,  1.94it/s]Epoch 19:   6%|▌         | 18/300 [00:09<02:30,  1.88it/s]Epoch 19:   6%|▋         | 19/300 [00:09<02:25,  1.93it/s]06/19/2022 20:37:26 - INFO - __main__ - global step: 2860; train loss: 7.662442684173584; dev loss: 7.470175743103027
Epoch 19:   7%|▋         | 20/300 [00:10<02:21,  1.97it/s]Epoch 19:   7%|▋         | 21/300 [00:10<02:22,  1.96it/s]Epoch 19:   7%|▋         | 22/300 [00:11<02:30,  1.84it/s]Epoch 19:   8%|▊         | 23/300 [00:11<02:24,  1.92it/s]Epoch 19:   8%|▊         | 24/300 [00:12<02:26,  1.88it/s]Epoch 19:   8%|▊         | 25/300 [00:12<02:21,  1.95it/s]Epoch 19:   9%|▊         | 26/300 [00:13<02:25,  1.89it/s]Epoch 19:   9%|▉         | 27/300 [00:13<02:19,  1.95it/s]Epoch 19:   9%|▉         | 28/300 [00:14<02:19,  1.95it/s]Epoch 19:  10%|▉         | 29/300 [00:14<02:15,  2.00it/s]Epoch 19:  10%|█         | 30/300 [00:15<02:20,  1.92it/s]Epoch 19:  10%|█         | 31/300 [00:16<02:20,  1.92it/s]Epoch 19:  11%|█         | 32/300 [00:16<02:16,  1.96it/s]Epoch 19:  11%|█         | 33/300 [00:17<02:16,  1.95it/s]Epoch 19:  11%|█▏        | 34/300 [00:17<02:27,  1.80it/s]Epoch 19:  12%|█▏        | 35/300 [00:18<02:21,  1.88it/s]Epoch 19:  12%|█▏        | 36/300 [00:18<02:23,  1.85it/s]Epoch 19:  12%|█▏        | 37/300 [00:19<02:17,  1.91it/s]Epoch 19:  13%|█▎        | 38/300 [00:19<02:21,  1.85it/s]Epoch 19:  13%|█▎        | 39/300 [00:20<02:16,  1.91it/s]06/19/2022 20:37:36 - INFO - __main__ - global step: 2870; train loss: 7.3173394203186035; dev loss: 7.397304534912109
Epoch 19:  13%|█▎        | 40/300 [00:20<02:13,  1.95it/s]Epoch 19:  14%|█▎        | 41/300 [00:21<02:10,  1.99it/s]Epoch 19:  14%|█▍        | 42/300 [00:21<02:11,  1.97it/s]Epoch 19:  14%|█▍        | 43/300 [00:22<02:15,  1.89it/s]Epoch 19:  15%|█▍        | 44/300 [00:22<02:11,  1.94it/s]Epoch 19:  15%|█▌        | 45/300 [00:23<02:08,  1.98it/s]Epoch 19:  15%|█▌        | 46/300 [00:23<02:06,  2.02it/s]Epoch 19:  16%|█▌        | 47/300 [00:24<02:10,  1.93it/s]Epoch 19:  16%|█▌        | 48/300 [00:24<02:07,  1.98it/s]Epoch 19:  16%|█▋        | 49/300 [00:25<02:04,  2.01it/s]Epoch 19:  17%|█▋        | 50/300 [00:25<02:06,  1.97it/s]Epoch 19:  17%|█▋        | 51/300 [00:26<02:11,  1.90it/s]Epoch 19:  17%|█▋        | 52/300 [00:26<02:07,  1.94it/s]Epoch 19:  18%|█▊        | 53/300 [00:27<02:04,  1.98it/s]Epoch 19:  18%|█▊        | 54/300 [00:27<02:09,  1.90it/s]Epoch 19:  18%|█▊        | 55/300 [00:28<02:13,  1.83it/s]Epoch 19:  19%|█▊        | 56/300 [00:29<02:14,  1.81it/s]Epoch 19:  19%|█▉        | 57/300 [00:29<02:09,  1.87it/s]Epoch 19:  19%|█▉        | 58/300 [00:30<02:05,  1.93it/s]Epoch 19:  20%|█▉        | 59/300 [00:30<02:10,  1.85it/s]06/19/2022 20:37:47 - INFO - __main__ - global step: 2880; train loss: 7.942767143249512; dev loss: 8.220438003540039
Epoch 19:  20%|██        | 60/300 [00:31<02:11,  1.83it/s]Epoch 19:  20%|██        | 61/300 [00:31<02:08,  1.86it/s]Epoch 19:  21%|██        | 62/300 [00:32<02:03,  1.93it/s]Epoch 19:  21%|██        | 63/300 [00:32<02:06,  1.87it/s]Epoch 19:  21%|██▏       | 64/300 [00:33<02:04,  1.89it/s]Epoch 19:  22%|██▏       | 65/300 [00:33<02:03,  1.90it/s]Epoch 19:  22%|██▏       | 66/300 [00:34<02:00,  1.95it/s]Epoch 19:  22%|██▏       | 67/300 [00:34<01:57,  1.98it/s]Epoch 19:  23%|██▎       | 68/300 [00:35<02:01,  1.91it/s]Epoch 19:  23%|██▎       | 69/300 [00:35<01:57,  1.97it/s]Epoch 19:  23%|██▎       | 70/300 [00:36<01:54,  2.00it/s]Epoch 19:  24%|██▎       | 71/300 [00:36<01:52,  2.03it/s]Epoch 19:  24%|██▍       | 72/300 [00:37<01:57,  1.95it/s]Epoch 19:  24%|██▍       | 73/300 [00:37<01:54,  1.98it/s]Epoch 19:  25%|██▍       | 74/300 [00:38<01:52,  2.01it/s]Epoch 19:  25%|██▌       | 75/300 [00:38<01:51,  2.02it/s]Epoch 19:  25%|██▌       | 76/300 [00:39<01:56,  1.93it/s]Epoch 19:  26%|██▌       | 77/300 [00:39<01:52,  1.97it/s]Epoch 19:  26%|██▌       | 78/300 [00:40<01:50,  2.01it/s]Epoch 19:  26%|██▋       | 79/300 [00:40<01:54,  1.93it/s]06/19/2022 20:37:57 - INFO - __main__ - global step: 2890; train loss: 7.832070827484131; dev loss: 7.771052360534668
Epoch 19:  27%|██▋       | 80/300 [00:41<01:57,  1.87it/s]Epoch 19:  27%|██▋       | 81/300 [00:41<01:53,  1.93it/s]Epoch 19:  27%|██▋       | 82/300 [00:42<01:50,  1.98it/s]Epoch 19:  28%|██▊       | 83/300 [00:42<01:48,  2.01it/s]Epoch 19:  28%|██▊       | 84/300 [00:43<01:52,  1.91it/s]Epoch 19:  28%|██▊       | 85/300 [00:43<01:49,  1.96it/s]Epoch 19:  29%|██▊       | 86/300 [00:44<01:46,  2.01it/s]Epoch 19:  29%|██▉       | 87/300 [00:44<01:44,  2.04it/s]Epoch 19:  29%|██▉       | 88/300 [00:45<01:48,  1.95it/s]Epoch 19:  30%|██▉       | 89/300 [00:46<01:51,  1.89it/s]Epoch 19:  30%|███       | 90/300 [00:46<01:48,  1.94it/s]Epoch 19:  30%|███       | 91/300 [00:46<01:45,  1.99it/s]Epoch 19:  31%|███       | 92/300 [00:47<01:49,  1.90it/s]Epoch 19:  31%|███       | 93/300 [00:48<01:51,  1.86it/s]Epoch 19:  31%|███▏      | 94/300 [00:48<01:47,  1.92it/s]Epoch 19:  32%|███▏      | 95/300 [00:49<01:44,  1.96it/s]Epoch 19:  32%|███▏      | 96/300 [00:49<01:42,  2.00it/s]Epoch 19:  32%|███▏      | 97/300 [00:50<01:46,  1.91it/s]Epoch 19:  33%|███▎      | 98/300 [00:50<01:43,  1.96it/s]Epoch 19:  33%|███▎      | 99/300 [00:51<01:40,  1.99it/s]06/19/2022 20:38:07 - INFO - __main__ - global step: 2900; train loss: 7.472748756408691; dev loss: 7.875298976898193
Epoch 19:  33%|███▎      | 100/300 [00:51<01:38,  2.02it/s]Epoch 19:  34%|███▎      | 101/300 [00:52<01:42,  1.94it/s]Epoch 19:  34%|███▍      | 102/300 [00:52<01:42,  1.93it/s]Epoch 19:  34%|███▍      | 103/300 [00:53<01:39,  1.97it/s]Epoch 19:  35%|███▍      | 104/300 [00:53<01:40,  1.96it/s]Epoch 19:  35%|███▌      | 105/300 [00:54<01:45,  1.85it/s]Epoch 19:  35%|███▌      | 106/300 [00:54<01:45,  1.83it/s]Epoch 19:  36%|███▌      | 107/300 [00:55<01:41,  1.91it/s]Epoch 19:  36%|███▌      | 108/300 [00:55<01:38,  1.95it/s]Epoch 19:  36%|███▋      | 109/300 [00:56<01:46,  1.79it/s]Epoch 19:  37%|███▋      | 110/300 [00:56<01:41,  1.87it/s]Epoch 19:  37%|███▋      | 111/300 [00:57<01:43,  1.83it/s]Epoch 19:  37%|███▋      | 112/300 [00:58<01:38,  1.90it/s]Epoch 19:  38%|███▊      | 113/300 [00:58<01:41,  1.84it/s]Epoch 19:  38%|███▊      | 114/300 [00:59<01:37,  1.91it/s]Epoch 19:  38%|███▊      | 115/300 [00:59<01:36,  1.91it/s]Epoch 19:  39%|███▊      | 116/300 [01:00<01:33,  1.96it/s]Epoch 19:  39%|███▉      | 117/300 [01:00<01:38,  1.85it/s]Epoch 19:  39%|███▉      | 118/300 [01:01<01:36,  1.88it/s]Epoch 19:  40%|███▉      | 119/300 [01:01<01:33,  1.94it/s]06/19/2022 20:38:18 - INFO - __main__ - global step: 2910; train loss: 7.425684928894043; dev loss: 7.650104522705078
Epoch 19:  40%|████      | 120/300 [01:02<01:35,  1.89it/s]Epoch 19:  40%|████      | 121/300 [01:02<01:31,  1.95it/s]Epoch 19:  41%|████      | 122/300 [01:03<01:34,  1.89it/s]Epoch 19:  41%|████      | 123/300 [01:03<01:31,  1.94it/s]Epoch 19:  41%|████▏     | 124/300 [01:04<01:32,  1.90it/s]Epoch 19:  42%|████▏     | 125/300 [01:04<01:29,  1.95it/s]Epoch 19:  42%|████▏     | 126/300 [01:05<01:34,  1.84it/s]Epoch 19:  42%|████▏     | 127/300 [01:05<01:30,  1.90it/s]Epoch 19:  43%|████▎     | 128/300 [01:06<01:30,  1.90it/s]Epoch 19:  43%|████▎     | 129/300 [01:06<01:27,  1.94it/s]Epoch 19:  43%|████▎     | 130/300 [01:07<01:34,  1.79it/s]Epoch 19:  44%|████▎     | 131/300 [01:08<01:34,  1.79it/s]Epoch 19:  44%|████▍     | 132/300 [01:08<01:30,  1.85it/s]Epoch 19:  44%|████▍     | 133/300 [01:09<01:31,  1.83it/s]Epoch 19:  45%|████▍     | 134/300 [01:09<01:32,  1.80it/s]Epoch 19:  45%|████▌     | 135/300 [01:10<01:31,  1.80it/s]Epoch 19:  45%|████▌     | 136/300 [01:10<01:27,  1.87it/s]Epoch 19:  46%|████▌     | 137/300 [01:11<01:28,  1.85it/s]Epoch 19:  46%|████▌     | 138/300 [01:11<01:29,  1.81it/s]Epoch 19:  46%|████▋     | 139/300 [01:12<01:25,  1.88it/s]06/19/2022 20:38:28 - INFO - __main__ - global step: 2920; train loss: 7.341404914855957; dev loss: 7.312538146972656
Epoch 19:  47%|████▋     | 140/300 [01:12<01:24,  1.90it/s]Epoch 19:  47%|████▋     | 141/300 [01:13<01:21,  1.94it/s]Epoch 19:  47%|████▋     | 142/300 [01:13<01:23,  1.88it/s]Epoch 19:  48%|████▊     | 143/300 [01:14<01:22,  1.89it/s]Epoch 19:  48%|████▊     | 144/300 [01:14<01:20,  1.95it/s]Epoch 19:  48%|████▊     | 145/300 [01:15<01:21,  1.90it/s]Epoch 19:  49%|████▊     | 146/300 [01:16<01:23,  1.84it/s]Epoch 19:  49%|████▉     | 147/300 [01:16<01:20,  1.91it/s]Epoch 19:  49%|████▉     | 148/300 [01:17<01:19,  1.91it/s]Epoch 19:  50%|████▉     | 149/300 [01:17<01:20,  1.87it/s]Epoch 19:  50%|█████     | 150/300 [01:18<01:17,  1.93it/s]Epoch 19:  50%|█████     | 151/300 [01:18<01:20,  1.86it/s]Epoch 19:  51%|█████     | 152/300 [01:19<01:20,  1.83it/s]Epoch 19:  51%|█████     | 153/300 [01:19<01:17,  1.91it/s]Epoch 19:  51%|█████▏    | 154/300 [01:20<01:14,  1.95it/s]Epoch 19:  52%|█████▏    | 155/300 [01:20<01:16,  1.89it/s]Epoch 19:  52%|█████▏    | 156/300 [01:21<01:13,  1.95it/s]Epoch 19:  52%|█████▏    | 157/300 [01:21<01:11,  1.99it/s]Epoch 19:  53%|█████▎    | 158/300 [01:22<01:10,  2.02it/s]Epoch 19:  53%|█████▎    | 159/300 [01:22<01:14,  1.89it/s]06/19/2022 20:38:39 - INFO - __main__ - global step: 2930; train loss: 7.586378574371338; dev loss: 7.801339626312256
Epoch 19:  53%|█████▎    | 160/300 [01:23<01:12,  1.93it/s]Epoch 19:  54%|█████▎    | 161/300 [01:23<01:10,  1.98it/s]Epoch 19:  54%|█████▍    | 162/300 [01:24<01:08,  2.01it/s]Epoch 19:  54%|█████▍    | 163/300 [01:24<01:11,  1.93it/s]Epoch 19:  55%|█████▍    | 164/300 [01:25<01:08,  1.98it/s]Epoch 19:  55%|█████▌    | 165/300 [01:25<01:10,  1.91it/s]Epoch 19:  55%|█████▌    | 166/300 [01:26<01:08,  1.95it/s]Epoch 19:  56%|█████▌    | 167/300 [01:27<01:14,  1.79it/s]Epoch 19:  56%|█████▌    | 168/300 [01:27<01:13,  1.78it/s]Epoch 19:  56%|█████▋    | 169/300 [01:28<01:11,  1.83it/s]Epoch 19:  57%|█████▋    | 170/300 [01:28<01:08,  1.90it/s]Epoch 19:  57%|█████▋    | 171/300 [01:29<01:12,  1.78it/s]Epoch 19:  57%|█████▋    | 172/300 [01:29<01:09,  1.85it/s]Epoch 19:  58%|█████▊    | 173/300 [01:30<01:06,  1.92it/s]Epoch 19:  58%|█████▊    | 174/300 [01:30<01:04,  1.96it/s]Epoch 19:  58%|█████▊    | 175/300 [01:31<01:02,  2.00it/s]Epoch 19:  59%|█████▊    | 176/300 [01:31<01:04,  1.92it/s]Epoch 19:  59%|█████▉    | 177/300 [01:32<01:02,  1.97it/s]Epoch 19:  59%|█████▉    | 178/300 [01:32<01:00,  2.00it/s]Epoch 19:  60%|█████▉    | 179/300 [01:33<00:59,  2.03it/s]06/19/2022 20:38:49 - INFO - __main__ - global step: 2940; train loss: 7.828027248382568; dev loss: 7.6715192794799805
Epoch 19:  60%|██████    | 180/300 [01:33<01:05,  1.84it/s]Epoch 19:  60%|██████    | 181/300 [01:34<01:03,  1.87it/s]Epoch 19:  61%|██████    | 182/300 [01:34<01:03,  1.84it/s]Epoch 19:  61%|██████    | 183/300 [01:35<01:04,  1.82it/s]Epoch 19:  61%|██████▏   | 184/300 [01:36<01:07,  1.72it/s]Epoch 19:  62%|██████▏   | 185/300 [01:36<01:03,  1.81it/s]Epoch 19:  62%|██████▏   | 186/300 [01:37<01:02,  1.84it/s]Epoch 19:  62%|██████▏   | 187/300 [01:37<00:59,  1.89it/s]Epoch 19:  63%|██████▎   | 188/300 [01:38<01:04,  1.75it/s]Epoch 19:  63%|██████▎   | 189/300 [01:38<01:00,  1.83it/s]Epoch 19:  63%|██████▎   | 190/300 [01:39<00:58,  1.89it/s]Epoch 19:  64%|██████▎   | 191/300 [01:39<00:56,  1.93it/s]Epoch 19:  64%|██████▍   | 192/300 [01:40<00:59,  1.82it/s]Epoch 19:  64%|██████▍   | 193/300 [01:40<00:58,  1.84it/s]Epoch 19:  65%|██████▍   | 194/300 [01:41<00:58,  1.81it/s]Epoch 19:  65%|██████▌   | 195/300 [01:42<00:56,  1.87it/s]Epoch 19:  65%|██████▌   | 196/300 [01:42<00:59,  1.74it/s]Epoch 19:  66%|██████▌   | 197/300 [01:43<00:57,  1.78it/s]Epoch 19:  66%|██████▌   | 198/300 [01:43<00:54,  1.86it/s]Epoch 19:  66%|██████▋   | 199/300 [01:44<00:54,  1.86it/s]06/19/2022 20:39:00 - INFO - __main__ - global step: 2950; train loss: 7.620214939117432; dev loss: 7.370255470275879
Epoch 19:  67%|██████▋   | 200/300 [01:44<00:55,  1.81it/s]Epoch 19:  67%|██████▋   | 201/300 [01:45<00:52,  1.87it/s]Epoch 19:  67%|██████▋   | 202/300 [01:45<00:51,  1.92it/s]Epoch 19:  68%|██████▊   | 203/300 [01:46<00:52,  1.86it/s]Epoch 19:  68%|██████▊   | 204/300 [01:46<00:49,  1.93it/s]Epoch 19:  68%|██████▊   | 205/300 [01:47<00:50,  1.87it/s]Epoch 19:  69%|██████▊   | 206/300 [01:47<00:49,  1.89it/s]Epoch 19:  69%|██████▉   | 207/300 [01:48<00:47,  1.95it/s]Epoch 19:  69%|██████▉   | 208/300 [01:48<00:46,  2.00it/s]Epoch 19:  70%|██████▉   | 209/300 [01:49<00:47,  1.92it/s]Epoch 19:  70%|███████   | 210/300 [01:49<00:45,  1.97it/s]Epoch 19:  70%|███████   | 211/300 [01:50<00:44,  2.00it/s]Epoch 19:  71%|███████   | 212/300 [01:50<00:43,  2.03it/s]Epoch 19:  71%|███████   | 213/300 [01:51<00:44,  1.94it/s]Epoch 19:  71%|███████▏  | 214/300 [01:52<00:45,  1.89it/s]Epoch 19:  72%|███████▏  | 215/300 [01:52<00:44,  1.92it/s]Epoch 19:  72%|███████▏  | 216/300 [01:53<00:43,  1.92it/s]Epoch 19:  72%|███████▏  | 217/300 [01:53<00:46,  1.79it/s]Epoch 19:  73%|███████▎  | 218/300 [01:54<00:43,  1.86it/s]Epoch 19:  73%|███████▎  | 219/300 [01:54<00:42,  1.92it/s]06/19/2022 20:39:11 - INFO - __main__ - global step: 2960; train loss: 7.580530643463135; dev loss: 7.436624050140381
Epoch 19:  73%|███████▎  | 220/300 [01:55<00:40,  1.97it/s]Epoch 19:  74%|███████▎  | 221/300 [01:55<00:41,  1.90it/s]Epoch 19:  74%|███████▍  | 222/300 [01:56<00:39,  1.95it/s]Epoch 19:  74%|███████▍  | 223/300 [01:56<00:38,  1.99it/s]Epoch 19:  75%|███████▍  | 224/300 [01:57<00:37,  2.02it/s]Epoch 19:  75%|███████▌  | 225/300 [01:57<00:39,  1.91it/s]Epoch 19:  75%|███████▌  | 226/300 [01:58<00:37,  1.96it/s]Epoch 19:  76%|███████▌  | 227/300 [01:58<00:36,  2.00it/s]Epoch 19:  76%|███████▌  | 228/300 [01:59<00:35,  2.03it/s]Epoch 19:  76%|███████▋  | 229/300 [01:59<00:35,  1.99it/s]Epoch 19:  77%|███████▋  | 230/300 [02:00<00:36,  1.91it/s]Epoch 19:  77%|███████▋  | 231/300 [02:00<00:36,  1.87it/s]Epoch 19:  77%|███████▋  | 232/300 [02:01<00:35,  1.94it/s]Epoch 19:  78%|███████▊  | 233/300 [02:01<00:33,  1.98it/s]Epoch 19:  78%|███████▊  | 234/300 [02:02<00:34,  1.90it/s]Epoch 19:  78%|███████▊  | 235/300 [02:02<00:33,  1.95it/s]Epoch 19:  79%|███████▊  | 236/300 [02:03<00:32,  1.99it/s]Epoch 19:  79%|███████▉  | 237/300 [02:03<00:32,  1.96it/s]Epoch 19:  79%|███████▉  | 238/300 [02:04<00:32,  1.89it/s]Epoch 19:  80%|███████▉  | 239/300 [02:04<00:31,  1.94it/s]06/19/2022 20:39:21 - INFO - __main__ - global step: 2970; train loss: 7.874937534332275; dev loss: 7.651303768157959
Epoch 19:  80%|████████  | 240/300 [02:05<00:31,  1.89it/s]Epoch 19:  80%|████████  | 241/300 [02:05<00:30,  1.95it/s]Epoch 19:  81%|████████  | 242/300 [02:06<00:30,  1.87it/s]Epoch 19:  81%|████████  | 243/300 [02:07<00:30,  1.86it/s]Epoch 19:  81%|████████▏ | 244/300 [02:07<00:29,  1.92it/s]Epoch 19:  82%|████████▏ | 245/300 [02:08<00:29,  1.88it/s]Epoch 19:  82%|████████▏ | 246/300 [02:08<00:29,  1.80it/s]Epoch 19:  82%|████████▏ | 247/300 [02:09<00:28,  1.88it/s]Epoch 19:  83%|████████▎ | 248/300 [02:09<00:28,  1.85it/s]Epoch 19:  83%|████████▎ | 249/300 [02:10<00:27,  1.84it/s]Epoch 19:  83%|████████▎ | 250/300 [02:10<00:27,  1.81it/s]Epoch 19:  84%|████████▎ | 251/300 [02:11<00:26,  1.85it/s]Epoch 19:  84%|████████▍ | 252/300 [02:11<00:25,  1.91it/s]Epoch 19:  84%|████████▍ | 253/300 [02:12<00:24,  1.96it/s]Epoch 19:  85%|████████▍ | 254/300 [02:12<00:24,  1.90it/s]Epoch 19:  85%|████████▌ | 255/300 [02:13<00:23,  1.91it/s]Epoch 19:  85%|████████▌ | 256/300 [02:13<00:22,  1.96it/s]Epoch 19:  86%|████████▌ | 257/300 [02:14<00:22,  1.90it/s]Epoch 19:  86%|████████▌ | 258/300 [02:14<00:21,  1.96it/s]Epoch 19:  86%|████████▋ | 259/300 [02:15<00:21,  1.89it/s]06/19/2022 20:39:31 - INFO - __main__ - global step: 2980; train loss: 7.162713527679443; dev loss: 7.557731628417969
Epoch 19:  87%|████████▋ | 260/300 [02:16<00:20,  1.94it/s]Epoch 19:  87%|████████▋ | 261/300 [02:16<00:19,  1.99it/s]Epoch 19:  87%|████████▋ | 262/300 [02:17<00:19,  1.91it/s]Epoch 19:  88%|████████▊ | 263/300 [02:17<00:19,  1.87it/s]Epoch 19:  88%|████████▊ | 264/300 [02:18<00:18,  1.93it/s]Epoch 19:  88%|████████▊ | 265/300 [02:18<00:17,  1.97it/s]Epoch 19:  89%|████████▊ | 266/300 [02:19<00:16,  2.00it/s]Epoch 19:  89%|████████▉ | 267/300 [02:19<00:17,  1.92it/s]Epoch 19:  89%|████████▉ | 268/300 [02:20<00:17,  1.87it/s]Epoch 19:  90%|████████▉ | 269/300 [02:20<00:16,  1.93it/s]Epoch 19:  90%|█████████ | 270/300 [02:21<00:15,  1.97it/s]Epoch 19:  90%|█████████ | 271/300 [02:21<00:15,  1.90it/s]Epoch 19:  91%|█████████ | 272/300 [02:22<00:14,  1.94it/s]Epoch 19:  91%|█████████ | 273/300 [02:22<00:13,  1.98it/s]Epoch 19:  91%|█████████▏| 274/300 [02:23<00:12,  2.02it/s]Epoch 19:  92%|█████████▏| 275/300 [02:23<00:13,  1.91it/s]Epoch 19:  92%|█████████▏| 276/300 [02:24<00:12,  1.96it/s]Epoch 19:  92%|█████████▏| 277/300 [02:24<00:12,  1.90it/s]Epoch 19:  93%|█████████▎| 278/300 [02:25<00:11,  1.96it/s]Epoch 19:  93%|█████████▎| 279/300 [02:25<00:11,  1.90it/s]06/19/2022 20:39:42 - INFO - __main__ - global step: 2990; train loss: 7.777159690856934; dev loss: 7.389176845550537
Epoch 19:  93%|█████████▎| 280/300 [02:26<00:10,  1.95it/s]Epoch 19:  94%|█████████▎| 281/300 [02:26<00:09,  1.99it/s]Epoch 19:  94%|█████████▍| 282/300 [02:27<00:08,  2.01it/s]Epoch 19:  94%|█████████▍| 283/300 [02:27<00:08,  2.04it/s]Epoch 19:  95%|█████████▍| 284/300 [02:28<00:08,  1.93it/s]Epoch 19:  95%|█████████▌| 285/300 [02:28<00:07,  1.88it/s]Epoch 19:  95%|█████████▌| 286/300 [02:29<00:07,  1.90it/s]Epoch 19:  96%|█████████▌| 287/300 [02:30<00:06,  1.87it/s]Epoch 19:  96%|█████████▌| 288/300 [02:30<00:06,  1.82it/s]Epoch 19:  96%|█████████▋| 289/300 [02:31<00:05,  1.90it/s]Epoch 19:  97%|█████████▋| 290/300 [02:31<00:05,  1.86it/s]Epoch 19:  97%|█████████▋| 291/300 [02:32<00:04,  1.93it/s]Epoch 19:  97%|█████████▋| 292/300 [02:32<00:04,  1.84it/s]Epoch 19:  98%|█████████▊| 293/300 [02:33<00:03,  1.91it/s]Epoch 19:  98%|█████████▊| 294/300 [02:33<00:03,  1.92it/s]Epoch 19:  98%|█████████▊| 295/300 [02:34<00:02,  1.98it/s]Epoch 19:  99%|█████████▊| 296/300 [02:34<00:02,  1.81it/s]Epoch 19:  99%|█████████▉| 297/300 [02:35<00:01,  1.85it/s]Epoch 19:  99%|█████████▉| 298/300 [02:35<00:01,  1.92it/s]Epoch 19: 100%|█████████▉| 299/300 [02:36<00:00,  1.97it/s]06/19/2022 20:39:52 - INFO - __main__ - global step: 3000; train loss: 7.948133945465088; dev loss: 7.860709190368652
Epoch 19: 100%|██████████| 300/300 [02:36<00:00,  1.81it/s]Epoch 19: 100%|██████████| 300/300 [02:36<00:00,  1.91it/s]
Epoch 20:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 20:   0%|          | 1/300 [00:00<02:20,  2.13it/s]Epoch 20:   1%|          | 2/300 [00:00<02:19,  2.13it/s]Epoch 20:   1%|          | 3/300 [00:01<02:20,  2.12it/s]Epoch 20:   1%|▏         | 4/300 [00:02<02:36,  1.89it/s]Epoch 20:   2%|▏         | 5/300 [00:02<02:39,  1.85it/s]Epoch 20:   2%|▏         | 6/300 [00:03<02:32,  1.93it/s]Epoch 20:   2%|▏         | 7/300 [00:03<02:30,  1.95it/s]Epoch 20:   3%|▎         | 8/300 [00:04<02:35,  1.87it/s]Epoch 20:   3%|▎         | 9/300 [00:04<02:35,  1.87it/s]Epoch 20:   3%|▎         | 10/300 [00:05<02:30,  1.93it/s]Epoch 20:   4%|▎         | 11/300 [00:05<02:26,  1.97it/s]Epoch 20:   4%|▍         | 12/300 [00:06<02:23,  2.01it/s]Epoch 20:   4%|▍         | 13/300 [00:06<02:35,  1.84it/s]Epoch 20:   5%|▍         | 14/300 [00:07<02:30,  1.91it/s]Epoch 20:   5%|▌         | 15/300 [00:07<02:25,  1.95it/s]Epoch 20:   5%|▌         | 16/300 [00:08<02:26,  1.93it/s]Epoch 20:   6%|▌         | 17/300 [00:08<02:33,  1.85it/s]Epoch 20:   6%|▌         | 18/300 [00:09<02:27,  1.92it/s]Epoch 20:   6%|▋         | 19/300 [00:09<02:26,  1.91it/s]06/19/2022 20:40:03 - INFO - __main__ - global step: 3010; train loss: 7.49130392074585; dev loss: 7.323022365570068
Epoch 20:   7%|▋         | 20/300 [00:10<02:22,  1.97it/s]Epoch 20:   7%|▋         | 21/300 [00:10<02:27,  1.89it/s]Epoch 20:   7%|▋         | 22/300 [00:11<02:23,  1.94it/s]Epoch 20:   8%|▊         | 23/300 [00:11<02:21,  1.96it/s]Epoch 20:   8%|▊         | 24/300 [00:12<02:18,  1.99it/s]Epoch 20:   8%|▊         | 25/300 [00:12<02:27,  1.87it/s]Epoch 20:   9%|▊         | 26/300 [00:13<02:21,  1.94it/s]Epoch 20:   9%|▉         | 27/300 [00:13<02:17,  1.98it/s]Epoch 20:   9%|▉         | 28/300 [00:14<02:15,  2.01it/s]Epoch 20:  10%|▉         | 29/300 [00:14<02:20,  1.92it/s]Epoch 20:  10%|█         | 30/300 [00:15<02:17,  1.97it/s]Epoch 20:  10%|█         | 31/300 [00:15<02:14,  2.00it/s]Epoch 20:  11%|█         | 32/300 [00:16<02:12,  2.03it/s]Epoch 20:  11%|█         | 33/300 [00:17<02:21,  1.89it/s]Epoch 20:  11%|█▏        | 34/300 [00:17<02:16,  1.95it/s]Epoch 20:  12%|█▏        | 35/300 [00:17<02:13,  1.99it/s]Epoch 20:  12%|█▏        | 36/300 [00:18<02:10,  2.02it/s]Epoch 20:  12%|█▏        | 37/300 [00:18<02:08,  2.04it/s]Epoch 20:  13%|█▎        | 38/300 [00:19<02:14,  1.94it/s]Epoch 20:  13%|█▎        | 39/300 [00:20<02:17,  1.89it/s]06/19/2022 20:40:13 - INFO - __main__ - global step: 3020; train loss: 7.961055755615234; dev loss: 7.942599296569824
Epoch 20:  13%|█▎        | 40/300 [00:20<02:16,  1.90it/s]Epoch 20:  14%|█▎        | 41/300 [00:21<02:12,  1.96it/s]Epoch 20:  14%|█▍        | 42/300 [00:21<02:16,  1.89it/s]Epoch 20:  14%|█▍        | 43/300 [00:22<02:11,  1.95it/s]Epoch 20:  15%|█▍        | 44/300 [00:22<02:10,  1.97it/s]Epoch 20:  15%|█▌        | 45/300 [00:23<02:07,  2.00it/s]Epoch 20:  15%|█▌        | 46/300 [00:23<02:12,  1.91it/s]Epoch 20:  16%|█▌        | 47/300 [00:24<02:08,  1.96it/s]Epoch 20:  16%|█▌        | 48/300 [00:24<02:05,  2.01it/s]Epoch 20:  16%|█▋        | 49/300 [00:25<02:03,  2.04it/s]Epoch 20:  17%|█▋        | 50/300 [00:25<02:14,  1.86it/s]Epoch 20:  17%|█▋        | 51/300 [00:26<02:09,  1.92it/s]Epoch 20:  17%|█▋        | 52/300 [00:26<02:05,  1.97it/s]Epoch 20:  18%|█▊        | 53/300 [00:27<02:04,  1.99it/s]Epoch 20:  18%|█▊        | 54/300 [00:27<02:09,  1.90it/s]Epoch 20:  18%|█▊        | 55/300 [00:28<02:06,  1.93it/s]Epoch 20:  19%|█▊        | 56/300 [00:28<02:09,  1.88it/s]Epoch 20:  19%|█▉        | 57/300 [00:29<02:11,  1.85it/s]Epoch 20:  19%|█▉        | 58/300 [00:29<02:13,  1.82it/s]Epoch 20:  20%|█▉        | 59/300 [00:30<02:08,  1.88it/s]06/19/2022 20:40:23 - INFO - __main__ - global step: 3030; train loss: 7.858733177185059; dev loss: 7.912850379943848
Epoch 20:  20%|██        | 60/300 [00:30<02:04,  1.93it/s]Epoch 20:  20%|██        | 61/300 [00:31<02:01,  1.96it/s]Epoch 20:  21%|██        | 62/300 [00:32<02:05,  1.90it/s]Epoch 20:  21%|██        | 63/300 [00:32<02:02,  1.94it/s]Epoch 20:  21%|██▏       | 64/300 [00:33<02:02,  1.93it/s]Epoch 20:  22%|██▏       | 65/300 [00:33<02:04,  1.88it/s]Epoch 20:  22%|██▏       | 66/300 [00:34<02:06,  1.85it/s]Epoch 20:  22%|██▏       | 67/300 [00:34<02:08,  1.81it/s]Epoch 20:  23%|██▎       | 68/300 [00:35<02:03,  1.89it/s]Epoch 20:  23%|██▎       | 69/300 [00:35<02:01,  1.89it/s]Epoch 20:  23%|██▎       | 70/300 [00:36<01:57,  1.95it/s]Epoch 20:  24%|██▎       | 71/300 [00:36<02:00,  1.90it/s]Epoch 20:  24%|██▍       | 72/300 [00:37<01:56,  1.95it/s]Epoch 20:  24%|██▍       | 73/300 [00:37<01:59,  1.89it/s]Epoch 20:  25%|██▍       | 74/300 [00:38<01:56,  1.94it/s]Epoch 20:  25%|██▌       | 75/300 [00:38<01:59,  1.88it/s]Epoch 20:  25%|██▌       | 76/300 [00:39<01:58,  1.89it/s]Epoch 20:  26%|██▌       | 77/300 [00:39<01:54,  1.95it/s]Epoch 20:  26%|██▌       | 78/300 [00:40<01:52,  1.98it/s]Epoch 20:  26%|██▋       | 79/300 [00:40<01:58,  1.86it/s]06/19/2022 20:40:34 - INFO - __main__ - global step: 3040; train loss: 7.256166934967041; dev loss: 7.4876251220703125
Epoch 20:  27%|██▋       | 80/300 [00:41<01:58,  1.85it/s]Epoch 20:  27%|██▋       | 81/300 [00:41<01:54,  1.91it/s]Epoch 20:  27%|██▋       | 82/300 [00:42<01:56,  1.87it/s]Epoch 20:  28%|██▊       | 83/300 [00:43<02:04,  1.74it/s]Epoch 20:  28%|██▊       | 84/300 [00:43<01:59,  1.81it/s]Epoch 20:  28%|██▊       | 85/300 [00:44<01:54,  1.88it/s]Epoch 20:  29%|██▊       | 86/300 [00:44<01:50,  1.93it/s]Epoch 20:  29%|██▉       | 87/300 [00:45<01:56,  1.82it/s]Epoch 20:  29%|██▉       | 88/300 [00:45<01:51,  1.90it/s]Epoch 20:  30%|██▉       | 89/300 [00:46<01:47,  1.96it/s]Epoch 20:  30%|███       | 90/300 [00:46<01:44,  2.00it/s]Epoch 20:  30%|███       | 91/300 [00:47<01:42,  2.03it/s]Epoch 20:  31%|███       | 92/300 [00:47<01:47,  1.94it/s]Epoch 20:  31%|███       | 93/300 [00:48<01:44,  1.98it/s]Epoch 20:  31%|███▏      | 94/300 [00:48<01:42,  2.01it/s]Epoch 20:  32%|███▏      | 95/300 [00:49<01:40,  2.04it/s]Epoch 20:  32%|███▏      | 96/300 [00:49<01:44,  1.95it/s]Epoch 20:  32%|███▏      | 97/300 [00:50<01:41,  1.99it/s]Epoch 20:  33%|███▎      | 98/300 [00:50<01:40,  2.01it/s]Epoch 20:  33%|███▎      | 99/300 [00:51<01:38,  2.03it/s]06/19/2022 20:40:44 - INFO - __main__ - global step: 3050; train loss: 7.4423065185546875; dev loss: 7.3620805740356445
Epoch 20:  33%|███▎      | 100/300 [00:51<01:42,  1.95it/s]Epoch 20:  34%|███▎      | 101/300 [00:52<01:40,  1.99it/s]Epoch 20:  34%|███▍      | 102/300 [00:52<01:38,  2.02it/s]Epoch 20:  34%|███▍      | 103/300 [00:53<01:41,  1.95it/s]Epoch 20:  35%|███▍      | 104/300 [00:53<01:44,  1.88it/s]Epoch 20:  35%|███▌      | 105/300 [00:54<01:40,  1.94it/s]Epoch 20:  35%|███▌      | 106/300 [00:54<01:42,  1.90it/s]Epoch 20:  36%|███▌      | 107/300 [00:55<01:40,  1.91it/s]Epoch 20:  36%|███▌      | 108/300 [00:55<01:42,  1.87it/s]Epoch 20:  36%|███▋      | 109/300 [00:56<01:38,  1.94it/s]Epoch 20:  37%|███▋      | 110/300 [00:56<01:35,  1.99it/s]Epoch 20:  37%|███▋      | 111/300 [00:57<01:33,  2.03it/s]Epoch 20:  37%|███▋      | 112/300 [00:57<01:36,  1.94it/s]Epoch 20:  38%|███▊      | 113/300 [00:58<01:33,  1.99it/s]Epoch 20:  38%|███▊      | 114/300 [00:58<01:31,  2.02it/s]Epoch 20:  38%|███▊      | 115/300 [00:59<01:30,  2.05it/s]Epoch 20:  39%|███▊      | 116/300 [00:59<01:34,  1.94it/s]Epoch 20:  39%|███▉      | 117/300 [01:00<01:32,  1.98it/s]Epoch 20:  39%|███▉      | 118/300 [01:00<01:34,  1.92it/s]Epoch 20:  40%|███▉      | 119/300 [01:01<01:36,  1.87it/s]06/19/2022 20:40:54 - INFO - __main__ - global step: 3060; train loss: 7.719738006591797; dev loss: 7.964341640472412
Epoch 20:  40%|████      | 120/300 [01:02<01:37,  1.85it/s]Epoch 20:  40%|████      | 121/300 [01:02<01:38,  1.82it/s]Epoch 20:  41%|████      | 122/300 [01:03<01:35,  1.86it/s]Epoch 20:  41%|████      | 123/300 [01:03<01:32,  1.91it/s]Epoch 20:  41%|████▏     | 124/300 [01:04<01:29,  1.96it/s]Epoch 20:  42%|████▏     | 125/300 [01:04<01:31,  1.91it/s]Epoch 20:  42%|████▏     | 126/300 [01:05<01:28,  1.96it/s]Epoch 20:  42%|████▏     | 127/300 [01:05<01:26,  2.00it/s]Epoch 20:  43%|████▎     | 128/300 [01:06<01:24,  2.03it/s]Epoch 20:  43%|████▎     | 129/300 [01:06<01:29,  1.90it/s]Epoch 20:  43%|████▎     | 130/300 [01:07<01:26,  1.96it/s]Epoch 20:  44%|████▎     | 131/300 [01:07<01:24,  2.01it/s]Epoch 20:  44%|████▍     | 132/300 [01:08<01:22,  2.04it/s]Epoch 20:  44%|████▍     | 133/300 [01:08<01:26,  1.94it/s]Epoch 20:  45%|████▍     | 134/300 [01:09<01:27,  1.89it/s]Epoch 20:  45%|████▌     | 135/300 [01:09<01:28,  1.87it/s]Epoch 20:  45%|████▌     | 136/300 [01:10<01:24,  1.93it/s]Epoch 20:  46%|████▌     | 137/300 [01:10<01:30,  1.81it/s]Epoch 20:  46%|████▌     | 138/300 [01:11<01:29,  1.80it/s]Epoch 20:  46%|████▋     | 139/300 [01:11<01:25,  1.89it/s]06/19/2022 20:41:05 - INFO - __main__ - global step: 3070; train loss: 7.455644130706787; dev loss: 7.6414666175842285
Epoch 20:  47%|████▋     | 140/300 [01:12<01:21,  1.95it/s]Epoch 20:  47%|████▋     | 141/300 [01:13<01:23,  1.90it/s]Epoch 20:  47%|████▋     | 142/300 [01:13<01:22,  1.90it/s]Epoch 20:  48%|████▊     | 143/300 [01:14<01:23,  1.87it/s]Epoch 20:  48%|████▊     | 144/300 [01:14<01:21,  1.92it/s]Epoch 20:  48%|████▊     | 145/300 [01:15<01:18,  1.97it/s]Epoch 20:  49%|████▊     | 146/300 [01:15<01:21,  1.90it/s]Epoch 20:  49%|████▉     | 147/300 [01:16<01:22,  1.86it/s]Epoch 20:  49%|████▉     | 148/300 [01:16<01:22,  1.84it/s]Epoch 20:  50%|████▉     | 149/300 [01:17<01:18,  1.92it/s]Epoch 20:  50%|█████     | 150/300 [01:17<01:20,  1.87it/s]Epoch 20:  50%|█████     | 151/300 [01:18<01:17,  1.93it/s]Epoch 20:  51%|█████     | 152/300 [01:18<01:16,  1.93it/s]Epoch 20:  51%|█████     | 153/300 [01:19<01:13,  1.99it/s]Epoch 20:  51%|█████▏    | 154/300 [01:19<01:16,  1.91it/s]Epoch 20:  52%|█████▏    | 155/300 [01:20<01:13,  1.97it/s]Epoch 20:  52%|█████▏    | 156/300 [01:20<01:11,  2.01it/s]Epoch 20:  52%|█████▏    | 157/300 [01:21<01:09,  2.05it/s]Epoch 20:  53%|█████▎    | 158/300 [01:21<01:12,  1.96it/s]Epoch 20:  53%|█████▎    | 159/300 [01:22<01:12,  1.96it/s]06/19/2022 20:41:15 - INFO - __main__ - global step: 3080; train loss: 7.478180885314941; dev loss: 7.756312370300293
Epoch 20:  53%|█████▎    | 160/300 [01:22<01:10,  2.00it/s]Epoch 20:  54%|█████▎    | 161/300 [01:23<01:11,  1.94it/s]Epoch 20:  54%|█████▍    | 162/300 [01:23<01:13,  1.89it/s]Epoch 20:  54%|█████▍    | 163/300 [01:24<01:11,  1.91it/s]Epoch 20:  55%|█████▍    | 164/300 [01:24<01:12,  1.87it/s]Epoch 20:  55%|█████▌    | 165/300 [01:25<01:11,  1.89it/s]Epoch 20:  55%|█████▌    | 166/300 [01:26<01:12,  1.85it/s]Epoch 20:  56%|█████▌    | 167/300 [01:26<01:09,  1.92it/s]Epoch 20:  56%|█████▌    | 168/300 [01:27<01:06,  1.98it/s]Epoch 20:  56%|█████▋    | 169/300 [01:27<01:05,  2.01it/s]Epoch 20:  57%|█████▋    | 170/300 [01:28<01:07,  1.93it/s]Epoch 20:  57%|█████▋    | 171/300 [01:28<01:08,  1.88it/s]Epoch 20:  57%|█████▋    | 172/300 [01:29<01:06,  1.94it/s]Epoch 20:  58%|█████▊    | 173/300 [01:29<01:07,  1.89it/s]Epoch 20:  58%|█████▊    | 174/300 [01:30<01:04,  1.95it/s]Epoch 20:  58%|█████▊    | 175/300 [01:30<01:06,  1.88it/s]Epoch 20:  59%|█████▊    | 176/300 [01:31<01:04,  1.93it/s]Epoch 20:  59%|█████▉    | 177/300 [01:31<01:02,  1.98it/s]Epoch 20:  59%|█████▉    | 178/300 [01:32<01:00,  2.01it/s]Epoch 20:  60%|█████▉    | 179/300 [01:32<01:02,  1.93it/s]06/19/2022 20:41:26 - INFO - __main__ - global step: 3090; train loss: 7.475350856781006; dev loss: 7.20425271987915
Epoch 20:  60%|██████    | 180/300 [01:33<01:00,  1.97it/s]Epoch 20:  60%|██████    | 181/300 [01:33<00:59,  2.01it/s]Epoch 20:  61%|██████    | 182/300 [01:34<00:58,  2.03it/s]Epoch 20:  61%|██████    | 183/300 [01:34<01:00,  1.94it/s]Epoch 20:  61%|██████▏   | 184/300 [01:35<00:58,  1.98it/s]Epoch 20:  62%|██████▏   | 185/300 [01:35<00:56,  2.02it/s]Epoch 20:  62%|██████▏   | 186/300 [01:36<00:55,  2.04it/s]Epoch 20:  62%|██████▏   | 187/300 [01:36<00:59,  1.89it/s]Epoch 20:  63%|██████▎   | 188/300 [01:37<00:59,  1.87it/s]Epoch 20:  63%|██████▎   | 189/300 [01:37<00:58,  1.89it/s]Epoch 20:  63%|██████▎   | 190/300 [01:38<00:56,  1.94it/s]Epoch 20:  64%|██████▎   | 191/300 [01:38<00:57,  1.89it/s]Epoch 20:  64%|██████▍   | 192/300 [01:39<00:55,  1.94it/s]Epoch 20:  64%|██████▍   | 193/300 [01:39<00:53,  1.98it/s]Epoch 20:  65%|██████▍   | 194/300 [01:40<00:52,  2.01it/s]Epoch 20:  65%|██████▌   | 195/300 [01:40<00:54,  1.93it/s]Epoch 20:  65%|██████▌   | 196/300 [01:41<00:55,  1.89it/s]Epoch 20:  66%|██████▌   | 197/300 [01:41<00:52,  1.95it/s]Epoch 20:  66%|██████▌   | 198/300 [01:42<00:52,  1.95it/s]Epoch 20:  66%|██████▋   | 199/300 [01:42<00:53,  1.90it/s]06/19/2022 20:41:36 - INFO - __main__ - global step: 3100; train loss: 7.861545562744141; dev loss: 7.7530646324157715
Epoch 20:  67%|██████▋   | 200/300 [01:43<00:54,  1.84it/s]Epoch 20:  67%|██████▋   | 201/300 [01:44<00:51,  1.91it/s]Epoch 20:  67%|██████▋   | 202/300 [01:44<00:50,  1.96it/s]Epoch 20:  68%|██████▊   | 203/300 [01:45<00:48,  2.00it/s]Epoch 20:  68%|██████▊   | 204/300 [01:45<00:52,  1.84it/s]Epoch 20:  68%|██████▊   | 205/300 [01:46<00:49,  1.91it/s]Epoch 20:  69%|██████▊   | 206/300 [01:46<00:48,  1.92it/s]Epoch 20:  69%|██████▉   | 207/300 [01:47<00:49,  1.87it/s]Epoch 20:  69%|██████▉   | 208/300 [01:47<00:50,  1.83it/s]Epoch 20:  70%|██████▉   | 209/300 [01:48<00:50,  1.82it/s]Epoch 20:  70%|███████   | 210/300 [01:48<00:47,  1.90it/s]Epoch 20:  70%|███████   | 211/300 [01:49<00:45,  1.96it/s]Epoch 20:  71%|███████   | 212/300 [01:49<00:48,  1.81it/s]Epoch 20:  71%|███████   | 213/300 [01:50<00:45,  1.90it/s]Epoch 20:  71%|███████▏  | 214/300 [01:50<00:43,  1.96it/s]Epoch 20:  72%|███████▏  | 215/300 [01:51<00:42,  2.00it/s]Epoch 20:  72%|███████▏  | 216/300 [01:51<00:44,  1.88it/s]Epoch 20:  72%|███████▏  | 217/300 [01:52<00:42,  1.95it/s]Epoch 20:  73%|███████▎  | 218/300 [01:52<00:41,  1.99it/s]Epoch 20:  73%|███████▎  | 219/300 [01:53<00:41,  1.97it/s]06/19/2022 20:41:46 - INFO - __main__ - global step: 3110; train loss: 7.455281734466553; dev loss: 7.5101141929626465
Epoch 20:  73%|███████▎  | 220/300 [01:54<00:42,  1.89it/s]Epoch 20:  74%|███████▎  | 221/300 [01:54<00:40,  1.95it/s]Epoch 20:  74%|███████▍  | 222/300 [01:54<00:39,  1.99it/s]Epoch 20:  74%|███████▍  | 223/300 [01:55<00:39,  1.97it/s]Epoch 20:  75%|███████▍  | 224/300 [01:56<00:40,  1.87it/s]Epoch 20:  75%|███████▌  | 225/300 [01:56<00:40,  1.84it/s]Epoch 20:  75%|███████▌  | 226/300 [01:57<00:38,  1.91it/s]Epoch 20:  76%|███████▌  | 227/300 [01:57<00:37,  1.95it/s]Epoch 20:  76%|███████▌  | 228/300 [01:58<00:36,  1.99it/s]Epoch 20:  76%|███████▋  | 229/300 [01:58<00:38,  1.84it/s]Epoch 20:  77%|███████▋  | 230/300 [01:59<00:36,  1.91it/s]Epoch 20:  77%|███████▋  | 231/300 [01:59<00:35,  1.96it/s]Epoch 20:  77%|███████▋  | 232/300 [02:00<00:34,  1.95it/s]Epoch 20:  78%|███████▊  | 233/300 [02:00<00:35,  1.90it/s]Epoch 20:  78%|███████▊  | 234/300 [02:01<00:33,  1.95it/s]Epoch 20:  78%|███████▊  | 235/300 [02:01<00:32,  1.98it/s]Epoch 20:  79%|███████▊  | 236/300 [02:02<00:32,  1.97it/s]Epoch 20:  79%|███████▉  | 237/300 [02:02<00:33,  1.86it/s]Epoch 20:  79%|███████▉  | 238/300 [02:03<00:32,  1.93it/s]Epoch 20:  80%|███████▉  | 239/300 [02:03<00:30,  1.98it/s]06/19/2022 20:41:57 - INFO - __main__ - global step: 3120; train loss: 7.275152683258057; dev loss: 7.451171875
Epoch 20:  80%|████████  | 240/300 [02:04<00:30,  1.96it/s]Epoch 20:  80%|████████  | 241/300 [02:04<00:32,  1.81it/s]Epoch 20:  81%|████████  | 242/300 [02:05<00:32,  1.80it/s]Epoch 20:  81%|████████  | 243/300 [02:05<00:30,  1.88it/s]Epoch 20:  81%|████████▏ | 244/300 [02:06<00:28,  1.94it/s]Epoch 20:  82%|████████▏ | 245/300 [02:07<00:30,  1.79it/s]Epoch 20:  82%|████████▏ | 246/300 [02:07<00:29,  1.82it/s]Epoch 20:  82%|████████▏ | 247/300 [02:08<00:27,  1.90it/s]Epoch 20:  83%|████████▎ | 248/300 [02:08<00:26,  1.96it/s]Epoch 20:  83%|████████▎ | 249/300 [02:09<00:27,  1.86it/s]Epoch 20:  83%|████████▎ | 250/300 [02:09<00:26,  1.92it/s]Epoch 20:  84%|████████▎ | 251/300 [02:10<00:24,  1.98it/s]Epoch 20:  84%|████████▍ | 252/300 [02:10<00:24,  1.97it/s]Epoch 20:  84%|████████▍ | 253/300 [02:11<00:23,  2.01it/s]Epoch 20:  85%|████████▍ | 254/300 [02:11<00:23,  1.93it/s]Epoch 20:  85%|████████▌ | 255/300 [02:12<00:22,  1.97it/s]Epoch 20:  85%|████████▌ | 256/300 [02:12<00:22,  1.96it/s]Epoch 20:  86%|████████▌ | 257/300 [02:13<00:22,  1.91it/s]Epoch 20:  86%|████████▌ | 258/300 [02:13<00:22,  1.87it/s]Epoch 20:  86%|████████▋ | 259/300 [02:14<00:21,  1.93it/s]06/19/2022 20:42:07 - INFO - __main__ - global step: 3130; train loss: 7.1774187088012695; dev loss: 7.389484405517578
Epoch 20:  87%|████████▋ | 260/300 [02:14<00:20,  1.93it/s]Epoch 20:  87%|████████▋ | 261/300 [02:15<00:19,  1.97it/s]Epoch 20:  87%|████████▋ | 262/300 [02:15<00:19,  1.91it/s]Epoch 20:  88%|████████▊ | 263/300 [02:16<00:18,  1.95it/s]Epoch 20:  88%|████████▊ | 264/300 [02:16<00:18,  1.90it/s]Epoch 20:  88%|████████▊ | 265/300 [02:17<00:18,  1.87it/s]Epoch 20:  89%|████████▊ | 266/300 [02:18<00:18,  1.83it/s]Epoch 20:  89%|████████▉ | 267/300 [02:18<00:17,  1.91it/s]Epoch 20:  89%|████████▉ | 268/300 [02:18<00:16,  1.97it/s]Epoch 20:  90%|████████▉ | 269/300 [02:19<00:16,  1.91it/s]Epoch 20:  90%|█████████ | 270/300 [02:20<00:16,  1.85it/s]Epoch 20:  90%|█████████ | 271/300 [02:20<00:15,  1.92it/s]Epoch 20:  91%|█████████ | 272/300 [02:21<00:14,  1.88it/s]Epoch 20:  91%|█████████ | 273/300 [02:21<00:13,  1.95it/s]Epoch 20:  91%|█████████▏| 274/300 [02:22<00:13,  1.89it/s]Epoch 20:  92%|█████████▏| 275/300 [02:22<00:12,  1.94it/s]Epoch 20:  92%|█████████▏| 276/300 [02:23<00:12,  1.99it/s]Epoch 20:  92%|█████████▏| 277/300 [02:23<00:11,  1.97it/s]Epoch 20:  93%|█████████▎| 278/300 [02:24<00:11,  1.86it/s]Epoch 20:  93%|█████████▎| 279/300 [02:24<00:10,  1.93it/s]06/19/2022 20:42:18 - INFO - __main__ - global step: 3140; train loss: 8.014272689819336; dev loss: 7.829186916351318
Epoch 20:  93%|█████████▎| 280/300 [02:25<00:10,  1.88it/s]Epoch 20:  94%|█████████▎| 281/300 [02:25<00:09,  1.93it/s]Epoch 20:  94%|█████████▍| 282/300 [02:26<00:09,  1.97it/s]Epoch 20:  94%|█████████▍| 283/300 [02:26<00:09,  1.82it/s]Epoch 20:  95%|█████████▍| 284/300 [02:27<00:08,  1.89it/s]Epoch 20:  95%|█████████▌| 285/300 [02:27<00:08,  1.85it/s]Epoch 20:  95%|█████████▌| 286/300 [02:28<00:07,  1.87it/s]Epoch 20:  96%|█████████▌| 287/300 [02:29<00:07,  1.76it/s]Epoch 20:  96%|█████████▌| 288/300 [02:29<00:06,  1.85it/s]Epoch 20:  96%|█████████▋| 289/300 [02:30<00:05,  1.92it/s]Epoch 20:  97%|█████████▋| 290/300 [02:30<00:05,  1.96it/s]Epoch 20:  97%|█████████▋| 291/300 [02:31<00:04,  1.81it/s]Epoch 20:  97%|█████████▋| 292/300 [02:31<00:04,  1.84it/s]Epoch 20:  98%|█████████▊| 293/300 [02:32<00:03,  1.90it/s]Epoch 20:  98%|█████████▊| 294/300 [02:32<00:03,  1.96it/s]Epoch 20:  98%|█████████▊| 295/300 [02:33<00:02,  1.88it/s]Epoch 20:  99%|█████████▊| 296/300 [02:33<00:02,  1.94it/s]Epoch 20:  99%|█████████▉| 297/300 [02:34<00:01,  1.98it/s]Epoch 20:  99%|█████████▉| 298/300 [02:34<00:01,  1.96it/s]Epoch 20: 100%|█████████▉| 299/300 [02:35<00:00,  1.86it/s]06/19/2022 20:42:28 - INFO - __main__ - global step: 3150; train loss: 7.486001491546631; dev loss: 7.799204349517822
Epoch 20: 100%|██████████| 300/300 [02:35<00:00,  1.92it/s]Epoch 20: 100%|██████████| 300/300 [02:35<00:00,  1.92it/s]
Epoch 21:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 21:   0%|          | 1/300 [00:00<02:25,  2.05it/s]Epoch 21:   1%|          | 2/300 [00:00<02:23,  2.08it/s]Epoch 21:   1%|          | 3/300 [00:01<02:46,  1.78it/s]Epoch 21:   1%|▏         | 4/300 [00:02<02:36,  1.89it/s]Epoch 21:   2%|▏         | 5/300 [00:02<02:30,  1.97it/s]Epoch 21:   2%|▏         | 6/300 [00:03<02:34,  1.90it/s]Epoch 21:   2%|▏         | 7/300 [00:03<02:29,  1.95it/s]Epoch 21:   3%|▎         | 8/300 [00:04<02:35,  1.88it/s]Epoch 21:   3%|▎         | 9/300 [00:04<02:30,  1.93it/s]Epoch 21:   3%|▎         | 10/300 [00:05<02:26,  1.98it/s]Epoch 21:   4%|▎         | 11/300 [00:05<02:22,  2.02it/s]Epoch 21:   4%|▍         | 12/300 [00:06<02:28,  1.94it/s]Epoch 21:   4%|▍         | 13/300 [00:06<02:24,  1.98it/s]Epoch 21:   5%|▍         | 14/300 [00:07<02:22,  2.01it/s]Epoch 21:   5%|▌         | 15/300 [00:07<02:19,  2.04it/s]Epoch 21:   5%|▌         | 16/300 [00:08<02:29,  1.90it/s]Epoch 21:   6%|▌         | 17/300 [00:08<02:24,  1.96it/s]Epoch 21:   6%|▌         | 18/300 [00:09<02:28,  1.90it/s]Epoch 21:   6%|▋         | 19/300 [00:09<02:26,  1.91it/s]06/19/2022 20:42:39 - INFO - __main__ - global step: 3160; train loss: 7.740155220031738; dev loss: 7.709958553314209
Epoch 21:   7%|▋         | 20/300 [00:10<02:31,  1.85it/s]Epoch 21:   7%|▋         | 21/300 [00:10<02:25,  1.92it/s]Epoch 21:   7%|▋         | 22/300 [00:11<02:21,  1.97it/s]Epoch 21:   8%|▊         | 23/300 [00:11<02:18,  2.01it/s]Epoch 21:   8%|▊         | 24/300 [00:12<02:30,  1.84it/s]Epoch 21:   8%|▊         | 25/300 [00:12<02:23,  1.92it/s]Epoch 21:   9%|▊         | 26/300 [00:13<02:18,  1.98it/s]Epoch 21:   9%|▉         | 27/300 [00:13<02:16,  2.01it/s]Epoch 21:   9%|▉         | 28/300 [00:14<02:27,  1.84it/s]Epoch 21:  10%|▉         | 29/300 [00:14<02:21,  1.92it/s]Epoch 21:  10%|█         | 30/300 [00:15<02:16,  1.97it/s]Epoch 21:  10%|█         | 31/300 [00:15<02:14,  2.01it/s]Epoch 21:  11%|█         | 32/300 [00:16<02:23,  1.87it/s]Epoch 21:  11%|█         | 33/300 [00:17<02:21,  1.89it/s]Epoch 21:  11%|█▏        | 34/300 [00:17<02:16,  1.94it/s]Epoch 21:  12%|█▏        | 35/300 [00:18<02:14,  1.98it/s]Epoch 21:  12%|█▏        | 36/300 [00:18<02:15,  1.96it/s]Epoch 21:  12%|█▏        | 37/300 [00:19<02:23,  1.84it/s]Epoch 21:  13%|█▎        | 38/300 [00:19<02:19,  1.87it/s]Epoch 21:  13%|█▎        | 39/300 [00:20<02:19,  1.88it/s]06/19/2022 20:42:49 - INFO - __main__ - global step: 3170; train loss: 7.906214714050293; dev loss: 7.702722072601318
Epoch 21:  13%|█▎        | 40/300 [00:20<02:15,  1.92it/s]Epoch 21:  14%|█▎        | 41/300 [00:21<02:18,  1.87it/s]Epoch 21:  14%|█▍        | 42/300 [00:21<02:14,  1.92it/s]Epoch 21:  14%|█▍        | 43/300 [00:22<02:10,  1.97it/s]Epoch 21:  15%|█▍        | 44/300 [00:22<02:10,  1.96it/s]Epoch 21:  15%|█▌        | 45/300 [00:23<02:14,  1.89it/s]Epoch 21:  15%|█▌        | 46/300 [00:23<02:10,  1.95it/s]Epoch 21:  16%|█▌        | 47/300 [00:24<02:08,  1.96it/s]Epoch 21:  16%|█▌        | 48/300 [00:24<02:06,  1.99it/s]Epoch 21:  16%|█▋        | 49/300 [00:25<02:11,  1.91it/s]Epoch 21:  17%|█▋        | 50/300 [00:25<02:07,  1.96it/s]Epoch 21:  17%|█▋        | 51/300 [00:26<02:04,  1.99it/s]Epoch 21:  17%|█▋        | 52/300 [00:26<02:02,  2.02it/s]Epoch 21:  18%|█▊        | 53/300 [00:27<02:08,  1.92it/s]Epoch 21:  18%|█▊        | 54/300 [00:27<02:05,  1.96it/s]Epoch 21:  18%|█▊        | 55/300 [00:28<02:02,  2.00it/s]Epoch 21:  19%|█▊        | 56/300 [00:28<02:00,  2.02it/s]Epoch 21:  19%|█▉        | 57/300 [00:29<02:06,  1.92it/s]Epoch 21:  19%|█▉        | 58/300 [00:29<02:03,  1.97it/s]Epoch 21:  20%|█▉        | 59/300 [00:30<02:01,  1.98it/s]06/19/2022 20:42:59 - INFO - __main__ - global step: 3180; train loss: 7.963456153869629; dev loss: 8.21739387512207
Epoch 21:  20%|██        | 60/300 [00:30<01:59,  2.00it/s]Epoch 21:  20%|██        | 61/300 [00:31<01:58,  2.02it/s]Epoch 21:  21%|██        | 62/300 [00:31<02:04,  1.91it/s]Epoch 21:  21%|██        | 63/300 [00:32<02:02,  1.93it/s]Epoch 21:  21%|██▏       | 64/300 [00:32<02:02,  1.93it/s]Epoch 21:  22%|██▏       | 65/300 [00:33<01:59,  1.96it/s]Epoch 21:  22%|██▏       | 66/300 [00:34<02:05,  1.87it/s]Epoch 21:  22%|██▏       | 67/300 [00:34<02:00,  1.93it/s]Epoch 21:  23%|██▎       | 68/300 [00:35<01:57,  1.98it/s]Epoch 21:  23%|██▎       | 69/300 [00:35<01:55,  2.01it/s]Epoch 21:  23%|██▎       | 70/300 [00:36<01:59,  1.93it/s]Epoch 21:  24%|██▎       | 71/300 [00:36<01:55,  1.98it/s]Epoch 21:  24%|██▍       | 72/300 [00:37<01:53,  2.01it/s]Epoch 21:  24%|██▍       | 73/300 [00:37<01:51,  2.03it/s]Epoch 21:  25%|██▍       | 74/300 [00:38<01:58,  1.91it/s]Epoch 21:  25%|██▌       | 75/300 [00:38<01:54,  1.96it/s]Epoch 21:  25%|██▌       | 76/300 [00:39<01:51,  2.00it/s]Epoch 21:  26%|██▌       | 77/300 [00:39<01:53,  1.97it/s]Epoch 21:  26%|██▌       | 78/300 [00:40<01:59,  1.86it/s]Epoch 21:  26%|██▋       | 79/300 [00:40<01:57,  1.88it/s]06/19/2022 20:43:09 - INFO - __main__ - global step: 3190; train loss: 7.135331153869629; dev loss: 7.114584922790527
Epoch 21:  27%|██▋       | 80/300 [00:41<01:54,  1.91it/s]Epoch 21:  27%|██▋       | 81/300 [00:41<01:52,  1.95it/s]Epoch 21:  27%|██▋       | 82/300 [00:42<01:56,  1.88it/s]Epoch 21:  28%|██▊       | 83/300 [00:42<01:52,  1.94it/s]Epoch 21:  28%|██▊       | 84/300 [00:43<01:49,  1.98it/s]Epoch 21:  28%|██▊       | 85/300 [00:43<01:47,  2.01it/s]Epoch 21:  29%|██▊       | 86/300 [00:44<01:51,  1.91it/s]Epoch 21:  29%|██▉       | 87/300 [00:44<01:48,  1.97it/s]Epoch 21:  29%|██▉       | 88/300 [00:45<01:47,  1.97it/s]Epoch 21:  30%|██▉       | 89/300 [00:45<01:45,  2.00it/s]Epoch 21:  30%|███       | 90/300 [00:46<01:43,  2.03it/s]Epoch 21:  30%|███       | 91/300 [00:46<01:48,  1.93it/s]Epoch 21:  31%|███       | 92/300 [00:47<01:44,  1.98it/s]Epoch 21:  31%|███       | 93/300 [00:47<01:43,  2.01it/s]Epoch 21:  31%|███▏      | 94/300 [00:48<01:41,  2.03it/s]Epoch 21:  32%|███▏      | 95/300 [00:48<01:48,  1.89it/s]Epoch 21:  32%|███▏      | 96/300 [00:49<01:45,  1.94it/s]Epoch 21:  32%|███▏      | 97/300 [00:49<01:42,  1.98it/s]Epoch 21:  33%|███▎      | 98/300 [00:50<01:42,  1.97it/s]Epoch 21:  33%|███▎      | 99/300 [00:50<01:45,  1.90it/s]06/19/2022 20:43:20 - INFO - __main__ - global step: 3200; train loss: 7.410421848297119; dev loss: 7.389313697814941
Epoch 21:  33%|███▎      | 100/300 [00:51<01:45,  1.89it/s]Epoch 21:  34%|███▎      | 101/300 [00:51<01:42,  1.95it/s]Epoch 21:  34%|███▍      | 102/300 [00:52<01:40,  1.98it/s]Epoch 21:  34%|███▍      | 103/300 [00:52<01:44,  1.89it/s]Epoch 21:  35%|███▍      | 104/300 [00:53<01:40,  1.95it/s]Epoch 21:  35%|███▌      | 105/300 [00:53<01:38,  1.99it/s]Epoch 21:  35%|███▌      | 106/300 [00:54<01:42,  1.89it/s]Epoch 21:  36%|███▌      | 107/300 [00:55<01:44,  1.85it/s]Epoch 21:  36%|███▌      | 108/300 [00:55<01:40,  1.90it/s]Epoch 21:  36%|███▋      | 109/300 [00:56<01:39,  1.92it/s]Epoch 21:  37%|███▋      | 110/300 [00:56<01:38,  1.93it/s]Epoch 21:  37%|███▋      | 111/300 [00:57<01:41,  1.87it/s]Epoch 21:  37%|███▋      | 112/300 [00:57<01:37,  1.92it/s]Epoch 21:  38%|███▊      | 113/300 [00:58<01:34,  1.98it/s]Epoch 21:  38%|███▊      | 114/300 [00:58<01:37,  1.90it/s]Epoch 21:  38%|███▊      | 115/300 [00:59<01:34,  1.96it/s]Epoch 21:  39%|███▊      | 116/300 [00:59<01:39,  1.84it/s]Epoch 21:  39%|███▉      | 117/300 [01:00<01:36,  1.90it/s]Epoch 21:  39%|███▉      | 118/300 [01:00<01:34,  1.93it/s]Epoch 21:  40%|███▉      | 119/300 [01:01<01:31,  1.98it/s]06/19/2022 20:43:30 - INFO - __main__ - global step: 3210; train loss: 7.3664093017578125; dev loss: 7.877581596374512
Epoch 21:  40%|████      | 120/300 [01:01<01:39,  1.81it/s]Epoch 21:  40%|████      | 121/300 [01:02<01:36,  1.85it/s]Epoch 21:  41%|████      | 122/300 [01:02<01:33,  1.91it/s]Epoch 21:  41%|████      | 123/300 [01:03<01:34,  1.87it/s]Epoch 21:  41%|████▏     | 124/300 [01:04<01:40,  1.76it/s]Epoch 21:  42%|████▏     | 125/300 [01:04<01:34,  1.85it/s]Epoch 21:  42%|████▏     | 126/300 [01:05<01:35,  1.83it/s]Epoch 21:  42%|████▏     | 127/300 [01:05<01:31,  1.90it/s]Epoch 21:  43%|████▎     | 128/300 [01:06<01:33,  1.84it/s]Epoch 21:  43%|████▎     | 129/300 [01:06<01:29,  1.90it/s]Epoch 21:  43%|████▎     | 130/300 [01:07<01:26,  1.96it/s]Epoch 21:  44%|████▎     | 131/300 [01:07<01:24,  1.99it/s]Epoch 21:  44%|████▍     | 132/300 [01:08<01:27,  1.91it/s]Epoch 21:  44%|████▍     | 133/300 [01:08<01:25,  1.96it/s]Epoch 21:  45%|████▍     | 134/300 [01:09<01:23,  2.00it/s]Epoch 21:  45%|████▌     | 135/300 [01:09<01:21,  2.02it/s]Epoch 21:  45%|████▌     | 136/300 [01:10<01:24,  1.94it/s]Epoch 21:  46%|████▌     | 137/300 [01:10<01:21,  1.99it/s]Epoch 21:  46%|████▌     | 138/300 [01:11<01:20,  2.02it/s]Epoch 21:  46%|████▋     | 139/300 [01:11<01:22,  1.95it/s]06/19/2022 20:43:41 - INFO - __main__ - global step: 3220; train loss: 7.625489711761475; dev loss: 7.4210968017578125
Epoch 21:  47%|████▋     | 140/300 [01:12<01:25,  1.88it/s]Epoch 21:  47%|████▋     | 141/300 [01:12<01:23,  1.90it/s]Epoch 21:  47%|████▋     | 142/300 [01:13<01:20,  1.95it/s]Epoch 21:  48%|████▊     | 143/300 [01:13<01:22,  1.90it/s]Epoch 21:  48%|████▊     | 144/300 [01:14<01:19,  1.95it/s]Epoch 21:  48%|████▊     | 145/300 [01:14<01:22,  1.89it/s]Epoch 21:  49%|████▊     | 146/300 [01:15<01:19,  1.95it/s]Epoch 21:  49%|████▉     | 147/300 [01:15<01:17,  1.97it/s]Epoch 21:  49%|████▉     | 148/300 [01:16<01:15,  2.00it/s]Epoch 21:  50%|████▉     | 149/300 [01:16<01:18,  1.92it/s]Epoch 21:  50%|█████     | 150/300 [01:17<01:16,  1.97it/s]Epoch 21:  50%|█████     | 151/300 [01:17<01:14,  1.99it/s]Epoch 21:  51%|█████     | 152/300 [01:18<01:16,  1.93it/s]Epoch 21:  51%|█████     | 153/300 [01:19<01:18,  1.88it/s]Epoch 21:  51%|█████▏    | 154/300 [01:19<01:15,  1.94it/s]Epoch 21:  52%|█████▏    | 155/300 [01:20<01:13,  1.98it/s]Epoch 21:  52%|█████▏    | 156/300 [01:20<01:11,  2.01it/s]Epoch 21:  52%|█████▏    | 157/300 [01:21<01:16,  1.86it/s]Epoch 21:  53%|█████▎    | 158/300 [01:21<01:13,  1.92it/s]Epoch 21:  53%|█████▎    | 159/300 [01:22<01:13,  1.93it/s]06/19/2022 20:43:51 - INFO - __main__ - global step: 3230; train loss: 7.270524501800537; dev loss: 7.031289577484131
Epoch 21:  53%|█████▎    | 160/300 [01:22<01:10,  1.97it/s]Epoch 21:  54%|█████▎    | 161/300 [01:23<01:12,  1.91it/s]Epoch 21:  54%|█████▍    | 162/300 [01:23<01:11,  1.94it/s]Epoch 21:  54%|█████▍    | 163/300 [01:24<01:09,  1.98it/s]Epoch 21:  55%|█████▍    | 164/300 [01:24<01:07,  2.02it/s]Epoch 21:  55%|█████▌    | 165/300 [01:25<01:10,  1.93it/s]Epoch 21:  55%|█████▌    | 166/300 [01:25<01:11,  1.88it/s]Epoch 21:  56%|█████▌    | 167/300 [01:26<01:08,  1.95it/s]Epoch 21:  56%|█████▌    | 168/300 [01:26<01:08,  1.92it/s]Epoch 21:  56%|█████▋    | 169/300 [01:27<01:06,  1.97it/s]Epoch 21:  57%|█████▋    | 170/300 [01:27<01:08,  1.90it/s]Epoch 21:  57%|█████▋    | 171/300 [01:28<01:05,  1.96it/s]Epoch 21:  57%|█████▋    | 172/300 [01:28<01:07,  1.89it/s]Epoch 21:  58%|█████▊    | 173/300 [01:29<01:05,  1.94it/s]Epoch 21:  58%|█████▊    | 174/300 [01:29<01:09,  1.80it/s]Epoch 21:  58%|█████▊    | 175/300 [01:30<01:06,  1.89it/s]Epoch 21:  59%|█████▊    | 176/300 [01:31<01:07,  1.85it/s]Epoch 21:  59%|█████▉    | 177/300 [01:31<01:04,  1.92it/s]Epoch 21:  59%|█████▉    | 178/300 [01:32<01:06,  1.83it/s]Epoch 21:  60%|█████▉    | 179/300 [01:32<01:03,  1.90it/s]06/19/2022 20:44:01 - INFO - __main__ - global step: 3240; train loss: 7.516214847564697; dev loss: 7.711949348449707
Epoch 21:  60%|██████    | 180/300 [01:33<01:04,  1.86it/s]Epoch 21:  60%|██████    | 181/300 [01:33<01:02,  1.92it/s]Epoch 21:  61%|██████    | 182/300 [01:34<01:03,  1.87it/s]Epoch 21:  61%|██████    | 183/300 [01:34<01:00,  1.94it/s]Epoch 21:  61%|██████▏   | 184/300 [01:35<01:00,  1.93it/s]Epoch 21:  62%|██████▏   | 185/300 [01:35<00:58,  1.97it/s]Epoch 21:  62%|██████▏   | 186/300 [01:36<01:00,  1.90it/s]Epoch 21:  62%|██████▏   | 187/300 [01:36<00:57,  1.95it/s]Epoch 21:  63%|██████▎   | 188/300 [01:37<00:56,  1.99it/s]Epoch 21:  63%|██████▎   | 189/300 [01:37<00:55,  2.00it/s]Epoch 21:  63%|██████▎   | 190/300 [01:38<00:57,  1.92it/s]Epoch 21:  64%|██████▎   | 191/300 [01:38<00:55,  1.96it/s]Epoch 21:  64%|██████▍   | 192/300 [01:39<00:54,  2.00it/s]Epoch 21:  64%|██████▍   | 193/300 [01:39<00:53,  2.02it/s]Epoch 21:  65%|██████▍   | 194/300 [01:40<00:57,  1.84it/s]Epoch 21:  65%|██████▌   | 195/300 [01:40<00:54,  1.91it/s]Epoch 21:  65%|██████▌   | 196/300 [01:41<00:53,  1.96it/s]Epoch 21:  66%|██████▌   | 197/300 [01:41<00:52,  1.95it/s]Epoch 21:  66%|██████▌   | 198/300 [01:42<00:52,  1.95it/s]Epoch 21:  66%|██████▋   | 199/300 [01:42<00:54,  1.84it/s]06/19/2022 20:44:12 - INFO - __main__ - global step: 3250; train loss: 7.453731536865234; dev loss: 7.895899295806885
Epoch 21:  67%|██████▋   | 200/300 [01:43<00:52,  1.91it/s]Epoch 21:  67%|██████▋   | 201/300 [01:43<00:50,  1.96it/s]Epoch 21:  67%|██████▋   | 202/300 [01:44<00:51,  1.91it/s]Epoch 21:  68%|██████▊   | 203/300 [01:45<00:54,  1.78it/s]Epoch 21:  68%|██████▊   | 204/300 [01:45<00:51,  1.85it/s]Epoch 21:  68%|██████▊   | 205/300 [01:46<00:49,  1.91it/s]Epoch 21:  69%|██████▊   | 206/300 [01:46<00:50,  1.87it/s]Epoch 21:  69%|██████▉   | 207/300 [01:47<00:50,  1.83it/s]Epoch 21:  69%|██████▉   | 208/300 [01:47<00:49,  1.86it/s]Epoch 21:  70%|██████▉   | 209/300 [01:48<00:47,  1.93it/s]Epoch 21:  70%|███████   | 210/300 [01:48<00:47,  1.88it/s]Epoch 21:  70%|███████   | 211/300 [01:49<00:48,  1.83it/s]Epoch 21:  71%|███████   | 212/300 [01:49<00:46,  1.90it/s]Epoch 21:  71%|███████   | 213/300 [01:50<00:44,  1.94it/s]Epoch 21:  71%|███████▏  | 214/300 [01:50<00:43,  1.97it/s]Epoch 21:  72%|███████▏  | 215/300 [01:51<00:45,  1.85it/s]Epoch 21:  72%|███████▏  | 216/300 [01:51<00:43,  1.91it/s]Epoch 21:  72%|███████▏  | 217/300 [01:52<00:42,  1.95it/s]Epoch 21:  73%|███████▎  | 218/300 [01:52<00:43,  1.90it/s]Epoch 21:  73%|███████▎  | 219/300 [01:53<00:43,  1.86it/s]06/19/2022 20:44:22 - INFO - __main__ - global step: 3260; train loss: 7.839992523193359; dev loss: 7.963296413421631
Epoch 21:  73%|███████▎  | 220/300 [01:54<00:41,  1.93it/s]Epoch 21:  74%|███████▎  | 221/300 [01:54<00:40,  1.97it/s]Epoch 21:  74%|███████▍  | 222/300 [01:54<00:38,  2.00it/s]Epoch 21:  74%|███████▍  | 223/300 [01:55<00:37,  2.03it/s]Epoch 21:  75%|███████▍  | 224/300 [01:56<00:39,  1.93it/s]Epoch 21:  75%|███████▌  | 225/300 [01:56<00:37,  1.98it/s]Epoch 21:  75%|███████▌  | 226/300 [01:56<00:37,  2.00it/s]Epoch 21:  76%|███████▌  | 227/300 [01:57<00:36,  2.02it/s]Epoch 21:  76%|███████▌  | 228/300 [01:58<00:37,  1.94it/s]Epoch 21:  76%|███████▋  | 229/300 [01:58<00:35,  1.98it/s]Epoch 21:  77%|███████▋  | 230/300 [01:59<00:35,  1.96it/s]Epoch 21:  77%|███████▋  | 231/300 [01:59<00:36,  1.90it/s]Epoch 21:  77%|███████▋  | 232/300 [02:00<00:38,  1.77it/s]Epoch 21:  78%|███████▊  | 233/300 [02:00<00:36,  1.82it/s]Epoch 21:  78%|███████▊  | 234/300 [02:01<00:34,  1.90it/s]Epoch 21:  78%|███████▊  | 235/300 [02:01<00:34,  1.87it/s]Epoch 21:  79%|███████▊  | 236/300 [02:02<00:34,  1.83it/s]Epoch 21:  79%|███████▉  | 237/300 [02:02<00:33,  1.90it/s]Epoch 21:  79%|███████▉  | 238/300 [02:03<00:31,  1.96it/s]Epoch 21:  80%|███████▉  | 239/300 [02:03<00:31,  1.94it/s]06/19/2022 20:44:33 - INFO - __main__ - global step: 3270; train loss: 7.6400580406188965; dev loss: 7.5379228591918945
Epoch 21:  80%|████████  | 240/300 [02:04<00:32,  1.87it/s]Epoch 21:  80%|████████  | 241/300 [02:04<00:30,  1.93it/s]Epoch 21:  81%|████████  | 242/300 [02:05<00:29,  1.98it/s]Epoch 21:  81%|████████  | 243/300 [02:05<00:29,  1.97it/s]Epoch 21:  81%|████████▏ | 244/300 [02:06<00:29,  1.89it/s]Epoch 21:  82%|████████▏ | 245/300 [02:06<00:28,  1.96it/s]Epoch 21:  82%|████████▏ | 246/300 [02:07<00:27,  2.00it/s]Epoch 21:  82%|████████▏ | 247/300 [02:07<00:26,  2.02it/s]Epoch 21:  83%|████████▎ | 248/300 [02:08<00:26,  1.93it/s]Epoch 21:  83%|████████▎ | 249/300 [02:08<00:25,  1.98it/s]Epoch 21:  83%|████████▎ | 250/300 [02:09<00:25,  1.98it/s]Epoch 21:  84%|████████▎ | 251/300 [02:09<00:24,  2.01it/s]Epoch 21:  84%|████████▍ | 252/300 [02:10<00:23,  2.03it/s]Epoch 21:  84%|████████▍ | 253/300 [02:10<00:24,  1.93it/s]Epoch 21:  85%|████████▍ | 254/300 [02:11<00:23,  1.98it/s]Epoch 21:  85%|████████▌ | 255/300 [02:11<00:22,  1.96it/s]Epoch 21:  85%|████████▌ | 256/300 [02:12<00:21,  2.00it/s]Epoch 21:  86%|████████▌ | 257/300 [02:13<00:23,  1.84it/s]Epoch 21:  86%|████████▌ | 258/300 [02:13<00:22,  1.91it/s]Epoch 21:  86%|████████▋ | 259/300 [02:14<00:20,  1.96it/s]06/19/2022 20:44:43 - INFO - __main__ - global step: 3280; train loss: 7.664639949798584; dev loss: 7.659116268157959
Epoch 21:  87%|████████▋ | 260/300 [02:14<00:20,  2.00it/s]Epoch 21:  87%|████████▋ | 261/300 [02:15<00:20,  1.91it/s]Epoch 21:  87%|████████▋ | 262/300 [02:15<00:20,  1.87it/s]Epoch 21:  88%|████████▊ | 263/300 [02:16<00:19,  1.93it/s]Epoch 21:  88%|████████▊ | 264/300 [02:16<00:18,  1.93it/s]Epoch 21:  88%|████████▊ | 265/300 [02:17<00:19,  1.79it/s]Epoch 21:  89%|████████▊ | 266/300 [02:17<00:18,  1.87it/s]Epoch 21:  89%|████████▉ | 267/300 [02:18<00:18,  1.83it/s]Epoch 21:  89%|████████▉ | 268/300 [02:18<00:16,  1.90it/s]Epoch 21:  90%|████████▉ | 269/300 [02:19<00:16,  1.85it/s]Epoch 21:  90%|█████████ | 270/300 [02:19<00:16,  1.87it/s]Epoch 21:  90%|█████████ | 271/300 [02:20<00:14,  1.94it/s]Epoch 21:  91%|█████████ | 272/300 [02:20<00:14,  1.97it/s]Epoch 21:  91%|█████████ | 273/300 [02:21<00:14,  1.81it/s]Epoch 21:  91%|█████████▏| 274/300 [02:22<00:13,  1.88it/s]Epoch 21:  92%|█████████▏| 275/300 [02:22<00:13,  1.92it/s]Epoch 21:  92%|█████████▏| 276/300 [02:23<00:12,  1.88it/s]Epoch 21:  92%|█████████▏| 277/300 [02:23<00:12,  1.85it/s]Epoch 21:  93%|█████████▎| 278/300 [02:24<00:12,  1.81it/s]Epoch 21:  93%|█████████▎| 279/300 [02:24<00:11,  1.89it/s]06/19/2022 20:44:53 - INFO - __main__ - global step: 3290; train loss: 7.771766662597656; dev loss: 7.887887001037598
Epoch 21:  93%|█████████▎| 280/300 [02:25<00:10,  1.95it/s]Epoch 21:  94%|█████████▎| 281/300 [02:25<00:09,  1.98it/s]Epoch 21:  94%|█████████▍| 282/300 [02:26<00:09,  1.92it/s]Epoch 21:  94%|█████████▍| 283/300 [02:26<00:08,  1.96it/s]Epoch 21:  95%|█████████▍| 284/300 [02:27<00:08,  2.00it/s]Epoch 21:  95%|█████████▌| 285/300 [02:27<00:07,  1.93it/s]Epoch 21:  95%|█████████▌| 286/300 [02:28<00:07,  1.88it/s]Epoch 21:  96%|█████████▌| 287/300 [02:28<00:06,  1.94it/s]Epoch 21:  96%|█████████▌| 288/300 [02:29<00:06,  1.98it/s]Epoch 21:  96%|█████████▋| 289/300 [02:29<00:05,  2.02it/s]Epoch 21:  97%|█████████▋| 290/300 [02:30<00:05,  1.92it/s]Epoch 21:  97%|█████████▋| 291/300 [02:30<00:04,  1.96it/s]Epoch 21:  97%|█████████▋| 292/300 [02:31<00:04,  1.94it/s]Epoch 21:  98%|█████████▊| 293/300 [02:31<00:03,  1.98it/s]Epoch 21:  98%|█████████▊| 294/300 [02:32<00:03,  1.80it/s]Epoch 21:  98%|█████████▊| 295/300 [02:33<00:02,  1.87it/s]Epoch 21:  99%|█████████▊| 296/300 [02:33<00:02,  1.92it/s]Epoch 21:  99%|█████████▉| 297/300 [02:34<00:01,  1.87it/s]Epoch 21:  99%|█████████▉| 298/300 [02:34<00:01,  1.83it/s]Epoch 21: 100%|█████████▉| 299/300 [02:35<00:00,  1.91it/s]06/19/2022 20:45:04 - INFO - __main__ - global step: 3300; train loss: 7.083672523498535; dev loss: 7.473789215087891
Epoch 21: 100%|██████████| 300/300 [02:35<00:00,  1.87it/s]Epoch 21: 100%|██████████| 300/300 [02:35<00:00,  1.93it/s]
Epoch 22:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 22:   0%|          | 1/300 [00:00<02:22,  2.09it/s]Epoch 22:   1%|          | 2/300 [00:01<02:39,  1.87it/s]Epoch 22:   1%|          | 3/300 [00:01<02:42,  1.83it/s]Epoch 22:   1%|▏         | 4/300 [00:02<02:43,  1.81it/s]Epoch 22:   2%|▏         | 5/300 [00:02<02:35,  1.90it/s]Epoch 22:   2%|▏         | 6/300 [00:03<02:30,  1.96it/s]Epoch 22:   2%|▏         | 7/300 [00:03<02:40,  1.83it/s]Epoch 22:   3%|▎         | 8/300 [00:04<02:37,  1.86it/s]Epoch 22:   3%|▎         | 9/300 [00:04<02:35,  1.87it/s]Epoch 22:   3%|▎         | 10/300 [00:05<02:37,  1.84it/s]Epoch 22:   4%|▎         | 11/300 [00:05<02:39,  1.81it/s]Epoch 22:   4%|▍         | 12/300 [00:06<02:32,  1.89it/s]Epoch 22:   4%|▍         | 13/300 [00:06<02:27,  1.94it/s]Epoch 22:   5%|▍         | 14/300 [00:07<02:24,  1.98it/s]Epoch 22:   5%|▌         | 15/300 [00:07<02:30,  1.90it/s]Epoch 22:   5%|▌         | 16/300 [00:08<02:25,  1.95it/s]Epoch 22:   6%|▌         | 17/300 [00:08<02:29,  1.90it/s]Epoch 22:   6%|▌         | 18/300 [00:09<02:31,  1.86it/s]Epoch 22:   6%|▋         | 19/300 [00:10<02:33,  1.83it/s]06/19/2022 20:45:14 - INFO - __main__ - global step: 3310; train loss: 7.292675018310547; dev loss: 7.45397424697876
Epoch 22:   7%|▋         | 20/300 [00:10<02:28,  1.88it/s]Epoch 22:   7%|▋         | 21/300 [00:11<02:23,  1.94it/s]Epoch 22:   7%|▋         | 22/300 [00:11<02:19,  2.00it/s]Epoch 22:   8%|▊         | 23/300 [00:12<02:31,  1.83it/s]Epoch 22:   8%|▊         | 24/300 [00:12<02:25,  1.90it/s]Epoch 22:   8%|▊         | 25/300 [00:13<02:20,  1.96it/s]Epoch 22:   9%|▊         | 26/300 [00:13<02:20,  1.96it/s]Epoch 22:   9%|▉         | 27/300 [00:14<02:24,  1.89it/s]Epoch 22:   9%|▉         | 28/300 [00:14<02:19,  1.95it/s]Epoch 22:  10%|▉         | 29/300 [00:15<02:16,  1.99it/s]Epoch 22:  10%|█         | 30/300 [00:15<02:14,  2.01it/s]Epoch 22:  10%|█         | 31/300 [00:16<02:12,  2.03it/s]Epoch 22:  11%|█         | 32/300 [00:16<02:18,  1.94it/s]Epoch 22:  11%|█         | 33/300 [00:17<02:21,  1.89it/s]Epoch 22:  11%|█▏        | 34/300 [00:17<02:16,  1.95it/s]Epoch 22:  12%|█▏        | 35/300 [00:18<02:13,  1.99it/s]Epoch 22:  12%|█▏        | 36/300 [00:18<02:24,  1.83it/s]Epoch 22:  12%|█▏        | 37/300 [00:19<02:18,  1.90it/s]Epoch 22:  13%|█▎        | 38/300 [00:19<02:13,  1.96it/s]Epoch 22:  13%|█▎        | 39/300 [00:20<02:11,  1.99it/s]06/19/2022 20:45:25 - INFO - __main__ - global step: 3320; train loss: 7.4542717933654785; dev loss: 7.417941093444824
Epoch 22:  13%|█▎        | 40/300 [00:20<02:15,  1.91it/s]Epoch 22:  14%|█▎        | 41/300 [00:21<02:11,  1.97it/s]Epoch 22:  14%|█▍        | 42/300 [00:21<02:12,  1.95it/s]Epoch 22:  14%|█▍        | 43/300 [00:22<02:11,  1.95it/s]Epoch 22:  15%|█▍        | 44/300 [00:22<02:15,  1.89it/s]Epoch 22:  15%|█▌        | 45/300 [00:23<02:11,  1.95it/s]Epoch 22:  15%|█▌        | 46/300 [00:24<02:14,  1.89it/s]Epoch 22:  16%|█▌        | 47/300 [00:24<02:10,  1.94it/s]Epoch 22:  16%|█▌        | 48/300 [00:25<02:13,  1.88it/s]Epoch 22:  16%|█▋        | 49/300 [00:25<02:16,  1.83it/s]Epoch 22:  17%|█▋        | 50/300 [00:26<02:18,  1.81it/s]Epoch 22:  17%|█▋        | 51/300 [00:26<02:17,  1.81it/s]Epoch 22:  17%|█▋        | 52/300 [00:27<02:24,  1.71it/s]Epoch 22:  18%|█▊        | 53/300 [00:27<02:19,  1.77it/s]Epoch 22:  18%|█▊        | 54/300 [00:28<02:15,  1.82it/s]Epoch 22:  18%|█▊        | 55/300 [00:28<02:09,  1.90it/s]Epoch 22:  19%|█▊        | 56/300 [00:29<02:12,  1.85it/s]Epoch 22:  19%|█▉        | 57/300 [00:30<02:13,  1.83it/s]Epoch 22:  19%|█▉        | 58/300 [00:30<02:10,  1.86it/s]Epoch 22:  20%|█▉        | 59/300 [00:31<02:05,  1.92it/s]06/19/2022 20:45:35 - INFO - __main__ - global step: 3330; train loss: 7.142025947570801; dev loss: 7.350700378417969
Epoch 22:  20%|██        | 60/300 [00:31<02:02,  1.96it/s]Epoch 22:  20%|██        | 61/300 [00:32<02:12,  1.81it/s]Epoch 22:  21%|██        | 62/300 [00:32<02:06,  1.88it/s]Epoch 22:  21%|██        | 63/300 [00:33<02:01,  1.95it/s]Epoch 22:  21%|██▏       | 64/300 [00:33<01:59,  1.98it/s]Epoch 22:  22%|██▏       | 65/300 [00:34<02:05,  1.87it/s]Epoch 22:  22%|██▏       | 66/300 [00:34<02:03,  1.89it/s]Epoch 22:  22%|██▏       | 67/300 [00:35<02:00,  1.94it/s]Epoch 22:  23%|██▎       | 68/300 [00:35<01:57,  1.98it/s]Epoch 22:  23%|██▎       | 69/300 [00:36<02:00,  1.91it/s]Epoch 22:  23%|██▎       | 70/300 [00:36<01:57,  1.96it/s]Epoch 22:  24%|██▎       | 71/300 [00:37<01:54,  1.99it/s]Epoch 22:  24%|██▍       | 72/300 [00:37<01:53,  2.02it/s]Epoch 22:  24%|██▍       | 73/300 [00:38<01:57,  1.93it/s]Epoch 22:  25%|██▍       | 74/300 [00:38<01:54,  1.97it/s]Epoch 22:  25%|██▌       | 75/300 [00:39<01:51,  2.01it/s]Epoch 22:  25%|██▌       | 76/300 [00:39<01:50,  2.03it/s]Epoch 22:  26%|██▌       | 77/300 [00:40<01:54,  1.94it/s]Epoch 22:  26%|██▌       | 78/300 [00:40<01:52,  1.98it/s]Epoch 22:  26%|██▋       | 79/300 [00:41<01:49,  2.01it/s]06/19/2022 20:45:46 - INFO - __main__ - global step: 3340; train loss: 7.695564270019531; dev loss: 7.840562343597412
Epoch 22:  27%|██▋       | 80/300 [00:41<01:48,  2.03it/s]Epoch 22:  27%|██▋       | 81/300 [00:42<01:53,  1.94it/s]Epoch 22:  27%|██▋       | 82/300 [00:42<01:50,  1.97it/s]Epoch 22:  28%|██▊       | 83/300 [00:43<01:51,  1.95it/s]Epoch 22:  28%|██▊       | 84/300 [00:43<01:50,  1.95it/s]Epoch 22:  28%|██▊       | 85/300 [00:44<01:49,  1.96it/s]Epoch 22:  29%|██▊       | 86/300 [00:44<01:53,  1.89it/s]Epoch 22:  29%|██▉       | 87/300 [00:45<01:54,  1.86it/s]Epoch 22:  29%|██▉       | 88/300 [00:46<01:51,  1.90it/s]Epoch 22:  30%|██▉       | 89/300 [00:46<01:47,  1.96it/s]Epoch 22:  30%|███       | 90/300 [00:47<01:50,  1.90it/s]Epoch 22:  30%|███       | 91/300 [00:47<01:47,  1.95it/s]Epoch 22:  31%|███       | 92/300 [00:48<01:45,  1.98it/s]Epoch 22:  31%|███       | 93/300 [00:48<01:42,  2.01it/s]Epoch 22:  31%|███▏      | 94/300 [00:49<01:46,  1.93it/s]Epoch 22:  32%|███▏      | 95/300 [00:49<01:48,  1.89it/s]Epoch 22:  32%|███▏      | 96/300 [00:50<01:45,  1.94it/s]Epoch 22:  32%|███▏      | 97/300 [00:50<01:42,  1.97it/s]Epoch 22:  33%|███▎      | 98/300 [00:51<01:48,  1.86it/s]Epoch 22:  33%|███▎      | 99/300 [00:51<01:46,  1.88it/s]06/19/2022 20:45:56 - INFO - __main__ - global step: 3350; train loss: 7.418769836425781; dev loss: 7.429358005523682
Epoch 22:  33%|███▎      | 100/300 [00:52<01:43,  1.94it/s]Epoch 22:  34%|███▎      | 101/300 [00:52<01:40,  1.98it/s]Epoch 22:  34%|███▍      | 102/300 [00:53<01:46,  1.86it/s]Epoch 22:  34%|███▍      | 103/300 [00:53<01:42,  1.92it/s]Epoch 22:  35%|███▍      | 104/300 [00:54<01:44,  1.88it/s]Epoch 22:  35%|███▌      | 105/300 [00:54<01:41,  1.93it/s]Epoch 22:  35%|███▌      | 106/300 [00:55<01:49,  1.78it/s]Epoch 22:  36%|███▌      | 107/300 [00:56<01:48,  1.78it/s]Epoch 22:  36%|███▌      | 108/300 [00:56<01:44,  1.83it/s]Epoch 22:  36%|███▋      | 109/300 [00:57<01:40,  1.90it/s]Epoch 22:  37%|███▋      | 110/300 [00:57<01:42,  1.85it/s]Epoch 22:  37%|███▋      | 111/300 [00:58<01:39,  1.90it/s]Epoch 22:  37%|███▋      | 112/300 [00:58<01:36,  1.95it/s]Epoch 22:  38%|███▊      | 113/300 [00:59<01:38,  1.89it/s]Epoch 22:  38%|███▊      | 114/300 [00:59<01:35,  1.94it/s]Epoch 22:  38%|███▊      | 115/300 [01:00<01:43,  1.79it/s]Epoch 22:  39%|███▊      | 116/300 [01:00<01:38,  1.86it/s]Epoch 22:  39%|███▉      | 117/300 [01:01<01:34,  1.93it/s]Epoch 22:  39%|███▉      | 118/300 [01:01<01:32,  1.98it/s]Epoch 22:  40%|███▉      | 119/300 [01:02<01:35,  1.90it/s]06/19/2022 20:46:07 - INFO - __main__ - global step: 3360; train loss: 6.9351806640625; dev loss: 7.304417610168457
Epoch 22:  40%|████      | 120/300 [01:02<01:36,  1.86it/s]Epoch 22:  40%|████      | 121/300 [01:03<01:32,  1.93it/s]Epoch 22:  41%|████      | 122/300 [01:03<01:34,  1.88it/s]Epoch 22:  41%|████      | 123/300 [01:04<01:41,  1.75it/s]Epoch 22:  41%|████▏     | 124/300 [01:05<01:40,  1.76it/s]Epoch 22:  42%|████▏     | 125/300 [01:05<01:39,  1.76it/s]Epoch 22:  42%|████▏     | 126/300 [01:06<01:34,  1.85it/s]Epoch 22:  42%|████▏     | 127/300 [01:06<01:37,  1.77it/s]Epoch 22:  43%|████▎     | 128/300 [01:07<01:32,  1.85it/s]Epoch 22:  43%|████▎     | 129/300 [01:07<01:29,  1.92it/s]Epoch 22:  43%|████▎     | 130/300 [01:08<01:26,  1.97it/s]Epoch 22:  44%|████▎     | 131/300 [01:08<01:33,  1.81it/s]Epoch 22:  44%|████▍     | 132/300 [01:09<01:29,  1.88it/s]Epoch 22:  44%|████▍     | 133/300 [01:09<01:26,  1.94it/s]Epoch 22:  45%|████▍     | 134/300 [01:10<01:23,  1.98it/s]Epoch 22:  45%|████▌     | 135/300 [01:10<01:28,  1.86it/s]Epoch 22:  45%|████▌     | 136/300 [01:11<01:25,  1.92it/s]Epoch 22:  46%|████▌     | 137/300 [01:11<01:22,  1.97it/s]Epoch 22:  46%|████▌     | 138/300 [01:12<01:21,  2.00it/s]Epoch 22:  46%|████▋     | 139/300 [01:12<01:23,  1.93it/s]06/19/2022 20:46:17 - INFO - __main__ - global step: 3370; train loss: 7.256126403808594; dev loss: 7.271087646484375
Epoch 22:  47%|████▋     | 140/300 [01:13<01:25,  1.87it/s]Epoch 22:  47%|████▋     | 141/300 [01:13<01:22,  1.93it/s]Epoch 22:  47%|████▋     | 142/300 [01:14<01:19,  1.98it/s]Epoch 22:  48%|████▊     | 143/300 [01:14<01:18,  2.01it/s]Epoch 22:  48%|████▊     | 144/300 [01:15<01:21,  1.92it/s]Epoch 22:  48%|████▊     | 145/300 [01:15<01:18,  1.97it/s]Epoch 22:  49%|████▊     | 146/300 [01:16<01:16,  2.01it/s]Epoch 22:  49%|████▉     | 147/300 [01:16<01:16,  2.01it/s]Epoch 22:  49%|████▉     | 148/300 [01:17<01:18,  1.93it/s]Epoch 22:  50%|████▉     | 149/300 [01:18<01:16,  1.97it/s]Epoch 22:  50%|█████     | 150/300 [01:18<01:16,  1.96it/s]Epoch 22:  50%|█████     | 151/300 [01:19<01:14,  2.00it/s]Epoch 22:  51%|█████     | 152/300 [01:19<01:17,  1.91it/s]Epoch 22:  51%|█████     | 153/300 [01:20<01:18,  1.87it/s]Epoch 22:  51%|█████▏    | 154/300 [01:20<01:19,  1.84it/s]Epoch 22:  52%|█████▏    | 155/300 [01:21<01:16,  1.90it/s]Epoch 22:  52%|█████▏    | 156/300 [01:21<01:18,  1.84it/s]Epoch 22:  52%|█████▏    | 157/300 [01:22<01:18,  1.82it/s]Epoch 22:  53%|█████▎    | 158/300 [01:22<01:15,  1.88it/s]Epoch 22:  53%|█████▎    | 159/300 [01:23<01:16,  1.85it/s]06/19/2022 20:46:28 - INFO - __main__ - global step: 3380; train loss: 7.961188316345215; dev loss: 8.154123306274414
Epoch 22:  53%|█████▎    | 160/300 [01:23<01:17,  1.81it/s]Epoch 22:  54%|█████▎    | 161/300 [01:24<01:17,  1.80it/s]Epoch 22:  54%|█████▍    | 162/300 [01:25<01:15,  1.84it/s]Epoch 22:  54%|█████▍    | 163/300 [01:25<01:11,  1.91it/s]Epoch 22:  55%|█████▍    | 164/300 [01:26<01:13,  1.86it/s]Epoch 22:  55%|█████▌    | 165/300 [01:26<01:09,  1.94it/s]Epoch 22:  55%|█████▌    | 166/300 [01:27<01:09,  1.93it/s]Epoch 22:  56%|█████▌    | 167/300 [01:27<01:09,  1.92it/s]Epoch 22:  56%|█████▌    | 168/300 [01:28<01:07,  1.97it/s]Epoch 22:  56%|█████▋    | 169/300 [01:28<01:09,  1.90it/s]Epoch 22:  57%|█████▋    | 170/300 [01:29<01:06,  1.95it/s]Epoch 22:  57%|█████▋    | 171/300 [01:29<01:05,  1.98it/s]Epoch 22:  57%|█████▋    | 172/300 [01:30<01:03,  2.01it/s]Epoch 22:  58%|█████▊    | 173/300 [01:30<01:08,  1.85it/s]Epoch 22:  58%|█████▊    | 174/300 [01:31<01:08,  1.83it/s]Epoch 22:  58%|█████▊    | 175/300 [01:31<01:05,  1.91it/s]Epoch 22:  59%|█████▊    | 176/300 [01:32<01:03,  1.96it/s]Epoch 22:  59%|█████▉    | 177/300 [01:32<01:07,  1.81it/s]Epoch 22:  59%|█████▉    | 178/300 [01:33<01:06,  1.84it/s]Epoch 22:  60%|█████▉    | 179/300 [01:33<01:03,  1.90it/s]06/19/2022 20:46:38 - INFO - __main__ - global step: 3390; train loss: 7.896939754486084; dev loss: 7.671414375305176
Epoch 22:  60%|██████    | 180/300 [01:34<01:01,  1.95it/s]Epoch 22:  60%|██████    | 181/300 [01:34<01:03,  1.89it/s]Epoch 22:  61%|██████    | 182/300 [01:35<01:02,  1.90it/s]Epoch 22:  61%|██████    | 183/300 [01:36<01:02,  1.86it/s]Epoch 22:  61%|██████▏   | 184/300 [01:36<01:03,  1.84it/s]Epoch 22:  62%|██████▏   | 185/300 [01:37<01:04,  1.79it/s]Epoch 22:  62%|██████▏   | 186/300 [01:37<01:02,  1.82it/s]Epoch 22:  62%|██████▏   | 187/300 [01:38<01:00,  1.87it/s]Epoch 22:  63%|██████▎   | 188/300 [01:38<00:58,  1.93it/s]Epoch 22:  63%|██████▎   | 189/300 [01:39<00:59,  1.87it/s]Epoch 22:  63%|██████▎   | 190/300 [01:39<00:58,  1.87it/s]Epoch 22:  64%|██████▎   | 191/300 [01:40<00:57,  1.89it/s]Epoch 22:  64%|██████▍   | 192/300 [01:40<00:55,  1.94it/s]Epoch 22:  64%|██████▍   | 193/300 [01:41<00:53,  1.99it/s]Epoch 22:  65%|██████▍   | 194/300 [01:41<00:55,  1.91it/s]Epoch 22:  65%|██████▌   | 195/300 [01:42<00:53,  1.96it/s]Epoch 22:  65%|██████▌   | 196/300 [01:42<00:52,  2.00it/s]Epoch 22:  66%|██████▌   | 197/300 [01:43<00:50,  2.03it/s]Epoch 22:  66%|██████▌   | 198/300 [01:43<00:52,  1.94it/s]Epoch 22:  66%|██████▋   | 199/300 [01:44<00:51,  1.98it/s]06/19/2022 20:46:49 - INFO - __main__ - global step: 3400; train loss: 7.288012504577637; dev loss: 7.361367225646973
Epoch 22:  67%|██████▋   | 200/300 [01:44<00:49,  2.01it/s]Epoch 22:  67%|██████▋   | 201/300 [01:45<00:51,  1.94it/s]Epoch 22:  67%|██████▋   | 202/300 [01:46<00:54,  1.80it/s]Epoch 22:  68%|██████▊   | 203/300 [01:46<00:51,  1.88it/s]Epoch 22:  68%|██████▊   | 204/300 [01:47<00:50,  1.90it/s]Epoch 22:  68%|██████▊   | 205/300 [01:47<00:48,  1.95it/s]Epoch 22:  69%|██████▊   | 206/300 [01:48<00:49,  1.89it/s]Epoch 22:  69%|██████▉   | 207/300 [01:48<00:47,  1.95it/s]Epoch 22:  69%|██████▉   | 208/300 [01:49<00:46,  1.96it/s]Epoch 22:  70%|██████▉   | 209/300 [01:49<00:46,  1.95it/s]Epoch 22:  70%|███████   | 210/300 [01:50<00:47,  1.88it/s]Epoch 22:  70%|███████   | 211/300 [01:50<00:48,  1.85it/s]Epoch 22:  71%|███████   | 212/300 [01:51<00:47,  1.87it/s]Epoch 22:  71%|███████   | 213/300 [01:51<00:44,  1.93it/s]Epoch 22:  71%|███████▏  | 214/300 [01:52<00:47,  1.83it/s]Epoch 22:  72%|███████▏  | 215/300 [01:52<00:44,  1.90it/s]Epoch 22:  72%|███████▏  | 216/300 [01:53<00:44,  1.91it/s]Epoch 22:  72%|███████▏  | 217/300 [01:53<00:42,  1.95it/s]Epoch 22:  73%|███████▎  | 218/300 [01:54<00:44,  1.86it/s]Epoch 22:  73%|███████▎  | 219/300 [01:54<00:42,  1.89it/s]06/19/2022 20:46:59 - INFO - __main__ - global step: 3410; train loss: 7.886784553527832; dev loss: 7.555708408355713
Epoch 22:  73%|███████▎  | 220/300 [01:55<00:41,  1.94it/s]Epoch 22:  74%|███████▎  | 221/300 [01:55<00:41,  1.91it/s]Epoch 22:  74%|███████▍  | 222/300 [01:56<00:39,  1.96it/s]Epoch 22:  74%|███████▍  | 223/300 [01:57<00:42,  1.80it/s]Epoch 22:  75%|███████▍  | 224/300 [01:57<00:40,  1.88it/s]Epoch 22:  75%|███████▌  | 225/300 [01:58<00:38,  1.94it/s]Epoch 22:  75%|███████▌  | 226/300 [01:58<00:37,  1.99it/s]Epoch 22:  76%|███████▌  | 227/300 [01:59<00:38,  1.91it/s]Epoch 22:  76%|███████▌  | 228/300 [01:59<00:36,  1.97it/s]Epoch 22:  76%|███████▋  | 229/300 [02:00<00:35,  2.00it/s]Epoch 22:  77%|███████▋  | 230/300 [02:00<00:35,  1.99it/s]Epoch 22:  77%|███████▋  | 231/300 [02:01<00:35,  1.92it/s]Epoch 22:  77%|███████▋  | 232/300 [02:01<00:34,  1.98it/s]Epoch 22:  78%|███████▊  | 233/300 [02:02<00:33,  2.00it/s]Epoch 22:  78%|███████▊  | 234/300 [02:02<00:32,  2.03it/s]Epoch 22:  78%|███████▊  | 235/300 [02:03<00:33,  1.93it/s]Epoch 22:  79%|███████▊  | 236/300 [02:03<00:32,  1.97it/s]Epoch 22:  79%|███████▉  | 237/300 [02:04<00:31,  2.00it/s]Epoch 22:  79%|███████▉  | 238/300 [02:04<00:30,  2.01it/s]Epoch 22:  80%|███████▉  | 239/300 [02:05<00:32,  1.89it/s]06/19/2022 20:47:10 - INFO - __main__ - global step: 3420; train loss: 7.716522216796875; dev loss: 7.641747951507568
Epoch 22:  80%|████████  | 240/300 [02:05<00:30,  1.94it/s]Epoch 22:  80%|████████  | 241/300 [02:06<00:29,  1.97it/s]Epoch 22:  81%|████████  | 242/300 [02:06<00:28,  2.00it/s]Epoch 22:  81%|████████  | 243/300 [02:07<00:30,  1.90it/s]Epoch 22:  81%|████████▏ | 244/300 [02:07<00:28,  1.95it/s]Epoch 22:  82%|████████▏ | 245/300 [02:08<00:27,  1.99it/s]Epoch 22:  82%|████████▏ | 246/300 [02:08<00:26,  2.02it/s]Epoch 22:  82%|████████▏ | 247/300 [02:09<00:26,  2.03it/s]Epoch 22:  83%|████████▎ | 248/300 [02:09<00:28,  1.84it/s]Epoch 22:  83%|████████▎ | 249/300 [02:10<00:26,  1.91it/s]Epoch 22:  83%|████████▎ | 250/300 [02:10<00:25,  1.96it/s]Epoch 22:  84%|████████▎ | 251/300 [02:11<00:24,  1.99it/s]Epoch 22:  84%|████████▍ | 252/300 [02:11<00:25,  1.91it/s]Epoch 22:  84%|████████▍ | 253/300 [02:12<00:24,  1.92it/s]Epoch 22:  85%|████████▍ | 254/300 [02:12<00:23,  1.92it/s]Epoch 22:  85%|████████▌ | 255/300 [02:13<00:22,  1.98it/s]Epoch 22:  85%|████████▌ | 256/300 [02:13<00:23,  1.91it/s]Epoch 22:  86%|████████▌ | 257/300 [02:14<00:21,  1.97it/s]Epoch 22:  86%|████████▌ | 258/300 [02:14<00:21,  2.00it/s]Epoch 22:  86%|████████▋ | 259/300 [02:15<00:20,  2.03it/s]06/19/2022 20:47:20 - INFO - __main__ - global step: 3430; train loss: 7.732368469238281; dev loss: 7.620661735534668
Epoch 22:  87%|████████▋ | 260/300 [02:15<00:21,  1.88it/s]Epoch 22:  87%|████████▋ | 261/300 [02:16<00:20,  1.94it/s]Epoch 22:  87%|████████▋ | 262/300 [02:16<00:20,  1.89it/s]Epoch 22:  88%|████████▊ | 263/300 [02:17<00:19,  1.94it/s]Epoch 22:  88%|████████▊ | 264/300 [02:18<00:19,  1.87it/s]Epoch 22:  88%|████████▊ | 265/300 [02:18<00:18,  1.89it/s]Epoch 22:  89%|████████▊ | 266/300 [02:19<00:17,  1.94it/s]Epoch 22:  89%|████████▉ | 267/300 [02:19<00:16,  1.98it/s]Epoch 22:  89%|████████▉ | 268/300 [02:20<00:16,  1.90it/s]Epoch 22:  90%|████████▉ | 269/300 [02:20<00:16,  1.87it/s]Epoch 22:  90%|█████████ | 270/300 [02:21<00:15,  1.92it/s]Epoch 22:  90%|█████████ | 271/300 [02:21<00:14,  1.97it/s]Epoch 22:  91%|█████████ | 272/300 [02:22<00:15,  1.83it/s]Epoch 22:  91%|█████████ | 273/300 [02:22<00:14,  1.90it/s]Epoch 22:  91%|█████████▏| 274/300 [02:23<00:13,  1.90it/s]Epoch 22:  92%|█████████▏| 275/300 [02:23<00:12,  1.95it/s]Epoch 22:  92%|█████████▏| 276/300 [02:24<00:12,  1.98it/s]Epoch 22:  92%|█████████▏| 277/300 [02:24<00:12,  1.91it/s]Epoch 22:  93%|█████████▎| 278/300 [02:25<00:11,  1.95it/s]Epoch 22:  93%|█████████▎| 279/300 [02:25<00:10,  1.99it/s]06/19/2022 20:47:30 - INFO - __main__ - global step: 3440; train loss: 7.2895355224609375; dev loss: 7.062234401702881
Epoch 22:  93%|█████████▎| 280/300 [02:26<00:09,  2.01it/s]Epoch 22:  94%|█████████▎| 281/300 [02:26<00:09,  1.91it/s]Epoch 22:  94%|█████████▍| 282/300 [02:27<00:09,  1.96it/s]Epoch 22:  94%|█████████▍| 283/300 [02:27<00:08,  1.99it/s]Epoch 22:  95%|█████████▍| 284/300 [02:28<00:08,  1.92it/s]Epoch 22:  95%|█████████▌| 285/300 [02:28<00:08,  1.79it/s]Epoch 22:  95%|█████████▌| 286/300 [02:29<00:07,  1.79it/s]Epoch 22:  96%|█████████▌| 287/300 [02:30<00:07,  1.84it/s]Epoch 22:  96%|█████████▌| 288/300 [02:30<00:06,  1.91it/s]Epoch 22:  96%|█████████▋| 289/300 [02:31<00:05,  1.86it/s]Epoch 22:  97%|█████████▋| 290/300 [02:31<00:05,  1.93it/s]Epoch 22:  97%|█████████▋| 291/300 [02:32<00:04,  1.97it/s]Epoch 22:  97%|█████████▋| 292/300 [02:32<00:04,  1.89it/s]Epoch 22:  98%|█████████▊| 293/300 [02:33<00:03,  1.77it/s]Epoch 22:  98%|█████████▊| 294/300 [02:33<00:03,  1.85it/s]Epoch 22:  98%|█████████▊| 295/300 [02:34<00:02,  1.91it/s]Epoch 22:  99%|█████████▊| 296/300 [02:34<00:02,  1.96it/s]Epoch 22:  99%|█████████▉| 297/300 [02:35<00:01,  1.83it/s]Epoch 22:  99%|█████████▉| 298/300 [02:35<00:01,  1.90it/s]Epoch 22: 100%|█████████▉| 299/300 [02:36<00:00,  1.95it/s]06/19/2022 20:47:41 - INFO - __main__ - global step: 3450; train loss: 7.205016136169434; dev loss: 7.274859428405762
Epoch 22: 100%|██████████| 300/300 [02:36<00:00,  1.98it/s]Epoch 22: 100%|██████████| 300/300 [02:36<00:00,  1.91it/s]
Epoch 23:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 23:   0%|          | 1/300 [00:00<02:21,  2.11it/s]Epoch 23:   1%|          | 2/300 [00:01<02:38,  1.87it/s]Epoch 23:   1%|          | 3/300 [00:01<02:31,  1.96it/s]Epoch 23:   1%|▏         | 4/300 [00:02<02:28,  1.99it/s]Epoch 23:   2%|▏         | 5/300 [00:02<02:34,  1.91it/s]Epoch 23:   2%|▏         | 6/300 [00:03<02:38,  1.86it/s]Epoch 23:   2%|▏         | 7/300 [00:03<02:40,  1.83it/s]Epoch 23:   3%|▎         | 8/300 [00:04<02:33,  1.91it/s]Epoch 23:   3%|▎         | 9/300 [00:04<02:28,  1.96it/s]Epoch 23:   3%|▎         | 10/300 [00:05<02:34,  1.88it/s]Epoch 23:   4%|▎         | 11/300 [00:05<02:28,  1.94it/s]Epoch 23:   4%|▍         | 12/300 [00:06<02:25,  1.98it/s]Epoch 23:   4%|▍         | 13/300 [00:06<02:22,  2.02it/s]Epoch 23:   5%|▍         | 14/300 [00:07<02:29,  1.91it/s]Epoch 23:   5%|▌         | 15/300 [00:07<02:25,  1.96it/s]Epoch 23:   5%|▌         | 16/300 [00:08<02:21,  2.00it/s]Epoch 23:   6%|▌         | 17/300 [00:08<02:26,  1.93it/s]Epoch 23:   6%|▌         | 18/300 [00:09<02:39,  1.77it/s]Epoch 23:   6%|▋         | 19/300 [00:09<02:32,  1.84it/s]06/19/2022 20:47:51 - INFO - __main__ - global step: 3460; train loss: 7.819934844970703; dev loss: 7.840306282043457
Epoch 23:   7%|▋         | 20/300 [00:10<02:29,  1.87it/s]Epoch 23:   7%|▋         | 21/300 [00:10<02:28,  1.88it/s]Epoch 23:   7%|▋         | 22/300 [00:11<02:31,  1.84it/s]Epoch 23:   8%|▊         | 23/300 [00:12<02:29,  1.85it/s]Epoch 23:   8%|▊         | 24/300 [00:12<02:26,  1.89it/s]Epoch 23:   8%|▊         | 25/300 [00:13<02:21,  1.94it/s]Epoch 23:   9%|▊         | 26/300 [00:13<02:26,  1.87it/s]Epoch 23:   9%|▉         | 27/300 [00:14<02:21,  1.92it/s]Epoch 23:   9%|▉         | 28/300 [00:14<02:20,  1.93it/s]Epoch 23:  10%|▉         | 29/300 [00:15<02:17,  1.97it/s]Epoch 23:  10%|█         | 30/300 [00:15<02:18,  1.96it/s]Epoch 23:  10%|█         | 31/300 [00:16<02:22,  1.88it/s]Epoch 23:  11%|█         | 32/300 [00:16<02:18,  1.94it/s]Epoch 23:  11%|█         | 33/300 [00:17<02:14,  1.98it/s]Epoch 23:  11%|█▏        | 34/300 [00:17<02:12,  2.01it/s]Epoch 23:  12%|█▏        | 35/300 [00:18<02:16,  1.94it/s]Epoch 23:  12%|█▏        | 36/300 [00:18<02:19,  1.89it/s]Epoch 23:  12%|█▏        | 37/300 [00:19<02:15,  1.94it/s]Epoch 23:  13%|█▎        | 38/300 [00:19<02:15,  1.93it/s]Epoch 23:  13%|█▎        | 39/300 [00:20<02:19,  1.87it/s]06/19/2022 20:48:02 - INFO - __main__ - global step: 3470; train loss: 7.592984676361084; dev loss: 7.576298713684082
Epoch 23:  13%|█▎        | 40/300 [00:20<02:14,  1.93it/s]Epoch 23:  14%|█▎        | 41/300 [00:21<02:12,  1.96it/s]Epoch 23:  14%|█▍        | 42/300 [00:21<02:16,  1.90it/s]Epoch 23:  14%|█▍        | 43/300 [00:22<02:19,  1.85it/s]Epoch 23:  15%|█▍        | 44/300 [00:23<02:19,  1.83it/s]Epoch 23:  15%|█▌        | 45/300 [00:23<02:20,  1.82it/s]Epoch 23:  15%|█▌        | 46/300 [00:24<02:14,  1.89it/s]Epoch 23:  16%|█▌        | 47/300 [00:24<02:22,  1.77it/s]Epoch 23:  16%|█▌        | 48/300 [00:25<02:22,  1.77it/s]Epoch 23:  16%|█▋        | 49/300 [00:25<02:14,  1.86it/s]Epoch 23:  17%|█▋        | 50/300 [00:26<02:09,  1.93it/s]Epoch 23:  17%|█▋        | 51/300 [00:26<02:20,  1.77it/s]Epoch 23:  17%|█▋        | 52/300 [00:27<02:14,  1.85it/s]Epoch 23:  18%|█▊        | 53/300 [00:27<02:09,  1.90it/s]Epoch 23:  18%|█▊        | 54/300 [00:28<02:12,  1.86it/s]Epoch 23:  18%|█▊        | 55/300 [00:28<02:07,  1.92it/s]Epoch 23:  19%|█▊        | 56/300 [00:29<02:11,  1.85it/s]Epoch 23:  19%|█▉        | 57/300 [00:30<02:13,  1.82it/s]Epoch 23:  19%|█▉        | 58/300 [00:30<02:07,  1.90it/s]Epoch 23:  20%|█▉        | 59/300 [00:31<02:04,  1.94it/s]06/19/2022 20:48:12 - INFO - __main__ - global step: 3480; train loss: 7.904780387878418; dev loss: 7.918265342712402
Epoch 23:  20%|██        | 60/300 [00:31<02:08,  1.87it/s]Epoch 23:  20%|██        | 61/300 [00:32<02:10,  1.84it/s]Epoch 23:  21%|██        | 62/300 [00:32<02:10,  1.82it/s]Epoch 23:  21%|██        | 63/300 [00:33<02:05,  1.89it/s]Epoch 23:  21%|██▏       | 64/300 [00:33<02:07,  1.85it/s]Epoch 23:  22%|██▏       | 65/300 [00:34<02:03,  1.90it/s]Epoch 23:  22%|██▏       | 66/300 [00:34<01:59,  1.96it/s]Epoch 23:  22%|██▏       | 67/300 [00:35<02:00,  1.93it/s]Epoch 23:  23%|██▎       | 68/300 [00:35<02:05,  1.86it/s]Epoch 23:  23%|██▎       | 69/300 [00:36<02:00,  1.92it/s]Epoch 23:  23%|██▎       | 70/300 [00:36<01:56,  1.97it/s]Epoch 23:  24%|██▎       | 71/300 [00:37<01:54,  2.00it/s]Epoch 23:  24%|██▍       | 72/300 [00:37<02:04,  1.84it/s]Epoch 23:  24%|██▍       | 73/300 [00:38<01:58,  1.91it/s]Epoch 23:  25%|██▍       | 74/300 [00:39<02:01,  1.87it/s]Epoch 23:  25%|██▌       | 75/300 [00:39<01:56,  1.93it/s]Epoch 23:  25%|██▌       | 76/300 [00:40<01:59,  1.87it/s]Epoch 23:  26%|██▌       | 77/300 [00:40<01:55,  1.93it/s]Epoch 23:  26%|██▌       | 78/300 [00:41<01:54,  1.94it/s]Epoch 23:  26%|██▋       | 79/300 [00:41<01:51,  1.98it/s]06/19/2022 20:48:23 - INFO - __main__ - global step: 3490; train loss: 7.484461784362793; dev loss: 7.379797458648682
Epoch 23:  27%|██▋       | 80/300 [00:42<01:56,  1.88it/s]Epoch 23:  27%|██▋       | 81/300 [00:42<01:54,  1.91it/s]Epoch 23:  27%|██▋       | 82/300 [00:43<01:51,  1.96it/s]Epoch 23:  28%|██▊       | 83/300 [00:43<01:48,  1.99it/s]Epoch 23:  28%|██▊       | 84/300 [00:44<01:52,  1.93it/s]Epoch 23:  28%|██▊       | 85/300 [00:44<01:55,  1.86it/s]Epoch 23:  29%|██▊       | 86/300 [00:45<01:51,  1.91it/s]Epoch 23:  29%|██▉       | 87/300 [00:45<01:53,  1.88it/s]Epoch 23:  29%|██▉       | 88/300 [00:46<01:49,  1.94it/s]Epoch 23:  30%|██▉       | 89/300 [00:46<01:54,  1.85it/s]Epoch 23:  30%|███       | 90/300 [00:47<01:49,  1.91it/s]Epoch 23:  30%|███       | 91/300 [00:47<01:47,  1.95it/s]Epoch 23:  31%|███       | 92/300 [00:48<01:44,  1.99it/s]Epoch 23:  31%|███       | 93/300 [00:48<01:47,  1.92it/s]Epoch 23:  31%|███▏      | 94/300 [00:49<01:44,  1.97it/s]Epoch 23:  32%|███▏      | 95/300 [00:49<01:42,  2.00it/s]Epoch 23:  32%|███▏      | 96/300 [00:50<01:40,  2.03it/s]Epoch 23:  32%|███▏      | 97/300 [00:50<01:44,  1.93it/s]Epoch 23:  33%|███▎      | 98/300 [00:51<01:42,  1.97it/s]Epoch 23:  33%|███▎      | 99/300 [00:51<01:40,  2.01it/s]06/19/2022 20:48:33 - INFO - __main__ - global step: 3500; train loss: 7.39685583114624; dev loss: 7.817399501800537
Epoch 23:  33%|███▎      | 100/300 [00:52<01:40,  1.99it/s]Epoch 23:  34%|███▎      | 101/300 [00:52<01:44,  1.91it/s]Epoch 23:  34%|███▍      | 102/300 [00:53<01:41,  1.96it/s]Epoch 23:  34%|███▍      | 103/300 [00:53<01:43,  1.90it/s]Epoch 23:  35%|███▍      | 104/300 [00:54<01:42,  1.90it/s]Epoch 23:  35%|███▌      | 105/300 [00:55<01:45,  1.85it/s]Epoch 23:  35%|███▌      | 106/300 [00:55<01:41,  1.91it/s]Epoch 23:  36%|███▌      | 107/300 [00:56<01:38,  1.96it/s]Epoch 23:  36%|███▌      | 108/300 [00:56<01:36,  1.99it/s]Epoch 23:  36%|███▋      | 109/300 [00:56<01:34,  2.02it/s]Epoch 23:  37%|███▋      | 110/300 [00:57<01:39,  1.92it/s]Epoch 23:  37%|███▋      | 111/300 [00:58<01:36,  1.96it/s]Epoch 23:  37%|███▋      | 112/300 [00:58<01:34,  2.00it/s]Epoch 23:  38%|███▊      | 113/300 [00:59<01:32,  2.02it/s]Epoch 23:  38%|███▊      | 114/300 [00:59<01:41,  1.84it/s]Epoch 23:  38%|███▊      | 115/300 [01:00<01:39,  1.86it/s]Epoch 23:  39%|███▊      | 116/300 [01:00<01:40,  1.83it/s]Epoch 23:  39%|███▉      | 117/300 [01:01<01:36,  1.89it/s]Epoch 23:  39%|███▉      | 118/300 [01:01<01:42,  1.77it/s]Epoch 23:  40%|███▉      | 119/300 [01:02<01:39,  1.81it/s]06/19/2022 20:48:44 - INFO - __main__ - global step: 3510; train loss: 7.648685455322266; dev loss: 7.203023433685303
Epoch 23:  40%|████      | 120/300 [01:02<01:37,  1.84it/s]Epoch 23:  40%|████      | 121/300 [01:03<01:33,  1.91it/s]Epoch 23:  41%|████      | 122/300 [01:04<01:40,  1.77it/s]Epoch 23:  41%|████      | 123/300 [01:04<01:35,  1.86it/s]Epoch 23:  41%|████▏     | 124/300 [01:05<01:31,  1.93it/s]Epoch 23:  42%|████▏     | 125/300 [01:05<01:28,  1.98it/s]Epoch 23:  42%|████▏     | 126/300 [01:06<01:31,  1.90it/s]Epoch 23:  42%|████▏     | 127/300 [01:06<01:32,  1.86it/s]Epoch 23:  43%|████▎     | 128/300 [01:07<01:29,  1.93it/s]Epoch 23:  43%|████▎     | 129/300 [01:07<01:30,  1.88it/s]Epoch 23:  43%|████▎     | 130/300 [01:08<01:32,  1.84it/s]Epoch 23:  44%|████▎     | 131/300 [01:08<01:28,  1.91it/s]Epoch 23:  44%|████▍     | 132/300 [01:09<01:25,  1.96it/s]Epoch 23:  44%|████▍     | 133/300 [01:09<01:23,  2.00it/s]Epoch 23:  45%|████▍     | 134/300 [01:10<01:27,  1.90it/s]Epoch 23:  45%|████▌     | 135/300 [01:10<01:25,  1.94it/s]Epoch 23:  45%|████▌     | 136/300 [01:11<01:22,  1.98it/s]Epoch 23:  46%|████▌     | 137/300 [01:11<01:25,  1.91it/s]Epoch 23:  46%|████▌     | 138/300 [01:12<01:26,  1.87it/s]Epoch 23:  46%|████▋     | 139/300 [01:12<01:28,  1.82it/s]06/19/2022 20:48:54 - INFO - __main__ - global step: 3520; train loss: 7.801078796386719; dev loss: 7.822666168212891
Epoch 23:  47%|████▋     | 140/300 [01:13<01:24,  1.89it/s]Epoch 23:  47%|████▋     | 141/300 [01:13<01:21,  1.94it/s]Epoch 23:  47%|████▋     | 142/300 [01:14<01:19,  1.98it/s]Epoch 23:  48%|████▊     | 143/300 [01:14<01:22,  1.91it/s]Epoch 23:  48%|████▊     | 144/300 [01:15<01:19,  1.97it/s]Epoch 23:  48%|████▊     | 145/300 [01:15<01:17,  2.00it/s]Epoch 23:  49%|████▊     | 146/300 [01:16<01:15,  2.03it/s]Epoch 23:  49%|████▉     | 147/300 [01:16<01:19,  1.94it/s]Epoch 23:  49%|████▉     | 148/300 [01:17<01:16,  1.98it/s]Epoch 23:  50%|████▉     | 149/300 [01:17<01:15,  2.00it/s]Epoch 23:  50%|█████     | 150/300 [01:18<01:14,  2.02it/s]Epoch 23:  50%|█████     | 151/300 [01:19<01:18,  1.90it/s]Epoch 23:  51%|█████     | 152/300 [01:19<01:19,  1.86it/s]Epoch 23:  51%|█████     | 153/300 [01:20<01:16,  1.91it/s]Epoch 23:  51%|█████▏    | 154/300 [01:20<01:14,  1.96it/s]Epoch 23:  52%|█████▏    | 155/300 [01:21<01:17,  1.88it/s]Epoch 23:  52%|█████▏    | 156/300 [01:21<01:15,  1.90it/s]Epoch 23:  52%|█████▏    | 157/300 [01:22<01:12,  1.96it/s]Epoch 23:  53%|█████▎    | 158/300 [01:22<01:11,  2.00it/s]Epoch 23:  53%|█████▎    | 159/300 [01:23<01:13,  1.92it/s]06/19/2022 20:49:04 - INFO - __main__ - global step: 3530; train loss: 7.682360649108887; dev loss: 7.664315223693848
Epoch 23:  53%|█████▎    | 160/300 [01:23<01:13,  1.91it/s]Epoch 23:  54%|█████▎    | 161/300 [01:24<01:14,  1.87it/s]Epoch 23:  54%|█████▍    | 162/300 [01:24<01:14,  1.85it/s]Epoch 23:  54%|█████▍    | 163/300 [01:25<01:11,  1.91it/s]Epoch 23:  55%|█████▍    | 164/300 [01:25<01:16,  1.78it/s]Epoch 23:  55%|█████▌    | 165/300 [01:26<01:12,  1.86it/s]Epoch 23:  55%|█████▌    | 166/300 [01:26<01:13,  1.83it/s]Epoch 23:  56%|█████▌    | 167/300 [01:27<01:13,  1.81it/s]Epoch 23:  56%|█████▌    | 168/300 [01:28<01:13,  1.79it/s]Epoch 23:  56%|█████▋    | 169/300 [01:28<01:09,  1.87it/s]Epoch 23:  57%|█████▋    | 170/300 [01:29<01:08,  1.90it/s]Epoch 23:  57%|█████▋    | 171/300 [01:29<01:09,  1.87it/s]Epoch 23:  57%|█████▋    | 172/300 [01:30<01:12,  1.76it/s]Epoch 23:  58%|█████▊    | 173/300 [01:30<01:08,  1.85it/s]Epoch 23:  58%|█████▊    | 174/300 [01:31<01:07,  1.87it/s]Epoch 23:  58%|█████▊    | 175/300 [01:31<01:07,  1.85it/s]Epoch 23:  59%|█████▊    | 176/300 [01:32<01:08,  1.82it/s]Epoch 23:  59%|█████▉    | 177/300 [01:32<01:05,  1.88it/s]Epoch 23:  59%|█████▉    | 178/300 [01:33<01:02,  1.94it/s]Epoch 23:  60%|█████▉    | 179/300 [01:33<01:00,  1.99it/s]06/19/2022 20:49:15 - INFO - __main__ - global step: 3540; train loss: 7.2380523681640625; dev loss: 7.285719871520996
Epoch 23:  60%|██████    | 180/300 [01:34<01:02,  1.91it/s]Epoch 23:  60%|██████    | 181/300 [01:34<01:00,  1.97it/s]Epoch 23:  61%|██████    | 182/300 [01:35<00:58,  2.01it/s]Epoch 23:  61%|██████    | 183/300 [01:35<01:00,  1.94it/s]Epoch 23:  61%|██████▏   | 184/300 [01:36<01:01,  1.88it/s]Epoch 23:  62%|██████▏   | 185/300 [01:37<00:59,  1.94it/s]Epoch 23:  62%|██████▏   | 186/300 [01:37<00:58,  1.96it/s]Epoch 23:  62%|██████▏   | 187/300 [01:37<00:56,  1.99it/s]Epoch 23:  63%|██████▎   | 188/300 [01:38<01:01,  1.82it/s]Epoch 23:  63%|██████▎   | 189/300 [01:39<00:58,  1.89it/s]Epoch 23:  63%|██████▎   | 190/300 [01:39<00:56,  1.95it/s]Epoch 23:  64%|██████▎   | 191/300 [01:40<00:54,  1.99it/s]Epoch 23:  64%|██████▍   | 192/300 [01:40<00:56,  1.93it/s]Epoch 23:  64%|██████▍   | 193/300 [01:41<00:57,  1.86it/s]Epoch 23:  65%|██████▍   | 194/300 [01:41<00:55,  1.92it/s]Epoch 23:  65%|██████▌   | 195/300 [01:42<00:53,  1.97it/s]Epoch 23:  65%|██████▌   | 196/300 [01:42<00:51,  2.01it/s]Epoch 23:  66%|██████▌   | 197/300 [01:43<00:53,  1.91it/s]Epoch 23:  66%|██████▌   | 198/300 [01:43<00:51,  1.97it/s]Epoch 23:  66%|██████▋   | 199/300 [01:44<00:50,  1.99it/s]06/19/2022 20:49:25 - INFO - __main__ - global step: 3550; train loss: 7.50836181640625; dev loss: 7.352882385253906
Epoch 23:  67%|██████▋   | 200/300 [01:44<00:49,  2.02it/s]Epoch 23:  67%|██████▋   | 201/300 [01:45<00:51,  1.93it/s]Epoch 23:  67%|██████▋   | 202/300 [01:45<00:52,  1.88it/s]Epoch 23:  68%|██████▊   | 203/300 [01:46<00:49,  1.95it/s]Epoch 23:  68%|██████▊   | 204/300 [01:46<00:48,  1.98it/s]Epoch 23:  68%|██████▊   | 205/300 [01:47<00:50,  1.89it/s]Epoch 23:  69%|██████▊   | 206/300 [01:47<00:48,  1.94it/s]Epoch 23:  69%|██████▉   | 207/300 [01:48<00:48,  1.93it/s]Epoch 23:  69%|██████▉   | 208/300 [01:48<00:46,  1.97it/s]Epoch 23:  70%|██████▉   | 209/300 [01:49<00:48,  1.89it/s]Epoch 23:  70%|███████   | 210/300 [01:49<00:47,  1.90it/s]Epoch 23:  70%|███████   | 211/300 [01:50<00:45,  1.95it/s]Epoch 23:  71%|███████   | 212/300 [01:50<00:44,  1.99it/s]Epoch 23:  71%|███████   | 213/300 [01:51<00:45,  1.91it/s]Epoch 23:  71%|███████▏  | 214/300 [01:51<00:43,  1.98it/s]Epoch 23:  72%|███████▏  | 215/300 [01:52<00:42,  1.98it/s]Epoch 23:  72%|███████▏  | 216/300 [01:52<00:41,  2.01it/s]Epoch 23:  72%|███████▏  | 217/300 [01:53<00:42,  1.94it/s]Epoch 23:  73%|███████▎  | 218/300 [01:54<00:43,  1.86it/s]Epoch 23:  73%|███████▎  | 219/300 [01:54<00:44,  1.84it/s]06/19/2022 20:49:36 - INFO - __main__ - global step: 3560; train loss: 7.655540466308594; dev loss: 7.849775791168213
Epoch 23:  73%|███████▎  | 220/300 [01:55<00:41,  1.91it/s]Epoch 23:  74%|███████▎  | 221/300 [01:55<00:40,  1.96it/s]Epoch 23:  74%|███████▍  | 222/300 [01:56<00:43,  1.81it/s]Epoch 23:  74%|███████▍  | 223/300 [01:56<00:40,  1.88it/s]Epoch 23:  75%|███████▍  | 224/300 [01:57<00:39,  1.93it/s]Epoch 23:  75%|███████▌  | 225/300 [01:57<00:38,  1.93it/s]Epoch 23:  75%|███████▌  | 226/300 [01:58<00:39,  1.87it/s]Epoch 23:  76%|███████▌  | 227/300 [01:58<00:37,  1.92it/s]Epoch 23:  76%|███████▌  | 228/300 [01:59<00:37,  1.92it/s]Epoch 23:  76%|███████▋  | 229/300 [01:59<00:37,  1.88it/s]Epoch 23:  77%|███████▋  | 230/300 [02:00<00:38,  1.83it/s]Epoch 23:  77%|███████▋  | 231/300 [02:00<00:36,  1.91it/s]Epoch 23:  77%|███████▋  | 232/300 [02:01<00:35,  1.91it/s]Epoch 23:  78%|███████▊  | 233/300 [02:01<00:34,  1.96it/s]Epoch 23:  78%|███████▊  | 234/300 [02:02<00:35,  1.84it/s]Epoch 23:  78%|███████▊  | 235/300 [02:03<00:34,  1.91it/s]Epoch 23:  79%|███████▊  | 236/300 [02:03<00:32,  1.95it/s]Epoch 23:  79%|███████▉  | 237/300 [02:04<00:33,  1.90it/s]Epoch 23:  79%|███████▉  | 238/300 [02:04<00:33,  1.85it/s]Epoch 23:  80%|███████▉  | 239/300 [02:05<00:31,  1.91it/s]06/19/2022 20:49:46 - INFO - __main__ - global step: 3570; train loss: 7.617368221282959; dev loss: 7.551504611968994
Epoch 23:  80%|████████  | 240/300 [02:05<00:30,  1.96it/s]Epoch 23:  80%|████████  | 241/300 [02:06<00:29,  2.00it/s]Epoch 23:  81%|████████  | 242/300 [02:06<00:31,  1.87it/s]Epoch 23:  81%|████████  | 243/300 [02:07<00:30,  1.89it/s]Epoch 23:  81%|████████▏ | 244/300 [02:07<00:28,  1.94it/s]Epoch 23:  82%|████████▏ | 245/300 [02:08<00:29,  1.89it/s]Epoch 23:  82%|████████▏ | 246/300 [02:08<00:27,  1.94it/s]Epoch 23:  82%|████████▏ | 247/300 [02:09<00:28,  1.87it/s]Epoch 23:  83%|████████▎ | 248/300 [02:09<00:27,  1.92it/s]Epoch 23:  83%|████████▎ | 249/300 [02:10<00:25,  1.96it/s]Epoch 23:  83%|████████▎ | 250/300 [02:10<00:25,  2.00it/s]Epoch 23:  84%|████████▎ | 251/300 [02:11<00:25,  1.91it/s]Epoch 23:  84%|████████▍ | 252/300 [02:11<00:24,  1.97it/s]Epoch 23:  84%|████████▍ | 253/300 [02:12<00:24,  1.90it/s]Epoch 23:  85%|████████▍ | 254/300 [02:12<00:24,  1.87it/s]Epoch 23:  85%|████████▌ | 255/300 [02:13<00:24,  1.83it/s]Epoch 23:  85%|████████▌ | 256/300 [02:13<00:23,  1.90it/s]Epoch 23:  86%|████████▌ | 257/300 [02:14<00:23,  1.86it/s]Epoch 23:  86%|████████▌ | 258/300 [02:15<00:21,  1.92it/s]Epoch 23:  86%|████████▋ | 259/300 [02:15<00:23,  1.78it/s]06/19/2022 20:49:57 - INFO - __main__ - global step: 3580; train loss: 7.204499244689941; dev loss: 7.357740879058838
Epoch 23:  87%|████████▋ | 260/300 [02:16<00:21,  1.85it/s]Epoch 23:  87%|████████▋ | 261/300 [02:16<00:20,  1.87it/s]Epoch 23:  87%|████████▋ | 262/300 [02:17<00:19,  1.92it/s]Epoch 23:  88%|████████▊ | 263/300 [02:17<00:20,  1.81it/s]Epoch 23:  88%|████████▊ | 264/300 [02:18<00:19,  1.88it/s]Epoch 23:  88%|████████▊ | 265/300 [02:18<00:18,  1.85it/s]Epoch 23:  89%|████████▊ | 266/300 [02:19<00:17,  1.91it/s]Epoch 23:  89%|████████▉ | 267/300 [02:19<00:18,  1.81it/s]Epoch 23:  89%|████████▉ | 268/300 [02:20<00:17,  1.80it/s]Epoch 23:  90%|████████▉ | 269/300 [02:21<00:16,  1.88it/s]Epoch 23:  90%|█████████ | 270/300 [02:21<00:16,  1.85it/s]Epoch 23:  90%|█████████ | 271/300 [02:22<00:15,  1.92it/s]Epoch 23:  91%|█████████ | 272/300 [02:22<00:15,  1.78it/s]Epoch 23:  91%|█████████ | 273/300 [02:23<00:14,  1.86it/s]Epoch 23:  91%|█████████▏| 274/300 [02:23<00:13,  1.88it/s]Epoch 23:  92%|█████████▏| 275/300 [02:24<00:12,  1.93it/s]Epoch 23:  92%|█████████▏| 276/300 [02:24<00:12,  1.88it/s]Epoch 23:  92%|█████████▏| 277/300 [02:25<00:12,  1.90it/s]Epoch 23:  93%|█████████▎| 278/300 [02:25<00:11,  1.94it/s]Epoch 23:  93%|█████████▎| 279/300 [02:26<00:10,  1.96it/s]06/19/2022 20:50:07 - INFO - __main__ - global step: 3590; train loss: 7.493581295013428; dev loss: 7.019073486328125
Epoch 23:  93%|█████████▎| 280/300 [02:26<00:10,  1.90it/s]Epoch 23:  94%|█████████▎| 281/300 [02:27<00:10,  1.86it/s]Epoch 23:  94%|█████████▍| 282/300 [02:27<00:09,  1.91it/s]Epoch 23:  94%|█████████▍| 283/300 [02:28<00:08,  1.94it/s]Epoch 23:  95%|█████████▍| 284/300 [02:28<00:08,  1.88it/s]Epoch 23:  95%|█████████▌| 285/300 [02:29<00:07,  1.88it/s]Epoch 23:  95%|█████████▌| 286/300 [02:29<00:07,  1.94it/s]Epoch 23:  96%|█████████▌| 287/300 [02:30<00:06,  1.89it/s]Epoch 23:  96%|█████████▌| 288/300 [02:31<00:06,  1.83it/s]Epoch 23:  96%|█████████▋| 289/300 [02:31<00:05,  1.86it/s]Epoch 23:  97%|█████████▋| 290/300 [02:32<00:05,  1.82it/s]Epoch 23:  97%|█████████▋| 291/300 [02:32<00:04,  1.89it/s]Epoch 23:  97%|█████████▋| 292/300 [02:33<00:04,  1.84it/s]Epoch 23:  98%|█████████▊| 293/300 [02:33<00:03,  1.91it/s]Epoch 23:  98%|█████████▊| 294/300 [02:34<00:03,  1.95it/s]Epoch 23:  98%|█████████▊| 295/300 [02:34<00:02,  1.99it/s]Epoch 23:  99%|█████████▊| 296/300 [02:35<00:02,  1.90it/s]Epoch 23:  99%|█████████▉| 297/300 [02:35<00:01,  1.94it/s]Epoch 23:  99%|█████████▉| 298/300 [02:36<00:01,  1.87it/s]Epoch 23: 100%|█████████▉| 299/300 [02:36<00:00,  1.93it/s]06/19/2022 20:50:18 - INFO - __main__ - global step: 3600; train loss: 7.534693717956543; dev loss: 6.715306758880615
Epoch 23: 100%|██████████| 300/300 [02:37<00:00,  1.97it/s]Epoch 23: 100%|██████████| 300/300 [02:37<00:00,  1.91it/s]
Epoch 24:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 24:   0%|          | 1/300 [00:00<02:51,  1.74it/s]Epoch 24:   1%|          | 2/300 [00:01<02:36,  1.91it/s]Epoch 24:   1%|          | 3/300 [00:01<02:30,  1.98it/s]Epoch 24:   1%|▏         | 4/300 [00:02<02:32,  1.95it/s]Epoch 24:   2%|▏         | 5/300 [00:02<02:42,  1.82it/s]Epoch 24:   2%|▏         | 6/300 [00:03<02:38,  1.86it/s]Epoch 24:   2%|▏         | 7/300 [00:03<02:32,  1.92it/s]Epoch 24:   3%|▎         | 8/300 [00:04<02:28,  1.97it/s]Epoch 24:   3%|▎         | 9/300 [00:04<02:37,  1.85it/s]Epoch 24:   3%|▎         | 10/300 [00:05<02:31,  1.91it/s]Epoch 24:   4%|▎         | 11/300 [00:05<02:27,  1.96it/s]Epoch 24:   4%|▍         | 12/300 [00:06<02:25,  1.99it/s]Epoch 24:   4%|▍         | 13/300 [00:06<02:30,  1.91it/s]Epoch 24:   5%|▍         | 14/300 [00:07<02:25,  1.96it/s]Epoch 24:   5%|▌         | 15/300 [00:07<02:22,  2.00it/s]Epoch 24:   5%|▌         | 16/300 [00:08<02:20,  2.03it/s]Epoch 24:   6%|▌         | 17/300 [00:08<02:33,  1.85it/s]Epoch 24:   6%|▌         | 18/300 [00:09<02:27,  1.91it/s]Epoch 24:   6%|▋         | 19/300 [00:09<02:30,  1.87it/s]06/19/2022 20:50:28 - INFO - __main__ - global step: 3610; train loss: 6.855146884918213; dev loss: 6.8642072677612305
Epoch 24:   7%|▋         | 20/300 [00:10<02:31,  1.84it/s]Epoch 24:   7%|▋         | 21/300 [00:11<02:34,  1.80it/s]Epoch 24:   7%|▋         | 22/300 [00:11<02:27,  1.88it/s]Epoch 24:   8%|▊         | 23/300 [00:12<02:26,  1.89it/s]Epoch 24:   8%|▊         | 24/300 [00:12<02:21,  1.95it/s]Epoch 24:   8%|▊         | 25/300 [00:13<02:18,  1.98it/s]Epoch 24:   9%|▊         | 26/300 [00:13<02:30,  1.82it/s]Epoch 24:   9%|▉         | 27/300 [00:14<02:24,  1.89it/s]Epoch 24:   9%|▉         | 28/300 [00:14<02:19,  1.95it/s]Epoch 24:  10%|▉         | 29/300 [00:15<02:15,  1.99it/s]Epoch 24:  10%|█         | 30/300 [00:15<02:21,  1.91it/s]Epoch 24:  10%|█         | 31/300 [00:16<02:16,  1.97it/s]Epoch 24:  11%|█         | 32/300 [00:16<02:14,  2.00it/s]Epoch 24:  11%|█         | 33/300 [00:17<02:11,  2.03it/s]Epoch 24:  11%|█▏        | 34/300 [00:17<02:17,  1.94it/s]Epoch 24:  12%|█▏        | 35/300 [00:18<02:13,  1.99it/s]Epoch 24:  12%|█▏        | 36/300 [00:18<02:10,  2.02it/s]Epoch 24:  12%|█▏        | 37/300 [00:19<02:15,  1.95it/s]Epoch 24:  13%|█▎        | 38/300 [00:19<02:23,  1.83it/s]Epoch 24:  13%|█▎        | 39/300 [00:20<02:23,  1.82it/s]06/19/2022 20:50:39 - INFO - __main__ - global step: 3620; train loss: 7.3641252517700195; dev loss: 7.1872124671936035
Epoch 24:  13%|█▎        | 40/300 [00:20<02:20,  1.85it/s]Epoch 24:  14%|█▎        | 41/300 [00:21<02:14,  1.92it/s]Epoch 24:  14%|█▍        | 42/300 [00:22<02:25,  1.78it/s]Epoch 24:  14%|█▍        | 43/300 [00:22<02:17,  1.86it/s]Epoch 24:  15%|█▍        | 44/300 [00:23<02:13,  1.92it/s]Epoch 24:  15%|█▌        | 45/300 [00:23<02:09,  1.97it/s]Epoch 24:  15%|█▌        | 46/300 [00:24<02:19,  1.82it/s]Epoch 24:  16%|█▌        | 47/300 [00:24<02:13,  1.89it/s]Epoch 24:  16%|█▌        | 48/300 [00:25<02:09,  1.95it/s]Epoch 24:  16%|█▋        | 49/300 [00:25<02:05,  1.99it/s]Epoch 24:  17%|█▋        | 50/300 [00:26<02:11,  1.90it/s]Epoch 24:  17%|█▋        | 51/300 [00:26<02:06,  1.96it/s]Epoch 24:  17%|█▋        | 52/300 [00:27<02:07,  1.95it/s]Epoch 24:  18%|█▊        | 53/300 [00:27<02:04,  1.99it/s]Epoch 24:  18%|█▊        | 54/300 [00:28<02:08,  1.91it/s]Epoch 24:  18%|█▊        | 55/300 [00:28<02:14,  1.82it/s]Epoch 24:  19%|█▊        | 56/300 [00:29<02:08,  1.89it/s]Epoch 24:  19%|█▉        | 57/300 [00:29<02:10,  1.86it/s]Epoch 24:  19%|█▉        | 58/300 [00:30<02:05,  1.93it/s]Epoch 24:  20%|█▉        | 59/300 [00:30<02:08,  1.87it/s]06/19/2022 20:50:49 - INFO - __main__ - global step: 3630; train loss: 7.310385704040527; dev loss: 7.309848785400391
Epoch 24:  20%|██        | 60/300 [00:31<02:04,  1.93it/s]Epoch 24:  20%|██        | 61/300 [00:31<02:01,  1.97it/s]Epoch 24:  21%|██        | 62/300 [00:32<01:59,  1.99it/s]Epoch 24:  21%|██        | 63/300 [00:32<02:03,  1.91it/s]Epoch 24:  21%|██▏       | 64/300 [00:33<02:03,  1.92it/s]Epoch 24:  22%|██▏       | 65/300 [00:33<02:05,  1.88it/s]Epoch 24:  22%|██▏       | 66/300 [00:34<02:00,  1.93it/s]Epoch 24:  22%|██▏       | 67/300 [00:35<02:04,  1.87it/s]Epoch 24:  23%|██▎       | 68/300 [00:35<02:00,  1.93it/s]Epoch 24:  23%|██▎       | 69/300 [00:36<01:57,  1.97it/s]Epoch 24:  23%|██▎       | 70/300 [00:36<01:55,  1.99it/s]Epoch 24:  24%|██▎       | 71/300 [00:37<01:59,  1.91it/s]Epoch 24:  24%|██▍       | 72/300 [00:37<01:56,  1.96it/s]Epoch 24:  24%|██▍       | 73/300 [00:38<01:54,  1.99it/s]Epoch 24:  25%|██▍       | 74/300 [00:38<01:54,  1.98it/s]Epoch 24:  25%|██▌       | 75/300 [00:39<02:03,  1.82it/s]Epoch 24:  25%|██▌       | 76/300 [00:39<02:00,  1.85it/s]Epoch 24:  26%|██▌       | 77/300 [00:40<01:58,  1.88it/s]Epoch 24:  26%|██▌       | 78/300 [00:40<01:54,  1.93it/s]Epoch 24:  26%|██▋       | 79/300 [00:41<01:52,  1.97it/s]06/19/2022 20:51:00 - INFO - __main__ - global step: 3640; train loss: 7.332026481628418; dev loss: 7.223371982574463
Epoch 24:  27%|██▋       | 80/300 [00:41<01:55,  1.90it/s]Epoch 24:  27%|██▋       | 81/300 [00:42<01:51,  1.96it/s]Epoch 24:  27%|██▋       | 82/300 [00:42<01:49,  2.00it/s]Epoch 24:  28%|██▊       | 83/300 [00:43<01:52,  1.93it/s]Epoch 24:  28%|██▊       | 84/300 [00:43<02:00,  1.80it/s]Epoch 24:  28%|██▊       | 85/300 [00:44<01:53,  1.89it/s]Epoch 24:  29%|██▊       | 86/300 [00:44<01:49,  1.95it/s]Epoch 24:  29%|██▉       | 87/300 [00:45<01:46,  1.99it/s]Epoch 24:  29%|██▉       | 88/300 [00:45<01:55,  1.83it/s]Epoch 24:  30%|██▉       | 89/300 [00:46<01:50,  1.91it/s]Epoch 24:  30%|███       | 90/300 [00:46<01:47,  1.96it/s]Epoch 24:  30%|███       | 91/300 [00:47<01:44,  2.00it/s]Epoch 24:  31%|███       | 92/300 [00:48<01:51,  1.87it/s]Epoch 24:  31%|███       | 93/300 [00:48<01:49,  1.89it/s]Epoch 24:  31%|███▏      | 94/300 [00:49<01:46,  1.94it/s]Epoch 24:  32%|███▏      | 95/300 [00:49<01:43,  1.98it/s]Epoch 24:  32%|███▏      | 96/300 [00:50<01:51,  1.82it/s]Epoch 24:  32%|███▏      | 97/300 [00:50<01:46,  1.90it/s]Epoch 24:  33%|███▎      | 98/300 [00:51<01:43,  1.96it/s]Epoch 24:  33%|███▎      | 99/300 [00:51<01:40,  2.00it/s]06/19/2022 20:51:10 - INFO - __main__ - global step: 3650; train loss: 7.418589115142822; dev loss: 7.426255702972412
Epoch 24:  33%|███▎      | 100/300 [00:52<01:48,  1.84it/s]Epoch 24:  34%|███▎      | 101/300 [00:52<01:44,  1.90it/s]Epoch 24:  34%|███▍      | 102/300 [00:53<01:45,  1.87it/s]Epoch 24:  34%|███▍      | 103/300 [00:53<01:47,  1.84it/s]Epoch 24:  35%|███▍      | 104/300 [00:54<01:48,  1.81it/s]Epoch 24:  35%|███▌      | 105/300 [00:54<01:43,  1.89it/s]Epoch 24:  35%|███▌      | 106/300 [00:55<01:39,  1.95it/s]Epoch 24:  36%|███▌      | 107/300 [00:55<01:39,  1.95it/s]Epoch 24:  36%|███▌      | 108/300 [00:56<01:36,  1.99it/s]Epoch 24:  36%|███▋      | 109/300 [00:56<01:40,  1.90it/s]Epoch 24:  37%|███▋      | 110/300 [00:57<01:37,  1.96it/s]Epoch 24:  37%|███▋      | 111/300 [00:57<01:34,  2.00it/s]Epoch 24:  37%|███▋      | 112/300 [00:58<01:33,  2.02it/s]Epoch 24:  38%|███▊      | 113/300 [00:59<01:40,  1.86it/s]Epoch 24:  38%|███▊      | 114/300 [00:59<01:38,  1.88it/s]Epoch 24:  38%|███▊      | 115/300 [00:59<01:35,  1.94it/s]Epoch 24:  39%|███▊      | 116/300 [01:00<01:34,  1.94it/s]Epoch 24:  39%|███▉      | 117/300 [01:01<01:38,  1.85it/s]Epoch 24:  39%|███▉      | 118/300 [01:01<01:34,  1.92it/s]Epoch 24:  40%|███▉      | 119/300 [01:02<01:32,  1.96it/s]06/19/2022 20:51:21 - INFO - __main__ - global step: 3660; train loss: 7.686456203460693; dev loss: 7.9427080154418945
Epoch 24:  40%|████      | 120/300 [01:02<01:34,  1.90it/s]Epoch 24:  40%|████      | 121/300 [01:03<01:36,  1.86it/s]Epoch 24:  41%|████      | 122/300 [01:03<01:34,  1.88it/s]Epoch 24:  41%|████      | 123/300 [01:04<01:33,  1.89it/s]Epoch 24:  41%|████▏     | 124/300 [01:04<01:30,  1.95it/s]Epoch 24:  42%|████▏     | 125/300 [01:05<01:37,  1.80it/s]Epoch 24:  42%|████▏     | 126/300 [01:05<01:33,  1.86it/s]Epoch 24:  42%|████▏     | 127/300 [01:06<01:29,  1.93it/s]Epoch 24:  43%|████▎     | 128/300 [01:06<01:31,  1.88it/s]Epoch 24:  43%|████▎     | 129/300 [01:07<01:32,  1.84it/s]Epoch 24:  43%|████▎     | 130/300 [01:07<01:28,  1.91it/s]Epoch 24:  44%|████▎     | 131/300 [01:08<01:25,  1.97it/s]Epoch 24:  44%|████▍     | 132/300 [01:08<01:24,  1.99it/s]Epoch 24:  44%|████▍     | 133/300 [01:09<01:22,  2.02it/s]Epoch 24:  45%|████▍     | 134/300 [01:09<01:25,  1.94it/s]Epoch 24:  45%|████▌     | 135/300 [01:10<01:23,  1.99it/s]Epoch 24:  45%|████▌     | 136/300 [01:10<01:21,  2.00it/s]Epoch 24:  46%|████▌     | 137/300 [01:11<01:20,  2.02it/s]Epoch 24:  46%|████▌     | 138/300 [01:11<01:25,  1.89it/s]Epoch 24:  46%|████▋     | 139/300 [01:12<01:22,  1.95it/s]06/19/2022 20:51:31 - INFO - __main__ - global step: 3670; train loss: 6.69272518157959; dev loss: 6.628481864929199
Epoch 24:  47%|████▋     | 140/300 [01:12<01:20,  1.99it/s]Epoch 24:  47%|████▋     | 141/300 [01:13<01:18,  2.02it/s]Epoch 24:  47%|████▋     | 142/300 [01:13<01:21,  1.94it/s]Epoch 24:  48%|████▊     | 143/300 [01:14<01:23,  1.89it/s]Epoch 24:  48%|████▊     | 144/300 [01:15<01:19,  1.95it/s]Epoch 24:  48%|████▊     | 145/300 [01:15<01:17,  2.00it/s]Epoch 24:  49%|████▊     | 146/300 [01:16<01:23,  1.84it/s]Epoch 24:  49%|████▉     | 147/300 [01:16<01:22,  1.87it/s]Epoch 24:  49%|████▉     | 148/300 [01:17<01:22,  1.83it/s]Epoch 24:  50%|████▉     | 149/300 [01:17<01:19,  1.91it/s]Epoch 24:  50%|█████     | 150/300 [01:18<01:20,  1.86it/s]Epoch 24:  50%|█████     | 151/300 [01:18<01:20,  1.85it/s]Epoch 24:  51%|█████     | 152/300 [01:19<01:16,  1.92it/s]Epoch 24:  51%|█████     | 153/300 [01:19<01:18,  1.88it/s]Epoch 24:  51%|█████▏    | 154/300 [01:20<01:19,  1.85it/s]Epoch 24:  52%|█████▏    | 155/300 [01:20<01:15,  1.92it/s]Epoch 24:  52%|█████▏    | 156/300 [01:21<01:12,  1.97it/s]Epoch 24:  52%|█████▏    | 157/300 [01:21<01:11,  2.01it/s]Epoch 24:  53%|█████▎    | 158/300 [01:22<01:17,  1.84it/s]Epoch 24:  53%|█████▎    | 159/300 [01:22<01:13,  1.91it/s]06/19/2022 20:51:41 - INFO - __main__ - global step: 3680; train loss: 7.637765407562256; dev loss: 7.903079032897949
Epoch 24:  53%|█████▎    | 160/300 [01:23<01:13,  1.91it/s]Epoch 24:  54%|█████▎    | 161/300 [01:24<01:12,  1.92it/s]Epoch 24:  54%|█████▍    | 162/300 [01:24<01:09,  1.97it/s]Epoch 24:  54%|█████▍    | 163/300 [01:25<01:12,  1.90it/s]Epoch 24:  55%|█████▍    | 164/300 [01:25<01:12,  1.86it/s]Epoch 24:  55%|█████▌    | 165/300 [01:26<01:10,  1.93it/s]Epoch 24:  55%|█████▌    | 166/300 [01:26<01:07,  1.98it/s]Epoch 24:  56%|█████▌    | 167/300 [01:27<01:09,  1.91it/s]Epoch 24:  56%|█████▌    | 168/300 [01:27<01:08,  1.92it/s]Epoch 24:  56%|█████▋    | 169/300 [01:28<01:06,  1.96it/s]Epoch 24:  57%|█████▋    | 170/300 [01:28<01:06,  1.95it/s]Epoch 24:  57%|█████▋    | 171/300 [01:29<01:08,  1.89it/s]Epoch 24:  57%|█████▋    | 172/300 [01:29<01:05,  1.95it/s]Epoch 24:  58%|█████▊    | 173/300 [01:30<01:03,  2.00it/s]Epoch 24:  58%|█████▊    | 174/300 [01:30<01:03,  1.98it/s]Epoch 24:  58%|█████▊    | 175/300 [01:31<01:05,  1.90it/s]Epoch 24:  59%|█████▊    | 176/300 [01:31<01:03,  1.96it/s]Epoch 24:  59%|█████▉    | 177/300 [01:32<01:01,  2.00it/s]Epoch 24:  59%|█████▉    | 178/300 [01:32<01:00,  2.03it/s]Epoch 24:  60%|█████▉    | 179/300 [01:33<01:05,  1.85it/s]06/19/2022 20:51:52 - INFO - __main__ - global step: 3690; train loss: 7.2543158531188965; dev loss: 7.2104620933532715
Epoch 24:  60%|██████    | 180/300 [01:33<01:02,  1.92it/s]Epoch 24:  60%|██████    | 181/300 [01:34<01:00,  1.97it/s]Epoch 24:  61%|██████    | 182/300 [01:34<01:00,  1.96it/s]Epoch 24:  61%|██████    | 183/300 [01:35<01:01,  1.89it/s]Epoch 24:  61%|██████▏   | 184/300 [01:35<00:59,  1.94it/s]Epoch 24:  62%|██████▏   | 185/300 [01:36<00:57,  1.99it/s]Epoch 24:  62%|██████▏   | 186/300 [01:36<00:56,  2.01it/s]Epoch 24:  62%|██████▏   | 187/300 [01:37<00:56,  1.99it/s]Epoch 24:  63%|██████▎   | 188/300 [01:37<00:59,  1.90it/s]Epoch 24:  63%|██████▎   | 189/300 [01:38<00:56,  1.96it/s]Epoch 24:  63%|██████▎   | 190/300 [01:38<00:57,  1.90it/s]Epoch 24:  64%|██████▎   | 191/300 [01:39<00:56,  1.92it/s]Epoch 24:  64%|██████▍   | 192/300 [01:40<00:59,  1.82it/s]Epoch 24:  64%|██████▍   | 193/300 [01:40<00:56,  1.90it/s]Epoch 24:  65%|██████▍   | 194/300 [01:41<00:57,  1.86it/s]Epoch 24:  65%|██████▌   | 195/300 [01:41<00:54,  1.92it/s]Epoch 24:  65%|██████▌   | 196/300 [01:42<00:55,  1.88it/s]Epoch 24:  66%|██████▌   | 197/300 [01:42<00:53,  1.94it/s]Epoch 24:  66%|██████▌   | 198/300 [01:43<00:51,  1.98it/s]Epoch 24:  66%|██████▋   | 199/300 [01:43<00:52,  1.92it/s]06/19/2022 20:52:02 - INFO - __main__ - global step: 3700; train loss: 7.717231750488281; dev loss: 7.475630283355713
Epoch 24:  67%|██████▋   | 200/300 [01:44<00:53,  1.86it/s]Epoch 24:  67%|██████▋   | 201/300 [01:44<00:51,  1.93it/s]Epoch 24:  67%|██████▋   | 202/300 [01:45<00:49,  1.98it/s]Epoch 24:  68%|██████▊   | 203/300 [01:45<00:48,  1.99it/s]Epoch 24:  68%|██████▊   | 204/300 [01:46<00:51,  1.87it/s]Epoch 24:  68%|██████▊   | 205/300 [01:46<00:49,  1.93it/s]Epoch 24:  69%|██████▊   | 206/300 [01:47<00:47,  1.96it/s]Epoch 24:  69%|██████▉   | 207/300 [01:47<00:46,  2.00it/s]Epoch 24:  69%|██████▉   | 208/300 [01:48<00:49,  1.84it/s]Epoch 24:  70%|██████▉   | 209/300 [01:48<00:47,  1.91it/s]Epoch 24:  70%|███████   | 210/300 [01:49<00:45,  1.96it/s]Epoch 24:  70%|███████   | 211/300 [01:49<00:44,  2.00it/s]Epoch 24:  71%|███████   | 212/300 [01:50<00:46,  1.91it/s]Epoch 24:  71%|███████   | 213/300 [01:50<00:45,  1.93it/s]Epoch 24:  71%|███████▏  | 214/300 [01:51<00:45,  1.88it/s]Epoch 24:  72%|███████▏  | 215/300 [01:52<00:46,  1.83it/s]Epoch 24:  72%|███████▏  | 216/300 [01:52<00:44,  1.90it/s]Epoch 24:  72%|███████▏  | 217/300 [01:53<00:44,  1.85it/s]Epoch 24:  73%|███████▎  | 218/300 [01:53<00:44,  1.86it/s]Epoch 24:  73%|███████▎  | 219/300 [01:54<00:42,  1.91it/s]06/19/2022 20:52:13 - INFO - __main__ - global step: 3710; train loss: 8.090553283691406; dev loss: 7.84616756439209
Epoch 24:  73%|███████▎  | 220/300 [01:54<00:41,  1.95it/s]Epoch 24:  74%|███████▎  | 221/300 [01:55<00:42,  1.87it/s]Epoch 24:  74%|███████▍  | 222/300 [01:55<00:40,  1.91it/s]Epoch 24:  74%|███████▍  | 223/300 [01:56<00:39,  1.95it/s]Epoch 24:  75%|███████▍  | 224/300 [01:56<00:39,  1.90it/s]Epoch 24:  75%|███████▌  | 225/300 [01:57<00:41,  1.82it/s]Epoch 24:  75%|███████▌  | 226/300 [01:57<00:39,  1.89it/s]Epoch 24:  76%|███████▌  | 227/300 [01:58<00:37,  1.95it/s]Epoch 24:  76%|███████▌  | 228/300 [01:58<00:37,  1.90it/s]Epoch 24:  76%|███████▋  | 229/300 [01:59<00:38,  1.86it/s]Epoch 24:  77%|███████▋  | 230/300 [01:59<00:36,  1.93it/s]Epoch 24:  77%|███████▋  | 231/300 [02:00<00:36,  1.89it/s]Epoch 24:  77%|███████▋  | 232/300 [02:00<00:35,  1.91it/s]Epoch 24:  78%|███████▊  | 233/300 [02:01<00:36,  1.86it/s]Epoch 24:  78%|███████▊  | 234/300 [02:02<00:36,  1.83it/s]Epoch 24:  78%|███████▊  | 235/300 [02:02<00:35,  1.85it/s]Epoch 24:  79%|███████▊  | 236/300 [02:03<00:33,  1.89it/s]Epoch 24:  79%|███████▉  | 237/300 [02:03<00:34,  1.84it/s]Epoch 24:  79%|███████▉  | 238/300 [02:04<00:32,  1.91it/s]Epoch 24:  80%|███████▉  | 239/300 [02:04<00:31,  1.96it/s]06/19/2022 20:52:23 - INFO - __main__ - global step: 3720; train loss: 7.591797828674316; dev loss: 7.414566993713379
Epoch 24:  80%|████████  | 240/300 [02:05<00:31,  1.93it/s]Epoch 24:  80%|████████  | 241/300 [02:05<00:29,  1.98it/s]Epoch 24:  81%|████████  | 242/300 [02:06<00:30,  1.91it/s]Epoch 24:  81%|████████  | 243/300 [02:06<00:29,  1.96it/s]Epoch 24:  81%|████████▏ | 244/300 [02:07<00:28,  2.00it/s]Epoch 24:  82%|████████▏ | 245/300 [02:07<00:27,  1.98it/s]Epoch 24:  82%|████████▏ | 246/300 [02:08<00:28,  1.91it/s]Epoch 24:  82%|████████▏ | 247/300 [02:08<00:27,  1.94it/s]Epoch 24:  83%|████████▎ | 248/300 [02:09<00:26,  1.96it/s]Epoch 24:  83%|████████▎ | 249/300 [02:09<00:25,  2.00it/s]Epoch 24:  83%|████████▎ | 250/300 [02:10<00:26,  1.87it/s]Epoch 24:  84%|████████▎ | 251/300 [02:10<00:26,  1.86it/s]Epoch 24:  84%|████████▍ | 252/300 [02:11<00:24,  1.92it/s]Epoch 24:  84%|████████▍ | 253/300 [02:11<00:23,  1.96it/s]Epoch 24:  85%|████████▍ | 254/300 [02:12<00:24,  1.90it/s]Epoch 24:  85%|████████▌ | 255/300 [02:12<00:22,  1.96it/s]Epoch 24:  85%|████████▌ | 256/300 [02:13<00:21,  2.01it/s]Epoch 24:  86%|████████▌ | 257/300 [02:13<00:21,  2.03it/s]Epoch 24:  86%|████████▌ | 258/300 [02:14<00:21,  1.95it/s]Epoch 24:  86%|████████▋ | 259/300 [02:14<00:20,  1.99it/s]06/19/2022 20:52:33 - INFO - __main__ - global step: 3730; train loss: 7.502643585205078; dev loss: 7.664738655090332
Epoch 24:  87%|████████▋ | 260/300 [02:15<00:20,  1.96it/s]Epoch 24:  87%|████████▋ | 261/300 [02:15<00:20,  1.91it/s]Epoch 24:  87%|████████▋ | 262/300 [02:16<00:20,  1.86it/s]Epoch 24:  88%|████████▊ | 263/300 [02:17<00:19,  1.93it/s]Epoch 24:  88%|████████▊ | 264/300 [02:17<00:18,  1.97it/s]Epoch 24:  88%|████████▊ | 265/300 [02:18<00:18,  1.90it/s]Epoch 24:  89%|████████▊ | 266/300 [02:18<00:18,  1.86it/s]Epoch 24:  89%|████████▉ | 267/300 [02:19<00:17,  1.89it/s]Epoch 24:  89%|████████▉ | 268/300 [02:19<00:16,  1.94it/s]Epoch 24:  90%|████████▉ | 269/300 [02:20<00:15,  1.98it/s]Epoch 24:  90%|█████████ | 270/300 [02:20<00:15,  1.92it/s]Epoch 24:  90%|█████████ | 271/300 [02:21<00:15,  1.87it/s]Epoch 24:  91%|█████████ | 272/300 [02:21<00:14,  1.93it/s]Epoch 24:  91%|█████████ | 273/300 [02:22<00:13,  1.97it/s]Epoch 24:  91%|█████████▏| 274/300 [02:22<00:12,  2.01it/s]Epoch 24:  92%|█████████▏| 275/300 [02:23<00:13,  1.92it/s]Epoch 24:  92%|█████████▏| 276/300 [02:23<00:12,  1.97it/s]Epoch 24:  92%|█████████▏| 277/300 [02:24<00:11,  2.00it/s]Epoch 24:  93%|█████████▎| 278/300 [02:24<00:10,  2.03it/s]Epoch 24:  93%|█████████▎| 279/300 [02:25<00:10,  1.95it/s]06/19/2022 20:52:44 - INFO - __main__ - global step: 3740; train loss: 7.811467170715332; dev loss: 7.341794013977051
Epoch 24:  93%|█████████▎| 280/300 [02:25<00:10,  1.99it/s]Epoch 24:  94%|█████████▎| 281/300 [02:26<00:09,  2.02it/s]Epoch 24:  94%|█████████▍| 282/300 [02:26<00:09,  1.93it/s]Epoch 24:  94%|█████████▍| 283/300 [02:27<00:09,  1.88it/s]Epoch 24:  95%|█████████▍| 284/300 [02:27<00:08,  1.84it/s]Epoch 24:  95%|█████████▌| 285/300 [02:28<00:08,  1.83it/s]Epoch 24:  95%|█████████▌| 286/300 [02:28<00:07,  1.86it/s]Epoch 24:  96%|█████████▌| 287/300 [02:29<00:07,  1.75it/s]Epoch 24:  96%|█████████▌| 288/300 [02:30<00:06,  1.84it/s]Epoch 24:  96%|█████████▋| 289/300 [02:30<00:05,  1.91it/s]Epoch 24:  97%|█████████▋| 290/300 [02:31<00:05,  1.87it/s]Epoch 24:  97%|█████████▋| 291/300 [02:31<00:05,  1.75it/s]Epoch 24:  97%|█████████▋| 292/300 [02:32<00:04,  1.76it/s]Epoch 24:  98%|█████████▊| 293/300 [02:32<00:03,  1.84it/s]Epoch 24:  98%|█████████▊| 294/300 [02:33<00:03,  1.90it/s]Epoch 24:  98%|█████████▊| 295/300 [02:33<00:02,  1.95it/s]Epoch 24:  99%|█████████▊| 296/300 [02:34<00:02,  1.80it/s]Epoch 24:  99%|█████████▉| 297/300 [02:34<00:01,  1.87it/s]Epoch 24:  99%|█████████▉| 298/300 [02:35<00:01,  1.93it/s]Epoch 24: 100%|█████████▉| 299/300 [02:35<00:00,  1.98it/s]06/19/2022 20:52:54 - INFO - __main__ - global step: 3750; train loss: 7.742877006530762; dev loss: 7.692021369934082
Epoch 24: 100%|██████████| 300/300 [02:36<00:00,  1.91it/s]Epoch 24: 100%|██████████| 300/300 [02:36<00:00,  1.92it/s]
Epoch 25:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 25:   0%|          | 1/300 [00:00<02:37,  1.90it/s]Epoch 25:   1%|          | 2/300 [00:01<02:28,  2.01it/s]Epoch 25:   1%|          | 3/300 [00:01<02:30,  1.97it/s]Epoch 25:   1%|▏         | 4/300 [00:02<02:37,  1.89it/s]Epoch 25:   2%|▏         | 5/300 [00:02<02:39,  1.85it/s]Epoch 25:   2%|▏         | 6/300 [00:03<02:34,  1.91it/s]Epoch 25:   2%|▏         | 7/300 [00:03<02:28,  1.97it/s]Epoch 25:   3%|▎         | 8/300 [00:04<02:35,  1.88it/s]Epoch 25:   3%|▎         | 9/300 [00:04<02:29,  1.94it/s]Epoch 25:   3%|▎         | 10/300 [00:05<02:29,  1.94it/s]Epoch 25:   4%|▎         | 11/300 [00:05<02:25,  1.98it/s]Epoch 25:   4%|▍         | 12/300 [00:06<02:30,  1.91it/s]Epoch 25:   4%|▍         | 13/300 [00:06<02:26,  1.96it/s]Epoch 25:   5%|▍         | 14/300 [00:07<02:26,  1.95it/s]Epoch 25:   5%|▌         | 15/300 [00:07<02:22,  2.00it/s]Epoch 25:   5%|▌         | 16/300 [00:08<02:29,  1.90it/s]Epoch 25:   6%|▌         | 17/300 [00:08<02:24,  1.95it/s]Epoch 25:   6%|▌         | 18/300 [00:09<02:22,  1.98it/s]Epoch 25:   6%|▋         | 19/300 [00:09<02:19,  2.01it/s]06/19/2022 20:53:05 - INFO - __main__ - global step: 3760; train loss: 7.490621089935303; dev loss: 7.536118984222412
Epoch 25:   7%|▋         | 20/300 [00:10<02:26,  1.92it/s]Epoch 25:   7%|▋         | 21/300 [00:10<02:22,  1.96it/s]Epoch 25:   7%|▋         | 22/300 [00:11<02:18,  2.01it/s]Epoch 25:   8%|▊         | 23/300 [00:11<02:16,  2.03it/s]Epoch 25:   8%|▊         | 24/300 [00:12<02:16,  2.03it/s]Epoch 25:   8%|▊         | 25/300 [00:12<02:21,  1.94it/s]Epoch 25:   9%|▊         | 26/300 [00:13<02:17,  1.99it/s]Epoch 25:   9%|▉         | 27/300 [00:13<02:21,  1.92it/s]Epoch 25:   9%|▉         | 28/300 [00:14<02:18,  1.96it/s]Epoch 25:  10%|▉         | 29/300 [00:14<02:22,  1.90it/s]Epoch 25:  10%|█         | 30/300 [00:15<02:18,  1.95it/s]Epoch 25:  10%|█         | 31/300 [00:15<02:18,  1.94it/s]Epoch 25:  11%|█         | 32/300 [00:16<02:16,  1.96it/s]Epoch 25:  11%|█         | 33/300 [00:16<02:20,  1.90it/s]Epoch 25:  11%|█▏        | 34/300 [00:17<02:16,  1.95it/s]Epoch 25:  12%|█▏        | 35/300 [00:17<02:12,  1.99it/s]Epoch 25:  12%|█▏        | 36/300 [00:18<02:13,  1.98it/s]Epoch 25:  12%|█▏        | 37/300 [00:19<02:21,  1.86it/s]Epoch 25:  13%|█▎        | 38/300 [00:19<02:22,  1.85it/s]Epoch 25:  13%|█▎        | 39/300 [00:20<02:16,  1.91it/s]06/19/2022 20:53:15 - INFO - __main__ - global step: 3770; train loss: 7.682727813720703; dev loss: 7.986685276031494
Epoch 25:  13%|█▎        | 40/300 [00:20<02:17,  1.90it/s]Epoch 25:  14%|█▎        | 41/300 [00:21<02:24,  1.79it/s]Epoch 25:  14%|█▍        | 42/300 [00:21<02:18,  1.86it/s]Epoch 25:  14%|█▍        | 43/300 [00:22<02:14,  1.91it/s]Epoch 25:  15%|█▍        | 44/300 [00:22<02:17,  1.87it/s]Epoch 25:  15%|█▌        | 45/300 [00:23<02:22,  1.78it/s]Epoch 25:  15%|█▌        | 46/300 [00:23<02:15,  1.87it/s]Epoch 25:  16%|█▌        | 47/300 [00:24<02:15,  1.86it/s]Epoch 25:  16%|█▌        | 48/300 [00:24<02:16,  1.84it/s]Epoch 25:  16%|█▋        | 49/300 [00:25<02:13,  1.88it/s]Epoch 25:  17%|█▋        | 50/300 [00:26<02:16,  1.83it/s]Epoch 25:  17%|█▋        | 51/300 [00:26<02:12,  1.88it/s]Epoch 25:  17%|█▋        | 52/300 [00:27<02:08,  1.93it/s]Epoch 25:  18%|█▊        | 53/300 [00:27<02:04,  1.98it/s]Epoch 25:  18%|█▊        | 54/300 [00:28<02:09,  1.91it/s]Epoch 25:  18%|█▊        | 55/300 [00:28<02:05,  1.95it/s]Epoch 25:  19%|█▊        | 56/300 [00:29<02:06,  1.92it/s]Epoch 25:  19%|█▉        | 57/300 [00:29<02:03,  1.97it/s]Epoch 25:  19%|█▉        | 58/300 [00:30<02:06,  1.91it/s]Epoch 25:  20%|█▉        | 59/300 [00:30<02:08,  1.87it/s]06/19/2022 20:53:26 - INFO - __main__ - global step: 3780; train loss: 7.501871585845947; dev loss: 7.528905391693115
Epoch 25:  20%|██        | 60/300 [00:31<02:11,  1.83it/s]Epoch 25:  20%|██        | 61/300 [00:31<02:08,  1.86it/s]Epoch 25:  21%|██        | 62/300 [00:32<02:10,  1.83it/s]Epoch 25:  21%|██        | 63/300 [00:32<02:04,  1.90it/s]Epoch 25:  21%|██▏       | 64/300 [00:33<02:00,  1.95it/s]Epoch 25:  22%|██▏       | 65/300 [00:33<02:03,  1.90it/s]Epoch 25:  22%|██▏       | 66/300 [00:34<02:12,  1.77it/s]Epoch 25:  22%|██▏       | 67/300 [00:35<02:05,  1.86it/s]Epoch 25:  23%|██▎       | 68/300 [00:35<02:00,  1.92it/s]Epoch 25:  23%|██▎       | 69/300 [00:35<01:57,  1.97it/s]Epoch 25:  23%|██▎       | 70/300 [00:36<02:07,  1.80it/s]Epoch 25:  24%|██▎       | 71/300 [00:37<02:01,  1.88it/s]Epoch 25:  24%|██▍       | 72/300 [00:37<01:57,  1.94it/s]Epoch 25:  24%|██▍       | 73/300 [00:38<01:54,  1.99it/s]Epoch 25:  25%|██▍       | 74/300 [00:38<02:05,  1.80it/s]Epoch 25:  25%|██▌       | 75/300 [00:39<01:59,  1.88it/s]Epoch 25:  25%|██▌       | 76/300 [00:39<01:55,  1.94it/s]Epoch 25:  26%|██▌       | 77/300 [00:40<01:54,  1.95it/s]Epoch 25:  26%|██▌       | 78/300 [00:40<01:51,  1.99it/s]Epoch 25:  26%|██▋       | 79/300 [00:41<01:55,  1.91it/s]06/19/2022 20:53:36 - INFO - __main__ - global step: 3790; train loss: 7.599163055419922; dev loss: 7.651953220367432
Epoch 25:  27%|██▋       | 80/300 [00:41<01:52,  1.95it/s]Epoch 25:  27%|██▋       | 81/300 [00:42<01:49,  1.99it/s]Epoch 25:  27%|██▋       | 82/300 [00:42<01:50,  1.97it/s]Epoch 25:  28%|██▊       | 83/300 [00:43<01:53,  1.91it/s]Epoch 25:  28%|██▊       | 84/300 [00:43<01:50,  1.96it/s]Epoch 25:  28%|██▊       | 85/300 [00:44<01:50,  1.95it/s]Epoch 25:  29%|██▊       | 86/300 [00:44<01:49,  1.95it/s]Epoch 25:  29%|██▉       | 87/300 [00:45<01:57,  1.81it/s]Epoch 25:  29%|██▉       | 88/300 [00:45<01:52,  1.89it/s]Epoch 25:  30%|██▉       | 89/300 [00:46<01:49,  1.93it/s]Epoch 25:  30%|███       | 90/300 [00:46<01:48,  1.93it/s]Epoch 25:  30%|███       | 91/300 [00:47<01:56,  1.79it/s]Epoch 25:  31%|███       | 92/300 [00:48<01:50,  1.88it/s]Epoch 25:  31%|███       | 93/300 [00:48<01:46,  1.94it/s]Epoch 25:  31%|███▏      | 94/300 [00:49<01:45,  1.94it/s]Epoch 25:  32%|███▏      | 95/300 [00:49<01:51,  1.84it/s]Epoch 25:  32%|███▏      | 96/300 [00:50<01:46,  1.91it/s]Epoch 25:  32%|███▏      | 97/300 [00:50<01:42,  1.97it/s]Epoch 25:  33%|███▎      | 98/300 [00:51<01:40,  2.01it/s]Epoch 25:  33%|███▎      | 99/300 [00:51<01:49,  1.83it/s]06/19/2022 20:53:47 - INFO - __main__ - global step: 3800; train loss: 7.821616172790527; dev loss: 7.69195556640625
Epoch 25:  33%|███▎      | 100/300 [00:52<01:46,  1.89it/s]Epoch 25:  34%|███▎      | 101/300 [00:52<01:42,  1.94it/s]Epoch 25:  34%|███▍      | 102/300 [00:53<01:40,  1.98it/s]Epoch 25:  34%|███▍      | 103/300 [00:53<01:42,  1.92it/s]Epoch 25:  35%|███▍      | 104/300 [00:54<01:46,  1.85it/s]Epoch 25:  35%|███▌      | 105/300 [00:54<01:42,  1.91it/s]Epoch 25:  35%|███▌      | 106/300 [00:55<01:38,  1.96it/s]Epoch 25:  36%|███▌      | 107/300 [00:55<01:36,  2.00it/s]Epoch 25:  36%|███▌      | 108/300 [00:56<01:41,  1.89it/s]Epoch 25:  36%|███▋      | 109/300 [00:56<01:38,  1.93it/s]Epoch 25:  37%|███▋      | 110/300 [00:57<01:36,  1.97it/s]Epoch 25:  37%|███▋      | 111/300 [00:57<01:35,  1.99it/s]Epoch 25:  37%|███▋      | 112/300 [00:58<01:40,  1.86it/s]Epoch 25:  38%|███▊      | 113/300 [00:58<01:37,  1.93it/s]Epoch 25:  38%|███▊      | 114/300 [00:59<01:35,  1.94it/s]Epoch 25:  38%|███▊      | 115/300 [00:59<01:34,  1.95it/s]Epoch 25:  39%|███▊      | 116/300 [01:00<01:38,  1.87it/s]Epoch 25:  39%|███▉      | 117/300 [01:01<01:34,  1.93it/s]Epoch 25:  39%|███▉      | 118/300 [01:01<01:32,  1.97it/s]Epoch 25:  40%|███▉      | 119/300 [01:02<01:32,  1.96it/s]06/19/2022 20:53:57 - INFO - __main__ - global step: 3810; train loss: 7.5381011962890625; dev loss: 7.570638179779053
Epoch 25:  40%|████      | 120/300 [01:02<01:38,  1.83it/s]Epoch 25:  40%|████      | 121/300 [01:03<01:40,  1.78it/s]Epoch 25:  41%|████      | 122/300 [01:03<01:39,  1.78it/s]Epoch 25:  41%|████      | 123/300 [01:04<01:39,  1.78it/s]Epoch 25:  41%|████▏     | 124/300 [01:05<01:42,  1.72it/s]Epoch 25:  42%|████▏     | 125/300 [01:05<01:36,  1.81it/s]Epoch 25:  42%|████▏     | 126/300 [01:05<01:32,  1.88it/s]Epoch 25:  42%|████▏     | 127/300 [01:06<01:32,  1.88it/s]Epoch 25:  43%|████▎     | 128/300 [01:07<01:34,  1.83it/s]Epoch 25:  43%|████▎     | 129/300 [01:07<01:30,  1.90it/s]Epoch 25:  43%|████▎     | 130/300 [01:08<01:27,  1.95it/s]Epoch 25:  44%|████▎     | 131/300 [01:08<01:25,  1.98it/s]Epoch 25:  44%|████▍     | 132/300 [01:09<01:24,  2.00it/s]Epoch 25:  44%|████▍     | 133/300 [01:09<01:29,  1.86it/s]Epoch 25:  45%|████▍     | 134/300 [01:10<01:27,  1.90it/s]Epoch 25:  45%|████▌     | 135/300 [01:10<01:24,  1.95it/s]Epoch 25:  45%|████▌     | 136/300 [01:11<01:23,  1.98it/s]Epoch 25:  46%|████▌     | 137/300 [01:11<01:25,  1.91it/s]Epoch 25:  46%|████▌     | 138/300 [01:12<01:24,  1.92it/s]Epoch 25:  46%|████▋     | 139/300 [01:12<01:21,  1.98it/s]06/19/2022 20:54:08 - INFO - __main__ - global step: 3820; train loss: 7.5185041427612305; dev loss: 7.428228855133057
Epoch 25:  47%|████▋     | 140/300 [01:13<01:19,  2.00it/s]Epoch 25:  47%|████▋     | 141/300 [01:13<01:22,  1.92it/s]Epoch 25:  47%|████▋     | 142/300 [01:14<01:20,  1.97it/s]Epoch 25:  48%|████▊     | 143/300 [01:14<01:18,  2.01it/s]Epoch 25:  48%|████▊     | 144/300 [01:15<01:16,  2.03it/s]Epoch 25:  48%|████▊     | 145/300 [01:15<01:23,  1.85it/s]Epoch 25:  49%|████▊     | 146/300 [01:16<01:22,  1.87it/s]Epoch 25:  49%|████▉     | 147/300 [01:16<01:18,  1.94it/s]Epoch 25:  49%|████▉     | 148/300 [01:17<01:17,  1.96it/s]Epoch 25:  50%|████▉     | 149/300 [01:17<01:21,  1.84it/s]Epoch 25:  50%|█████     | 150/300 [01:18<01:18,  1.91it/s]Epoch 25:  50%|█████     | 151/300 [01:18<01:15,  1.97it/s]Epoch 25:  51%|█████     | 152/300 [01:19<01:13,  2.01it/s]Epoch 25:  51%|█████     | 153/300 [01:19<01:17,  1.90it/s]Epoch 25:  51%|█████▏    | 154/300 [01:20<01:16,  1.91it/s]Epoch 25:  52%|█████▏    | 155/300 [01:20<01:13,  1.97it/s]Epoch 25:  52%|█████▏    | 156/300 [01:21<01:12,  1.99it/s]Epoch 25:  52%|█████▏    | 157/300 [01:21<01:10,  2.02it/s]Epoch 25:  53%|█████▎    | 158/300 [01:22<01:13,  1.92it/s]Epoch 25:  53%|█████▎    | 159/300 [01:22<01:11,  1.98it/s]06/19/2022 20:54:18 - INFO - __main__ - global step: 3830; train loss: 7.41541051864624; dev loss: 7.507813453674316
Epoch 25:  53%|█████▎    | 160/300 [01:23<01:09,  2.01it/s]Epoch 25:  54%|█████▎    | 161/300 [01:23<01:08,  2.04it/s]Epoch 25:  54%|█████▍    | 162/300 [01:24<01:10,  1.95it/s]Epoch 25:  54%|█████▍    | 163/300 [01:24<01:08,  2.00it/s]Epoch 25:  55%|█████▍    | 164/300 [01:25<01:11,  1.91it/s]Epoch 25:  55%|█████▌    | 165/300 [01:26<01:11,  1.88it/s]Epoch 25:  55%|█████▌    | 166/300 [01:26<01:12,  1.85it/s]Epoch 25:  56%|█████▌    | 167/300 [01:27<01:09,  1.91it/s]Epoch 25:  56%|█████▌    | 168/300 [01:27<01:07,  1.97it/s]Epoch 25:  56%|█████▋    | 169/300 [01:28<01:05,  2.00it/s]Epoch 25:  57%|█████▋    | 170/300 [01:28<01:08,  1.91it/s]Epoch 25:  57%|█████▋    | 171/300 [01:29<01:08,  1.89it/s]Epoch 25:  57%|█████▋    | 172/300 [01:29<01:06,  1.93it/s]Epoch 25:  58%|█████▊    | 173/300 [01:30<01:06,  1.90it/s]Epoch 25:  58%|█████▊    | 174/300 [01:30<01:08,  1.84it/s]Epoch 25:  58%|█████▊    | 175/300 [01:31<01:05,  1.90it/s]Epoch 25:  59%|█████▊    | 176/300 [01:31<01:05,  1.90it/s]Epoch 25:  59%|█████▉    | 177/300 [01:32<01:04,  1.90it/s]Epoch 25:  59%|█████▉    | 178/300 [01:32<01:07,  1.82it/s]Epoch 25:  60%|█████▉    | 179/300 [01:33<01:05,  1.84it/s]06/19/2022 20:54:28 - INFO - __main__ - global step: 3840; train loss: 7.334050178527832; dev loss: 7.537118434906006
Epoch 25:  60%|██████    | 180/300 [01:34<01:05,  1.84it/s]Epoch 25:  60%|██████    | 181/300 [01:34<01:04,  1.84it/s]Epoch 25:  61%|██████    | 182/300 [01:35<01:05,  1.79it/s]Epoch 25:  61%|██████    | 183/300 [01:35<01:02,  1.86it/s]Epoch 25:  61%|██████▏   | 184/300 [01:36<01:01,  1.89it/s]Epoch 25:  62%|██████▏   | 185/300 [01:36<01:03,  1.81it/s]Epoch 25:  62%|██████▏   | 186/300 [01:37<01:01,  1.86it/s]Epoch 25:  62%|██████▏   | 187/300 [01:37<01:02,  1.82it/s]Epoch 25:  63%|██████▎   | 188/300 [01:38<00:59,  1.88it/s]Epoch 25:  63%|██████▎   | 189/300 [01:38<00:59,  1.88it/s]Epoch 25:  63%|██████▎   | 190/300 [01:39<00:57,  1.93it/s]Epoch 25:  64%|██████▎   | 191/300 [01:39<00:58,  1.87it/s]Epoch 25:  64%|██████▍   | 192/300 [01:40<00:56,  1.91it/s]Epoch 25:  64%|██████▍   | 193/300 [01:40<00:54,  1.97it/s]Epoch 25:  65%|██████▍   | 194/300 [01:41<00:53,  1.97it/s]Epoch 25:  65%|██████▌   | 195/300 [01:41<00:56,  1.87it/s]Epoch 25:  65%|██████▌   | 196/300 [01:42<00:53,  1.94it/s]Epoch 25:  66%|██████▌   | 197/300 [01:42<00:52,  1.98it/s]Epoch 25:  66%|██████▌   | 198/300 [01:43<00:52,  1.94it/s]Epoch 25:  66%|██████▋   | 199/300 [01:44<00:53,  1.88it/s]06/19/2022 20:54:39 - INFO - __main__ - global step: 3850; train loss: 7.878564357757568; dev loss: 7.328436374664307
Epoch 25:  67%|██████▋   | 200/300 [01:44<00:52,  1.92it/s]Epoch 25:  67%|██████▋   | 201/300 [01:45<00:50,  1.96it/s]Epoch 25:  67%|██████▋   | 202/300 [01:45<00:50,  1.94it/s]Epoch 25:  68%|██████▊   | 203/300 [01:46<00:51,  1.87it/s]Epoch 25:  68%|██████▊   | 204/300 [01:46<00:49,  1.93it/s]Epoch 25:  68%|██████▊   | 205/300 [01:47<00:47,  1.98it/s]Epoch 25:  69%|██████▊   | 206/300 [01:47<00:47,  1.98it/s]Epoch 25:  69%|██████▉   | 207/300 [01:48<00:48,  1.90it/s]Epoch 25:  69%|██████▉   | 208/300 [01:48<00:47,  1.95it/s]Epoch 25:  70%|██████▉   | 209/300 [01:49<00:45,  1.98it/s]Epoch 25:  70%|███████   | 210/300 [01:49<00:46,  1.95it/s]Epoch 25:  70%|███████   | 211/300 [01:50<00:45,  1.96it/s]Epoch 25:  71%|███████   | 212/300 [01:50<00:46,  1.88it/s]Epoch 25:  71%|███████   | 213/300 [01:51<00:46,  1.89it/s]Epoch 25:  71%|███████▏  | 214/300 [01:51<00:44,  1.95it/s]Epoch 25:  72%|███████▏  | 215/300 [01:52<00:42,  1.99it/s]Epoch 25:  72%|███████▏  | 216/300 [01:52<00:43,  1.91it/s]Epoch 25:  72%|███████▏  | 217/300 [01:53<00:42,  1.96it/s]Epoch 25:  73%|███████▎  | 218/300 [01:53<00:41,  2.00it/s]Epoch 25:  73%|███████▎  | 219/300 [01:54<00:40,  1.99it/s]06/19/2022 20:54:49 - INFO - __main__ - global step: 3860; train loss: 7.797769069671631; dev loss: 7.457198143005371
Epoch 25:  73%|███████▎  | 220/300 [01:54<00:42,  1.88it/s]Epoch 25:  74%|███████▎  | 221/300 [01:55<00:40,  1.93it/s]Epoch 25:  74%|███████▍  | 222/300 [01:55<00:39,  1.96it/s]Epoch 25:  74%|███████▍  | 223/300 [01:56<00:39,  1.97it/s]Epoch 25:  75%|███████▍  | 224/300 [01:56<00:40,  1.89it/s]Epoch 25:  75%|███████▌  | 225/300 [01:57<00:39,  1.91it/s]Epoch 25:  75%|███████▌  | 226/300 [01:57<00:37,  1.96it/s]Epoch 25:  76%|███████▌  | 227/300 [01:58<00:36,  1.98it/s]Epoch 25:  76%|███████▌  | 228/300 [01:59<00:38,  1.89it/s]Epoch 25:  76%|███████▋  | 229/300 [01:59<00:37,  1.91it/s]Epoch 25:  77%|███████▋  | 230/300 [02:00<00:36,  1.90it/s]Epoch 25:  77%|███████▋  | 231/300 [02:00<00:35,  1.92it/s]Epoch 25:  77%|███████▋  | 232/300 [02:01<00:36,  1.87it/s]Epoch 25:  78%|███████▊  | 233/300 [02:01<00:36,  1.86it/s]Epoch 25:  78%|███████▊  | 234/300 [02:02<00:34,  1.92it/s]Epoch 25:  78%|███████▊  | 235/300 [02:02<00:33,  1.97it/s]Epoch 25:  79%|███████▊  | 236/300 [02:03<00:33,  1.89it/s]Epoch 25:  79%|███████▉  | 237/300 [02:03<00:32,  1.95it/s]Epoch 25:  79%|███████▉  | 238/300 [02:04<00:32,  1.89it/s]Epoch 25:  80%|███████▉  | 239/300 [02:04<00:31,  1.94it/s]06/19/2022 20:55:00 - INFO - __main__ - global step: 3870; train loss: 6.921743869781494; dev loss: 7.420514106750488
Epoch 25:  80%|████████  | 240/300 [02:05<00:31,  1.93it/s]Epoch 25:  80%|████████  | 241/300 [02:05<00:31,  1.87it/s]Epoch 25:  81%|████████  | 242/300 [02:06<00:30,  1.92it/s]Epoch 25:  81%|████████  | 243/300 [02:06<00:28,  1.97it/s]Epoch 25:  81%|████████▏ | 244/300 [02:07<00:28,  1.99it/s]Epoch 25:  82%|████████▏ | 245/300 [02:07<00:28,  1.92it/s]Epoch 25:  82%|████████▏ | 246/300 [02:08<00:27,  1.96it/s]Epoch 25:  82%|████████▏ | 247/300 [02:08<00:26,  2.00it/s]Epoch 25:  83%|████████▎ | 248/300 [02:09<00:25,  2.02it/s]Epoch 25:  83%|████████▎ | 249/300 [02:09<00:26,  1.94it/s]Epoch 25:  83%|████████▎ | 250/300 [02:10<00:26,  1.89it/s]Epoch 25:  84%|████████▎ | 251/300 [02:10<00:25,  1.95it/s]Epoch 25:  84%|████████▍ | 252/300 [02:11<00:24,  1.98it/s]Epoch 25:  84%|████████▍ | 253/300 [02:11<00:24,  1.90it/s]Epoch 25:  85%|████████▍ | 254/300 [02:12<00:24,  1.87it/s]Epoch 25:  85%|████████▌ | 255/300 [02:13<00:24,  1.85it/s]Epoch 25:  85%|████████▌ | 256/300 [02:13<00:23,  1.91it/s]Epoch 25:  86%|████████▌ | 257/300 [02:14<00:24,  1.79it/s]Epoch 25:  86%|████████▌ | 258/300 [02:14<00:22,  1.85it/s]Epoch 25:  86%|████████▋ | 259/300 [02:15<00:21,  1.90it/s]06/19/2022 20:55:10 - INFO - __main__ - global step: 3880; train loss: 7.243084907531738; dev loss: 7.436535835266113
Epoch 25:  87%|████████▋ | 260/300 [02:15<00:20,  1.95it/s]Epoch 25:  87%|████████▋ | 261/300 [02:16<00:20,  1.87it/s]Epoch 25:  87%|████████▋ | 262/300 [02:16<00:20,  1.88it/s]Epoch 25:  88%|████████▊ | 263/300 [02:17<00:19,  1.90it/s]Epoch 25:  88%|████████▊ | 264/300 [02:17<00:18,  1.91it/s]Epoch 25:  88%|████████▊ | 265/300 [02:18<00:17,  1.96it/s]Epoch 25:  89%|████████▊ | 266/300 [02:18<00:18,  1.88it/s]Epoch 25:  89%|████████▉ | 267/300 [02:19<00:17,  1.90it/s]Epoch 25:  89%|████████▉ | 268/300 [02:19<00:16,  1.94it/s]Epoch 25:  90%|████████▉ | 269/300 [02:20<00:15,  1.94it/s]Epoch 25:  90%|█████████ | 270/300 [02:20<00:16,  1.85it/s]Epoch 25:  90%|█████████ | 271/300 [02:21<00:15,  1.84it/s]Epoch 25:  91%|█████████ | 272/300 [02:22<00:15,  1.80it/s]Epoch 25:  91%|█████████ | 273/300 [02:22<00:14,  1.85it/s]Epoch 25:  91%|█████████▏| 274/300 [02:23<00:14,  1.76it/s]Epoch 25:  92%|█████████▏| 275/300 [02:23<00:13,  1.80it/s]Epoch 25:  92%|█████████▏| 276/300 [02:24<00:13,  1.84it/s]Epoch 25:  92%|█████████▏| 277/300 [02:24<00:12,  1.88it/s]Epoch 25:  93%|█████████▎| 278/300 [02:25<00:11,  1.84it/s]Epoch 25:  93%|█████████▎| 279/300 [02:25<00:11,  1.78it/s]06/19/2022 20:55:21 - INFO - __main__ - global step: 3890; train loss: 7.103245735168457; dev loss: 7.532609462738037
Epoch 25:  93%|█████████▎| 280/300 [02:26<00:11,  1.71it/s]Epoch 25:  94%|█████████▎| 281/300 [02:27<00:11,  1.67it/s]Epoch 25:  94%|█████████▍| 282/300 [02:27<00:11,  1.56it/s]Epoch 25:  94%|█████████▍| 283/300 [02:28<00:10,  1.56it/s]Epoch 25:  95%|█████████▍| 284/300 [02:29<00:10,  1.54it/s]Epoch 25:  95%|█████████▌| 285/300 [02:29<00:09,  1.56it/s]Epoch 25:  95%|█████████▌| 286/300 [02:30<00:09,  1.50it/s]Epoch 25:  96%|█████████▌| 287/300 [02:31<00:08,  1.57it/s]Epoch 25:  96%|█████████▌| 288/300 [02:31<00:07,  1.63it/s]Epoch 25:  96%|█████████▋| 289/300 [02:32<00:06,  1.67it/s]Epoch 25:  97%|█████████▋| 290/300 [02:33<00:06,  1.54it/s]Epoch 25:  97%|█████████▋| 291/300 [02:33<00:05,  1.60it/s]Epoch 25:  97%|█████████▋| 292/300 [02:34<00:04,  1.71it/s]Epoch 25:  98%|█████████▊| 293/300 [02:34<00:04,  1.73it/s]Epoch 25:  98%|█████████▊| 294/300 [02:35<00:03,  1.82it/s]Epoch 25:  98%|█████████▊| 295/300 [02:35<00:02,  1.80it/s]Epoch 25:  99%|█████████▊| 296/300 [02:36<00:02,  1.87it/s]Epoch 25:  99%|█████████▉| 297/300 [02:36<00:01,  1.92it/s]Epoch 25:  99%|█████████▉| 298/300 [02:37<00:01,  1.88it/s]Epoch 25: 100%|█████████▉| 299/300 [02:37<00:00,  1.85it/s]06/19/2022 20:55:33 - INFO - __main__ - global step: 3900; train loss: 7.682824611663818; dev loss: 7.6190314292907715
Epoch 25: 100%|██████████| 300/300 [02:38<00:00,  1.92it/s]Epoch 25: 100%|██████████| 300/300 [02:38<00:00,  1.89it/s]
Epoch 26:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 26:   0%|          | 1/300 [00:00<02:22,  2.10it/s]Epoch 26:   1%|          | 2/300 [00:00<02:24,  2.06it/s]Epoch 26:   1%|          | 3/300 [00:01<02:42,  1.82it/s]Epoch 26:   1%|▏         | 4/300 [00:02<02:43,  1.81it/s]Epoch 26:   2%|▏         | 5/300 [00:02<02:34,  1.90it/s]Epoch 26:   2%|▏         | 6/300 [00:03<02:37,  1.87it/s]Epoch 26:   2%|▏         | 7/300 [00:03<02:39,  1.83it/s]Epoch 26:   3%|▎         | 8/300 [00:04<02:40,  1.82it/s]Epoch 26:   3%|▎         | 9/300 [00:04<02:33,  1.89it/s]Epoch 26:   3%|▎         | 10/300 [00:05<02:36,  1.85it/s]Epoch 26:   4%|▎         | 11/300 [00:05<02:39,  1.81it/s]Epoch 26:   4%|▍         | 12/300 [00:06<02:39,  1.80it/s]Epoch 26:   4%|▍         | 13/300 [00:07<02:35,  1.84it/s]Epoch 26:   5%|▍         | 14/300 [00:07<02:33,  1.87it/s]Epoch 26:   5%|▌         | 15/300 [00:08<02:43,  1.74it/s]Epoch 26:   5%|▌         | 16/300 [00:08<02:34,  1.84it/s]Epoch 26:   6%|▌         | 17/300 [00:09<02:28,  1.90it/s]Epoch 26:   6%|▌         | 18/300 [00:09<02:30,  1.87it/s]Epoch 26:   6%|▋         | 19/300 [00:10<02:25,  1.94it/s]06/19/2022 20:55:44 - INFO - __main__ - global step: 3910; train loss: 7.16555118560791; dev loss: 7.487174987792969
Epoch 26:   7%|▋         | 20/300 [00:10<02:36,  1.79it/s]Epoch 26:   7%|▋         | 21/300 [00:11<02:29,  1.86it/s]Epoch 26:   7%|▋         | 22/300 [00:11<02:24,  1.93it/s]Epoch 26:   8%|▊         | 23/300 [00:12<02:19,  1.98it/s]Epoch 26:   8%|▊         | 24/300 [00:12<02:25,  1.90it/s]Epoch 26:   8%|▊         | 25/300 [00:13<02:25,  1.89it/s]Epoch 26:   9%|▊         | 26/300 [00:13<02:19,  1.96it/s]Epoch 26:   9%|▉         | 27/300 [00:14<02:23,  1.91it/s]Epoch 26:   9%|▉         | 28/300 [00:14<02:26,  1.86it/s]Epoch 26:  10%|▉         | 29/300 [00:15<02:21,  1.92it/s]Epoch 26:  10%|█         | 30/300 [00:15<02:17,  1.96it/s]Epoch 26:  10%|█         | 31/300 [00:16<02:19,  1.93it/s]Epoch 26:  11%|█         | 32/300 [00:17<02:24,  1.85it/s]Epoch 26:  11%|█         | 33/300 [00:17<02:27,  1.81it/s]Epoch 26:  11%|█▏        | 34/300 [00:18<02:21,  1.88it/s]Epoch 26:  12%|█▏        | 35/300 [00:18<02:20,  1.89it/s]Epoch 26:  12%|█▏        | 36/300 [00:19<02:26,  1.80it/s]Epoch 26:  12%|█▏        | 37/300 [00:19<02:23,  1.84it/s]Epoch 26:  13%|█▎        | 38/300 [00:20<02:24,  1.81it/s]Epoch 26:  13%|█▎        | 39/300 [00:20<02:18,  1.89it/s]06/19/2022 20:55:54 - INFO - __main__ - global step: 3920; train loss: 7.119651794433594; dev loss: 7.035107612609863
Epoch 26:  13%|█▎        | 40/300 [00:21<02:21,  1.83it/s]Epoch 26:  14%|█▎        | 41/300 [00:21<02:18,  1.87it/s]Epoch 26:  14%|█▍        | 42/300 [00:22<02:16,  1.89it/s]Epoch 26:  14%|█▍        | 43/300 [00:23<02:18,  1.86it/s]Epoch 26:  15%|█▍        | 44/300 [00:23<02:19,  1.83it/s]Epoch 26:  15%|█▌        | 45/300 [00:24<02:13,  1.91it/s]Epoch 26:  15%|█▌        | 46/300 [00:24<02:09,  1.96it/s]Epoch 26:  16%|█▌        | 47/300 [00:25<02:10,  1.94it/s]Epoch 26:  16%|█▌        | 48/300 [00:25<02:07,  1.98it/s]Epoch 26:  16%|█▋        | 49/300 [00:26<02:15,  1.85it/s]Epoch 26:  17%|█▋        | 50/300 [00:26<02:13,  1.87it/s]Epoch 26:  17%|█▋        | 51/300 [00:27<02:08,  1.94it/s]Epoch 26:  17%|█▋        | 52/300 [00:27<02:05,  1.98it/s]Epoch 26:  18%|█▊        | 53/300 [00:28<02:10,  1.89it/s]Epoch 26:  18%|█▊        | 54/300 [00:28<02:06,  1.95it/s]Epoch 26:  18%|█▊        | 55/300 [00:29<02:06,  1.94it/s]Epoch 26:  19%|█▊        | 56/300 [00:29<02:02,  1.99it/s]Epoch 26:  19%|█▉        | 57/300 [00:30<02:12,  1.83it/s]Epoch 26:  19%|█▉        | 58/300 [00:30<02:12,  1.82it/s]Epoch 26:  20%|█▉        | 59/300 [00:31<02:10,  1.85it/s]06/19/2022 20:56:05 - INFO - __main__ - global step: 3930; train loss: 7.261179447174072; dev loss: 7.110420227050781
Epoch 26:  20%|██        | 60/300 [00:31<02:08,  1.87it/s]Epoch 26:  20%|██        | 61/300 [00:32<02:15,  1.76it/s]Epoch 26:  21%|██        | 62/300 [00:33<02:08,  1.85it/s]Epoch 26:  21%|██        | 63/300 [00:33<02:09,  1.83it/s]Epoch 26:  21%|██▏       | 64/300 [00:34<02:10,  1.81it/s]Epoch 26:  22%|██▏       | 65/300 [00:34<02:10,  1.80it/s]Epoch 26:  22%|██▏       | 66/300 [00:35<02:04,  1.88it/s]Epoch 26:  22%|██▏       | 67/300 [00:35<02:01,  1.92it/s]Epoch 26:  23%|██▎       | 68/300 [00:36<01:57,  1.97it/s]Epoch 26:  23%|██▎       | 69/300 [00:36<02:01,  1.90it/s]Epoch 26:  23%|██▎       | 70/300 [00:37<01:58,  1.95it/s]Epoch 26:  24%|██▎       | 71/300 [00:37<01:54,  1.99it/s]Epoch 26:  24%|██▍       | 72/300 [00:38<01:53,  2.01it/s]Epoch 26:  24%|██▍       | 73/300 [00:38<01:57,  1.94it/s]Epoch 26:  25%|██▍       | 74/300 [00:39<02:00,  1.87it/s]Epoch 26:  25%|██▌       | 75/300 [00:39<01:56,  1.94it/s]Epoch 26:  25%|██▌       | 76/300 [00:40<01:56,  1.92it/s]Epoch 26:  26%|██▌       | 77/300 [00:40<01:53,  1.97it/s]Epoch 26:  26%|██▌       | 78/300 [00:41<01:56,  1.90it/s]Epoch 26:  26%|██▋       | 79/300 [00:41<01:52,  1.96it/s]06/19/2022 20:56:15 - INFO - __main__ - global step: 3940; train loss: 7.7400407791137695; dev loss: 7.129189968109131
Epoch 26:  27%|██▋       | 80/300 [00:42<01:50,  2.00it/s]Epoch 26:  27%|██▋       | 81/300 [00:42<01:48,  2.01it/s]Epoch 26:  27%|██▋       | 82/300 [00:43<01:53,  1.92it/s]Epoch 26:  28%|██▊       | 83/300 [00:43<01:52,  1.93it/s]Epoch 26:  28%|██▊       | 84/300 [00:44<01:49,  1.97it/s]Epoch 26:  28%|██▊       | 85/300 [00:44<01:49,  1.97it/s]Epoch 26:  29%|██▊       | 86/300 [00:45<01:57,  1.82it/s]Epoch 26:  29%|██▉       | 87/300 [00:46<01:54,  1.87it/s]Epoch 26:  29%|██▉       | 88/300 [00:46<01:49,  1.94it/s]Epoch 26:  30%|██▉       | 89/300 [00:47<01:46,  1.98it/s]Epoch 26:  30%|███       | 90/300 [00:47<01:51,  1.89it/s]Epoch 26:  30%|███       | 91/300 [00:48<01:47,  1.95it/s]Epoch 26:  31%|███       | 92/300 [00:48<01:50,  1.88it/s]Epoch 26:  31%|███       | 93/300 [00:49<01:46,  1.95it/s]Epoch 26:  31%|███▏      | 94/300 [00:49<01:54,  1.79it/s]Epoch 26:  32%|███▏      | 95/300 [00:50<01:49,  1.87it/s]Epoch 26:  32%|███▏      | 96/300 [00:50<01:45,  1.94it/s]Epoch 26:  32%|███▏      | 97/300 [00:51<01:42,  1.97it/s]Epoch 26:  33%|███▎      | 98/300 [00:51<01:46,  1.89it/s]Epoch 26:  33%|███▎      | 99/300 [00:52<01:43,  1.94it/s]06/19/2022 20:56:26 - INFO - __main__ - global step: 3950; train loss: 7.324337959289551; dev loss: 7.5133514404296875
Epoch 26:  33%|███▎      | 100/300 [00:52<01:41,  1.98it/s]Epoch 26:  34%|███▎      | 101/300 [00:53<01:38,  2.01it/s]Epoch 26:  34%|███▍      | 102/300 [00:53<01:39,  1.98it/s]Epoch 26:  34%|███▍      | 103/300 [00:54<01:45,  1.87it/s]Epoch 26:  35%|███▍      | 104/300 [00:54<01:41,  1.94it/s]Epoch 26:  35%|███▌      | 105/300 [00:55<01:41,  1.93it/s]Epoch 26:  35%|███▌      | 106/300 [00:55<01:39,  1.96it/s]Epoch 26:  36%|███▌      | 107/300 [00:56<01:44,  1.84it/s]Epoch 26:  36%|███▌      | 108/300 [00:56<01:40,  1.91it/s]Epoch 26:  36%|███▋      | 109/300 [00:57<01:37,  1.96it/s]Epoch 26:  37%|███▋      | 110/300 [00:57<01:36,  1.97it/s]Epoch 26:  37%|███▋      | 111/300 [00:58<01:39,  1.90it/s]Epoch 26:  37%|███▋      | 112/300 [00:58<01:36,  1.94it/s]Epoch 26:  38%|███▊      | 113/300 [00:59<01:35,  1.97it/s]Epoch 26:  38%|███▊      | 114/300 [00:59<01:33,  1.98it/s]Epoch 26:  38%|███▊      | 115/300 [01:00<01:37,  1.91it/s]Epoch 26:  39%|███▊      | 116/300 [01:01<01:36,  1.91it/s]Epoch 26:  39%|███▉      | 117/300 [01:01<01:33,  1.96it/s]Epoch 26:  39%|███▉      | 118/300 [01:02<01:31,  2.00it/s]Epoch 26:  40%|███▉      | 119/300 [01:02<01:34,  1.91it/s]06/19/2022 20:56:36 - INFO - __main__ - global step: 3960; train loss: 7.547893524169922; dev loss: 7.498935699462891
Epoch 26:  40%|████      | 120/300 [01:03<01:32,  1.95it/s]Epoch 26:  40%|████      | 121/300 [01:03<01:29,  1.99it/s]Epoch 26:  41%|████      | 122/300 [01:04<01:29,  1.99it/s]Epoch 26:  41%|████      | 123/300 [01:04<01:33,  1.89it/s]Epoch 26:  41%|████▏     | 124/300 [01:05<01:30,  1.94it/s]Epoch 26:  42%|████▏     | 125/300 [01:05<01:30,  1.93it/s]Epoch 26:  42%|████▏     | 126/300 [01:06<01:27,  1.98it/s]Epoch 26:  42%|████▏     | 127/300 [01:06<01:26,  2.01it/s]Epoch 26:  43%|████▎     | 128/300 [01:07<01:29,  1.92it/s]Epoch 26:  43%|████▎     | 129/300 [01:07<01:27,  1.94it/s]Epoch 26:  43%|████▎     | 130/300 [01:08<01:25,  1.98it/s]Epoch 26:  44%|████▎     | 131/300 [01:08<01:23,  2.01it/s]Epoch 26:  44%|████▍     | 132/300 [01:09<01:27,  1.92it/s]Epoch 26:  44%|████▍     | 133/300 [01:09<01:24,  1.97it/s]Epoch 26:  45%|████▍     | 134/300 [01:10<01:22,  2.00it/s]Epoch 26:  45%|████▌     | 135/300 [01:10<01:24,  1.95it/s]Epoch 26:  45%|████▌     | 136/300 [01:11<01:28,  1.84it/s]Epoch 26:  46%|████▌     | 137/300 [01:11<01:26,  1.89it/s]Epoch 26:  46%|████▌     | 138/300 [01:12<01:23,  1.94it/s]Epoch 26:  46%|████▋     | 139/300 [01:12<01:21,  1.98it/s]06/19/2022 20:56:46 - INFO - __main__ - global step: 3970; train loss: 7.351044654846191; dev loss: 7.302008152008057
Epoch 26:  47%|████▋     | 140/300 [01:13<01:24,  1.90it/s]Epoch 26:  47%|████▋     | 141/300 [01:13<01:21,  1.95it/s]Epoch 26:  47%|████▋     | 142/300 [01:14<01:19,  1.99it/s]Epoch 26:  48%|████▊     | 143/300 [01:14<01:18,  1.99it/s]Epoch 26:  48%|████▊     | 144/300 [01:15<01:21,  1.92it/s]Epoch 26:  48%|████▊     | 145/300 [01:15<01:18,  1.97it/s]Epoch 26:  49%|████▊     | 146/300 [01:16<01:17,  1.98it/s]Epoch 26:  49%|████▉     | 147/300 [01:16<01:16,  2.01it/s]Epoch 26:  49%|████▉     | 148/300 [01:17<01:19,  1.92it/s]Epoch 26:  50%|████▉     | 149/300 [01:17<01:16,  1.98it/s]Epoch 26:  50%|█████     | 150/300 [01:18<01:14,  2.01it/s]Epoch 26:  50%|█████     | 151/300 [01:18<01:13,  2.03it/s]Epoch 26:  51%|█████     | 152/300 [01:19<01:16,  1.93it/s]Epoch 26:  51%|█████     | 153/300 [01:19<01:14,  1.99it/s]Epoch 26:  51%|█████▏    | 154/300 [01:20<01:12,  2.01it/s]Epoch 26:  52%|█████▏    | 155/300 [01:20<01:11,  2.04it/s]Epoch 26:  52%|█████▏    | 156/300 [01:21<01:10,  2.04it/s]Epoch 26:  52%|█████▏    | 157/300 [01:21<01:13,  1.95it/s]Epoch 26:  53%|█████▎    | 158/300 [01:22<01:11,  1.98it/s]Epoch 26:  53%|█████▎    | 159/300 [01:22<01:10,  2.01it/s]06/19/2022 20:56:56 - INFO - __main__ - global step: 3980; train loss: 6.971158027648926; dev loss: 7.579853057861328
Epoch 26:  53%|█████▎    | 160/300 [01:23<01:08,  2.04it/s]Epoch 26:  54%|█████▎    | 161/300 [01:23<01:11,  1.95it/s]Epoch 26:  54%|█████▍    | 162/300 [01:24<01:09,  2.00it/s]Epoch 26:  54%|█████▍    | 163/300 [01:24<01:07,  2.04it/s]Epoch 26:  55%|█████▍    | 164/300 [01:25<01:07,  2.01it/s]Epoch 26:  55%|█████▌    | 165/300 [01:25<01:10,  1.92it/s]Epoch 26:  55%|█████▌    | 166/300 [01:26<01:08,  1.95it/s]Epoch 26:  56%|█████▌    | 167/300 [01:26<01:06,  2.00it/s]Epoch 26:  56%|█████▌    | 168/300 [01:27<01:05,  2.03it/s]Epoch 26:  56%|█████▋    | 169/300 [01:27<01:07,  1.94it/s]Epoch 26:  57%|█████▋    | 170/300 [01:28<01:06,  1.95it/s]Epoch 26:  57%|█████▋    | 171/300 [01:28<01:04,  1.99it/s]Epoch 26:  57%|█████▋    | 172/300 [01:29<01:03,  2.02it/s]Epoch 26:  58%|█████▊    | 173/300 [01:30<01:05,  1.94it/s]Epoch 26:  58%|█████▊    | 174/300 [01:30<01:06,  1.90it/s]Epoch 26:  58%|█████▊    | 175/300 [01:31<01:09,  1.81it/s]Epoch 26:  59%|█████▊    | 176/300 [01:31<01:11,  1.75it/s]Epoch 26:  59%|█████▉    | 177/300 [01:32<01:16,  1.61it/s]Epoch 26:  59%|█████▉    | 178/300 [01:33<01:15,  1.61it/s]Epoch 26:  60%|█████▉    | 179/300 [01:33<01:15,  1.60it/s]06/19/2022 20:57:07 - INFO - __main__ - global step: 3990; train loss: 7.835236549377441; dev loss: 7.6934332847595215
Epoch 26:  60%|██████    | 180/300 [01:34<01:15,  1.60it/s]Epoch 26:  60%|██████    | 181/300 [01:35<01:14,  1.60it/s]Epoch 26:  61%|██████    | 182/300 [01:35<01:12,  1.63it/s]Epoch 26:  61%|██████    | 183/300 [01:36<01:08,  1.71it/s]Epoch 26:  61%|██████▏   | 184/300 [01:36<01:05,  1.78it/s]Epoch 26:  62%|██████▏   | 185/300 [01:37<01:01,  1.86it/s]Epoch 26:  62%|██████▏   | 186/300 [01:37<01:02,  1.83it/s]Epoch 26:  62%|██████▏   | 187/300 [01:38<00:59,  1.90it/s]Epoch 26:  63%|██████▎   | 188/300 [01:38<00:57,  1.95it/s]Epoch 26:  63%|██████▎   | 189/300 [01:39<00:56,  1.97it/s]Epoch 26:  63%|██████▎   | 190/300 [01:39<01:00,  1.82it/s]Epoch 26:  64%|██████▎   | 191/300 [01:40<00:58,  1.85it/s]Epoch 26:  64%|██████▍   | 192/300 [01:40<00:56,  1.92it/s]Epoch 26:  64%|██████▍   | 193/300 [01:41<00:54,  1.96it/s]Epoch 26:  65%|██████▍   | 194/300 [01:41<00:55,  1.90it/s]Epoch 26:  65%|██████▌   | 195/300 [01:42<00:56,  1.87it/s]Epoch 26:  65%|██████▌   | 196/300 [01:42<00:53,  1.93it/s]Epoch 26:  66%|██████▌   | 197/300 [01:43<00:52,  1.98it/s]Epoch 26:  66%|██████▌   | 198/300 [01:43<00:54,  1.86it/s]Epoch 26:  66%|██████▋   | 199/300 [01:44<00:52,  1.93it/s]06/19/2022 20:57:18 - INFO - __main__ - global step: 4000; train loss: 7.449278831481934; dev loss: 7.629511833190918
Epoch 26:  67%|██████▋   | 200/300 [01:44<00:51,  1.92it/s]Epoch 26:  67%|██████▋   | 201/300 [01:45<00:50,  1.97it/s]Epoch 26:  67%|██████▋   | 202/300 [01:46<00:52,  1.85it/s]Epoch 26:  68%|██████▊   | 203/300 [01:46<00:50,  1.92it/s]Epoch 26:  68%|██████▊   | 204/300 [01:47<00:49,  1.94it/s]Epoch 26:  68%|██████▊   | 205/300 [01:47<00:47,  1.98it/s]Epoch 26:  69%|██████▊   | 206/300 [01:48<00:50,  1.87it/s]Epoch 26:  69%|██████▉   | 207/300 [01:48<00:48,  1.93it/s]Epoch 26:  69%|██████▉   | 208/300 [01:49<00:46,  1.97it/s]Epoch 26:  70%|██████▉   | 209/300 [01:49<00:45,  2.00it/s]Epoch 26:  70%|███████   | 210/300 [01:50<00:45,  2.00it/s]Epoch 26:  70%|███████   | 211/300 [01:50<00:46,  1.91it/s]Epoch 26:  71%|███████   | 212/300 [01:51<00:45,  1.95it/s]Epoch 26:  71%|███████   | 213/300 [01:51<00:43,  1.99it/s]Epoch 26:  71%|███████▏  | 214/300 [01:52<00:42,  2.02it/s]Epoch 26:  72%|███████▏  | 215/300 [01:52<00:44,  1.93it/s]Epoch 26:  72%|███████▏  | 216/300 [01:53<00:42,  1.97it/s]Epoch 26:  72%|███████▏  | 217/300 [01:53<00:41,  2.00it/s]Epoch 26:  73%|███████▎  | 218/300 [01:54<00:40,  2.03it/s]Epoch 26:  73%|███████▎  | 219/300 [01:54<00:41,  1.94it/s]06/19/2022 20:57:28 - INFO - __main__ - global step: 4010; train loss: 7.874176979064941; dev loss: 7.517819881439209
Epoch 26:  73%|███████▎  | 220/300 [01:55<00:40,  1.97it/s]Epoch 26:  74%|███████▎  | 221/300 [01:55<00:39,  1.99it/s]Epoch 26:  74%|███████▍  | 222/300 [01:56<00:39,  1.98it/s]Epoch 26:  74%|███████▍  | 223/300 [01:56<00:40,  1.90it/s]Epoch 26:  75%|███████▍  | 224/300 [01:57<00:38,  1.95it/s]Epoch 26:  75%|███████▌  | 225/300 [01:57<00:37,  1.99it/s]Epoch 26:  75%|███████▌  | 226/300 [01:58<00:36,  2.02it/s]Epoch 26:  76%|███████▌  | 227/300 [01:58<00:37,  1.92it/s]Epoch 26:  76%|███████▌  | 228/300 [01:59<00:36,  1.97it/s]Epoch 26:  76%|███████▋  | 229/300 [01:59<00:36,  1.97it/s]Epoch 26:  77%|███████▋  | 230/300 [02:00<00:35,  1.97it/s]Epoch 26:  77%|███████▋  | 231/300 [02:00<00:36,  1.90it/s]Epoch 26:  77%|███████▋  | 232/300 [02:01<00:34,  1.95it/s]Epoch 26:  78%|███████▊  | 233/300 [02:01<00:33,  2.00it/s]Epoch 26:  78%|███████▊  | 234/300 [02:02<00:32,  2.02it/s]Epoch 26:  78%|███████▊  | 235/300 [02:02<00:32,  2.00it/s]Epoch 26:  79%|███████▊  | 236/300 [02:03<00:33,  1.91it/s]Epoch 26:  79%|███████▉  | 237/300 [02:03<00:32,  1.96it/s]Epoch 26:  79%|███████▉  | 238/300 [02:04<00:31,  2.00it/s]Epoch 26:  80%|███████▉  | 239/300 [02:04<00:30,  2.03it/s]06/19/2022 20:57:38 - INFO - __main__ - global step: 4020; train loss: 7.689412593841553; dev loss: 7.265016078948975
Epoch 26:  80%|████████  | 240/300 [02:05<00:30,  1.94it/s]Epoch 26:  80%|████████  | 241/300 [02:05<00:29,  1.99it/s]Epoch 26:  81%|████████  | 242/300 [02:06<00:28,  2.01it/s]Epoch 26:  81%|████████  | 243/300 [02:06<00:28,  2.03it/s]Epoch 26:  81%|████████▏ | 244/300 [02:07<00:28,  1.93it/s]Epoch 26:  82%|████████▏ | 245/300 [02:07<00:27,  1.98it/s]Epoch 26:  82%|████████▏ | 246/300 [02:08<00:29,  1.83it/s]Epoch 26:  82%|████████▏ | 247/300 [02:09<00:30,  1.75it/s]Epoch 26:  83%|████████▎ | 248/300 [02:09<00:32,  1.62it/s]Epoch 26:  83%|████████▎ | 249/300 [02:10<00:31,  1.63it/s]Epoch 26:  83%|████████▎ | 250/300 [02:11<00:30,  1.64it/s]Epoch 26:  84%|████████▎ | 251/300 [02:11<00:29,  1.65it/s]Epoch 26:  84%|████████▍ | 252/300 [02:12<00:29,  1.65it/s]Epoch 26:  84%|████████▍ | 253/300 [02:12<00:26,  1.77it/s]Epoch 26:  85%|████████▍ | 254/300 [02:13<00:25,  1.81it/s]Epoch 26:  85%|████████▌ | 255/300 [02:13<00:24,  1.85it/s]Epoch 26:  85%|████████▌ | 256/300 [02:14<00:25,  1.75it/s]Epoch 26:  86%|████████▌ | 257/300 [02:14<00:23,  1.84it/s]Epoch 26:  86%|████████▌ | 258/300 [02:15<00:21,  1.92it/s]Epoch 26:  86%|████████▋ | 259/300 [02:15<00:20,  1.95it/s]06/19/2022 20:57:49 - INFO - __main__ - global step: 4030; train loss: 7.386152744293213; dev loss: 7.315592288970947
Epoch 26:  87%|████████▋ | 260/300 [02:16<00:21,  1.84it/s]Epoch 26:  87%|████████▋ | 261/300 [02:16<00:20,  1.87it/s]Epoch 26:  87%|████████▋ | 262/300 [02:17<00:19,  1.93it/s]Epoch 26:  88%|████████▊ | 263/300 [02:17<00:18,  1.97it/s]Epoch 26:  88%|████████▊ | 264/300 [02:18<00:18,  2.00it/s]Epoch 26:  88%|████████▊ | 265/300 [02:18<00:18,  1.90it/s]Epoch 26:  89%|████████▊ | 266/300 [02:19<00:17,  1.89it/s]Epoch 26:  89%|████████▉ | 267/300 [02:20<00:16,  1.94it/s]Epoch 26:  89%|████████▉ | 268/300 [02:20<00:16,  1.98it/s]Epoch 26:  90%|████████▉ | 269/300 [02:21<00:16,  1.91it/s]Epoch 26:  90%|█████████ | 270/300 [02:21<00:15,  1.96it/s]Epoch 26:  90%|█████████ | 271/300 [02:22<00:14,  1.99it/s]Epoch 26:  91%|█████████ | 272/300 [02:22<00:13,  2.01it/s]Epoch 26:  91%|█████████ | 273/300 [02:23<00:14,  1.92it/s]Epoch 26:  91%|█████████▏| 274/300 [02:23<00:13,  1.96it/s]Epoch 26:  92%|█████████▏| 275/300 [02:24<00:12,  2.00it/s]Epoch 26:  92%|█████████▏| 276/300 [02:24<00:12,  1.97it/s]Epoch 26:  92%|█████████▏| 277/300 [02:25<00:12,  1.90it/s]Epoch 26:  93%|█████████▎| 278/300 [02:25<00:11,  1.94it/s]Epoch 26:  93%|█████████▎| 279/300 [02:26<00:10,  1.98it/s]06/19/2022 20:57:59 - INFO - __main__ - global step: 4040; train loss: 7.543032646179199; dev loss: 7.627863883972168
Epoch 26:  93%|█████████▎| 280/300 [02:26<00:09,  2.02it/s]Epoch 26:  94%|█████████▎| 281/300 [02:27<00:09,  1.93it/s]Epoch 26:  94%|█████████▍| 282/300 [02:27<00:09,  1.97it/s]Epoch 26:  94%|█████████▍| 283/300 [02:28<00:08,  2.00it/s]Epoch 26:  95%|█████████▍| 284/300 [02:28<00:07,  2.00it/s]Epoch 26:  95%|█████████▌| 285/300 [02:29<00:07,  1.92it/s]Epoch 26:  95%|█████████▌| 286/300 [02:29<00:07,  1.96it/s]Epoch 26:  96%|█████████▌| 287/300 [02:30<00:06,  1.96it/s]Epoch 26:  96%|█████████▌| 288/300 [02:30<00:05,  2.00it/s]Epoch 26:  96%|█████████▋| 289/300 [02:31<00:05,  2.01it/s]Epoch 26:  97%|█████████▋| 290/300 [02:31<00:05,  1.86it/s]Epoch 26:  97%|█████████▋| 291/300 [02:32<00:04,  1.93it/s]Epoch 26:  97%|█████████▋| 292/300 [02:32<00:04,  1.97it/s]Epoch 26:  98%|█████████▊| 293/300 [02:33<00:03,  2.00it/s]Epoch 26:  98%|█████████▊| 294/300 [02:33<00:03,  1.93it/s]Epoch 26:  98%|█████████▊| 295/300 [02:34<00:02,  1.98it/s]Epoch 26:  99%|█████████▊| 296/300 [02:34<00:01,  2.01it/s]Epoch 26:  99%|█████████▉| 297/300 [02:35<00:01,  2.02it/s]Epoch 26:  99%|█████████▉| 298/300 [02:35<00:01,  1.93it/s]Epoch 26: 100%|█████████▉| 299/300 [02:36<00:00,  1.98it/s]06/19/2022 20:58:10 - INFO - __main__ - global step: 4050; train loss: 7.600921630859375; dev loss: 7.462430477142334
Epoch 26: 100%|██████████| 300/300 [02:36<00:00,  2.01it/s]Epoch 26: 100%|██████████| 300/300 [02:36<00:00,  1.91it/s]
Epoch 27:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 27:   0%|          | 1/300 [00:00<02:23,  2.09it/s]Epoch 27:   1%|          | 2/300 [00:01<02:38,  1.89it/s]Epoch 27:   1%|          | 3/300 [00:01<02:29,  1.98it/s]Epoch 27:   1%|▏         | 4/300 [00:02<02:27,  2.01it/s]Epoch 27:   2%|▏         | 5/300 [00:02<02:29,  1.97it/s]Epoch 27:   2%|▏         | 6/300 [00:03<02:40,  1.83it/s]Epoch 27:   2%|▏         | 7/300 [00:03<02:37,  1.86it/s]Epoch 27:   3%|▎         | 8/300 [00:04<02:32,  1.91it/s]Epoch 27:   3%|▎         | 9/300 [00:04<02:29,  1.95it/s]Epoch 27:   3%|▎         | 10/300 [00:05<02:37,  1.84it/s]Epoch 27:   4%|▎         | 11/300 [00:05<02:34,  1.87it/s]Epoch 27:   4%|▍         | 12/300 [00:06<02:30,  1.91it/s]Epoch 27:   4%|▍         | 13/300 [00:06<02:27,  1.94it/s]Epoch 27:   5%|▍         | 14/300 [00:07<02:33,  1.87it/s]Epoch 27:   5%|▌         | 15/300 [00:07<02:26,  1.94it/s]Epoch 27:   5%|▌         | 16/300 [00:08<02:23,  1.98it/s]Epoch 27:   6%|▌         | 17/300 [00:08<02:20,  2.01it/s]Epoch 27:   6%|▌         | 18/300 [00:09<02:22,  1.98it/s]Epoch 27:   6%|▋         | 19/300 [00:09<02:27,  1.90it/s]06/19/2022 20:58:20 - INFO - __main__ - global step: 4060; train loss: 7.45648193359375; dev loss: 7.2509942054748535
Epoch 27:   7%|▋         | 20/300 [00:10<02:23,  1.96it/s]Epoch 27:   7%|▋         | 21/300 [00:10<02:19,  1.99it/s]Epoch 27:   7%|▋         | 22/300 [00:11<02:17,  2.02it/s]Epoch 27:   8%|▊         | 23/300 [00:11<02:26,  1.90it/s]Epoch 27:   8%|▊         | 24/300 [00:12<02:21,  1.95it/s]Epoch 27:   8%|▊         | 25/300 [00:12<02:22,  1.94it/s]Epoch 27:   9%|▊         | 26/300 [00:13<02:18,  1.98it/s]Epoch 27:   9%|▉         | 27/300 [00:13<02:23,  1.91it/s]Epoch 27:   9%|▉         | 28/300 [00:14<02:19,  1.94it/s]Epoch 27:  10%|▉         | 29/300 [00:14<02:20,  1.93it/s]Epoch 27:  10%|█         | 30/300 [00:15<02:16,  1.97it/s]Epoch 27:  10%|█         | 31/300 [00:16<02:21,  1.90it/s]Epoch 27:  11%|█         | 32/300 [00:16<02:17,  1.95it/s]Epoch 27:  11%|█         | 33/300 [00:17<02:16,  1.96it/s]Epoch 27:  11%|█▏        | 34/300 [00:17<02:13,  1.99it/s]Epoch 27:  12%|█▏        | 35/300 [00:18<02:19,  1.91it/s]Epoch 27:  12%|█▏        | 36/300 [00:18<02:15,  1.95it/s]Epoch 27:  12%|█▏        | 37/300 [00:19<02:16,  1.92it/s]Epoch 27:  13%|█▎        | 38/300 [00:19<02:13,  1.96it/s]Epoch 27:  13%|█▎        | 39/300 [00:20<02:21,  1.85it/s]06/19/2022 20:58:30 - INFO - __main__ - global step: 4070; train loss: 7.915977478027344; dev loss: 7.719751834869385
Epoch 27:  13%|█▎        | 40/300 [00:20<02:16,  1.91it/s]Epoch 27:  14%|█▎        | 41/300 [00:21<02:11,  1.97it/s]Epoch 27:  14%|█▍        | 42/300 [00:21<02:08,  2.01it/s]Epoch 27:  14%|█▍        | 43/300 [00:22<02:08,  1.99it/s]Epoch 27:  15%|█▍        | 44/300 [00:22<02:17,  1.87it/s]Epoch 27:  15%|█▌        | 45/300 [00:23<02:12,  1.92it/s]Epoch 27:  15%|█▌        | 46/300 [00:23<02:09,  1.97it/s]Epoch 27:  16%|█▌        | 47/300 [00:24<02:06,  2.00it/s]Epoch 27:  16%|█▌        | 48/300 [00:24<02:13,  1.89it/s]Epoch 27:  16%|█▋        | 49/300 [00:25<02:10,  1.92it/s]Epoch 27:  17%|█▋        | 50/300 [00:25<02:07,  1.96it/s]Epoch 27:  17%|█▋        | 51/300 [00:26<02:04,  1.99it/s]Epoch 27:  17%|█▋        | 52/300 [00:26<02:12,  1.87it/s]Epoch 27:  18%|█▊        | 53/300 [00:27<02:07,  1.93it/s]Epoch 27:  18%|█▊        | 54/300 [00:27<02:04,  1.97it/s]Epoch 27:  18%|█▊        | 55/300 [00:28<02:03,  1.98it/s]Epoch 27:  19%|█▊        | 56/300 [00:28<02:08,  1.90it/s]Epoch 27:  19%|█▉        | 57/300 [00:29<02:04,  1.95it/s]Epoch 27:  19%|█▉        | 58/300 [00:29<02:01,  1.99it/s]Epoch 27:  20%|█▉        | 59/300 [00:30<02:02,  1.96it/s]06/19/2022 20:58:41 - INFO - __main__ - global step: 4080; train loss: 7.44524621963501; dev loss: 7.708086967468262
Epoch 27:  20%|██        | 60/300 [00:31<02:08,  1.86it/s]Epoch 27:  20%|██        | 61/300 [00:31<02:03,  1.93it/s]Epoch 27:  21%|██        | 62/300 [00:31<02:00,  1.97it/s]Epoch 27:  21%|██        | 63/300 [00:32<02:00,  1.97it/s]Epoch 27:  21%|██▏       | 64/300 [00:33<02:05,  1.88it/s]Epoch 27:  22%|██▏       | 65/300 [00:33<02:01,  1.94it/s]Epoch 27:  22%|██▏       | 66/300 [00:34<01:58,  1.98it/s]Epoch 27:  22%|██▏       | 67/300 [00:34<01:57,  1.99it/s]Epoch 27:  23%|██▎       | 68/300 [00:35<02:02,  1.90it/s]Epoch 27:  23%|██▎       | 69/300 [00:35<01:58,  1.96it/s]Epoch 27:  23%|██▎       | 70/300 [00:36<01:55,  2.00it/s]Epoch 27:  24%|██▎       | 71/300 [00:36<01:55,  1.98it/s]Epoch 27:  24%|██▍       | 72/300 [00:37<01:53,  2.00it/s]Epoch 27:  24%|██▍       | 73/300 [00:37<02:01,  1.86it/s]Epoch 27:  25%|██▍       | 74/300 [00:38<01:57,  1.93it/s]Epoch 27:  25%|██▌       | 75/300 [00:38<01:57,  1.91it/s]Epoch 27:  25%|██▌       | 76/300 [00:39<01:57,  1.91it/s]Epoch 27:  26%|██▌       | 77/300 [00:39<02:01,  1.84it/s]Epoch 27:  26%|██▌       | 78/300 [00:40<01:56,  1.91it/s]Epoch 27:  26%|██▋       | 79/300 [00:40<01:52,  1.96it/s]06/19/2022 20:58:51 - INFO - __main__ - global step: 4090; train loss: 7.274233818054199; dev loss: 6.990674018859863
Epoch 27:  27%|██▋       | 80/300 [00:41<01:51,  1.98it/s]Epoch 27:  27%|██▋       | 81/300 [00:41<01:54,  1.91it/s]Epoch 27:  27%|██▋       | 82/300 [00:42<01:52,  1.93it/s]Epoch 27:  28%|██▊       | 83/300 [00:42<01:49,  1.98it/s]Epoch 27:  28%|██▊       | 84/300 [00:43<01:47,  2.01it/s]Epoch 27:  28%|██▊       | 85/300 [00:43<01:51,  1.93it/s]Epoch 27:  29%|██▊       | 86/300 [00:44<01:49,  1.96it/s]Epoch 27:  29%|██▉       | 87/300 [00:44<01:46,  2.00it/s]Epoch 27:  29%|██▉       | 88/300 [00:45<01:44,  2.02it/s]Epoch 27:  30%|██▉       | 89/300 [00:45<01:48,  1.94it/s]Epoch 27:  30%|███       | 90/300 [00:46<01:45,  1.99it/s]Epoch 27:  30%|███       | 91/300 [00:46<01:43,  2.01it/s]Epoch 27:  31%|███       | 92/300 [00:47<01:44,  1.99it/s]Epoch 27:  31%|███       | 93/300 [00:47<01:48,  1.90it/s]Epoch 27:  31%|███▏      | 94/300 [00:48<01:45,  1.95it/s]Epoch 27:  32%|███▏      | 95/300 [00:48<01:43,  1.99it/s]Epoch 27:  32%|███▏      | 96/300 [00:49<01:41,  2.01it/s]Epoch 27:  32%|███▏      | 97/300 [00:49<01:39,  2.03it/s]Epoch 27:  33%|███▎      | 98/300 [00:50<01:45,  1.92it/s]Epoch 27:  33%|███▎      | 99/300 [00:50<01:42,  1.97it/s]06/19/2022 20:59:01 - INFO - __main__ - global step: 4100; train loss: 7.929022312164307; dev loss: 7.797369956970215
Epoch 27:  33%|███▎      | 100/300 [00:51<01:40,  2.00it/s]Epoch 27:  34%|███▎      | 101/300 [00:51<01:38,  2.01it/s]Epoch 27:  34%|███▍      | 102/300 [00:52<01:44,  1.89it/s]Epoch 27:  34%|███▍      | 103/300 [00:52<01:42,  1.92it/s]Epoch 27:  35%|███▍      | 104/300 [00:53<01:40,  1.96it/s]Epoch 27:  35%|███▌      | 105/300 [00:53<01:38,  1.97it/s]Epoch 27:  35%|███▌      | 106/300 [00:54<01:42,  1.89it/s]Epoch 27:  36%|███▌      | 107/300 [00:55<01:39,  1.94it/s]Epoch 27:  36%|███▌      | 108/300 [00:55<01:38,  1.96it/s]Epoch 27:  36%|███▋      | 109/300 [00:56<01:35,  2.00it/s]Epoch 27:  37%|███▋      | 110/300 [00:56<01:40,  1.90it/s]Epoch 27:  37%|███▋      | 111/300 [00:57<01:37,  1.94it/s]Epoch 27:  37%|███▋      | 112/300 [00:57<01:36,  1.95it/s]Epoch 27:  38%|███▊      | 113/300 [00:58<01:34,  1.98it/s]Epoch 27:  38%|███▊      | 114/300 [00:58<01:38,  1.90it/s]Epoch 27:  38%|███▊      | 115/300 [00:59<01:37,  1.90it/s]Epoch 27:  39%|███▊      | 116/300 [00:59<01:40,  1.82it/s]Epoch 27:  39%|███▉      | 117/300 [01:00<01:43,  1.77it/s]Epoch 27:  39%|███▉      | 118/300 [01:01<01:48,  1.68it/s]Epoch 27:  40%|███▉      | 119/300 [01:01<01:44,  1.73it/s]06/19/2022 20:59:12 - INFO - __main__ - global step: 4110; train loss: 7.1613287925720215; dev loss: 7.232956886291504
Epoch 27:  40%|████      | 120/300 [01:02<01:44,  1.73it/s]Epoch 27:  40%|████      | 121/300 [01:02<01:44,  1.72it/s]Epoch 27:  41%|████      | 122/300 [01:03<01:49,  1.63it/s]Epoch 27:  41%|████      | 123/300 [01:04<01:45,  1.67it/s]Epoch 27:  41%|████▏     | 124/300 [01:04<01:43,  1.70it/s]Epoch 27:  42%|████▏     | 125/300 [01:05<01:41,  1.73it/s]Epoch 27:  42%|████▏     | 126/300 [01:05<01:35,  1.82it/s]Epoch 27:  42%|████▏     | 127/300 [01:06<01:39,  1.74it/s]Epoch 27:  43%|████▎     | 128/300 [01:06<01:37,  1.76it/s]Epoch 27:  43%|████▎     | 129/300 [01:07<01:37,  1.76it/s]Epoch 27:  43%|████▎     | 130/300 [01:07<01:37,  1.74it/s]Epoch 27:  44%|████▎     | 131/300 [01:08<01:42,  1.64it/s]Epoch 27:  44%|████▍     | 132/300 [01:09<01:38,  1.70it/s]Epoch 27:  44%|████▍     | 133/300 [01:09<01:37,  1.72it/s]Epoch 27:  45%|████▍     | 134/300 [01:10<01:37,  1.70it/s]Epoch 27:  45%|████▌     | 135/300 [01:11<01:43,  1.60it/s]Epoch 27:  45%|████▌     | 136/300 [01:11<01:40,  1.63it/s]Epoch 27:  46%|████▌     | 137/300 [01:12<01:38,  1.66it/s]Epoch 27:  46%|████▌     | 138/300 [01:12<01:36,  1.68it/s]Epoch 27:  46%|████▋     | 139/300 [01:13<01:38,  1.63it/s]06/19/2022 20:59:24 - INFO - __main__ - global step: 4120; train loss: 7.639845848083496; dev loss: 7.463212490081787
Epoch 27:  47%|████▋     | 140/300 [01:13<01:33,  1.71it/s]Epoch 27:  47%|████▋     | 141/300 [01:14<01:32,  1.72it/s]Epoch 27:  47%|████▋     | 142/300 [01:15<01:31,  1.74it/s]Epoch 27:  48%|████▊     | 143/300 [01:15<01:36,  1.63it/s]Epoch 27:  48%|████▊     | 144/300 [01:16<01:34,  1.65it/s]Epoch 27:  48%|████▊     | 145/300 [01:16<01:33,  1.66it/s]Epoch 27:  49%|████▊     | 146/300 [01:17<01:32,  1.66it/s]Epoch 27:  49%|████▉     | 147/300 [01:18<01:36,  1.59it/s]Epoch 27:  49%|████▉     | 148/300 [01:18<01:32,  1.64it/s]Epoch 27:  50%|████▉     | 149/300 [01:19<01:29,  1.68it/s]Epoch 27:  50%|█████     | 150/300 [01:19<01:26,  1.73it/s]Epoch 27:  50%|█████     | 151/300 [01:20<01:26,  1.73it/s]Epoch 27:  51%|█████     | 152/300 [01:21<01:31,  1.63it/s]Epoch 27:  51%|█████     | 153/300 [01:21<01:25,  1.72it/s]Epoch 27:  51%|█████▏    | 154/300 [01:22<01:26,  1.69it/s]Epoch 27:  52%|█████▏    | 155/300 [01:22<01:24,  1.71it/s]Epoch 27:  52%|█████▏    | 156/300 [01:23<01:27,  1.65it/s]Epoch 27:  52%|█████▏    | 157/300 [01:24<01:26,  1.65it/s]Epoch 27:  53%|█████▎    | 158/300 [01:24<01:25,  1.66it/s]Epoch 27:  53%|█████▎    | 159/300 [01:25<01:24,  1.67it/s]06/19/2022 20:59:36 - INFO - __main__ - global step: 4130; train loss: 7.837926387786865; dev loss: 7.792714595794678
Epoch 27:  53%|█████▎    | 160/300 [01:26<01:27,  1.59it/s]Epoch 27:  54%|█████▎    | 161/300 [01:26<01:24,  1.64it/s]Epoch 27:  54%|█████▍    | 162/300 [01:27<01:23,  1.65it/s]Epoch 27:  54%|█████▍    | 163/300 [01:27<01:22,  1.65it/s]Epoch 27:  55%|█████▍    | 164/300 [01:28<01:24,  1.60it/s]Epoch 27:  55%|█████▌    | 165/300 [01:28<01:18,  1.73it/s]Epoch 27:  55%|█████▌    | 166/300 [01:29<01:13,  1.82it/s]Epoch 27:  56%|█████▌    | 167/300 [01:29<01:10,  1.90it/s]Epoch 27:  56%|█████▌    | 168/300 [01:30<01:11,  1.84it/s]Epoch 27:  56%|█████▋    | 169/300 [01:30<01:08,  1.92it/s]Epoch 27:  57%|█████▋    | 170/300 [01:31<01:06,  1.97it/s]Epoch 27:  57%|█████▋    | 171/300 [01:31<01:04,  2.00it/s]Epoch 27:  57%|█████▋    | 172/300 [01:32<01:08,  1.87it/s]Epoch 27:  58%|█████▊    | 173/300 [01:33<01:05,  1.93it/s]Epoch 27:  58%|█████▊    | 174/300 [01:33<01:05,  1.93it/s]Epoch 27:  58%|█████▊    | 175/300 [01:34<01:03,  1.97it/s]Epoch 27:  59%|█████▊    | 176/300 [01:34<01:07,  1.85it/s]Epoch 27:  59%|█████▉    | 177/300 [01:35<01:04,  1.91it/s]Epoch 27:  59%|█████▉    | 178/300 [01:35<01:02,  1.96it/s]Epoch 27:  60%|█████▉    | 179/300 [01:36<01:00,  1.98it/s]06/19/2022 20:59:46 - INFO - __main__ - global step: 4140; train loss: 6.853427886962891; dev loss: 7.071011543273926
Epoch 27:  60%|██████    | 180/300 [01:36<01:01,  1.97it/s]Epoch 27:  60%|██████    | 181/300 [01:37<01:04,  1.85it/s]Epoch 27:  61%|██████    | 182/300 [01:37<01:02,  1.90it/s]Epoch 27:  61%|██████    | 183/300 [01:38<01:00,  1.93it/s]Epoch 27:  61%|██████▏   | 184/300 [01:38<00:58,  1.97it/s]Epoch 27:  62%|██████▏   | 185/300 [01:39<01:00,  1.90it/s]Epoch 27:  62%|██████▏   | 186/300 [01:39<00:59,  1.92it/s]Epoch 27:  62%|██████▏   | 187/300 [01:40<00:57,  1.96it/s]Epoch 27:  63%|██████▎   | 188/300 [01:40<00:56,  1.99it/s]Epoch 27:  63%|██████▎   | 189/300 [01:41<00:59,  1.87it/s]Epoch 27:  63%|██████▎   | 190/300 [01:41<00:56,  1.93it/s]Epoch 27:  64%|██████▎   | 191/300 [01:42<00:55,  1.97it/s]Epoch 27:  64%|██████▍   | 192/300 [01:42<00:55,  1.95it/s]Epoch 27:  64%|██████▍   | 193/300 [01:43<00:56,  1.88it/s]Epoch 27:  65%|██████▍   | 194/300 [01:43<00:54,  1.94it/s]Epoch 27:  65%|██████▌   | 195/300 [01:44<00:53,  1.97it/s]Epoch 27:  65%|██████▌   | 196/300 [01:44<00:53,  1.95it/s]Epoch 27:  66%|██████▌   | 197/300 [01:45<00:55,  1.85it/s]Epoch 27:  66%|██████▌   | 198/300 [01:46<00:53,  1.91it/s]Epoch 27:  66%|██████▋   | 199/300 [01:46<00:51,  1.95it/s]06/19/2022 20:59:57 - INFO - __main__ - global step: 4150; train loss: 7.100090026855469; dev loss: 7.273167610168457
Epoch 27:  67%|██████▋   | 200/300 [01:46<00:50,  1.99it/s]Epoch 27:  67%|██████▋   | 201/300 [01:47<00:52,  1.90it/s]Epoch 27:  67%|██████▋   | 202/300 [01:48<00:50,  1.96it/s]Epoch 27:  68%|██████▊   | 203/300 [01:48<00:48,  2.00it/s]Epoch 27:  68%|██████▊   | 204/300 [01:49<00:48,  1.96it/s]Epoch 27:  68%|██████▊   | 205/300 [01:49<00:48,  1.98it/s]Epoch 27:  69%|██████▊   | 206/300 [01:50<00:49,  1.91it/s]Epoch 27:  69%|██████▉   | 207/300 [01:50<00:48,  1.91it/s]Epoch 27:  69%|██████▉   | 208/300 [01:51<00:46,  1.96it/s]Epoch 27:  70%|██████▉   | 209/300 [01:51<00:45,  1.99it/s]Epoch 27:  70%|███████   | 210/300 [01:52<00:48,  1.87it/s]Epoch 27:  70%|███████   | 211/300 [01:52<00:46,  1.93it/s]Epoch 27:  71%|███████   | 212/300 [01:53<00:44,  1.98it/s]Epoch 27:  71%|███████   | 213/300 [01:53<00:43,  2.01it/s]Epoch 27:  71%|███████▏  | 214/300 [01:54<00:44,  1.92it/s]Epoch 27:  72%|███████▏  | 215/300 [01:54<00:43,  1.97it/s]Epoch 27:  72%|███████▏  | 216/300 [01:55<00:41,  2.01it/s]Epoch 27:  72%|███████▏  | 217/300 [01:55<00:40,  2.03it/s]Epoch 27:  73%|███████▎  | 218/300 [01:56<00:42,  1.94it/s]Epoch 27:  73%|███████▎  | 219/300 [01:56<00:41,  1.97it/s]06/19/2022 21:00:07 - INFO - __main__ - global step: 4160; train loss: 7.41876745223999; dev loss: 7.4239325523376465
Epoch 27:  73%|███████▎  | 220/300 [01:57<00:40,  1.99it/s]Epoch 27:  74%|███████▎  | 221/300 [01:57<00:39,  2.02it/s]Epoch 27:  74%|███████▍  | 222/300 [01:58<00:41,  1.87it/s]Epoch 27:  74%|███████▍  | 223/300 [01:58<00:39,  1.93it/s]Epoch 27:  75%|███████▍  | 224/300 [01:59<00:38,  1.96it/s]Epoch 27:  75%|███████▌  | 225/300 [01:59<00:37,  1.98it/s]Epoch 27:  75%|███████▌  | 226/300 [02:00<00:38,  1.91it/s]Epoch 27:  76%|███████▌  | 227/300 [02:00<00:37,  1.96it/s]Epoch 27:  76%|███████▌  | 228/300 [02:01<00:36,  1.96it/s]Epoch 27:  76%|███████▋  | 229/300 [02:01<00:35,  2.00it/s]Epoch 27:  77%|███████▋  | 230/300 [02:02<00:37,  1.85it/s]Epoch 27:  77%|███████▋  | 231/300 [02:02<00:36,  1.87it/s]Epoch 27:  77%|███████▋  | 232/300 [02:03<00:37,  1.83it/s]Epoch 27:  78%|███████▊  | 233/300 [02:04<00:36,  1.82it/s]Epoch 27:  78%|███████▊  | 234/300 [02:04<00:36,  1.81it/s]Epoch 27:  78%|███████▊  | 235/300 [02:05<00:36,  1.79it/s]Epoch 27:  79%|███████▊  | 236/300 [02:05<00:34,  1.88it/s]Epoch 27:  79%|███████▉  | 237/300 [02:06<00:32,  1.94it/s]Epoch 27:  79%|███████▉  | 238/300 [02:06<00:31,  1.97it/s]Epoch 27:  80%|███████▉  | 239/300 [02:07<00:32,  1.89it/s]06/19/2022 21:00:17 - INFO - __main__ - global step: 4170; train loss: 7.2406768798828125; dev loss: 7.607143402099609
Epoch 27:  80%|████████  | 240/300 [02:07<00:30,  1.95it/s]Epoch 27:  80%|████████  | 241/300 [02:08<00:30,  1.95it/s]Epoch 27:  81%|████████  | 242/300 [02:08<00:30,  1.90it/s]Epoch 27:  81%|████████  | 243/300 [02:09<00:30,  1.86it/s]Epoch 27:  81%|████████▏ | 244/300 [02:09<00:30,  1.84it/s]Epoch 27:  82%|████████▏ | 245/300 [02:10<00:28,  1.92it/s]Epoch 27:  82%|████████▏ | 246/300 [02:10<00:28,  1.88it/s]Epoch 27:  82%|████████▏ | 247/300 [02:11<00:29,  1.81it/s]Epoch 27:  83%|████████▎ | 248/300 [02:12<00:28,  1.80it/s]Epoch 27:  83%|████████▎ | 249/300 [02:12<00:27,  1.87it/s]Epoch 27:  83%|████████▎ | 250/300 [02:13<00:25,  1.93it/s]Epoch 27:  84%|████████▎ | 251/300 [02:13<00:26,  1.87it/s]Epoch 27:  84%|████████▍ | 252/300 [02:14<00:24,  1.93it/s]Epoch 27:  84%|████████▍ | 253/300 [02:14<00:23,  1.98it/s]Epoch 27:  85%|████████▍ | 254/300 [02:15<00:22,  2.01it/s]Epoch 27:  85%|████████▌ | 255/300 [02:15<00:23,  1.89it/s]Epoch 27:  85%|████████▌ | 256/300 [02:16<00:22,  1.95it/s]Epoch 27:  86%|████████▌ | 257/300 [02:16<00:21,  1.99it/s]Epoch 27:  86%|████████▌ | 258/300 [02:17<00:21,  1.96it/s]Epoch 27:  86%|████████▋ | 259/300 [02:17<00:20,  2.01it/s]06/19/2022 21:00:28 - INFO - __main__ - global step: 4180; train loss: 6.913249969482422; dev loss: 6.848597526550293
Epoch 27:  87%|████████▋ | 260/300 [02:18<00:21,  1.87it/s]Epoch 27:  87%|████████▋ | 261/300 [02:18<00:20,  1.93it/s]Epoch 27:  87%|████████▋ | 262/300 [02:19<00:19,  1.97it/s]Epoch 27:  88%|████████▊ | 263/300 [02:19<00:18,  2.00it/s]Epoch 27:  88%|████████▊ | 264/300 [02:20<00:18,  1.94it/s]Epoch 27:  88%|████████▊ | 265/300 [02:20<00:17,  1.99it/s]Epoch 27:  89%|████████▊ | 266/300 [02:21<00:16,  2.01it/s]Epoch 27:  89%|████████▉ | 267/300 [02:21<00:16,  1.98it/s]Epoch 27:  89%|████████▉ | 268/300 [02:22<00:17,  1.82it/s]Epoch 27:  90%|████████▉ | 269/300 [02:22<00:16,  1.90it/s]Epoch 27:  90%|█████████ | 270/300 [02:23<00:15,  1.95it/s]Epoch 27:  90%|█████████ | 271/300 [02:23<00:14,  2.00it/s]Epoch 27:  91%|█████████ | 272/300 [02:24<00:14,  1.90it/s]Epoch 27:  91%|█████████ | 273/300 [02:24<00:13,  1.95it/s]Epoch 27:  91%|█████████▏| 274/300 [02:25<00:13,  1.98it/s]Epoch 27:  92%|█████████▏| 275/300 [02:25<00:13,  1.92it/s]Epoch 27:  92%|█████████▏| 276/300 [02:26<00:12,  1.85it/s]Epoch 27:  92%|█████████▏| 277/300 [02:26<00:11,  1.92it/s]Epoch 27:  93%|█████████▎| 278/300 [02:27<00:11,  1.88it/s]Epoch 27:  93%|█████████▎| 279/300 [02:27<00:10,  1.94it/s]06/19/2022 21:00:38 - INFO - __main__ - global step: 4190; train loss: 7.024089813232422; dev loss: 7.2656378746032715
Epoch 27:  93%|█████████▎| 280/300 [02:28<00:10,  1.88it/s]Epoch 27:  94%|█████████▎| 281/300 [02:29<00:09,  1.93it/s]Epoch 27:  94%|█████████▍| 282/300 [02:29<00:09,  1.98it/s]Epoch 27:  94%|█████████▍| 283/300 [02:29<00:08,  2.01it/s]Epoch 27:  95%|█████████▍| 284/300 [02:30<00:08,  1.92it/s]Epoch 27:  95%|█████████▌| 285/300 [02:31<00:07,  1.89it/s]Epoch 27:  95%|█████████▌| 286/300 [02:31<00:07,  1.95it/s]Epoch 27:  96%|█████████▌| 287/300 [02:32<00:06,  1.95it/s]Epoch 27:  96%|█████████▌| 288/300 [02:32<00:06,  1.91it/s]Epoch 27:  96%|█████████▋| 289/300 [02:33<00:05,  1.86it/s]Epoch 27:  97%|█████████▋| 290/300 [02:33<00:05,  1.89it/s]Epoch 27:  97%|█████████▋| 291/300 [02:34<00:04,  1.95it/s]Epoch 27:  97%|█████████▋| 292/300 [02:34<00:04,  1.95it/s]Epoch 27:  98%|█████████▊| 293/300 [02:35<00:03,  1.81it/s]Epoch 27:  98%|█████████▊| 294/300 [02:35<00:03,  1.89it/s]Epoch 27:  98%|█████████▊| 295/300 [02:36<00:02,  1.96it/s]Epoch 27:  99%|█████████▊| 296/300 [02:36<00:01,  2.00it/s]Epoch 27:  99%|█████████▉| 297/300 [02:37<00:01,  1.93it/s]Epoch 27:  99%|█████████▉| 298/300 [02:37<00:01,  1.89it/s]Epoch 27: 100%|█████████▉| 299/300 [02:38<00:00,  1.95it/s]06/19/2022 21:00:48 - INFO - __main__ - global step: 4200; train loss: 7.125664710998535; dev loss: 7.380985260009766
Epoch 27: 100%|██████████| 300/300 [02:38<00:00,  1.90it/s]Epoch 27: 100%|██████████| 300/300 [02:38<00:00,  1.89it/s]
Epoch 28:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 28:   0%|          | 1/300 [00:00<02:49,  1.77it/s]Epoch 28:   1%|          | 2/300 [00:01<02:32,  1.96it/s]Epoch 28:   1%|          | 3/300 [00:01<02:38,  1.87it/s]Epoch 28:   1%|▏         | 4/300 [00:02<02:41,  1.83it/s]Epoch 28:   2%|▏         | 5/300 [00:02<02:43,  1.80it/s]Epoch 28:   2%|▏         | 6/300 [00:03<02:36,  1.88it/s]Epoch 28:   2%|▏         | 7/300 [00:03<02:30,  1.95it/s]Epoch 28:   3%|▎         | 8/300 [00:04<02:25,  2.01it/s]Epoch 28:   3%|▎         | 9/300 [00:04<02:31,  1.92it/s]Epoch 28:   3%|▎         | 10/300 [00:05<02:27,  1.96it/s]Epoch 28:   4%|▎         | 11/300 [00:05<02:24,  2.01it/s]Epoch 28:   4%|▍         | 12/300 [00:06<02:24,  1.99it/s]Epoch 28:   4%|▍         | 13/300 [00:06<02:22,  2.02it/s]Epoch 28:   5%|▍         | 14/300 [00:07<02:29,  1.92it/s]Epoch 28:   5%|▌         | 15/300 [00:07<02:25,  1.96it/s]Epoch 28:   5%|▌         | 16/300 [00:08<02:21,  2.00it/s]Epoch 28:   6%|▌         | 17/300 [00:08<02:19,  2.02it/s]Epoch 28:   6%|▌         | 18/300 [00:09<02:25,  1.94it/s]Epoch 28:   6%|▋         | 19/300 [00:09<02:20,  2.00it/s]06/19/2022 21:00:59 - INFO - __main__ - global step: 4210; train loss: 6.871870994567871; dev loss: 7.346548557281494
Epoch 28:   7%|▋         | 20/300 [00:10<02:25,  1.92it/s]Epoch 28:   7%|▋         | 21/300 [00:10<02:21,  1.97it/s]Epoch 28:   7%|▋         | 22/300 [00:11<02:26,  1.89it/s]Epoch 28:   8%|▊         | 23/300 [00:11<02:28,  1.87it/s]Epoch 28:   8%|▊         | 24/300 [00:12<02:22,  1.94it/s]Epoch 28:   8%|▊         | 25/300 [00:12<02:25,  1.89it/s]Epoch 28:   9%|▊         | 26/300 [00:13<02:28,  1.84it/s]Epoch 28:   9%|▉         | 27/300 [00:13<02:22,  1.91it/s]Epoch 28:   9%|▉         | 28/300 [00:14<02:18,  1.96it/s]Epoch 28:  10%|▉         | 29/300 [00:14<02:15,  2.01it/s]Epoch 28:  10%|█         | 30/300 [00:15<02:20,  1.93it/s]Epoch 28:  10%|█         | 31/300 [00:16<02:19,  1.93it/s]Epoch 28:  11%|█         | 32/300 [00:16<02:16,  1.97it/s]Epoch 28:  11%|█         | 33/300 [00:17<02:19,  1.92it/s]Epoch 28:  11%|█▏        | 34/300 [00:17<02:21,  1.88it/s]Epoch 28:  12%|█▏        | 35/300 [00:18<02:16,  1.95it/s]Epoch 28:  12%|█▏        | 36/300 [00:18<02:12,  1.99it/s]Epoch 28:  12%|█▏        | 37/300 [00:19<02:09,  2.03it/s]Epoch 28:  13%|█▎        | 38/300 [00:19<02:21,  1.86it/s]Epoch 28:  13%|█▎        | 39/300 [00:20<02:18,  1.88it/s]06/19/2022 21:01:09 - INFO - __main__ - global step: 4220; train loss: 7.051120758056641; dev loss: 7.299130916595459
Epoch 28:  13%|█▎        | 40/300 [00:20<02:19,  1.86it/s]Epoch 28:  14%|█▎        | 41/300 [00:21<02:20,  1.85it/s]Epoch 28:  14%|█▍        | 42/300 [00:21<02:14,  1.91it/s]Epoch 28:  14%|█▍        | 43/300 [00:22<02:18,  1.86it/s]Epoch 28:  15%|█▍        | 44/300 [00:22<02:13,  1.92it/s]Epoch 28:  15%|█▌        | 45/300 [00:23<02:09,  1.97it/s]Epoch 28:  15%|█▌        | 46/300 [00:23<02:06,  2.01it/s]Epoch 28:  16%|█▌        | 47/300 [00:24<02:11,  1.92it/s]Epoch 28:  16%|█▌        | 48/300 [00:24<02:07,  1.98it/s]Epoch 28:  16%|█▋        | 49/300 [00:25<02:05,  2.00it/s]Epoch 28:  17%|█▋        | 50/300 [00:25<02:03,  2.03it/s]Epoch 28:  17%|█▋        | 51/300 [00:26<02:09,  1.92it/s]Epoch 28:  17%|█▋        | 52/300 [00:26<02:06,  1.96it/s]Epoch 28:  18%|█▊        | 53/300 [00:27<02:03,  2.00it/s]Epoch 28:  18%|█▊        | 54/300 [00:27<02:01,  2.02it/s]Epoch 28:  18%|█▊        | 55/300 [00:28<02:13,  1.83it/s]Epoch 28:  19%|█▊        | 56/300 [00:28<02:07,  1.92it/s]Epoch 28:  19%|█▉        | 57/300 [00:29<02:04,  1.96it/s]Epoch 28:  19%|█▉        | 58/300 [00:29<02:01,  2.00it/s]Epoch 28:  20%|█▉        | 59/300 [00:30<02:05,  1.93it/s]06/19/2022 21:01:19 - INFO - __main__ - global step: 4230; train loss: 7.4770097732543945; dev loss: 7.448455810546875
Epoch 28:  20%|██        | 60/300 [00:30<02:04,  1.93it/s]Epoch 28:  20%|██        | 61/300 [00:31<02:00,  1.99it/s]Epoch 28:  21%|██        | 62/300 [00:31<02:00,  1.97it/s]Epoch 28:  21%|██        | 63/300 [00:32<02:04,  1.90it/s]Epoch 28:  21%|██▏       | 64/300 [00:33<02:00,  1.96it/s]Epoch 28:  22%|██▏       | 65/300 [00:33<02:02,  1.91it/s]Epoch 28:  22%|██▏       | 66/300 [00:34<01:59,  1.96it/s]Epoch 28:  22%|██▏       | 67/300 [00:34<02:01,  1.91it/s]Epoch 28:  23%|██▎       | 68/300 [00:35<02:07,  1.83it/s]Epoch 28:  23%|██▎       | 69/300 [00:35<02:01,  1.90it/s]Epoch 28:  23%|██▎       | 70/300 [00:36<01:57,  1.95it/s]Epoch 28:  24%|██▎       | 71/300 [00:36<01:54,  1.99it/s]Epoch 28:  24%|██▍       | 72/300 [00:37<02:01,  1.88it/s]Epoch 28:  24%|██▍       | 73/300 [00:37<01:56,  1.94it/s]Epoch 28:  25%|██▍       | 74/300 [00:38<01:53,  1.98it/s]Epoch 28:  25%|██▌       | 75/300 [00:38<01:51,  2.03it/s]Epoch 28:  25%|██▌       | 76/300 [00:39<01:56,  1.92it/s]Epoch 28:  26%|██▌       | 77/300 [00:39<01:58,  1.88it/s]Epoch 28:  26%|██▌       | 78/300 [00:40<01:54,  1.94it/s]Epoch 28:  26%|██▋       | 79/300 [00:40<01:53,  1.94it/s]06/19/2022 21:01:30 - INFO - __main__ - global step: 4240; train loss: 7.408830165863037; dev loss: 7.488572597503662
Epoch 28:  27%|██▋       | 80/300 [00:41<01:57,  1.88it/s]Epoch 28:  27%|██▋       | 81/300 [00:41<01:52,  1.94it/s]Epoch 28:  27%|██▋       | 82/300 [00:42<01:49,  1.99it/s]Epoch 28:  28%|██▊       | 83/300 [00:42<01:47,  2.03it/s]Epoch 28:  28%|██▊       | 84/300 [00:43<01:51,  1.94it/s]Epoch 28:  28%|██▊       | 85/300 [00:43<01:47,  1.99it/s]Epoch 28:  29%|██▊       | 86/300 [00:44<01:45,  2.02it/s]Epoch 28:  29%|██▉       | 87/300 [00:44<01:44,  2.04it/s]Epoch 28:  29%|██▉       | 88/300 [00:45<01:48,  1.95it/s]Epoch 28:  30%|██▉       | 89/300 [00:45<01:46,  1.99it/s]Epoch 28:  30%|███       | 90/300 [00:46<01:44,  2.01it/s]Epoch 28:  30%|███       | 91/300 [00:46<01:42,  2.04it/s]Epoch 28:  31%|███       | 92/300 [00:47<01:48,  1.91it/s]Epoch 28:  31%|███       | 93/300 [00:47<01:45,  1.96it/s]Epoch 28:  31%|███▏      | 94/300 [00:48<01:47,  1.92it/s]Epoch 28:  32%|███▏      | 95/300 [00:48<01:44,  1.97it/s]Epoch 28:  32%|███▏      | 96/300 [00:49<01:42,  2.00it/s]Epoch 28:  32%|███▏      | 97/300 [00:49<01:48,  1.87it/s]Epoch 28:  33%|███▎      | 98/300 [00:50<01:44,  1.93it/s]Epoch 28:  33%|███▎      | 99/300 [00:50<01:41,  1.97it/s]06/19/2022 21:01:40 - INFO - __main__ - global step: 4250; train loss: 7.645543575286865; dev loss: 7.449269771575928
Epoch 28:  33%|███▎      | 100/300 [00:51<01:40,  2.00it/s]Epoch 28:  34%|███▎      | 101/300 [00:52<01:43,  1.93it/s]Epoch 28:  34%|███▍      | 102/300 [00:52<01:39,  1.98it/s]Epoch 28:  34%|███▍      | 103/300 [00:52<01:39,  1.98it/s]Epoch 28:  35%|███▍      | 104/300 [00:53<01:37,  2.01it/s]Epoch 28:  35%|███▌      | 105/300 [00:54<01:45,  1.85it/s]Epoch 28:  35%|███▌      | 106/300 [00:54<01:40,  1.92it/s]Epoch 28:  36%|███▌      | 107/300 [00:55<01:37,  1.98it/s]Epoch 28:  36%|███▌      | 108/300 [00:55<01:36,  1.98it/s]Epoch 28:  36%|███▋      | 109/300 [00:56<01:39,  1.91it/s]Epoch 28:  37%|███▋      | 110/300 [00:56<01:41,  1.88it/s]Epoch 28:  37%|███▋      | 111/300 [00:57<01:37,  1.94it/s]Epoch 28:  37%|███▋      | 112/300 [00:57<01:34,  1.99it/s]Epoch 28:  38%|███▊      | 113/300 [00:58<01:37,  1.91it/s]Epoch 28:  38%|███▊      | 114/300 [00:58<01:39,  1.87it/s]Epoch 28:  38%|███▊      | 115/300 [00:59<01:35,  1.94it/s]Epoch 28:  39%|███▊      | 116/300 [00:59<01:32,  2.00it/s]Epoch 28:  39%|███▉      | 117/300 [01:00<01:35,  1.92it/s]Epoch 28:  39%|███▉      | 118/300 [01:00<01:31,  1.98it/s]Epoch 28:  40%|███▉      | 119/300 [01:01<01:32,  1.96it/s]06/19/2022 21:01:50 - INFO - __main__ - global step: 4260; train loss: 7.215002536773682; dev loss: 7.19369649887085
Epoch 28:  40%|████      | 120/300 [01:01<01:32,  1.95it/s]Epoch 28:  40%|████      | 121/300 [01:02<01:29,  2.00it/s]Epoch 28:  41%|████      | 122/300 [01:02<01:32,  1.92it/s]Epoch 28:  41%|████      | 123/300 [01:03<01:31,  1.94it/s]Epoch 28:  41%|████▏     | 124/300 [01:03<01:31,  1.92it/s]Epoch 28:  42%|████▏     | 125/300 [01:04<01:33,  1.88it/s]Epoch 28:  42%|████▏     | 126/300 [01:05<01:36,  1.80it/s]Epoch 28:  42%|████▏     | 127/300 [01:05<01:31,  1.89it/s]Epoch 28:  43%|████▎     | 128/300 [01:05<01:30,  1.90it/s]Epoch 28:  43%|████▎     | 129/300 [01:06<01:27,  1.96it/s]Epoch 28:  43%|████▎     | 130/300 [01:07<01:29,  1.90it/s]Epoch 28:  44%|████▎     | 131/300 [01:07<01:27,  1.94it/s]Epoch 28:  44%|████▍     | 132/300 [01:08<01:24,  1.98it/s]Epoch 28:  44%|████▍     | 133/300 [01:08<01:24,  1.98it/s]Epoch 28:  45%|████▍     | 134/300 [01:09<01:26,  1.92it/s]Epoch 28:  45%|████▌     | 135/300 [01:09<01:25,  1.93it/s]Epoch 28:  45%|████▌     | 136/300 [01:10<01:22,  1.98it/s]Epoch 28:  46%|████▌     | 137/300 [01:10<01:20,  2.02it/s]Epoch 28:  46%|████▌     | 138/300 [01:11<01:23,  1.95it/s]Epoch 28:  46%|████▋     | 139/300 [01:11<01:24,  1.90it/s]06/19/2022 21:02:01 - INFO - __main__ - global step: 4270; train loss: 7.042675971984863; dev loss: 7.029655456542969
Epoch 28:  47%|████▋     | 140/300 [01:12<01:23,  1.91it/s]Epoch 28:  47%|████▋     | 141/300 [01:12<01:20,  1.98it/s]Epoch 28:  47%|████▋     | 142/300 [01:13<01:24,  1.87it/s]Epoch 28:  48%|████▊     | 143/300 [01:13<01:21,  1.92it/s]Epoch 28:  48%|████▊     | 144/300 [01:14<01:22,  1.88it/s]Epoch 28:  48%|████▊     | 145/300 [01:14<01:23,  1.85it/s]Epoch 28:  49%|████▊     | 146/300 [01:15<01:24,  1.83it/s]Epoch 28:  49%|████▉     | 147/300 [01:15<01:23,  1.83it/s]Epoch 28:  49%|████▉     | 148/300 [01:16<01:23,  1.82it/s]Epoch 28:  50%|████▉     | 149/300 [01:17<01:23,  1.81it/s]Epoch 28:  50%|█████     | 150/300 [01:17<01:20,  1.87it/s]Epoch 28:  50%|█████     | 151/300 [01:18<01:22,  1.80it/s]Epoch 28:  51%|█████     | 152/300 [01:18<01:19,  1.87it/s]Epoch 28:  51%|█████     | 153/300 [01:19<01:15,  1.94it/s]Epoch 28:  51%|█████▏    | 154/300 [01:19<01:13,  1.99it/s]Epoch 28:  52%|█████▏    | 155/300 [01:20<01:15,  1.91it/s]Epoch 28:  52%|█████▏    | 156/300 [01:20<01:15,  1.91it/s]Epoch 28:  52%|█████▏    | 157/300 [01:21<01:14,  1.92it/s]Epoch 28:  53%|█████▎    | 158/300 [01:21<01:16,  1.87it/s]Epoch 28:  53%|█████▎    | 159/300 [01:22<01:18,  1.81it/s]06/19/2022 21:02:11 - INFO - __main__ - global step: 4280; train loss: 7.905524253845215; dev loss: 7.598745822906494
Epoch 28:  53%|█████▎    | 160/300 [01:22<01:17,  1.81it/s]Epoch 28:  54%|█████▎    | 161/300 [01:23<01:15,  1.84it/s]Epoch 28:  54%|█████▍    | 162/300 [01:23<01:12,  1.91it/s]Epoch 28:  54%|█████▍    | 163/300 [01:24<01:16,  1.79it/s]Epoch 28:  55%|█████▍    | 164/300 [01:25<01:12,  1.87it/s]Epoch 28:  55%|█████▌    | 165/300 [01:25<01:09,  1.94it/s]Epoch 28:  55%|█████▌    | 166/300 [01:25<01:07,  1.99it/s]Epoch 28:  56%|█████▌    | 167/300 [01:26<01:09,  1.91it/s]Epoch 28:  56%|█████▌    | 168/300 [01:27<01:07,  1.97it/s]Epoch 28:  56%|█████▋    | 169/300 [01:27<01:05,  1.98it/s]Epoch 28:  57%|█████▋    | 170/300 [01:28<01:07,  1.92it/s]Epoch 28:  57%|█████▋    | 171/300 [01:28<01:08,  1.87it/s]Epoch 28:  57%|█████▋    | 172/300 [01:29<01:06,  1.94it/s]Epoch 28:  58%|█████▊    | 173/300 [01:29<01:04,  1.98it/s]Epoch 28:  58%|█████▊    | 174/300 [01:30<01:02,  2.02it/s]Epoch 28:  58%|█████▊    | 175/300 [01:30<01:01,  2.04it/s]Epoch 28:  59%|█████▊    | 176/300 [01:31<01:05,  1.90it/s]Epoch 28:  59%|█████▉    | 177/300 [01:31<01:05,  1.87it/s]Epoch 28:  59%|█████▉    | 178/300 [01:32<01:04,  1.89it/s]Epoch 28:  60%|█████▉    | 179/300 [01:32<01:01,  1.96it/s]06/19/2022 21:02:22 - INFO - __main__ - global step: 4290; train loss: 7.159597873687744; dev loss: 6.9496941566467285
Epoch 28:  60%|██████    | 180/300 [01:33<01:04,  1.86it/s]Epoch 28:  60%|██████    | 181/300 [01:33<01:01,  1.94it/s]Epoch 28:  61%|██████    | 182/300 [01:34<01:01,  1.93it/s]Epoch 28:  61%|██████    | 183/300 [01:34<00:59,  1.98it/s]Epoch 28:  61%|██████▏   | 184/300 [01:35<01:00,  1.91it/s]Epoch 28:  62%|██████▏   | 185/300 [01:35<00:58,  1.96it/s]Epoch 28:  62%|██████▏   | 186/300 [01:36<00:57,  1.99it/s]Epoch 28:  62%|██████▏   | 187/300 [01:36<00:57,  1.97it/s]Epoch 28:  63%|██████▎   | 188/300 [01:37<00:58,  1.90it/s]Epoch 28:  63%|██████▎   | 189/300 [01:37<00:56,  1.95it/s]Epoch 28:  63%|██████▎   | 190/300 [01:38<00:57,  1.90it/s]Epoch 28:  64%|██████▎   | 191/300 [01:38<00:58,  1.87it/s]Epoch 28:  64%|██████▍   | 192/300 [01:39<01:00,  1.80it/s]Epoch 28:  64%|██████▍   | 193/300 [01:40<00:56,  1.89it/s]Epoch 28:  65%|██████▍   | 194/300 [01:40<00:54,  1.94it/s]Epoch 28:  65%|██████▌   | 195/300 [01:40<00:52,  1.99it/s]Epoch 28:  65%|██████▌   | 196/300 [01:41<00:54,  1.91it/s]Epoch 28:  66%|██████▌   | 197/300 [01:42<00:52,  1.97it/s]Epoch 28:  66%|██████▌   | 198/300 [01:42<00:52,  1.94it/s]Epoch 28:  66%|██████▋   | 199/300 [01:43<00:50,  1.99it/s]06/19/2022 21:02:32 - INFO - __main__ - global step: 4300; train loss: 7.198936462402344; dev loss: 7.286320686340332
Epoch 28:  67%|██████▋   | 200/300 [01:43<00:54,  1.83it/s]Epoch 28:  67%|██████▋   | 201/300 [01:44<00:51,  1.91it/s]Epoch 28:  67%|██████▋   | 202/300 [01:44<00:52,  1.87it/s]Epoch 28:  68%|██████▊   | 203/300 [01:45<00:50,  1.92it/s]Epoch 28:  68%|██████▊   | 204/300 [01:45<00:48,  1.98it/s]Epoch 28:  68%|██████▊   | 205/300 [01:46<00:51,  1.83it/s]Epoch 28:  69%|██████▊   | 206/300 [01:46<00:49,  1.90it/s]Epoch 28:  69%|██████▉   | 207/300 [01:47<00:48,  1.91it/s]Epoch 28:  69%|██████▉   | 208/300 [01:47<00:49,  1.88it/s]Epoch 28:  70%|██████▉   | 209/300 [01:48<00:49,  1.84it/s]Epoch 28:  70%|███████   | 210/300 [01:48<00:46,  1.92it/s]Epoch 28:  70%|███████   | 211/300 [01:49<00:44,  1.98it/s]Epoch 28:  71%|███████   | 212/300 [01:49<00:43,  2.01it/s]Epoch 28:  71%|███████   | 213/300 [01:50<00:45,  1.93it/s]Epoch 28:  71%|███████▏  | 214/300 [01:50<00:43,  1.98it/s]Epoch 28:  72%|███████▏  | 215/300 [01:51<00:43,  1.97it/s]Epoch 28:  72%|███████▏  | 216/300 [01:51<00:42,  1.96it/s]Epoch 28:  72%|███████▏  | 217/300 [01:52<00:43,  1.90it/s]Epoch 28:  73%|███████▎  | 218/300 [01:52<00:41,  1.96it/s]Epoch 28:  73%|███████▎  | 219/300 [01:53<00:40,  2.00it/s]06/19/2022 21:02:42 - INFO - __main__ - global step: 4310; train loss: 7.303380489349365; dev loss: 7.195634365081787
Epoch 28:  73%|███████▎  | 220/300 [01:53<00:41,  1.93it/s]Epoch 28:  74%|███████▎  | 221/300 [01:54<00:44,  1.79it/s]Epoch 28:  74%|███████▍  | 222/300 [01:55<00:41,  1.87it/s]Epoch 28:  74%|███████▍  | 223/300 [01:55<00:39,  1.93it/s]Epoch 28:  75%|███████▍  | 224/300 [01:56<00:38,  1.98it/s]Epoch 28:  75%|███████▌  | 225/300 [01:56<00:39,  1.91it/s]Epoch 28:  75%|███████▌  | 226/300 [01:57<00:37,  1.96it/s]Epoch 28:  76%|███████▌  | 227/300 [01:57<00:38,  1.90it/s]Epoch 28:  76%|███████▌  | 228/300 [01:58<00:36,  1.96it/s]Epoch 28:  76%|███████▋  | 229/300 [01:58<00:35,  2.00it/s]Epoch 28:  77%|███████▋  | 230/300 [01:59<00:36,  1.91it/s]Epoch 28:  77%|███████▋  | 231/300 [01:59<00:35,  1.96it/s]Epoch 28:  77%|███████▋  | 232/300 [02:00<00:34,  1.99it/s]Epoch 28:  78%|███████▊  | 233/300 [02:00<00:32,  2.03it/s]Epoch 28:  78%|███████▊  | 234/300 [02:01<00:33,  1.95it/s]Epoch 28:  78%|███████▊  | 235/300 [02:01<00:32,  2.00it/s]Epoch 28:  79%|███████▊  | 236/300 [02:02<00:32,  1.94it/s]Epoch 28:  79%|███████▉  | 237/300 [02:02<00:31,  1.99it/s]Epoch 28:  79%|███████▉  | 238/300 [02:03<00:32,  1.91it/s]Epoch 28:  80%|███████▉  | 239/300 [02:03<00:31,  1.92it/s]06/19/2022 21:02:53 - INFO - __main__ - global step: 4320; train loss: 7.549125671386719; dev loss: 7.298920631408691
Epoch 28:  80%|████████  | 240/300 [02:04<00:30,  1.97it/s]Epoch 28:  80%|████████  | 241/300 [02:04<00:29,  2.02it/s]Epoch 28:  81%|████████  | 242/300 [02:05<00:30,  1.87it/s]Epoch 28:  81%|████████  | 243/300 [02:05<00:29,  1.93it/s]Epoch 28:  81%|████████▏ | 244/300 [02:06<00:29,  1.89it/s]Epoch 28:  82%|████████▏ | 245/300 [02:06<00:28,  1.91it/s]Epoch 28:  82%|████████▏ | 246/300 [02:07<00:29,  1.86it/s]Epoch 28:  82%|████████▏ | 247/300 [02:07<00:27,  1.92it/s]Epoch 28:  83%|████████▎ | 248/300 [02:08<00:26,  1.97it/s]Epoch 28:  83%|████████▎ | 249/300 [02:08<00:26,  1.96it/s]Epoch 28:  83%|████████▎ | 250/300 [02:09<00:26,  1.87it/s]Epoch 28:  84%|████████▎ | 251/300 [02:10<00:25,  1.93it/s]Epoch 28:  84%|████████▍ | 252/300 [02:10<00:24,  1.93it/s]Epoch 28:  84%|████████▍ | 253/300 [02:10<00:23,  1.98it/s]Epoch 28:  85%|████████▍ | 254/300 [02:11<00:24,  1.88it/s]Epoch 28:  85%|████████▌ | 255/300 [02:12<00:24,  1.85it/s]Epoch 28:  85%|████████▌ | 256/300 [02:12<00:22,  1.92it/s]Epoch 28:  86%|████████▌ | 257/300 [02:13<00:22,  1.88it/s]Epoch 28:  86%|████████▌ | 258/300 [02:13<00:21,  1.94it/s]Epoch 28:  86%|████████▋ | 259/300 [02:14<00:21,  1.88it/s]06/19/2022 21:03:03 - INFO - __main__ - global step: 4330; train loss: 7.003706455230713; dev loss: 7.560159206390381
Epoch 28:  87%|████████▋ | 260/300 [02:14<00:20,  1.94it/s]Epoch 28:  87%|████████▋ | 261/300 [02:15<00:19,  1.98it/s]Epoch 28:  87%|████████▋ | 262/300 [02:15<00:18,  2.02it/s]Epoch 28:  88%|████████▊ | 263/300 [02:16<00:19,  1.89it/s]Epoch 28:  88%|████████▊ | 264/300 [02:16<00:18,  1.95it/s]Epoch 28:  88%|████████▊ | 265/300 [02:17<00:17,  1.97it/s]Epoch 28:  89%|████████▊ | 266/300 [02:17<00:17,  1.96it/s]Epoch 28:  89%|████████▉ | 267/300 [02:18<00:18,  1.82it/s]Epoch 28:  89%|████████▉ | 268/300 [02:18<00:16,  1.89it/s]Epoch 28:  90%|████████▉ | 269/300 [02:19<00:16,  1.90it/s]Epoch 28:  90%|█████████ | 270/300 [02:19<00:15,  1.93it/s]Epoch 28:  90%|█████████ | 271/300 [02:20<00:15,  1.87it/s]Epoch 28:  91%|█████████ | 272/300 [02:20<00:14,  1.93it/s]Epoch 28:  91%|█████████ | 273/300 [02:21<00:13,  1.94it/s]Epoch 28:  91%|█████████▏| 274/300 [02:21<00:13,  1.99it/s]Epoch 28:  92%|█████████▏| 275/300 [02:22<00:13,  1.92it/s]Epoch 28:  92%|█████████▏| 276/300 [02:23<00:12,  1.89it/s]Epoch 28:  92%|█████████▏| 277/300 [02:23<00:11,  1.95it/s]Epoch 28:  93%|█████████▎| 278/300 [02:23<00:10,  2.00it/s]Epoch 28:  93%|█████████▎| 279/300 [02:24<00:10,  1.92it/s]06/19/2022 21:03:14 - INFO - __main__ - global step: 4340; train loss: 7.4274139404296875; dev loss: 7.0855255126953125
Epoch 28:  93%|█████████▎| 280/300 [02:25<00:10,  1.92it/s]Epoch 28:  94%|█████████▎| 281/300 [02:25<00:09,  1.98it/s]Epoch 28:  94%|█████████▍| 282/300 [02:26<00:08,  2.01it/s]Epoch 28:  94%|█████████▍| 283/300 [02:26<00:08,  2.04it/s]Epoch 28:  95%|█████████▍| 284/300 [02:27<00:08,  1.94it/s]Epoch 28:  95%|█████████▌| 285/300 [02:27<00:07,  1.99it/s]Epoch 28:  95%|█████████▌| 286/300 [02:28<00:06,  2.02it/s]Epoch 28:  96%|█████████▌| 287/300 [02:28<00:06,  2.05it/s]Epoch 28:  96%|█████████▌| 288/300 [02:29<00:06,  1.91it/s]Epoch 28:  96%|█████████▋| 289/300 [02:29<00:05,  1.97it/s]Epoch 28:  97%|█████████▋| 290/300 [02:30<00:05,  1.92it/s]Epoch 28:  97%|█████████▋| 291/300 [02:30<00:04,  1.88it/s]Epoch 28:  97%|█████████▋| 292/300 [02:31<00:04,  1.75it/s]Epoch 28:  98%|█████████▊| 293/300 [02:31<00:03,  1.85it/s]Epoch 28:  98%|█████████▊| 294/300 [02:32<00:03,  1.83it/s]Epoch 28:  98%|█████████▊| 295/300 [02:32<00:02,  1.87it/s]Epoch 28:  99%|█████████▊| 296/300 [02:33<00:02,  1.83it/s]Epoch 28:  99%|█████████▉| 297/300 [02:33<00:01,  1.86it/s]Epoch 28:  99%|█████████▉| 298/300 [02:34<00:01,  1.91it/s]Epoch 28: 100%|█████████▉| 299/300 [02:34<00:00,  1.96it/s]06/19/2022 21:03:24 - INFO - __main__ - global step: 4350; train loss: 6.865529537200928; dev loss: 6.539727210998535
Epoch 28: 100%|██████████| 300/300 [02:35<00:00,  1.81it/s]Epoch 28: 100%|██████████| 300/300 [02:35<00:00,  1.93it/s]
Epoch 29:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 29:   0%|          | 1/300 [00:00<02:20,  2.12it/s]Epoch 29:   1%|          | 2/300 [00:00<02:27,  2.01it/s]Epoch 29:   1%|          | 3/300 [00:01<02:24,  2.06it/s]Epoch 29:   1%|▏         | 4/300 [00:02<02:32,  1.94it/s]Epoch 29:   2%|▏         | 5/300 [00:02<02:27,  2.00it/s]Epoch 29:   2%|▏         | 6/300 [00:02<02:24,  2.03it/s]Epoch 29:   2%|▏         | 7/300 [00:03<02:30,  1.95it/s]Epoch 29:   3%|▎         | 8/300 [00:04<02:34,  1.89it/s]Epoch 29:   3%|▎         | 9/300 [00:04<02:28,  1.96it/s]Epoch 29:   3%|▎         | 10/300 [00:05<02:25,  2.00it/s]Epoch 29:   4%|▎         | 11/300 [00:05<02:22,  2.03it/s]Epoch 29:   4%|▍         | 12/300 [00:05<02:19,  2.06it/s]Epoch 29:   4%|▍         | 13/300 [00:06<02:26,  1.96it/s]Epoch 29:   5%|▍         | 14/300 [00:07<02:22,  2.01it/s]Epoch 29:   5%|▌         | 15/300 [00:07<02:19,  2.04it/s]Epoch 29:   5%|▌         | 16/300 [00:07<02:17,  2.06it/s]Epoch 29:   6%|▌         | 17/300 [00:08<02:27,  1.92it/s]Epoch 29:   6%|▌         | 18/300 [00:09<02:23,  1.97it/s]Epoch 29:   6%|▋         | 19/300 [00:09<02:20,  2.00it/s]06/19/2022 21:03:34 - INFO - __main__ - global step: 4360; train loss: 7.039186000823975; dev loss: 7.195333003997803
Epoch 29:   7%|▋         | 20/300 [00:10<02:18,  2.03it/s]Epoch 29:   7%|▋         | 21/300 [00:10<02:23,  1.94it/s]Epoch 29:   7%|▋         | 22/300 [00:11<02:20,  1.98it/s]Epoch 29:   8%|▊         | 23/300 [00:11<02:17,  2.01it/s]Epoch 29:   8%|▊         | 24/300 [00:12<02:16,  2.03it/s]Epoch 29:   8%|▊         | 25/300 [00:12<02:25,  1.90it/s]Epoch 29:   9%|▊         | 26/300 [00:13<02:20,  1.95it/s]Epoch 29:   9%|▉         | 27/300 [00:13<02:20,  1.94it/s]Epoch 29:   9%|▉         | 28/300 [00:14<02:16,  1.99it/s]Epoch 29:  10%|▉         | 29/300 [00:14<02:21,  1.92it/s]Epoch 29:  10%|█         | 30/300 [00:15<02:16,  1.97it/s]Epoch 29:  10%|█         | 31/300 [00:15<02:19,  1.93it/s]Epoch 29:  11%|█         | 32/300 [00:16<02:15,  1.97it/s]Epoch 29:  11%|█         | 33/300 [00:16<02:20,  1.90it/s]Epoch 29:  11%|█▏        | 34/300 [00:17<02:16,  1.95it/s]Epoch 29:  12%|█▏        | 35/300 [00:17<02:14,  1.98it/s]Epoch 29:  12%|█▏        | 36/300 [00:18<02:12,  2.00it/s]Epoch 29:  12%|█▏        | 37/300 [00:18<02:12,  1.98it/s]Epoch 29:  13%|█▎        | 38/300 [00:19<02:16,  1.92it/s]Epoch 29:  13%|█▎        | 39/300 [00:19<02:15,  1.93it/s]06/19/2022 21:03:44 - INFO - __main__ - global step: 4370; train loss: 6.997478485107422; dev loss: 6.656922817230225
Epoch 29:  13%|█▎        | 40/300 [00:20<02:11,  1.98it/s]Epoch 29:  14%|█▎        | 41/300 [00:20<02:09,  2.00it/s]Epoch 29:  14%|█▍        | 42/300 [00:21<02:13,  1.93it/s]Epoch 29:  14%|█▍        | 43/300 [00:21<02:09,  1.99it/s]Epoch 29:  15%|█▍        | 44/300 [00:22<02:07,  2.02it/s]Epoch 29:  15%|█▌        | 45/300 [00:22<02:04,  2.04it/s]Epoch 29:  15%|█▌        | 46/300 [00:23<02:09,  1.96it/s]Epoch 29:  16%|█▌        | 47/300 [00:23<02:06,  2.00it/s]Epoch 29:  16%|█▌        | 48/300 [00:24<02:07,  1.98it/s]Epoch 29:  16%|█▋        | 49/300 [00:24<02:04,  2.01it/s]Epoch 29:  17%|█▋        | 50/300 [00:25<02:13,  1.88it/s]Epoch 29:  17%|█▋        | 51/300 [00:25<02:13,  1.86it/s]Epoch 29:  17%|█▋        | 52/300 [00:26<02:11,  1.89it/s]Epoch 29:  18%|█▊        | 53/300 [00:26<02:10,  1.89it/s]Epoch 29:  18%|█▊        | 54/300 [00:27<02:13,  1.84it/s]Epoch 29:  18%|█▊        | 55/300 [00:28<02:13,  1.84it/s]Epoch 29:  19%|█▊        | 56/300 [00:28<02:07,  1.91it/s]Epoch 29:  19%|█▉        | 57/300 [00:29<02:03,  1.96it/s]Epoch 29:  19%|█▉        | 58/300 [00:29<02:07,  1.89it/s]Epoch 29:  20%|█▉        | 59/300 [00:30<02:04,  1.94it/s]06/19/2022 21:03:55 - INFO - __main__ - global step: 4380; train loss: 7.2031965255737305; dev loss: 7.2797441482543945
Epoch 29:  20%|██        | 60/300 [00:30<02:03,  1.95it/s]Epoch 29:  20%|██        | 61/300 [00:31<01:59,  2.00it/s]Epoch 29:  21%|██        | 62/300 [00:31<02:04,  1.91it/s]Epoch 29:  21%|██        | 63/300 [00:32<02:00,  1.96it/s]Epoch 29:  21%|██▏       | 64/300 [00:32<01:58,  1.99it/s]Epoch 29:  22%|██▏       | 65/300 [00:33<01:55,  2.03it/s]Epoch 29:  22%|██▏       | 66/300 [00:33<01:54,  2.05it/s]Epoch 29:  22%|██▏       | 67/300 [00:34<01:59,  1.95it/s]Epoch 29:  23%|██▎       | 68/300 [00:34<01:56,  1.99it/s]Epoch 29:  23%|██▎       | 69/300 [00:35<01:57,  1.96it/s]Epoch 29:  23%|██▎       | 70/300 [00:35<01:57,  1.95it/s]Epoch 29:  24%|██▎       | 71/300 [00:36<02:02,  1.87it/s]Epoch 29:  24%|██▍       | 72/300 [00:36<01:59,  1.90it/s]Epoch 29:  24%|██▍       | 73/300 [00:37<01:58,  1.91it/s]Epoch 29:  25%|██▍       | 74/300 [00:37<02:00,  1.87it/s]Epoch 29:  25%|██▌       | 75/300 [00:38<02:03,  1.82it/s]Epoch 29:  25%|██▌       | 76/300 [00:38<02:01,  1.85it/s]Epoch 29:  26%|██▌       | 77/300 [00:39<01:57,  1.89it/s]Epoch 29:  26%|██▌       | 78/300 [00:39<01:53,  1.95it/s]Epoch 29:  26%|██▋       | 79/300 [00:40<01:56,  1.90it/s]06/19/2022 21:04:05 - INFO - __main__ - global step: 4390; train loss: 7.632346153259277; dev loss: 7.5692315101623535
Epoch 29:  27%|██▋       | 80/300 [00:40<01:55,  1.90it/s]Epoch 29:  27%|██▋       | 81/300 [00:41<01:52,  1.94it/s]Epoch 29:  27%|██▋       | 82/300 [00:41<01:50,  1.97it/s]Epoch 29:  28%|██▊       | 83/300 [00:42<01:54,  1.89it/s]Epoch 29:  28%|██▊       | 84/300 [00:43<01:53,  1.90it/s]Epoch 29:  28%|██▊       | 85/300 [00:43<01:52,  1.90it/s]Epoch 29:  29%|██▊       | 86/300 [00:44<01:50,  1.94it/s]Epoch 29:  29%|██▉       | 87/300 [00:44<01:54,  1.86it/s]Epoch 29:  29%|██▉       | 88/300 [00:45<01:50,  1.92it/s]Epoch 29:  30%|██▉       | 89/300 [00:45<01:50,  1.92it/s]Epoch 29:  30%|███       | 90/300 [00:46<01:47,  1.95it/s]Epoch 29:  30%|███       | 91/300 [00:46<01:45,  1.99it/s]Epoch 29:  31%|███       | 92/300 [00:47<01:48,  1.91it/s]Epoch 29:  31%|███       | 93/300 [00:47<01:48,  1.91it/s]Epoch 29:  31%|███▏      | 94/300 [00:48<01:45,  1.96it/s]Epoch 29:  32%|███▏      | 95/300 [00:48<01:43,  1.99it/s]Epoch 29:  32%|███▏      | 96/300 [00:49<01:49,  1.87it/s]Epoch 29:  32%|███▏      | 97/300 [00:49<01:44,  1.93it/s]Epoch 29:  33%|███▎      | 98/300 [00:50<01:42,  1.98it/s]Epoch 29:  33%|███▎      | 99/300 [00:50<01:40,  2.01it/s]06/19/2022 21:04:15 - INFO - __main__ - global step: 4400; train loss: 7.001976013183594; dev loss: 6.7640485763549805
Epoch 29:  33%|███▎      | 100/300 [00:51<01:44,  1.92it/s]Epoch 29:  34%|███▎      | 101/300 [00:51<01:42,  1.95it/s]Epoch 29:  34%|███▍      | 102/300 [00:52<01:39,  1.99it/s]Epoch 29:  34%|███▍      | 103/300 [00:52<01:37,  2.01it/s]Epoch 29:  35%|███▍      | 104/300 [00:53<01:43,  1.90it/s]Epoch 29:  35%|███▌      | 105/300 [00:53<01:39,  1.95it/s]Epoch 29:  35%|███▌      | 106/300 [00:54<01:37,  2.00it/s]Epoch 29:  36%|███▌      | 107/300 [00:54<01:36,  2.01it/s]Epoch 29:  36%|███▌      | 108/300 [00:55<01:39,  1.92it/s]Epoch 29:  36%|███▋      | 109/300 [00:55<01:39,  1.93it/s]Epoch 29:  37%|███▋      | 110/300 [00:56<01:35,  1.98it/s]Epoch 29:  37%|███▋      | 111/300 [00:56<01:34,  2.01it/s]Epoch 29:  37%|███▋      | 112/300 [00:57<01:38,  1.91it/s]Epoch 29:  38%|███▊      | 113/300 [00:57<01:36,  1.94it/s]Epoch 29:  38%|███▊      | 114/300 [00:58<01:34,  1.98it/s]Epoch 29:  38%|███▊      | 115/300 [00:58<01:32,  2.01it/s]Epoch 29:  39%|███▊      | 116/300 [00:59<01:35,  1.92it/s]Epoch 29:  39%|███▉      | 117/300 [00:59<01:34,  1.93it/s]Epoch 29:  39%|███▉      | 118/300 [01:00<01:35,  1.91it/s]Epoch 29:  40%|███▉      | 119/300 [01:01<01:35,  1.89it/s]06/19/2022 21:04:26 - INFO - __main__ - global step: 4410; train loss: 7.442104339599609; dev loss: 7.1223464012146
Epoch 29:  40%|████      | 120/300 [01:01<01:32,  1.94it/s]Epoch 29:  40%|████      | 121/300 [01:02<01:39,  1.79it/s]Epoch 29:  41%|████      | 122/300 [01:02<01:41,  1.75it/s]Epoch 29:  41%|████      | 123/300 [01:03<01:37,  1.82it/s]Epoch 29:  41%|████▏     | 124/300 [01:03<01:33,  1.89it/s]Epoch 29:  42%|████▏     | 125/300 [01:04<01:34,  1.85it/s]Epoch 29:  42%|████▏     | 126/300 [01:04<01:32,  1.88it/s]Epoch 29:  42%|████▏     | 127/300 [01:05<01:32,  1.87it/s]Epoch 29:  43%|████▎     | 128/300 [01:05<01:30,  1.90it/s]Epoch 29:  43%|████▎     | 129/300 [01:06<01:35,  1.78it/s]Epoch 29:  43%|████▎     | 130/300 [01:07<01:30,  1.87it/s]Epoch 29:  44%|████▎     | 131/300 [01:07<01:27,  1.94it/s]Epoch 29:  44%|████▍     | 132/300 [01:07<01:25,  1.97it/s]Epoch 29:  44%|████▍     | 133/300 [01:08<01:29,  1.86it/s]Epoch 29:  45%|████▍     | 134/300 [01:09<01:25,  1.93it/s]Epoch 29:  45%|████▌     | 135/300 [01:09<01:23,  1.99it/s]Epoch 29:  45%|████▌     | 136/300 [01:10<01:26,  1.91it/s]Epoch 29:  46%|████▌     | 137/300 [01:10<01:28,  1.84it/s]Epoch 29:  46%|████▌     | 138/300 [01:11<01:25,  1.89it/s]Epoch 29:  46%|████▋     | 139/300 [01:11<01:22,  1.94it/s]06/19/2022 21:04:36 - INFO - __main__ - global step: 4420; train loss: 6.5561723709106445; dev loss: 6.930868625640869
Epoch 29:  47%|████▋     | 140/300 [01:12<01:20,  1.98it/s]Epoch 29:  47%|████▋     | 141/300 [01:12<01:27,  1.83it/s]Epoch 29:  47%|████▋     | 142/300 [01:13<01:30,  1.75it/s]Epoch 29:  48%|████▊     | 143/300 [01:14<01:29,  1.76it/s]Epoch 29:  48%|████▊     | 144/300 [01:14<01:24,  1.85it/s]Epoch 29:  48%|████▊     | 145/300 [01:14<01:20,  1.91it/s]Epoch 29:  49%|████▊     | 146/300 [01:15<01:22,  1.87it/s]Epoch 29:  49%|████▉     | 147/300 [01:16<01:19,  1.92it/s]Epoch 29:  49%|████▉     | 148/300 [01:16<01:17,  1.96it/s]Epoch 29:  50%|████▉     | 149/300 [01:16<01:15,  1.99it/s]Epoch 29:  50%|█████     | 150/300 [01:17<01:18,  1.91it/s]Epoch 29:  50%|█████     | 151/300 [01:18<01:16,  1.96it/s]Epoch 29:  51%|█████     | 152/300 [01:18<01:14,  1.99it/s]Epoch 29:  51%|█████     | 153/300 [01:19<01:12,  2.02it/s]Epoch 29:  51%|█████▏    | 154/300 [01:19<01:18,  1.85it/s]Epoch 29:  52%|█████▏    | 155/300 [01:20<01:15,  1.92it/s]Epoch 29:  52%|█████▏    | 156/300 [01:20<01:16,  1.87it/s]Epoch 29:  52%|█████▏    | 157/300 [01:21<01:13,  1.93it/s]Epoch 29:  53%|█████▎    | 158/300 [01:21<01:16,  1.87it/s]Epoch 29:  53%|█████▎    | 159/300 [01:22<01:14,  1.88it/s]06/19/2022 21:04:47 - INFO - __main__ - global step: 4430; train loss: 6.928395748138428; dev loss: 6.189214706420898
Epoch 29:  53%|█████▎    | 160/300 [01:22<01:12,  1.93it/s]Epoch 29:  54%|█████▎    | 161/300 [01:23<01:09,  1.99it/s]Epoch 29:  54%|█████▍    | 162/300 [01:23<01:12,  1.91it/s]Epoch 29:  54%|█████▍    | 163/300 [01:24<01:09,  1.96it/s]Epoch 29:  55%|█████▍    | 164/300 [01:24<01:08,  2.00it/s]Epoch 29:  55%|█████▌    | 165/300 [01:25<01:06,  2.03it/s]Epoch 29:  55%|█████▌    | 166/300 [01:25<01:09,  1.92it/s]Epoch 29:  56%|█████▌    | 167/300 [01:26<01:07,  1.97it/s]Epoch 29:  56%|█████▌    | 168/300 [01:26<01:06,  2.00it/s]Epoch 29:  56%|█████▋    | 169/300 [01:27<01:04,  2.02it/s]Epoch 29:  57%|█████▋    | 170/300 [01:27<01:08,  1.91it/s]Epoch 29:  57%|█████▋    | 171/300 [01:28<01:06,  1.95it/s]Epoch 29:  57%|█████▋    | 172/300 [01:28<01:05,  1.94it/s]Epoch 29:  58%|█████▊    | 173/300 [01:29<01:04,  1.98it/s]Epoch 29:  58%|█████▊    | 174/300 [01:29<01:02,  2.01it/s]Epoch 29:  58%|█████▊    | 175/300 [01:30<01:08,  1.83it/s]Epoch 29:  59%|█████▊    | 176/300 [01:30<01:05,  1.90it/s]Epoch 29:  59%|█████▉    | 177/300 [01:31<01:02,  1.97it/s]Epoch 29:  59%|█████▉    | 178/300 [01:31<01:00,  2.01it/s]Epoch 29:  60%|█████▉    | 179/300 [01:32<01:05,  1.84it/s]06/19/2022 21:04:57 - INFO - __main__ - global step: 4440; train loss: 6.85143518447876; dev loss: 6.960170745849609
Epoch 29:  60%|██████    | 180/300 [01:33<01:04,  1.87it/s]Epoch 29:  60%|██████    | 181/300 [01:33<01:04,  1.85it/s]Epoch 29:  61%|██████    | 182/300 [01:34<01:04,  1.83it/s]Epoch 29:  61%|██████    | 183/300 [01:34<01:05,  1.78it/s]Epoch 29:  61%|██████▏   | 184/300 [01:35<01:02,  1.86it/s]Epoch 29:  62%|██████▏   | 185/300 [01:35<00:59,  1.92it/s]Epoch 29:  62%|██████▏   | 186/300 [01:36<01:01,  1.87it/s]Epoch 29:  62%|██████▏   | 187/300 [01:36<01:04,  1.74it/s]Epoch 29:  63%|██████▎   | 188/300 [01:37<01:01,  1.83it/s]Epoch 29:  63%|██████▎   | 189/300 [01:37<00:58,  1.90it/s]Epoch 29:  63%|██████▎   | 190/300 [01:38<00:57,  1.90it/s]Epoch 29:  64%|██████▎   | 191/300 [01:39<00:59,  1.82it/s]Epoch 29:  64%|██████▍   | 192/300 [01:39<00:59,  1.81it/s]Epoch 29:  64%|██████▍   | 193/300 [01:40<00:56,  1.89it/s]Epoch 29:  65%|██████▍   | 194/300 [01:40<00:54,  1.96it/s]Epoch 29:  65%|██████▌   | 195/300 [01:41<00:55,  1.89it/s]Epoch 29:  65%|██████▌   | 196/300 [01:41<00:53,  1.95it/s]Epoch 29:  66%|██████▌   | 197/300 [01:42<00:52,  1.95it/s]Epoch 29:  66%|██████▌   | 198/300 [01:42<00:51,  1.96it/s]Epoch 29:  66%|██████▋   | 199/300 [01:43<00:50,  2.00it/s]06/19/2022 21:05:08 - INFO - __main__ - global step: 4450; train loss: 7.095947265625; dev loss: 7.261704444885254
Epoch 29:  67%|██████▋   | 200/300 [01:43<00:55,  1.82it/s]Epoch 29:  67%|██████▋   | 201/300 [01:44<00:52,  1.90it/s]Epoch 29:  67%|██████▋   | 202/300 [01:44<00:50,  1.94it/s]Epoch 29:  68%|██████▊   | 203/300 [01:45<00:50,  1.94it/s]Epoch 29:  68%|██████▊   | 204/300 [01:45<00:51,  1.86it/s]Epoch 29:  68%|██████▊   | 205/300 [01:46<00:49,  1.92it/s]Epoch 29:  69%|██████▊   | 206/300 [01:46<00:47,  1.97it/s]Epoch 29:  69%|██████▉   | 207/300 [01:47<00:47,  1.95it/s]Epoch 29:  69%|██████▉   | 208/300 [01:47<00:48,  1.88it/s]Epoch 29:  70%|██████▉   | 209/300 [01:48<00:47,  1.91it/s]Epoch 29:  70%|███████   | 210/300 [01:48<00:47,  1.91it/s]Epoch 29:  70%|███████   | 211/300 [01:49<00:45,  1.96it/s]Epoch 29:  71%|███████   | 212/300 [01:49<00:46,  1.88it/s]Epoch 29:  71%|███████   | 213/300 [01:50<00:45,  1.93it/s]Epoch 29:  71%|███████▏  | 214/300 [01:50<00:43,  1.98it/s]Epoch 29:  72%|███████▏  | 215/300 [01:51<00:42,  2.01it/s]Epoch 29:  72%|███████▏  | 216/300 [01:51<00:43,  1.92it/s]Epoch 29:  72%|███████▏  | 217/300 [01:52<00:42,  1.96it/s]Epoch 29:  73%|███████▎  | 218/300 [01:53<00:43,  1.88it/s]Epoch 29:  73%|███████▎  | 219/300 [01:53<00:41,  1.94it/s]06/19/2022 21:05:18 - INFO - __main__ - global step: 4460; train loss: 7.210916996002197; dev loss: 6.747653007507324
Epoch 29:  73%|███████▎  | 220/300 [01:54<00:42,  1.88it/s]Epoch 29:  74%|███████▎  | 221/300 [01:54<00:41,  1.89it/s]Epoch 29:  74%|███████▍  | 222/300 [01:55<00:40,  1.95it/s]Epoch 29:  74%|███████▍  | 223/300 [01:55<00:39,  1.96it/s]Epoch 29:  75%|███████▍  | 224/300 [01:56<00:42,  1.80it/s]Epoch 29:  75%|███████▌  | 225/300 [01:56<00:39,  1.88it/s]Epoch 29:  75%|███████▌  | 226/300 [01:57<00:38,  1.93it/s]Epoch 29:  76%|███████▌  | 227/300 [01:57<00:38,  1.88it/s]Epoch 29:  76%|███████▌  | 228/300 [01:58<00:38,  1.85it/s]Epoch 29:  76%|███████▋  | 229/300 [01:58<00:38,  1.82it/s]Epoch 29:  77%|███████▋  | 230/300 [01:59<00:36,  1.89it/s]Epoch 29:  77%|███████▋  | 231/300 [01:59<00:36,  1.90it/s]Epoch 29:  77%|███████▋  | 232/300 [02:00<00:34,  1.95it/s]Epoch 29:  78%|███████▊  | 233/300 [02:00<00:35,  1.90it/s]Epoch 29:  78%|███████▊  | 234/300 [02:01<00:33,  1.95it/s]Epoch 29:  78%|███████▊  | 235/300 [02:01<00:32,  1.97it/s]Epoch 29:  79%|███████▊  | 236/300 [02:02<00:32,  1.94it/s]Epoch 29:  79%|███████▉  | 237/300 [02:03<00:33,  1.88it/s]Epoch 29:  79%|███████▉  | 238/300 [02:03<00:32,  1.90it/s]Epoch 29:  80%|███████▉  | 239/300 [02:04<00:32,  1.86it/s]06/19/2022 21:05:29 - INFO - __main__ - global step: 4470; train loss: 6.917418003082275; dev loss: 7.216357231140137
Epoch 29:  80%|████████  | 240/300 [02:04<00:31,  1.89it/s]Epoch 29:  80%|████████  | 241/300 [02:05<00:32,  1.81it/s]Epoch 29:  81%|████████  | 242/300 [02:05<00:30,  1.87it/s]Epoch 29:  81%|████████  | 243/300 [02:06<00:29,  1.94it/s]Epoch 29:  81%|████████▏ | 244/300 [02:06<00:28,  1.94it/s]Epoch 29:  82%|████████▏ | 245/300 [02:07<00:30,  1.78it/s]Epoch 29:  82%|████████▏ | 246/300 [02:07<00:29,  1.81it/s]Epoch 29:  82%|████████▏ | 247/300 [02:08<00:30,  1.75it/s]Epoch 29:  83%|████████▎ | 248/300 [02:09<00:30,  1.73it/s]Epoch 29:  83%|████████▎ | 249/300 [02:09<00:29,  1.73it/s]Epoch 29:  83%|████████▎ | 250/300 [02:10<00:27,  1.79it/s]Epoch 29:  84%|████████▎ | 251/300 [02:10<00:26,  1.88it/s]Epoch 29:  84%|████████▍ | 252/300 [02:11<00:24,  1.94it/s]Epoch 29:  84%|████████▍ | 253/300 [02:11<00:23,  1.99it/s]Epoch 29:  85%|████████▍ | 254/300 [02:12<00:24,  1.88it/s]Epoch 29:  85%|████████▌ | 255/300 [02:12<00:23,  1.93it/s]Epoch 29:  85%|████████▌ | 256/300 [02:13<00:22,  1.97it/s]Epoch 29:  86%|████████▌ | 257/300 [02:13<00:21,  2.01it/s]Epoch 29:  86%|████████▌ | 258/300 [02:14<00:21,  1.94it/s]Epoch 29:  86%|████████▋ | 259/300 [02:14<00:20,  1.95it/s]06/19/2022 21:05:39 - INFO - __main__ - global step: 4480; train loss: 7.267672538757324; dev loss: 7.091486930847168
Epoch 29:  87%|████████▋ | 260/300 [02:15<00:21,  1.88it/s]Epoch 29:  87%|████████▋ | 261/300 [02:15<00:20,  1.94it/s]Epoch 29:  87%|████████▋ | 262/300 [02:16<00:20,  1.89it/s]Epoch 29:  88%|████████▊ | 263/300 [02:16<00:18,  1.96it/s]Epoch 29:  88%|████████▊ | 264/300 [02:17<00:17,  2.00it/s]Epoch 29:  88%|████████▊ | 265/300 [02:17<00:17,  1.98it/s]Epoch 29:  89%|████████▊ | 266/300 [02:18<00:17,  1.89it/s]Epoch 29:  89%|████████▉ | 267/300 [02:18<00:16,  1.95it/s]Epoch 29:  89%|████████▉ | 268/300 [02:19<00:16,  1.92it/s]Epoch 29:  90%|████████▉ | 269/300 [02:19<00:15,  1.94it/s]Epoch 29:  90%|█████████ | 270/300 [02:20<00:16,  1.85it/s]Epoch 29:  90%|█████████ | 271/300 [02:20<00:15,  1.92it/s]Epoch 29:  91%|█████████ | 272/300 [02:21<00:14,  1.97it/s]Epoch 29:  91%|█████████ | 273/300 [02:21<00:13,  2.01it/s]Epoch 29:  91%|█████████▏| 274/300 [02:22<00:13,  1.92it/s]Epoch 29:  92%|█████████▏| 275/300 [02:23<00:12,  1.94it/s]Epoch 29:  92%|█████████▏| 276/300 [02:23<00:12,  1.93it/s]Epoch 29:  92%|█████████▏| 277/300 [02:24<00:11,  1.93it/s]Epoch 29:  93%|█████████▎| 278/300 [02:24<00:11,  1.86it/s]Epoch 29:  93%|█████████▎| 279/300 [02:25<00:10,  1.91it/s]06/19/2022 21:05:50 - INFO - __main__ - global step: 4490; train loss: 7.206336975097656; dev loss: 7.0148210525512695
Epoch 29:  93%|█████████▎| 280/300 [02:25<00:10,  1.96it/s]Epoch 29:  94%|█████████▎| 281/300 [02:26<00:09,  2.00it/s]Epoch 29:  94%|█████████▍| 282/300 [02:26<00:08,  2.03it/s]Epoch 29:  94%|█████████▍| 283/300 [02:27<00:08,  1.90it/s]Epoch 29:  95%|█████████▍| 284/300 [02:27<00:08,  1.96it/s]Epoch 29:  95%|█████████▌| 285/300 [02:28<00:07,  1.97it/s]Epoch 29:  95%|█████████▌| 286/300 [02:28<00:06,  2.01it/s]Epoch 29:  96%|█████████▌| 287/300 [02:29<00:06,  1.93it/s]Epoch 29:  96%|█████████▌| 288/300 [02:29<00:06,  1.97it/s]Epoch 29:  96%|█████████▋| 289/300 [02:30<00:05,  1.94it/s]Epoch 29:  97%|█████████▋| 290/300 [02:30<00:05,  1.95it/s]Epoch 29:  97%|█████████▋| 291/300 [02:31<00:04,  1.88it/s]Epoch 29:  97%|█████████▋| 292/300 [02:31<00:04,  1.93it/s]Epoch 29:  98%|█████████▊| 293/300 [02:32<00:03,  1.98it/s]Epoch 29:  98%|█████████▊| 294/300 [02:32<00:03,  1.98it/s]Epoch 29:  98%|█████████▊| 295/300 [02:33<00:02,  1.90it/s]Epoch 29:  99%|█████████▊| 296/300 [02:33<00:02,  1.88it/s]Epoch 29:  99%|█████████▉| 297/300 [02:34<00:01,  1.88it/s]Epoch 29:  99%|█████████▉| 298/300 [02:34<00:01,  1.89it/s]Epoch 29: 100%|█████████▉| 299/300 [02:35<00:00,  1.85it/s]06/19/2022 21:06:00 - INFO - __main__ - global step: 4500; train loss: 6.72986364364624; dev loss: 6.978597164154053
Epoch 29: 100%|██████████| 300/300 [02:36<00:00,  1.85it/s]Epoch 29: 100%|██████████| 300/300 [02:36<00:00,  1.92it/s]
Epoch 30:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 30:   0%|          | 1/300 [00:00<02:20,  2.13it/s]Epoch 30:   1%|          | 2/300 [00:00<02:22,  2.09it/s]Epoch 30:   1%|          | 3/300 [00:01<02:34,  1.92it/s]Epoch 30:   1%|▏         | 4/300 [00:02<02:29,  1.98it/s]Epoch 30:   2%|▏         | 5/300 [00:02<02:29,  1.97it/s]Epoch 30:   2%|▏         | 6/300 [00:03<02:27,  2.00it/s]Epoch 30:   2%|▏         | 7/300 [00:03<02:24,  2.02it/s]Epoch 30:   3%|▎         | 8/300 [00:04<02:30,  1.93it/s]Epoch 30:   3%|▎         | 9/300 [00:04<02:27,  1.97it/s]Epoch 30:   3%|▎         | 10/300 [00:05<02:24,  2.01it/s]Epoch 30:   4%|▎         | 11/300 [00:05<02:30,  1.92it/s]Epoch 30:   4%|▍         | 12/300 [00:06<02:40,  1.79it/s]Epoch 30:   4%|▍         | 13/300 [00:06<02:38,  1.81it/s]Epoch 30:   5%|▍         | 14/300 [00:07<02:30,  1.90it/s]Epoch 30:   5%|▌         | 15/300 [00:07<02:25,  1.95it/s]Epoch 30:   5%|▌         | 16/300 [00:08<02:29,  1.90it/s]Epoch 30:   6%|▌         | 17/300 [00:08<02:28,  1.90it/s]Epoch 30:   6%|▌         | 18/300 [00:09<02:29,  1.88it/s]Epoch 30:   6%|▋         | 19/300 [00:09<02:31,  1.86it/s]06/19/2022 21:06:11 - INFO - __main__ - global step: 4510; train loss: 7.389710903167725; dev loss: 7.567191123962402
Epoch 30:   7%|▋         | 20/300 [00:10<02:42,  1.72it/s]Epoch 30:   7%|▋         | 21/300 [00:11<02:41,  1.73it/s]Epoch 30:   7%|▋         | 22/300 [00:11<02:40,  1.73it/s]Epoch 30:   8%|▊         | 23/300 [00:12<02:39,  1.73it/s]Epoch 30:   8%|▊         | 24/300 [00:12<02:47,  1.65it/s]Epoch 30:   8%|▊         | 25/300 [00:13<02:44,  1.67it/s]Epoch 30:   9%|▊         | 26/300 [00:14<02:41,  1.70it/s]Epoch 30:   9%|▉         | 27/300 [00:14<02:31,  1.81it/s]Epoch 30:   9%|▉         | 28/300 [00:15<02:31,  1.79it/s]Epoch 30:  10%|▉         | 29/300 [00:15<02:24,  1.87it/s]Epoch 30:  10%|█         | 30/300 [00:16<02:23,  1.88it/s]Epoch 30:  10%|█         | 31/300 [00:16<02:21,  1.90it/s]Epoch 30:  11%|█         | 32/300 [00:17<02:26,  1.83it/s]Epoch 30:  11%|█         | 33/300 [00:17<02:24,  1.85it/s]Epoch 30:  11%|█▏        | 34/300 [00:18<02:19,  1.91it/s]Epoch 30:  12%|█▏        | 35/300 [00:18<02:15,  1.96it/s]Epoch 30:  12%|█▏        | 36/300 [00:19<02:12,  2.00it/s]Epoch 30:  12%|█▏        | 37/300 [00:19<02:20,  1.87it/s]Epoch 30:  13%|█▎        | 38/300 [00:20<02:15,  1.93it/s]Epoch 30:  13%|█▎        | 39/300 [00:20<02:15,  1.93it/s]06/19/2022 21:06:21 - INFO - __main__ - global step: 4520; train loss: 7.036285400390625; dev loss: 6.845714569091797
Epoch 30:  13%|█▎        | 40/300 [00:21<02:11,  1.98it/s]Epoch 30:  14%|█▎        | 41/300 [00:21<02:15,  1.91it/s]Epoch 30:  14%|█▍        | 42/300 [00:22<02:11,  1.97it/s]Epoch 30:  14%|█▍        | 43/300 [00:22<02:10,  1.98it/s]Epoch 30:  15%|█▍        | 44/300 [00:23<02:13,  1.92it/s]Epoch 30:  15%|█▌        | 45/300 [00:23<02:15,  1.88it/s]Epoch 30:  15%|█▌        | 46/300 [00:24<02:10,  1.95it/s]Epoch 30:  16%|█▌        | 47/300 [00:24<02:07,  1.99it/s]Epoch 30:  16%|█▌        | 48/300 [00:25<02:06,  2.00it/s]Epoch 30:  16%|█▋        | 49/300 [00:25<02:10,  1.92it/s]Epoch 30:  17%|█▋        | 50/300 [00:26<02:06,  1.98it/s]Epoch 30:  17%|█▋        | 51/300 [00:26<02:07,  1.95it/s]Epoch 30:  17%|█▋        | 52/300 [00:27<02:04,  1.99it/s]Epoch 30:  18%|█▊        | 53/300 [00:28<02:10,  1.89it/s]Epoch 30:  18%|█▊        | 54/300 [00:28<02:06,  1.95it/s]Epoch 30:  18%|█▊        | 55/300 [00:29<02:04,  1.96it/s]Epoch 30:  19%|█▊        | 56/300 [00:29<02:05,  1.94it/s]Epoch 30:  19%|█▉        | 57/300 [00:30<02:13,  1.82it/s]Epoch 30:  19%|█▉        | 58/300 [00:30<02:11,  1.84it/s]Epoch 30:  20%|█▉        | 59/300 [00:31<02:07,  1.89it/s]06/19/2022 21:06:32 - INFO - __main__ - global step: 4530; train loss: 7.179164886474609; dev loss: 6.916560173034668
Epoch 30:  20%|██        | 60/300 [00:31<02:06,  1.89it/s]Epoch 30:  20%|██        | 61/300 [00:32<02:02,  1.95it/s]Epoch 30:  21%|██        | 62/300 [00:32<02:05,  1.89it/s]Epoch 30:  21%|██        | 63/300 [00:33<02:00,  1.96it/s]Epoch 30:  21%|██▏       | 64/300 [00:33<01:58,  2.00it/s]Epoch 30:  22%|██▏       | 65/300 [00:34<01:56,  2.02it/s]Epoch 30:  22%|██▏       | 66/300 [00:34<02:01,  1.93it/s]Epoch 30:  22%|██▏       | 67/300 [00:35<01:57,  1.98it/s]Epoch 30:  23%|██▎       | 68/300 [00:35<01:57,  1.98it/s]Epoch 30:  23%|██▎       | 69/300 [00:36<01:57,  1.96it/s]Epoch 30:  23%|██▎       | 70/300 [00:36<02:02,  1.87it/s]Epoch 30:  24%|██▎       | 71/300 [00:37<02:00,  1.90it/s]Epoch 30:  24%|██▍       | 72/300 [00:37<01:56,  1.95it/s]Epoch 30:  24%|██▍       | 73/300 [00:38<01:53,  1.99it/s]Epoch 30:  25%|██▍       | 74/300 [00:38<02:00,  1.88it/s]Epoch 30:  25%|██▌       | 75/300 [00:39<01:59,  1.88it/s]Epoch 30:  25%|██▌       | 76/300 [00:39<01:56,  1.92it/s]Epoch 30:  26%|██▌       | 77/300 [00:40<01:55,  1.93it/s]Epoch 30:  26%|██▌       | 78/300 [00:41<01:58,  1.87it/s]Epoch 30:  26%|██▋       | 79/300 [00:41<01:53,  1.94it/s]06/19/2022 21:06:42 - INFO - __main__ - global step: 4540; train loss: 6.642006874084473; dev loss: 6.580945014953613
Epoch 30:  27%|██▋       | 80/300 [00:42<01:53,  1.94it/s]Epoch 30:  27%|██▋       | 81/300 [00:42<01:50,  1.98it/s]Epoch 30:  27%|██▋       | 82/300 [00:43<01:54,  1.90it/s]Epoch 30:  28%|██▊       | 83/300 [00:43<01:51,  1.95it/s]Epoch 30:  28%|██▊       | 84/300 [00:44<01:48,  1.99it/s]Epoch 30:  28%|██▊       | 85/300 [00:44<01:47,  1.99it/s]Epoch 30:  29%|██▊       | 86/300 [00:45<01:51,  1.91it/s]Epoch 30:  29%|██▉       | 87/300 [00:45<01:52,  1.90it/s]Epoch 30:  29%|██▉       | 88/300 [00:46<01:53,  1.86it/s]Epoch 30:  30%|██▉       | 89/300 [00:46<01:53,  1.86it/s]Epoch 30:  30%|███       | 90/300 [00:47<01:49,  1.92it/s]Epoch 30:  30%|███       | 91/300 [00:47<01:51,  1.87it/s]Epoch 30:  31%|███       | 92/300 [00:48<01:47,  1.93it/s]Epoch 30:  31%|███       | 93/300 [00:48<01:46,  1.95it/s]Epoch 30:  31%|███▏      | 94/300 [00:49<01:43,  1.98it/s]Epoch 30:  32%|███▏      | 95/300 [00:49<01:47,  1.91it/s]Epoch 30:  32%|███▏      | 96/300 [00:50<01:49,  1.87it/s]Epoch 30:  32%|███▏      | 97/300 [00:50<01:50,  1.84it/s]Epoch 30:  33%|███▎      | 98/300 [00:51<01:45,  1.91it/s]Epoch 30:  33%|███▎      | 99/300 [00:52<01:48,  1.85it/s]06/19/2022 21:06:53 - INFO - __main__ - global step: 4550; train loss: 6.907193183898926; dev loss: 7.20172119140625
Epoch 30:  33%|███▎      | 100/300 [00:52<01:44,  1.92it/s]Epoch 30:  34%|███▎      | 101/300 [00:53<01:45,  1.88it/s]Epoch 30:  34%|███▍      | 102/300 [00:53<01:44,  1.90it/s]Epoch 30:  34%|███▍      | 103/300 [00:54<01:48,  1.81it/s]Epoch 30:  35%|███▍      | 104/300 [00:54<01:44,  1.88it/s]Epoch 30:  35%|███▌      | 105/300 [00:55<01:40,  1.95it/s]Epoch 30:  35%|███▌      | 106/300 [00:55<01:37,  1.99it/s]Epoch 30:  36%|███▌      | 107/300 [00:56<01:45,  1.83it/s]Epoch 30:  36%|███▌      | 108/300 [00:56<01:41,  1.90it/s]Epoch 30:  36%|███▋      | 109/300 [00:57<01:42,  1.87it/s]Epoch 30:  37%|███▋      | 110/300 [00:57<01:38,  1.93it/s]Epoch 30:  37%|███▋      | 111/300 [00:58<01:40,  1.88it/s]Epoch 30:  37%|███▋      | 112/300 [00:58<01:36,  1.94it/s]Epoch 30:  38%|███▊      | 113/300 [00:59<01:34,  1.99it/s]Epoch 30:  38%|███▊      | 114/300 [00:59<01:34,  1.97it/s]Epoch 30:  38%|███▊      | 115/300 [01:00<01:32,  2.01it/s]Epoch 30:  39%|███▊      | 116/300 [01:00<01:36,  1.91it/s]Epoch 30:  39%|███▉      | 117/300 [01:01<01:32,  1.97it/s]Epoch 30:  39%|███▉      | 118/300 [01:01<01:30,  2.02it/s]Epoch 30:  40%|███▉      | 119/300 [01:02<01:28,  2.05it/s]06/19/2022 21:07:03 - INFO - __main__ - global step: 4560; train loss: 6.710337162017822; dev loss: 6.880344390869141
Epoch 30:  40%|████      | 120/300 [01:02<01:33,  1.93it/s]Epoch 30:  40%|████      | 121/300 [01:03<01:30,  1.99it/s]Epoch 30:  41%|████      | 122/300 [01:03<01:27,  2.03it/s]Epoch 30:  41%|████      | 123/300 [01:04<01:26,  2.05it/s]Epoch 30:  41%|████▏     | 124/300 [01:04<01:29,  1.96it/s]Epoch 30:  42%|████▏     | 125/300 [01:05<01:27,  2.01it/s]Epoch 30:  42%|████▏     | 126/300 [01:05<01:26,  2.02it/s]Epoch 30:  42%|████▏     | 127/300 [01:06<01:29,  1.94it/s]Epoch 30:  43%|████▎     | 128/300 [01:07<01:35,  1.80it/s]Epoch 30:  43%|████▎     | 129/300 [01:07<01:30,  1.88it/s]Epoch 30:  43%|████▎     | 130/300 [01:07<01:27,  1.94it/s]Epoch 30:  44%|████▎     | 131/300 [01:08<01:25,  1.98it/s]Epoch 30:  44%|████▍     | 132/300 [01:09<01:30,  1.86it/s]Epoch 30:  44%|████▍     | 133/300 [01:09<01:27,  1.92it/s]Epoch 30:  45%|████▍     | 134/300 [01:10<01:24,  1.96it/s]Epoch 30:  45%|████▌     | 135/300 [01:10<01:26,  1.90it/s]Epoch 30:  45%|████▌     | 136/300 [01:11<01:32,  1.78it/s]Epoch 30:  46%|████▌     | 137/300 [01:11<01:27,  1.86it/s]Epoch 30:  46%|████▌     | 138/300 [01:12<01:23,  1.93it/s]Epoch 30:  46%|████▋     | 139/300 [01:12<01:25,  1.89it/s]06/19/2022 21:07:13 - INFO - __main__ - global step: 4570; train loss: 7.102947235107422; dev loss: 6.740011692047119
Epoch 30:  47%|████▋     | 140/300 [01:13<01:27,  1.84it/s]Epoch 30:  47%|████▋     | 141/300 [01:13<01:22,  1.92it/s]Epoch 30:  47%|████▋     | 142/300 [01:14<01:20,  1.97it/s]Epoch 30:  48%|████▊     | 143/300 [01:14<01:19,  1.99it/s]Epoch 30:  48%|████▊     | 144/300 [01:15<01:17,  2.01it/s]Epoch 30:  48%|████▊     | 145/300 [01:15<01:22,  1.88it/s]Epoch 30:  49%|████▊     | 146/300 [01:16<01:23,  1.85it/s]Epoch 30:  49%|████▉     | 147/300 [01:16<01:19,  1.92it/s]Epoch 30:  49%|████▉     | 148/300 [01:17<01:20,  1.88it/s]Epoch 30:  50%|████▉     | 149/300 [01:18<01:25,  1.76it/s]Epoch 30:  50%|█████     | 150/300 [01:18<01:22,  1.81it/s]Epoch 30:  50%|█████     | 151/300 [01:19<01:22,  1.81it/s]Epoch 30:  51%|█████     | 152/300 [01:19<01:19,  1.87it/s]Epoch 30:  51%|█████     | 153/300 [01:20<01:20,  1.84it/s]Epoch 30:  51%|█████▏    | 154/300 [01:20<01:16,  1.91it/s]Epoch 30:  52%|█████▏    | 155/300 [01:21<01:14,  1.96it/s]Epoch 30:  52%|█████▏    | 156/300 [01:21<01:12,  1.99it/s]Epoch 30:  52%|█████▏    | 157/300 [01:22<01:14,  1.92it/s]Epoch 30:  53%|█████▎    | 158/300 [01:22<01:15,  1.88it/s]Epoch 30:  53%|█████▎    | 159/300 [01:23<01:12,  1.95it/s]06/19/2022 21:07:24 - INFO - __main__ - global step: 4580; train loss: 6.645604133605957; dev loss: 6.9185028076171875
Epoch 30:  53%|█████▎    | 160/300 [01:23<01:12,  1.93it/s]Epoch 30:  54%|█████▎    | 161/300 [01:24<01:18,  1.78it/s]Epoch 30:  54%|█████▍    | 162/300 [01:25<01:17,  1.77it/s]Epoch 30:  54%|█████▍    | 163/300 [01:25<01:15,  1.81it/s]Epoch 30:  55%|█████▍    | 164/300 [01:26<01:12,  1.87it/s]Epoch 30:  55%|█████▌    | 165/300 [01:26<01:14,  1.82it/s]Epoch 30:  55%|█████▌    | 166/300 [01:27<01:10,  1.89it/s]Epoch 30:  56%|█████▌    | 167/300 [01:27<01:11,  1.86it/s]Epoch 30:  56%|█████▌    | 168/300 [01:28<01:08,  1.92it/s]Epoch 30:  56%|█████▋    | 169/300 [01:28<01:06,  1.96it/s]Epoch 30:  57%|█████▋    | 170/300 [01:29<01:08,  1.90it/s]Epoch 30:  57%|█████▋    | 171/300 [01:29<01:05,  1.96it/s]Epoch 30:  57%|█████▋    | 172/300 [01:30<01:04,  2.00it/s]Epoch 30:  58%|█████▊    | 173/300 [01:30<01:05,  1.93it/s]Epoch 30:  58%|█████▊    | 174/300 [01:31<01:06,  1.89it/s]Epoch 30:  58%|█████▊    | 175/300 [01:31<01:04,  1.94it/s]Epoch 30:  59%|█████▊    | 176/300 [01:32<01:05,  1.89it/s]Epoch 30:  59%|█████▉    | 177/300 [01:32<01:06,  1.86it/s]Epoch 30:  59%|█████▉    | 178/300 [01:33<01:06,  1.83it/s]Epoch 30:  60%|█████▉    | 179/300 [01:33<01:03,  1.91it/s]06/19/2022 21:07:35 - INFO - __main__ - global step: 4590; train loss: 6.500186920166016; dev loss: 6.21428918838501
Epoch 30:  60%|██████    | 180/300 [01:34<01:02,  1.92it/s]Epoch 30:  60%|██████    | 181/300 [01:34<01:00,  1.97it/s]Epoch 30:  61%|██████    | 182/300 [01:35<01:02,  1.90it/s]Epoch 30:  61%|██████    | 183/300 [01:35<00:59,  1.96it/s]Epoch 30:  61%|██████▏   | 184/300 [01:36<00:57,  2.00it/s]Epoch 30:  62%|██████▏   | 185/300 [01:36<00:56,  2.04it/s]Epoch 30:  62%|██████▏   | 186/300 [01:37<01:00,  1.90it/s]Epoch 30:  62%|██████▏   | 187/300 [01:37<00:57,  1.96it/s]Epoch 30:  63%|██████▎   | 188/300 [01:38<00:56,  2.00it/s]Epoch 30:  63%|██████▎   | 189/300 [01:38<00:54,  2.02it/s]Epoch 30:  63%|██████▎   | 190/300 [01:39<00:56,  1.93it/s]Epoch 30:  64%|██████▎   | 191/300 [01:39<00:55,  1.98it/s]Epoch 30:  64%|██████▍   | 192/300 [01:40<00:53,  2.01it/s]Epoch 30:  64%|██████▍   | 193/300 [01:40<00:53,  2.01it/s]Epoch 30:  65%|██████▍   | 194/300 [01:41<00:54,  1.93it/s]Epoch 30:  65%|██████▌   | 195/300 [01:42<00:52,  1.98it/s]Epoch 30:  65%|██████▌   | 196/300 [01:42<00:51,  2.02it/s]Epoch 30:  66%|██████▌   | 197/300 [01:43<00:52,  1.94it/s]Epoch 30:  66%|██████▌   | 198/300 [01:43<00:53,  1.90it/s]Epoch 30:  66%|██████▋   | 199/300 [01:44<00:54,  1.85it/s]06/19/2022 21:07:45 - INFO - __main__ - global step: 4600; train loss: 7.193503379821777; dev loss: 7.239912509918213
Epoch 30:  67%|██████▋   | 200/300 [01:44<00:53,  1.87it/s]Epoch 30:  67%|██████▋   | 201/300 [01:45<00:50,  1.94it/s]Epoch 30:  67%|██████▋   | 202/300 [01:45<00:49,  1.98it/s]Epoch 30:  68%|██████▊   | 203/300 [01:46<00:52,  1.86it/s]Epoch 30:  68%|██████▊   | 204/300 [01:46<00:49,  1.93it/s]Epoch 30:  68%|██████▊   | 205/300 [01:47<00:48,  1.97it/s]Epoch 30:  69%|██████▊   | 206/300 [01:47<00:48,  1.94it/s]Epoch 30:  69%|██████▉   | 207/300 [01:48<00:51,  1.80it/s]Epoch 30:  69%|██████▉   | 208/300 [01:48<00:48,  1.88it/s]Epoch 30:  70%|██████▉   | 209/300 [01:49<00:46,  1.94it/s]Epoch 30:  70%|███████   | 210/300 [01:49<00:45,  1.96it/s]Epoch 30:  70%|███████   | 211/300 [01:50<00:47,  1.87it/s]Epoch 30:  71%|███████   | 212/300 [01:50<00:47,  1.87it/s]Epoch 30:  71%|███████   | 213/300 [01:51<00:45,  1.92it/s]Epoch 30:  71%|███████▏  | 214/300 [01:52<00:45,  1.89it/s]Epoch 30:  72%|███████▏  | 215/300 [01:52<00:46,  1.82it/s]Epoch 30:  72%|███████▏  | 216/300 [01:53<00:44,  1.87it/s]Epoch 30:  72%|███████▏  | 217/300 [01:53<00:43,  1.91it/s]Epoch 30:  73%|███████▎  | 218/300 [01:54<00:42,  1.94it/s]Epoch 30:  73%|███████▎  | 219/300 [01:54<00:43,  1.85it/s]06/19/2022 21:07:55 - INFO - __main__ - global step: 4610; train loss: 6.497182369232178; dev loss: 6.817099571228027
Epoch 30:  73%|███████▎  | 220/300 [01:55<00:42,  1.89it/s]Epoch 30:  74%|███████▎  | 221/300 [01:55<00:43,  1.84it/s]Epoch 30:  74%|███████▍  | 222/300 [01:56<00:41,  1.89it/s]Epoch 30:  74%|███████▍  | 223/300 [01:56<00:42,  1.83it/s]Epoch 30:  75%|███████▍  | 224/300 [01:57<00:42,  1.80it/s]Epoch 30:  75%|███████▌  | 225/300 [01:57<00:40,  1.86it/s]Epoch 30:  75%|███████▌  | 226/300 [01:58<00:39,  1.86it/s]Epoch 30:  76%|███████▌  | 227/300 [01:58<00:38,  1.91it/s]Epoch 30:  76%|███████▌  | 228/300 [01:59<00:38,  1.85it/s]Epoch 30:  76%|███████▋  | 229/300 [02:00<00:37,  1.89it/s]Epoch 30:  77%|███████▋  | 230/300 [02:00<00:38,  1.83it/s]Epoch 30:  77%|███████▋  | 231/300 [02:01<00:38,  1.78it/s]Epoch 30:  77%|███████▋  | 232/300 [02:01<00:38,  1.75it/s]Epoch 30:  78%|███████▊  | 233/300 [02:02<00:38,  1.73it/s]Epoch 30:  78%|███████▊  | 234/300 [02:02<00:36,  1.81it/s]Epoch 30:  78%|███████▊  | 235/300 [02:03<00:34,  1.87it/s]Epoch 30:  79%|███████▊  | 236/300 [02:03<00:35,  1.81it/s]Epoch 30:  79%|███████▉  | 237/300 [02:04<00:33,  1.87it/s]Epoch 30:  79%|███████▉  | 238/300 [02:04<00:32,  1.91it/s]Epoch 30:  80%|███████▉  | 239/300 [02:05<00:31,  1.94it/s]06/19/2022 21:08:06 - INFO - __main__ - global step: 4620; train loss: 6.732832431793213; dev loss: 6.863847255706787
Epoch 30:  80%|████████  | 240/300 [02:06<00:33,  1.78it/s]Epoch 30:  80%|████████  | 241/300 [02:06<00:32,  1.84it/s]Epoch 30:  81%|████████  | 242/300 [02:07<00:30,  1.89it/s]Epoch 30:  81%|████████  | 243/300 [02:07<00:29,  1.92it/s]Epoch 30:  81%|████████▏ | 244/300 [02:08<00:31,  1.77it/s]Epoch 30:  82%|████████▏ | 245/300 [02:08<00:29,  1.83it/s]Epoch 30:  82%|████████▏ | 246/300 [02:09<00:29,  1.84it/s]Epoch 30:  82%|████████▏ | 247/300 [02:09<00:27,  1.89it/s]Epoch 30:  83%|████████▎ | 248/300 [02:10<00:29,  1.74it/s]Epoch 30:  83%|████████▎ | 249/300 [02:11<00:28,  1.81it/s]Epoch 30:  83%|████████▎ | 250/300 [02:11<00:26,  1.87it/s]Epoch 30:  84%|████████▎ | 251/300 [02:12<00:26,  1.86it/s]Epoch 30:  84%|████████▍ | 252/300 [02:12<00:25,  1.90it/s]Epoch 30:  84%|████████▍ | 253/300 [02:13<00:26,  1.76it/s]Epoch 30:  85%|████████▍ | 254/300 [02:13<00:25,  1.80it/s]Epoch 30:  85%|████████▌ | 255/300 [02:14<00:25,  1.78it/s]Epoch 30:  85%|████████▌ | 256/300 [02:14<00:23,  1.85it/s]Epoch 30:  86%|████████▌ | 257/300 [02:15<00:23,  1.81it/s]Epoch 30:  86%|████████▌ | 258/300 [02:15<00:22,  1.87it/s]Epoch 30:  86%|████████▋ | 259/300 [02:16<00:21,  1.91it/s]06/19/2022 21:08:17 - INFO - __main__ - global step: 4630; train loss: 6.731047630310059; dev loss: 6.948296546936035
Epoch 30:  87%|████████▋ | 260/300 [02:16<00:20,  1.94it/s]Epoch 30:  87%|████████▋ | 261/300 [02:17<00:21,  1.78it/s]Epoch 30:  87%|████████▋ | 262/300 [02:18<00:20,  1.83it/s]Epoch 30:  88%|████████▊ | 263/300 [02:18<00:19,  1.88it/s]Epoch 30:  88%|████████▊ | 264/300 [02:19<00:18,  1.92it/s]Epoch 30:  88%|████████▊ | 265/300 [02:19<00:18,  1.85it/s]Epoch 30:  89%|████████▊ | 266/300 [02:20<00:17,  1.90it/s]Epoch 30:  89%|████████▉ | 267/300 [02:20<00:17,  1.94it/s]Epoch 30:  89%|████████▉ | 268/300 [02:21<00:17,  1.86it/s]Epoch 30:  90%|████████▉ | 269/300 [02:21<00:17,  1.81it/s]Epoch 30:  90%|█████████ | 270/300 [02:22<00:16,  1.87it/s]Epoch 30:  90%|█████████ | 271/300 [02:22<00:15,  1.91it/s]Epoch 30:  91%|█████████ | 272/300 [02:23<00:14,  1.94it/s]Epoch 30:  91%|█████████ | 273/300 [02:23<00:14,  1.85it/s]Epoch 30:  91%|█████████▏| 274/300 [02:24<00:13,  1.90it/s]Epoch 30:  92%|█████████▏| 275/300 [02:24<00:12,  1.94it/s]Epoch 30:  92%|█████████▏| 276/300 [02:25<00:12,  1.97it/s]Epoch 30:  92%|█████████▏| 277/300 [02:25<00:11,  1.99it/s]Epoch 30:  93%|█████████▎| 278/300 [02:26<00:11,  1.85it/s]Epoch 30:  93%|█████████▎| 279/300 [02:27<00:11,  1.82it/s]06/19/2022 21:08:28 - INFO - __main__ - global step: 4640; train loss: 6.509703636169434; dev loss: 6.5252180099487305
Epoch 30:  93%|█████████▎| 280/300 [02:27<00:10,  1.87it/s]Epoch 30:  94%|█████████▎| 281/300 [02:28<00:09,  1.91it/s]Epoch 30:  94%|█████████▍| 282/300 [02:28<00:09,  1.85it/s]Epoch 30:  94%|█████████▍| 283/300 [02:29<00:08,  1.90it/s]Epoch 30:  95%|█████████▍| 284/300 [02:29<00:08,  1.89it/s]Epoch 30:  95%|█████████▌| 285/300 [02:30<00:08,  1.84it/s]Epoch 30:  95%|█████████▌| 286/300 [02:30<00:07,  1.79it/s]Epoch 30:  96%|█████████▌| 287/300 [02:31<00:07,  1.84it/s]Epoch 30:  96%|█████████▌| 288/300 [02:31<00:06,  1.80it/s]Epoch 30:  96%|█████████▋| 289/300 [02:32<00:06,  1.82it/s]Epoch 30:  97%|█████████▋| 290/300 [02:33<00:05,  1.78it/s]Epoch 30:  97%|█████████▋| 291/300 [02:33<00:04,  1.84it/s]Epoch 30:  97%|█████████▋| 292/300 [02:34<00:04,  1.85it/s]Epoch 30:  98%|█████████▊| 293/300 [02:34<00:03,  1.90it/s]Epoch 30:  98%|█████████▊| 294/300 [02:35<00:03,  1.83it/s]Epoch 30:  98%|█████████▊| 295/300 [02:35<00:02,  1.78it/s]Epoch 30:  99%|█████████▊| 296/300 [02:36<00:02,  1.77it/s]Epoch 30:  99%|█████████▉| 297/300 [02:36<00:01,  1.84it/s]Epoch 30:  99%|█████████▉| 298/300 [02:37<00:01,  1.80it/s]Epoch 30: 100%|█████████▉| 299/300 [02:37<00:00,  1.85it/s]06/19/2022 21:08:39 - INFO - __main__ - global step: 4650; train loss: 6.803974151611328; dev loss: 6.5251922607421875
Epoch 30: 100%|██████████| 300/300 [02:38<00:00,  1.85it/s]Epoch 30: 100%|██████████| 300/300 [02:38<00:00,  1.89it/s]
Epoch 31:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 31:   0%|          | 1/300 [00:00<02:38,  1.89it/s]Epoch 31:   1%|          | 2/300 [00:01<02:55,  1.70it/s]Epoch 31:   1%|          | 3/300 [00:01<02:41,  1.84it/s]Epoch 31:   1%|▏         | 4/300 [00:02<02:44,  1.80it/s]Epoch 31:   2%|▏         | 5/300 [00:02<02:37,  1.87it/s]Epoch 31:   2%|▏         | 6/300 [00:03<02:46,  1.76it/s]Epoch 31:   2%|▏         | 7/300 [00:03<02:53,  1.69it/s]Epoch 31:   3%|▎         | 8/300 [00:04<02:44,  1.78it/s]Epoch 31:   3%|▎         | 9/300 [00:04<02:37,  1.85it/s]Epoch 31:   3%|▎         | 10/300 [00:05<02:33,  1.90it/s]Epoch 31:   4%|▎         | 11/300 [00:06<02:43,  1.77it/s]Epoch 31:   4%|▍         | 12/300 [00:06<02:41,  1.78it/s]Epoch 31:   4%|▍         | 13/300 [00:07<02:43,  1.75it/s]Epoch 31:   5%|▍         | 14/300 [00:07<02:40,  1.78it/s]Epoch 31:   5%|▌         | 15/300 [00:08<02:42,  1.75it/s]Epoch 31:   5%|▌         | 16/300 [00:08<02:35,  1.82it/s]Epoch 31:   6%|▌         | 17/300 [00:09<02:30,  1.88it/s]Epoch 31:   6%|▌         | 18/300 [00:09<02:27,  1.91it/s]Epoch 31:   6%|▋         | 19/300 [00:10<02:32,  1.85it/s]06/19/2022 21:08:50 - INFO - __main__ - global step: 4660; train loss: 6.552696228027344; dev loss: 6.696347236633301
Epoch 31:   7%|▋         | 20/300 [00:11<02:35,  1.80it/s]Epoch 31:   7%|▋         | 21/300 [00:11<02:30,  1.86it/s]Epoch 31:   7%|▋         | 22/300 [00:12<02:26,  1.90it/s]Epoch 31:   8%|▊         | 23/300 [00:12<02:30,  1.84it/s]Epoch 31:   8%|▊         | 24/300 [00:13<02:26,  1.89it/s]Epoch 31:   8%|▊         | 25/300 [00:13<02:22,  1.93it/s]Epoch 31:   9%|▊         | 26/300 [00:14<02:27,  1.86it/s]Epoch 31:   9%|▉         | 27/300 [00:14<02:31,  1.80it/s]Epoch 31:   9%|▉         | 28/300 [00:15<02:25,  1.87it/s]Epoch 31:  10%|▉         | 29/300 [00:15<02:25,  1.87it/s]Epoch 31:  10%|█         | 30/300 [00:16<02:29,  1.81it/s]Epoch 31:  10%|█         | 31/300 [00:17<02:31,  1.78it/s]Epoch 31:  11%|█         | 32/300 [00:17<02:32,  1.75it/s]Epoch 31:  11%|█         | 33/300 [00:18<02:33,  1.74it/s]Epoch 31:  11%|█▏        | 34/300 [00:18<02:27,  1.81it/s]Epoch 31:  12%|█▏        | 35/300 [00:19<02:22,  1.86it/s]Epoch 31:  12%|█▏        | 36/300 [00:19<02:25,  1.81it/s]Epoch 31:  12%|█▏        | 37/300 [00:20<02:20,  1.88it/s]Epoch 31:  13%|█▎        | 38/300 [00:20<02:23,  1.83it/s]Epoch 31:  13%|█▎        | 39/300 [00:21<02:18,  1.89it/s]06/19/2022 21:09:01 - INFO - __main__ - global step: 4670; train loss: 6.784134864807129; dev loss: 6.868653774261475
Epoch 31:  13%|█▎        | 40/300 [00:21<02:22,  1.83it/s]Epoch 31:  14%|█▎        | 41/300 [00:22<02:24,  1.80it/s]Epoch 31:  14%|█▍        | 42/300 [00:23<02:25,  1.77it/s]Epoch 31:  14%|█▍        | 43/300 [00:23<02:19,  1.84it/s]Epoch 31:  15%|█▍        | 44/300 [00:24<02:22,  1.79it/s]Epoch 31:  15%|█▌        | 45/300 [00:24<02:24,  1.77it/s]Epoch 31:  15%|█▌        | 46/300 [00:25<02:24,  1.75it/s]Epoch 31:  16%|█▌        | 47/300 [00:25<02:21,  1.79it/s]Epoch 31:  16%|█▌        | 48/300 [00:26<02:23,  1.75it/s]Epoch 31:  16%|█▋        | 49/300 [00:26<02:17,  1.83it/s]Epoch 31:  17%|█▋        | 50/300 [00:27<02:12,  1.88it/s]Epoch 31:  17%|█▋        | 51/300 [00:27<02:09,  1.93it/s]Epoch 31:  17%|█▋        | 52/300 [00:28<02:14,  1.84it/s]Epoch 31:  18%|█▊        | 53/300 [00:29<02:16,  1.81it/s]Epoch 31:  18%|█▊        | 54/300 [00:29<02:11,  1.87it/s]Epoch 31:  18%|█▊        | 55/300 [00:30<02:07,  1.91it/s]Epoch 31:  19%|█▊        | 56/300 [00:30<02:13,  1.83it/s]Epoch 31:  19%|█▉        | 57/300 [00:31<02:08,  1.89it/s]Epoch 31:  19%|█▉        | 58/300 [00:31<02:06,  1.92it/s]Epoch 31:  20%|█▉        | 59/300 [00:32<02:03,  1.94it/s]06/19/2022 21:09:11 - INFO - __main__ - global step: 4680; train loss: 6.743325710296631; dev loss: 6.3802642822265625
Epoch 31:  20%|██        | 60/300 [00:32<02:02,  1.97it/s]Epoch 31:  20%|██        | 61/300 [00:33<02:14,  1.78it/s]Epoch 31:  21%|██        | 62/300 [00:33<02:08,  1.85it/s]Epoch 31:  21%|██        | 63/300 [00:34<02:08,  1.85it/s]Epoch 31:  21%|██▏       | 64/300 [00:34<02:04,  1.89it/s]Epoch 31:  22%|██▏       | 65/300 [00:35<02:07,  1.84it/s]Epoch 31:  22%|██▏       | 66/300 [00:36<02:06,  1.86it/s]Epoch 31:  22%|██▏       | 67/300 [00:36<02:01,  1.91it/s]Epoch 31:  23%|██▎       | 68/300 [00:37<01:59,  1.94it/s]Epoch 31:  23%|██▎       | 69/300 [00:37<02:10,  1.77it/s]Epoch 31:  23%|██▎       | 70/300 [00:38<02:04,  1.84it/s]Epoch 31:  24%|██▎       | 71/300 [00:38<02:06,  1.80it/s]Epoch 31:  24%|██▍       | 72/300 [00:39<02:02,  1.86it/s]Epoch 31:  24%|██▍       | 73/300 [00:39<02:05,  1.81it/s]Epoch 31:  25%|██▍       | 74/300 [00:40<02:06,  1.78it/s]Epoch 31:  25%|██▌       | 75/300 [00:40<02:01,  1.85it/s]Epoch 31:  25%|██▌       | 76/300 [00:41<01:58,  1.89it/s]Epoch 31:  26%|██▌       | 77/300 [00:42<02:02,  1.82it/s]Epoch 31:  26%|██▌       | 78/300 [00:42<01:58,  1.87it/s]Epoch 31:  26%|██▋       | 79/300 [00:43<01:58,  1.87it/s]06/19/2022 21:09:22 - INFO - __main__ - global step: 4690; train loss: 6.7003679275512695; dev loss: 6.708895683288574
Epoch 31:  27%|██▋       | 80/300 [00:43<02:00,  1.82it/s]Epoch 31:  27%|██▋       | 81/300 [00:44<02:08,  1.71it/s]Epoch 31:  27%|██▋       | 82/300 [00:44<02:01,  1.79it/s]Epoch 31:  28%|██▊       | 83/300 [00:45<01:57,  1.85it/s]Epoch 31:  28%|██▊       | 84/300 [00:45<01:57,  1.83it/s]Epoch 31:  28%|██▊       | 85/300 [00:46<01:54,  1.88it/s]Epoch 31:  29%|██▊       | 86/300 [00:46<01:57,  1.82it/s]Epoch 31:  29%|██▉       | 87/300 [00:47<01:56,  1.83it/s]Epoch 31:  29%|██▉       | 88/300 [00:48<01:58,  1.79it/s]Epoch 31:  30%|██▉       | 89/300 [00:48<01:53,  1.85it/s]Epoch 31:  30%|███       | 90/300 [00:49<01:55,  1.82it/s]Epoch 31:  30%|███       | 91/300 [00:49<01:57,  1.79it/s]Epoch 31:  31%|███       | 92/300 [00:50<01:52,  1.85it/s]Epoch 31:  31%|███       | 93/300 [00:50<01:49,  1.90it/s]Epoch 31:  31%|███▏      | 94/300 [00:51<01:52,  1.83it/s]Epoch 31:  32%|███▏      | 95/300 [00:51<01:51,  1.84it/s]Epoch 31:  32%|███▏      | 96/300 [00:52<01:48,  1.89it/s]Epoch 31:  32%|███▏      | 97/300 [00:52<01:51,  1.83it/s]Epoch 31:  33%|███▎      | 98/300 [00:53<01:52,  1.79it/s]Epoch 31:  33%|███▎      | 99/300 [00:54<01:48,  1.86it/s]06/19/2022 21:09:33 - INFO - __main__ - global step: 4700; train loss: 6.620434761047363; dev loss: 6.625977993011475
Epoch 31:  33%|███▎      | 100/300 [00:54<01:45,  1.90it/s]Epoch 31:  34%|███▎      | 101/300 [00:55<01:42,  1.93it/s]Epoch 31:  34%|███▍      | 102/300 [00:55<01:46,  1.85it/s]Epoch 31:  34%|███▍      | 103/300 [00:56<01:43,  1.91it/s]Epoch 31:  35%|███▍      | 104/300 [00:56<01:43,  1.89it/s]Epoch 31:  35%|███▌      | 105/300 [00:57<01:47,  1.82it/s]Epoch 31:  35%|███▌      | 106/300 [00:57<01:49,  1.77it/s]Epoch 31:  36%|███▌      | 107/300 [00:58<01:45,  1.83it/s]Epoch 31:  36%|███▌      | 108/300 [00:58<01:47,  1.79it/s]Epoch 31:  36%|███▋      | 109/300 [00:59<01:43,  1.85it/s]Epoch 31:  37%|███▋      | 110/300 [01:00<01:50,  1.71it/s]Epoch 31:  37%|███▋      | 111/300 [01:00<01:45,  1.79it/s]Epoch 31:  37%|███▋      | 112/300 [01:01<01:42,  1.84it/s]Epoch 31:  38%|███▊      | 113/300 [01:01<01:40,  1.86it/s]Epoch 31:  38%|███▊      | 114/300 [01:02<01:43,  1.81it/s]Epoch 31:  38%|███▊      | 115/300 [01:02<01:45,  1.75it/s]Epoch 31:  39%|███▊      | 116/300 [01:03<01:41,  1.82it/s]Epoch 31:  39%|███▉      | 117/300 [01:03<01:37,  1.87it/s]Epoch 31:  39%|███▉      | 118/300 [01:04<01:35,  1.90it/s]Epoch 31:  40%|███▉      | 119/300 [01:04<01:38,  1.83it/s]06/19/2022 21:09:44 - INFO - __main__ - global step: 4710; train loss: 6.709627628326416; dev loss: 6.70217752456665
Epoch 31:  40%|████      | 120/300 [01:05<01:35,  1.88it/s]Epoch 31:  40%|████      | 121/300 [01:05<01:33,  1.91it/s]Epoch 31:  41%|████      | 122/300 [01:06<01:34,  1.88it/s]Epoch 31:  41%|████      | 123/300 [01:07<01:37,  1.82it/s]Epoch 31:  41%|████▏     | 124/300 [01:07<01:39,  1.78it/s]Epoch 31:  42%|████▏     | 125/300 [01:08<01:34,  1.85it/s]Epoch 31:  42%|████▏     | 126/300 [01:08<01:33,  1.85it/s]Epoch 31:  42%|████▏     | 127/300 [01:09<01:37,  1.77it/s]Epoch 31:  43%|████▎     | 128/300 [01:09<01:33,  1.84it/s]Epoch 31:  43%|████▎     | 129/300 [01:10<01:30,  1.89it/s]Epoch 31:  43%|████▎     | 130/300 [01:10<01:28,  1.93it/s]Epoch 31:  44%|████▎     | 131/300 [01:11<01:33,  1.81it/s]Epoch 31:  44%|████▍     | 132/300 [01:11<01:29,  1.87it/s]Epoch 31:  44%|████▍     | 133/300 [01:12<01:26,  1.92it/s]Epoch 31:  45%|████▍     | 134/300 [01:12<01:29,  1.86it/s]Epoch 31:  45%|████▌     | 135/300 [01:13<01:31,  1.81it/s]Epoch 31:  45%|████▌     | 136/300 [01:14<01:30,  1.81it/s]Epoch 31:  46%|████▌     | 137/300 [01:14<01:27,  1.86it/s]Epoch 31:  46%|████▌     | 138/300 [01:15<01:24,  1.91it/s]Epoch 31:  46%|████▋     | 139/300 [01:15<01:24,  1.91it/s]06/19/2022 21:09:55 - INFO - __main__ - global step: 4720; train loss: 6.395367622375488; dev loss: 6.353424549102783
Epoch 31:  47%|████▋     | 140/300 [01:16<01:30,  1.77it/s]Epoch 31:  47%|████▋     | 141/300 [01:16<01:26,  1.84it/s]Epoch 31:  47%|████▋     | 142/300 [01:17<01:26,  1.83it/s]Epoch 31:  48%|████▊     | 143/300 [01:17<01:23,  1.89it/s]Epoch 31:  48%|████▊     | 144/300 [01:18<01:28,  1.77it/s]Epoch 31:  48%|████▊     | 145/300 [01:18<01:24,  1.83it/s]Epoch 31:  49%|████▊     | 146/300 [01:19<01:21,  1.88it/s]Epoch 31:  49%|████▉     | 147/300 [01:20<01:20,  1.89it/s]Epoch 31:  49%|████▉     | 148/300 [01:20<01:24,  1.80it/s]Epoch 31:  50%|████▉     | 149/300 [01:21<01:21,  1.86it/s]Epoch 31:  50%|█████     | 150/300 [01:21<01:18,  1.91it/s]Epoch 31:  50%|█████     | 151/300 [01:22<01:17,  1.93it/s]Epoch 31:  51%|█████     | 152/300 [01:22<01:19,  1.85it/s]Epoch 31:  51%|█████     | 153/300 [01:23<01:17,  1.90it/s]Epoch 31:  51%|█████▏    | 154/300 [01:23<01:17,  1.89it/s]Epoch 31:  52%|█████▏    | 155/300 [01:24<01:17,  1.88it/s]Epoch 31:  52%|█████▏    | 156/300 [01:24<01:19,  1.81it/s]Epoch 31:  52%|█████▏    | 157/300 [01:25<01:17,  1.86it/s]Epoch 31:  53%|█████▎    | 158/300 [01:25<01:16,  1.85it/s]Epoch 31:  53%|█████▎    | 159/300 [01:26<01:17,  1.81it/s]06/19/2022 21:10:06 - INFO - __main__ - global step: 4730; train loss: 6.413529872894287; dev loss: 6.618485450744629
Epoch 31:  53%|█████▎    | 160/300 [01:27<01:18,  1.77it/s]Epoch 31:  54%|█████▎    | 161/300 [01:27<01:15,  1.84it/s]Epoch 31:  54%|█████▍    | 162/300 [01:28<01:12,  1.89it/s]Epoch 31:  54%|█████▍    | 163/300 [01:28<01:11,  1.93it/s]Epoch 31:  55%|█████▍    | 164/300 [01:29<01:16,  1.78it/s]Epoch 31:  55%|█████▌    | 165/300 [01:29<01:16,  1.76it/s]Epoch 31:  55%|█████▌    | 166/300 [01:30<01:13,  1.82it/s]Epoch 31:  56%|█████▌    | 167/300 [01:30<01:12,  1.83it/s]Epoch 31:  56%|█████▌    | 168/300 [01:31<01:12,  1.83it/s]Epoch 31:  56%|█████▋    | 169/300 [01:32<01:13,  1.77it/s]Epoch 31:  57%|█████▋    | 170/300 [01:32<01:12,  1.79it/s]Epoch 31:  57%|█████▋    | 171/300 [01:33<01:09,  1.84it/s]Epoch 31:  57%|█████▋    | 172/300 [01:33<01:07,  1.89it/s]Epoch 31:  58%|█████▊    | 173/300 [01:34<01:09,  1.82it/s]Epoch 31:  58%|█████▊    | 174/300 [01:34<01:07,  1.87it/s]Epoch 31:  58%|█████▊    | 175/300 [01:35<01:08,  1.81it/s]Epoch 31:  59%|█████▊    | 176/300 [01:35<01:06,  1.86it/s]Epoch 31:  59%|█████▉    | 177/300 [01:36<01:08,  1.80it/s]Epoch 31:  59%|█████▉    | 178/300 [01:36<01:05,  1.86it/s]Epoch 31:  60%|█████▉    | 179/300 [01:37<01:05,  1.86it/s]06/19/2022 21:10:16 - INFO - __main__ - global step: 4740; train loss: 6.27285623550415; dev loss: 6.6354546546936035
Epoch 31:  60%|██████    | 180/300 [01:37<01:03,  1.89it/s]Epoch 31:  60%|██████    | 181/300 [01:38<01:05,  1.82it/s]Epoch 31:  61%|██████    | 182/300 [01:39<01:03,  1.87it/s]Epoch 31:  61%|██████    | 183/300 [01:39<01:01,  1.91it/s]Epoch 31:  61%|██████▏   | 184/300 [01:39<00:59,  1.94it/s]Epoch 31:  62%|██████▏   | 185/300 [01:40<01:01,  1.87it/s]Epoch 31:  62%|██████▏   | 186/300 [01:41<00:59,  1.91it/s]Epoch 31:  62%|██████▏   | 187/300 [01:41<00:58,  1.94it/s]Epoch 31:  63%|██████▎   | 188/300 [01:42<01:00,  1.86it/s]Epoch 31:  63%|██████▎   | 189/300 [01:42<01:01,  1.81it/s]Epoch 31:  63%|██████▎   | 190/300 [01:43<00:59,  1.86it/s]Epoch 31:  64%|██████▎   | 191/300 [01:43<00:57,  1.91it/s]Epoch 31:  64%|██████▍   | 192/300 [01:44<00:55,  1.94it/s]Epoch 31:  64%|██████▍   | 193/300 [01:44<00:57,  1.87it/s]Epoch 31:  65%|██████▍   | 194/300 [01:45<01:01,  1.73it/s]Epoch 31:  65%|██████▌   | 195/300 [01:46<01:00,  1.73it/s]Epoch 31:  65%|██████▌   | 196/300 [01:46<00:57,  1.81it/s]Epoch 31:  66%|██████▌   | 197/300 [01:47<00:54,  1.87it/s]Epoch 31:  66%|██████▌   | 198/300 [01:47<00:55,  1.82it/s]Epoch 31:  66%|██████▋   | 199/300 [01:48<00:53,  1.88it/s]06/19/2022 21:10:27 - INFO - __main__ - global step: 4750; train loss: 6.726243495941162; dev loss: 6.880735874176025
Epoch 31:  67%|██████▋   | 200/300 [01:48<00:53,  1.88it/s]Epoch 31:  67%|██████▋   | 201/300 [01:49<00:51,  1.91it/s]Epoch 31:  67%|██████▋   | 202/300 [01:49<00:55,  1.76it/s]Epoch 31:  68%|██████▊   | 203/300 [01:50<00:52,  1.83it/s]Epoch 31:  68%|██████▊   | 204/300 [01:50<00:51,  1.88it/s]Epoch 31:  68%|██████▊   | 205/300 [01:51<00:49,  1.92it/s]Epoch 31:  69%|██████▊   | 206/300 [01:51<00:52,  1.80it/s]Epoch 31:  69%|██████▉   | 207/300 [01:52<00:50,  1.86it/s]Epoch 31:  69%|██████▉   | 208/300 [01:53<00:49,  1.85it/s]Epoch 31:  70%|██████▉   | 209/300 [01:53<00:47,  1.91it/s]Epoch 31:  70%|███████   | 210/300 [01:54<00:48,  1.85it/s]Epoch 31:  70%|███████   | 211/300 [01:54<00:46,  1.90it/s]Epoch 31:  71%|███████   | 212/300 [01:55<00:45,  1.93it/s]Epoch 31:  71%|███████   | 213/300 [01:55<00:44,  1.96it/s]Epoch 31:  71%|███████▏  | 214/300 [01:56<00:45,  1.87it/s]Epoch 31:  72%|███████▏  | 215/300 [01:56<00:44,  1.92it/s]Epoch 31:  72%|███████▏  | 216/300 [01:57<00:42,  1.95it/s]Epoch 31:  72%|███████▏  | 217/300 [01:57<00:42,  1.97it/s]Epoch 31:  73%|███████▎  | 218/300 [01:58<00:43,  1.89it/s]Epoch 31:  73%|███████▎  | 219/300 [01:58<00:43,  1.88it/s]06/19/2022 21:10:38 - INFO - __main__ - global step: 4760; train loss: 6.568737030029297; dev loss: 6.361027240753174
Epoch 31:  73%|███████▎  | 220/300 [01:59<00:42,  1.86it/s]Epoch 31:  74%|███████▎  | 221/300 [01:59<00:43,  1.82it/s]Epoch 31:  74%|███████▍  | 222/300 [02:00<00:41,  1.87it/s]Epoch 31:  74%|███████▍  | 223/300 [02:01<00:44,  1.73it/s]Epoch 31:  75%|███████▍  | 224/300 [02:01<00:43,  1.77it/s]Epoch 31:  75%|███████▌  | 225/300 [02:02<00:41,  1.80it/s]Epoch 31:  75%|███████▌  | 226/300 [02:02<00:41,  1.77it/s]Epoch 31:  76%|███████▌  | 227/300 [02:03<00:41,  1.75it/s]Epoch 31:  76%|███████▌  | 228/300 [02:03<00:39,  1.83it/s]Epoch 31:  76%|███████▋  | 229/300 [02:04<00:37,  1.88it/s]Epoch 31:  77%|███████▋  | 230/300 [02:04<00:36,  1.91it/s]Epoch 31:  77%|███████▋  | 231/300 [02:05<00:37,  1.84it/s]Epoch 31:  77%|███████▋  | 232/300 [02:05<00:36,  1.84it/s]Epoch 31:  78%|███████▊  | 233/300 [02:06<00:35,  1.88it/s]Epoch 31:  78%|███████▊  | 234/300 [02:07<00:36,  1.83it/s]Epoch 31:  78%|███████▊  | 235/300 [02:07<00:36,  1.78it/s]Epoch 31:  79%|███████▊  | 236/300 [02:08<00:34,  1.84it/s]Epoch 31:  79%|███████▉  | 237/300 [02:08<00:33,  1.89it/s]Epoch 31:  79%|███████▉  | 238/300 [02:09<00:32,  1.88it/s]Epoch 31:  80%|███████▉  | 239/300 [02:09<00:33,  1.83it/s]06/19/2022 21:10:49 - INFO - __main__ - global step: 4770; train loss: 6.895737648010254; dev loss: 6.69762659072876
Epoch 31:  80%|████████  | 240/300 [02:10<00:31,  1.88it/s]Epoch 31:  80%|████████  | 241/300 [02:10<00:32,  1.83it/s]Epoch 31:  81%|████████  | 242/300 [02:11<00:30,  1.88it/s]Epoch 31:  81%|████████  | 243/300 [02:11<00:31,  1.83it/s]Epoch 31:  81%|████████▏ | 244/300 [02:12<00:29,  1.88it/s]Epoch 31:  82%|████████▏ | 245/300 [02:12<00:30,  1.83it/s]Epoch 31:  82%|████████▏ | 246/300 [02:13<00:28,  1.88it/s]Epoch 31:  82%|████████▏ | 247/300 [02:13<00:27,  1.92it/s]Epoch 31:  83%|████████▎ | 248/300 [02:14<00:28,  1.84it/s]Epoch 31:  83%|████████▎ | 249/300 [02:15<00:28,  1.81it/s]Epoch 31:  83%|████████▎ | 250/300 [02:15<00:26,  1.87it/s]Epoch 31:  84%|████████▎ | 251/300 [02:16<00:25,  1.91it/s]Epoch 31:  84%|████████▍ | 252/300 [02:16<00:27,  1.76it/s]Epoch 31:  84%|████████▍ | 253/300 [02:17<00:25,  1.83it/s]Epoch 31:  85%|████████▍ | 254/300 [02:17<00:24,  1.88it/s]Epoch 31:  85%|████████▌ | 255/300 [02:18<00:23,  1.92it/s]Epoch 31:  85%|████████▌ | 256/300 [02:18<00:23,  1.84it/s]Epoch 31:  86%|████████▌ | 257/300 [02:19<00:22,  1.88it/s]Epoch 31:  86%|████████▌ | 258/300 [02:19<00:21,  1.92it/s]Epoch 31:  86%|████████▋ | 259/300 [02:20<00:20,  1.96it/s]06/19/2022 21:11:00 - INFO - __main__ - global step: 4780; train loss: 6.8080596923828125; dev loss: 6.584504127502441
Epoch 31:  87%|████████▋ | 260/300 [02:20<00:21,  1.87it/s]Epoch 31:  87%|████████▋ | 261/300 [02:21<00:20,  1.91it/s]Epoch 31:  87%|████████▋ | 262/300 [02:21<00:19,  1.94it/s]Epoch 31:  88%|████████▊ | 263/300 [02:22<00:18,  1.96it/s]Epoch 31:  88%|████████▊ | 264/300 [02:23<00:19,  1.83it/s]Epoch 31:  88%|████████▊ | 265/300 [02:23<00:18,  1.88it/s]Epoch 31:  89%|████████▊ | 266/300 [02:24<00:17,  1.92it/s]Epoch 31:  89%|████████▉ | 267/300 [02:24<00:16,  1.95it/s]Epoch 31:  89%|████████▉ | 268/300 [02:25<00:17,  1.86it/s]Epoch 31:  90%|████████▉ | 269/300 [02:25<00:16,  1.91it/s]Epoch 31:  90%|█████████ | 270/300 [02:26<00:15,  1.95it/s]Epoch 31:  90%|█████████ | 271/300 [02:26<00:14,  1.97it/s]Epoch 31:  91%|█████████ | 272/300 [02:27<00:15,  1.85it/s]Epoch 31:  91%|█████████ | 273/300 [02:27<00:14,  1.85it/s]Epoch 31:  91%|█████████▏| 274/300 [02:28<00:13,  1.89it/s]Epoch 31:  92%|█████████▏| 275/300 [02:28<00:13,  1.84it/s]Epoch 31:  92%|█████████▏| 276/300 [02:29<00:12,  1.89it/s]Epoch 31:  92%|█████████▏| 277/300 [02:29<00:12,  1.83it/s]Epoch 31:  93%|█████████▎| 278/300 [02:30<00:11,  1.89it/s]Epoch 31:  93%|█████████▎| 279/300 [02:30<00:10,  1.93it/s]06/19/2022 21:11:10 - INFO - __main__ - global step: 4790; train loss: 6.3663010597229; dev loss: 6.161023139953613
Epoch 31:  93%|█████████▎| 280/300 [02:31<00:10,  1.95it/s]Epoch 31:  94%|█████████▎| 281/300 [02:32<00:10,  1.88it/s]Epoch 31:  94%|█████████▍| 282/300 [02:32<00:09,  1.92it/s]Epoch 31:  94%|█████████▍| 283/300 [02:32<00:08,  1.95it/s]Epoch 31:  95%|█████████▍| 284/300 [02:33<00:08,  1.95it/s]Epoch 31:  95%|█████████▌| 285/300 [02:34<00:08,  1.87it/s]Epoch 31:  95%|█████████▌| 286/300 [02:34<00:07,  1.82it/s]Epoch 31:  96%|█████████▌| 287/300 [02:35<00:06,  1.88it/s]Epoch 31:  96%|█████████▌| 288/300 [02:35<00:06,  1.92it/s]Epoch 31:  96%|█████████▋| 289/300 [02:36<00:05,  1.85it/s]Epoch 31:  97%|█████████▋| 290/300 [02:36<00:05,  1.88it/s]Epoch 31:  97%|█████████▋| 291/300 [02:37<00:04,  1.92it/s]Epoch 31:  97%|█████████▋| 292/300 [02:37<00:04,  1.95it/s]Epoch 31:  98%|█████████▊| 293/300 [02:38<00:03,  1.82it/s]Epoch 31:  98%|█████████▊| 294/300 [02:38<00:03,  1.88it/s]Epoch 31:  98%|█████████▊| 295/300 [02:39<00:02,  1.83it/s]Epoch 31:  99%|█████████▊| 296/300 [02:39<00:02,  1.88it/s]Epoch 31:  99%|█████████▉| 297/300 [02:40<00:01,  1.83it/s]Epoch 31:  99%|█████████▉| 298/300 [02:41<00:01,  1.89it/s]Epoch 31: 100%|█████████▉| 299/300 [02:41<00:00,  1.93it/s]06/19/2022 21:11:21 - INFO - __main__ - global step: 4800; train loss: 6.606881141662598; dev loss: 6.434049129486084
Epoch 31: 100%|██████████| 300/300 [02:42<00:00,  1.95it/s]Epoch 31: 100%|██████████| 300/300 [02:42<00:00,  1.85it/s]
Epoch 32:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 32:   0%|          | 1/300 [00:00<02:27,  2.03it/s]Epoch 32:   1%|          | 2/300 [00:01<02:43,  1.82it/s]Epoch 32:   1%|          | 3/300 [00:01<02:46,  1.78it/s]Epoch 32:   1%|▏         | 4/300 [00:02<02:38,  1.86it/s]Epoch 32:   2%|▏         | 5/300 [00:02<02:33,  1.92it/s]Epoch 32:   2%|▏         | 6/300 [00:03<02:48,  1.75it/s]Epoch 32:   2%|▏         | 7/300 [00:03<02:39,  1.83it/s]Epoch 32:   3%|▎         | 8/300 [00:04<02:42,  1.80it/s]Epoch 32:   3%|▎         | 9/300 [00:04<02:35,  1.87it/s]Epoch 32:   3%|▎         | 10/300 [00:05<02:39,  1.82it/s]Epoch 32:   4%|▎         | 11/300 [00:05<02:34,  1.86it/s]Epoch 32:   4%|▍         | 12/300 [00:06<02:30,  1.91it/s]Epoch 32:   4%|▍         | 13/300 [00:06<02:28,  1.94it/s]Epoch 32:   5%|▍         | 14/300 [00:07<02:34,  1.85it/s]Epoch 32:   5%|▌         | 15/300 [00:08<02:31,  1.89it/s]Epoch 32:   5%|▌         | 16/300 [00:08<02:27,  1.93it/s]Epoch 32:   6%|▌         | 17/300 [00:09<02:29,  1.90it/s]Epoch 32:   6%|▌         | 18/300 [00:09<02:33,  1.83it/s]Epoch 32:   6%|▋         | 19/300 [00:10<02:32,  1.85it/s]06/19/2022 21:11:31 - INFO - __main__ - global step: 4810; train loss: 6.646462917327881; dev loss: 6.575925350189209
Epoch 32:   7%|▋         | 20/300 [00:10<02:28,  1.89it/s]Epoch 32:   7%|▋         | 21/300 [00:11<02:25,  1.92it/s]Epoch 32:   7%|▋         | 22/300 [00:11<02:29,  1.86it/s]Epoch 32:   8%|▊         | 23/300 [00:12<02:32,  1.81it/s]Epoch 32:   8%|▊         | 24/300 [00:12<02:27,  1.87it/s]Epoch 32:   8%|▊         | 25/300 [00:13<02:30,  1.83it/s]Epoch 32:   9%|▊         | 26/300 [00:14<02:33,  1.79it/s]Epoch 32:   9%|▉         | 27/300 [00:14<02:27,  1.85it/s]Epoch 32:   9%|▉         | 28/300 [00:15<02:23,  1.90it/s]Epoch 32:  10%|▉         | 29/300 [00:15<02:20,  1.92it/s]Epoch 32:  10%|█         | 30/300 [00:16<02:19,  1.94it/s]Epoch 32:  10%|█         | 31/300 [00:16<02:25,  1.85it/s]Epoch 32:  11%|█         | 32/300 [00:17<02:20,  1.90it/s]Epoch 32:  11%|█         | 33/300 [00:17<02:20,  1.89it/s]Epoch 32:  11%|█▏        | 34/300 [00:18<02:18,  1.92it/s]Epoch 32:  12%|█▏        | 35/300 [00:18<02:23,  1.85it/s]Epoch 32:  12%|█▏        | 36/300 [00:19<02:18,  1.90it/s]Epoch 32:  12%|█▏        | 37/300 [00:19<02:16,  1.93it/s]Epoch 32:  13%|█▎        | 38/300 [00:20<02:13,  1.96it/s]Epoch 32:  13%|█▎        | 39/300 [00:20<02:19,  1.87it/s]06/19/2022 21:11:42 - INFO - __main__ - global step: 4820; train loss: 6.5244903564453125; dev loss: 6.365143775939941
Epoch 32:  13%|█▎        | 40/300 [00:21<02:16,  1.91it/s]Epoch 32:  14%|█▎        | 41/300 [00:21<02:13,  1.94it/s]Epoch 32:  14%|█▍        | 42/300 [00:22<02:11,  1.97it/s]Epoch 32:  14%|█▍        | 43/300 [00:23<02:24,  1.78it/s]Epoch 32:  15%|█▍        | 44/300 [00:23<02:19,  1.84it/s]Epoch 32:  15%|█▌        | 45/300 [00:24<02:14,  1.89it/s]Epoch 32:  15%|█▌        | 46/300 [00:24<02:11,  1.93it/s]Epoch 32:  16%|█▌        | 47/300 [00:25<02:17,  1.85it/s]Epoch 32:  16%|█▌        | 48/300 [00:25<02:12,  1.90it/s]Epoch 32:  16%|█▋        | 49/300 [00:26<02:09,  1.94it/s]Epoch 32:  17%|█▋        | 50/300 [00:26<02:07,  1.96it/s]Epoch 32:  17%|█▋        | 51/300 [00:27<02:13,  1.86it/s]Epoch 32:  17%|█▋        | 52/300 [00:27<02:10,  1.89it/s]Epoch 32:  18%|█▊        | 53/300 [00:28<02:08,  1.93it/s]Epoch 32:  18%|█▊        | 54/300 [00:28<02:06,  1.95it/s]Epoch 32:  18%|█▊        | 55/300 [00:29<02:04,  1.97it/s]Epoch 32:  19%|█▊        | 56/300 [00:29<02:12,  1.84it/s]Epoch 32:  19%|█▉        | 57/300 [00:30<02:07,  1.90it/s]Epoch 32:  19%|█▉        | 58/300 [00:30<02:07,  1.89it/s]Epoch 32:  20%|█▉        | 59/300 [00:31<02:05,  1.92it/s]06/19/2022 21:11:53 - INFO - __main__ - global step: 4830; train loss: 6.519334316253662; dev loss: 6.321600914001465
Epoch 32:  20%|██        | 60/300 [00:31<02:10,  1.84it/s]Epoch 32:  20%|██        | 61/300 [00:32<02:05,  1.90it/s]Epoch 32:  21%|██        | 62/300 [00:32<02:03,  1.93it/s]Epoch 32:  21%|██        | 63/300 [00:33<02:01,  1.95it/s]Epoch 32:  21%|██▏       | 64/300 [00:34<02:08,  1.84it/s]Epoch 32:  22%|██▏       | 65/300 [00:34<02:04,  1.89it/s]Epoch 32:  22%|██▏       | 66/300 [00:35<02:04,  1.89it/s]Epoch 32:  22%|██▏       | 67/300 [00:35<02:07,  1.83it/s]Epoch 32:  23%|██▎       | 68/300 [00:36<02:12,  1.75it/s]Epoch 32:  23%|██▎       | 69/300 [00:36<02:06,  1.83it/s]Epoch 32:  23%|██▎       | 70/300 [00:37<02:02,  1.88it/s]Epoch 32:  24%|██▎       | 71/300 [00:37<02:05,  1.83it/s]Epoch 32:  24%|██▍       | 72/300 [00:38<02:07,  1.78it/s]Epoch 32:  24%|██▍       | 73/300 [00:38<02:02,  1.85it/s]Epoch 32:  25%|██▍       | 74/300 [00:39<01:59,  1.90it/s]Epoch 32:  25%|██▌       | 75/300 [00:39<01:56,  1.93it/s]Epoch 32:  25%|██▌       | 76/300 [00:40<02:01,  1.85it/s]Epoch 32:  26%|██▌       | 77/300 [00:40<01:57,  1.90it/s]Epoch 32:  26%|██▌       | 78/300 [00:41<01:54,  1.93it/s]Epoch 32:  26%|██▋       | 79/300 [00:41<01:53,  1.95it/s]06/19/2022 21:12:03 - INFO - __main__ - global step: 4840; train loss: 6.487344264984131; dev loss: 6.815676689147949
Epoch 32:  27%|██▋       | 80/300 [00:42<01:58,  1.86it/s]Epoch 32:  27%|██▋       | 81/300 [00:43<02:00,  1.82it/s]Epoch 32:  27%|██▋       | 82/300 [00:43<01:56,  1.88it/s]Epoch 32:  28%|██▊       | 83/300 [00:44<01:53,  1.92it/s]Epoch 32:  28%|██▊       | 84/300 [00:44<01:50,  1.95it/s]Epoch 32:  28%|██▊       | 85/300 [00:45<01:55,  1.87it/s]Epoch 32:  29%|██▊       | 86/300 [00:45<01:57,  1.82it/s]Epoch 32:  29%|██▉       | 87/300 [00:46<01:53,  1.88it/s]Epoch 32:  29%|██▉       | 88/300 [00:46<01:51,  1.91it/s]Epoch 32:  30%|██▉       | 89/300 [00:47<01:54,  1.84it/s]Epoch 32:  30%|███       | 90/300 [00:47<01:57,  1.79it/s]Epoch 32:  30%|███       | 91/300 [00:48<01:52,  1.86it/s]Epoch 32:  31%|███       | 92/300 [00:48<01:49,  1.90it/s]Epoch 32:  31%|███       | 93/300 [00:49<01:52,  1.84it/s]Epoch 32:  31%|███▏      | 94/300 [00:50<01:51,  1.85it/s]Epoch 32:  32%|███▏      | 95/300 [00:50<01:50,  1.85it/s]Epoch 32:  32%|███▏      | 96/300 [00:51<01:47,  1.90it/s]Epoch 32:  32%|███▏      | 97/300 [00:51<01:52,  1.80it/s]Epoch 32:  33%|███▎      | 98/300 [00:52<01:48,  1.86it/s]Epoch 32:  33%|███▎      | 99/300 [00:52<01:45,  1.91it/s]06/19/2022 21:12:14 - INFO - __main__ - global step: 4850; train loss: 6.461616516113281; dev loss: 6.774041652679443
Epoch 32:  33%|███▎      | 100/300 [00:53<01:43,  1.94it/s]Epoch 32:  34%|███▎      | 101/300 [00:53<01:47,  1.85it/s]Epoch 32:  34%|███▍      | 102/300 [00:54<01:44,  1.90it/s]Epoch 32:  34%|███▍      | 103/300 [00:54<01:41,  1.94it/s]Epoch 32:  35%|███▍      | 104/300 [00:55<01:40,  1.96it/s]Epoch 32:  35%|███▌      | 105/300 [00:56<01:50,  1.77it/s]Epoch 32:  35%|███▌      | 106/300 [00:56<01:45,  1.84it/s]Epoch 32:  36%|███▌      | 107/300 [00:56<01:41,  1.90it/s]Epoch 32:  36%|███▌      | 108/300 [00:57<01:39,  1.93it/s]Epoch 32:  36%|███▋      | 109/300 [00:57<01:37,  1.95it/s]Epoch 32:  37%|███▋      | 110/300 [00:58<01:41,  1.87it/s]Epoch 32:  37%|███▋      | 111/300 [00:59<01:41,  1.87it/s]Epoch 32:  37%|███▋      | 112/300 [00:59<01:43,  1.82it/s]Epoch 32:  38%|███▊      | 113/300 [01:00<01:39,  1.87it/s]Epoch 32:  38%|███▊      | 114/300 [01:00<01:42,  1.82it/s]Epoch 32:  38%|███▊      | 115/300 [01:01<01:40,  1.83it/s]Epoch 32:  39%|███▊      | 116/300 [01:01<01:37,  1.88it/s]Epoch 32:  39%|███▉      | 117/300 [01:02<01:35,  1.92it/s]Epoch 32:  39%|███▉      | 118/300 [01:02<01:42,  1.77it/s]Epoch 32:  40%|███▉      | 119/300 [01:03<01:40,  1.80it/s]06/19/2022 21:12:25 - INFO - __main__ - global step: 4860; train loss: 6.433072566986084; dev loss: 6.606248378753662
Epoch 32:  40%|████      | 120/300 [01:04<01:39,  1.81it/s]Epoch 32:  40%|████      | 121/300 [01:04<01:35,  1.87it/s]Epoch 32:  41%|████      | 122/300 [01:05<01:37,  1.82it/s]Epoch 32:  41%|████      | 123/300 [01:05<01:34,  1.87it/s]Epoch 32:  41%|████▏     | 124/300 [01:06<01:34,  1.86it/s]Epoch 32:  42%|████▏     | 125/300 [01:06<01:34,  1.86it/s]Epoch 32:  42%|████▏     | 126/300 [01:07<01:36,  1.81it/s]Epoch 32:  42%|████▏     | 127/300 [01:07<01:34,  1.83it/s]Epoch 32:  43%|████▎     | 128/300 [01:08<01:31,  1.88it/s]Epoch 32:  43%|████▎     | 129/300 [01:08<01:28,  1.92it/s]Epoch 32:  43%|████▎     | 130/300 [01:09<01:33,  1.81it/s]Epoch 32:  44%|████▎     | 131/300 [01:09<01:30,  1.87it/s]Epoch 32:  44%|████▍     | 132/300 [01:10<01:27,  1.91it/s]Epoch 32:  44%|████▍     | 133/300 [01:11<01:30,  1.85it/s]Epoch 32:  45%|████▍     | 134/300 [01:11<01:34,  1.76it/s]Epoch 32:  45%|████▌     | 135/300 [01:12<01:30,  1.82it/s]Epoch 32:  45%|████▌     | 136/300 [01:12<01:27,  1.88it/s]Epoch 32:  46%|████▌     | 137/300 [01:13<01:25,  1.91it/s]Epoch 32:  46%|████▌     | 138/300 [01:13<01:23,  1.94it/s]Epoch 32:  46%|████▋     | 139/300 [01:14<01:26,  1.85it/s]06/19/2022 21:12:35 - INFO - __main__ - global step: 4870; train loss: 6.012092590332031; dev loss: 5.99314022064209
Epoch 32:  47%|████▋     | 140/300 [01:14<01:28,  1.81it/s]Epoch 32:  47%|████▋     | 141/300 [01:15<01:24,  1.88it/s]Epoch 32:  47%|████▋     | 142/300 [01:15<01:26,  1.82it/s]Epoch 32:  48%|████▊     | 143/300 [01:16<01:27,  1.79it/s]Epoch 32:  48%|████▊     | 144/300 [01:17<01:28,  1.77it/s]Epoch 32:  48%|████▊     | 145/300 [01:17<01:25,  1.81it/s]Epoch 32:  49%|████▊     | 146/300 [01:18<01:22,  1.87it/s]Epoch 32:  49%|████▉     | 147/300 [01:18<01:23,  1.82it/s]Epoch 32:  49%|████▉     | 148/300 [01:19<01:20,  1.88it/s]Epoch 32:  50%|████▉     | 149/300 [01:19<01:18,  1.92it/s]Epoch 32:  50%|█████     | 150/300 [01:20<01:19,  1.90it/s]Epoch 32:  50%|█████     | 151/300 [01:20<01:21,  1.83it/s]Epoch 32:  51%|█████     | 152/300 [01:21<01:18,  1.88it/s]Epoch 32:  51%|█████     | 153/300 [01:21<01:16,  1.91it/s]Epoch 32:  51%|█████▏    | 154/300 [01:22<01:15,  1.94it/s]Epoch 32:  52%|█████▏    | 155/300 [01:22<01:18,  1.86it/s]Epoch 32:  52%|█████▏    | 156/300 [01:23<01:17,  1.86it/s]Epoch 32:  52%|█████▏    | 157/300 [01:23<01:18,  1.82it/s]Epoch 32:  53%|█████▎    | 158/300 [01:24<01:15,  1.88it/s]Epoch 32:  53%|█████▎    | 159/300 [01:25<01:17,  1.82it/s]06/19/2022 21:12:46 - INFO - __main__ - global step: 4880; train loss: 6.120730400085449; dev loss: 6.4911322593688965
Epoch 32:  53%|█████▎    | 160/300 [01:25<01:14,  1.87it/s]Epoch 32:  54%|█████▎    | 161/300 [01:26<01:16,  1.82it/s]Epoch 32:  54%|█████▍    | 162/300 [01:26<01:13,  1.88it/s]Epoch 32:  54%|█████▍    | 163/300 [01:27<01:11,  1.91it/s]Epoch 32:  55%|█████▍    | 164/300 [01:27<01:13,  1.84it/s]Epoch 32:  55%|█████▌    | 165/300 [01:28<01:13,  1.85it/s]Epoch 32:  55%|█████▌    | 166/300 [01:28<01:10,  1.89it/s]Epoch 32:  56%|█████▌    | 167/300 [01:29<01:09,  1.93it/s]Epoch 32:  56%|█████▌    | 168/300 [01:29<01:11,  1.86it/s]Epoch 32:  56%|█████▋    | 169/300 [01:30<01:08,  1.90it/s]Epoch 32:  57%|█████▋    | 170/300 [01:30<01:08,  1.89it/s]Epoch 32:  57%|█████▋    | 171/300 [01:31<01:06,  1.93it/s]Epoch 32:  57%|█████▋    | 172/300 [01:32<01:11,  1.78it/s]Epoch 32:  58%|█████▊    | 173/300 [01:32<01:10,  1.80it/s]Epoch 32:  58%|█████▊    | 174/300 [01:33<01:07,  1.86it/s]Epoch 32:  58%|█████▊    | 175/300 [01:33<01:05,  1.90it/s]Epoch 32:  59%|█████▊    | 176/300 [01:34<01:07,  1.83it/s]Epoch 32:  59%|█████▉    | 177/300 [01:34<01:05,  1.88it/s]Epoch 32:  59%|█████▉    | 178/300 [01:35<01:03,  1.92it/s]Epoch 32:  60%|█████▉    | 179/300 [01:35<01:03,  1.90it/s]06/19/2022 21:12:57 - INFO - __main__ - global step: 4890; train loss: 6.3055644035339355; dev loss: 6.274293422698975
Epoch 32:  60%|██████    | 180/300 [01:36<01:08,  1.75it/s]Epoch 32:  60%|██████    | 181/300 [01:36<01:05,  1.82it/s]Epoch 32:  61%|██████    | 182/300 [01:37<01:04,  1.83it/s]Epoch 32:  61%|██████    | 183/300 [01:37<01:02,  1.88it/s]Epoch 32:  61%|██████▏   | 184/300 [01:38<01:05,  1.78it/s]Epoch 32:  62%|██████▏   | 185/300 [01:39<01:02,  1.85it/s]Epoch 32:  62%|██████▏   | 186/300 [01:39<01:00,  1.90it/s]Epoch 32:  62%|██████▏   | 187/300 [01:40<00:58,  1.93it/s]Epoch 32:  63%|██████▎   | 188/300 [01:40<01:00,  1.84it/s]Epoch 32:  63%|██████▎   | 189/300 [01:41<01:00,  1.85it/s]Epoch 32:  63%|██████▎   | 190/300 [01:41<00:58,  1.89it/s]Epoch 32:  64%|██████▎   | 191/300 [01:42<00:56,  1.93it/s]Epoch 32:  64%|██████▍   | 192/300 [01:42<00:58,  1.86it/s]Epoch 32:  64%|██████▍   | 193/300 [01:43<01:00,  1.76it/s]Epoch 32:  65%|██████▍   | 194/300 [01:43<00:58,  1.82it/s]Epoch 32:  65%|██████▌   | 195/300 [01:44<00:58,  1.80it/s]Epoch 32:  65%|██████▌   | 196/300 [01:44<00:56,  1.86it/s]Epoch 32:  66%|██████▌   | 197/300 [01:45<00:59,  1.73it/s]Epoch 32:  66%|██████▌   | 198/300 [01:46<00:56,  1.80it/s]Epoch 32:  66%|██████▋   | 199/300 [01:46<00:54,  1.86it/s]06/19/2022 21:13:08 - INFO - __main__ - global step: 4900; train loss: 6.038567543029785; dev loss: 6.174251556396484
Epoch 32:  67%|██████▋   | 200/300 [01:47<00:52,  1.89it/s]Epoch 32:  67%|██████▋   | 201/300 [01:47<00:54,  1.83it/s]Epoch 32:  67%|██████▋   | 202/300 [01:48<00:52,  1.88it/s]Epoch 32:  68%|██████▊   | 203/300 [01:48<00:50,  1.92it/s]Epoch 32:  68%|██████▊   | 204/300 [01:49<00:49,  1.95it/s]Epoch 32:  68%|██████▊   | 205/300 [01:49<00:50,  1.86it/s]Epoch 32:  69%|██████▊   | 206/300 [01:50<00:50,  1.86it/s]Epoch 32:  69%|██████▉   | 207/300 [01:50<00:51,  1.82it/s]Epoch 32:  69%|██████▉   | 208/300 [01:51<00:49,  1.87it/s]Epoch 32:  70%|██████▉   | 209/300 [01:52<00:50,  1.82it/s]Epoch 32:  70%|███████   | 210/300 [01:52<00:48,  1.87it/s]Epoch 32:  70%|███████   | 211/300 [01:53<00:47,  1.87it/s]Epoch 32:  71%|███████   | 212/300 [01:53<00:46,  1.90it/s]Epoch 32:  71%|███████   | 213/300 [01:54<00:47,  1.84it/s]Epoch 32:  71%|███████▏  | 214/300 [01:54<00:45,  1.89it/s]Epoch 32:  72%|███████▏  | 215/300 [01:55<00:45,  1.88it/s]Epoch 32:  72%|███████▏  | 216/300 [01:55<00:43,  1.91it/s]Epoch 32:  72%|███████▏  | 217/300 [01:56<00:43,  1.89it/s]Epoch 32:  73%|███████▎  | 218/300 [01:56<00:44,  1.82it/s]Epoch 32:  73%|███████▎  | 219/300 [01:57<00:45,  1.79it/s]06/19/2022 21:13:18 - INFO - __main__ - global step: 4910; train loss: 6.4042534828186035; dev loss: 5.868975639343262
Epoch 32:  73%|███████▎  | 220/300 [01:57<00:43,  1.85it/s]Epoch 32:  74%|███████▎  | 221/300 [01:58<00:43,  1.81it/s]Epoch 32:  74%|███████▍  | 222/300 [01:59<00:45,  1.70it/s]Epoch 32:  74%|███████▍  | 223/300 [01:59<00:43,  1.77it/s]Epoch 32:  75%|███████▍  | 224/300 [02:00<00:41,  1.83it/s]Epoch 32:  75%|███████▌  | 225/300 [02:00<00:40,  1.87it/s]Epoch 32:  75%|███████▌  | 226/300 [02:01<00:40,  1.81it/s]Epoch 32:  76%|███████▌  | 227/300 [02:01<00:40,  1.82it/s]Epoch 32:  76%|███████▌  | 228/300 [02:02<00:38,  1.87it/s]Epoch 32:  76%|███████▋  | 229/300 [02:02<00:37,  1.91it/s]Epoch 32:  77%|███████▋  | 230/300 [02:03<00:38,  1.84it/s]Epoch 32:  77%|███████▋  | 231/300 [02:03<00:37,  1.85it/s]Epoch 32:  77%|███████▋  | 232/300 [02:04<00:35,  1.89it/s]Epoch 32:  78%|███████▊  | 233/300 [02:04<00:34,  1.92it/s]Epoch 32:  78%|███████▊  | 234/300 [02:05<00:35,  1.84it/s]Epoch 32:  78%|███████▊  | 235/300 [02:06<00:35,  1.85it/s]Epoch 32:  79%|███████▊  | 236/300 [02:06<00:35,  1.81it/s]Epoch 32:  79%|███████▉  | 237/300 [02:07<00:33,  1.87it/s]Epoch 32:  79%|███████▉  | 238/300 [02:07<00:35,  1.73it/s]Epoch 32:  80%|███████▉  | 239/300 [02:08<00:33,  1.81it/s]06/19/2022 21:13:29 - INFO - __main__ - global step: 4920; train loss: 6.967981815338135; dev loss: 6.9952826499938965
Epoch 32:  80%|████████  | 240/300 [02:08<00:33,  1.78it/s]Epoch 32:  80%|████████  | 241/300 [02:09<00:31,  1.85it/s]Epoch 32:  81%|████████  | 242/300 [02:10<00:33,  1.75it/s]Epoch 32:  81%|████████  | 243/300 [02:10<00:31,  1.83it/s]Epoch 32:  81%|████████▏ | 244/300 [02:11<00:30,  1.84it/s]Epoch 32:  82%|████████▏ | 245/300 [02:11<00:29,  1.88it/s]Epoch 32:  82%|████████▏ | 246/300 [02:12<00:28,  1.92it/s]Epoch 32:  82%|████████▏ | 247/300 [02:12<00:28,  1.85it/s]Epoch 32:  83%|████████▎ | 248/300 [02:13<00:27,  1.90it/s]Epoch 32:  83%|████████▎ | 249/300 [02:13<00:26,  1.94it/s]Epoch 32:  83%|████████▎ | 250/300 [02:14<00:26,  1.86it/s]Epoch 32:  84%|████████▎ | 251/300 [02:14<00:27,  1.81it/s]Epoch 32:  84%|████████▍ | 252/300 [02:15<00:25,  1.85it/s]Epoch 32:  84%|████████▍ | 253/300 [02:15<00:24,  1.90it/s]Epoch 32:  85%|████████▍ | 254/300 [02:16<00:24,  1.90it/s]Epoch 32:  85%|████████▌ | 255/300 [02:16<00:25,  1.79it/s]Epoch 32:  85%|████████▌ | 256/300 [02:17<00:24,  1.76it/s]Epoch 32:  86%|████████▌ | 257/300 [02:18<00:23,  1.83it/s]Epoch 32:  86%|████████▌ | 258/300 [02:18<00:22,  1.88it/s]Epoch 32:  86%|████████▋ | 259/300 [02:19<00:22,  1.82it/s]06/19/2022 21:13:40 - INFO - __main__ - global step: 4930; train loss: 6.299943447113037; dev loss: 6.376850128173828
Epoch 32:  87%|████████▋ | 260/300 [02:19<00:21,  1.87it/s]Epoch 32:  87%|████████▋ | 261/300 [02:20<00:20,  1.91it/s]Epoch 32:  87%|████████▋ | 262/300 [02:20<00:20,  1.85it/s]Epoch 32:  88%|████████▊ | 263/300 [02:21<00:20,  1.76it/s]Epoch 32:  88%|████████▊ | 264/300 [02:21<00:20,  1.79it/s]Epoch 32:  88%|████████▊ | 265/300 [02:22<00:19,  1.77it/s]Epoch 32:  89%|████████▊ | 266/300 [02:22<00:18,  1.83it/s]Epoch 32:  89%|████████▉ | 267/300 [02:23<00:18,  1.79it/s]Epoch 32:  89%|████████▉ | 268/300 [02:24<00:18,  1.77it/s]Epoch 32:  90%|████████▉ | 269/300 [02:24<00:16,  1.85it/s]Epoch 32:  90%|█████████ | 270/300 [02:25<00:16,  1.81it/s]Epoch 32:  90%|█████████ | 271/300 [02:25<00:15,  1.86it/s]Epoch 32:  91%|█████████ | 272/300 [02:26<00:16,  1.73it/s]Epoch 32:  91%|█████████ | 273/300 [02:26<00:14,  1.81it/s]Epoch 32:  91%|█████████▏| 274/300 [02:27<00:14,  1.78it/s]Epoch 32:  92%|█████████▏| 275/300 [02:27<00:13,  1.85it/s]Epoch 32:  92%|█████████▏| 276/300 [02:28<00:13,  1.80it/s]Epoch 32:  92%|█████████▏| 277/300 [02:29<00:12,  1.80it/s]Epoch 32:  93%|█████████▎| 278/300 [02:29<00:11,  1.86it/s]Epoch 32:  93%|█████████▎| 279/300 [02:30<00:11,  1.90it/s]06/19/2022 21:13:51 - INFO - __main__ - global step: 4940; train loss: 6.513774871826172; dev loss: 6.258692264556885
Epoch 32:  93%|█████████▎| 280/300 [02:30<00:10,  1.82it/s]Epoch 32:  94%|█████████▎| 281/300 [02:31<00:10,  1.85it/s]Epoch 32:  94%|█████████▍| 282/300 [02:31<00:09,  1.84it/s]Epoch 32:  94%|█████████▍| 283/300 [02:32<00:09,  1.88it/s]Epoch 32:  95%|█████████▍| 284/300 [02:32<00:08,  1.82it/s]Epoch 32:  95%|█████████▌| 285/300 [02:33<00:08,  1.87it/s]Epoch 32:  95%|█████████▌| 286/300 [02:33<00:07,  1.90it/s]Epoch 32:  96%|█████████▌| 287/300 [02:34<00:06,  1.93it/s]Epoch 32:  96%|█████████▌| 288/300 [02:34<00:06,  1.85it/s]Epoch 32:  96%|█████████▋| 289/300 [02:35<00:05,  1.84it/s]Epoch 32:  97%|█████████▋| 290/300 [02:35<00:05,  1.89it/s]Epoch 32:  97%|█████████▋| 291/300 [02:36<00:04,  1.89it/s]Epoch 32:  97%|█████████▋| 292/300 [02:37<00:04,  1.80it/s]Epoch 32:  98%|█████████▊| 293/300 [02:37<00:03,  1.86it/s]Epoch 32:  98%|█████████▊| 294/300 [02:38<00:03,  1.91it/s]Epoch 32:  98%|█████████▊| 295/300 [02:38<00:02,  1.88it/s]Epoch 32:  99%|█████████▊| 296/300 [02:39<00:02,  1.77it/s]Epoch 32:  99%|█████████▉| 297/300 [02:39<00:01,  1.84it/s]Epoch 32:  99%|█████████▉| 298/300 [02:40<00:01,  1.90it/s]Epoch 32: 100%|█████████▉| 299/300 [02:40<00:00,  1.93it/s]06/19/2022 21:14:02 - INFO - __main__ - global step: 4950; train loss: 7.024835109710693; dev loss: 6.851856231689453
Epoch 32: 100%|██████████| 300/300 [02:41<00:00,  1.90it/s]Epoch 32: 100%|██████████| 300/300 [02:41<00:00,  1.86it/s]
Epoch 33:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 33:   0%|          | 1/300 [00:00<02:58,  1.67it/s]Epoch 33:   1%|          | 2/300 [00:01<02:40,  1.85it/s]Epoch 33:   1%|          | 3/300 [00:01<02:33,  1.94it/s]Epoch 33:   1%|▏         | 4/300 [00:02<02:31,  1.96it/s]Epoch 33:   2%|▏         | 5/300 [00:02<02:39,  1.85it/s]Epoch 33:   2%|▏         | 6/300 [00:03<02:39,  1.85it/s]Epoch 33:   2%|▏         | 7/300 [00:03<02:36,  1.87it/s]Epoch 33:   3%|▎         | 8/300 [00:04<02:37,  1.85it/s]Epoch 33:   3%|▎         | 9/300 [00:04<02:42,  1.79it/s]Epoch 33:   3%|▎         | 10/300 [00:05<02:36,  1.85it/s]Epoch 33:   4%|▎         | 11/300 [00:05<02:32,  1.90it/s]Epoch 33:   4%|▍         | 12/300 [00:06<02:29,  1.93it/s]Epoch 33:   4%|▍         | 13/300 [00:06<02:34,  1.86it/s]Epoch 33:   5%|▍         | 14/300 [00:07<02:30,  1.90it/s]Epoch 33:   5%|▌         | 15/300 [00:07<02:29,  1.91it/s]Epoch 33:   5%|▌         | 16/300 [00:08<02:26,  1.94it/s]Epoch 33:   6%|▌         | 17/300 [00:09<02:36,  1.80it/s]Epoch 33:   6%|▌         | 18/300 [00:09<02:31,  1.86it/s]Epoch 33:   6%|▋         | 19/300 [00:10<02:27,  1.90it/s]06/19/2022 21:14:13 - INFO - __main__ - global step: 4960; train loss: 6.231184959411621; dev loss: 6.290963649749756
Epoch 33:   7%|▋         | 20/300 [00:10<02:28,  1.88it/s]Epoch 33:   7%|▋         | 21/300 [00:11<02:32,  1.83it/s]Epoch 33:   7%|▋         | 22/300 [00:11<02:27,  1.89it/s]Epoch 33:   8%|▊         | 23/300 [00:12<02:23,  1.93it/s]Epoch 33:   8%|▊         | 24/300 [00:12<02:23,  1.92it/s]Epoch 33:   8%|▊         | 25/300 [00:13<02:21,  1.95it/s]Epoch 33:   9%|▊         | 26/300 [00:13<02:27,  1.86it/s]Epoch 33:   9%|▉         | 27/300 [00:14<02:23,  1.91it/s]Epoch 33:   9%|▉         | 28/300 [00:14<02:22,  1.91it/s]Epoch 33:  10%|▉         | 29/300 [00:15<02:21,  1.91it/s]Epoch 33:  10%|█         | 30/300 [00:16<02:28,  1.82it/s]Epoch 33:  10%|█         | 31/300 [00:16<02:23,  1.88it/s]Epoch 33:  11%|█         | 32/300 [00:17<02:20,  1.91it/s]Epoch 33:  11%|█         | 33/300 [00:17<02:17,  1.94it/s]Epoch 33:  11%|█▏        | 34/300 [00:18<02:23,  1.86it/s]Epoch 33:  12%|█▏        | 35/300 [00:18<02:18,  1.91it/s]Epoch 33:  12%|█▏        | 36/300 [00:19<02:17,  1.91it/s]Epoch 33:  12%|█▏        | 37/300 [00:19<02:15,  1.95it/s]Epoch 33:  13%|█▎        | 38/300 [00:20<02:20,  1.86it/s]Epoch 33:  13%|█▎        | 39/300 [00:20<02:16,  1.91it/s]06/19/2022 21:14:23 - INFO - __main__ - global step: 4970; train loss: 6.712890625; dev loss: 7.01657772064209
Epoch 33:  13%|█▎        | 40/300 [00:21<02:14,  1.93it/s]Epoch 33:  14%|█▎        | 41/300 [00:21<02:16,  1.90it/s]Epoch 33:  14%|█▍        | 42/300 [00:22<02:20,  1.83it/s]Epoch 33:  14%|█▍        | 43/300 [00:22<02:17,  1.88it/s]Epoch 33:  15%|█▍        | 44/300 [00:23<02:14,  1.90it/s]Epoch 33:  15%|█▌        | 45/300 [00:23<02:11,  1.94it/s]Epoch 33:  15%|█▌        | 46/300 [00:24<02:16,  1.86it/s]Epoch 33:  16%|█▌        | 47/300 [00:24<02:16,  1.85it/s]Epoch 33:  16%|█▌        | 48/300 [00:25<02:12,  1.90it/s]Epoch 33:  16%|█▋        | 49/300 [00:25<02:10,  1.93it/s]Epoch 33:  17%|█▋        | 50/300 [00:26<02:15,  1.85it/s]Epoch 33:  17%|█▋        | 51/300 [00:27<02:11,  1.89it/s]Epoch 33:  17%|█▋        | 52/300 [00:27<02:11,  1.89it/s]Epoch 33:  18%|█▊        | 53/300 [00:28<02:10,  1.89it/s]Epoch 33:  18%|█▊        | 54/300 [00:28<02:07,  1.93it/s]Epoch 33:  18%|█▊        | 55/300 [00:29<02:12,  1.85it/s]Epoch 33:  19%|█▊        | 56/300 [00:29<02:10,  1.87it/s]Epoch 33:  19%|█▉        | 57/300 [00:30<02:06,  1.92it/s]Epoch 33:  19%|█▉        | 58/300 [00:30<02:04,  1.94it/s]Epoch 33:  20%|█▉        | 59/300 [00:31<02:08,  1.88it/s]06/19/2022 21:14:34 - INFO - __main__ - global step: 4980; train loss: 6.407256126403809; dev loss: 6.255767822265625
Epoch 33:  20%|██        | 60/300 [00:31<02:04,  1.92it/s]Epoch 33:  20%|██        | 61/300 [00:32<02:02,  1.96it/s]Epoch 33:  21%|██        | 62/300 [00:32<02:08,  1.85it/s]Epoch 33:  21%|██        | 63/300 [00:33<02:14,  1.76it/s]Epoch 33:  21%|██▏       | 64/300 [00:34<02:13,  1.77it/s]Epoch 33:  22%|██▏       | 65/300 [00:34<02:08,  1.84it/s]Epoch 33:  22%|██▏       | 66/300 [00:35<02:08,  1.82it/s]Epoch 33:  22%|██▏       | 67/300 [00:35<02:12,  1.76it/s]Epoch 33:  23%|██▎       | 68/300 [00:36<02:07,  1.82it/s]Epoch 33:  23%|██▎       | 69/300 [00:36<02:03,  1.87it/s]Epoch 33:  23%|██▎       | 70/300 [00:37<02:04,  1.85it/s]Epoch 33:  24%|██▎       | 71/300 [00:37<02:08,  1.78it/s]Epoch 33:  24%|██▍       | 72/300 [00:38<02:03,  1.84it/s]Epoch 33:  24%|██▍       | 73/300 [00:38<02:00,  1.88it/s]Epoch 33:  25%|██▍       | 74/300 [00:39<01:58,  1.91it/s]Epoch 33:  25%|██▌       | 75/300 [00:40<02:02,  1.83it/s]Epoch 33:  25%|██▌       | 76/300 [00:40<02:01,  1.84it/s]Epoch 33:  26%|██▌       | 77/300 [00:41<01:57,  1.90it/s]Epoch 33:  26%|██▌       | 78/300 [00:41<01:55,  1.93it/s]Epoch 33:  26%|██▋       | 79/300 [00:42<01:53,  1.96it/s]06/19/2022 21:14:45 - INFO - __main__ - global step: 4990; train loss: 5.631691932678223; dev loss: 5.859429836273193
Epoch 33:  27%|██▋       | 80/300 [00:42<01:57,  1.87it/s]Epoch 33:  27%|██▋       | 81/300 [00:43<01:55,  1.90it/s]Epoch 33:  27%|██▋       | 82/300 [00:43<01:56,  1.88it/s]Epoch 33:  28%|██▊       | 83/300 [00:44<01:53,  1.90it/s]Epoch 33:  28%|██▊       | 84/300 [00:44<01:58,  1.83it/s]Epoch 33:  28%|██▊       | 85/300 [00:45<01:54,  1.87it/s]Epoch 33:  29%|██▊       | 86/300 [00:45<01:52,  1.90it/s]Epoch 33:  29%|██▉       | 87/300 [00:46<01:50,  1.92it/s]Epoch 33:  29%|██▉       | 88/300 [00:46<01:57,  1.80it/s]Epoch 33:  30%|██▉       | 89/300 [00:47<01:54,  1.85it/s]Epoch 33:  30%|███       | 90/300 [00:47<01:52,  1.86it/s]Epoch 33:  30%|███       | 91/300 [00:48<01:49,  1.90it/s]Epoch 33:  31%|███       | 92/300 [00:49<01:53,  1.83it/s]Epoch 33:  31%|███       | 93/300 [00:49<01:50,  1.88it/s]Epoch 33:  31%|███▏      | 94/300 [00:50<01:47,  1.91it/s]Epoch 33:  32%|███▏      | 95/300 [00:50<01:45,  1.94it/s]Epoch 33:  32%|███▏      | 96/300 [00:51<01:50,  1.84it/s]Epoch 33:  32%|███▏      | 97/300 [00:51<01:47,  1.89it/s]Epoch 33:  33%|███▎      | 98/300 [00:52<01:45,  1.92it/s]Epoch 33:  33%|███▎      | 99/300 [00:52<01:44,  1.92it/s]06/19/2022 21:14:55 - INFO - __main__ - global step: 5000; train loss: 6.35556697845459; dev loss: 6.759245872497559
Epoch 33:  33%|███▎      | 99/300 [00:53<01:48,  1.86it/s]
06/19/2022 21:14:55 - INFO - __main__ - save model!
t5base para maml downstream
Task: glue-mrpc, Checkpoint: models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt, Identifier: T5-base-maml-nopara2para-3e-5-2-5000-5e-1
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta_nopara2para.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta_nopara2para.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-mrpc/log.txt'
06/19/2022 21:14:59 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-maml-nopara2para-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 21:14:59 - INFO - __main__ - models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-mrpc
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_meta_nopara2para.py', '--local_rank=1', '--task_dir', 'data/glue-mrpc/', '--task_name', 'glue-mrpc', '--identifier', 'T5-base-maml-nopara2para-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-mrpc', '--cuda', '2,3', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', '--model', 'google/t5-v1_1-base', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 88002
Killing subprocess 88003
++++++++++++++++++++++++++++++
kill: (88009): No such process
Task: glue-qqp, Checkpoint: models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt, Identifier: T5-base-maml-nopara2para-3e-5-2-5000-5e-1
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta_nopara2para.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta_nopara2para.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-qqp/log.txt'
06/19/2022 21:15:03 - INFO - __main__ - Namespace(task_dir='data/glue-qqp/', task_name='glue-qqp', identifier='T5-base-maml-nopara2para-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-qqp', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 21:15:03 - INFO - __main__ - models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-qqp
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_meta_nopara2para.py', '--local_rank=1', '--task_dir', 'data/glue-qqp/', '--task_name', 'glue-qqp', '--identifier', 'T5-base-maml-nopara2para-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-glue-qqp', '--cuda', '2,3', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', '--model', 'google/t5-v1_1-base', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 88014
Killing subprocess 88015
++++++++++++++++++++++++++++++
kill: (88019): No such process
Task: medical_questions_pairs, Checkpoint: models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt, Identifier: T5-base-maml-nopara2para-3e-5-2-5000-5e-1
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta_nopara2para.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta_nopara2para.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-medical_questions_pairs/log.txt'
06/19/2022 21:15:06 - INFO - __main__ - Namespace(task_dir='data/medical_questions_pairs/', task_name='medical_questions_pairs', identifier='T5-base-maml-nopara2para-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-medical_questions_pairs', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 21:15:06 - INFO - __main__ - models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-medical_questions_pairs
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_meta_nopara2para.py', '--local_rank=1', '--task_dir', 'data/medical_questions_pairs/', '--task_name', 'medical_questions_pairs', '--identifier', 'T5-base-maml-nopara2para-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-medical_questions_pairs', '--cuda', '2,3', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', '--model', 'google/t5-v1_1-base', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 88024
Killing subprocess 88025
++++++++++++++++++++++++++++++
kill: (88031): No such process
Task: paws, Checkpoint: models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt, Identifier: T5-base-maml-nopara2para-3e-5-2-5000-5e-1
06/19/2022 21:15:10 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-maml-nopara2para-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 21:15:10 - INFO - __main__ - models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws
06/19/2022 21:15:10 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-maml-nopara2para-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-nopara2para-3e-5-2-5000-5e-1-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
06/19/2022 21:15:10 - INFO - __main__ - models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws
06/19/2022 21:15:11 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/19/2022 21:15:11 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/19/2022 21:15:11 - INFO - __main__ - args.device: cuda:0
06/19/2022 21:15:11 - INFO - __main__ - Using 2 gpus
06/19/2022 21:15:11 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/19/2022 21:15:11 - INFO - __main__ - args.device: cuda:1
06/19/2022 21:15:11 - INFO - __main__ - Using 2 gpus
06/19/2022 21:15:11 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/19/2022 21:15:15 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.5, bsz=8 ...
06/19/2022 21:15:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:15:16 - INFO - __main__ - Printing 3 examples
06/19/2022 21:15:16 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 21:15:16 - INFO - __main__ - ['1']
06/19/2022 21:15:16 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 21:15:16 - INFO - __main__ - ['1']
06/19/2022 21:15:16 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 21:15:16 - INFO - __main__ - ['1']
06/19/2022 21:15:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:15:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:15:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:15:16 - INFO - __main__ - Printing 3 examples
06/19/2022 21:15:16 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 21:15:16 - INFO - __main__ - ['1']
06/19/2022 21:15:16 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 21:15:16 - INFO - __main__ - ['1']
06/19/2022 21:15:16 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 21:15:16 - INFO - __main__ - ['1']
06/19/2022 21:15:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:15:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:15:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:15:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:15:16 - INFO - __main__ - Printing 3 examples
06/19/2022 21:15:16 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 21:15:16 - INFO - __main__ - ['1']
06/19/2022 21:15:16 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 21:15:16 - INFO - __main__ - ['1']
06/19/2022 21:15:16 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 21:15:16 - INFO - __main__ - ['1']
06/19/2022 21:15:16 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:15:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:15:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:15:16 - INFO - __main__ - Printing 3 examples
06/19/2022 21:15:16 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 21:15:16 - INFO - __main__ - ['1']
06/19/2022 21:15:16 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 21:15:16 - INFO - __main__ - ['1']
06/19/2022 21:15:16 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 21:15:16 - INFO - __main__ - ['1']
06/19/2022 21:15:16 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:15:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:15:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:15:16 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:15:16 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:15:22 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:15:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:15:22 - INFO - __main__ - Starting training!
06/19/2022 21:15:23 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:15:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:15:28 - INFO - __main__ - Starting training!
06/19/2022 21:15:30 - INFO - __main__ - Step 10 Global step 10 Train loss 5.89 on epoch=4
06/19/2022 21:15:32 - INFO - __main__ - Step 20 Global step 20 Train loss 5.26 on epoch=9
06/19/2022 21:15:33 - INFO - __main__ - Step 30 Global step 30 Train loss 4.65 on epoch=14
06/19/2022 21:15:34 - INFO - __main__ - Step 40 Global step 40 Train loss 4.28 on epoch=19
06/19/2022 21:15:35 - INFO - __main__ - Step 50 Global step 50 Train loss 4.08 on epoch=24
06/19/2022 21:15:42 - INFO - __main__ - Global step 50 Train loss 4.83 Classification-F1 0.0 on epoch=24
06/19/2022 21:15:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 21:15:43 - INFO - __main__ - Step 60 Global step 60 Train loss 3.87 on epoch=29
06/19/2022 21:15:44 - INFO - __main__ - Step 70 Global step 70 Train loss 3.79 on epoch=34
06/19/2022 21:15:46 - INFO - __main__ - Step 80 Global step 80 Train loss 3.70 on epoch=39
06/19/2022 21:15:47 - INFO - __main__ - Step 90 Global step 90 Train loss 3.40 on epoch=44
06/19/2022 21:15:48 - INFO - __main__ - Step 100 Global step 100 Train loss 3.31 on epoch=49
06/19/2022 21:15:55 - INFO - __main__ - Global step 100 Train loss 3.61 Classification-F1 0.0 on epoch=49
06/19/2022 21:15:56 - INFO - __main__ - Step 110 Global step 110 Train loss 3.27 on epoch=54
06/19/2022 21:15:57 - INFO - __main__ - Step 120 Global step 120 Train loss 3.06 on epoch=59
06/19/2022 21:15:59 - INFO - __main__ - Step 130 Global step 130 Train loss 3.04 on epoch=64
06/19/2022 21:16:00 - INFO - __main__ - Step 140 Global step 140 Train loss 3.06 on epoch=69
06/19/2022 21:16:01 - INFO - __main__ - Step 150 Global step 150 Train loss 2.80 on epoch=74
06/19/2022 21:16:02 - INFO - __main__ - Global step 150 Train loss 3.05 Classification-F1 0.12444444444444444 on epoch=74
06/19/2022 21:16:03 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.12444444444444444 on epoch=74, global_step=150
06/19/2022 21:16:04 - INFO - __main__ - Step 160 Global step 160 Train loss 2.64 on epoch=79
06/19/2022 21:16:05 - INFO - __main__ - Step 170 Global step 170 Train loss 2.59 on epoch=84
06/19/2022 21:16:06 - INFO - __main__ - Step 180 Global step 180 Train loss 2.44 on epoch=89
06/19/2022 21:16:08 - INFO - __main__ - Step 190 Global step 190 Train loss 2.19 on epoch=94
06/19/2022 21:16:09 - INFO - __main__ - Step 200 Global step 200 Train loss 2.23 on epoch=99
06/19/2022 21:16:10 - INFO - __main__ - Global step 200 Train loss 2.42 Classification-F1 0.5151515151515151 on epoch=99
06/19/2022 21:16:10 - INFO - __main__ - Saving model with best Classification-F1: 0.12444444444444444 -> 0.5151515151515151 on epoch=99, global_step=200
06/19/2022 21:16:11 - INFO - __main__ - Step 210 Global step 210 Train loss 2.06 on epoch=104
06/19/2022 21:16:12 - INFO - __main__ - Step 220 Global step 220 Train loss 2.08 on epoch=109
06/19/2022 21:16:13 - INFO - __main__ - Step 230 Global step 230 Train loss 2.05 on epoch=114
06/19/2022 21:16:15 - INFO - __main__ - Step 240 Global step 240 Train loss 1.89 on epoch=119
06/19/2022 21:16:16 - INFO - __main__ - Step 250 Global step 250 Train loss 1.98 on epoch=124
06/19/2022 21:16:18 - INFO - __main__ - Global step 250 Train loss 2.01 Classification-F1 0.4181818181818182 on epoch=124
06/19/2022 21:16:19 - INFO - __main__ - Step 260 Global step 260 Train loss 1.74 on epoch=129
06/19/2022 21:16:20 - INFO - __main__ - Step 270 Global step 270 Train loss 1.69 on epoch=134
06/19/2022 21:16:21 - INFO - __main__ - Step 280 Global step 280 Train loss 1.77 on epoch=139
06/19/2022 21:16:23 - INFO - __main__ - Step 290 Global step 290 Train loss 1.62 on epoch=144
06/19/2022 21:16:24 - INFO - __main__ - Step 300 Global step 300 Train loss 1.65 on epoch=149
06/19/2022 21:16:24 - INFO - __main__ - Global step 300 Train loss 1.69 Classification-F1 0.4682306940371457 on epoch=149
06/19/2022 21:16:26 - INFO - __main__ - Step 310 Global step 310 Train loss 1.54 on epoch=154
06/19/2022 21:16:27 - INFO - __main__ - Step 320 Global step 320 Train loss 1.47 on epoch=159
06/19/2022 21:16:28 - INFO - __main__ - Step 330 Global step 330 Train loss 1.53 on epoch=164
06/19/2022 21:16:29 - INFO - __main__ - Step 340 Global step 340 Train loss 1.35 on epoch=169
06/19/2022 21:16:31 - INFO - __main__ - Step 350 Global step 350 Train loss 1.34 on epoch=174
06/19/2022 21:16:31 - INFO - __main__ - Global step 350 Train loss 1.45 Classification-F1 0.4920634920634921 on epoch=174
06/19/2022 21:16:32 - INFO - __main__ - Step 360 Global step 360 Train loss 1.26 on epoch=179
06/19/2022 21:16:33 - INFO - __main__ - Step 370 Global step 370 Train loss 1.34 on epoch=184
06/19/2022 21:16:35 - INFO - __main__ - Step 380 Global step 380 Train loss 1.37 on epoch=189
06/19/2022 21:16:36 - INFO - __main__ - Step 390 Global step 390 Train loss 1.29 on epoch=194
06/19/2022 21:16:37 - INFO - __main__ - Step 400 Global step 400 Train loss 1.43 on epoch=199
06/19/2022 21:16:38 - INFO - __main__ - Global step 400 Train loss 1.34 Classification-F1 0.3522267206477733 on epoch=199
06/19/2022 21:16:39 - INFO - __main__ - Step 410 Global step 410 Train loss 1.11 on epoch=204
06/19/2022 21:16:40 - INFO - __main__ - Step 420 Global step 420 Train loss 1.28 on epoch=209
06/19/2022 21:16:41 - INFO - __main__ - Step 430 Global step 430 Train loss 1.15 on epoch=214
06/19/2022 21:16:43 - INFO - __main__ - Step 440 Global step 440 Train loss 1.21 on epoch=219
06/19/2022 21:16:44 - INFO - __main__ - Step 450 Global step 450 Train loss 1.14 on epoch=224
06/19/2022 21:16:44 - INFO - __main__ - Global step 450 Train loss 1.18 Classification-F1 0.46843853820598 on epoch=224
06/19/2022 21:16:45 - INFO - __main__ - Step 460 Global step 460 Train loss 1.08 on epoch=229
06/19/2022 21:16:47 - INFO - __main__ - Step 470 Global step 470 Train loss 1.19 on epoch=234
06/19/2022 21:16:48 - INFO - __main__ - Step 480 Global step 480 Train loss 1.06 on epoch=239
06/19/2022 21:16:49 - INFO - __main__ - Step 490 Global step 490 Train loss 1.03 on epoch=244
06/19/2022 21:16:50 - INFO - __main__ - Step 500 Global step 500 Train loss 1.03 on epoch=249
06/19/2022 21:16:51 - INFO - __main__ - Global step 500 Train loss 1.08 Classification-F1 0.46843853820598 on epoch=249
06/19/2022 21:16:52 - INFO - __main__ - Step 510 Global step 510 Train loss 1.00 on epoch=254
06/19/2022 21:16:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.97 on epoch=259
06/19/2022 21:16:54 - INFO - __main__ - Step 530 Global step 530 Train loss 1.02 on epoch=264
06/19/2022 21:16:56 - INFO - __main__ - Step 540 Global step 540 Train loss 1.02 on epoch=269
06/19/2022 21:16:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.95 on epoch=274
06/19/2022 21:16:57 - INFO - __main__ - Global step 550 Train loss 0.99 Classification-F1 0.36374269005847953 on epoch=274
06/19/2022 21:16:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.90 on epoch=279
06/19/2022 21:17:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.91 on epoch=284
06/19/2022 21:17:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.99 on epoch=289
06/19/2022 21:17:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.77 on epoch=294
06/19/2022 21:17:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.89 on epoch=299
06/19/2022 21:17:04 - INFO - __main__ - Global step 600 Train loss 0.89 Classification-F1 0.30158730158730157 on epoch=299
06/19/2022 21:17:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.98 on epoch=304
06/19/2022 21:17:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.93 on epoch=309
06/19/2022 21:17:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.86 on epoch=314
06/19/2022 21:17:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.91 on epoch=319
06/19/2022 21:17:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.88 on epoch=324
06/19/2022 21:17:11 - INFO - __main__ - Global step 650 Train loss 0.91 Classification-F1 0.5307917888563051 on epoch=324
06/19/2022 21:17:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.5307917888563051 on epoch=324, global_step=650
06/19/2022 21:17:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.82 on epoch=329
06/19/2022 21:17:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.85 on epoch=334
06/19/2022 21:17:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.81 on epoch=339
06/19/2022 21:17:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.85 on epoch=344
06/19/2022 21:17:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.80 on epoch=349
06/19/2022 21:17:17 - INFO - __main__ - Global step 700 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 21:17:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.75 on epoch=354
06/19/2022 21:17:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.76 on epoch=359
06/19/2022 21:17:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.84 on epoch=364
06/19/2022 21:17:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.86 on epoch=369
06/19/2022 21:17:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.79 on epoch=374
06/19/2022 21:17:24 - INFO - __main__ - Global step 750 Train loss 0.80 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 21:17:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=379
06/19/2022 21:17:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.84 on epoch=384
06/19/2022 21:17:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.82 on epoch=389
06/19/2022 21:17:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.77 on epoch=394
06/19/2022 21:17:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.84 on epoch=399
06/19/2022 21:17:31 - INFO - __main__ - Global step 800 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 21:17:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.84 on epoch=404
06/19/2022 21:17:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.85 on epoch=409
06/19/2022 21:17:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.76 on epoch=414
06/19/2022 21:17:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.64 on epoch=419
06/19/2022 21:17:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.73 on epoch=424
06/19/2022 21:17:37 - INFO - __main__ - Global step 850 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 21:17:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.70 on epoch=429
06/19/2022 21:17:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.79 on epoch=434
06/19/2022 21:17:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.68 on epoch=439
06/19/2022 21:17:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.76 on epoch=444
06/19/2022 21:17:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.76 on epoch=449
06/19/2022 21:17:44 - INFO - __main__ - Global step 900 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 21:17:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.65 on epoch=454
06/19/2022 21:17:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.74 on epoch=459
06/19/2022 21:17:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.85 on epoch=464
06/19/2022 21:17:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.67 on epoch=469
06/19/2022 21:17:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.60 on epoch=474
06/19/2022 21:17:50 - INFO - __main__ - Global step 950 Train loss 0.70 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 21:17:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.71 on epoch=479
06/19/2022 21:17:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.70 on epoch=484
06/19/2022 21:17:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.77 on epoch=489
06/19/2022 21:17:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.68 on epoch=494
06/19/2022 21:17:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.73 on epoch=499
06/19/2022 21:17:57 - INFO - __main__ - Global step 1000 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 21:17:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.70 on epoch=504
06/19/2022 21:17:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.80 on epoch=509
06/19/2022 21:18:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.68 on epoch=514
06/19/2022 21:18:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.69 on epoch=519
06/19/2022 21:18:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.80 on epoch=524
06/19/2022 21:18:03 - INFO - __main__ - Global step 1050 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 21:18:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.71 on epoch=529
06/19/2022 21:18:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.74 on epoch=534
06/19/2022 21:18:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.68 on epoch=539
06/19/2022 21:18:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.57 on epoch=544
06/19/2022 21:18:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.52 on epoch=549
06/19/2022 21:18:10 - INFO - __main__ - Global step 1100 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 21:18:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.65 on epoch=554
06/19/2022 21:18:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.65 on epoch=559
06/19/2022 21:18:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.67 on epoch=564
06/19/2022 21:18:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.60 on epoch=569
06/19/2022 21:18:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.71 on epoch=574
06/19/2022 21:18:16 - INFO - __main__ - Global step 1150 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 21:18:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.63 on epoch=579
06/19/2022 21:18:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.65 on epoch=584
06/19/2022 21:18:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.67 on epoch=589
06/19/2022 21:18:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.63 on epoch=594
06/19/2022 21:18:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.62 on epoch=599
06/19/2022 21:18:23 - INFO - __main__ - Global step 1200 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 21:18:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.67 on epoch=604
06/19/2022 21:18:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.52 on epoch=609
06/19/2022 21:18:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.53 on epoch=614
06/19/2022 21:18:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.64 on epoch=619
06/19/2022 21:18:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.61 on epoch=624
06/19/2022 21:18:29 - INFO - __main__ - Global step 1250 Train loss 0.59 Classification-F1 0.3191489361702127 on epoch=624
06/19/2022 21:18:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.72 on epoch=629
06/19/2022 21:18:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=634
06/19/2022 21:18:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.64 on epoch=639
06/19/2022 21:18:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.59 on epoch=644
06/19/2022 21:18:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.68 on epoch=649
06/19/2022 21:18:36 - INFO - __main__ - Global step 1300 Train loss 0.62 Classification-F1 0.3191489361702127 on epoch=649
06/19/2022 21:18:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.63 on epoch=654
06/19/2022 21:18:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.53 on epoch=659
06/19/2022 21:18:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.60 on epoch=664
06/19/2022 21:18:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.73 on epoch=669
06/19/2022 21:18:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.62 on epoch=674
06/19/2022 21:18:42 - INFO - __main__ - Global step 1350 Train loss 0.62 Classification-F1 0.3992490613266583 on epoch=674
06/19/2022 21:18:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.59 on epoch=679
06/19/2022 21:18:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.58 on epoch=684
06/19/2022 21:18:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.53 on epoch=689
06/19/2022 21:18:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.56 on epoch=694
06/19/2022 21:18:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.63 on epoch=699
06/19/2022 21:18:49 - INFO - __main__ - Global step 1400 Train loss 0.58 Classification-F1 0.36374269005847953 on epoch=699
06/19/2022 21:18:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.54 on epoch=704
06/19/2022 21:18:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.56 on epoch=709
06/19/2022 21:18:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.65 on epoch=714
06/19/2022 21:18:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.55 on epoch=719
06/19/2022 21:18:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.62 on epoch=724
06/19/2022 21:18:56 - INFO - __main__ - Global step 1450 Train loss 0.58 Classification-F1 0.3816425120772947 on epoch=724
06/19/2022 21:18:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.67 on epoch=729
06/19/2022 21:18:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.54 on epoch=734
06/19/2022 21:18:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.58 on epoch=739
06/19/2022 21:19:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.49 on epoch=744
06/19/2022 21:19:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.47 on epoch=749
06/19/2022 21:19:02 - INFO - __main__ - Global step 1500 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 21:19:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.59 on epoch=754
06/19/2022 21:19:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.61 on epoch=759
06/19/2022 21:19:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.61 on epoch=764
06/19/2022 21:19:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.66 on epoch=769
06/19/2022 21:19:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.54 on epoch=774
06/19/2022 21:19:09 - INFO - __main__ - Global step 1550 Train loss 0.60 Classification-F1 0.3191489361702127 on epoch=774
06/19/2022 21:19:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.54 on epoch=779
06/19/2022 21:19:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.60 on epoch=784
06/19/2022 21:19:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.56 on epoch=789
06/19/2022 21:19:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.57 on epoch=794
06/19/2022 21:19:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.52 on epoch=799
06/19/2022 21:19:15 - INFO - __main__ - Global step 1600 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 21:19:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.58 on epoch=804
06/19/2022 21:19:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.57 on epoch=809
06/19/2022 21:19:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.60 on epoch=814
06/19/2022 21:19:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.66 on epoch=819
06/19/2022 21:19:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.60 on epoch=824
06/19/2022 21:19:22 - INFO - __main__ - Global step 1650 Train loss 0.60 Classification-F1 0.3816425120772947 on epoch=824
06/19/2022 21:19:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.54 on epoch=829
06/19/2022 21:19:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.61 on epoch=834
06/19/2022 21:19:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.56 on epoch=839
06/19/2022 21:19:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.57 on epoch=844
06/19/2022 21:19:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=849
06/19/2022 21:19:28 - INFO - __main__ - Global step 1700 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 21:19:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.53 on epoch=854
06/19/2022 21:19:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.59 on epoch=859
06/19/2022 21:19:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.62 on epoch=864
06/19/2022 21:19:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.55 on epoch=869
06/19/2022 21:19:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.64 on epoch=874
06/19/2022 21:19:35 - INFO - __main__ - Global step 1750 Train loss 0.59 Classification-F1 0.5933528836754642 on epoch=874
06/19/2022 21:19:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5307917888563051 -> 0.5933528836754642 on epoch=874, global_step=1750
06/19/2022 21:19:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.59 on epoch=879
06/19/2022 21:19:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.57 on epoch=884
06/19/2022 21:19:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.55 on epoch=889
06/19/2022 21:19:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.50 on epoch=894
06/19/2022 21:19:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.49 on epoch=899
06/19/2022 21:19:41 - INFO - __main__ - Global step 1800 Train loss 0.54 Classification-F1 0.3191489361702127 on epoch=899
06/19/2022 21:19:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.66 on epoch=904
06/19/2022 21:19:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.61 on epoch=909
06/19/2022 21:19:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.60 on epoch=914
06/19/2022 21:19:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.51 on epoch=919
06/19/2022 21:19:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=924
06/19/2022 21:19:48 - INFO - __main__ - Global step 1850 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 21:19:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.60 on epoch=929
06/19/2022 21:19:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.50 on epoch=934
06/19/2022 21:19:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.53 on epoch=939
06/19/2022 21:19:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.53 on epoch=944
06/19/2022 21:19:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.55 on epoch=949
06/19/2022 21:19:55 - INFO - __main__ - Global step 1900 Train loss 0.54 Classification-F1 0.4385964912280702 on epoch=949
06/19/2022 21:19:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.63 on epoch=954
06/19/2022 21:19:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.57 on epoch=959
06/19/2022 21:19:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.54 on epoch=964
06/19/2022 21:20:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.52 on epoch=969
06/19/2022 21:20:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.65 on epoch=974
06/19/2022 21:20:01 - INFO - __main__ - Global step 1950 Train loss 0.58 Classification-F1 0.36374269005847953 on epoch=974
06/19/2022 21:20:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.52 on epoch=979
06/19/2022 21:20:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.52 on epoch=984
06/19/2022 21:20:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.49 on epoch=989
06/19/2022 21:20:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.52 on epoch=994
06/19/2022 21:20:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.52 on epoch=999
06/19/2022 21:20:08 - INFO - __main__ - Global step 2000 Train loss 0.51 Classification-F1 0.3043478260869565 on epoch=999
06/19/2022 21:20:08 - INFO - __main__ - save last model!
06/19/2022 21:20:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 21:20:08 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 21:20:08 - INFO - __main__ - Printing 3 examples
06/19/2022 21:20:08 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 21:20:08 - INFO - __main__ - ['0']
06/19/2022 21:20:08 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 21:20:08 - INFO - __main__ - ['1']
06/19/2022 21:20:08 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 21:20:08 - INFO - __main__ - ['1']
06/19/2022 21:20:08 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:20:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:20:08 - INFO - __main__ - Printing 3 examples
06/19/2022 21:20:08 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 21:20:08 - INFO - __main__ - ['1']
06/19/2022 21:20:08 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 21:20:08 - INFO - __main__ - ['1']
06/19/2022 21:20:08 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 21:20:08 - INFO - __main__ - ['1']
06/19/2022 21:20:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:20:08 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:20:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:20:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:20:08 - INFO - __main__ - Printing 3 examples
06/19/2022 21:20:08 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 21:20:08 - INFO - __main__ - ['1']
06/19/2022 21:20:08 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 21:20:08 - INFO - __main__ - ['1']
06/19/2022 21:20:08 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 21:20:08 - INFO - __main__ - ['1']
06/19/2022 21:20:08 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:20:08 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:20:09 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:20:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:20:14 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:20:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:20:15 - INFO - __main__ - Starting training!
06/19/2022 21:20:20 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 21:21:45 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_100_0.5_8_predictions.txt
06/19/2022 21:21:45 - INFO - __main__ - Classification-F1 on test data: 0.3454
06/19/2022 21:21:45 - INFO - __main__ - prefix=paws_16_100, lr=0.5, bsz=8, dev_performance=0.5933528836754642, test_performance=0.3454412997631778
06/19/2022 21:21:45 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.4, bsz=8 ...
06/19/2022 21:21:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:21:46 - INFO - __main__ - Printing 3 examples
06/19/2022 21:21:46 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 21:21:46 - INFO - __main__ - ['1']
06/19/2022 21:21:46 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 21:21:46 - INFO - __main__ - ['1']
06/19/2022 21:21:46 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 21:21:46 - INFO - __main__ - ['1']
06/19/2022 21:21:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:21:46 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:21:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:21:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:21:46 - INFO - __main__ - Printing 3 examples
06/19/2022 21:21:46 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 21:21:46 - INFO - __main__ - ['1']
06/19/2022 21:21:46 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 21:21:46 - INFO - __main__ - ['1']
06/19/2022 21:21:46 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 21:21:46 - INFO - __main__ - ['1']
06/19/2022 21:21:46 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:21:46 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:21:46 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:21:52 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:21:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:21:53 - INFO - __main__ - Starting training!
06/19/2022 21:21:54 - INFO - __main__ - Step 10 Global step 10 Train loss 5.98 on epoch=4
06/19/2022 21:21:56 - INFO - __main__ - Step 20 Global step 20 Train loss 5.49 on epoch=9
06/19/2022 21:21:57 - INFO - __main__ - Step 30 Global step 30 Train loss 4.74 on epoch=14
06/19/2022 21:21:59 - INFO - __main__ - Step 40 Global step 40 Train loss 4.60 on epoch=19
06/19/2022 21:22:00 - INFO - __main__ - Step 50 Global step 50 Train loss 4.35 on epoch=24
06/19/2022 21:22:01 - INFO - __main__ - Global step 50 Train loss 5.03 Classification-F1 0.0 on epoch=24
06/19/2022 21:22:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 21:22:02 - INFO - __main__ - Step 60 Global step 60 Train loss 4.22 on epoch=29
06/19/2022 21:22:04 - INFO - __main__ - Step 70 Global step 70 Train loss 4.14 on epoch=34
06/19/2022 21:22:05 - INFO - __main__ - Step 80 Global step 80 Train loss 4.01 on epoch=39
06/19/2022 21:22:06 - INFO - __main__ - Step 90 Global step 90 Train loss 3.90 on epoch=44
06/19/2022 21:22:08 - INFO - __main__ - Step 100 Global step 100 Train loss 3.81 on epoch=49
06/19/2022 21:22:15 - INFO - __main__ - Global step 100 Train loss 4.02 Classification-F1 0.0 on epoch=49
06/19/2022 21:22:16 - INFO - __main__ - Step 110 Global step 110 Train loss 3.83 on epoch=54
06/19/2022 21:22:18 - INFO - __main__ - Step 120 Global step 120 Train loss 3.74 on epoch=59
06/19/2022 21:22:19 - INFO - __main__ - Step 130 Global step 130 Train loss 3.66 on epoch=64
06/19/2022 21:22:20 - INFO - __main__ - Step 140 Global step 140 Train loss 3.37 on epoch=69
06/19/2022 21:22:22 - INFO - __main__ - Step 150 Global step 150 Train loss 3.42 on epoch=74
06/19/2022 21:22:24 - INFO - __main__ - Global step 150 Train loss 3.60 Classification-F1 0.0 on epoch=74
06/19/2022 21:22:25 - INFO - __main__ - Step 160 Global step 160 Train loss 3.26 on epoch=79
06/19/2022 21:22:26 - INFO - __main__ - Step 170 Global step 170 Train loss 3.25 on epoch=84
06/19/2022 21:22:28 - INFO - __main__ - Step 180 Global step 180 Train loss 3.24 on epoch=89
06/19/2022 21:22:29 - INFO - __main__ - Step 190 Global step 190 Train loss 3.15 on epoch=94
06/19/2022 21:22:30 - INFO - __main__ - Step 200 Global step 200 Train loss 2.88 on epoch=99
06/19/2022 21:22:32 - INFO - __main__ - Global step 200 Train loss 3.16 Classification-F1 0.10784313725490197 on epoch=99
06/19/2022 21:22:32 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.10784313725490197 on epoch=99, global_step=200
06/19/2022 21:22:33 - INFO - __main__ - Step 210 Global step 210 Train loss 2.98 on epoch=104
06/19/2022 21:22:34 - INFO - __main__ - Step 220 Global step 220 Train loss 2.74 on epoch=109
06/19/2022 21:22:36 - INFO - __main__ - Step 230 Global step 230 Train loss 2.70 on epoch=114
06/19/2022 21:22:37 - INFO - __main__ - Step 240 Global step 240 Train loss 2.61 on epoch=119
06/19/2022 21:22:38 - INFO - __main__ - Step 250 Global step 250 Train loss 2.65 on epoch=124
06/19/2022 21:22:39 - INFO - __main__ - Global step 250 Train loss 2.73 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 21:22:39 - INFO - __main__ - Saving model with best Classification-F1: 0.10784313725490197 -> 0.3333333333333333 on epoch=124, global_step=250
06/19/2022 21:22:40 - INFO - __main__ - Step 260 Global step 260 Train loss 2.63 on epoch=129
06/19/2022 21:22:41 - INFO - __main__ - Step 270 Global step 270 Train loss 2.65 on epoch=134
06/19/2022 21:22:42 - INFO - __main__ - Step 280 Global step 280 Train loss 2.63 on epoch=139
06/19/2022 21:22:44 - INFO - __main__ - Step 290 Global step 290 Train loss 2.51 on epoch=144
06/19/2022 21:22:45 - INFO - __main__ - Step 300 Global step 300 Train loss 2.35 on epoch=149
06/19/2022 21:22:45 - INFO - __main__ - Global step 300 Train loss 2.55 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 21:22:47 - INFO - __main__ - Step 310 Global step 310 Train loss 2.19 on epoch=154
06/19/2022 21:22:48 - INFO - __main__ - Step 320 Global step 320 Train loss 2.13 on epoch=159
06/19/2022 21:22:49 - INFO - __main__ - Step 330 Global step 330 Train loss 2.26 on epoch=164
06/19/2022 21:22:51 - INFO - __main__ - Step 340 Global step 340 Train loss 1.95 on epoch=169
06/19/2022 21:22:52 - INFO - __main__ - Step 350 Global step 350 Train loss 2.09 on epoch=174
06/19/2022 21:22:52 - INFO - __main__ - Global step 350 Train loss 2.13 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 21:22:54 - INFO - __main__ - Step 360 Global step 360 Train loss 1.86 on epoch=179
06/19/2022 21:22:55 - INFO - __main__ - Step 370 Global step 370 Train loss 1.70 on epoch=184
06/19/2022 21:22:56 - INFO - __main__ - Step 380 Global step 380 Train loss 1.79 on epoch=189
06/19/2022 21:22:57 - INFO - __main__ - Step 390 Global step 390 Train loss 1.88 on epoch=194
06/19/2022 21:22:59 - INFO - __main__ - Step 400 Global step 400 Train loss 1.66 on epoch=199
06/19/2022 21:22:59 - INFO - __main__ - Global step 400 Train loss 1.78 Classification-F1 0.46843853820598 on epoch=199
06/19/2022 21:22:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.46843853820598 on epoch=199, global_step=400
06/19/2022 21:23:00 - INFO - __main__ - Step 410 Global step 410 Train loss 1.50 on epoch=204
06/19/2022 21:23:02 - INFO - __main__ - Step 420 Global step 420 Train loss 1.68 on epoch=209
06/19/2022 21:23:03 - INFO - __main__ - Step 430 Global step 430 Train loss 1.63 on epoch=214
06/19/2022 21:23:04 - INFO - __main__ - Step 440 Global step 440 Train loss 1.54 on epoch=219
06/19/2022 21:23:06 - INFO - __main__ - Step 450 Global step 450 Train loss 1.57 on epoch=224
06/19/2022 21:23:06 - INFO - __main__ - Global step 450 Train loss 1.59 Classification-F1 0.39999999999999997 on epoch=224
06/19/2022 21:23:07 - INFO - __main__ - Step 460 Global step 460 Train loss 1.29 on epoch=229
06/19/2022 21:23:09 - INFO - __main__ - Step 470 Global step 470 Train loss 1.53 on epoch=234
06/19/2022 21:23:10 - INFO - __main__ - Step 480 Global step 480 Train loss 1.37 on epoch=239
06/19/2022 21:23:11 - INFO - __main__ - Step 490 Global step 490 Train loss 1.34 on epoch=244
06/19/2022 21:23:12 - INFO - __main__ - Step 500 Global step 500 Train loss 1.25 on epoch=249
06/19/2022 21:23:13 - INFO - __main__ - Global step 500 Train loss 1.36 Classification-F1 0.5588547189819725 on epoch=249
06/19/2022 21:23:13 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5588547189819725 on epoch=249, global_step=500
06/19/2022 21:23:14 - INFO - __main__ - Step 510 Global step 510 Train loss 1.31 on epoch=254
06/19/2022 21:23:15 - INFO - __main__ - Step 520 Global step 520 Train loss 1.37 on epoch=259
06/19/2022 21:23:17 - INFO - __main__ - Step 530 Global step 530 Train loss 1.41 on epoch=264
06/19/2022 21:23:18 - INFO - __main__ - Step 540 Global step 540 Train loss 1.29 on epoch=269
06/19/2022 21:23:19 - INFO - __main__ - Step 550 Global step 550 Train loss 1.24 on epoch=274
06/19/2022 21:23:20 - INFO - __main__ - Global step 550 Train loss 1.32 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 21:23:21 - INFO - __main__ - Step 560 Global step 560 Train loss 1.25 on epoch=279
06/19/2022 21:23:22 - INFO - __main__ - Step 570 Global step 570 Train loss 1.11 on epoch=284
06/19/2022 21:23:23 - INFO - __main__ - Step 580 Global step 580 Train loss 1.22 on epoch=289
06/19/2022 21:23:25 - INFO - __main__ - Step 590 Global step 590 Train loss 1.10 on epoch=294
06/19/2022 21:23:26 - INFO - __main__ - Step 600 Global step 600 Train loss 1.20 on epoch=299
06/19/2022 21:23:26 - INFO - __main__ - Global step 600 Train loss 1.18 Classification-F1 0.5607843137254902 on epoch=299
06/19/2022 21:23:26 - INFO - __main__ - Saving model with best Classification-F1: 0.5588547189819725 -> 0.5607843137254902 on epoch=299, global_step=600
06/19/2022 21:23:28 - INFO - __main__ - Step 610 Global step 610 Train loss 1.12 on epoch=304
06/19/2022 21:23:29 - INFO - __main__ - Step 620 Global step 620 Train loss 1.17 on epoch=309
06/19/2022 21:23:30 - INFO - __main__ - Step 630 Global step 630 Train loss 1.05 on epoch=314
06/19/2022 21:23:31 - INFO - __main__ - Step 640 Global step 640 Train loss 1.19 on epoch=319
06/19/2022 21:23:33 - INFO - __main__ - Step 650 Global step 650 Train loss 1.09 on epoch=324
06/19/2022 21:23:33 - INFO - __main__ - Global step 650 Train loss 1.12 Classification-F1 0.39756367663344405 on epoch=324
06/19/2022 21:23:34 - INFO - __main__ - Step 660 Global step 660 Train loss 1.09 on epoch=329
06/19/2022 21:23:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.99 on epoch=334
06/19/2022 21:23:37 - INFO - __main__ - Step 680 Global step 680 Train loss 1.07 on epoch=339
06/19/2022 21:23:38 - INFO - __main__ - Step 690 Global step 690 Train loss 1.10 on epoch=344
06/19/2022 21:23:39 - INFO - __main__ - Step 700 Global step 700 Train loss 1.07 on epoch=349
06/19/2022 21:23:40 - INFO - __main__ - Global step 700 Train loss 1.07 Classification-F1 0.5588547189819725 on epoch=349
06/19/2022 21:23:41 - INFO - __main__ - Step 710 Global step 710 Train loss 1.03 on epoch=354
06/19/2022 21:23:42 - INFO - __main__ - Step 720 Global step 720 Train loss 1.03 on epoch=359
06/19/2022 21:23:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.97 on epoch=364
06/19/2022 21:23:45 - INFO - __main__ - Step 740 Global step 740 Train loss 1.01 on epoch=369
06/19/2022 21:23:46 - INFO - __main__ - Step 750 Global step 750 Train loss 1.04 on epoch=374
06/19/2022 21:23:47 - INFO - __main__ - Global step 750 Train loss 1.01 Classification-F1 0.4375 on epoch=374
06/19/2022 21:23:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.88 on epoch=379
06/19/2022 21:23:49 - INFO - __main__ - Step 770 Global step 770 Train loss 1.11 on epoch=384
06/19/2022 21:23:51 - INFO - __main__ - Step 780 Global step 780 Train loss 1.00 on epoch=389
06/19/2022 21:23:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.95 on epoch=394
06/19/2022 21:23:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.96 on epoch=399
06/19/2022 21:23:53 - INFO - __main__ - Global step 800 Train loss 0.98 Classification-F1 0.3454545454545454 on epoch=399
06/19/2022 21:23:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.87 on epoch=404
06/19/2022 21:23:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.94 on epoch=409
06/19/2022 21:23:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.92 on epoch=414
06/19/2022 21:23:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.94 on epoch=419
06/19/2022 21:24:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.91 on epoch=424
06/19/2022 21:24:00 - INFO - __main__ - Global step 850 Train loss 0.92 Classification-F1 0.4554554554554554 on epoch=424
06/19/2022 21:24:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.96 on epoch=429
06/19/2022 21:24:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.94 on epoch=434
06/19/2022 21:24:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.70 on epoch=439
06/19/2022 21:24:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.93 on epoch=444
06/19/2022 21:24:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.83 on epoch=449
06/19/2022 21:24:07 - INFO - __main__ - Global step 900 Train loss 0.87 Classification-F1 0.39756367663344405 on epoch=449
06/19/2022 21:24:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.96 on epoch=454
06/19/2022 21:24:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.86 on epoch=459
06/19/2022 21:24:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.87 on epoch=464
06/19/2022 21:24:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.76 on epoch=469
06/19/2022 21:24:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.80 on epoch=474
06/19/2022 21:24:14 - INFO - __main__ - Global step 950 Train loss 0.85 Classification-F1 0.39756367663344405 on epoch=474
06/19/2022 21:24:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.80 on epoch=479
06/19/2022 21:24:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.79 on epoch=484
06/19/2022 21:24:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.73 on epoch=489
06/19/2022 21:24:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.72 on epoch=494
06/19/2022 21:24:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.89 on epoch=499
06/19/2022 21:24:20 - INFO - __main__ - Global step 1000 Train loss 0.79 Classification-F1 0.37662337662337664 on epoch=499
06/19/2022 21:24:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.84 on epoch=504
06/19/2022 21:24:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.77 on epoch=509
06/19/2022 21:24:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.73 on epoch=514
06/19/2022 21:24:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.76 on epoch=519
06/19/2022 21:24:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.78 on epoch=524
06/19/2022 21:24:27 - INFO - __main__ - Global step 1050 Train loss 0.78 Classification-F1 0.4420512820512821 on epoch=524
06/19/2022 21:24:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.76 on epoch=529
06/19/2022 21:24:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.74 on epoch=534
06/19/2022 21:24:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.70 on epoch=539
06/19/2022 21:24:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.74 on epoch=544
06/19/2022 21:24:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.74 on epoch=549
06/19/2022 21:24:34 - INFO - __main__ - Global step 1100 Train loss 0.74 Classification-F1 0.3043478260869565 on epoch=549
06/19/2022 21:24:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.81 on epoch=554
06/19/2022 21:24:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.64 on epoch=559
06/19/2022 21:24:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.73 on epoch=564
06/19/2022 21:24:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.76 on epoch=569
06/19/2022 21:24:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.82 on epoch=574
06/19/2022 21:24:41 - INFO - __main__ - Global step 1150 Train loss 0.75 Classification-F1 0.539313399778516 on epoch=574
06/19/2022 21:24:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.67 on epoch=579
06/19/2022 21:24:43 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.76 on epoch=584
06/19/2022 21:24:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.68 on epoch=589
06/19/2022 21:24:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.70 on epoch=594
06/19/2022 21:24:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.77 on epoch=599
06/19/2022 21:24:47 - INFO - __main__ - Global step 1200 Train loss 0.72 Classification-F1 0.3191489361702127 on epoch=599
06/19/2022 21:24:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.76 on epoch=604
06/19/2022 21:24:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.72 on epoch=609
06/19/2022 21:24:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.69 on epoch=614
06/19/2022 21:24:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.62 on epoch=619
06/19/2022 21:24:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.71 on epoch=624
06/19/2022 21:24:54 - INFO - __main__ - Global step 1250 Train loss 0.70 Classification-F1 0.28888888888888886 on epoch=624
06/19/2022 21:24:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.77 on epoch=629
06/19/2022 21:24:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.69 on epoch=634
06/19/2022 21:24:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.76 on epoch=639
06/19/2022 21:24:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.72 on epoch=644
06/19/2022 21:25:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.72 on epoch=649
06/19/2022 21:25:01 - INFO - __main__ - Global step 1300 Train loss 0.73 Classification-F1 0.3522267206477733 on epoch=649
06/19/2022 21:25:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.72 on epoch=654
06/19/2022 21:25:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.79 on epoch=659
06/19/2022 21:25:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.70 on epoch=664
06/19/2022 21:25:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.61 on epoch=669
06/19/2022 21:25:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.64 on epoch=674
06/19/2022 21:25:08 - INFO - __main__ - Global step 1350 Train loss 0.69 Classification-F1 0.4285714285714286 on epoch=674
06/19/2022 21:25:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.60 on epoch=679
06/19/2022 21:25:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.62 on epoch=684
06/19/2022 21:25:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.58 on epoch=689
06/19/2022 21:25:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.66 on epoch=694
06/19/2022 21:25:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.61 on epoch=699
06/19/2022 21:25:14 - INFO - __main__ - Global step 1400 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 21:25:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.65 on epoch=704
06/19/2022 21:25:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.67 on epoch=709
06/19/2022 21:25:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.59 on epoch=714
06/19/2022 21:25:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.55 on epoch=719
06/19/2022 21:25:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.57 on epoch=724
06/19/2022 21:25:21 - INFO - __main__ - Global step 1450 Train loss 0.61 Classification-F1 0.4385964912280702 on epoch=724
06/19/2022 21:25:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.66 on epoch=729
06/19/2022 21:25:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.66 on epoch=734
06/19/2022 21:25:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.61 on epoch=739
06/19/2022 21:25:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.61 on epoch=744
06/19/2022 21:25:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.63 on epoch=749
06/19/2022 21:25:28 - INFO - __main__ - Global step 1500 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 21:25:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.67 on epoch=754
06/19/2022 21:25:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.66 on epoch=759
06/19/2022 21:25:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.60 on epoch=764
06/19/2022 21:25:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.62 on epoch=769
06/19/2022 21:25:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.52 on epoch=774
06/19/2022 21:25:35 - INFO - __main__ - Global step 1550 Train loss 0.61 Classification-F1 0.4181818181818182 on epoch=774
06/19/2022 21:25:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.52 on epoch=779
06/19/2022 21:25:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.54 on epoch=784
06/19/2022 21:25:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.58 on epoch=789
06/19/2022 21:25:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.54 on epoch=794
06/19/2022 21:25:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.60 on epoch=799
06/19/2022 21:25:41 - INFO - __main__ - Global step 1600 Train loss 0.56 Classification-F1 0.5555555555555556 on epoch=799
06/19/2022 21:25:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.61 on epoch=804
06/19/2022 21:25:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.63 on epoch=809
06/19/2022 21:25:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.60 on epoch=814
06/19/2022 21:25:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.63 on epoch=819
06/19/2022 21:25:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.56 on epoch=824
06/19/2022 21:25:48 - INFO - __main__ - Global step 1650 Train loss 0.60 Classification-F1 0.4920634920634921 on epoch=824
06/19/2022 21:25:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=829
06/19/2022 21:25:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.59 on epoch=834
06/19/2022 21:25:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.61 on epoch=839
06/19/2022 21:25:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.52 on epoch=844
06/19/2022 21:25:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.70 on epoch=849
06/19/2022 21:25:55 - INFO - __main__ - Global step 1700 Train loss 0.58 Classification-F1 0.41700404858299595 on epoch=849
06/19/2022 21:25:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.53 on epoch=854
06/19/2022 21:25:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.60 on epoch=859
06/19/2022 21:25:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.58 on epoch=864
06/19/2022 21:26:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.57 on epoch=869
06/19/2022 21:26:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.59 on epoch=874
06/19/2022 21:26:02 - INFO - __main__ - Global step 1750 Train loss 0.57 Classification-F1 0.3191489361702127 on epoch=874
06/19/2022 21:26:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.62 on epoch=879
06/19/2022 21:26:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.53 on epoch=884
06/19/2022 21:26:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.62 on epoch=889
06/19/2022 21:26:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.66 on epoch=894
06/19/2022 21:26:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.58 on epoch=899
06/19/2022 21:26:08 - INFO - __main__ - Global step 1800 Train loss 0.60 Classification-F1 0.4385964912280702 on epoch=899
06/19/2022 21:26:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.59 on epoch=904
06/19/2022 21:26:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.56 on epoch=909
06/19/2022 21:26:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.55 on epoch=914
06/19/2022 21:26:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.58 on epoch=919
06/19/2022 21:26:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.62 on epoch=924
06/19/2022 21:26:15 - INFO - __main__ - Global step 1850 Train loss 0.58 Classification-F1 0.4181818181818182 on epoch=924
06/19/2022 21:26:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.58 on epoch=929
06/19/2022 21:26:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.50 on epoch=934
06/19/2022 21:26:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.53 on epoch=939
06/19/2022 21:26:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.53 on epoch=944
06/19/2022 21:26:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.54 on epoch=949
06/19/2022 21:26:22 - INFO - __main__ - Global step 1900 Train loss 0.54 Classification-F1 0.464039408866995 on epoch=949
06/19/2022 21:26:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.55 on epoch=954
06/19/2022 21:26:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.54 on epoch=959
06/19/2022 21:26:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.57 on epoch=964
06/19/2022 21:26:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.58 on epoch=969
06/19/2022 21:26:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=974
06/19/2022 21:26:29 - INFO - __main__ - Global step 1950 Train loss 0.55 Classification-F1 0.4554554554554554 on epoch=974
06/19/2022 21:26:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.61 on epoch=979
06/19/2022 21:26:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.48 on epoch=984
06/19/2022 21:26:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.51 on epoch=989
06/19/2022 21:26:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.59 on epoch=994
06/19/2022 21:26:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.52 on epoch=999
06/19/2022 21:26:35 - INFO - __main__ - Global step 2000 Train loss 0.54 Classification-F1 0.37662337662337664 on epoch=999
06/19/2022 21:26:35 - INFO - __main__ - save last model!
06/19/2022 21:26:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 21:26:36 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 21:26:36 - INFO - __main__ - Printing 3 examples
06/19/2022 21:26:36 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 21:26:36 - INFO - __main__ - ['0']
06/19/2022 21:26:36 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 21:26:36 - INFO - __main__ - ['1']
06/19/2022 21:26:36 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 21:26:36 - INFO - __main__ - ['1']
06/19/2022 21:26:36 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:26:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:26:36 - INFO - __main__ - Printing 3 examples
06/19/2022 21:26:36 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 21:26:36 - INFO - __main__ - ['1']
06/19/2022 21:26:36 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 21:26:36 - INFO - __main__ - ['1']
06/19/2022 21:26:36 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 21:26:36 - INFO - __main__ - ['1']
06/19/2022 21:26:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:26:36 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:26:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:26:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:26:36 - INFO - __main__ - Printing 3 examples
06/19/2022 21:26:36 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 21:26:36 - INFO - __main__ - ['1']
06/19/2022 21:26:36 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 21:26:36 - INFO - __main__ - ['1']
06/19/2022 21:26:36 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 21:26:36 - INFO - __main__ - ['1']
06/19/2022 21:26:36 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:26:36 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:26:36 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:26:40 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:26:42 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:26:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:26:43 - INFO - __main__ - Starting training!
06/19/2022 21:26:47 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 21:28:17 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_100_0.4_8_predictions.txt
06/19/2022 21:28:17 - INFO - __main__ - Classification-F1 on test data: 0.4450
06/19/2022 21:28:17 - INFO - __main__ - prefix=paws_16_100, lr=0.4, bsz=8, dev_performance=0.5607843137254902, test_performance=0.44498182537485376
06/19/2022 21:28:17 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.3, bsz=8 ...
06/19/2022 21:28:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:28:18 - INFO - __main__ - Printing 3 examples
06/19/2022 21:28:18 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 21:28:18 - INFO - __main__ - ['1']
06/19/2022 21:28:18 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 21:28:18 - INFO - __main__ - ['1']
06/19/2022 21:28:18 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 21:28:18 - INFO - __main__ - ['1']
06/19/2022 21:28:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:28:18 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:28:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:28:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:28:18 - INFO - __main__ - Printing 3 examples
06/19/2022 21:28:18 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 21:28:18 - INFO - __main__ - ['1']
06/19/2022 21:28:18 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 21:28:18 - INFO - __main__ - ['1']
06/19/2022 21:28:18 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 21:28:18 - INFO - __main__ - ['1']
06/19/2022 21:28:18 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:28:18 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:28:18 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:28:24 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:28:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:28:24 - INFO - __main__ - Starting training!
06/19/2022 21:28:26 - INFO - __main__ - Step 10 Global step 10 Train loss 5.96 on epoch=4
06/19/2022 21:28:27 - INFO - __main__ - Step 20 Global step 20 Train loss 5.61 on epoch=9
06/19/2022 21:28:28 - INFO - __main__ - Step 30 Global step 30 Train loss 5.16 on epoch=14
06/19/2022 21:28:29 - INFO - __main__ - Step 40 Global step 40 Train loss 4.63 on epoch=19
06/19/2022 21:28:31 - INFO - __main__ - Step 50 Global step 50 Train loss 4.46 on epoch=24
06/19/2022 21:28:34 - INFO - __main__ - Global step 50 Train loss 5.16 Classification-F1 0.0 on epoch=24
06/19/2022 21:28:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 21:28:35 - INFO - __main__ - Step 60 Global step 60 Train loss 4.31 on epoch=29
06/19/2022 21:28:36 - INFO - __main__ - Step 70 Global step 70 Train loss 4.05 on epoch=34
06/19/2022 21:28:38 - INFO - __main__ - Step 80 Global step 80 Train loss 3.98 on epoch=39
06/19/2022 21:28:39 - INFO - __main__ - Step 90 Global step 90 Train loss 3.81 on epoch=44
06/19/2022 21:28:40 - INFO - __main__ - Step 100 Global step 100 Train loss 3.69 on epoch=49
06/19/2022 21:28:51 - INFO - __main__ - Global step 100 Train loss 3.97 Classification-F1 0.0 on epoch=49
06/19/2022 21:28:53 - INFO - __main__ - Step 110 Global step 110 Train loss 3.94 on epoch=54
06/19/2022 21:28:54 - INFO - __main__ - Step 120 Global step 120 Train loss 3.70 on epoch=59
06/19/2022 21:28:55 - INFO - __main__ - Step 130 Global step 130 Train loss 3.64 on epoch=64
06/19/2022 21:28:56 - INFO - __main__ - Step 140 Global step 140 Train loss 3.51 on epoch=69
06/19/2022 21:28:57 - INFO - __main__ - Step 150 Global step 150 Train loss 3.39 on epoch=74
06/19/2022 21:29:04 - INFO - __main__ - Global step 150 Train loss 3.64 Classification-F1 0.0 on epoch=74
06/19/2022 21:29:05 - INFO - __main__ - Step 160 Global step 160 Train loss 3.27 on epoch=79
06/19/2022 21:29:06 - INFO - __main__ - Step 170 Global step 170 Train loss 3.35 on epoch=84
06/19/2022 21:29:07 - INFO - __main__ - Step 180 Global step 180 Train loss 3.31 on epoch=89
06/19/2022 21:29:09 - INFO - __main__ - Step 190 Global step 190 Train loss 3.25 on epoch=94
06/19/2022 21:29:10 - INFO - __main__ - Step 200 Global step 200 Train loss 3.14 on epoch=99
06/19/2022 21:29:18 - INFO - __main__ - Global step 200 Train loss 3.26 Classification-F1 0.045454545454545456 on epoch=99
06/19/2022 21:29:18 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.045454545454545456 on epoch=99, global_step=200
06/19/2022 21:29:19 - INFO - __main__ - Step 210 Global step 210 Train loss 3.09 on epoch=104
06/19/2022 21:29:20 - INFO - __main__ - Step 220 Global step 220 Train loss 3.03 on epoch=109
06/19/2022 21:29:22 - INFO - __main__ - Step 230 Global step 230 Train loss 2.86 on epoch=114
06/19/2022 21:29:23 - INFO - __main__ - Step 240 Global step 240 Train loss 2.79 on epoch=119
06/19/2022 21:29:24 - INFO - __main__ - Step 250 Global step 250 Train loss 2.71 on epoch=124
06/19/2022 21:29:27 - INFO - __main__ - Global step 250 Train loss 2.89 Classification-F1 0.0755650882692625 on epoch=124
06/19/2022 21:29:27 - INFO - __main__ - Saving model with best Classification-F1: 0.045454545454545456 -> 0.0755650882692625 on epoch=124, global_step=250
06/19/2022 21:29:28 - INFO - __main__ - Step 260 Global step 260 Train loss 2.75 on epoch=129
06/19/2022 21:29:30 - INFO - __main__ - Step 270 Global step 270 Train loss 2.69 on epoch=134
06/19/2022 21:29:31 - INFO - __main__ - Step 280 Global step 280 Train loss 2.47 on epoch=139
06/19/2022 21:29:32 - INFO - __main__ - Step 290 Global step 290 Train loss 2.68 on epoch=144
06/19/2022 21:29:34 - INFO - __main__ - Step 300 Global step 300 Train loss 2.49 on epoch=149
06/19/2022 21:29:36 - INFO - __main__ - Global step 300 Train loss 2.62 Classification-F1 0.08225108225108226 on epoch=149
06/19/2022 21:29:36 - INFO - __main__ - Saving model with best Classification-F1: 0.0755650882692625 -> 0.08225108225108226 on epoch=149, global_step=300
06/19/2022 21:29:37 - INFO - __main__ - Step 310 Global step 310 Train loss 2.32 on epoch=154
06/19/2022 21:29:39 - INFO - __main__ - Step 320 Global step 320 Train loss 2.35 on epoch=159
06/19/2022 21:29:40 - INFO - __main__ - Step 330 Global step 330 Train loss 2.13 on epoch=164
06/19/2022 21:29:41 - INFO - __main__ - Step 340 Global step 340 Train loss 2.10 on epoch=169
06/19/2022 21:29:43 - INFO - __main__ - Step 350 Global step 350 Train loss 2.12 on epoch=174
06/19/2022 21:29:45 - INFO - __main__ - Global step 350 Train loss 2.20 Classification-F1 0.17978021978021977 on epoch=174
06/19/2022 21:29:45 - INFO - __main__ - Saving model with best Classification-F1: 0.08225108225108226 -> 0.17978021978021977 on epoch=174, global_step=350
06/19/2022 21:29:46 - INFO - __main__ - Step 360 Global step 360 Train loss 2.16 on epoch=179
06/19/2022 21:29:48 - INFO - __main__ - Step 370 Global step 370 Train loss 2.14 on epoch=184
06/19/2022 21:29:49 - INFO - __main__ - Step 380 Global step 380 Train loss 2.10 on epoch=189
06/19/2022 21:29:50 - INFO - __main__ - Step 390 Global step 390 Train loss 1.93 on epoch=194
06/19/2022 21:29:52 - INFO - __main__ - Step 400 Global step 400 Train loss 1.84 on epoch=199
06/19/2022 21:29:53 - INFO - __main__ - Global step 400 Train loss 2.03 Classification-F1 0.5333333333333333 on epoch=199
06/19/2022 21:29:53 - INFO - __main__ - Saving model with best Classification-F1: 0.17978021978021977 -> 0.5333333333333333 on epoch=199, global_step=400
06/19/2022 21:29:55 - INFO - __main__ - Step 410 Global step 410 Train loss 1.82 on epoch=204
06/19/2022 21:29:56 - INFO - __main__ - Step 420 Global step 420 Train loss 1.80 on epoch=209
06/19/2022 21:29:57 - INFO - __main__ - Step 430 Global step 430 Train loss 1.81 on epoch=214
06/19/2022 21:29:59 - INFO - __main__ - Step 440 Global step 440 Train loss 1.74 on epoch=219
06/19/2022 21:30:00 - INFO - __main__ - Step 450 Global step 450 Train loss 1.77 on epoch=224
06/19/2022 21:30:00 - INFO - __main__ - Global step 450 Train loss 1.79 Classification-F1 0.3816425120772947 on epoch=224
06/19/2022 21:30:02 - INFO - __main__ - Step 460 Global step 460 Train loss 1.73 on epoch=229
06/19/2022 21:30:03 - INFO - __main__ - Step 470 Global step 470 Train loss 1.64 on epoch=234
06/19/2022 21:30:04 - INFO - __main__ - Step 480 Global step 480 Train loss 1.69 on epoch=239
06/19/2022 21:30:05 - INFO - __main__ - Step 490 Global step 490 Train loss 1.66 on epoch=244
06/19/2022 21:30:07 - INFO - __main__ - Step 500 Global step 500 Train loss 1.45 on epoch=249
06/19/2022 21:30:07 - INFO - __main__ - Global step 500 Train loss 1.63 Classification-F1 0.36374269005847953 on epoch=249
06/19/2022 21:30:08 - INFO - __main__ - Step 510 Global step 510 Train loss 1.51 on epoch=254
06/19/2022 21:30:10 - INFO - __main__ - Step 520 Global step 520 Train loss 1.48 on epoch=259
06/19/2022 21:30:11 - INFO - __main__ - Step 530 Global step 530 Train loss 1.42 on epoch=264
06/19/2022 21:30:12 - INFO - __main__ - Step 540 Global step 540 Train loss 1.35 on epoch=269
06/19/2022 21:30:13 - INFO - __main__ - Step 550 Global step 550 Train loss 1.42 on epoch=274
06/19/2022 21:30:14 - INFO - __main__ - Global step 550 Train loss 1.44 Classification-F1 0.3992490613266583 on epoch=274
06/19/2022 21:30:15 - INFO - __main__ - Step 560 Global step 560 Train loss 1.34 on epoch=279
06/19/2022 21:30:16 - INFO - __main__ - Step 570 Global step 570 Train loss 1.40 on epoch=284
06/19/2022 21:30:18 - INFO - __main__ - Step 580 Global step 580 Train loss 1.25 on epoch=289
06/19/2022 21:30:19 - INFO - __main__ - Step 590 Global step 590 Train loss 1.40 on epoch=294
06/19/2022 21:30:20 - INFO - __main__ - Step 600 Global step 600 Train loss 1.27 on epoch=299
06/19/2022 21:30:20 - INFO - __main__ - Global step 600 Train loss 1.33 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 21:30:22 - INFO - __main__ - Step 610 Global step 610 Train loss 1.18 on epoch=304
06/19/2022 21:30:23 - INFO - __main__ - Step 620 Global step 620 Train loss 1.10 on epoch=309
06/19/2022 21:30:24 - INFO - __main__ - Step 630 Global step 630 Train loss 1.09 on epoch=314
06/19/2022 21:30:25 - INFO - __main__ - Step 640 Global step 640 Train loss 1.14 on epoch=319
06/19/2022 21:30:27 - INFO - __main__ - Step 650 Global step 650 Train loss 1.15 on epoch=324
06/19/2022 21:30:27 - INFO - __main__ - Global step 650 Train loss 1.13 Classification-F1 0.3992490613266583 on epoch=324
06/19/2022 21:30:28 - INFO - __main__ - Step 660 Global step 660 Train loss 1.18 on epoch=329
06/19/2022 21:30:30 - INFO - __main__ - Step 670 Global step 670 Train loss 1.09 on epoch=334
06/19/2022 21:30:31 - INFO - __main__ - Step 680 Global step 680 Train loss 1.14 on epoch=339
06/19/2022 21:30:32 - INFO - __main__ - Step 690 Global step 690 Train loss 1.10 on epoch=344
06/19/2022 21:30:33 - INFO - __main__ - Step 700 Global step 700 Train loss 1.10 on epoch=349
06/19/2022 21:30:34 - INFO - __main__ - Global step 700 Train loss 1.12 Classification-F1 0.3816425120772947 on epoch=349
06/19/2022 21:30:35 - INFO - __main__ - Step 710 Global step 710 Train loss 1.01 on epoch=354
06/19/2022 21:30:36 - INFO - __main__ - Step 720 Global step 720 Train loss 1.06 on epoch=359
06/19/2022 21:30:37 - INFO - __main__ - Step 730 Global step 730 Train loss 1.06 on epoch=364
06/19/2022 21:30:39 - INFO - __main__ - Step 740 Global step 740 Train loss 1.03 on epoch=369
06/19/2022 21:30:40 - INFO - __main__ - Step 750 Global step 750 Train loss 1.12 on epoch=374
06/19/2022 21:30:40 - INFO - __main__ - Global step 750 Train loss 1.06 Classification-F1 0.3816425120772947 on epoch=374
06/19/2022 21:30:42 - INFO - __main__ - Step 760 Global step 760 Train loss 1.08 on epoch=379
06/19/2022 21:30:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.95 on epoch=384
06/19/2022 21:30:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.89 on epoch=389
06/19/2022 21:30:45 - INFO - __main__ - Step 790 Global step 790 Train loss 1.04 on epoch=394
06/19/2022 21:30:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.85 on epoch=399
06/19/2022 21:30:47 - INFO - __main__ - Global step 800 Train loss 0.96 Classification-F1 0.37662337662337664 on epoch=399
06/19/2022 21:30:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.94 on epoch=404
06/19/2022 21:30:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.92 on epoch=409
06/19/2022 21:30:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.84 on epoch=414
06/19/2022 21:30:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.90 on epoch=419
06/19/2022 21:30:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.92 on epoch=424
06/19/2022 21:30:53 - INFO - __main__ - Global step 850 Train loss 0.90 Classification-F1 0.3992490613266583 on epoch=424
06/19/2022 21:30:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.89 on epoch=429
06/19/2022 21:30:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.94 on epoch=434
06/19/2022 21:30:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.94 on epoch=439
06/19/2022 21:30:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.80 on epoch=444
06/19/2022 21:31:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.84 on epoch=449
06/19/2022 21:31:00 - INFO - __main__ - Global step 900 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 21:31:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.95 on epoch=454
06/19/2022 21:31:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.88 on epoch=459
06/19/2022 21:31:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.90 on epoch=464
06/19/2022 21:31:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.78 on epoch=469
06/19/2022 21:31:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.73 on epoch=474
06/19/2022 21:31:07 - INFO - __main__ - Global step 950 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 21:31:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.84 on epoch=479
06/19/2022 21:31:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.80 on epoch=484
06/19/2022 21:31:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.91 on epoch=489
06/19/2022 21:31:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.83 on epoch=494
06/19/2022 21:31:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.68 on epoch=499
06/19/2022 21:31:13 - INFO - __main__ - Global step 1000 Train loss 0.81 Classification-F1 0.3816425120772947 on epoch=499
06/19/2022 21:31:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.78 on epoch=504
06/19/2022 21:31:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.71 on epoch=509
06/19/2022 21:31:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.81 on epoch=514
06/19/2022 21:31:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.85 on epoch=519
06/19/2022 21:31:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.76 on epoch=524
06/19/2022 21:31:20 - INFO - __main__ - Global step 1050 Train loss 0.78 Classification-F1 0.4589371980676329 on epoch=524
06/19/2022 21:31:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.79 on epoch=529
06/19/2022 21:31:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.76 on epoch=534
06/19/2022 21:31:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.86 on epoch=539
06/19/2022 21:31:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.80 on epoch=544
06/19/2022 21:31:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.82 on epoch=549
06/19/2022 21:31:26 - INFO - __main__ - Global step 1100 Train loss 0.80 Classification-F1 0.3454545454545454 on epoch=549
06/19/2022 21:31:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.73 on epoch=554
06/19/2022 21:31:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.77 on epoch=559
06/19/2022 21:31:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.68 on epoch=564
06/19/2022 21:31:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.75 on epoch=569
06/19/2022 21:31:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.76 on epoch=574
06/19/2022 21:31:33 - INFO - __main__ - Global step 1150 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 21:31:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.74 on epoch=579
06/19/2022 21:31:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.77 on epoch=584
06/19/2022 21:31:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.79 on epoch=589
06/19/2022 21:31:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.72 on epoch=594
06/19/2022 21:31:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=599
06/19/2022 21:31:40 - INFO - __main__ - Global step 1200 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 21:31:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.74 on epoch=604
06/19/2022 21:31:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.73 on epoch=609
06/19/2022 21:31:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.66 on epoch=614
06/19/2022 21:31:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.73 on epoch=619
06/19/2022 21:31:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.78 on epoch=624
06/19/2022 21:31:46 - INFO - __main__ - Global step 1250 Train loss 0.73 Classification-F1 0.3816425120772947 on epoch=624
06/19/2022 21:31:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.86 on epoch=629
06/19/2022 21:31:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.65 on epoch=634
06/19/2022 21:31:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.75 on epoch=639
06/19/2022 21:31:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.68 on epoch=644
06/19/2022 21:31:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.71 on epoch=649
06/19/2022 21:31:53 - INFO - __main__ - Global step 1300 Train loss 0.73 Classification-F1 0.4231177094379639 on epoch=649
06/19/2022 21:31:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.71 on epoch=654
06/19/2022 21:31:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.75 on epoch=659
06/19/2022 21:31:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.71 on epoch=664
06/19/2022 21:31:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.72 on epoch=669
06/19/2022 21:31:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.70 on epoch=674
06/19/2022 21:32:00 - INFO - __main__ - Global step 1350 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 21:32:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.64 on epoch=679
06/19/2022 21:32:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.71 on epoch=684
06/19/2022 21:32:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.71 on epoch=689
06/19/2022 21:32:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.61 on epoch=694
06/19/2022 21:32:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.72 on epoch=699
06/19/2022 21:32:06 - INFO - __main__ - Global step 1400 Train loss 0.68 Classification-F1 0.3992490613266583 on epoch=699
06/19/2022 21:32:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.63 on epoch=704
06/19/2022 21:32:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.68 on epoch=709
06/19/2022 21:32:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.64 on epoch=714
06/19/2022 21:32:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.64 on epoch=719
06/19/2022 21:32:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.62 on epoch=724
06/19/2022 21:32:13 - INFO - __main__ - Global step 1450 Train loss 0.64 Classification-F1 0.3043478260869565 on epoch=724
06/19/2022 21:32:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.68 on epoch=729
06/19/2022 21:32:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.71 on epoch=734
06/19/2022 21:32:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.67 on epoch=739
06/19/2022 21:32:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.76 on epoch=744
06/19/2022 21:32:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.73 on epoch=749
06/19/2022 21:32:19 - INFO - __main__ - Global step 1500 Train loss 0.71 Classification-F1 0.4589371980676329 on epoch=749
06/19/2022 21:32:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.68 on epoch=754
06/19/2022 21:32:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.58 on epoch=759
06/19/2022 21:32:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.72 on epoch=764
06/19/2022 21:32:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.61 on epoch=769
06/19/2022 21:32:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.67 on epoch=774
06/19/2022 21:32:26 - INFO - __main__ - Global step 1550 Train loss 0.65 Classification-F1 0.3992490613266583 on epoch=774
06/19/2022 21:32:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.60 on epoch=779
06/19/2022 21:32:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.69 on epoch=784
06/19/2022 21:32:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.63 on epoch=789
06/19/2022 21:32:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.65 on epoch=794
06/19/2022 21:32:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.64 on epoch=799
06/19/2022 21:32:33 - INFO - __main__ - Global step 1600 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 21:32:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.63 on epoch=804
06/19/2022 21:32:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.68 on epoch=809
06/19/2022 21:32:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.52 on epoch=814
06/19/2022 21:32:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.67 on epoch=819
06/19/2022 21:32:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.58 on epoch=824
06/19/2022 21:32:39 - INFO - __main__ - Global step 1650 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 21:32:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.67 on epoch=829
06/19/2022 21:32:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.62 on epoch=834
06/19/2022 21:32:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.61 on epoch=839
06/19/2022 21:32:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.64 on epoch=844
06/19/2022 21:32:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.71 on epoch=849
06/19/2022 21:32:46 - INFO - __main__ - Global step 1700 Train loss 0.65 Classification-F1 0.36374269005847953 on epoch=849
06/19/2022 21:32:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.59 on epoch=854
06/19/2022 21:32:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.54 on epoch=859
06/19/2022 21:32:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.65 on epoch=864
06/19/2022 21:32:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.63 on epoch=869
06/19/2022 21:32:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.56 on epoch=874
06/19/2022 21:32:53 - INFO - __main__ - Global step 1750 Train loss 0.59 Classification-F1 0.36374269005847953 on epoch=874
06/19/2022 21:32:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.56 on epoch=879
06/19/2022 21:32:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.61 on epoch=884
06/19/2022 21:32:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.63 on epoch=889
06/19/2022 21:32:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.62 on epoch=894
06/19/2022 21:32:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.56 on epoch=899
06/19/2022 21:33:00 - INFO - __main__ - Global step 1800 Train loss 0.60 Classification-F1 0.28888888888888886 on epoch=899
06/19/2022 21:33:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.59 on epoch=904
06/19/2022 21:33:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.59 on epoch=909
06/19/2022 21:33:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.56 on epoch=914
06/19/2022 21:33:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.57 on epoch=919
06/19/2022 21:33:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.57 on epoch=924
06/19/2022 21:33:06 - INFO - __main__ - Global step 1850 Train loss 0.58 Classification-F1 0.39756367663344405 on epoch=924
06/19/2022 21:33:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.55 on epoch=929
06/19/2022 21:33:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.58 on epoch=934
06/19/2022 21:33:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.64 on epoch=939
06/19/2022 21:33:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.62 on epoch=944
06/19/2022 21:33:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.75 on epoch=949
06/19/2022 21:33:13 - INFO - __main__ - Global step 1900 Train loss 0.63 Classification-F1 0.3992490613266583 on epoch=949
06/19/2022 21:33:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.61 on epoch=954
06/19/2022 21:33:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.55 on epoch=959
06/19/2022 21:33:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.65 on epoch=964
06/19/2022 21:33:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.69 on epoch=969
06/19/2022 21:33:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.61 on epoch=974
06/19/2022 21:33:20 - INFO - __main__ - Global step 1950 Train loss 0.62 Classification-F1 0.4385964912280702 on epoch=974
06/19/2022 21:33:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.56 on epoch=979
06/19/2022 21:33:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.62 on epoch=984
06/19/2022 21:33:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.52 on epoch=989
06/19/2022 21:33:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.61 on epoch=994
06/19/2022 21:33:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.55 on epoch=999
06/19/2022 21:33:26 - INFO - __main__ - Global step 2000 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 21:33:26 - INFO - __main__ - save last model!
06/19/2022 21:33:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 21:33:26 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 21:33:26 - INFO - __main__ - Printing 3 examples
06/19/2022 21:33:26 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 21:33:26 - INFO - __main__ - ['0']
06/19/2022 21:33:26 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 21:33:26 - INFO - __main__ - ['1']
06/19/2022 21:33:26 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 21:33:26 - INFO - __main__ - ['1']
06/19/2022 21:33:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:33:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:33:27 - INFO - __main__ - Printing 3 examples
06/19/2022 21:33:27 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 21:33:27 - INFO - __main__ - ['1']
06/19/2022 21:33:27 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 21:33:27 - INFO - __main__ - ['1']
06/19/2022 21:33:27 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 21:33:27 - INFO - __main__ - ['1']
06/19/2022 21:33:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:33:27 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:33:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:33:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:33:27 - INFO - __main__ - Printing 3 examples
06/19/2022 21:33:27 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 21:33:27 - INFO - __main__ - ['1']
06/19/2022 21:33:27 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 21:33:27 - INFO - __main__ - ['1']
06/19/2022 21:33:27 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 21:33:27 - INFO - __main__ - ['1']
06/19/2022 21:33:27 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:33:27 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:33:27 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:33:30 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:33:32 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:33:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:33:33 - INFO - __main__ - Starting training!
06/19/2022 21:33:38 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 21:35:02 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_100_0.3_8_predictions.txt
06/19/2022 21:35:02 - INFO - __main__ - Classification-F1 on test data: 0.3075
06/19/2022 21:35:02 - INFO - __main__ - prefix=paws_16_100, lr=0.3, bsz=8, dev_performance=0.5333333333333333, test_performance=0.3074599667985192
06/19/2022 21:35:02 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.2, bsz=8 ...
06/19/2022 21:35:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:35:03 - INFO - __main__ - Printing 3 examples
06/19/2022 21:35:03 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/19/2022 21:35:03 - INFO - __main__ - ['1']
06/19/2022 21:35:03 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/19/2022 21:35:03 - INFO - __main__ - ['1']
06/19/2022 21:35:03 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/19/2022 21:35:03 - INFO - __main__ - ['1']
06/19/2022 21:35:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:35:03 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:35:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:35:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:35:03 - INFO - __main__ - Printing 3 examples
06/19/2022 21:35:03 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/19/2022 21:35:03 - INFO - __main__ - ['1']
06/19/2022 21:35:03 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/19/2022 21:35:03 - INFO - __main__ - ['1']
06/19/2022 21:35:03 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/19/2022 21:35:03 - INFO - __main__ - ['1']
06/19/2022 21:35:03 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:35:03 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:35:03 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:35:09 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:35:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:35:09 - INFO - __main__ - Starting training!
06/19/2022 21:35:11 - INFO - __main__ - Step 10 Global step 10 Train loss 5.86 on epoch=4
06/19/2022 21:35:12 - INFO - __main__ - Step 20 Global step 20 Train loss 5.71 on epoch=9
06/19/2022 21:35:13 - INFO - __main__ - Step 30 Global step 30 Train loss 5.38 on epoch=14
06/19/2022 21:35:15 - INFO - __main__ - Step 40 Global step 40 Train loss 4.96 on epoch=19
06/19/2022 21:35:16 - INFO - __main__ - Step 50 Global step 50 Train loss 4.63 on epoch=24
06/19/2022 21:35:23 - INFO - __main__ - Global step 50 Train loss 5.31 Classification-F1 0.0 on epoch=24
06/19/2022 21:35:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 21:35:25 - INFO - __main__ - Step 60 Global step 60 Train loss 4.45 on epoch=29
06/19/2022 21:35:26 - INFO - __main__ - Step 70 Global step 70 Train loss 4.29 on epoch=34
06/19/2022 21:35:27 - INFO - __main__ - Step 80 Global step 80 Train loss 4.25 on epoch=39
06/19/2022 21:35:29 - INFO - __main__ - Step 90 Global step 90 Train loss 4.06 on epoch=44
06/19/2022 21:35:30 - INFO - __main__ - Step 100 Global step 100 Train loss 4.09 on epoch=49
06/19/2022 21:35:31 - INFO - __main__ - Global step 100 Train loss 4.23 Classification-F1 0.0 on epoch=49
06/19/2022 21:35:33 - INFO - __main__ - Step 110 Global step 110 Train loss 3.91 on epoch=54
06/19/2022 21:35:34 - INFO - __main__ - Step 120 Global step 120 Train loss 3.94 on epoch=59
06/19/2022 21:35:35 - INFO - __main__ - Step 130 Global step 130 Train loss 3.92 on epoch=64
06/19/2022 21:35:37 - INFO - __main__ - Step 140 Global step 140 Train loss 3.89 on epoch=69
06/19/2022 21:35:38 - INFO - __main__ - Step 150 Global step 150 Train loss 3.78 on epoch=74
06/19/2022 21:35:40 - INFO - __main__ - Global step 150 Train loss 3.89 Classification-F1 0.0 on epoch=74
06/19/2022 21:35:41 - INFO - __main__ - Step 160 Global step 160 Train loss 3.75 on epoch=79
06/19/2022 21:35:43 - INFO - __main__ - Step 170 Global step 170 Train loss 3.63 on epoch=84
06/19/2022 21:35:44 - INFO - __main__ - Step 180 Global step 180 Train loss 3.58 on epoch=89
06/19/2022 21:35:45 - INFO - __main__ - Step 190 Global step 190 Train loss 3.60 on epoch=94
06/19/2022 21:35:46 - INFO - __main__ - Step 200 Global step 200 Train loss 3.53 on epoch=99
06/19/2022 21:35:48 - INFO - __main__ - Global step 200 Train loss 3.62 Classification-F1 0.0 on epoch=99
06/19/2022 21:35:50 - INFO - __main__ - Step 210 Global step 210 Train loss 3.46 on epoch=104
06/19/2022 21:35:51 - INFO - __main__ - Step 220 Global step 220 Train loss 3.49 on epoch=109
06/19/2022 21:35:52 - INFO - __main__ - Step 230 Global step 230 Train loss 3.31 on epoch=114
06/19/2022 21:35:53 - INFO - __main__ - Step 240 Global step 240 Train loss 3.26 on epoch=119
06/19/2022 21:35:55 - INFO - __main__ - Step 250 Global step 250 Train loss 3.25 on epoch=124
06/19/2022 21:35:58 - INFO - __main__ - Global step 250 Train loss 3.35 Classification-F1 0.027210884353741496 on epoch=124
06/19/2022 21:35:58 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.027210884353741496 on epoch=124, global_step=250
06/19/2022 21:35:59 - INFO - __main__ - Step 260 Global step 260 Train loss 3.25 on epoch=129
06/19/2022 21:36:00 - INFO - __main__ - Step 270 Global step 270 Train loss 3.26 on epoch=134
06/19/2022 21:36:01 - INFO - __main__ - Step 280 Global step 280 Train loss 3.00 on epoch=139
06/19/2022 21:36:03 - INFO - __main__ - Step 290 Global step 290 Train loss 3.08 on epoch=144
06/19/2022 21:36:04 - INFO - __main__ - Step 300 Global step 300 Train loss 2.90 on epoch=149
06/19/2022 21:36:05 - INFO - __main__ - Global step 300 Train loss 3.10 Classification-F1 0.11555555555555555 on epoch=149
06/19/2022 21:36:05 - INFO - __main__ - Saving model with best Classification-F1: 0.027210884353741496 -> 0.11555555555555555 on epoch=149, global_step=300
06/19/2022 21:36:06 - INFO - __main__ - Step 310 Global step 310 Train loss 3.01 on epoch=154
06/19/2022 21:36:08 - INFO - __main__ - Step 320 Global step 320 Train loss 2.80 on epoch=159
06/19/2022 21:36:09 - INFO - __main__ - Step 330 Global step 330 Train loss 2.96 on epoch=164
06/19/2022 21:36:10 - INFO - __main__ - Step 340 Global step 340 Train loss 2.82 on epoch=169
06/19/2022 21:36:12 - INFO - __main__ - Step 350 Global step 350 Train loss 2.63 on epoch=174
06/19/2022 21:36:14 - INFO - __main__ - Global step 350 Train loss 2.84 Classification-F1 0.09523809523809523 on epoch=174
06/19/2022 21:36:15 - INFO - __main__ - Step 360 Global step 360 Train loss 2.53 on epoch=179
06/19/2022 21:36:16 - INFO - __main__ - Step 370 Global step 370 Train loss 2.67 on epoch=184
06/19/2022 21:36:18 - INFO - __main__ - Step 380 Global step 380 Train loss 2.61 on epoch=189
06/19/2022 21:36:19 - INFO - __main__ - Step 390 Global step 390 Train loss 2.64 on epoch=194
06/19/2022 21:36:20 - INFO - __main__ - Step 400 Global step 400 Train loss 2.47 on epoch=199
06/19/2022 21:36:21 - INFO - __main__ - Global step 400 Train loss 2.58 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 21:36:21 - INFO - __main__ - Saving model with best Classification-F1: 0.11555555555555555 -> 0.3333333333333333 on epoch=199, global_step=400
06/19/2022 21:36:22 - INFO - __main__ - Step 410 Global step 410 Train loss 2.36 on epoch=204
06/19/2022 21:36:24 - INFO - __main__ - Step 420 Global step 420 Train loss 2.62 on epoch=209
06/19/2022 21:36:25 - INFO - __main__ - Step 430 Global step 430 Train loss 2.61 on epoch=214
06/19/2022 21:36:26 - INFO - __main__ - Step 440 Global step 440 Train loss 2.37 on epoch=219
06/19/2022 21:36:27 - INFO - __main__ - Step 450 Global step 450 Train loss 2.29 on epoch=224
06/19/2022 21:36:29 - INFO - __main__ - Global step 450 Train loss 2.45 Classification-F1 0.22695035460992907 on epoch=224
06/19/2022 21:36:30 - INFO - __main__ - Step 460 Global step 460 Train loss 2.33 on epoch=229
06/19/2022 21:36:32 - INFO - __main__ - Step 470 Global step 470 Train loss 2.22 on epoch=234
06/19/2022 21:36:33 - INFO - __main__ - Step 480 Global step 480 Train loss 2.32 on epoch=239
06/19/2022 21:36:34 - INFO - __main__ - Step 490 Global step 490 Train loss 2.32 on epoch=244
06/19/2022 21:36:35 - INFO - __main__ - Step 500 Global step 500 Train loss 2.27 on epoch=249
06/19/2022 21:36:36 - INFO - __main__ - Global step 500 Train loss 2.29 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 21:36:37 - INFO - __main__ - Step 510 Global step 510 Train loss 2.21 on epoch=254
06/19/2022 21:36:39 - INFO - __main__ - Step 520 Global step 520 Train loss 1.99 on epoch=259
06/19/2022 21:36:40 - INFO - __main__ - Step 530 Global step 530 Train loss 1.96 on epoch=264
06/19/2022 21:36:41 - INFO - __main__ - Step 540 Global step 540 Train loss 2.12 on epoch=269
06/19/2022 21:36:43 - INFO - __main__ - Step 550 Global step 550 Train loss 2.08 on epoch=274
06/19/2022 21:36:43 - INFO - __main__ - Global step 550 Train loss 2.07 Classification-F1 0.4181818181818182 on epoch=274
06/19/2022 21:36:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4181818181818182 on epoch=274, global_step=550
06/19/2022 21:36:44 - INFO - __main__ - Step 560 Global step 560 Train loss 1.94 on epoch=279
06/19/2022 21:36:46 - INFO - __main__ - Step 570 Global step 570 Train loss 1.97 on epoch=284
06/19/2022 21:36:47 - INFO - __main__ - Step 580 Global step 580 Train loss 1.78 on epoch=289
06/19/2022 21:36:48 - INFO - __main__ - Step 590 Global step 590 Train loss 1.84 on epoch=294
06/19/2022 21:36:49 - INFO - __main__ - Step 600 Global step 600 Train loss 1.79 on epoch=299
06/19/2022 21:36:50 - INFO - __main__ - Global step 600 Train loss 1.87 Classification-F1 0.5151515151515151 on epoch=299
06/19/2022 21:36:50 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.5151515151515151 on epoch=299, global_step=600
06/19/2022 21:36:51 - INFO - __main__ - Step 610 Global step 610 Train loss 1.88 on epoch=304
06/19/2022 21:36:52 - INFO - __main__ - Step 620 Global step 620 Train loss 1.78 on epoch=309
06/19/2022 21:36:54 - INFO - __main__ - Step 630 Global step 630 Train loss 1.90 on epoch=314
06/19/2022 21:36:55 - INFO - __main__ - Step 640 Global step 640 Train loss 1.71 on epoch=319
06/19/2022 21:36:56 - INFO - __main__ - Step 650 Global step 650 Train loss 1.85 on epoch=324
06/19/2022 21:36:57 - INFO - __main__ - Global step 650 Train loss 1.82 Classification-F1 0.41700404858299595 on epoch=324
06/19/2022 21:36:58 - INFO - __main__ - Step 660 Global step 660 Train loss 1.78 on epoch=329
06/19/2022 21:36:59 - INFO - __main__ - Step 670 Global step 670 Train loss 1.65 on epoch=334
06/19/2022 21:37:01 - INFO - __main__ - Step 680 Global step 680 Train loss 1.64 on epoch=339
06/19/2022 21:37:02 - INFO - __main__ - Step 690 Global step 690 Train loss 1.77 on epoch=344
06/19/2022 21:37:03 - INFO - __main__ - Step 700 Global step 700 Train loss 1.75 on epoch=349
06/19/2022 21:37:04 - INFO - __main__ - Global step 700 Train loss 1.72 Classification-F1 0.4458874458874459 on epoch=349
06/19/2022 21:37:05 - INFO - __main__ - Step 710 Global step 710 Train loss 1.64 on epoch=354
06/19/2022 21:37:06 - INFO - __main__ - Step 720 Global step 720 Train loss 1.65 on epoch=359
06/19/2022 21:37:07 - INFO - __main__ - Step 730 Global step 730 Train loss 1.63 on epoch=364
06/19/2022 21:37:09 - INFO - __main__ - Step 740 Global step 740 Train loss 1.48 on epoch=369
06/19/2022 21:37:10 - INFO - __main__ - Step 750 Global step 750 Train loss 1.46 on epoch=374
06/19/2022 21:37:10 - INFO - __main__ - Global step 750 Train loss 1.57 Classification-F1 0.5 on epoch=374
06/19/2022 21:37:12 - INFO - __main__ - Step 760 Global step 760 Train loss 1.57 on epoch=379
06/19/2022 21:37:13 - INFO - __main__ - Step 770 Global step 770 Train loss 1.61 on epoch=384
06/19/2022 21:37:14 - INFO - __main__ - Step 780 Global step 780 Train loss 1.44 on epoch=389
06/19/2022 21:37:16 - INFO - __main__ - Step 790 Global step 790 Train loss 1.45 on epoch=394
06/19/2022 21:37:17 - INFO - __main__ - Step 800 Global step 800 Train loss 1.51 on epoch=399
06/19/2022 21:37:17 - INFO - __main__ - Global step 800 Train loss 1.52 Classification-F1 0.5588547189819725 on epoch=399
06/19/2022 21:37:17 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.5588547189819725 on epoch=399, global_step=800
06/19/2022 21:37:19 - INFO - __main__ - Step 810 Global step 810 Train loss 1.61 on epoch=404
06/19/2022 21:37:20 - INFO - __main__ - Step 820 Global step 820 Train loss 1.45 on epoch=409
06/19/2022 21:37:21 - INFO - __main__ - Step 830 Global step 830 Train loss 1.49 on epoch=414
06/19/2022 21:37:22 - INFO - __main__ - Step 840 Global step 840 Train loss 1.43 on epoch=419
06/19/2022 21:37:24 - INFO - __main__ - Step 850 Global step 850 Train loss 1.54 on epoch=424
06/19/2022 21:37:24 - INFO - __main__ - Global step 850 Train loss 1.50 Classification-F1 0.3522267206477733 on epoch=424
06/19/2022 21:37:25 - INFO - __main__ - Step 860 Global step 860 Train loss 1.50 on epoch=429
06/19/2022 21:37:27 - INFO - __main__ - Step 870 Global step 870 Train loss 1.50 on epoch=434
06/19/2022 21:37:28 - INFO - __main__ - Step 880 Global step 880 Train loss 1.40 on epoch=439
06/19/2022 21:37:29 - INFO - __main__ - Step 890 Global step 890 Train loss 1.33 on epoch=444
06/19/2022 21:37:30 - INFO - __main__ - Step 900 Global step 900 Train loss 1.38 on epoch=449
06/19/2022 21:37:31 - INFO - __main__ - Global step 900 Train loss 1.42 Classification-F1 0.40566959921798634 on epoch=449
06/19/2022 21:37:32 - INFO - __main__ - Step 910 Global step 910 Train loss 1.38 on epoch=454
06/19/2022 21:37:33 - INFO - __main__ - Step 920 Global step 920 Train loss 1.36 on epoch=459
06/19/2022 21:37:35 - INFO - __main__ - Step 930 Global step 930 Train loss 1.34 on epoch=464
06/19/2022 21:37:36 - INFO - __main__ - Step 940 Global step 940 Train loss 1.56 on epoch=469
06/19/2022 21:37:37 - INFO - __main__ - Step 950 Global step 950 Train loss 1.36 on epoch=474
06/19/2022 21:37:38 - INFO - __main__ - Global step 950 Train loss 1.40 Classification-F1 0.3191489361702127 on epoch=474
06/19/2022 21:37:39 - INFO - __main__ - Step 960 Global step 960 Train loss 1.49 on epoch=479
06/19/2022 21:37:40 - INFO - __main__ - Step 970 Global step 970 Train loss 1.39 on epoch=484
06/19/2022 21:37:41 - INFO - __main__ - Step 980 Global step 980 Train loss 1.17 on epoch=489
06/19/2022 21:37:43 - INFO - __main__ - Step 990 Global step 990 Train loss 1.32 on epoch=494
06/19/2022 21:37:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.48 on epoch=499
06/19/2022 21:37:44 - INFO - __main__ - Global step 1000 Train loss 1.37 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 21:37:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.32 on epoch=504
06/19/2022 21:37:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.34 on epoch=509
06/19/2022 21:37:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.25 on epoch=514
06/19/2022 21:37:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.16 on epoch=519
06/19/2022 21:37:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.36 on epoch=524
06/19/2022 21:37:51 - INFO - __main__ - Global step 1050 Train loss 1.29 Classification-F1 0.36374269005847953 on epoch=524
06/19/2022 21:37:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.36 on epoch=529
06/19/2022 21:37:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.18 on epoch=534
06/19/2022 21:37:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.31 on epoch=539
06/19/2022 21:37:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.16 on epoch=544
06/19/2022 21:37:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.19 on epoch=549
06/19/2022 21:37:58 - INFO - __main__ - Global step 1100 Train loss 1.24 Classification-F1 0.3650793650793651 on epoch=549
06/19/2022 21:37:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.15 on epoch=554
06/19/2022 21:38:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.25 on epoch=559
06/19/2022 21:38:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.19 on epoch=564
06/19/2022 21:38:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.23 on epoch=569
06/19/2022 21:38:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.16 on epoch=574
06/19/2022 21:38:04 - INFO - __main__ - Global step 1150 Train loss 1.20 Classification-F1 0.3043478260869565 on epoch=574
06/19/2022 21:38:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.12 on epoch=579
06/19/2022 21:38:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.09 on epoch=584
06/19/2022 21:38:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.14 on epoch=589
06/19/2022 21:38:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.15 on epoch=594
06/19/2022 21:38:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.18 on epoch=599
06/19/2022 21:38:11 - INFO - __main__ - Global step 1200 Train loss 1.13 Classification-F1 0.3191489361702127 on epoch=599
06/19/2022 21:38:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.13 on epoch=604
06/19/2022 21:38:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.16 on epoch=609
06/19/2022 21:38:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.07 on epoch=614
06/19/2022 21:38:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.14 on epoch=619
06/19/2022 21:38:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.04 on epoch=624
06/19/2022 21:38:18 - INFO - __main__ - Global step 1250 Train loss 1.11 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 21:38:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.16 on epoch=629
06/19/2022 21:38:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.16 on epoch=634
06/19/2022 21:38:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.07 on epoch=639
06/19/2022 21:38:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.05 on epoch=644
06/19/2022 21:38:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.09 on epoch=649
06/19/2022 21:38:24 - INFO - __main__ - Global step 1300 Train loss 1.10 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 21:38:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.97 on epoch=654
06/19/2022 21:38:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.02 on epoch=659
06/19/2022 21:38:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.92 on epoch=664
06/19/2022 21:38:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.97 on epoch=669
06/19/2022 21:38:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.09 on epoch=674
06/19/2022 21:38:31 - INFO - __main__ - Global step 1350 Train loss 1.00 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 21:38:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.06 on epoch=679
06/19/2022 21:38:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.00 on epoch=684
06/19/2022 21:38:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.07 on epoch=689
06/19/2022 21:38:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.96 on epoch=694
06/19/2022 21:38:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.04 on epoch=699
06/19/2022 21:38:38 - INFO - __main__ - Global step 1400 Train loss 1.02 Classification-F1 0.36374269005847953 on epoch=699
06/19/2022 21:38:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.92 on epoch=704
06/19/2022 21:38:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.06 on epoch=709
06/19/2022 21:38:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.09 on epoch=714
06/19/2022 21:38:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.04 on epoch=719
06/19/2022 21:38:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.03 on epoch=724
06/19/2022 21:38:45 - INFO - __main__ - Global step 1450 Train loss 1.03 Classification-F1 0.3191489361702127 on epoch=724
06/19/2022 21:38:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.92 on epoch=729
06/19/2022 21:38:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.93 on epoch=734
06/19/2022 21:38:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.93 on epoch=739
06/19/2022 21:38:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.00 on epoch=744
06/19/2022 21:38:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.01 on epoch=749
06/19/2022 21:38:51 - INFO - __main__ - Global step 1500 Train loss 0.96 Classification-F1 0.3992490613266583 on epoch=749
06/19/2022 21:38:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.87 on epoch=754
06/19/2022 21:38:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.86 on epoch=759
06/19/2022 21:38:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.95 on epoch=764
06/19/2022 21:38:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.95 on epoch=769
06/19/2022 21:38:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.00 on epoch=774
06/19/2022 21:38:58 - INFO - __main__ - Global step 1550 Train loss 0.93 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 21:38:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.00 on epoch=779
06/19/2022 21:39:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.87 on epoch=784
06/19/2022 21:39:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.92 on epoch=789
06/19/2022 21:39:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.94 on epoch=794
06/19/2022 21:39:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.82 on epoch=799
06/19/2022 21:39:05 - INFO - __main__ - Global step 1600 Train loss 0.91 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 21:39:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.92 on epoch=804
06/19/2022 21:39:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.91 on epoch=809
06/19/2022 21:39:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.77 on epoch=814
06/19/2022 21:39:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.86 on epoch=819
06/19/2022 21:39:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.88 on epoch=824
06/19/2022 21:39:12 - INFO - __main__ - Global step 1650 Train loss 0.87 Classification-F1 0.4181818181818182 on epoch=824
06/19/2022 21:39:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.85 on epoch=829
06/19/2022 21:39:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.91 on epoch=834
06/19/2022 21:39:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.80 on epoch=839
06/19/2022 21:39:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.83 on epoch=844
06/19/2022 21:39:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.94 on epoch=849
06/19/2022 21:39:18 - INFO - __main__ - Global step 1700 Train loss 0.87 Classification-F1 0.5555555555555556 on epoch=849
06/19/2022 21:39:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.87 on epoch=854
06/19/2022 21:39:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.78 on epoch=859
06/19/2022 21:39:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.80 on epoch=864
06/19/2022 21:39:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.84 on epoch=869
06/19/2022 21:39:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.81 on epoch=874
06/19/2022 21:39:25 - INFO - __main__ - Global step 1750 Train loss 0.82 Classification-F1 0.5933528836754642 on epoch=874
06/19/2022 21:39:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5588547189819725 -> 0.5933528836754642 on epoch=874, global_step=1750
06/19/2022 21:39:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.82 on epoch=879
06/19/2022 21:39:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.92 on epoch=884
06/19/2022 21:39:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.88 on epoch=889
06/19/2022 21:39:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.81 on epoch=894
06/19/2022 21:39:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.91 on epoch=899
06/19/2022 21:39:32 - INFO - __main__ - Global step 1800 Train loss 0.87 Classification-F1 0.4920634920634921 on epoch=899
06/19/2022 21:39:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.73 on epoch=904
06/19/2022 21:39:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.90 on epoch=909
06/19/2022 21:39:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.86 on epoch=914
06/19/2022 21:39:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.74 on epoch=919
06/19/2022 21:39:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.82 on epoch=924
06/19/2022 21:39:39 - INFO - __main__ - Global step 1850 Train loss 0.81 Classification-F1 0.4385964912280702 on epoch=924
06/19/2022 21:39:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.80 on epoch=929
06/19/2022 21:39:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.84 on epoch=934
06/19/2022 21:39:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.80 on epoch=939
06/19/2022 21:39:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.85 on epoch=944
06/19/2022 21:39:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.87 on epoch=949
06/19/2022 21:39:45 - INFO - __main__ - Global step 1900 Train loss 0.83 Classification-F1 0.4980392156862745 on epoch=949
06/19/2022 21:39:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.86 on epoch=954
06/19/2022 21:39:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.80 on epoch=959
06/19/2022 21:39:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.80 on epoch=964
06/19/2022 21:39:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.71 on epoch=969
06/19/2022 21:39:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.70 on epoch=974
06/19/2022 21:39:52 - INFO - __main__ - Global step 1950 Train loss 0.77 Classification-F1 0.4458874458874459 on epoch=974
06/19/2022 21:39:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.83 on epoch=979
06/19/2022 21:39:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.77 on epoch=984
06/19/2022 21:39:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.74 on epoch=989
06/19/2022 21:39:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.79 on epoch=994
06/19/2022 21:39:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.72 on epoch=999
06/19/2022 21:39:59 - INFO - __main__ - Global step 2000 Train loss 0.77 Classification-F1 0.4817813765182186 on epoch=999
06/19/2022 21:39:59 - INFO - __main__ - save last model!
06/19/2022 21:39:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 21:39:59 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 21:39:59 - INFO - __main__ - Printing 3 examples
06/19/2022 21:39:59 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 21:39:59 - INFO - __main__ - ['0']
06/19/2022 21:39:59 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 21:39:59 - INFO - __main__ - ['1']
06/19/2022 21:39:59 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 21:39:59 - INFO - __main__ - ['1']
06/19/2022 21:39:59 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:40:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:40:00 - INFO - __main__ - Printing 3 examples
06/19/2022 21:40:00 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 21:40:00 - INFO - __main__ - ['1']
06/19/2022 21:40:00 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 21:40:00 - INFO - __main__ - ['1']
06/19/2022 21:40:00 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 21:40:00 - INFO - __main__ - ['1']
06/19/2022 21:40:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:40:00 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:40:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:40:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:40:00 - INFO - __main__ - Printing 3 examples
06/19/2022 21:40:00 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 21:40:00 - INFO - __main__ - ['1']
06/19/2022 21:40:00 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 21:40:00 - INFO - __main__ - ['1']
06/19/2022 21:40:00 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 21:40:00 - INFO - __main__ - ['1']
06/19/2022 21:40:00 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:40:00 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:40:00 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:40:03 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:40:05 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:40:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:40:06 - INFO - __main__ - Starting training!
06/19/2022 21:40:11 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 21:41:39 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_100_0.2_8_predictions.txt
06/19/2022 21:41:39 - INFO - __main__ - Classification-F1 on test data: 0.4804
06/19/2022 21:41:39 - INFO - __main__ - prefix=paws_16_100, lr=0.2, bsz=8, dev_performance=0.5933528836754642, test_performance=0.4803558891895293
06/19/2022 21:41:39 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.5, bsz=8 ...
06/19/2022 21:41:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:41:40 - INFO - __main__ - Printing 3 examples
06/19/2022 21:41:40 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 21:41:40 - INFO - __main__ - ['1']
06/19/2022 21:41:40 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 21:41:40 - INFO - __main__ - ['1']
06/19/2022 21:41:40 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 21:41:40 - INFO - __main__ - ['1']
06/19/2022 21:41:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:41:40 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:41:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:41:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:41:40 - INFO - __main__ - Printing 3 examples
06/19/2022 21:41:40 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 21:41:40 - INFO - __main__ - ['1']
06/19/2022 21:41:40 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 21:41:40 - INFO - __main__ - ['1']
06/19/2022 21:41:40 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 21:41:40 - INFO - __main__ - ['1']
06/19/2022 21:41:40 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:41:40 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:41:40 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:41:45 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:41:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:41:45 - INFO - __main__ - Starting training!
06/19/2022 21:41:47 - INFO - __main__ - Step 10 Global step 10 Train loss 5.82 on epoch=4
06/19/2022 21:41:48 - INFO - __main__ - Step 20 Global step 20 Train loss 5.35 on epoch=9
06/19/2022 21:41:49 - INFO - __main__ - Step 30 Global step 30 Train loss 4.69 on epoch=14
06/19/2022 21:41:51 - INFO - __main__ - Step 40 Global step 40 Train loss 4.38 on epoch=19
06/19/2022 21:41:52 - INFO - __main__ - Step 50 Global step 50 Train loss 4.22 on epoch=24
06/19/2022 21:41:53 - INFO - __main__ - Global step 50 Train loss 4.89 Classification-F1 0.0 on epoch=24
06/19/2022 21:41:53 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 21:41:54 - INFO - __main__ - Step 60 Global step 60 Train loss 4.07 on epoch=29
06/19/2022 21:41:55 - INFO - __main__ - Step 70 Global step 70 Train loss 4.02 on epoch=34
06/19/2022 21:41:57 - INFO - __main__ - Step 80 Global step 80 Train loss 3.88 on epoch=39
06/19/2022 21:41:58 - INFO - __main__ - Step 90 Global step 90 Train loss 3.70 on epoch=44
06/19/2022 21:41:59 - INFO - __main__ - Step 100 Global step 100 Train loss 3.73 on epoch=49
06/19/2022 21:42:06 - INFO - __main__ - Global step 100 Train loss 3.88 Classification-F1 0.0 on epoch=49
06/19/2022 21:42:07 - INFO - __main__ - Step 110 Global step 110 Train loss 3.63 on epoch=54
06/19/2022 21:42:08 - INFO - __main__ - Step 120 Global step 120 Train loss 3.44 on epoch=59
06/19/2022 21:42:09 - INFO - __main__ - Step 130 Global step 130 Train loss 3.44 on epoch=64
06/19/2022 21:42:11 - INFO - __main__ - Step 140 Global step 140 Train loss 3.46 on epoch=69
06/19/2022 21:42:12 - INFO - __main__ - Step 150 Global step 150 Train loss 3.03 on epoch=74
06/19/2022 21:42:14 - INFO - __main__ - Global step 150 Train loss 3.40 Classification-F1 0.0 on epoch=74
06/19/2022 21:42:16 - INFO - __main__ - Step 160 Global step 160 Train loss 3.08 on epoch=79
06/19/2022 21:42:17 - INFO - __main__ - Step 170 Global step 170 Train loss 3.11 on epoch=84
06/19/2022 21:42:18 - INFO - __main__ - Step 180 Global step 180 Train loss 3.09 on epoch=89
06/19/2022 21:42:19 - INFO - __main__ - Step 190 Global step 190 Train loss 2.86 on epoch=94
06/19/2022 21:42:21 - INFO - __main__ - Step 200 Global step 200 Train loss 2.66 on epoch=99
06/19/2022 21:42:23 - INFO - __main__ - Global step 200 Train loss 2.96 Classification-F1 0.07926829268292683 on epoch=99
06/19/2022 21:42:23 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.07926829268292683 on epoch=99, global_step=200
06/19/2022 21:42:24 - INFO - __main__ - Step 210 Global step 210 Train loss 2.63 on epoch=104
06/19/2022 21:42:26 - INFO - __main__ - Step 220 Global step 220 Train loss 2.62 on epoch=109
06/19/2022 21:42:27 - INFO - __main__ - Step 230 Global step 230 Train loss 2.63 on epoch=114
06/19/2022 21:42:28 - INFO - __main__ - Step 240 Global step 240 Train loss 2.51 on epoch=119
06/19/2022 21:42:29 - INFO - __main__ - Step 250 Global step 250 Train loss 2.36 on epoch=124
06/19/2022 21:42:31 - INFO - __main__ - Global step 250 Train loss 2.55 Classification-F1 0.26143790849673204 on epoch=124
06/19/2022 21:42:31 - INFO - __main__ - Saving model with best Classification-F1: 0.07926829268292683 -> 0.26143790849673204 on epoch=124, global_step=250
06/19/2022 21:42:32 - INFO - __main__ - Step 260 Global step 260 Train loss 2.19 on epoch=129
06/19/2022 21:42:33 - INFO - __main__ - Step 270 Global step 270 Train loss 2.04 on epoch=134
06/19/2022 21:42:35 - INFO - __main__ - Step 280 Global step 280 Train loss 2.09 on epoch=139
06/19/2022 21:42:36 - INFO - __main__ - Step 290 Global step 290 Train loss 2.06 on epoch=144
06/19/2022 21:42:37 - INFO - __main__ - Step 300 Global step 300 Train loss 1.91 on epoch=149
06/19/2022 21:42:38 - INFO - __main__ - Global step 300 Train loss 2.06 Classification-F1 0.4920634920634921 on epoch=149
06/19/2022 21:42:38 - INFO - __main__ - Saving model with best Classification-F1: 0.26143790849673204 -> 0.4920634920634921 on epoch=149, global_step=300
06/19/2022 21:42:39 - INFO - __main__ - Step 310 Global step 310 Train loss 2.06 on epoch=154
06/19/2022 21:42:41 - INFO - __main__ - Step 320 Global step 320 Train loss 2.00 on epoch=159
06/19/2022 21:42:42 - INFO - __main__ - Step 330 Global step 330 Train loss 1.77 on epoch=164
06/19/2022 21:42:43 - INFO - __main__ - Step 340 Global step 340 Train loss 1.87 on epoch=169
06/19/2022 21:42:44 - INFO - __main__ - Step 350 Global step 350 Train loss 1.74 on epoch=174
06/19/2022 21:42:45 - INFO - __main__ - Global step 350 Train loss 1.89 Classification-F1 0.4589371980676329 on epoch=174
06/19/2022 21:42:47 - INFO - __main__ - Step 360 Global step 360 Train loss 1.65 on epoch=179
06/19/2022 21:42:48 - INFO - __main__ - Step 370 Global step 370 Train loss 1.69 on epoch=184
06/19/2022 21:42:49 - INFO - __main__ - Step 380 Global step 380 Train loss 1.57 on epoch=189
06/19/2022 21:42:50 - INFO - __main__ - Step 390 Global step 390 Train loss 1.56 on epoch=194
06/19/2022 21:42:52 - INFO - __main__ - Step 400 Global step 400 Train loss 1.53 on epoch=199
06/19/2022 21:42:53 - INFO - __main__ - Global step 400 Train loss 1.60 Classification-F1 0.3191489361702127 on epoch=199
06/19/2022 21:42:54 - INFO - __main__ - Step 410 Global step 410 Train loss 1.55 on epoch=204
06/19/2022 21:42:55 - INFO - __main__ - Step 420 Global step 420 Train loss 1.51 on epoch=209
06/19/2022 21:42:57 - INFO - __main__ - Step 430 Global step 430 Train loss 1.57 on epoch=214
06/19/2022 21:42:58 - INFO - __main__ - Step 440 Global step 440 Train loss 1.49 on epoch=219
06/19/2022 21:42:59 - INFO - __main__ - Step 450 Global step 450 Train loss 1.33 on epoch=224
06/19/2022 21:42:59 - INFO - __main__ - Global step 450 Train loss 1.49 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 21:43:01 - INFO - __main__ - Step 460 Global step 460 Train loss 1.43 on epoch=229
06/19/2022 21:43:02 - INFO - __main__ - Step 470 Global step 470 Train loss 1.34 on epoch=234
06/19/2022 21:43:03 - INFO - __main__ - Step 480 Global step 480 Train loss 1.28 on epoch=239
06/19/2022 21:43:04 - INFO - __main__ - Step 490 Global step 490 Train loss 1.33 on epoch=244
06/19/2022 21:43:06 - INFO - __main__ - Step 500 Global step 500 Train loss 1.16 on epoch=249
06/19/2022 21:43:06 - INFO - __main__ - Global step 500 Train loss 1.31 Classification-F1 0.3552492046659597 on epoch=249
06/19/2022 21:43:07 - INFO - __main__ - Step 510 Global step 510 Train loss 1.29 on epoch=254
06/19/2022 21:43:08 - INFO - __main__ - Step 520 Global step 520 Train loss 1.30 on epoch=259
06/19/2022 21:43:10 - INFO - __main__ - Step 530 Global step 530 Train loss 1.18 on epoch=264
06/19/2022 21:43:11 - INFO - __main__ - Step 540 Global step 540 Train loss 1.18 on epoch=269
06/19/2022 21:43:12 - INFO - __main__ - Step 550 Global step 550 Train loss 1.18 on epoch=274
06/19/2022 21:43:13 - INFO - __main__ - Global step 550 Train loss 1.23 Classification-F1 0.5835835835835835 on epoch=274
06/19/2022 21:43:13 - INFO - __main__ - Saving model with best Classification-F1: 0.4920634920634921 -> 0.5835835835835835 on epoch=274, global_step=550
06/19/2022 21:43:14 - INFO - __main__ - Step 560 Global step 560 Train loss 1.17 on epoch=279
06/19/2022 21:43:15 - INFO - __main__ - Step 570 Global step 570 Train loss 1.22 on epoch=284
06/19/2022 21:43:16 - INFO - __main__ - Step 580 Global step 580 Train loss 1.04 on epoch=289
06/19/2022 21:43:18 - INFO - __main__ - Step 590 Global step 590 Train loss 1.04 on epoch=294
06/19/2022 21:43:19 - INFO - __main__ - Step 600 Global step 600 Train loss 1.03 on epoch=299
06/19/2022 21:43:19 - INFO - __main__ - Global step 600 Train loss 1.10 Classification-F1 0.3764102564102564 on epoch=299
06/19/2022 21:43:20 - INFO - __main__ - Step 610 Global step 610 Train loss 1.04 on epoch=304
06/19/2022 21:43:22 - INFO - __main__ - Step 620 Global step 620 Train loss 1.00 on epoch=309
06/19/2022 21:43:23 - INFO - __main__ - Step 630 Global step 630 Train loss 1.09 on epoch=314
06/19/2022 21:43:24 - INFO - __main__ - Step 640 Global step 640 Train loss 1.07 on epoch=319
06/19/2022 21:43:25 - INFO - __main__ - Step 650 Global step 650 Train loss 1.11 on epoch=324
06/19/2022 21:43:26 - INFO - __main__ - Global step 650 Train loss 1.06 Classification-F1 0.39139139139139134 on epoch=324
06/19/2022 21:43:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.96 on epoch=329
06/19/2022 21:43:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.90 on epoch=334
06/19/2022 21:43:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.96 on epoch=339
06/19/2022 21:43:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.86 on epoch=344
06/19/2022 21:43:32 - INFO - __main__ - Step 700 Global step 700 Train loss 1.08 on epoch=349
06/19/2022 21:43:32 - INFO - __main__ - Global step 700 Train loss 0.95 Classification-F1 0.3043478260869565 on epoch=349
06/19/2022 21:43:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.95 on epoch=354
06/19/2022 21:43:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.98 on epoch=359
06/19/2022 21:43:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.97 on epoch=364
06/19/2022 21:43:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.86 on epoch=369
06/19/2022 21:43:39 - INFO - __main__ - Step 750 Global step 750 Train loss 1.07 on epoch=374
06/19/2022 21:43:39 - INFO - __main__ - Global step 750 Train loss 0.97 Classification-F1 0.46843853820598 on epoch=374
06/19/2022 21:43:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.99 on epoch=379
06/19/2022 21:43:41 - INFO - __main__ - Step 770 Global step 770 Train loss 1.03 on epoch=384
06/19/2022 21:43:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.85 on epoch=389
06/19/2022 21:43:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.84 on epoch=394
06/19/2022 21:43:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.82 on epoch=399
06/19/2022 21:43:46 - INFO - __main__ - Global step 800 Train loss 0.91 Classification-F1 0.36374269005847953 on epoch=399
06/19/2022 21:43:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.79 on epoch=404
06/19/2022 21:43:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.85 on epoch=409
06/19/2022 21:43:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=414
06/19/2022 21:43:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.85 on epoch=419
06/19/2022 21:43:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.81 on epoch=424
06/19/2022 21:43:52 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.3107692307692308 on epoch=424
06/19/2022 21:43:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.83 on epoch=429
06/19/2022 21:43:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.78 on epoch=434
06/19/2022 21:43:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.72 on epoch=439
06/19/2022 21:43:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.87 on epoch=444
06/19/2022 21:43:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.84 on epoch=449
06/19/2022 21:43:59 - INFO - __main__ - Global step 900 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 21:44:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.79 on epoch=454
06/19/2022 21:44:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.78 on epoch=459
06/19/2022 21:44:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.79 on epoch=464
06/19/2022 21:44:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.74 on epoch=469
06/19/2022 21:44:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.71 on epoch=474
06/19/2022 21:44:05 - INFO - __main__ - Global step 950 Train loss 0.76 Classification-F1 0.39999999999999997 on epoch=474
06/19/2022 21:44:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.81 on epoch=479
06/19/2022 21:44:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.72 on epoch=484
06/19/2022 21:44:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.66 on epoch=489
06/19/2022 21:44:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.81 on epoch=494
06/19/2022 21:44:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.76 on epoch=499
06/19/2022 21:44:12 - INFO - __main__ - Global step 1000 Train loss 0.75 Classification-F1 0.4980392156862745 on epoch=499
06/19/2022 21:44:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.74 on epoch=504
06/19/2022 21:44:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.79 on epoch=509
06/19/2022 21:44:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.78 on epoch=514
06/19/2022 21:44:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.69 on epoch=519
06/19/2022 21:44:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.73 on epoch=524
06/19/2022 21:44:19 - INFO - __main__ - Global step 1050 Train loss 0.75 Classification-F1 0.3454545454545454 on epoch=524
06/19/2022 21:44:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.72 on epoch=529
06/19/2022 21:44:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.76 on epoch=534
06/19/2022 21:44:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.60 on epoch=539
06/19/2022 21:44:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.64 on epoch=544
06/19/2022 21:44:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.72 on epoch=549
06/19/2022 21:44:26 - INFO - __main__ - Global step 1100 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 21:44:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.74 on epoch=554
06/19/2022 21:44:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.65 on epoch=559
06/19/2022 21:44:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.60 on epoch=564
06/19/2022 21:44:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.66 on epoch=569
06/19/2022 21:44:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.61 on epoch=574
06/19/2022 21:44:32 - INFO - __main__ - Global step 1150 Train loss 0.65 Classification-F1 0.4554554554554554 on epoch=574
06/19/2022 21:44:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.61 on epoch=579
06/19/2022 21:44:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.71 on epoch=584
06/19/2022 21:44:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.69 on epoch=589
06/19/2022 21:44:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.74 on epoch=594
06/19/2022 21:44:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.65 on epoch=599
06/19/2022 21:44:39 - INFO - __main__ - Global step 1200 Train loss 0.68 Classification-F1 0.5465587044534412 on epoch=599
06/19/2022 21:44:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.77 on epoch=604
06/19/2022 21:44:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.68 on epoch=609
06/19/2022 21:44:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.61 on epoch=614
06/19/2022 21:44:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.53 on epoch=619
06/19/2022 21:44:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.63 on epoch=624
06/19/2022 21:44:46 - INFO - __main__ - Global step 1250 Train loss 0.64 Classification-F1 0.5588547189819725 on epoch=624
06/19/2022 21:44:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.59 on epoch=629
06/19/2022 21:44:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.75 on epoch=634
06/19/2022 21:44:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.64 on epoch=639
06/19/2022 21:44:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.65 on epoch=644
06/19/2022 21:44:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.69 on epoch=649
06/19/2022 21:44:52 - INFO - __main__ - Global step 1300 Train loss 0.66 Classification-F1 0.41700404858299595 on epoch=649
06/19/2022 21:44:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.76 on epoch=654
06/19/2022 21:44:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.62 on epoch=659
06/19/2022 21:44:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.60 on epoch=664
06/19/2022 21:44:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.55 on epoch=669
06/19/2022 21:44:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.60 on epoch=674
06/19/2022 21:44:59 - INFO - __main__ - Global step 1350 Train loss 0.62 Classification-F1 0.3454545454545454 on epoch=674
06/19/2022 21:45:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.69 on epoch=679
06/19/2022 21:45:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.58 on epoch=684
06/19/2022 21:45:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.66 on epoch=689
06/19/2022 21:45:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.60 on epoch=694
06/19/2022 21:45:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.62 on epoch=699
06/19/2022 21:45:06 - INFO - __main__ - Global step 1400 Train loss 0.63 Classification-F1 0.4909862142099682 on epoch=699
06/19/2022 21:45:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.51 on epoch=704
06/19/2022 21:45:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=709
06/19/2022 21:45:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.57 on epoch=714
06/19/2022 21:45:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.63 on epoch=719
06/19/2022 21:45:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.59 on epoch=724
06/19/2022 21:45:12 - INFO - __main__ - Global step 1450 Train loss 0.55 Classification-F1 0.3992490613266583 on epoch=724
06/19/2022 21:45:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.57 on epoch=729
06/19/2022 21:45:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.62 on epoch=734
06/19/2022 21:45:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.56 on epoch=739
06/19/2022 21:45:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.59 on epoch=744
06/19/2022 21:45:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.56 on epoch=749
06/19/2022 21:45:19 - INFO - __main__ - Global step 1500 Train loss 0.58 Classification-F1 0.3992490613266583 on epoch=749
06/19/2022 21:45:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.54 on epoch=754
06/19/2022 21:45:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.58 on epoch=759
06/19/2022 21:45:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.57 on epoch=764
06/19/2022 21:45:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.58 on epoch=769
06/19/2022 21:45:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.65 on epoch=774
06/19/2022 21:45:26 - INFO - __main__ - Global step 1550 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 21:45:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.53 on epoch=779
06/19/2022 21:45:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.53 on epoch=784
06/19/2022 21:45:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.59 on epoch=789
06/19/2022 21:45:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.63 on epoch=794
06/19/2022 21:45:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.59 on epoch=799
06/19/2022 21:45:33 - INFO - __main__ - Global step 1600 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 21:45:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.55 on epoch=804
06/19/2022 21:45:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.54 on epoch=809
06/19/2022 21:45:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.56 on epoch=814
06/19/2022 21:45:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.58 on epoch=819
06/19/2022 21:45:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.55 on epoch=824
06/19/2022 21:45:39 - INFO - __main__ - Global step 1650 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 21:45:40 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.56 on epoch=829
06/19/2022 21:45:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.55 on epoch=834
06/19/2022 21:45:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.52 on epoch=839
06/19/2022 21:45:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.57 on epoch=844
06/19/2022 21:45:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.56 on epoch=849
06/19/2022 21:45:46 - INFO - __main__ - Global step 1700 Train loss 0.55 Classification-F1 0.4920634920634921 on epoch=849
06/19/2022 21:45:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.58 on epoch=854
06/19/2022 21:45:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.49 on epoch=859
06/19/2022 21:45:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.60 on epoch=864
06/19/2022 21:45:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.53 on epoch=869
06/19/2022 21:45:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.55 on epoch=874
06/19/2022 21:45:53 - INFO - __main__ - Global step 1750 Train loss 0.55 Classification-F1 0.3816425120772947 on epoch=874
06/19/2022 21:45:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.50 on epoch=879
06/19/2022 21:45:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.57 on epoch=884
06/19/2022 21:45:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=889
06/19/2022 21:45:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.50 on epoch=894
06/19/2022 21:45:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.57 on epoch=899
06/19/2022 21:45:59 - INFO - __main__ - Global step 1800 Train loss 0.52 Classification-F1 0.3191489361702127 on epoch=899
06/19/2022 21:46:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.48 on epoch=904
06/19/2022 21:46:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.64 on epoch=909
06/19/2022 21:46:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.54 on epoch=914
06/19/2022 21:46:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.51 on epoch=919
06/19/2022 21:46:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.58 on epoch=924
06/19/2022 21:46:06 - INFO - __main__ - Global step 1850 Train loss 0.55 Classification-F1 0.3816425120772947 on epoch=924
06/19/2022 21:46:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.44 on epoch=929
06/19/2022 21:46:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.56 on epoch=934
06/19/2022 21:46:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.49 on epoch=939
06/19/2022 21:46:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=944
06/19/2022 21:46:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.58 on epoch=949
06/19/2022 21:46:13 - INFO - __main__ - Global step 1900 Train loss 0.51 Classification-F1 0.4231177094379639 on epoch=949
06/19/2022 21:46:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.50 on epoch=954
06/19/2022 21:46:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.51 on epoch=959
06/19/2022 21:46:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.52 on epoch=964
06/19/2022 21:46:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.45 on epoch=969
06/19/2022 21:46:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.50 on epoch=974
06/19/2022 21:46:19 - INFO - __main__ - Global step 1950 Train loss 0.49 Classification-F1 0.3992490613266583 on epoch=974
06/19/2022 21:46:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.52 on epoch=979
06/19/2022 21:46:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.48 on epoch=984
06/19/2022 21:46:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=989
06/19/2022 21:46:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.55 on epoch=994
06/19/2022 21:46:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.52 on epoch=999
06/19/2022 21:46:26 - INFO - __main__ - Global step 2000 Train loss 0.51 Classification-F1 0.5076923076923077 on epoch=999
06/19/2022 21:46:26 - INFO - __main__ - save last model!
06/19/2022 21:46:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 21:46:26 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 21:46:26 - INFO - __main__ - Printing 3 examples
06/19/2022 21:46:26 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 21:46:26 - INFO - __main__ - ['0']
06/19/2022 21:46:26 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 21:46:26 - INFO - __main__ - ['1']
06/19/2022 21:46:26 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 21:46:26 - INFO - __main__ - ['1']
06/19/2022 21:46:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:46:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:46:27 - INFO - __main__ - Printing 3 examples
06/19/2022 21:46:27 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 21:46:27 - INFO - __main__ - ['1']
06/19/2022 21:46:27 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 21:46:27 - INFO - __main__ - ['1']
06/19/2022 21:46:27 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 21:46:27 - INFO - __main__ - ['1']
06/19/2022 21:46:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:46:27 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:46:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:46:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:46:27 - INFO - __main__ - Printing 3 examples
06/19/2022 21:46:27 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 21:46:27 - INFO - __main__ - ['1']
06/19/2022 21:46:27 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 21:46:27 - INFO - __main__ - ['1']
06/19/2022 21:46:27 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 21:46:27 - INFO - __main__ - ['1']
06/19/2022 21:46:27 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:46:27 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:46:27 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:46:30 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:46:33 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:46:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:46:33 - INFO - __main__ - Starting training!
06/19/2022 21:46:38 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 21:48:08 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_13_0.5_8_predictions.txt
06/19/2022 21:48:09 - INFO - __main__ - Classification-F1 on test data: 0.4652
06/19/2022 21:48:09 - INFO - __main__ - prefix=paws_16_13, lr=0.5, bsz=8, dev_performance=0.5835835835835835, test_performance=0.46524707844830115
06/19/2022 21:48:09 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.4, bsz=8 ...
06/19/2022 21:48:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:48:10 - INFO - __main__ - Printing 3 examples
06/19/2022 21:48:10 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 21:48:10 - INFO - __main__ - ['1']
06/19/2022 21:48:10 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 21:48:10 - INFO - __main__ - ['1']
06/19/2022 21:48:10 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 21:48:10 - INFO - __main__ - ['1']
06/19/2022 21:48:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:48:10 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:48:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:48:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:48:10 - INFO - __main__ - Printing 3 examples
06/19/2022 21:48:10 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 21:48:10 - INFO - __main__ - ['1']
06/19/2022 21:48:10 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 21:48:10 - INFO - __main__ - ['1']
06/19/2022 21:48:10 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 21:48:10 - INFO - __main__ - ['1']
06/19/2022 21:48:10 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:48:10 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:48:10 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:48:16 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:48:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:48:16 - INFO - __main__ - Starting training!
06/19/2022 21:48:17 - INFO - __main__ - Step 10 Global step 10 Train loss 6.04 on epoch=4
06/19/2022 21:48:19 - INFO - __main__ - Step 20 Global step 20 Train loss 5.60 on epoch=9
06/19/2022 21:48:20 - INFO - __main__ - Step 30 Global step 30 Train loss 5.22 on epoch=14
06/19/2022 21:48:21 - INFO - __main__ - Step 40 Global step 40 Train loss 4.86 on epoch=19
06/19/2022 21:48:22 - INFO - __main__ - Step 50 Global step 50 Train loss 4.56 on epoch=24
06/19/2022 21:48:24 - INFO - __main__ - Global step 50 Train loss 5.25 Classification-F1 0.0 on epoch=24
06/19/2022 21:48:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 21:48:25 - INFO - __main__ - Step 60 Global step 60 Train loss 4.19 on epoch=29
06/19/2022 21:48:26 - INFO - __main__ - Step 70 Global step 70 Train loss 4.07 on epoch=34
06/19/2022 21:48:27 - INFO - __main__ - Step 80 Global step 80 Train loss 3.90 on epoch=39
06/19/2022 21:48:29 - INFO - __main__ - Step 90 Global step 90 Train loss 3.82 on epoch=44
06/19/2022 21:48:30 - INFO - __main__ - Step 100 Global step 100 Train loss 3.71 on epoch=49
06/19/2022 21:48:40 - INFO - __main__ - Global step 100 Train loss 3.94 Classification-F1 0.0 on epoch=49
06/19/2022 21:48:41 - INFO - __main__ - Step 110 Global step 110 Train loss 3.79 on epoch=54
06/19/2022 21:48:43 - INFO - __main__ - Step 120 Global step 120 Train loss 3.53 on epoch=59
06/19/2022 21:48:44 - INFO - __main__ - Step 130 Global step 130 Train loss 3.63 on epoch=64
06/19/2022 21:48:45 - INFO - __main__ - Step 140 Global step 140 Train loss 3.60 on epoch=69
06/19/2022 21:48:46 - INFO - __main__ - Step 150 Global step 150 Train loss 3.38 on epoch=74
06/19/2022 21:48:54 - INFO - __main__ - Global step 150 Train loss 3.59 Classification-F1 0.0 on epoch=74
06/19/2022 21:48:55 - INFO - __main__ - Step 160 Global step 160 Train loss 3.31 on epoch=79
06/19/2022 21:48:56 - INFO - __main__ - Step 170 Global step 170 Train loss 3.17 on epoch=84
06/19/2022 21:48:57 - INFO - __main__ - Step 180 Global step 180 Train loss 3.07 on epoch=89
06/19/2022 21:48:58 - INFO - __main__ - Step 190 Global step 190 Train loss 2.87 on epoch=94
06/19/2022 21:49:00 - INFO - __main__ - Step 200 Global step 200 Train loss 2.96 on epoch=99
06/19/2022 21:49:01 - INFO - __main__ - Global step 200 Train loss 3.07 Classification-F1 0.08771929824561403 on epoch=99
06/19/2022 21:49:01 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.08771929824561403 on epoch=99, global_step=200
06/19/2022 21:49:03 - INFO - __main__ - Step 210 Global step 210 Train loss 2.84 on epoch=104
06/19/2022 21:49:04 - INFO - __main__ - Step 220 Global step 220 Train loss 2.83 on epoch=109
06/19/2022 21:49:05 - INFO - __main__ - Step 230 Global step 230 Train loss 2.72 on epoch=114
06/19/2022 21:49:06 - INFO - __main__ - Step 240 Global step 240 Train loss 2.74 on epoch=119
06/19/2022 21:49:08 - INFO - __main__ - Step 250 Global step 250 Train loss 2.78 on epoch=124
06/19/2022 21:49:10 - INFO - __main__ - Global step 250 Train loss 2.78 Classification-F1 0.08637873754152824 on epoch=124
06/19/2022 21:49:11 - INFO - __main__ - Step 260 Global step 260 Train loss 2.56 on epoch=129
06/19/2022 21:49:13 - INFO - __main__ - Step 270 Global step 270 Train loss 2.51 on epoch=134
06/19/2022 21:49:14 - INFO - __main__ - Step 280 Global step 280 Train loss 2.36 on epoch=139
06/19/2022 21:49:15 - INFO - __main__ - Step 290 Global step 290 Train loss 2.42 on epoch=144
06/19/2022 21:49:16 - INFO - __main__ - Step 300 Global step 300 Train loss 2.21 on epoch=149
06/19/2022 21:49:19 - INFO - __main__ - Global step 300 Train loss 2.41 Classification-F1 0.21276595744680848 on epoch=149
06/19/2022 21:49:19 - INFO - __main__ - Saving model with best Classification-F1: 0.08771929824561403 -> 0.21276595744680848 on epoch=149, global_step=300
06/19/2022 21:49:20 - INFO - __main__ - Step 310 Global step 310 Train loss 2.28 on epoch=154
06/19/2022 21:49:22 - INFO - __main__ - Step 320 Global step 320 Train loss 2.16 on epoch=159
06/19/2022 21:49:23 - INFO - __main__ - Step 330 Global step 330 Train loss 2.12 on epoch=164
06/19/2022 21:49:25 - INFO - __main__ - Step 340 Global step 340 Train loss 2.11 on epoch=169
06/19/2022 21:49:26 - INFO - __main__ - Step 350 Global step 350 Train loss 2.11 on epoch=174
06/19/2022 21:49:26 - INFO - __main__ - Global step 350 Train loss 2.16 Classification-F1 0.43529411764705883 on epoch=174
06/19/2022 21:49:26 - INFO - __main__ - Saving model with best Classification-F1: 0.21276595744680848 -> 0.43529411764705883 on epoch=174, global_step=350
06/19/2022 21:49:28 - INFO - __main__ - Step 360 Global step 360 Train loss 1.86 on epoch=179
06/19/2022 21:49:30 - INFO - __main__ - Step 370 Global step 370 Train loss 1.86 on epoch=184
06/19/2022 21:49:31 - INFO - __main__ - Step 380 Global step 380 Train loss 1.71 on epoch=189
06/19/2022 21:49:32 - INFO - __main__ - Step 390 Global step 390 Train loss 1.68 on epoch=194
06/19/2022 21:49:34 - INFO - __main__ - Step 400 Global step 400 Train loss 1.60 on epoch=199
06/19/2022 21:49:35 - INFO - __main__ - Global step 400 Train loss 1.74 Classification-F1 0.5588547189819725 on epoch=199
06/19/2022 21:49:35 - INFO - __main__ - Saving model with best Classification-F1: 0.43529411764705883 -> 0.5588547189819725 on epoch=199, global_step=400
06/19/2022 21:49:37 - INFO - __main__ - Step 410 Global step 410 Train loss 1.61 on epoch=204
06/19/2022 21:49:38 - INFO - __main__ - Step 420 Global step 420 Train loss 1.57 on epoch=209
06/19/2022 21:49:39 - INFO - __main__ - Step 430 Global step 430 Train loss 1.60 on epoch=214
06/19/2022 21:49:41 - INFO - __main__ - Step 440 Global step 440 Train loss 1.47 on epoch=219
06/19/2022 21:49:42 - INFO - __main__ - Step 450 Global step 450 Train loss 1.51 on epoch=224
06/19/2022 21:49:42 - INFO - __main__ - Global step 450 Train loss 1.55 Classification-F1 0.37662337662337664 on epoch=224
06/19/2022 21:49:43 - INFO - __main__ - Step 460 Global step 460 Train loss 1.35 on epoch=229
06/19/2022 21:49:45 - INFO - __main__ - Step 470 Global step 470 Train loss 1.28 on epoch=234
06/19/2022 21:49:46 - INFO - __main__ - Step 480 Global step 480 Train loss 1.36 on epoch=239
06/19/2022 21:49:47 - INFO - __main__ - Step 490 Global step 490 Train loss 1.20 on epoch=244
06/19/2022 21:49:48 - INFO - __main__ - Step 500 Global step 500 Train loss 1.25 on epoch=249
06/19/2022 21:49:49 - INFO - __main__ - Global step 500 Train loss 1.29 Classification-F1 0.3454545454545454 on epoch=249
06/19/2022 21:49:50 - INFO - __main__ - Step 510 Global step 510 Train loss 1.23 on epoch=254
06/19/2022 21:49:51 - INFO - __main__ - Step 520 Global step 520 Train loss 1.10 on epoch=259
06/19/2022 21:49:52 - INFO - __main__ - Step 530 Global step 530 Train loss 1.09 on epoch=264
06/19/2022 21:49:54 - INFO - __main__ - Step 540 Global step 540 Train loss 1.10 on epoch=269
06/19/2022 21:49:55 - INFO - __main__ - Step 550 Global step 550 Train loss 1.09 on epoch=274
06/19/2022 21:49:55 - INFO - __main__ - Global step 550 Train loss 1.12 Classification-F1 0.3191489361702127 on epoch=274
06/19/2022 21:49:56 - INFO - __main__ - Step 560 Global step 560 Train loss 1.13 on epoch=279
06/19/2022 21:49:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.99 on epoch=284
06/19/2022 21:49:59 - INFO - __main__ - Step 580 Global step 580 Train loss 1.10 on epoch=289
06/19/2022 21:50:00 - INFO - __main__ - Step 590 Global step 590 Train loss 1.05 on epoch=294
06/19/2022 21:50:01 - INFO - __main__ - Step 600 Global step 600 Train loss 1.02 on epoch=299
06/19/2022 21:50:02 - INFO - __main__ - Global step 600 Train loss 1.06 Classification-F1 0.4420512820512821 on epoch=299
06/19/2022 21:50:03 - INFO - __main__ - Step 610 Global step 610 Train loss 1.02 on epoch=304
06/19/2022 21:50:04 - INFO - __main__ - Step 620 Global step 620 Train loss 1.00 on epoch=309
06/19/2022 21:50:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.88 on epoch=314
06/19/2022 21:50:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.93 on epoch=319
06/19/2022 21:50:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.96 on epoch=324
06/19/2022 21:50:08 - INFO - __main__ - Global step 650 Train loss 0.96 Classification-F1 0.4554554554554554 on epoch=324
06/19/2022 21:50:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.87 on epoch=329
06/19/2022 21:50:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.99 on epoch=334
06/19/2022 21:50:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.93 on epoch=339
06/19/2022 21:50:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.93 on epoch=344
06/19/2022 21:50:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.93 on epoch=349
06/19/2022 21:50:15 - INFO - __main__ - Global step 700 Train loss 0.93 Classification-F1 0.4181818181818182 on epoch=349
06/19/2022 21:50:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.85 on epoch=354
06/19/2022 21:50:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.87 on epoch=359
06/19/2022 21:50:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.86 on epoch=364
06/19/2022 21:50:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.92 on epoch=369
06/19/2022 21:50:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.85 on epoch=374
06/19/2022 21:50:21 - INFO - __main__ - Global step 750 Train loss 0.87 Classification-F1 0.39756367663344405 on epoch=374
06/19/2022 21:50:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.92 on epoch=379
06/19/2022 21:50:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.84 on epoch=384
06/19/2022 21:50:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.87 on epoch=389
06/19/2022 21:50:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.82 on epoch=394
06/19/2022 21:50:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.82 on epoch=399
06/19/2022 21:50:28 - INFO - __main__ - Global step 800 Train loss 0.85 Classification-F1 0.464039408866995 on epoch=399
06/19/2022 21:50:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.83 on epoch=404
06/19/2022 21:50:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.74 on epoch=409
06/19/2022 21:50:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=414
06/19/2022 21:50:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.75 on epoch=419
06/19/2022 21:50:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.76 on epoch=424
06/19/2022 21:50:34 - INFO - __main__ - Global step 850 Train loss 0.78 Classification-F1 0.3273273273273273 on epoch=424
06/19/2022 21:50:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.78 on epoch=429
06/19/2022 21:50:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.75 on epoch=434
06/19/2022 21:50:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.78 on epoch=439
06/19/2022 21:50:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.75 on epoch=444
06/19/2022 21:50:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.72 on epoch=449
06/19/2022 21:50:41 - INFO - __main__ - Global step 900 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 21:50:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.75 on epoch=454
06/19/2022 21:50:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.85 on epoch=459
06/19/2022 21:50:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.76 on epoch=464
06/19/2022 21:50:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.82 on epoch=469
06/19/2022 21:50:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.68 on epoch=474
06/19/2022 21:50:47 - INFO - __main__ - Global step 950 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 21:50:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.71 on epoch=479
06/19/2022 21:50:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.77 on epoch=484
06/19/2022 21:50:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.78 on epoch=489
06/19/2022 21:50:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.78 on epoch=494
06/19/2022 21:50:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.79 on epoch=499
06/19/2022 21:50:54 - INFO - __main__ - Global step 1000 Train loss 0.76 Classification-F1 0.5844155844155844 on epoch=499
06/19/2022 21:50:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5588547189819725 -> 0.5844155844155844 on epoch=499, global_step=1000
06/19/2022 21:50:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.71 on epoch=504
06/19/2022 21:50:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.69 on epoch=509
06/19/2022 21:50:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.77 on epoch=514
06/19/2022 21:50:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.77 on epoch=519
06/19/2022 21:51:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.66 on epoch=524
06/19/2022 21:51:00 - INFO - __main__ - Global step 1050 Train loss 0.72 Classification-F1 0.4385964912280702 on epoch=524
06/19/2022 21:51:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.77 on epoch=529
06/19/2022 21:51:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.71 on epoch=534
06/19/2022 21:51:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.68 on epoch=539
06/19/2022 21:51:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.71 on epoch=544
06/19/2022 21:51:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.71 on epoch=549
06/19/2022 21:51:07 - INFO - __main__ - Global step 1100 Train loss 0.71 Classification-F1 0.39756367663344405 on epoch=549
06/19/2022 21:51:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.72 on epoch=554
06/19/2022 21:51:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.65 on epoch=559
06/19/2022 21:51:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.60 on epoch=564
06/19/2022 21:51:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.70 on epoch=569
06/19/2022 21:51:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.75 on epoch=574
06/19/2022 21:51:13 - INFO - __main__ - Global step 1150 Train loss 0.68 Classification-F1 0.4181818181818182 on epoch=574
06/19/2022 21:51:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.58 on epoch=579
06/19/2022 21:51:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.71 on epoch=584
06/19/2022 21:51:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.69 on epoch=589
06/19/2022 21:51:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.62 on epoch=594
06/19/2022 21:51:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.65 on epoch=599
06/19/2022 21:51:20 - INFO - __main__ - Global step 1200 Train loss 0.65 Classification-F1 0.4231177094379639 on epoch=599
06/19/2022 21:51:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.63 on epoch=604
06/19/2022 21:51:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.66 on epoch=609
06/19/2022 21:51:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.64 on epoch=614
06/19/2022 21:51:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.66 on epoch=619
06/19/2022 21:51:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.68 on epoch=624
06/19/2022 21:51:26 - INFO - __main__ - Global step 1250 Train loss 0.65 Classification-F1 0.4231177094379639 on epoch=624
06/19/2022 21:51:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.70 on epoch=629
06/19/2022 21:51:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.70 on epoch=634
06/19/2022 21:51:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.58 on epoch=639
06/19/2022 21:51:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.62 on epoch=644
06/19/2022 21:51:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.60 on epoch=649
06/19/2022 21:51:33 - INFO - __main__ - Global step 1300 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 21:51:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.63 on epoch=654
06/19/2022 21:51:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.69 on epoch=659
06/19/2022 21:51:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.60 on epoch=664
06/19/2022 21:51:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.68 on epoch=669
06/19/2022 21:51:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.67 on epoch=674
06/19/2022 21:51:39 - INFO - __main__ - Global step 1350 Train loss 0.65 Classification-F1 0.3650793650793651 on epoch=674
06/19/2022 21:51:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.58 on epoch=679
06/19/2022 21:51:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.62 on epoch=684
06/19/2022 21:51:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.69 on epoch=689
06/19/2022 21:51:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.62 on epoch=694
06/19/2022 21:51:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.67 on epoch=699
06/19/2022 21:51:46 - INFO - __main__ - Global step 1400 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 21:51:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.55 on epoch=704
06/19/2022 21:51:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.65 on epoch=709
06/19/2022 21:51:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.57 on epoch=714
06/19/2022 21:51:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.62 on epoch=719
06/19/2022 21:51:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.66 on epoch=724
06/19/2022 21:51:52 - INFO - __main__ - Global step 1450 Train loss 0.61 Classification-F1 0.46843853820598 on epoch=724
06/19/2022 21:51:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.53 on epoch=729
06/19/2022 21:51:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.74 on epoch=734
06/19/2022 21:51:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.65 on epoch=739
06/19/2022 21:51:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.61 on epoch=744
06/19/2022 21:51:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.57 on epoch=749
06/19/2022 21:51:59 - INFO - __main__ - Global step 1500 Train loss 0.62 Classification-F1 0.4666666666666667 on epoch=749
06/19/2022 21:52:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.64 on epoch=754
06/19/2022 21:52:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.54 on epoch=759
06/19/2022 21:52:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.58 on epoch=764
06/19/2022 21:52:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.60 on epoch=769
06/19/2022 21:52:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.61 on epoch=774
06/19/2022 21:52:05 - INFO - __main__ - Global step 1550 Train loss 0.60 Classification-F1 0.5151515151515151 on epoch=774
06/19/2022 21:52:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.61 on epoch=779
06/19/2022 21:52:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.57 on epoch=784
06/19/2022 21:52:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.54 on epoch=789
06/19/2022 21:52:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.58 on epoch=794
06/19/2022 21:52:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.65 on epoch=799
06/19/2022 21:52:12 - INFO - __main__ - Global step 1600 Train loss 0.59 Classification-F1 0.5307917888563051 on epoch=799
06/19/2022 21:52:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.59 on epoch=804
06/19/2022 21:52:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.54 on epoch=809
06/19/2022 21:52:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.54 on epoch=814
06/19/2022 21:52:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.53 on epoch=819
06/19/2022 21:52:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.57 on epoch=824
06/19/2022 21:52:18 - INFO - __main__ - Global step 1650 Train loss 0.55 Classification-F1 0.26666666666666666 on epoch=824
06/19/2022 21:52:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.56 on epoch=829
06/19/2022 21:52:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.63 on epoch=834
06/19/2022 21:52:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.57 on epoch=839
06/19/2022 21:52:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.54 on epoch=844
06/19/2022 21:52:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.56 on epoch=849
06/19/2022 21:52:25 - INFO - __main__ - Global step 1700 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 21:52:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.60 on epoch=854
06/19/2022 21:52:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.54 on epoch=859
06/19/2022 21:52:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.54 on epoch=864
06/19/2022 21:52:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.52 on epoch=869
06/19/2022 21:52:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.61 on epoch=874
06/19/2022 21:52:31 - INFO - __main__ - Global step 1750 Train loss 0.56 Classification-F1 0.4980392156862745 on epoch=874
06/19/2022 21:52:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.52 on epoch=879
06/19/2022 21:52:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.52 on epoch=884
06/19/2022 21:52:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=889
06/19/2022 21:52:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.54 on epoch=894
06/19/2022 21:52:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.48 on epoch=899
06/19/2022 21:52:38 - INFO - __main__ - Global step 1800 Train loss 0.51 Classification-F1 0.40566959921798634 on epoch=899
06/19/2022 21:52:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.55 on epoch=904
06/19/2022 21:52:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.53 on epoch=909
06/19/2022 21:52:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.52 on epoch=914
06/19/2022 21:52:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.52 on epoch=919
06/19/2022 21:52:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.54 on epoch=924
06/19/2022 21:52:44 - INFO - __main__ - Global step 1850 Train loss 0.53 Classification-F1 0.4420512820512821 on epoch=924
06/19/2022 21:52:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.58 on epoch=929
06/19/2022 21:52:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.47 on epoch=934
06/19/2022 21:52:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.62 on epoch=939
06/19/2022 21:52:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.53 on epoch=944
06/19/2022 21:52:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.59 on epoch=949
06/19/2022 21:52:51 - INFO - __main__ - Global step 1900 Train loss 0.56 Classification-F1 0.43529411764705883 on epoch=949
06/19/2022 21:52:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.57 on epoch=954
06/19/2022 21:52:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.53 on epoch=959
06/19/2022 21:52:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.60 on epoch=964
06/19/2022 21:52:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.61 on epoch=969
06/19/2022 21:52:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.60 on epoch=974
06/19/2022 21:52:57 - INFO - __main__ - Global step 1950 Train loss 0.58 Classification-F1 0.3992490613266583 on epoch=974
06/19/2022 21:52:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.58 on epoch=979
06/19/2022 21:53:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.51 on epoch=984
06/19/2022 21:53:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.54 on epoch=989
06/19/2022 21:53:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.53 on epoch=994
06/19/2022 21:53:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.58 on epoch=999
06/19/2022 21:53:04 - INFO - __main__ - Global step 2000 Train loss 0.55 Classification-F1 0.5333333333333333 on epoch=999
06/19/2022 21:53:04 - INFO - __main__ - save last model!
06/19/2022 21:53:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 21:53:04 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 21:53:04 - INFO - __main__ - Printing 3 examples
06/19/2022 21:53:04 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 21:53:04 - INFO - __main__ - ['0']
06/19/2022 21:53:04 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 21:53:04 - INFO - __main__ - ['1']
06/19/2022 21:53:04 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 21:53:04 - INFO - __main__ - ['1']
06/19/2022 21:53:04 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:53:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:53:05 - INFO - __main__ - Printing 3 examples
06/19/2022 21:53:05 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 21:53:05 - INFO - __main__ - ['1']
06/19/2022 21:53:05 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 21:53:05 - INFO - __main__ - ['1']
06/19/2022 21:53:05 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 21:53:05 - INFO - __main__ - ['1']
06/19/2022 21:53:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:53:05 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:53:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:53:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:53:05 - INFO - __main__ - Printing 3 examples
06/19/2022 21:53:05 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 21:53:05 - INFO - __main__ - ['1']
06/19/2022 21:53:05 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 21:53:05 - INFO - __main__ - ['1']
06/19/2022 21:53:05 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 21:53:05 - INFO - __main__ - ['1']
06/19/2022 21:53:05 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:53:05 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:53:05 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:53:08 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:53:10 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:53:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:53:11 - INFO - __main__ - Starting training!
06/19/2022 21:53:16 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 21:54:43 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_13_0.4_8_predictions.txt
06/19/2022 21:54:43 - INFO - __main__ - Classification-F1 on test data: 0.4135
06/19/2022 21:54:44 - INFO - __main__ - prefix=paws_16_13, lr=0.4, bsz=8, dev_performance=0.5844155844155844, test_performance=0.4135401820846797
06/19/2022 21:54:44 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.3, bsz=8 ...
06/19/2022 21:54:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:54:45 - INFO - __main__ - Printing 3 examples
06/19/2022 21:54:45 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 21:54:45 - INFO - __main__ - ['1']
06/19/2022 21:54:45 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 21:54:45 - INFO - __main__ - ['1']
06/19/2022 21:54:45 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 21:54:45 - INFO - __main__ - ['1']
06/19/2022 21:54:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:54:45 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:54:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:54:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:54:45 - INFO - __main__ - Printing 3 examples
06/19/2022 21:54:45 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 21:54:45 - INFO - __main__ - ['1']
06/19/2022 21:54:45 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 21:54:45 - INFO - __main__ - ['1']
06/19/2022 21:54:45 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 21:54:45 - INFO - __main__ - ['1']
06/19/2022 21:54:45 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:54:45 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:54:45 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:54:50 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:54:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:54:51 - INFO - __main__ - Starting training!
06/19/2022 21:54:52 - INFO - __main__ - Step 10 Global step 10 Train loss 5.93 on epoch=4
06/19/2022 21:54:53 - INFO - __main__ - Step 20 Global step 20 Train loss 5.79 on epoch=9
06/19/2022 21:54:55 - INFO - __main__ - Step 30 Global step 30 Train loss 5.21 on epoch=14
06/19/2022 21:54:56 - INFO - __main__ - Step 40 Global step 40 Train loss 4.75 on epoch=19
06/19/2022 21:54:57 - INFO - __main__ - Step 50 Global step 50 Train loss 4.41 on epoch=24
06/19/2022 21:54:58 - INFO - __main__ - Global step 50 Train loss 5.22 Classification-F1 0.0 on epoch=24
06/19/2022 21:54:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 21:54:59 - INFO - __main__ - Step 60 Global step 60 Train loss 4.09 on epoch=29
06/19/2022 21:55:01 - INFO - __main__ - Step 70 Global step 70 Train loss 4.08 on epoch=34
06/19/2022 21:55:02 - INFO - __main__ - Step 80 Global step 80 Train loss 3.92 on epoch=39
06/19/2022 21:55:03 - INFO - __main__ - Step 90 Global step 90 Train loss 3.78 on epoch=44
06/19/2022 21:55:04 - INFO - __main__ - Step 100 Global step 100 Train loss 3.73 on epoch=49
06/19/2022 21:55:08 - INFO - __main__ - Global step 100 Train loss 3.92 Classification-F1 0.0 on epoch=49
06/19/2022 21:55:09 - INFO - __main__ - Step 110 Global step 110 Train loss 3.76 on epoch=54
06/19/2022 21:55:10 - INFO - __main__ - Step 120 Global step 120 Train loss 3.60 on epoch=59
06/19/2022 21:55:11 - INFO - __main__ - Step 130 Global step 130 Train loss 3.45 on epoch=64
06/19/2022 21:55:12 - INFO - __main__ - Step 140 Global step 140 Train loss 3.31 on epoch=69
06/19/2022 21:55:14 - INFO - __main__ - Step 150 Global step 150 Train loss 3.28 on epoch=74
06/19/2022 21:55:16 - INFO - __main__ - Global step 150 Train loss 3.48 Classification-F1 0.0 on epoch=74
06/19/2022 21:55:17 - INFO - __main__ - Step 160 Global step 160 Train loss 3.28 on epoch=79
06/19/2022 21:55:18 - INFO - __main__ - Step 170 Global step 170 Train loss 3.10 on epoch=84
06/19/2022 21:55:19 - INFO - __main__ - Step 180 Global step 180 Train loss 2.97 on epoch=89
06/19/2022 21:55:21 - INFO - __main__ - Step 190 Global step 190 Train loss 2.81 on epoch=94
06/19/2022 21:55:22 - INFO - __main__ - Step 200 Global step 200 Train loss 2.87 on epoch=99
06/19/2022 21:55:22 - INFO - __main__ - Global step 200 Train loss 3.01 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 21:55:22 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=99, global_step=200
06/19/2022 21:55:24 - INFO - __main__ - Step 210 Global step 210 Train loss 2.78 on epoch=104
06/19/2022 21:55:25 - INFO - __main__ - Step 220 Global step 220 Train loss 2.63 on epoch=109
06/19/2022 21:55:26 - INFO - __main__ - Step 230 Global step 230 Train loss 2.52 on epoch=114
06/19/2022 21:55:27 - INFO - __main__ - Step 240 Global step 240 Train loss 2.49 on epoch=119
06/19/2022 21:55:28 - INFO - __main__ - Step 250 Global step 250 Train loss 2.49 on epoch=124
06/19/2022 21:55:32 - INFO - __main__ - Global step 250 Train loss 2.58 Classification-F1 0.12444444444444444 on epoch=124
06/19/2022 21:55:33 - INFO - __main__ - Step 260 Global step 260 Train loss 2.44 on epoch=129
06/19/2022 21:55:34 - INFO - __main__ - Step 270 Global step 270 Train loss 2.41 on epoch=134
06/19/2022 21:55:36 - INFO - __main__ - Step 280 Global step 280 Train loss 2.23 on epoch=139
06/19/2022 21:55:37 - INFO - __main__ - Step 290 Global step 290 Train loss 2.39 on epoch=144
06/19/2022 21:55:38 - INFO - __main__ - Step 300 Global step 300 Train loss 2.10 on epoch=149
06/19/2022 21:55:39 - INFO - __main__ - Global step 300 Train loss 2.31 Classification-F1 0.4847870182555781 on epoch=149
06/19/2022 21:55:39 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4847870182555781 on epoch=149, global_step=300
06/19/2022 21:55:41 - INFO - __main__ - Step 310 Global step 310 Train loss 2.02 on epoch=154
06/19/2022 21:55:42 - INFO - __main__ - Step 320 Global step 320 Train loss 2.08 on epoch=159
06/19/2022 21:55:43 - INFO - __main__ - Step 330 Global step 330 Train loss 2.04 on epoch=164
06/19/2022 21:55:44 - INFO - __main__ - Step 340 Global step 340 Train loss 1.96 on epoch=169
06/19/2022 21:55:45 - INFO - __main__ - Step 350 Global step 350 Train loss 1.88 on epoch=174
06/19/2022 21:55:47 - INFO - __main__ - Global step 350 Train loss 2.00 Classification-F1 0.464039408866995 on epoch=174
06/19/2022 21:55:48 - INFO - __main__ - Step 360 Global step 360 Train loss 1.80 on epoch=179
06/19/2022 21:55:49 - INFO - __main__ - Step 370 Global step 370 Train loss 1.86 on epoch=184
06/19/2022 21:55:50 - INFO - __main__ - Step 380 Global step 380 Train loss 1.71 on epoch=189
06/19/2022 21:55:52 - INFO - __main__ - Step 390 Global step 390 Train loss 1.66 on epoch=194
06/19/2022 21:55:53 - INFO - __main__ - Step 400 Global step 400 Train loss 1.63 on epoch=199
06/19/2022 21:55:53 - INFO - __main__ - Global step 400 Train loss 1.73 Classification-F1 0.5151515151515151 on epoch=199
06/19/2022 21:55:53 - INFO - __main__ - Saving model with best Classification-F1: 0.4847870182555781 -> 0.5151515151515151 on epoch=199, global_step=400
06/19/2022 21:55:55 - INFO - __main__ - Step 410 Global step 410 Train loss 1.61 on epoch=204
06/19/2022 21:55:56 - INFO - __main__ - Step 420 Global step 420 Train loss 1.55 on epoch=209
06/19/2022 21:55:57 - INFO - __main__ - Step 430 Global step 430 Train loss 1.46 on epoch=214
06/19/2022 21:55:58 - INFO - __main__ - Step 440 Global step 440 Train loss 1.32 on epoch=219
06/19/2022 21:56:00 - INFO - __main__ - Step 450 Global step 450 Train loss 1.50 on epoch=224
06/19/2022 21:56:00 - INFO - __main__ - Global step 450 Train loss 1.49 Classification-F1 0.39756367663344405 on epoch=224
06/19/2022 21:56:01 - INFO - __main__ - Step 460 Global step 460 Train loss 1.46 on epoch=229
06/19/2022 21:56:02 - INFO - __main__ - Step 470 Global step 470 Train loss 1.33 on epoch=234
06/19/2022 21:56:04 - INFO - __main__ - Step 480 Global step 480 Train loss 1.38 on epoch=239
06/19/2022 21:56:05 - INFO - __main__ - Step 490 Global step 490 Train loss 1.45 on epoch=244
06/19/2022 21:56:06 - INFO - __main__ - Step 500 Global step 500 Train loss 1.31 on epoch=249
06/19/2022 21:56:07 - INFO - __main__ - Global step 500 Train loss 1.39 Classification-F1 0.5636363636363637 on epoch=249
06/19/2022 21:56:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.5636363636363637 on epoch=249, global_step=500
06/19/2022 21:56:08 - INFO - __main__ - Step 510 Global step 510 Train loss 1.36 on epoch=254
06/19/2022 21:56:09 - INFO - __main__ - Step 520 Global step 520 Train loss 1.30 on epoch=259
06/19/2022 21:56:10 - INFO - __main__ - Step 530 Global step 530 Train loss 1.42 on epoch=264
06/19/2022 21:56:12 - INFO - __main__ - Step 540 Global step 540 Train loss 1.20 on epoch=269
06/19/2022 21:56:13 - INFO - __main__ - Step 550 Global step 550 Train loss 1.19 on epoch=274
06/19/2022 21:56:13 - INFO - __main__ - Global step 550 Train loss 1.30 Classification-F1 0.49090909090909085 on epoch=274
06/19/2022 21:56:15 - INFO - __main__ - Step 560 Global step 560 Train loss 1.24 on epoch=279
06/19/2022 21:56:16 - INFO - __main__ - Step 570 Global step 570 Train loss 1.34 on epoch=284
06/19/2022 21:56:17 - INFO - __main__ - Step 580 Global step 580 Train loss 1.30 on epoch=289
06/19/2022 21:56:18 - INFO - __main__ - Step 590 Global step 590 Train loss 1.24 on epoch=294
06/19/2022 21:56:19 - INFO - __main__ - Step 600 Global step 600 Train loss 1.15 on epoch=299
06/19/2022 21:56:20 - INFO - __main__ - Global step 600 Train loss 1.25 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 21:56:21 - INFO - __main__ - Step 610 Global step 610 Train loss 1.09 on epoch=304
06/19/2022 21:56:22 - INFO - __main__ - Step 620 Global step 620 Train loss 1.26 on epoch=309
06/19/2022 21:56:23 - INFO - __main__ - Step 630 Global step 630 Train loss 1.18 on epoch=314
06/19/2022 21:56:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.98 on epoch=319
06/19/2022 21:56:26 - INFO - __main__ - Step 650 Global step 650 Train loss 1.09 on epoch=324
06/19/2022 21:56:26 - INFO - __main__ - Global step 650 Train loss 1.12 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 21:56:27 - INFO - __main__ - Step 660 Global step 660 Train loss 1.02 on epoch=329
06/19/2022 21:56:29 - INFO - __main__ - Step 670 Global step 670 Train loss 1.02 on epoch=334
06/19/2022 21:56:30 - INFO - __main__ - Step 680 Global step 680 Train loss 1.02 on epoch=339
06/19/2022 21:56:31 - INFO - __main__ - Step 690 Global step 690 Train loss 1.05 on epoch=344
06/19/2022 21:56:32 - INFO - __main__ - Step 700 Global step 700 Train loss 1.05 on epoch=349
06/19/2022 21:56:33 - INFO - __main__ - Global step 700 Train loss 1.03 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 21:56:34 - INFO - __main__ - Step 710 Global step 710 Train loss 1.00 on epoch=354
06/19/2022 21:56:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.89 on epoch=359
06/19/2022 21:56:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.92 on epoch=364
06/19/2022 21:56:38 - INFO - __main__ - Step 740 Global step 740 Train loss 1.05 on epoch=369
06/19/2022 21:56:39 - INFO - __main__ - Step 750 Global step 750 Train loss 1.04 on epoch=374
06/19/2022 21:56:39 - INFO - __main__ - Global step 750 Train loss 0.98 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 21:56:41 - INFO - __main__ - Step 760 Global step 760 Train loss 1.06 on epoch=379
06/19/2022 21:56:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.94 on epoch=384
06/19/2022 21:56:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.99 on epoch=389
06/19/2022 21:56:44 - INFO - __main__ - Step 790 Global step 790 Train loss 1.06 on epoch=394
06/19/2022 21:56:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.92 on epoch=399
06/19/2022 21:56:46 - INFO - __main__ - Global step 800 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 21:56:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.83 on epoch=404
06/19/2022 21:56:48 - INFO - __main__ - Step 820 Global step 820 Train loss 1.01 on epoch=409
06/19/2022 21:56:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.92 on epoch=414
06/19/2022 21:56:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.96 on epoch=419
06/19/2022 21:56:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.99 on epoch=424
06/19/2022 21:56:52 - INFO - __main__ - Global step 850 Train loss 0.94 Classification-F1 0.3191489361702127 on epoch=424
06/19/2022 21:56:54 - INFO - __main__ - Step 860 Global step 860 Train loss 1.00 on epoch=429
06/19/2022 21:56:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.87 on epoch=434
06/19/2022 21:56:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.91 on epoch=439
06/19/2022 21:56:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.91 on epoch=444
06/19/2022 21:56:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.92 on epoch=449
06/19/2022 21:56:59 - INFO - __main__ - Global step 900 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 21:57:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.86 on epoch=454
06/19/2022 21:57:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.88 on epoch=459
06/19/2022 21:57:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.83 on epoch=464
06/19/2022 21:57:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.86 on epoch=469
06/19/2022 21:57:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.80 on epoch=474
06/19/2022 21:57:05 - INFO - __main__ - Global step 950 Train loss 0.85 Classification-F1 0.3992490613266583 on epoch=474
06/19/2022 21:57:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.71 on epoch=479
06/19/2022 21:57:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.88 on epoch=484
06/19/2022 21:57:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.86 on epoch=489
06/19/2022 21:57:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.89 on epoch=494
06/19/2022 21:57:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.81 on epoch=499
06/19/2022 21:57:12 - INFO - __main__ - Global step 1000 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 21:57:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.85 on epoch=504
06/19/2022 21:57:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.91 on epoch=509
06/19/2022 21:57:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.75 on epoch=514
06/19/2022 21:57:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.77 on epoch=519
06/19/2022 21:57:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.74 on epoch=524
06/19/2022 21:57:18 - INFO - __main__ - Global step 1050 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 21:57:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.92 on epoch=529
06/19/2022 21:57:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.76 on epoch=534
06/19/2022 21:57:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.84 on epoch=539
06/19/2022 21:57:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.90 on epoch=544
06/19/2022 21:57:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.83 on epoch=549
06/19/2022 21:57:25 - INFO - __main__ - Global step 1100 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 21:57:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.85 on epoch=554
06/19/2022 21:57:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.71 on epoch=559
06/19/2022 21:57:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.86 on epoch=564
06/19/2022 21:57:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.76 on epoch=569
06/19/2022 21:57:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.80 on epoch=574
06/19/2022 21:57:31 - INFO - __main__ - Global step 1150 Train loss 0.80 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 21:57:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.81 on epoch=579
06/19/2022 21:57:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.82 on epoch=584
06/19/2022 21:57:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.75 on epoch=589
06/19/2022 21:57:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.83 on epoch=594
06/19/2022 21:57:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.72 on epoch=599
06/19/2022 21:57:38 - INFO - __main__ - Global step 1200 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 21:57:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.78 on epoch=604
06/19/2022 21:57:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.76 on epoch=609
06/19/2022 21:57:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.74 on epoch=614
06/19/2022 21:57:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.75 on epoch=619
06/19/2022 21:57:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.74 on epoch=624
06/19/2022 21:57:44 - INFO - __main__ - Global step 1250 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 21:57:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.72 on epoch=629
06/19/2022 21:57:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.71 on epoch=634
06/19/2022 21:57:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.72 on epoch=639
06/19/2022 21:57:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.74 on epoch=644
06/19/2022 21:57:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.81 on epoch=649
06/19/2022 21:57:51 - INFO - __main__ - Global step 1300 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 21:57:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=654
06/19/2022 21:57:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.62 on epoch=659
06/19/2022 21:57:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.63 on epoch=664
06/19/2022 21:57:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.68 on epoch=669
06/19/2022 21:57:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.69 on epoch=674
06/19/2022 21:57:58 - INFO - __main__ - Global step 1350 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 21:57:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.74 on epoch=679
06/19/2022 21:58:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.68 on epoch=684
06/19/2022 21:58:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.69 on epoch=689
06/19/2022 21:58:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.80 on epoch=694
06/19/2022 21:58:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.72 on epoch=699
06/19/2022 21:58:04 - INFO - __main__ - Global step 1400 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 21:58:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.66 on epoch=704
06/19/2022 21:58:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.76 on epoch=709
06/19/2022 21:58:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.64 on epoch=714
06/19/2022 21:58:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.64 on epoch=719
06/19/2022 21:58:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.68 on epoch=724
06/19/2022 21:58:11 - INFO - __main__ - Global step 1450 Train loss 0.68 Classification-F1 0.3816425120772947 on epoch=724
06/19/2022 21:58:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.75 on epoch=729
06/19/2022 21:58:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.61 on epoch=734
06/19/2022 21:58:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.67 on epoch=739
06/19/2022 21:58:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.62 on epoch=744
06/19/2022 21:58:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.67 on epoch=749
06/19/2022 21:58:17 - INFO - __main__ - Global step 1500 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 21:58:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.75 on epoch=754
06/19/2022 21:58:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.61 on epoch=759
06/19/2022 21:58:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.70 on epoch=764
06/19/2022 21:58:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.66 on epoch=769
06/19/2022 21:58:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.67 on epoch=774
06/19/2022 21:58:24 - INFO - __main__ - Global step 1550 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 21:58:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.59 on epoch=779
06/19/2022 21:58:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.66 on epoch=784
06/19/2022 21:58:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.59 on epoch=789
06/19/2022 21:58:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.71 on epoch=794
06/19/2022 21:58:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.68 on epoch=799
06/19/2022 21:58:30 - INFO - __main__ - Global step 1600 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 21:58:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.63 on epoch=804
06/19/2022 21:58:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.60 on epoch=809
06/19/2022 21:58:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.57 on epoch=814
06/19/2022 21:58:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.63 on epoch=819
06/19/2022 21:58:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.63 on epoch=824
06/19/2022 21:58:37 - INFO - __main__ - Global step 1650 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 21:58:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.59 on epoch=829
06/19/2022 21:58:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.63 on epoch=834
06/19/2022 21:58:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.51 on epoch=839
06/19/2022 21:58:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.66 on epoch=844
06/19/2022 21:58:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.64 on epoch=849
06/19/2022 21:58:43 - INFO - __main__ - Global step 1700 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 21:58:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.68 on epoch=854
06/19/2022 21:58:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.56 on epoch=859
06/19/2022 21:58:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.68 on epoch=864
06/19/2022 21:58:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.61 on epoch=869
06/19/2022 21:58:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.61 on epoch=874
06/19/2022 21:58:50 - INFO - __main__ - Global step 1750 Train loss 0.63 Classification-F1 0.3191489361702127 on epoch=874
06/19/2022 21:58:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.63 on epoch=879
06/19/2022 21:58:52 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.62 on epoch=884
06/19/2022 21:58:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.61 on epoch=889
06/19/2022 21:58:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.64 on epoch=894
06/19/2022 21:58:56 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.62 on epoch=899
06/19/2022 21:58:56 - INFO - __main__ - Global step 1800 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 21:58:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.56 on epoch=904
06/19/2022 21:58:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.62 on epoch=909
06/19/2022 21:59:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.57 on epoch=914
06/19/2022 21:59:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.57 on epoch=919
06/19/2022 21:59:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.60 on epoch=924
06/19/2022 21:59:02 - INFO - __main__ - Global step 1850 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 21:59:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.56 on epoch=929
06/19/2022 21:59:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.64 on epoch=934
06/19/2022 21:59:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.64 on epoch=939
06/19/2022 21:59:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.57 on epoch=944
06/19/2022 21:59:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.66 on epoch=949
06/19/2022 21:59:09 - INFO - __main__ - Global step 1900 Train loss 0.62 Classification-F1 0.3816425120772947 on epoch=949
06/19/2022 21:59:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.58 on epoch=954
06/19/2022 21:59:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.62 on epoch=959
06/19/2022 21:59:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.59 on epoch=964
06/19/2022 21:59:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.66 on epoch=969
06/19/2022 21:59:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.60 on epoch=974
06/19/2022 21:59:16 - INFO - __main__ - Global step 1950 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 21:59:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.62 on epoch=979
06/19/2022 21:59:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.56 on epoch=984
06/19/2022 21:59:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.58 on epoch=989
06/19/2022 21:59:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.56 on epoch=994
06/19/2022 21:59:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.63 on epoch=999
06/19/2022 21:59:22 - INFO - __main__ - Global step 2000 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 21:59:22 - INFO - __main__ - save last model!
06/19/2022 21:59:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 21:59:22 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 21:59:22 - INFO - __main__ - Printing 3 examples
06/19/2022 21:59:22 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 21:59:22 - INFO - __main__ - ['0']
06/19/2022 21:59:22 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 21:59:22 - INFO - __main__ - ['1']
06/19/2022 21:59:22 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 21:59:22 - INFO - __main__ - ['1']
06/19/2022 21:59:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:59:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:59:23 - INFO - __main__ - Printing 3 examples
06/19/2022 21:59:23 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 21:59:23 - INFO - __main__ - ['1']
06/19/2022 21:59:23 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 21:59:23 - INFO - __main__ - ['1']
06/19/2022 21:59:23 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 21:59:23 - INFO - __main__ - ['1']
06/19/2022 21:59:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:59:23 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:59:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:59:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:59:23 - INFO - __main__ - Printing 3 examples
06/19/2022 21:59:23 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 21:59:23 - INFO - __main__ - ['1']
06/19/2022 21:59:23 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 21:59:23 - INFO - __main__ - ['1']
06/19/2022 21:59:23 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 21:59:23 - INFO - __main__ - ['1']
06/19/2022 21:59:23 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:59:23 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:59:23 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:59:26 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:59:29 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:59:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:59:29 - INFO - __main__ - Starting training!
06/19/2022 21:59:34 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 22:00:58 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_13_0.3_8_predictions.txt
06/19/2022 22:00:58 - INFO - __main__ - Classification-F1 on test data: 0.3094
06/19/2022 22:00:58 - INFO - __main__ - prefix=paws_16_13, lr=0.3, bsz=8, dev_performance=0.5636363636363637, test_performance=0.30939735150628206
06/19/2022 22:00:58 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.2, bsz=8 ...
06/19/2022 22:00:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:00:59 - INFO - __main__ - Printing 3 examples
06/19/2022 22:00:59 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/19/2022 22:00:59 - INFO - __main__ - ['1']
06/19/2022 22:00:59 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/19/2022 22:00:59 - INFO - __main__ - ['1']
06/19/2022 22:00:59 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/19/2022 22:00:59 - INFO - __main__ - ['1']
06/19/2022 22:00:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:00:59 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:00:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:00:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:00:59 - INFO - __main__ - Printing 3 examples
06/19/2022 22:00:59 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/19/2022 22:00:59 - INFO - __main__ - ['1']
06/19/2022 22:00:59 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/19/2022 22:00:59 - INFO - __main__ - ['1']
06/19/2022 22:00:59 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/19/2022 22:00:59 - INFO - __main__ - ['1']
06/19/2022 22:00:59 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:00:59 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:00:59 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:01:05 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:01:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:01:05 - INFO - __main__ - Starting training!
06/19/2022 22:01:06 - INFO - __main__ - Step 10 Global step 10 Train loss 5.92 on epoch=4
06/19/2022 22:01:08 - INFO - __main__ - Step 20 Global step 20 Train loss 5.83 on epoch=9
06/19/2022 22:01:09 - INFO - __main__ - Step 30 Global step 30 Train loss 5.47 on epoch=14
06/19/2022 22:01:10 - INFO - __main__ - Step 40 Global step 40 Train loss 5.08 on epoch=19
06/19/2022 22:01:11 - INFO - __main__ - Step 50 Global step 50 Train loss 4.87 on epoch=24
06/19/2022 22:01:18 - INFO - __main__ - Global step 50 Train loss 5.43 Classification-F1 0.0 on epoch=24
06/19/2022 22:01:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 22:01:20 - INFO - __main__ - Step 60 Global step 60 Train loss 4.44 on epoch=29
06/19/2022 22:01:21 - INFO - __main__ - Step 70 Global step 70 Train loss 4.37 on epoch=34
06/19/2022 22:01:22 - INFO - __main__ - Step 80 Global step 80 Train loss 4.23 on epoch=39
06/19/2022 22:01:23 - INFO - __main__ - Step 90 Global step 90 Train loss 4.21 on epoch=44
06/19/2022 22:01:24 - INFO - __main__ - Step 100 Global step 100 Train loss 3.97 on epoch=49
06/19/2022 22:01:26 - INFO - __main__ - Global step 100 Train loss 4.24 Classification-F1 0.0 on epoch=49
06/19/2022 22:01:28 - INFO - __main__ - Step 110 Global step 110 Train loss 4.11 on epoch=54
06/19/2022 22:01:29 - INFO - __main__ - Step 120 Global step 120 Train loss 3.93 on epoch=59
06/19/2022 22:01:30 - INFO - __main__ - Step 130 Global step 130 Train loss 3.82 on epoch=64
06/19/2022 22:01:31 - INFO - __main__ - Step 140 Global step 140 Train loss 3.83 on epoch=69
06/19/2022 22:01:32 - INFO - __main__ - Step 150 Global step 150 Train loss 3.76 on epoch=74
06/19/2022 22:01:35 - INFO - __main__ - Global step 150 Train loss 3.89 Classification-F1 0.0 on epoch=74
06/19/2022 22:01:37 - INFO - __main__ - Step 160 Global step 160 Train loss 3.60 on epoch=79
06/19/2022 22:01:38 - INFO - __main__ - Step 170 Global step 170 Train loss 3.53 on epoch=84
06/19/2022 22:01:39 - INFO - __main__ - Step 180 Global step 180 Train loss 3.50 on epoch=89
06/19/2022 22:01:40 - INFO - __main__ - Step 190 Global step 190 Train loss 3.46 on epoch=94
06/19/2022 22:01:42 - INFO - __main__ - Step 200 Global step 200 Train loss 3.45 on epoch=99
06/19/2022 22:01:45 - INFO - __main__ - Global step 200 Train loss 3.51 Classification-F1 0.0 on epoch=99
06/19/2022 22:01:47 - INFO - __main__ - Step 210 Global step 210 Train loss 3.29 on epoch=104
06/19/2022 22:01:48 - INFO - __main__ - Step 220 Global step 220 Train loss 3.45 on epoch=109
06/19/2022 22:01:49 - INFO - __main__ - Step 230 Global step 230 Train loss 3.27 on epoch=114
06/19/2022 22:01:50 - INFO - __main__ - Step 240 Global step 240 Train loss 3.25 on epoch=119
06/19/2022 22:01:52 - INFO - __main__ - Step 250 Global step 250 Train loss 3.34 on epoch=124
06/19/2022 22:01:58 - INFO - __main__ - Global step 250 Train loss 3.32 Classification-F1 0.023529411764705882 on epoch=124
06/19/2022 22:01:58 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.023529411764705882 on epoch=124, global_step=250
06/19/2022 22:02:00 - INFO - __main__ - Step 260 Global step 260 Train loss 3.19 on epoch=129
06/19/2022 22:02:01 - INFO - __main__ - Step 270 Global step 270 Train loss 3.07 on epoch=134
06/19/2022 22:02:02 - INFO - __main__ - Step 280 Global step 280 Train loss 3.17 on epoch=139
06/19/2022 22:02:03 - INFO - __main__ - Step 290 Global step 290 Train loss 2.96 on epoch=144
06/19/2022 22:02:05 - INFO - __main__ - Step 300 Global step 300 Train loss 3.09 on epoch=149
06/19/2022 22:02:08 - INFO - __main__ - Global step 300 Train loss 3.10 Classification-F1 0.059620596205962065 on epoch=149
06/19/2022 22:02:08 - INFO - __main__ - Saving model with best Classification-F1: 0.023529411764705882 -> 0.059620596205962065 on epoch=149, global_step=300
06/19/2022 22:02:09 - INFO - __main__ - Step 310 Global step 310 Train loss 3.00 on epoch=154
06/19/2022 22:02:10 - INFO - __main__ - Step 320 Global step 320 Train loss 2.83 on epoch=159
06/19/2022 22:02:12 - INFO - __main__ - Step 330 Global step 330 Train loss 2.94 on epoch=164
06/19/2022 22:02:13 - INFO - __main__ - Step 340 Global step 340 Train loss 2.99 on epoch=169
06/19/2022 22:02:14 - INFO - __main__ - Step 350 Global step 350 Train loss 2.88 on epoch=174
06/19/2022 22:02:16 - INFO - __main__ - Global step 350 Train loss 2.93 Classification-F1 0.09523809523809523 on epoch=174
06/19/2022 22:02:16 - INFO - __main__ - Saving model with best Classification-F1: 0.059620596205962065 -> 0.09523809523809523 on epoch=174, global_step=350
06/19/2022 22:02:18 - INFO - __main__ - Step 360 Global step 360 Train loss 2.90 on epoch=179
06/19/2022 22:02:19 - INFO - __main__ - Step 370 Global step 370 Train loss 2.96 on epoch=184
06/19/2022 22:02:20 - INFO - __main__ - Step 380 Global step 380 Train loss 2.90 on epoch=189
06/19/2022 22:02:21 - INFO - __main__ - Step 390 Global step 390 Train loss 2.78 on epoch=194
06/19/2022 22:02:23 - INFO - __main__ - Step 400 Global step 400 Train loss 2.78 on epoch=199
06/19/2022 22:02:25 - INFO - __main__ - Global step 400 Train loss 2.87 Classification-F1 0.11627906976744186 on epoch=199
06/19/2022 22:02:25 - INFO - __main__ - Saving model with best Classification-F1: 0.09523809523809523 -> 0.11627906976744186 on epoch=199, global_step=400
06/19/2022 22:02:27 - INFO - __main__ - Step 410 Global step 410 Train loss 2.67 on epoch=204
06/19/2022 22:02:28 - INFO - __main__ - Step 420 Global step 420 Train loss 2.65 on epoch=209
06/19/2022 22:02:29 - INFO - __main__ - Step 430 Global step 430 Train loss 2.57 on epoch=214
06/19/2022 22:02:30 - INFO - __main__ - Step 440 Global step 440 Train loss 2.50 on epoch=219
06/19/2022 22:02:32 - INFO - __main__ - Step 450 Global step 450 Train loss 2.52 on epoch=224
06/19/2022 22:02:34 - INFO - __main__ - Global step 450 Train loss 2.58 Classification-F1 0.10569105691056911 on epoch=224
06/19/2022 22:02:35 - INFO - __main__ - Step 460 Global step 460 Train loss 2.54 on epoch=229
06/19/2022 22:02:37 - INFO - __main__ - Step 470 Global step 470 Train loss 2.54 on epoch=234
06/19/2022 22:02:38 - INFO - __main__ - Step 480 Global step 480 Train loss 2.41 on epoch=239
06/19/2022 22:02:39 - INFO - __main__ - Step 490 Global step 490 Train loss 2.44 on epoch=244
06/19/2022 22:02:40 - INFO - __main__ - Step 500 Global step 500 Train loss 2.38 on epoch=249
06/19/2022 22:02:43 - INFO - __main__ - Global step 500 Train loss 2.46 Classification-F1 0.22695035460992907 on epoch=249
06/19/2022 22:02:43 - INFO - __main__ - Saving model with best Classification-F1: 0.11627906976744186 -> 0.22695035460992907 on epoch=249, global_step=500
06/19/2022 22:02:44 - INFO - __main__ - Step 510 Global step 510 Train loss 2.46 on epoch=254
06/19/2022 22:02:45 - INFO - __main__ - Step 520 Global step 520 Train loss 2.53 on epoch=259
06/19/2022 22:02:46 - INFO - __main__ - Step 530 Global step 530 Train loss 2.33 on epoch=264
06/19/2022 22:02:48 - INFO - __main__ - Step 540 Global step 540 Train loss 2.29 on epoch=269
06/19/2022 22:02:49 - INFO - __main__ - Step 550 Global step 550 Train loss 2.28 on epoch=274
06/19/2022 22:02:50 - INFO - __main__ - Global step 550 Train loss 2.38 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 22:02:50 - INFO - __main__ - Saving model with best Classification-F1: 0.22695035460992907 -> 0.3333333333333333 on epoch=274, global_step=550
06/19/2022 22:02:51 - INFO - __main__ - Step 560 Global step 560 Train loss 2.21 on epoch=279
06/19/2022 22:02:52 - INFO - __main__ - Step 570 Global step 570 Train loss 2.17 on epoch=284
06/19/2022 22:02:54 - INFO - __main__ - Step 580 Global step 580 Train loss 2.27 on epoch=289
06/19/2022 22:02:55 - INFO - __main__ - Step 590 Global step 590 Train loss 2.26 on epoch=294
06/19/2022 22:02:56 - INFO - __main__ - Step 600 Global step 600 Train loss 2.29 on epoch=299
06/19/2022 22:02:57 - INFO - __main__ - Global step 600 Train loss 2.24 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 22:02:58 - INFO - __main__ - Step 610 Global step 610 Train loss 2.16 on epoch=304
06/19/2022 22:03:00 - INFO - __main__ - Step 620 Global step 620 Train loss 2.21 on epoch=309
06/19/2022 22:03:01 - INFO - __main__ - Step 630 Global step 630 Train loss 1.99 on epoch=314
06/19/2022 22:03:02 - INFO - __main__ - Step 640 Global step 640 Train loss 2.16 on epoch=319
06/19/2022 22:03:03 - INFO - __main__ - Step 650 Global step 650 Train loss 2.04 on epoch=324
06/19/2022 22:03:06 - INFO - __main__ - Global step 650 Train loss 2.11 Classification-F1 0.27109974424552435 on epoch=324
06/19/2022 22:03:07 - INFO - __main__ - Step 660 Global step 660 Train loss 2.02 on epoch=329
06/19/2022 22:03:08 - INFO - __main__ - Step 670 Global step 670 Train loss 2.03 on epoch=334
06/19/2022 22:03:10 - INFO - __main__ - Step 680 Global step 680 Train loss 1.98 on epoch=339
06/19/2022 22:03:11 - INFO - __main__ - Step 690 Global step 690 Train loss 1.77 on epoch=344
06/19/2022 22:03:12 - INFO - __main__ - Step 700 Global step 700 Train loss 1.97 on epoch=349
06/19/2022 22:03:13 - INFO - __main__ - Global step 700 Train loss 1.95 Classification-F1 0.4589371980676329 on epoch=349
06/19/2022 22:03:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4589371980676329 on epoch=349, global_step=700
06/19/2022 22:03:14 - INFO - __main__ - Step 710 Global step 710 Train loss 1.98 on epoch=354
06/19/2022 22:03:15 - INFO - __main__ - Step 720 Global step 720 Train loss 1.93 on epoch=359
06/19/2022 22:03:17 - INFO - __main__ - Step 730 Global step 730 Train loss 1.70 on epoch=364
06/19/2022 22:03:18 - INFO - __main__ - Step 740 Global step 740 Train loss 1.78 on epoch=369
06/19/2022 22:03:19 - INFO - __main__ - Step 750 Global step 750 Train loss 1.88 on epoch=374
06/19/2022 22:03:20 - INFO - __main__ - Global step 750 Train loss 1.85 Classification-F1 0.5076923076923077 on epoch=374
06/19/2022 22:03:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5076923076923077 on epoch=374, global_step=750
06/19/2022 22:03:21 - INFO - __main__ - Step 760 Global step 760 Train loss 1.78 on epoch=379
06/19/2022 22:03:23 - INFO - __main__ - Step 770 Global step 770 Train loss 1.87 on epoch=384
06/19/2022 22:03:24 - INFO - __main__ - Step 780 Global step 780 Train loss 1.88 on epoch=389
06/19/2022 22:03:25 - INFO - __main__ - Step 790 Global step 790 Train loss 1.86 on epoch=394
06/19/2022 22:03:26 - INFO - __main__ - Step 800 Global step 800 Train loss 1.75 on epoch=399
06/19/2022 22:03:27 - INFO - __main__ - Global step 800 Train loss 1.83 Classification-F1 0.46843853820598 on epoch=399
06/19/2022 22:03:28 - INFO - __main__ - Step 810 Global step 810 Train loss 1.66 on epoch=404
06/19/2022 22:03:30 - INFO - __main__ - Step 820 Global step 820 Train loss 1.87 on epoch=409
06/19/2022 22:03:31 - INFO - __main__ - Step 830 Global step 830 Train loss 1.61 on epoch=414
06/19/2022 22:03:32 - INFO - __main__ - Step 840 Global step 840 Train loss 1.64 on epoch=419
06/19/2022 22:03:33 - INFO - __main__ - Step 850 Global step 850 Train loss 1.77 on epoch=424
06/19/2022 22:03:34 - INFO - __main__ - Global step 850 Train loss 1.71 Classification-F1 0.3992490613266583 on epoch=424
06/19/2022 22:03:35 - INFO - __main__ - Step 860 Global step 860 Train loss 1.62 on epoch=429
06/19/2022 22:03:36 - INFO - __main__ - Step 870 Global step 870 Train loss 1.68 on epoch=434
06/19/2022 22:03:38 - INFO - __main__ - Step 880 Global step 880 Train loss 1.66 on epoch=439
06/19/2022 22:03:39 - INFO - __main__ - Step 890 Global step 890 Train loss 1.57 on epoch=444
06/19/2022 22:03:40 - INFO - __main__ - Step 900 Global step 900 Train loss 1.64 on epoch=449
06/19/2022 22:03:40 - INFO - __main__ - Global step 900 Train loss 1.63 Classification-F1 0.5195195195195195 on epoch=449
06/19/2022 22:03:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5076923076923077 -> 0.5195195195195195 on epoch=449, global_step=900
06/19/2022 22:03:42 - INFO - __main__ - Step 910 Global step 910 Train loss 1.68 on epoch=454
06/19/2022 22:03:43 - INFO - __main__ - Step 920 Global step 920 Train loss 1.64 on epoch=459
06/19/2022 22:03:44 - INFO - __main__ - Step 930 Global step 930 Train loss 1.60 on epoch=464
06/19/2022 22:03:45 - INFO - __main__ - Step 940 Global step 940 Train loss 1.52 on epoch=469
06/19/2022 22:03:47 - INFO - __main__ - Step 950 Global step 950 Train loss 1.44 on epoch=474
06/19/2022 22:03:47 - INFO - __main__ - Global step 950 Train loss 1.58 Classification-F1 0.4009852216748768 on epoch=474
06/19/2022 22:03:48 - INFO - __main__ - Step 960 Global step 960 Train loss 1.50 on epoch=479
06/19/2022 22:03:50 - INFO - __main__ - Step 970 Global step 970 Train loss 1.47 on epoch=484
06/19/2022 22:03:51 - INFO - __main__ - Step 980 Global step 980 Train loss 1.36 on epoch=489
06/19/2022 22:03:52 - INFO - __main__ - Step 990 Global step 990 Train loss 1.44 on epoch=494
06/19/2022 22:03:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.51 on epoch=499
06/19/2022 22:03:54 - INFO - __main__ - Global step 1000 Train loss 1.45 Classification-F1 0.4980392156862745 on epoch=499
06/19/2022 22:03:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.52 on epoch=504
06/19/2022 22:03:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.48 on epoch=509
06/19/2022 22:03:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.44 on epoch=514
06/19/2022 22:03:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.40 on epoch=519
06/19/2022 22:04:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.45 on epoch=524
06/19/2022 22:04:00 - INFO - __main__ - Global step 1050 Train loss 1.46 Classification-F1 0.39139139139139134 on epoch=524
06/19/2022 22:04:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.32 on epoch=529
06/19/2022 22:04:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.44 on epoch=534
06/19/2022 22:04:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.49 on epoch=539
06/19/2022 22:04:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.27 on epoch=544
06/19/2022 22:04:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.43 on epoch=549
06/19/2022 22:04:07 - INFO - __main__ - Global step 1100 Train loss 1.39 Classification-F1 0.40566959921798634 on epoch=549
06/19/2022 22:04:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.40 on epoch=554
06/19/2022 22:04:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.44 on epoch=559
06/19/2022 22:04:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.40 on epoch=564
06/19/2022 22:04:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.36 on epoch=569
06/19/2022 22:04:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.38 on epoch=574
06/19/2022 22:04:13 - INFO - __main__ - Global step 1150 Train loss 1.39 Classification-F1 0.4181818181818182 on epoch=574
06/19/2022 22:04:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.29 on epoch=579
06/19/2022 22:04:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.14 on epoch=584
06/19/2022 22:04:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.30 on epoch=589
06/19/2022 22:04:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.19 on epoch=594
06/19/2022 22:04:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.18 on epoch=599
06/19/2022 22:04:20 - INFO - __main__ - Global step 1200 Train loss 1.22 Classification-F1 0.3073593073593074 on epoch=599
06/19/2022 22:04:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.33 on epoch=604
06/19/2022 22:04:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.25 on epoch=609
06/19/2022 22:04:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.26 on epoch=614
06/19/2022 22:04:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.25 on epoch=619
06/19/2022 22:04:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.12 on epoch=624
06/19/2022 22:04:26 - INFO - __main__ - Global step 1250 Train loss 1.24 Classification-F1 0.3764102564102564 on epoch=624
06/19/2022 22:04:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.22 on epoch=629
06/19/2022 22:04:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.22 on epoch=634
06/19/2022 22:04:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.20 on epoch=639
06/19/2022 22:04:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.31 on epoch=644
06/19/2022 22:04:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.26 on epoch=649
06/19/2022 22:04:33 - INFO - __main__ - Global step 1300 Train loss 1.24 Classification-F1 0.4420512820512821 on epoch=649
06/19/2022 22:04:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.20 on epoch=654
06/19/2022 22:04:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.17 on epoch=659
06/19/2022 22:04:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.11 on epoch=664
06/19/2022 22:04:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.19 on epoch=669
06/19/2022 22:04:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.03 on epoch=674
06/19/2022 22:04:39 - INFO - __main__ - Global step 1350 Train loss 1.14 Classification-F1 0.3816425120772947 on epoch=674
06/19/2022 22:04:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.02 on epoch=679
06/19/2022 22:04:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.11 on epoch=684
06/19/2022 22:04:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.12 on epoch=689
06/19/2022 22:04:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.03 on epoch=694
06/19/2022 22:04:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.12 on epoch=699
06/19/2022 22:04:46 - INFO - __main__ - Global step 1400 Train loss 1.08 Classification-F1 0.3043478260869565 on epoch=699
06/19/2022 22:04:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.10 on epoch=704
06/19/2022 22:04:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.92 on epoch=709
06/19/2022 22:04:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.10 on epoch=714
06/19/2022 22:04:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.11 on epoch=719
06/19/2022 22:04:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.07 on epoch=724
06/19/2022 22:04:52 - INFO - __main__ - Global step 1450 Train loss 1.06 Classification-F1 0.3191489361702127 on epoch=724
06/19/2022 22:04:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.12 on epoch=729
06/19/2022 22:04:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.01 on epoch=734
06/19/2022 22:04:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.01 on epoch=739
06/19/2022 22:04:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.92 on epoch=744
06/19/2022 22:04:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.03 on epoch=749
06/19/2022 22:04:59 - INFO - __main__ - Global step 1500 Train loss 1.02 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 22:05:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.97 on epoch=754
06/19/2022 22:05:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.10 on epoch=759
06/19/2022 22:05:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.00 on epoch=764
06/19/2022 22:05:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.07 on epoch=769
06/19/2022 22:05:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.07 on epoch=774
06/19/2022 22:05:05 - INFO - __main__ - Global step 1550 Train loss 1.04 Classification-F1 0.3454545454545454 on epoch=774
06/19/2022 22:05:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.00 on epoch=779
06/19/2022 22:05:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.96 on epoch=784
06/19/2022 22:05:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.01 on epoch=789
06/19/2022 22:05:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.99 on epoch=794
06/19/2022 22:05:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.96 on epoch=799
06/19/2022 22:05:12 - INFO - __main__ - Global step 1600 Train loss 0.98 Classification-F1 0.3191489361702127 on epoch=799
06/19/2022 22:05:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.98 on epoch=804
06/19/2022 22:05:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.99 on epoch=809
06/19/2022 22:05:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.94 on epoch=814
06/19/2022 22:05:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.85 on epoch=819
06/19/2022 22:05:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.96 on epoch=824
06/19/2022 22:05:18 - INFO - __main__ - Global step 1650 Train loss 0.94 Classification-F1 0.39756367663344405 on epoch=824
06/19/2022 22:05:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.00 on epoch=829
06/19/2022 22:05:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.99 on epoch=834
06/19/2022 22:05:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.00 on epoch=839
06/19/2022 22:05:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.92 on epoch=844
06/19/2022 22:05:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.94 on epoch=849
06/19/2022 22:05:25 - INFO - __main__ - Global step 1700 Train loss 0.97 Classification-F1 0.4385964912280702 on epoch=849
06/19/2022 22:05:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.87 on epoch=854
06/19/2022 22:05:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.90 on epoch=859
06/19/2022 22:05:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.92 on epoch=864
06/19/2022 22:05:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.87 on epoch=869
06/19/2022 22:05:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.87 on epoch=874
06/19/2022 22:05:32 - INFO - __main__ - Global step 1750 Train loss 0.89 Classification-F1 0.3191489361702127 on epoch=874
06/19/2022 22:05:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.86 on epoch=879
06/19/2022 22:05:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.95 on epoch=884
06/19/2022 22:05:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.90 on epoch=889
06/19/2022 22:05:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.93 on epoch=894
06/19/2022 22:05:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.84 on epoch=899
06/19/2022 22:05:38 - INFO - __main__ - Global step 1800 Train loss 0.90 Classification-F1 0.36374269005847953 on epoch=899
06/19/2022 22:05:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.94 on epoch=904
06/19/2022 22:05:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.95 on epoch=909
06/19/2022 22:05:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.94 on epoch=914
06/19/2022 22:05:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.90 on epoch=919
06/19/2022 22:05:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.98 on epoch=924
06/19/2022 22:05:45 - INFO - __main__ - Global step 1850 Train loss 0.94 Classification-F1 0.36374269005847953 on epoch=924
06/19/2022 22:05:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.89 on epoch=929
06/19/2022 22:05:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.80 on epoch=934
06/19/2022 22:05:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.82 on epoch=939
06/19/2022 22:05:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.86 on epoch=944
06/19/2022 22:05:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.90 on epoch=949
06/19/2022 22:05:51 - INFO - __main__ - Global step 1900 Train loss 0.85 Classification-F1 0.3992490613266583 on epoch=949
06/19/2022 22:05:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.88 on epoch=954
06/19/2022 22:05:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.87 on epoch=959
06/19/2022 22:05:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.80 on epoch=964
06/19/2022 22:05:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.98 on epoch=969
06/19/2022 22:05:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.86 on epoch=974
06/19/2022 22:05:58 - INFO - __main__ - Global step 1950 Train loss 0.88 Classification-F1 0.4231177094379639 on epoch=974
06/19/2022 22:05:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.89 on epoch=979
06/19/2022 22:06:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.79 on epoch=984
06/19/2022 22:06:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.78 on epoch=989
06/19/2022 22:06:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.83 on epoch=994
06/19/2022 22:06:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.81 on epoch=999
06/19/2022 22:06:04 - INFO - __main__ - Global step 2000 Train loss 0.82 Classification-F1 0.39756367663344405 on epoch=999
06/19/2022 22:06:04 - INFO - __main__ - save last model!
06/19/2022 22:06:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 22:06:04 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 22:06:04 - INFO - __main__ - Printing 3 examples
06/19/2022 22:06:04 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 22:06:04 - INFO - __main__ - ['0']
06/19/2022 22:06:04 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 22:06:04 - INFO - __main__ - ['1']
06/19/2022 22:06:04 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 22:06:04 - INFO - __main__ - ['1']
06/19/2022 22:06:04 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:06:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:06:05 - INFO - __main__ - Printing 3 examples
06/19/2022 22:06:05 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 22:06:05 - INFO - __main__ - ['1']
06/19/2022 22:06:05 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 22:06:05 - INFO - __main__ - ['1']
06/19/2022 22:06:05 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 22:06:05 - INFO - __main__ - ['1']
06/19/2022 22:06:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:06:05 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:06:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:06:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:06:05 - INFO - __main__ - Printing 3 examples
06/19/2022 22:06:05 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 22:06:05 - INFO - __main__ - ['1']
06/19/2022 22:06:05 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 22:06:05 - INFO - __main__ - ['1']
06/19/2022 22:06:05 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 22:06:05 - INFO - __main__ - ['1']
06/19/2022 22:06:05 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:06:05 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:06:05 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:06:09 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:06:11 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:06:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:06:11 - INFO - __main__ - Starting training!
06/19/2022 22:06:16 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 22:07:43 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_13_0.2_8_predictions.txt
06/19/2022 22:07:43 - INFO - __main__ - Classification-F1 on test data: 0.3970
06/19/2022 22:07:43 - INFO - __main__ - prefix=paws_16_13, lr=0.2, bsz=8, dev_performance=0.5195195195195195, test_performance=0.39702498797852503
06/19/2022 22:07:43 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.5, bsz=8 ...
06/19/2022 22:07:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:07:44 - INFO - __main__ - Printing 3 examples
06/19/2022 22:07:44 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 22:07:44 - INFO - __main__ - ['1']
06/19/2022 22:07:44 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 22:07:44 - INFO - __main__ - ['1']
06/19/2022 22:07:44 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 22:07:44 - INFO - __main__ - ['1']
06/19/2022 22:07:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:07:44 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:07:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:07:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:07:44 - INFO - __main__ - Printing 3 examples
06/19/2022 22:07:44 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 22:07:44 - INFO - __main__ - ['1']
06/19/2022 22:07:44 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 22:07:44 - INFO - __main__ - ['1']
06/19/2022 22:07:44 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 22:07:44 - INFO - __main__ - ['1']
06/19/2022 22:07:44 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:07:44 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:07:44 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:07:49 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:07:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:07:50 - INFO - __main__ - Starting training!
06/19/2022 22:07:51 - INFO - __main__ - Step 10 Global step 10 Train loss 5.89 on epoch=4
06/19/2022 22:07:52 - INFO - __main__ - Step 20 Global step 20 Train loss 5.54 on epoch=9
06/19/2022 22:07:54 - INFO - __main__ - Step 30 Global step 30 Train loss 4.80 on epoch=14
06/19/2022 22:07:55 - INFO - __main__ - Step 40 Global step 40 Train loss 4.38 on epoch=19
06/19/2022 22:07:56 - INFO - __main__ - Step 50 Global step 50 Train loss 4.01 on epoch=24
06/19/2022 22:08:02 - INFO - __main__ - Global step 50 Train loss 4.92 Classification-F1 0.0 on epoch=24
06/19/2022 22:08:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 22:08:03 - INFO - __main__ - Step 60 Global step 60 Train loss 4.01 on epoch=29
06/19/2022 22:08:04 - INFO - __main__ - Step 70 Global step 70 Train loss 3.86 on epoch=34
06/19/2022 22:08:05 - INFO - __main__ - Step 80 Global step 80 Train loss 3.74 on epoch=39
06/19/2022 22:08:07 - INFO - __main__ - Step 90 Global step 90 Train loss 3.66 on epoch=44
06/19/2022 22:08:08 - INFO - __main__ - Step 100 Global step 100 Train loss 3.42 on epoch=49
06/19/2022 22:08:14 - INFO - __main__ - Global step 100 Train loss 3.74 Classification-F1 0.0 on epoch=49
06/19/2022 22:08:15 - INFO - __main__ - Step 110 Global step 110 Train loss 3.39 on epoch=54
06/19/2022 22:08:16 - INFO - __main__ - Step 120 Global step 120 Train loss 3.22 on epoch=59
06/19/2022 22:08:17 - INFO - __main__ - Step 130 Global step 130 Train loss 3.29 on epoch=64
06/19/2022 22:08:19 - INFO - __main__ - Step 140 Global step 140 Train loss 3.16 on epoch=69
06/19/2022 22:08:20 - INFO - __main__ - Step 150 Global step 150 Train loss 3.02 on epoch=74
06/19/2022 22:08:21 - INFO - __main__ - Global step 150 Train loss 3.21 Classification-F1 0.11363636363636363 on epoch=74
06/19/2022 22:08:22 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.11363636363636363 on epoch=74, global_step=150
06/19/2022 22:08:23 - INFO - __main__ - Step 160 Global step 160 Train loss 2.88 on epoch=79
06/19/2022 22:08:24 - INFO - __main__ - Step 170 Global step 170 Train loss 2.79 on epoch=84
06/19/2022 22:08:25 - INFO - __main__ - Step 180 Global step 180 Train loss 2.81 on epoch=89
06/19/2022 22:08:27 - INFO - __main__ - Step 190 Global step 190 Train loss 2.79 on epoch=94
06/19/2022 22:08:28 - INFO - __main__ - Step 200 Global step 200 Train loss 2.54 on epoch=99
06/19/2022 22:08:30 - INFO - __main__ - Global step 200 Train loss 2.76 Classification-F1 0.08536585365853659 on epoch=99
06/19/2022 22:08:32 - INFO - __main__ - Step 210 Global step 210 Train loss 2.33 on epoch=104
06/19/2022 22:08:33 - INFO - __main__ - Step 220 Global step 220 Train loss 2.65 on epoch=109
06/19/2022 22:08:34 - INFO - __main__ - Step 230 Global step 230 Train loss 2.19 on epoch=114
06/19/2022 22:08:35 - INFO - __main__ - Step 240 Global step 240 Train loss 2.18 on epoch=119
06/19/2022 22:08:37 - INFO - __main__ - Step 250 Global step 250 Train loss 2.18 on epoch=124
06/19/2022 22:08:38 - INFO - __main__ - Global step 250 Train loss 2.31 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 22:08:38 - INFO - __main__ - Saving model with best Classification-F1: 0.11363636363636363 -> 0.3333333333333333 on epoch=124, global_step=250
06/19/2022 22:08:39 - INFO - __main__ - Step 260 Global step 260 Train loss 2.05 on epoch=129
06/19/2022 22:08:40 - INFO - __main__ - Step 270 Global step 270 Train loss 2.12 on epoch=134
06/19/2022 22:08:41 - INFO - __main__ - Step 280 Global step 280 Train loss 1.90 on epoch=139
06/19/2022 22:08:43 - INFO - __main__ - Step 290 Global step 290 Train loss 1.94 on epoch=144
06/19/2022 22:08:44 - INFO - __main__ - Step 300 Global step 300 Train loss 1.83 on epoch=149
06/19/2022 22:08:44 - INFO - __main__ - Global step 300 Train loss 1.97 Classification-F1 0.3191489361702127 on epoch=149
06/19/2022 22:08:46 - INFO - __main__ - Step 310 Global step 310 Train loss 1.72 on epoch=154
06/19/2022 22:08:47 - INFO - __main__ - Step 320 Global step 320 Train loss 1.77 on epoch=159
06/19/2022 22:08:48 - INFO - __main__ - Step 330 Global step 330 Train loss 1.58 on epoch=164
06/19/2022 22:08:49 - INFO - __main__ - Step 340 Global step 340 Train loss 1.51 on epoch=169
06/19/2022 22:08:51 - INFO - __main__ - Step 350 Global step 350 Train loss 1.51 on epoch=174
06/19/2022 22:08:51 - INFO - __main__ - Global step 350 Train loss 1.62 Classification-F1 0.49090909090909085 on epoch=174
06/19/2022 22:08:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.49090909090909085 on epoch=174, global_step=350
06/19/2022 22:08:52 - INFO - __main__ - Step 360 Global step 360 Train loss 1.53 on epoch=179
06/19/2022 22:08:53 - INFO - __main__ - Step 370 Global step 370 Train loss 1.40 on epoch=184
06/19/2022 22:08:55 - INFO - __main__ - Step 380 Global step 380 Train loss 1.37 on epoch=189
06/19/2022 22:08:56 - INFO - __main__ - Step 390 Global step 390 Train loss 1.33 on epoch=194
06/19/2022 22:08:57 - INFO - __main__ - Step 400 Global step 400 Train loss 1.30 on epoch=199
06/19/2022 22:08:58 - INFO - __main__ - Global step 400 Train loss 1.39 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 22:08:59 - INFO - __main__ - Step 410 Global step 410 Train loss 1.29 on epoch=204
06/19/2022 22:09:00 - INFO - __main__ - Step 420 Global step 420 Train loss 1.33 on epoch=209
06/19/2022 22:09:01 - INFO - __main__ - Step 430 Global step 430 Train loss 1.22 on epoch=214
06/19/2022 22:09:03 - INFO - __main__ - Step 440 Global step 440 Train loss 1.24 on epoch=219
06/19/2022 22:09:04 - INFO - __main__ - Step 450 Global step 450 Train loss 1.23 on epoch=224
06/19/2022 22:09:04 - INFO - __main__ - Global step 450 Train loss 1.26 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 22:09:06 - INFO - __main__ - Step 460 Global step 460 Train loss 1.07 on epoch=229
06/19/2022 22:09:07 - INFO - __main__ - Step 470 Global step 470 Train loss 1.28 on epoch=234
06/19/2022 22:09:08 - INFO - __main__ - Step 480 Global step 480 Train loss 1.17 on epoch=239
06/19/2022 22:09:09 - INFO - __main__ - Step 490 Global step 490 Train loss 1.18 on epoch=244
06/19/2022 22:09:11 - INFO - __main__ - Step 500 Global step 500 Train loss 1.06 on epoch=249
06/19/2022 22:09:11 - INFO - __main__ - Global step 500 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 22:09:12 - INFO - __main__ - Step 510 Global step 510 Train loss 1.09 on epoch=254
06/19/2022 22:09:14 - INFO - __main__ - Step 520 Global step 520 Train loss 1.00 on epoch=259
06/19/2022 22:09:15 - INFO - __main__ - Step 530 Global step 530 Train loss 1.01 on epoch=264
06/19/2022 22:09:16 - INFO - __main__ - Step 540 Global step 540 Train loss 1.09 on epoch=269
06/19/2022 22:09:17 - INFO - __main__ - Step 550 Global step 550 Train loss 1.02 on epoch=274
06/19/2022 22:09:18 - INFO - __main__ - Global step 550 Train loss 1.04 Classification-F1 0.3043478260869565 on epoch=274
06/19/2022 22:09:19 - INFO - __main__ - Step 560 Global step 560 Train loss 1.03 on epoch=279
06/19/2022 22:09:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.91 on epoch=284
06/19/2022 22:09:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.91 on epoch=289
06/19/2022 22:09:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.92 on epoch=294
06/19/2022 22:09:24 - INFO - __main__ - Step 600 Global step 600 Train loss 1.05 on epoch=299
06/19/2022 22:09:24 - INFO - __main__ - Global step 600 Train loss 0.97 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 22:09:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.78 on epoch=304
06/19/2022 22:09:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.90 on epoch=309
06/19/2022 22:09:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.84 on epoch=314
06/19/2022 22:09:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.86 on epoch=319
06/19/2022 22:09:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.75 on epoch=324
06/19/2022 22:09:31 - INFO - __main__ - Global step 650 Train loss 0.83 Classification-F1 0.4458874458874459 on epoch=324
06/19/2022 22:09:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.81 on epoch=329
06/19/2022 22:09:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.85 on epoch=334
06/19/2022 22:09:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.77 on epoch=339
06/19/2022 22:09:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.74 on epoch=344
06/19/2022 22:09:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.75 on epoch=349
06/19/2022 22:09:38 - INFO - __main__ - Global step 700 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 22:09:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.86 on epoch=354
06/19/2022 22:09:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.84 on epoch=359
06/19/2022 22:09:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.76 on epoch=364
06/19/2022 22:09:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.64 on epoch=369
06/19/2022 22:09:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.70 on epoch=374
06/19/2022 22:09:44 - INFO - __main__ - Global step 750 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 22:09:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.74 on epoch=379
06/19/2022 22:09:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.71 on epoch=384
06/19/2022 22:09:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.67 on epoch=389
06/19/2022 22:09:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.70 on epoch=394
06/19/2022 22:09:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.72 on epoch=399
06/19/2022 22:09:51 - INFO - __main__ - Global step 800 Train loss 0.71 Classification-F1 0.3191489361702127 on epoch=399
06/19/2022 22:09:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.61 on epoch=404
06/19/2022 22:09:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.74 on epoch=409
06/19/2022 22:09:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.74 on epoch=414
06/19/2022 22:09:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.64 on epoch=419
06/19/2022 22:09:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.66 on epoch=424
06/19/2022 22:09:58 - INFO - __main__ - Global step 850 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 22:09:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.62 on epoch=429
06/19/2022 22:10:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.66 on epoch=434
06/19/2022 22:10:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.66 on epoch=439
06/19/2022 22:10:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.72 on epoch=444
06/19/2022 22:10:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.60 on epoch=449
06/19/2022 22:10:04 - INFO - __main__ - Global step 900 Train loss 0.65 Classification-F1 0.3191489361702127 on epoch=449
06/19/2022 22:10:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.66 on epoch=454
06/19/2022 22:10:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.64 on epoch=459
06/19/2022 22:10:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.58 on epoch=464
06/19/2022 22:10:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.81 on epoch=469
06/19/2022 22:10:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.66 on epoch=474
06/19/2022 22:10:11 - INFO - __main__ - Global step 950 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 22:10:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.68 on epoch=479
06/19/2022 22:10:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.63 on epoch=484
06/19/2022 22:10:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.64 on epoch=489
06/19/2022 22:10:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.57 on epoch=494
06/19/2022 22:10:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.75 on epoch=499
06/19/2022 22:10:18 - INFO - __main__ - Global step 1000 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 22:10:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.65 on epoch=504
06/19/2022 22:10:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.66 on epoch=509
06/19/2022 22:10:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.55 on epoch=514
06/19/2022 22:10:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.58 on epoch=519
06/19/2022 22:10:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.51 on epoch=524
06/19/2022 22:10:24 - INFO - __main__ - Global step 1050 Train loss 0.59 Classification-F1 0.3992490613266583 on epoch=524
06/19/2022 22:10:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.57 on epoch=529
06/19/2022 22:10:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.64 on epoch=534
06/19/2022 22:10:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.62 on epoch=539
06/19/2022 22:10:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.63 on epoch=544
06/19/2022 22:10:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.61 on epoch=549
06/19/2022 22:10:31 - INFO - __main__ - Global step 1100 Train loss 0.62 Classification-F1 0.39756367663344405 on epoch=549
06/19/2022 22:10:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.64 on epoch=554
06/19/2022 22:10:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.52 on epoch=559
06/19/2022 22:10:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.63 on epoch=564
06/19/2022 22:10:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.55 on epoch=569
06/19/2022 22:10:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.65 on epoch=574
06/19/2022 22:10:38 - INFO - __main__ - Global step 1150 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 22:10:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.69 on epoch=579
06/19/2022 22:10:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.55 on epoch=584
06/19/2022 22:10:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.62 on epoch=589
06/19/2022 22:10:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.62 on epoch=594
06/19/2022 22:10:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.59 on epoch=599
06/19/2022 22:10:45 - INFO - __main__ - Global step 1200 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 22:10:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.71 on epoch=604
06/19/2022 22:10:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.62 on epoch=609
06/19/2022 22:10:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.66 on epoch=614
06/19/2022 22:10:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.57 on epoch=619
06/19/2022 22:10:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.58 on epoch=624
06/19/2022 22:10:51 - INFO - __main__ - Global step 1250 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 22:10:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.61 on epoch=629
06/19/2022 22:10:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.65 on epoch=634
06/19/2022 22:10:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.55 on epoch=639
06/19/2022 22:10:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.50 on epoch=644
06/19/2022 22:10:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=649
06/19/2022 22:10:58 - INFO - __main__ - Global step 1300 Train loss 0.56 Classification-F1 0.3992490613266583 on epoch=649
06/19/2022 22:10:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.56 on epoch=654
06/19/2022 22:11:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.53 on epoch=659
06/19/2022 22:11:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.57 on epoch=664
06/19/2022 22:11:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.55 on epoch=669
06/19/2022 22:11:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.63 on epoch=674
06/19/2022 22:11:05 - INFO - __main__ - Global step 1350 Train loss 0.57 Classification-F1 0.3107692307692308 on epoch=674
06/19/2022 22:11:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=679
06/19/2022 22:11:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.50 on epoch=684
06/19/2022 22:11:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.53 on epoch=689
06/19/2022 22:11:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.66 on epoch=694
06/19/2022 22:11:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.55 on epoch=699
06/19/2022 22:11:11 - INFO - __main__ - Global step 1400 Train loss 0.55 Classification-F1 0.3191489361702127 on epoch=699
06/19/2022 22:11:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.60 on epoch=704
06/19/2022 22:11:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.54 on epoch=709
06/19/2022 22:11:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.63 on epoch=714
06/19/2022 22:11:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.47 on epoch=719
06/19/2022 22:11:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.52 on epoch=724
06/19/2022 22:11:18 - INFO - __main__ - Global step 1450 Train loss 0.55 Classification-F1 0.4980392156862745 on epoch=724
06/19/2022 22:11:18 - INFO - __main__ - Saving model with best Classification-F1: 0.49090909090909085 -> 0.4980392156862745 on epoch=724, global_step=1450
06/19/2022 22:11:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.48 on epoch=729
06/19/2022 22:11:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.51 on epoch=734
06/19/2022 22:11:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.57 on epoch=739
06/19/2022 22:11:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.52 on epoch=744
06/19/2022 22:11:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.53 on epoch=749
06/19/2022 22:11:25 - INFO - __main__ - Global step 1500 Train loss 0.52 Classification-F1 0.3816425120772947 on epoch=749
06/19/2022 22:11:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.50 on epoch=754
06/19/2022 22:11:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.51 on epoch=759
06/19/2022 22:11:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.48 on epoch=764
06/19/2022 22:11:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.53 on epoch=769
06/19/2022 22:11:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.52 on epoch=774
06/19/2022 22:11:31 - INFO - __main__ - Global step 1550 Train loss 0.51 Classification-F1 0.3191489361702127 on epoch=774
06/19/2022 22:11:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.51 on epoch=779
06/19/2022 22:11:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.48 on epoch=784
06/19/2022 22:11:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.48 on epoch=789
06/19/2022 22:11:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.49 on epoch=794
06/19/2022 22:11:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.52 on epoch=799
06/19/2022 22:11:38 - INFO - __main__ - Global step 1600 Train loss 0.50 Classification-F1 0.3191489361702127 on epoch=799
06/19/2022 22:11:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.50 on epoch=804
06/19/2022 22:11:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.60 on epoch=809
06/19/2022 22:11:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
06/19/2022 22:11:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=819
06/19/2022 22:11:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.49 on epoch=824
06/19/2022 22:11:45 - INFO - __main__ - Global step 1650 Train loss 0.51 Classification-F1 0.5333333333333333 on epoch=824
06/19/2022 22:11:45 - INFO - __main__ - Saving model with best Classification-F1: 0.4980392156862745 -> 0.5333333333333333 on epoch=824, global_step=1650
06/19/2022 22:11:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.52 on epoch=829
06/19/2022 22:11:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.46 on epoch=834
06/19/2022 22:11:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.54 on epoch=839
06/19/2022 22:11:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.51 on epoch=844
06/19/2022 22:11:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.54 on epoch=849
06/19/2022 22:11:51 - INFO - __main__ - Global step 1700 Train loss 0.51 Classification-F1 0.4420512820512821 on epoch=849
06/19/2022 22:11:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.55 on epoch=854
06/19/2022 22:11:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.50 on epoch=859
06/19/2022 22:11:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.51 on epoch=864
06/19/2022 22:11:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=869
06/19/2022 22:11:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.51 on epoch=874
06/19/2022 22:11:58 - INFO - __main__ - Global step 1750 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 22:11:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.57 on epoch=879
06/19/2022 22:12:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.56 on epoch=884
06/19/2022 22:12:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.51 on epoch=889
06/19/2022 22:12:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.50 on epoch=894
06/19/2022 22:12:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.47 on epoch=899
06/19/2022 22:12:04 - INFO - __main__ - Global step 1800 Train loss 0.52 Classification-F1 0.3043478260869565 on epoch=899
06/19/2022 22:12:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.56 on epoch=904
06/19/2022 22:12:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.45 on epoch=909
06/19/2022 22:12:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.55 on epoch=914
06/19/2022 22:12:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.51 on epoch=919
06/19/2022 22:12:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.50 on epoch=924
06/19/2022 22:12:11 - INFO - __main__ - Global step 1850 Train loss 0.51 Classification-F1 0.3992490613266583 on epoch=924
06/19/2022 22:12:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.48 on epoch=929
06/19/2022 22:12:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.52 on epoch=934
06/19/2022 22:12:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=939
06/19/2022 22:12:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.50 on epoch=944
06/19/2022 22:12:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.52 on epoch=949
06/19/2022 22:12:18 - INFO - __main__ - Global step 1900 Train loss 0.50 Classification-F1 0.5134502923976608 on epoch=949
06/19/2022 22:12:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.48 on epoch=954
06/19/2022 22:12:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
06/19/2022 22:12:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.50 on epoch=964
06/19/2022 22:12:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.45 on epoch=969
06/19/2022 22:12:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.48 on epoch=974
06/19/2022 22:12:24 - INFO - __main__ - Global step 1950 Train loss 0.47 Classification-F1 0.49090909090909085 on epoch=974
06/19/2022 22:12:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.50 on epoch=979
06/19/2022 22:12:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.55 on epoch=984
06/19/2022 22:12:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.49 on epoch=989
06/19/2022 22:12:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.46 on epoch=994
06/19/2022 22:12:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.50 on epoch=999
06/19/2022 22:12:31 - INFO - __main__ - Global step 2000 Train loss 0.50 Classification-F1 0.6532019704433498 on epoch=999
06/19/2022 22:12:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5333333333333333 -> 0.6532019704433498 on epoch=999, global_step=2000
06/19/2022 22:12:31 - INFO - __main__ - save last model!
06/19/2022 22:12:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 22:12:31 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 22:12:31 - INFO - __main__ - Printing 3 examples
06/19/2022 22:12:31 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 22:12:31 - INFO - __main__ - ['0']
06/19/2022 22:12:31 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 22:12:31 - INFO - __main__ - ['1']
06/19/2022 22:12:31 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 22:12:31 - INFO - __main__ - ['1']
06/19/2022 22:12:31 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:12:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:12:32 - INFO - __main__ - Printing 3 examples
06/19/2022 22:12:32 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 22:12:32 - INFO - __main__ - ['1']
06/19/2022 22:12:32 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 22:12:32 - INFO - __main__ - ['1']
06/19/2022 22:12:32 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 22:12:32 - INFO - __main__ - ['1']
06/19/2022 22:12:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:12:32 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:12:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:12:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:12:32 - INFO - __main__ - Printing 3 examples
06/19/2022 22:12:32 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 22:12:32 - INFO - __main__ - ['1']
06/19/2022 22:12:32 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 22:12:32 - INFO - __main__ - ['1']
06/19/2022 22:12:32 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 22:12:32 - INFO - __main__ - ['1']
06/19/2022 22:12:32 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:12:32 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:12:32 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:12:35 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:12:37 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:12:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:12:38 - INFO - __main__ - Starting training!
06/19/2022 22:12:43 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 22:14:11 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_21_0.5_8_predictions.txt
06/19/2022 22:14:11 - INFO - __main__ - Classification-F1 on test data: 0.4935
06/19/2022 22:14:11 - INFO - __main__ - prefix=paws_16_21, lr=0.5, bsz=8, dev_performance=0.6532019704433498, test_performance=0.49348215762895953
06/19/2022 22:14:11 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.4, bsz=8 ...
06/19/2022 22:14:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:14:12 - INFO - __main__ - Printing 3 examples
06/19/2022 22:14:12 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 22:14:12 - INFO - __main__ - ['1']
06/19/2022 22:14:12 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 22:14:12 - INFO - __main__ - ['1']
06/19/2022 22:14:12 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 22:14:12 - INFO - __main__ - ['1']
06/19/2022 22:14:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:14:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:14:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:14:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:14:12 - INFO - __main__ - Printing 3 examples
06/19/2022 22:14:12 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 22:14:12 - INFO - __main__ - ['1']
06/19/2022 22:14:12 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 22:14:12 - INFO - __main__ - ['1']
06/19/2022 22:14:12 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 22:14:12 - INFO - __main__ - ['1']
06/19/2022 22:14:12 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:14:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:14:12 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:14:17 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:14:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:14:18 - INFO - __main__ - Starting training!
06/19/2022 22:14:19 - INFO - __main__ - Step 10 Global step 10 Train loss 5.81 on epoch=4
06/19/2022 22:14:20 - INFO - __main__ - Step 20 Global step 20 Train loss 5.52 on epoch=9
06/19/2022 22:14:21 - INFO - __main__ - Step 30 Global step 30 Train loss 4.71 on epoch=14
06/19/2022 22:14:23 - INFO - __main__ - Step 40 Global step 40 Train loss 4.24 on epoch=19
06/19/2022 22:14:24 - INFO - __main__ - Step 50 Global step 50 Train loss 4.14 on epoch=24
06/19/2022 22:14:30 - INFO - __main__ - Global step 50 Train loss 4.89 Classification-F1 0.0 on epoch=24
06/19/2022 22:14:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 22:14:31 - INFO - __main__ - Step 60 Global step 60 Train loss 3.93 on epoch=29
06/19/2022 22:14:33 - INFO - __main__ - Step 70 Global step 70 Train loss 3.79 on epoch=34
06/19/2022 22:14:34 - INFO - __main__ - Step 80 Global step 80 Train loss 3.66 on epoch=39
06/19/2022 22:14:35 - INFO - __main__ - Step 90 Global step 90 Train loss 3.56 on epoch=44
06/19/2022 22:14:36 - INFO - __main__ - Step 100 Global step 100 Train loss 3.34 on epoch=49
06/19/2022 22:14:43 - INFO - __main__ - Global step 100 Train loss 3.66 Classification-F1 0.0 on epoch=49
06/19/2022 22:14:44 - INFO - __main__ - Step 110 Global step 110 Train loss 3.10 on epoch=54
06/19/2022 22:14:45 - INFO - __main__ - Step 120 Global step 120 Train loss 3.16 on epoch=59
06/19/2022 22:14:46 - INFO - __main__ - Step 130 Global step 130 Train loss 3.07 on epoch=64
06/19/2022 22:14:48 - INFO - __main__ - Step 140 Global step 140 Train loss 2.76 on epoch=69
06/19/2022 22:14:49 - INFO - __main__ - Step 150 Global step 150 Train loss 2.97 on epoch=74
06/19/2022 22:14:49 - INFO - __main__ - Global step 150 Train loss 3.01 Classification-F1 0.3333333333333333 on epoch=74
06/19/2022 22:14:49 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=74, global_step=150
06/19/2022 22:14:51 - INFO - __main__ - Step 160 Global step 160 Train loss 2.61 on epoch=79
06/19/2022 22:14:52 - INFO - __main__ - Step 170 Global step 170 Train loss 2.76 on epoch=84
06/19/2022 22:14:53 - INFO - __main__ - Step 180 Global step 180 Train loss 2.68 on epoch=89
06/19/2022 22:14:54 - INFO - __main__ - Step 190 Global step 190 Train loss 2.54 on epoch=94
06/19/2022 22:14:56 - INFO - __main__ - Step 200 Global step 200 Train loss 2.52 on epoch=99
06/19/2022 22:14:57 - INFO - __main__ - Global step 200 Train loss 2.62 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 22:14:58 - INFO - __main__ - Step 210 Global step 210 Train loss 2.36 on epoch=104
06/19/2022 22:14:59 - INFO - __main__ - Step 220 Global step 220 Train loss 2.42 on epoch=109
06/19/2022 22:15:01 - INFO - __main__ - Step 230 Global step 230 Train loss 2.32 on epoch=114
06/19/2022 22:15:02 - INFO - __main__ - Step 240 Global step 240 Train loss 2.15 on epoch=119
06/19/2022 22:15:03 - INFO - __main__ - Step 250 Global step 250 Train loss 2.05 on epoch=124
06/19/2022 22:15:09 - INFO - __main__ - Global step 250 Train loss 2.26 Classification-F1 0.30838530838530837 on epoch=124
06/19/2022 22:15:10 - INFO - __main__ - Step 260 Global step 260 Train loss 2.06 on epoch=129
06/19/2022 22:15:12 - INFO - __main__ - Step 270 Global step 270 Train loss 2.01 on epoch=134
06/19/2022 22:15:13 - INFO - __main__ - Step 280 Global step 280 Train loss 2.13 on epoch=139
06/19/2022 22:15:14 - INFO - __main__ - Step 290 Global step 290 Train loss 1.93 on epoch=144
06/19/2022 22:15:15 - INFO - __main__ - Step 300 Global step 300 Train loss 2.01 on epoch=149
06/19/2022 22:15:16 - INFO - __main__ - Global step 300 Train loss 2.03 Classification-F1 0.4920634920634921 on epoch=149
06/19/2022 22:15:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4920634920634921 on epoch=149, global_step=300
06/19/2022 22:15:17 - INFO - __main__ - Step 310 Global step 310 Train loss 1.80 on epoch=154
06/19/2022 22:15:19 - INFO - __main__ - Step 320 Global step 320 Train loss 1.72 on epoch=159
06/19/2022 22:15:20 - INFO - __main__ - Step 330 Global step 330 Train loss 1.74 on epoch=164
06/19/2022 22:15:21 - INFO - __main__ - Step 340 Global step 340 Train loss 1.53 on epoch=169
06/19/2022 22:15:22 - INFO - __main__ - Step 350 Global step 350 Train loss 1.47 on epoch=174
06/19/2022 22:15:23 - INFO - __main__ - Global step 350 Train loss 1.65 Classification-F1 0.46843853820598 on epoch=174
06/19/2022 22:15:24 - INFO - __main__ - Step 360 Global step 360 Train loss 1.46 on epoch=179
06/19/2022 22:15:25 - INFO - __main__ - Step 370 Global step 370 Train loss 1.50 on epoch=184
06/19/2022 22:15:26 - INFO - __main__ - Step 380 Global step 380 Train loss 1.40 on epoch=189
06/19/2022 22:15:28 - INFO - __main__ - Step 390 Global step 390 Train loss 1.45 on epoch=194
06/19/2022 22:15:29 - INFO - __main__ - Step 400 Global step 400 Train loss 1.50 on epoch=199
06/19/2022 22:15:29 - INFO - __main__ - Global step 400 Train loss 1.46 Classification-F1 0.4458874458874459 on epoch=199
06/19/2022 22:15:31 - INFO - __main__ - Step 410 Global step 410 Train loss 1.41 on epoch=204
06/19/2022 22:15:32 - INFO - __main__ - Step 420 Global step 420 Train loss 1.33 on epoch=209
06/19/2022 22:15:33 - INFO - __main__ - Step 430 Global step 430 Train loss 1.45 on epoch=214
06/19/2022 22:15:34 - INFO - __main__ - Step 440 Global step 440 Train loss 1.42 on epoch=219
06/19/2022 22:15:36 - INFO - __main__ - Step 450 Global step 450 Train loss 1.25 on epoch=224
06/19/2022 22:15:36 - INFO - __main__ - Global step 450 Train loss 1.37 Classification-F1 0.5333333333333333 on epoch=224
06/19/2022 22:15:36 - INFO - __main__ - Saving model with best Classification-F1: 0.4920634920634921 -> 0.5333333333333333 on epoch=224, global_step=450
06/19/2022 22:15:37 - INFO - __main__ - Step 460 Global step 460 Train loss 1.36 on epoch=229
06/19/2022 22:15:39 - INFO - __main__ - Step 470 Global step 470 Train loss 1.31 on epoch=234
06/19/2022 22:15:40 - INFO - __main__ - Step 480 Global step 480 Train loss 1.23 on epoch=239
06/19/2022 22:15:41 - INFO - __main__ - Step 490 Global step 490 Train loss 1.18 on epoch=244
06/19/2022 22:15:42 - INFO - __main__ - Step 500 Global step 500 Train loss 1.16 on epoch=249
06/19/2022 22:15:43 - INFO - __main__ - Global step 500 Train loss 1.25 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 22:15:44 - INFO - __main__ - Step 510 Global step 510 Train loss 1.19 on epoch=254
06/19/2022 22:15:45 - INFO - __main__ - Step 520 Global step 520 Train loss 1.22 on epoch=259
06/19/2022 22:15:46 - INFO - __main__ - Step 530 Global step 530 Train loss 1.11 on epoch=264
06/19/2022 22:15:48 - INFO - __main__ - Step 540 Global step 540 Train loss 1.21 on epoch=269
06/19/2022 22:15:49 - INFO - __main__ - Step 550 Global step 550 Train loss 1.13 on epoch=274
06/19/2022 22:15:49 - INFO - __main__ - Global step 550 Train loss 1.17 Classification-F1 0.3333333333333333 on epoch=274
06/19/2022 22:15:50 - INFO - __main__ - Step 560 Global step 560 Train loss 1.16 on epoch=279
06/19/2022 22:15:52 - INFO - __main__ - Step 570 Global step 570 Train loss 1.06 on epoch=284
06/19/2022 22:15:53 - INFO - __main__ - Step 580 Global step 580 Train loss 1.14 on epoch=289
06/19/2022 22:15:54 - INFO - __main__ - Step 590 Global step 590 Train loss 1.04 on epoch=294
06/19/2022 22:15:55 - INFO - __main__ - Step 600 Global step 600 Train loss 1.04 on epoch=299
06/19/2022 22:15:56 - INFO - __main__ - Global step 600 Train loss 1.09 Classification-F1 0.4385964912280702 on epoch=299
06/19/2022 22:15:57 - INFO - __main__ - Step 610 Global step 610 Train loss 1.08 on epoch=304
06/19/2022 22:15:58 - INFO - __main__ - Step 620 Global step 620 Train loss 1.04 on epoch=309
06/19/2022 22:15:59 - INFO - __main__ - Step 630 Global step 630 Train loss 1.09 on epoch=314
06/19/2022 22:16:01 - INFO - __main__ - Step 640 Global step 640 Train loss 1.03 on epoch=319
06/19/2022 22:16:02 - INFO - __main__ - Step 650 Global step 650 Train loss 1.05 on epoch=324
06/19/2022 22:16:02 - INFO - __main__ - Global step 650 Train loss 1.06 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 22:16:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.98 on epoch=329
06/19/2022 22:16:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.85 on epoch=334
06/19/2022 22:16:06 - INFO - __main__ - Step 680 Global step 680 Train loss 1.00 on epoch=339
06/19/2022 22:16:07 - INFO - __main__ - Step 690 Global step 690 Train loss 1.05 on epoch=344
06/19/2022 22:16:08 - INFO - __main__ - Step 700 Global step 700 Train loss 1.00 on epoch=349
06/19/2022 22:16:09 - INFO - __main__ - Global step 700 Train loss 0.97 Classification-F1 0.3191489361702127 on epoch=349
06/19/2022 22:16:10 - INFO - __main__ - Step 710 Global step 710 Train loss 1.11 on epoch=354
06/19/2022 22:16:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.93 on epoch=359
06/19/2022 22:16:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.95 on epoch=364
06/19/2022 22:16:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.96 on epoch=369
06/19/2022 22:16:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.95 on epoch=374
06/19/2022 22:16:15 - INFO - __main__ - Global step 750 Train loss 0.98 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 22:16:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.99 on epoch=379
06/19/2022 22:16:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.91 on epoch=384
06/19/2022 22:16:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.79 on epoch=389
06/19/2022 22:16:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.83 on epoch=394
06/19/2022 22:16:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.78 on epoch=399
06/19/2022 22:16:22 - INFO - __main__ - Global step 800 Train loss 0.86 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 22:16:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.94 on epoch=404
06/19/2022 22:16:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.90 on epoch=409
06/19/2022 22:16:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.83 on epoch=414
06/19/2022 22:16:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.88 on epoch=419
06/19/2022 22:16:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.76 on epoch=424
06/19/2022 22:16:29 - INFO - __main__ - Global step 850 Train loss 0.86 Classification-F1 0.3043478260869565 on epoch=424
06/19/2022 22:16:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.84 on epoch=429
06/19/2022 22:16:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.75 on epoch=434
06/19/2022 22:16:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.74 on epoch=439
06/19/2022 22:16:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.83 on epoch=444
06/19/2022 22:16:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.79 on epoch=449
06/19/2022 22:16:35 - INFO - __main__ - Global step 900 Train loss 0.79 Classification-F1 0.3191489361702127 on epoch=449
06/19/2022 22:16:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.67 on epoch=454
06/19/2022 22:16:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.77 on epoch=459
06/19/2022 22:16:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.71 on epoch=464
06/19/2022 22:16:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.82 on epoch=469
06/19/2022 22:16:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.79 on epoch=474
06/19/2022 22:16:42 - INFO - __main__ - Global step 950 Train loss 0.75 Classification-F1 0.3992490613266583 on epoch=474
06/19/2022 22:16:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.75 on epoch=479
06/19/2022 22:16:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.84 on epoch=484
06/19/2022 22:16:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.81 on epoch=489
06/19/2022 22:16:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.71 on epoch=494
06/19/2022 22:16:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.68 on epoch=499
06/19/2022 22:16:48 - INFO - __main__ - Global step 1000 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 22:16:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.66 on epoch=504
06/19/2022 22:16:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.65 on epoch=509
06/19/2022 22:16:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.77 on epoch=514
06/19/2022 22:16:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.68 on epoch=519
06/19/2022 22:16:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.76 on epoch=524
06/19/2022 22:16:55 - INFO - __main__ - Global step 1050 Train loss 0.70 Classification-F1 0.3816425120772947 on epoch=524
06/19/2022 22:16:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.70 on epoch=529
06/19/2022 22:16:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.76 on epoch=534
06/19/2022 22:16:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.73 on epoch=539
06/19/2022 22:17:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.71 on epoch=544
06/19/2022 22:17:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.77 on epoch=549
06/19/2022 22:17:01 - INFO - __main__ - Global step 1100 Train loss 0.74 Classification-F1 0.4385964912280702 on epoch=549
06/19/2022 22:17:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.72 on epoch=554
06/19/2022 22:17:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.74 on epoch=559
06/19/2022 22:17:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.66 on epoch=564
06/19/2022 22:17:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.68 on epoch=569
06/19/2022 22:17:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.66 on epoch=574
06/19/2022 22:17:08 - INFO - __main__ - Global step 1150 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 22:17:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.70 on epoch=579
06/19/2022 22:17:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.62 on epoch=584
06/19/2022 22:17:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.56 on epoch=589
06/19/2022 22:17:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.61 on epoch=594
06/19/2022 22:17:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.71 on epoch=599
06/19/2022 22:17:15 - INFO - __main__ - Global step 1200 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 22:17:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.69 on epoch=604
06/19/2022 22:17:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.54 on epoch=609
06/19/2022 22:17:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.67 on epoch=614
06/19/2022 22:17:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.67 on epoch=619
06/19/2022 22:17:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.65 on epoch=624
06/19/2022 22:17:21 - INFO - __main__ - Global step 1250 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 22:17:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.61 on epoch=629
06/19/2022 22:17:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.65 on epoch=634
06/19/2022 22:17:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.57 on epoch=639
06/19/2022 22:17:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.68 on epoch=644
06/19/2022 22:17:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.57 on epoch=649
06/19/2022 22:17:28 - INFO - __main__ - Global step 1300 Train loss 0.62 Classification-F1 0.3191489361702127 on epoch=649
06/19/2022 22:17:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.70 on epoch=654
06/19/2022 22:17:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.69 on epoch=659
06/19/2022 22:17:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.59 on epoch=664
06/19/2022 22:17:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.59 on epoch=669
06/19/2022 22:17:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.53 on epoch=674
06/19/2022 22:17:35 - INFO - __main__ - Global step 1350 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 22:17:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.72 on epoch=679
06/19/2022 22:17:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.54 on epoch=684
06/19/2022 22:17:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.60 on epoch=689
06/19/2022 22:17:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.52 on epoch=694
06/19/2022 22:17:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.59 on epoch=699
06/19/2022 22:17:42 - INFO - __main__ - Global step 1400 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 22:17:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.58 on epoch=704
06/19/2022 22:17:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.52 on epoch=709
06/19/2022 22:17:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.70 on epoch=714
06/19/2022 22:17:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.65 on epoch=719
06/19/2022 22:17:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.60 on epoch=724
06/19/2022 22:17:48 - INFO - __main__ - Global step 1450 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 22:17:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.53 on epoch=729
06/19/2022 22:17:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.58 on epoch=734
06/19/2022 22:17:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.72 on epoch=739
06/19/2022 22:17:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.63 on epoch=744
06/19/2022 22:17:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.62 on epoch=749
06/19/2022 22:17:55 - INFO - __main__ - Global step 1500 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 22:17:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.61 on epoch=754
06/19/2022 22:17:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.53 on epoch=759
06/19/2022 22:17:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.56 on epoch=764
06/19/2022 22:18:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.66 on epoch=769
06/19/2022 22:18:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.48 on epoch=774
06/19/2022 22:18:02 - INFO - __main__ - Global step 1550 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 22:18:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.62 on epoch=779
06/19/2022 22:18:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.55 on epoch=784
06/19/2022 22:18:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.61 on epoch=789
06/19/2022 22:18:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.55 on epoch=794
06/19/2022 22:18:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.52 on epoch=799
06/19/2022 22:18:09 - INFO - __main__ - Global step 1600 Train loss 0.57 Classification-F1 0.4666666666666667 on epoch=799
06/19/2022 22:18:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.67 on epoch=804
06/19/2022 22:18:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.57 on epoch=809
06/19/2022 22:18:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.57 on epoch=814
06/19/2022 22:18:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.61 on epoch=819
06/19/2022 22:18:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.55 on epoch=824
06/19/2022 22:18:15 - INFO - __main__ - Global step 1650 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 22:18:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.59 on epoch=829
06/19/2022 22:18:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.57 on epoch=834
06/19/2022 22:18:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.55 on epoch=839
06/19/2022 22:18:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.53 on epoch=844
06/19/2022 22:18:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.55 on epoch=849
06/19/2022 22:18:22 - INFO - __main__ - Global step 1700 Train loss 0.56 Classification-F1 0.3992490613266583 on epoch=849
06/19/2022 22:18:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.50 on epoch=854
06/19/2022 22:18:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.55 on epoch=859
06/19/2022 22:18:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.53 on epoch=864
06/19/2022 22:18:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.63 on epoch=869
06/19/2022 22:18:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.66 on epoch=874
06/19/2022 22:18:29 - INFO - __main__ - Global step 1750 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 22:18:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.52 on epoch=879
06/19/2022 22:18:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.50 on epoch=884
06/19/2022 22:18:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.53 on epoch=889
06/19/2022 22:18:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.57 on epoch=894
06/19/2022 22:18:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.53 on epoch=899
06/19/2022 22:18:36 - INFO - __main__ - Global step 1800 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 22:18:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.51 on epoch=904
06/19/2022 22:18:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.53 on epoch=909
06/19/2022 22:18:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.56 on epoch=914
06/19/2022 22:18:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.53 on epoch=919
06/19/2022 22:18:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.53 on epoch=924
06/19/2022 22:18:43 - INFO - __main__ - Global step 1850 Train loss 0.53 Classification-F1 0.3191489361702127 on epoch=924
06/19/2022 22:18:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.51 on epoch=929
06/19/2022 22:18:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.51 on epoch=934
06/19/2022 22:18:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.49 on epoch=939
06/19/2022 22:18:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.52 on epoch=944
06/19/2022 22:18:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.54 on epoch=949
06/19/2022 22:18:49 - INFO - __main__ - Global step 1900 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 22:18:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.53 on epoch=954
06/19/2022 22:18:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.51 on epoch=959
06/19/2022 22:18:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.52 on epoch=964
06/19/2022 22:18:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.54 on epoch=969
06/19/2022 22:18:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.50 on epoch=974
06/19/2022 22:18:56 - INFO - __main__ - Global step 1950 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 22:18:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.55 on epoch=979
06/19/2022 22:18:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.60 on epoch=984
06/19/2022 22:19:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.50 on epoch=989
06/19/2022 22:19:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.59 on epoch=994
06/19/2022 22:19:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.56 on epoch=999
06/19/2022 22:19:03 - INFO - __main__ - Global step 2000 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 22:19:03 - INFO - __main__ - save last model!
06/19/2022 22:19:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 22:19:03 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 22:19:03 - INFO - __main__ - Printing 3 examples
06/19/2022 22:19:03 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 22:19:03 - INFO - __main__ - ['0']
06/19/2022 22:19:03 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 22:19:03 - INFO - __main__ - ['1']
06/19/2022 22:19:03 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 22:19:03 - INFO - __main__ - ['1']
06/19/2022 22:19:03 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:19:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:19:03 - INFO - __main__ - Printing 3 examples
06/19/2022 22:19:03 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 22:19:03 - INFO - __main__ - ['1']
06/19/2022 22:19:03 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 22:19:03 - INFO - __main__ - ['1']
06/19/2022 22:19:03 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 22:19:03 - INFO - __main__ - ['1']
06/19/2022 22:19:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:19:03 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:19:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:19:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:19:03 - INFO - __main__ - Printing 3 examples
06/19/2022 22:19:03 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 22:19:03 - INFO - __main__ - ['1']
06/19/2022 22:19:03 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 22:19:03 - INFO - __main__ - ['1']
06/19/2022 22:19:03 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 22:19:03 - INFO - __main__ - ['1']
06/19/2022 22:19:03 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:19:03 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:19:03 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:19:07 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:19:09 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:19:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:19:09 - INFO - __main__ - Starting training!
06/19/2022 22:19:15 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 22:20:39 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_21_0.4_8_predictions.txt
06/19/2022 22:20:39 - INFO - __main__ - Classification-F1 on test data: 0.3076
06/19/2022 22:20:40 - INFO - __main__ - prefix=paws_16_21, lr=0.4, bsz=8, dev_performance=0.5333333333333333, test_performance=0.3075972342735259
06/19/2022 22:20:40 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.3, bsz=8 ...
06/19/2022 22:20:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:20:40 - INFO - __main__ - Printing 3 examples
06/19/2022 22:20:40 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 22:20:40 - INFO - __main__ - ['1']
06/19/2022 22:20:40 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 22:20:40 - INFO - __main__ - ['1']
06/19/2022 22:20:40 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 22:20:40 - INFO - __main__ - ['1']
06/19/2022 22:20:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:20:40 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:20:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:20:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:20:41 - INFO - __main__ - Printing 3 examples
06/19/2022 22:20:41 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 22:20:41 - INFO - __main__ - ['1']
06/19/2022 22:20:41 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 22:20:41 - INFO - __main__ - ['1']
06/19/2022 22:20:41 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 22:20:41 - INFO - __main__ - ['1']
06/19/2022 22:20:41 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:20:41 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:20:41 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:20:46 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:20:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:20:47 - INFO - __main__ - Starting training!
06/19/2022 22:20:48 - INFO - __main__ - Step 10 Global step 10 Train loss 5.91 on epoch=4
06/19/2022 22:20:50 - INFO - __main__ - Step 20 Global step 20 Train loss 5.50 on epoch=9
06/19/2022 22:20:51 - INFO - __main__ - Step 30 Global step 30 Train loss 5.29 on epoch=14
06/19/2022 22:20:52 - INFO - __main__ - Step 40 Global step 40 Train loss 4.73 on epoch=19
06/19/2022 22:20:54 - INFO - __main__ - Step 50 Global step 50 Train loss 4.40 on epoch=24
06/19/2022 22:20:59 - INFO - __main__ - Global step 50 Train loss 5.17 Classification-F1 0.0 on epoch=24
06/19/2022 22:20:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 22:21:01 - INFO - __main__ - Step 60 Global step 60 Train loss 4.20 on epoch=29
06/19/2022 22:21:02 - INFO - __main__ - Step 70 Global step 70 Train loss 4.11 on epoch=34
06/19/2022 22:21:03 - INFO - __main__ - Step 80 Global step 80 Train loss 3.91 on epoch=39
06/19/2022 22:21:04 - INFO - __main__ - Step 90 Global step 90 Train loss 3.90 on epoch=44
06/19/2022 22:21:06 - INFO - __main__ - Step 100 Global step 100 Train loss 3.89 on epoch=49
06/19/2022 22:21:08 - INFO - __main__ - Global step 100 Train loss 4.00 Classification-F1 0.0 on epoch=49
06/19/2022 22:21:09 - INFO - __main__ - Step 110 Global step 110 Train loss 3.70 on epoch=54
06/19/2022 22:21:11 - INFO - __main__ - Step 120 Global step 120 Train loss 3.67 on epoch=59
06/19/2022 22:21:12 - INFO - __main__ - Step 130 Global step 130 Train loss 3.65 on epoch=64
06/19/2022 22:21:13 - INFO - __main__ - Step 140 Global step 140 Train loss 3.52 on epoch=69
06/19/2022 22:21:14 - INFO - __main__ - Step 150 Global step 150 Train loss 3.38 on epoch=74
06/19/2022 22:21:25 - INFO - __main__ - Global step 150 Train loss 3.58 Classification-F1 0.0 on epoch=74
06/19/2022 22:21:26 - INFO - __main__ - Step 160 Global step 160 Train loss 3.34 on epoch=79
06/19/2022 22:21:27 - INFO - __main__ - Step 170 Global step 170 Train loss 3.42 on epoch=84
06/19/2022 22:21:28 - INFO - __main__ - Step 180 Global step 180 Train loss 3.16 on epoch=89
06/19/2022 22:21:30 - INFO - __main__ - Step 190 Global step 190 Train loss 3.04 on epoch=94
06/19/2022 22:21:31 - INFO - __main__ - Step 200 Global step 200 Train loss 2.94 on epoch=99
06/19/2022 22:21:32 - INFO - __main__ - Global step 200 Train loss 3.18 Classification-F1 0.21212121212121213 on epoch=99
06/19/2022 22:21:32 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.21212121212121213 on epoch=99, global_step=200
06/19/2022 22:21:33 - INFO - __main__ - Step 210 Global step 210 Train loss 2.89 on epoch=104
06/19/2022 22:21:35 - INFO - __main__ - Step 220 Global step 220 Train loss 2.78 on epoch=109
06/19/2022 22:21:36 - INFO - __main__ - Step 230 Global step 230 Train loss 2.78 on epoch=114
06/19/2022 22:21:37 - INFO - __main__ - Step 240 Global step 240 Train loss 2.74 on epoch=119
06/19/2022 22:21:38 - INFO - __main__ - Step 250 Global step 250 Train loss 2.70 on epoch=124
06/19/2022 22:21:40 - INFO - __main__ - Global step 250 Train loss 2.78 Classification-F1 0.1739130434782609 on epoch=124
06/19/2022 22:21:41 - INFO - __main__ - Step 260 Global step 260 Train loss 2.71 on epoch=129
06/19/2022 22:21:42 - INFO - __main__ - Step 270 Global step 270 Train loss 2.56 on epoch=134
06/19/2022 22:21:43 - INFO - __main__ - Step 280 Global step 280 Train loss 2.59 on epoch=139
06/19/2022 22:21:45 - INFO - __main__ - Step 290 Global step 290 Train loss 2.50 on epoch=144
06/19/2022 22:21:46 - INFO - __main__ - Step 300 Global step 300 Train loss 2.36 on epoch=149
06/19/2022 22:21:56 - INFO - __main__ - Global step 300 Train loss 2.54 Classification-F1 0.13071895424836602 on epoch=149
06/19/2022 22:21:58 - INFO - __main__ - Step 310 Global step 310 Train loss 2.19 on epoch=154
06/19/2022 22:21:59 - INFO - __main__ - Step 320 Global step 320 Train loss 2.38 on epoch=159
06/19/2022 22:22:00 - INFO - __main__ - Step 330 Global step 330 Train loss 2.47 on epoch=164
06/19/2022 22:22:01 - INFO - __main__ - Step 340 Global step 340 Train loss 2.31 on epoch=169
06/19/2022 22:22:03 - INFO - __main__ - Step 350 Global step 350 Train loss 2.30 on epoch=174
06/19/2022 22:22:09 - INFO - __main__ - Global step 350 Train loss 2.33 Classification-F1 0.16714285714285712 on epoch=174
06/19/2022 22:22:10 - INFO - __main__ - Step 360 Global step 360 Train loss 2.07 on epoch=179
06/19/2022 22:22:11 - INFO - __main__ - Step 370 Global step 370 Train loss 2.10 on epoch=184
06/19/2022 22:22:12 - INFO - __main__ - Step 380 Global step 380 Train loss 2.13 on epoch=189
06/19/2022 22:22:14 - INFO - __main__ - Step 390 Global step 390 Train loss 2.20 on epoch=194
06/19/2022 22:22:15 - INFO - __main__ - Step 400 Global step 400 Train loss 2.08 on epoch=199
06/19/2022 22:22:17 - INFO - __main__ - Global step 400 Train loss 2.11 Classification-F1 0.2873806998939555 on epoch=199
06/19/2022 22:22:17 - INFO - __main__ - Saving model with best Classification-F1: 0.21212121212121213 -> 0.2873806998939555 on epoch=199, global_step=400
06/19/2022 22:22:18 - INFO - __main__ - Step 410 Global step 410 Train loss 1.98 on epoch=204
06/19/2022 22:22:19 - INFO - __main__ - Step 420 Global step 420 Train loss 1.90 on epoch=209
06/19/2022 22:22:20 - INFO - __main__ - Step 430 Global step 430 Train loss 1.95 on epoch=214
06/19/2022 22:22:22 - INFO - __main__ - Step 440 Global step 440 Train loss 2.00 on epoch=219
06/19/2022 22:22:23 - INFO - __main__ - Step 450 Global step 450 Train loss 1.83 on epoch=224
06/19/2022 22:22:24 - INFO - __main__ - Global step 450 Train loss 1.93 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 22:22:24 - INFO - __main__ - Saving model with best Classification-F1: 0.2873806998939555 -> 0.3333333333333333 on epoch=224, global_step=450
06/19/2022 22:22:25 - INFO - __main__ - Step 460 Global step 460 Train loss 1.91 on epoch=229
06/19/2022 22:22:26 - INFO - __main__ - Step 470 Global step 470 Train loss 1.79 on epoch=234
06/19/2022 22:22:28 - INFO - __main__ - Step 480 Global step 480 Train loss 1.93 on epoch=239
06/19/2022 22:22:29 - INFO - __main__ - Step 490 Global step 490 Train loss 1.62 on epoch=244
06/19/2022 22:22:30 - INFO - __main__ - Step 500 Global step 500 Train loss 1.70 on epoch=249
06/19/2022 22:22:31 - INFO - __main__ - Global step 500 Train loss 1.79 Classification-F1 0.46843853820598 on epoch=249
06/19/2022 22:22:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.46843853820598 on epoch=249, global_step=500
06/19/2022 22:22:32 - INFO - __main__ - Step 510 Global step 510 Train loss 1.61 on epoch=254
06/19/2022 22:22:33 - INFO - __main__ - Step 520 Global step 520 Train loss 1.58 on epoch=259
06/19/2022 22:22:34 - INFO - __main__ - Step 530 Global step 530 Train loss 1.64 on epoch=264
06/19/2022 22:22:36 - INFO - __main__ - Step 540 Global step 540 Train loss 1.66 on epoch=269
06/19/2022 22:22:37 - INFO - __main__ - Step 550 Global step 550 Train loss 1.61 on epoch=274
06/19/2022 22:22:37 - INFO - __main__ - Global step 550 Train loss 1.62 Classification-F1 0.39139139139139134 on epoch=274
06/19/2022 22:22:38 - INFO - __main__ - Step 560 Global step 560 Train loss 1.45 on epoch=279
06/19/2022 22:22:40 - INFO - __main__ - Step 570 Global step 570 Train loss 1.63 on epoch=284
06/19/2022 22:22:41 - INFO - __main__ - Step 580 Global step 580 Train loss 1.36 on epoch=289
06/19/2022 22:22:42 - INFO - __main__ - Step 590 Global step 590 Train loss 1.39 on epoch=294
06/19/2022 22:22:43 - INFO - __main__ - Step 600 Global step 600 Train loss 1.47 on epoch=299
06/19/2022 22:22:44 - INFO - __main__ - Global step 600 Train loss 1.46 Classification-F1 0.49090909090909085 on epoch=299
06/19/2022 22:22:44 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.49090909090909085 on epoch=299, global_step=600
06/19/2022 22:22:45 - INFO - __main__ - Step 610 Global step 610 Train loss 1.49 on epoch=304
06/19/2022 22:22:46 - INFO - __main__ - Step 620 Global step 620 Train loss 1.33 on epoch=309
06/19/2022 22:22:47 - INFO - __main__ - Step 630 Global step 630 Train loss 1.27 on epoch=314
06/19/2022 22:22:49 - INFO - __main__ - Step 640 Global step 640 Train loss 1.50 on epoch=319
06/19/2022 22:22:50 - INFO - __main__ - Step 650 Global step 650 Train loss 1.26 on epoch=324
06/19/2022 22:22:50 - INFO - __main__ - Global step 650 Train loss 1.37 Classification-F1 0.3552492046659597 on epoch=324
06/19/2022 22:22:52 - INFO - __main__ - Step 660 Global step 660 Train loss 1.33 on epoch=329
06/19/2022 22:22:53 - INFO - __main__ - Step 670 Global step 670 Train loss 1.31 on epoch=334
06/19/2022 22:22:54 - INFO - __main__ - Step 680 Global step 680 Train loss 1.13 on epoch=339
06/19/2022 22:22:55 - INFO - __main__ - Step 690 Global step 690 Train loss 1.23 on epoch=344
06/19/2022 22:22:56 - INFO - __main__ - Step 700 Global step 700 Train loss 1.14 on epoch=349
06/19/2022 22:22:57 - INFO - __main__ - Global step 700 Train loss 1.23 Classification-F1 0.4385964912280702 on epoch=349
06/19/2022 22:22:58 - INFO - __main__ - Step 710 Global step 710 Train loss 1.23 on epoch=354
06/19/2022 22:22:59 - INFO - __main__ - Step 720 Global step 720 Train loss 1.32 on epoch=359
06/19/2022 22:23:01 - INFO - __main__ - Step 730 Global step 730 Train loss 1.29 on epoch=364
06/19/2022 22:23:02 - INFO - __main__ - Step 740 Global step 740 Train loss 1.22 on epoch=369
06/19/2022 22:23:03 - INFO - __main__ - Step 750 Global step 750 Train loss 1.01 on epoch=374
06/19/2022 22:23:03 - INFO - __main__ - Global step 750 Train loss 1.22 Classification-F1 0.3992490613266583 on epoch=374
06/19/2022 22:23:05 - INFO - __main__ - Step 760 Global step 760 Train loss 1.13 on epoch=379
06/19/2022 22:23:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.96 on epoch=384
06/19/2022 22:23:07 - INFO - __main__ - Step 780 Global step 780 Train loss 1.01 on epoch=389
06/19/2022 22:23:08 - INFO - __main__ - Step 790 Global step 790 Train loss 1.23 on epoch=394
06/19/2022 22:23:10 - INFO - __main__ - Step 800 Global step 800 Train loss 1.02 on epoch=399
06/19/2022 22:23:10 - INFO - __main__ - Global step 800 Train loss 1.07 Classification-F1 0.3552492046659597 on epoch=399
06/19/2022 22:23:11 - INFO - __main__ - Step 810 Global step 810 Train loss 1.04 on epoch=404
06/19/2022 22:23:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.94 on epoch=409
06/19/2022 22:23:14 - INFO - __main__ - Step 830 Global step 830 Train loss 1.09 on epoch=414
06/19/2022 22:23:15 - INFO - __main__ - Step 840 Global step 840 Train loss 1.11 on epoch=419
06/19/2022 22:23:16 - INFO - __main__ - Step 850 Global step 850 Train loss 1.08 on epoch=424
06/19/2022 22:23:16 - INFO - __main__ - Global step 850 Train loss 1.05 Classification-F1 0.4385964912280702 on epoch=424
06/19/2022 22:23:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.97 on epoch=429
06/19/2022 22:23:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.94 on epoch=434
06/19/2022 22:23:20 - INFO - __main__ - Step 880 Global step 880 Train loss 1.02 on epoch=439
06/19/2022 22:23:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.98 on epoch=444
06/19/2022 22:23:23 - INFO - __main__ - Step 900 Global step 900 Train loss 1.00 on epoch=449
06/19/2022 22:23:23 - INFO - __main__ - Global step 900 Train loss 0.98 Classification-F1 0.3992490613266583 on epoch=449
06/19/2022 22:23:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.96 on epoch=454
06/19/2022 22:23:26 - INFO - __main__ - Step 920 Global step 920 Train loss 1.01 on epoch=459
06/19/2022 22:23:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.94 on epoch=464
06/19/2022 22:23:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.90 on epoch=469
06/19/2022 22:23:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.92 on epoch=474
06/19/2022 22:23:30 - INFO - __main__ - Global step 950 Train loss 0.95 Classification-F1 0.2727272727272727 on epoch=474
06/19/2022 22:23:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.91 on epoch=479
06/19/2022 22:23:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.88 on epoch=484
06/19/2022 22:23:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.85 on epoch=489
06/19/2022 22:23:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.92 on epoch=494
06/19/2022 22:23:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.88 on epoch=499
06/19/2022 22:23:36 - INFO - __main__ - Global step 1000 Train loss 0.89 Classification-F1 0.5333333333333333 on epoch=499
06/19/2022 22:23:36 - INFO - __main__ - Saving model with best Classification-F1: 0.49090909090909085 -> 0.5333333333333333 on epoch=499, global_step=1000
06/19/2022 22:23:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.78 on epoch=504
06/19/2022 22:23:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.84 on epoch=509
06/19/2022 22:23:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.87 on epoch=514
06/19/2022 22:23:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.84 on epoch=519
06/19/2022 22:23:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.87 on epoch=524
06/19/2022 22:23:43 - INFO - __main__ - Global step 1050 Train loss 0.84 Classification-F1 0.4385964912280702 on epoch=524
06/19/2022 22:23:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.83 on epoch=529
06/19/2022 22:23:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.84 on epoch=534
06/19/2022 22:23:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.79 on epoch=539
06/19/2022 22:23:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.75 on epoch=544
06/19/2022 22:23:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.74 on epoch=549
06/19/2022 22:23:49 - INFO - __main__ - Global step 1100 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 22:23:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.75 on epoch=554
06/19/2022 22:23:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.78 on epoch=559
06/19/2022 22:23:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.80 on epoch=564
06/19/2022 22:23:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.66 on epoch=569
06/19/2022 22:23:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.87 on epoch=574
06/19/2022 22:23:56 - INFO - __main__ - Global step 1150 Train loss 0.78 Classification-F1 0.28888888888888886 on epoch=574
06/19/2022 22:23:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.78 on epoch=579
06/19/2022 22:23:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.78 on epoch=584
06/19/2022 22:24:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.78 on epoch=589
06/19/2022 22:24:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.83 on epoch=594
06/19/2022 22:24:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=599
06/19/2022 22:24:02 - INFO - __main__ - Global step 1200 Train loss 0.79 Classification-F1 0.3043478260869565 on epoch=599
06/19/2022 22:24:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.67 on epoch=604
06/19/2022 22:24:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.75 on epoch=609
06/19/2022 22:24:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.80 on epoch=614
06/19/2022 22:24:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.73 on epoch=619
06/19/2022 22:24:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.78 on epoch=624
06/19/2022 22:24:09 - INFO - __main__ - Global step 1250 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 22:24:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.66 on epoch=629
06/19/2022 22:24:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.65 on epoch=634
06/19/2022 22:24:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.75 on epoch=639
06/19/2022 22:24:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.73 on epoch=644
06/19/2022 22:24:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.72 on epoch=649
06/19/2022 22:24:16 - INFO - __main__ - Global step 1300 Train loss 0.70 Classification-F1 0.3191489361702127 on epoch=649
06/19/2022 22:24:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.70 on epoch=654
06/19/2022 22:24:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.68 on epoch=659
06/19/2022 22:24:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.60 on epoch=664
06/19/2022 22:24:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.61 on epoch=669
06/19/2022 22:24:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.87 on epoch=674
06/19/2022 22:24:22 - INFO - __main__ - Global step 1350 Train loss 0.69 Classification-F1 0.4181818181818182 on epoch=674
06/19/2022 22:24:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.66 on epoch=679
06/19/2022 22:24:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.75 on epoch=684
06/19/2022 22:24:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.75 on epoch=689
06/19/2022 22:24:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.67 on epoch=694
06/19/2022 22:24:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.68 on epoch=699
06/19/2022 22:24:29 - INFO - __main__ - Global step 1400 Train loss 0.70 Classification-F1 0.3992490613266583 on epoch=699
06/19/2022 22:24:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.64 on epoch=704
06/19/2022 22:24:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.72 on epoch=709
06/19/2022 22:24:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.61 on epoch=714
06/19/2022 22:24:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.66 on epoch=719
06/19/2022 22:24:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.65 on epoch=724
06/19/2022 22:24:35 - INFO - __main__ - Global step 1450 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 22:24:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.70 on epoch=729
06/19/2022 22:24:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.62 on epoch=734
06/19/2022 22:24:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.62 on epoch=739
06/19/2022 22:24:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.71 on epoch=744
06/19/2022 22:24:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.73 on epoch=749
06/19/2022 22:24:42 - INFO - __main__ - Global step 1500 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 22:24:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.68 on epoch=754
06/19/2022 22:24:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.65 on epoch=759
06/19/2022 22:24:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.65 on epoch=764
06/19/2022 22:24:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.66 on epoch=769
06/19/2022 22:24:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.63 on epoch=774
06/19/2022 22:24:49 - INFO - __main__ - Global step 1550 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 22:24:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.55 on epoch=779
06/19/2022 22:24:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.61 on epoch=784
06/19/2022 22:24:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.63 on epoch=789
06/19/2022 22:24:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.62 on epoch=794
06/19/2022 22:24:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.63 on epoch=799
06/19/2022 22:24:55 - INFO - __main__ - Global step 1600 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 22:24:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.55 on epoch=804
06/19/2022 22:24:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.60 on epoch=809
06/19/2022 22:24:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.65 on epoch=814
06/19/2022 22:25:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.63 on epoch=819
06/19/2022 22:25:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.57 on epoch=824
06/19/2022 22:25:02 - INFO - __main__ - Global step 1650 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 22:25:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.62 on epoch=829
06/19/2022 22:25:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.67 on epoch=834
06/19/2022 22:25:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.60 on epoch=839
06/19/2022 22:25:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.53 on epoch=844
06/19/2022 22:25:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.56 on epoch=849
06/19/2022 22:25:08 - INFO - __main__ - Global step 1700 Train loss 0.59 Classification-F1 0.3191489361702127 on epoch=849
06/19/2022 22:25:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.57 on epoch=854
06/19/2022 22:25:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.55 on epoch=859
06/19/2022 22:25:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.56 on epoch=864
06/19/2022 22:25:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.59 on epoch=869
06/19/2022 22:25:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.58 on epoch=874
06/19/2022 22:25:15 - INFO - __main__ - Global step 1750 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 22:25:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.59 on epoch=879
06/19/2022 22:25:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.59 on epoch=884
06/19/2022 22:25:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.56 on epoch=889
06/19/2022 22:25:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.51 on epoch=894
06/19/2022 22:25:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.59 on epoch=899
06/19/2022 22:25:22 - INFO - __main__ - Global step 1800 Train loss 0.57 Classification-F1 0.3191489361702127 on epoch=899
06/19/2022 22:25:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.61 on epoch=904
06/19/2022 22:25:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.57 on epoch=909
06/19/2022 22:25:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.53 on epoch=914
06/19/2022 22:25:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.62 on epoch=919
06/19/2022 22:25:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.56 on epoch=924
06/19/2022 22:25:28 - INFO - __main__ - Global step 1850 Train loss 0.58 Classification-F1 0.4589371980676329 on epoch=924
06/19/2022 22:25:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.53 on epoch=929
06/19/2022 22:25:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.55 on epoch=934
06/19/2022 22:25:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.63 on epoch=939
06/19/2022 22:25:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.53 on epoch=944
06/19/2022 22:25:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.57 on epoch=949
06/19/2022 22:25:35 - INFO - __main__ - Global step 1900 Train loss 0.56 Classification-F1 0.4666666666666667 on epoch=949
06/19/2022 22:25:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.58 on epoch=954
06/19/2022 22:25:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.54 on epoch=959
06/19/2022 22:25:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.60 on epoch=964
06/19/2022 22:25:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.50 on epoch=969
06/19/2022 22:25:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.62 on epoch=974
06/19/2022 22:25:42 - INFO - __main__ - Global step 1950 Train loss 0.57 Classification-F1 0.5134502923976608 on epoch=974
06/19/2022 22:25:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.53 on epoch=979
06/19/2022 22:25:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.61 on epoch=984
06/19/2022 22:25:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.57 on epoch=989
06/19/2022 22:25:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.49 on epoch=994
06/19/2022 22:25:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=999
06/19/2022 22:25:48 - INFO - __main__ - Global step 2000 Train loss 0.54 Classification-F1 0.3043478260869565 on epoch=999
06/19/2022 22:25:48 - INFO - __main__ - save last model!
06/19/2022 22:25:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 22:25:48 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 22:25:48 - INFO - __main__ - Printing 3 examples
06/19/2022 22:25:48 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 22:25:48 - INFO - __main__ - ['0']
06/19/2022 22:25:48 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 22:25:48 - INFO - __main__ - ['1']
06/19/2022 22:25:48 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 22:25:48 - INFO - __main__ - ['1']
06/19/2022 22:25:48 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:25:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:25:49 - INFO - __main__ - Printing 3 examples
06/19/2022 22:25:49 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 22:25:49 - INFO - __main__ - ['1']
06/19/2022 22:25:49 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 22:25:49 - INFO - __main__ - ['1']
06/19/2022 22:25:49 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 22:25:49 - INFO - __main__ - ['1']
06/19/2022 22:25:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:25:49 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:25:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:25:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:25:49 - INFO - __main__ - Printing 3 examples
06/19/2022 22:25:49 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 22:25:49 - INFO - __main__ - ['1']
06/19/2022 22:25:49 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 22:25:49 - INFO - __main__ - ['1']
06/19/2022 22:25:49 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 22:25:49 - INFO - __main__ - ['1']
06/19/2022 22:25:49 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:25:49 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:25:49 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:25:52 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:25:54 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:25:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:25:55 - INFO - __main__ - Starting training!
06/19/2022 22:26:00 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 22:27:27 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_21_0.3_8_predictions.txt
06/19/2022 22:27:27 - INFO - __main__ - Classification-F1 on test data: 0.3550
06/19/2022 22:27:27 - INFO - __main__ - prefix=paws_16_21, lr=0.3, bsz=8, dev_performance=0.5333333333333333, test_performance=0.3550067745515185
06/19/2022 22:27:27 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.2, bsz=8 ...
06/19/2022 22:27:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:27:28 - INFO - __main__ - Printing 3 examples
06/19/2022 22:27:28 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/19/2022 22:27:28 - INFO - __main__ - ['1']
06/19/2022 22:27:28 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/19/2022 22:27:28 - INFO - __main__ - ['1']
06/19/2022 22:27:28 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/19/2022 22:27:28 - INFO - __main__ - ['1']
06/19/2022 22:27:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:27:28 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:27:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:27:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:27:28 - INFO - __main__ - Printing 3 examples
06/19/2022 22:27:28 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/19/2022 22:27:28 - INFO - __main__ - ['1']
06/19/2022 22:27:28 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/19/2022 22:27:28 - INFO - __main__ - ['1']
06/19/2022 22:27:28 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/19/2022 22:27:28 - INFO - __main__ - ['1']
06/19/2022 22:27:28 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:27:28 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:27:28 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:27:34 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:27:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:27:34 - INFO - __main__ - Starting training!
06/19/2022 22:27:36 - INFO - __main__ - Step 10 Global step 10 Train loss 5.86 on epoch=4
06/19/2022 22:27:37 - INFO - __main__ - Step 20 Global step 20 Train loss 5.74 on epoch=9
06/19/2022 22:27:38 - INFO - __main__ - Step 30 Global step 30 Train loss 5.39 on epoch=14
06/19/2022 22:27:40 - INFO - __main__ - Step 40 Global step 40 Train loss 5.08 on epoch=19
06/19/2022 22:27:41 - INFO - __main__ - Step 50 Global step 50 Train loss 4.80 on epoch=24
06/19/2022 22:27:51 - INFO - __main__ - Global step 50 Train loss 5.37 Classification-F1 0.0 on epoch=24
06/19/2022 22:27:51 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 22:27:53 - INFO - __main__ - Step 60 Global step 60 Train loss 4.43 on epoch=29
06/19/2022 22:27:54 - INFO - __main__ - Step 70 Global step 70 Train loss 4.23 on epoch=34
06/19/2022 22:27:55 - INFO - __main__ - Step 80 Global step 80 Train loss 4.18 on epoch=39
06/19/2022 22:27:57 - INFO - __main__ - Step 90 Global step 90 Train loss 4.16 on epoch=44
06/19/2022 22:27:58 - INFO - __main__ - Step 100 Global step 100 Train loss 3.94 on epoch=49
06/19/2022 22:28:04 - INFO - __main__ - Global step 100 Train loss 4.19 Classification-F1 0.0 on epoch=49
06/19/2022 22:28:05 - INFO - __main__ - Step 110 Global step 110 Train loss 3.81 on epoch=54
06/19/2022 22:28:06 - INFO - __main__ - Step 120 Global step 120 Train loss 3.90 on epoch=59
06/19/2022 22:28:08 - INFO - __main__ - Step 130 Global step 130 Train loss 3.79 on epoch=64
06/19/2022 22:28:09 - INFO - __main__ - Step 140 Global step 140 Train loss 3.89 on epoch=69
06/19/2022 22:28:11 - INFO - __main__ - Step 150 Global step 150 Train loss 3.74 on epoch=74
06/19/2022 22:28:17 - INFO - __main__ - Global step 150 Train loss 3.83 Classification-F1 0.0 on epoch=74
06/19/2022 22:28:18 - INFO - __main__ - Step 160 Global step 160 Train loss 3.77 on epoch=79
06/19/2022 22:28:20 - INFO - __main__ - Step 170 Global step 170 Train loss 3.57 on epoch=84
06/19/2022 22:28:21 - INFO - __main__ - Step 180 Global step 180 Train loss 3.59 on epoch=89
06/19/2022 22:28:22 - INFO - __main__ - Step 190 Global step 190 Train loss 3.78 on epoch=94
06/19/2022 22:28:23 - INFO - __main__ - Step 200 Global step 200 Train loss 3.39 on epoch=99
06/19/2022 22:28:30 - INFO - __main__ - Global step 200 Train loss 3.62 Classification-F1 0.0 on epoch=99
06/19/2022 22:28:32 - INFO - __main__ - Step 210 Global step 210 Train loss 3.38 on epoch=104
06/19/2022 22:28:33 - INFO - __main__ - Step 220 Global step 220 Train loss 3.47 on epoch=109
06/19/2022 22:28:34 - INFO - __main__ - Step 230 Global step 230 Train loss 3.36 on epoch=114
06/19/2022 22:28:35 - INFO - __main__ - Step 240 Global step 240 Train loss 3.22 on epoch=119
06/19/2022 22:28:37 - INFO - __main__ - Step 250 Global step 250 Train loss 3.24 on epoch=124
06/19/2022 22:28:48 - INFO - __main__ - Global step 250 Train loss 3.33 Classification-F1 0.0 on epoch=124
06/19/2022 22:28:49 - INFO - __main__ - Step 260 Global step 260 Train loss 3.16 on epoch=129
06/19/2022 22:28:50 - INFO - __main__ - Step 270 Global step 270 Train loss 3.32 on epoch=134
06/19/2022 22:28:51 - INFO - __main__ - Step 280 Global step 280 Train loss 3.05 on epoch=139
06/19/2022 22:28:53 - INFO - __main__ - Step 290 Global step 290 Train loss 3.32 on epoch=144
06/19/2022 22:28:54 - INFO - __main__ - Step 300 Global step 300 Train loss 3.16 on epoch=149
06/19/2022 22:29:04 - INFO - __main__ - Global step 300 Train loss 3.20 Classification-F1 0.0106951871657754 on epoch=149
06/19/2022 22:29:04 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.0106951871657754 on epoch=149, global_step=300
06/19/2022 22:29:06 - INFO - __main__ - Step 310 Global step 310 Train loss 3.11 on epoch=154
06/19/2022 22:29:07 - INFO - __main__ - Step 320 Global step 320 Train loss 3.12 on epoch=159
06/19/2022 22:29:08 - INFO - __main__ - Step 330 Global step 330 Train loss 3.03 on epoch=164
06/19/2022 22:29:09 - INFO - __main__ - Step 340 Global step 340 Train loss 2.98 on epoch=169
06/19/2022 22:29:11 - INFO - __main__ - Step 350 Global step 350 Train loss 2.86 on epoch=174
06/19/2022 22:29:11 - INFO - __main__ - Global step 350 Train loss 3.02 Classification-F1 0.20952380952380953 on epoch=174
06/19/2022 22:29:11 - INFO - __main__ - Saving model with best Classification-F1: 0.0106951871657754 -> 0.20952380952380953 on epoch=174, global_step=350
06/19/2022 22:29:13 - INFO - __main__ - Step 360 Global step 360 Train loss 2.99 on epoch=179
06/19/2022 22:29:14 - INFO - __main__ - Step 370 Global step 370 Train loss 2.85 on epoch=184
06/19/2022 22:29:15 - INFO - __main__ - Step 380 Global step 380 Train loss 2.86 on epoch=189
06/19/2022 22:29:16 - INFO - __main__ - Step 390 Global step 390 Train loss 2.92 on epoch=194
06/19/2022 22:29:18 - INFO - __main__ - Step 400 Global step 400 Train loss 2.81 on epoch=199
06/19/2022 22:29:20 - INFO - __main__ - Global step 400 Train loss 2.89 Classification-F1 0.048484848484848485 on epoch=199
06/19/2022 22:29:21 - INFO - __main__ - Step 410 Global step 410 Train loss 2.87 on epoch=204
06/19/2022 22:29:23 - INFO - __main__ - Step 420 Global step 420 Train loss 2.72 on epoch=209
06/19/2022 22:29:24 - INFO - __main__ - Step 430 Global step 430 Train loss 2.88 on epoch=214
06/19/2022 22:29:25 - INFO - __main__ - Step 440 Global step 440 Train loss 2.77 on epoch=219
06/19/2022 22:29:26 - INFO - __main__ - Step 450 Global step 450 Train loss 2.66 on epoch=224
06/19/2022 22:29:28 - INFO - __main__ - Global step 450 Train loss 2.78 Classification-F1 0.11111111111111112 on epoch=224
06/19/2022 22:29:30 - INFO - __main__ - Step 460 Global step 460 Train loss 2.68 on epoch=229
06/19/2022 22:29:31 - INFO - __main__ - Step 470 Global step 470 Train loss 2.61 on epoch=234
06/19/2022 22:29:32 - INFO - __main__ - Step 480 Global step 480 Train loss 2.53 on epoch=239
06/19/2022 22:29:33 - INFO - __main__ - Step 490 Global step 490 Train loss 2.53 on epoch=244
06/19/2022 22:29:35 - INFO - __main__ - Step 500 Global step 500 Train loss 2.55 on epoch=249
06/19/2022 22:29:36 - INFO - __main__ - Global step 500 Train loss 2.58 Classification-F1 0.16304347826086957 on epoch=249
06/19/2022 22:29:38 - INFO - __main__ - Step 510 Global step 510 Train loss 2.43 on epoch=254
06/19/2022 22:29:39 - INFO - __main__ - Step 520 Global step 520 Train loss 2.44 on epoch=259
06/19/2022 22:29:40 - INFO - __main__ - Step 530 Global step 530 Train loss 2.27 on epoch=264
06/19/2022 22:29:42 - INFO - __main__ - Step 540 Global step 540 Train loss 2.31 on epoch=269
06/19/2022 22:29:43 - INFO - __main__ - Step 550 Global step 550 Train loss 2.47 on epoch=274
06/19/2022 22:29:45 - INFO - __main__ - Global step 550 Train loss 2.39 Classification-F1 0.18181818181818182 on epoch=274
06/19/2022 22:29:46 - INFO - __main__ - Step 560 Global step 560 Train loss 2.22 on epoch=279
06/19/2022 22:29:47 - INFO - __main__ - Step 570 Global step 570 Train loss 2.25 on epoch=284
06/19/2022 22:29:48 - INFO - __main__ - Step 580 Global step 580 Train loss 2.22 on epoch=289
06/19/2022 22:29:50 - INFO - __main__ - Step 590 Global step 590 Train loss 2.20 on epoch=294
06/19/2022 22:29:51 - INFO - __main__ - Step 600 Global step 600 Train loss 2.28 on epoch=299
06/19/2022 22:29:53 - INFO - __main__ - Global step 600 Train loss 2.23 Classification-F1 0.23188405797101452 on epoch=299
06/19/2022 22:29:53 - INFO - __main__ - Saving model with best Classification-F1: 0.20952380952380953 -> 0.23188405797101452 on epoch=299, global_step=600
06/19/2022 22:29:54 - INFO - __main__ - Step 610 Global step 610 Train loss 2.16 on epoch=304
06/19/2022 22:29:55 - INFO - __main__ - Step 620 Global step 620 Train loss 2.17 on epoch=309
06/19/2022 22:29:57 - INFO - __main__ - Step 630 Global step 630 Train loss 2.24 on epoch=314
06/19/2022 22:29:58 - INFO - __main__ - Step 640 Global step 640 Train loss 2.18 on epoch=319
06/19/2022 22:29:59 - INFO - __main__ - Step 650 Global step 650 Train loss 2.30 on epoch=324
06/19/2022 22:30:01 - INFO - __main__ - Global step 650 Train loss 2.21 Classification-F1 0.2566069906223359 on epoch=324
06/19/2022 22:30:01 - INFO - __main__ - Saving model with best Classification-F1: 0.23188405797101452 -> 0.2566069906223359 on epoch=324, global_step=650
06/19/2022 22:30:03 - INFO - __main__ - Step 660 Global step 660 Train loss 2.07 on epoch=329
06/19/2022 22:30:04 - INFO - __main__ - Step 670 Global step 670 Train loss 1.97 on epoch=334
06/19/2022 22:30:05 - INFO - __main__ - Step 680 Global step 680 Train loss 2.00 on epoch=339
06/19/2022 22:30:06 - INFO - __main__ - Step 690 Global step 690 Train loss 1.96 on epoch=344
06/19/2022 22:30:08 - INFO - __main__ - Step 700 Global step 700 Train loss 2.09 on epoch=349
06/19/2022 22:30:09 - INFO - __main__ - Global step 700 Train loss 2.02 Classification-F1 0.20289855072463767 on epoch=349
06/19/2022 22:30:10 - INFO - __main__ - Step 710 Global step 710 Train loss 1.83 on epoch=354
06/19/2022 22:30:12 - INFO - __main__ - Step 720 Global step 720 Train loss 2.05 on epoch=359
06/19/2022 22:30:13 - INFO - __main__ - Step 730 Global step 730 Train loss 1.89 on epoch=364
06/19/2022 22:30:14 - INFO - __main__ - Step 740 Global step 740 Train loss 1.83 on epoch=369
06/19/2022 22:30:15 - INFO - __main__ - Step 750 Global step 750 Train loss 1.88 on epoch=374
06/19/2022 22:30:16 - INFO - __main__ - Global step 750 Train loss 1.90 Classification-F1 0.3992490613266583 on epoch=374
06/19/2022 22:30:16 - INFO - __main__ - Saving model with best Classification-F1: 0.2566069906223359 -> 0.3992490613266583 on epoch=374, global_step=750
06/19/2022 22:30:17 - INFO - __main__ - Step 760 Global step 760 Train loss 1.82 on epoch=379
06/19/2022 22:30:18 - INFO - __main__ - Step 770 Global step 770 Train loss 1.80 on epoch=384
06/19/2022 22:30:20 - INFO - __main__ - Step 780 Global step 780 Train loss 1.89 on epoch=389
06/19/2022 22:30:21 - INFO - __main__ - Step 790 Global step 790 Train loss 1.96 on epoch=394
06/19/2022 22:30:22 - INFO - __main__ - Step 800 Global step 800 Train loss 1.79 on epoch=399
06/19/2022 22:30:23 - INFO - __main__ - Global step 800 Train loss 1.85 Classification-F1 0.5270935960591133 on epoch=399
06/19/2022 22:30:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.5270935960591133 on epoch=399, global_step=800
06/19/2022 22:30:24 - INFO - __main__ - Step 810 Global step 810 Train loss 1.59 on epoch=404
06/19/2022 22:30:25 - INFO - __main__ - Step 820 Global step 820 Train loss 1.78 on epoch=409
06/19/2022 22:30:27 - INFO - __main__ - Step 830 Global step 830 Train loss 1.63 on epoch=414
06/19/2022 22:30:28 - INFO - __main__ - Step 840 Global step 840 Train loss 1.67 on epoch=419
06/19/2022 22:30:29 - INFO - __main__ - Step 850 Global step 850 Train loss 1.56 on epoch=424
06/19/2022 22:30:30 - INFO - __main__ - Global step 850 Train loss 1.64 Classification-F1 0.4817813765182186 on epoch=424
06/19/2022 22:30:31 - INFO - __main__ - Step 860 Global step 860 Train loss 1.71 on epoch=429
06/19/2022 22:30:32 - INFO - __main__ - Step 870 Global step 870 Train loss 1.63 on epoch=434
06/19/2022 22:30:33 - INFO - __main__ - Step 880 Global step 880 Train loss 1.59 on epoch=439
06/19/2022 22:30:35 - INFO - __main__ - Step 890 Global step 890 Train loss 1.64 on epoch=444
06/19/2022 22:30:36 - INFO - __main__ - Step 900 Global step 900 Train loss 1.54 on epoch=449
06/19/2022 22:30:36 - INFO - __main__ - Global step 900 Train loss 1.62 Classification-F1 0.36374269005847953 on epoch=449
06/19/2022 22:30:38 - INFO - __main__ - Step 910 Global step 910 Train loss 1.46 on epoch=454
06/19/2022 22:30:39 - INFO - __main__ - Step 920 Global step 920 Train loss 1.49 on epoch=459
06/19/2022 22:30:40 - INFO - __main__ - Step 930 Global step 930 Train loss 1.59 on epoch=464
06/19/2022 22:30:41 - INFO - __main__ - Step 940 Global step 940 Train loss 1.59 on epoch=469
06/19/2022 22:30:43 - INFO - __main__ - Step 950 Global step 950 Train loss 1.49 on epoch=474
06/19/2022 22:30:43 - INFO - __main__ - Global step 950 Train loss 1.52 Classification-F1 0.3191489361702127 on epoch=474
06/19/2022 22:30:44 - INFO - __main__ - Step 960 Global step 960 Train loss 1.57 on epoch=479
06/19/2022 22:30:46 - INFO - __main__ - Step 970 Global step 970 Train loss 1.42 on epoch=484
06/19/2022 22:30:47 - INFO - __main__ - Step 980 Global step 980 Train loss 1.48 on epoch=489
06/19/2022 22:30:48 - INFO - __main__ - Step 990 Global step 990 Train loss 1.35 on epoch=494
06/19/2022 22:30:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.55 on epoch=499
06/19/2022 22:30:50 - INFO - __main__ - Global step 1000 Train loss 1.48 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 22:30:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.34 on epoch=504
06/19/2022 22:30:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.42 on epoch=509
06/19/2022 22:30:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.38 on epoch=514
06/19/2022 22:30:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.32 on epoch=519
06/19/2022 22:30:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.26 on epoch=524
06/19/2022 22:30:57 - INFO - __main__ - Global step 1050 Train loss 1.34 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 22:30:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.37 on epoch=529
06/19/2022 22:30:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.27 on epoch=534
06/19/2022 22:31:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.13 on epoch=539
06/19/2022 22:31:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.28 on epoch=544
06/19/2022 22:31:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.41 on epoch=549
06/19/2022 22:31:03 - INFO - __main__ - Global step 1100 Train loss 1.29 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 22:31:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.23 on epoch=554
06/19/2022 22:31:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.20 on epoch=559
06/19/2022 22:31:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.18 on epoch=564
06/19/2022 22:31:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.16 on epoch=569
06/19/2022 22:31:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.21 on epoch=574
06/19/2022 22:31:10 - INFO - __main__ - Global step 1150 Train loss 1.20 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 22:31:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.21 on epoch=579
06/19/2022 22:31:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.23 on epoch=584
06/19/2022 22:31:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.20 on epoch=589
06/19/2022 22:31:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.16 on epoch=594
06/19/2022 22:31:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.08 on epoch=599
06/19/2022 22:31:17 - INFO - __main__ - Global step 1200 Train loss 1.18 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 22:31:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.18 on epoch=604
06/19/2022 22:31:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.09 on epoch=609
06/19/2022 22:31:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.10 on epoch=614
06/19/2022 22:31:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.16 on epoch=619
06/19/2022 22:31:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.00 on epoch=624
06/19/2022 22:31:24 - INFO - __main__ - Global step 1250 Train loss 1.11 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 22:31:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.11 on epoch=629
06/19/2022 22:31:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.13 on epoch=634
06/19/2022 22:31:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.92 on epoch=639
06/19/2022 22:31:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.07 on epoch=644
06/19/2022 22:31:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.02 on epoch=649
06/19/2022 22:31:30 - INFO - __main__ - Global step 1300 Train loss 1.05 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 22:31:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.99 on epoch=654
06/19/2022 22:31:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.99 on epoch=659
06/19/2022 22:31:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.96 on epoch=664
06/19/2022 22:31:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.92 on epoch=669
06/19/2022 22:31:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.94 on epoch=674
06/19/2022 22:31:37 - INFO - __main__ - Global step 1350 Train loss 0.96 Classification-F1 0.3333333333333333 on epoch=674
06/19/2022 22:31:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.11 on epoch=679
06/19/2022 22:31:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.91 on epoch=684
06/19/2022 22:31:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.96 on epoch=689
06/19/2022 22:31:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.92 on epoch=694
06/19/2022 22:31:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.02 on epoch=699
06/19/2022 22:31:44 - INFO - __main__ - Global step 1400 Train loss 0.98 Classification-F1 0.4231177094379639 on epoch=699
06/19/2022 22:31:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.98 on epoch=704
06/19/2022 22:31:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.96 on epoch=709
06/19/2022 22:31:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.98 on epoch=714
06/19/2022 22:31:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.90 on epoch=719
06/19/2022 22:31:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.01 on epoch=724
06/19/2022 22:31:51 - INFO - __main__ - Global step 1450 Train loss 0.97 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 22:31:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.15 on epoch=729
06/19/2022 22:31:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.01 on epoch=734
06/19/2022 22:31:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.02 on epoch=739
06/19/2022 22:31:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.96 on epoch=744
06/19/2022 22:31:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.90 on epoch=749
06/19/2022 22:31:57 - INFO - __main__ - Global step 1500 Train loss 1.01 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 22:31:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.80 on epoch=754
06/19/2022 22:32:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.83 on epoch=759
06/19/2022 22:32:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.86 on epoch=764
06/19/2022 22:32:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.90 on epoch=769
06/19/2022 22:32:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.94 on epoch=774
06/19/2022 22:32:04 - INFO - __main__ - Global step 1550 Train loss 0.87 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 22:32:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.89 on epoch=779
06/19/2022 22:32:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.85 on epoch=784
06/19/2022 22:32:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.81 on epoch=789
06/19/2022 22:32:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.91 on epoch=794
06/19/2022 22:32:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.87 on epoch=799
06/19/2022 22:32:11 - INFO - __main__ - Global step 1600 Train loss 0.87 Classification-F1 0.3191489361702127 on epoch=799
06/19/2022 22:32:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.79 on epoch=804
06/19/2022 22:32:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.91 on epoch=809
06/19/2022 22:32:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.88 on epoch=814
06/19/2022 22:32:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.86 on epoch=819
06/19/2022 22:32:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.71 on epoch=824
06/19/2022 22:32:17 - INFO - __main__ - Global step 1650 Train loss 0.83 Classification-F1 0.3992490613266583 on epoch=824
06/19/2022 22:32:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.87 on epoch=829
06/19/2022 22:32:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.90 on epoch=834
06/19/2022 22:32:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.78 on epoch=839
06/19/2022 22:32:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.75 on epoch=844
06/19/2022 22:32:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.70 on epoch=849
06/19/2022 22:32:24 - INFO - __main__ - Global step 1700 Train loss 0.80 Classification-F1 0.3454545454545454 on epoch=849
06/19/2022 22:32:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.85 on epoch=854
06/19/2022 22:32:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.76 on epoch=859
06/19/2022 22:32:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.78 on epoch=864
06/19/2022 22:32:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.91 on epoch=869
06/19/2022 22:32:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.81 on epoch=874
06/19/2022 22:32:31 - INFO - __main__ - Global step 1750 Train loss 0.82 Classification-F1 0.4385964912280702 on epoch=874
06/19/2022 22:32:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.79 on epoch=879
06/19/2022 22:32:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.77 on epoch=884
06/19/2022 22:32:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.73 on epoch=889
06/19/2022 22:32:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.79 on epoch=894
06/19/2022 22:32:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.76 on epoch=899
06/19/2022 22:32:38 - INFO - __main__ - Global step 1800 Train loss 0.77 Classification-F1 0.36374269005847953 on epoch=899
06/19/2022 22:32:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.80 on epoch=904
06/19/2022 22:32:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.76 on epoch=909
06/19/2022 22:32:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.72 on epoch=914
06/19/2022 22:32:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.83 on epoch=919
06/19/2022 22:32:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.78 on epoch=924
06/19/2022 22:32:44 - INFO - __main__ - Global step 1850 Train loss 0.78 Classification-F1 0.3992490613266583 on epoch=924
06/19/2022 22:32:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.77 on epoch=929
06/19/2022 22:32:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.78 on epoch=934
06/19/2022 22:32:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.82 on epoch=939
06/19/2022 22:32:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.68 on epoch=944
06/19/2022 22:32:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.88 on epoch=949
06/19/2022 22:32:51 - INFO - __main__ - Global step 1900 Train loss 0.79 Classification-F1 0.4385964912280702 on epoch=949
06/19/2022 22:32:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.78 on epoch=954
06/19/2022 22:32:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.70 on epoch=959
06/19/2022 22:32:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.80 on epoch=964
06/19/2022 22:32:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.71 on epoch=969
06/19/2022 22:32:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.71 on epoch=974
06/19/2022 22:32:58 - INFO - __main__ - Global step 1950 Train loss 0.74 Classification-F1 0.7408906882591093 on epoch=974
06/19/2022 22:32:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5270935960591133 -> 0.7408906882591093 on epoch=974, global_step=1950
06/19/2022 22:32:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.80 on epoch=979
06/19/2022 22:33:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.68 on epoch=984
06/19/2022 22:33:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.74 on epoch=989
06/19/2022 22:33:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.78 on epoch=994
06/19/2022 22:33:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.69 on epoch=999
06/19/2022 22:33:05 - INFO - __main__ - Global step 2000 Train loss 0.74 Classification-F1 0.5625 on epoch=999
06/19/2022 22:33:05 - INFO - __main__ - save last model!
06/19/2022 22:33:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 22:33:05 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 22:33:05 - INFO - __main__ - Printing 3 examples
06/19/2022 22:33:05 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 22:33:05 - INFO - __main__ - ['0']
06/19/2022 22:33:05 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 22:33:05 - INFO - __main__ - ['1']
06/19/2022 22:33:05 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 22:33:05 - INFO - __main__ - ['1']
06/19/2022 22:33:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:33:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:33:05 - INFO - __main__ - Printing 3 examples
06/19/2022 22:33:05 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 22:33:05 - INFO - __main__ - ['1']
06/19/2022 22:33:05 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 22:33:05 - INFO - __main__ - ['1']
06/19/2022 22:33:05 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 22:33:05 - INFO - __main__ - ['1']
06/19/2022 22:33:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:33:05 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:33:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:33:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:33:05 - INFO - __main__ - Printing 3 examples
06/19/2022 22:33:05 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 22:33:05 - INFO - __main__ - ['1']
06/19/2022 22:33:05 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 22:33:05 - INFO - __main__ - ['1']
06/19/2022 22:33:05 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 22:33:05 - INFO - __main__ - ['1']
06/19/2022 22:33:05 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:33:05 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:33:05 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:33:09 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:33:11 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:33:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:33:11 - INFO - __main__ - Starting training!
06/19/2022 22:33:16 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 22:34:45 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_21_0.2_8_predictions.txt
06/19/2022 22:34:45 - INFO - __main__ - Classification-F1 on test data: 0.5086
06/19/2022 22:34:45 - INFO - __main__ - prefix=paws_16_21, lr=0.2, bsz=8, dev_performance=0.7408906882591093, test_performance=0.5085518770756945
06/19/2022 22:34:45 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.5, bsz=8 ...
06/19/2022 22:34:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:34:46 - INFO - __main__ - Printing 3 examples
06/19/2022 22:34:46 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 22:34:46 - INFO - __main__ - ['1']
06/19/2022 22:34:46 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 22:34:46 - INFO - __main__ - ['1']
06/19/2022 22:34:46 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 22:34:46 - INFO - __main__ - ['1']
06/19/2022 22:34:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:34:46 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:34:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:34:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:34:46 - INFO - __main__ - Printing 3 examples
06/19/2022 22:34:46 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 22:34:46 - INFO - __main__ - ['1']
06/19/2022 22:34:46 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 22:34:46 - INFO - __main__ - ['1']
06/19/2022 22:34:46 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 22:34:46 - INFO - __main__ - ['1']
06/19/2022 22:34:46 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:34:46 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:34:46 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:34:52 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:34:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:34:52 - INFO - __main__ - Starting training!
06/19/2022 22:34:54 - INFO - __main__ - Step 10 Global step 10 Train loss 5.93 on epoch=4
06/19/2022 22:34:55 - INFO - __main__ - Step 20 Global step 20 Train loss 5.43 on epoch=9
06/19/2022 22:34:56 - INFO - __main__ - Step 30 Global step 30 Train loss 4.52 on epoch=14
06/19/2022 22:34:57 - INFO - __main__ - Step 40 Global step 40 Train loss 4.25 on epoch=19
06/19/2022 22:34:59 - INFO - __main__ - Step 50 Global step 50 Train loss 3.98 on epoch=24
06/19/2022 22:35:01 - INFO - __main__ - Global step 50 Train loss 4.82 Classification-F1 0.0 on epoch=24
06/19/2022 22:35:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 22:35:02 - INFO - __main__ - Step 60 Global step 60 Train loss 3.68 on epoch=29
06/19/2022 22:35:04 - INFO - __main__ - Step 70 Global step 70 Train loss 3.46 on epoch=34
06/19/2022 22:35:05 - INFO - __main__ - Step 80 Global step 80 Train loss 3.45 on epoch=39
06/19/2022 22:35:06 - INFO - __main__ - Step 90 Global step 90 Train loss 3.37 on epoch=44
06/19/2022 22:35:07 - INFO - __main__ - Step 100 Global step 100 Train loss 3.25 on epoch=49
06/19/2022 22:35:18 - INFO - __main__ - Global step 100 Train loss 3.44 Classification-F1 0.009523809523809525 on epoch=49
06/19/2022 22:35:18 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.009523809523809525 on epoch=49, global_step=100
06/19/2022 22:35:20 - INFO - __main__ - Step 110 Global step 110 Train loss 3.17 on epoch=54
06/19/2022 22:35:21 - INFO - __main__ - Step 120 Global step 120 Train loss 3.06 on epoch=59
06/19/2022 22:35:22 - INFO - __main__ - Step 130 Global step 130 Train loss 2.81 on epoch=64
06/19/2022 22:35:24 - INFO - __main__ - Step 140 Global step 140 Train loss 2.89 on epoch=69
06/19/2022 22:35:25 - INFO - __main__ - Step 150 Global step 150 Train loss 2.69 on epoch=74
06/19/2022 22:35:26 - INFO - __main__ - Global step 150 Train loss 2.92 Classification-F1 0.22695035460992907 on epoch=74
06/19/2022 22:35:26 - INFO - __main__ - Saving model with best Classification-F1: 0.009523809523809525 -> 0.22695035460992907 on epoch=74, global_step=150
06/19/2022 22:35:27 - INFO - __main__ - Step 160 Global step 160 Train loss 2.63 on epoch=79
06/19/2022 22:35:28 - INFO - __main__ - Step 170 Global step 170 Train loss 2.67 on epoch=84
06/19/2022 22:35:30 - INFO - __main__ - Step 180 Global step 180 Train loss 2.50 on epoch=89
06/19/2022 22:35:31 - INFO - __main__ - Step 190 Global step 190 Train loss 2.46 on epoch=94
06/19/2022 22:35:32 - INFO - __main__ - Step 200 Global step 200 Train loss 2.27 on epoch=99
06/19/2022 22:35:32 - INFO - __main__ - Global step 200 Train loss 2.51 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 22:35:33 - INFO - __main__ - Saving model with best Classification-F1: 0.22695035460992907 -> 0.3333333333333333 on epoch=99, global_step=200
06/19/2022 22:35:34 - INFO - __main__ - Step 210 Global step 210 Train loss 2.31 on epoch=104
06/19/2022 22:35:35 - INFO - __main__ - Step 220 Global step 220 Train loss 2.25 on epoch=109
06/19/2022 22:35:36 - INFO - __main__ - Step 230 Global step 230 Train loss 2.10 on epoch=114
06/19/2022 22:35:38 - INFO - __main__ - Step 240 Global step 240 Train loss 2.19 on epoch=119
06/19/2022 22:35:39 - INFO - __main__ - Step 250 Global step 250 Train loss 2.08 on epoch=124
06/19/2022 22:35:39 - INFO - __main__ - Global step 250 Train loss 2.19 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 22:35:41 - INFO - __main__ - Step 260 Global step 260 Train loss 2.05 on epoch=129
06/19/2022 22:35:42 - INFO - __main__ - Step 270 Global step 270 Train loss 2.04 on epoch=134
06/19/2022 22:35:43 - INFO - __main__ - Step 280 Global step 280 Train loss 1.91 on epoch=139
06/19/2022 22:35:44 - INFO - __main__ - Step 290 Global step 290 Train loss 1.84 on epoch=144
06/19/2022 22:35:46 - INFO - __main__ - Step 300 Global step 300 Train loss 1.94 on epoch=149
06/19/2022 22:35:46 - INFO - __main__ - Global step 300 Train loss 1.96 Classification-F1 0.4589371980676329 on epoch=149
06/19/2022 22:35:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4589371980676329 on epoch=149, global_step=300
06/19/2022 22:35:47 - INFO - __main__ - Step 310 Global step 310 Train loss 1.70 on epoch=154
06/19/2022 22:35:49 - INFO - __main__ - Step 320 Global step 320 Train loss 1.67 on epoch=159
06/19/2022 22:35:50 - INFO - __main__ - Step 330 Global step 330 Train loss 1.65 on epoch=164
06/19/2022 22:35:51 - INFO - __main__ - Step 340 Global step 340 Train loss 1.66 on epoch=169
06/19/2022 22:35:53 - INFO - __main__ - Step 350 Global step 350 Train loss 1.54 on epoch=174
06/19/2022 22:35:53 - INFO - __main__ - Global step 350 Train loss 1.64 Classification-F1 0.4589371980676329 on epoch=174
06/19/2022 22:35:54 - INFO - __main__ - Step 360 Global step 360 Train loss 1.54 on epoch=179
06/19/2022 22:35:56 - INFO - __main__ - Step 370 Global step 370 Train loss 1.46 on epoch=184
06/19/2022 22:35:57 - INFO - __main__ - Step 380 Global step 380 Train loss 1.42 on epoch=189
06/19/2022 22:35:58 - INFO - __main__ - Step 390 Global step 390 Train loss 1.50 on epoch=194
06/19/2022 22:35:59 - INFO - __main__ - Step 400 Global step 400 Train loss 1.42 on epoch=199
06/19/2022 22:36:00 - INFO - __main__ - Global step 400 Train loss 1.47 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 22:36:01 - INFO - __main__ - Step 410 Global step 410 Train loss 1.36 on epoch=204
06/19/2022 22:36:02 - INFO - __main__ - Step 420 Global step 420 Train loss 1.39 on epoch=209
06/19/2022 22:36:04 - INFO - __main__ - Step 430 Global step 430 Train loss 1.22 on epoch=214
06/19/2022 22:36:05 - INFO - __main__ - Step 440 Global step 440 Train loss 1.33 on epoch=219
06/19/2022 22:36:06 - INFO - __main__ - Step 450 Global step 450 Train loss 1.32 on epoch=224
06/19/2022 22:36:06 - INFO - __main__ - Global step 450 Train loss 1.32 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 22:36:08 - INFO - __main__ - Step 460 Global step 460 Train loss 1.32 on epoch=229
06/19/2022 22:36:09 - INFO - __main__ - Step 470 Global step 470 Train loss 1.13 on epoch=234
06/19/2022 22:36:10 - INFO - __main__ - Step 480 Global step 480 Train loss 1.13 on epoch=239
06/19/2022 22:36:11 - INFO - __main__ - Step 490 Global step 490 Train loss 1.20 on epoch=244
06/19/2022 22:36:13 - INFO - __main__ - Step 500 Global step 500 Train loss 1.17 on epoch=249
06/19/2022 22:36:13 - INFO - __main__ - Global step 500 Train loss 1.19 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 22:36:14 - INFO - __main__ - Step 510 Global step 510 Train loss 1.20 on epoch=254
06/19/2022 22:36:16 - INFO - __main__ - Step 520 Global step 520 Train loss 1.10 on epoch=259
06/19/2022 22:36:17 - INFO - __main__ - Step 530 Global step 530 Train loss 1.04 on epoch=264
06/19/2022 22:36:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.97 on epoch=269
06/19/2022 22:36:19 - INFO - __main__ - Step 550 Global step 550 Train loss 1.08 on epoch=274
06/19/2022 22:36:20 - INFO - __main__ - Global step 550 Train loss 1.08 Classification-F1 0.5636363636363637 on epoch=274
06/19/2022 22:36:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5636363636363637 on epoch=274, global_step=550
06/19/2022 22:36:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.95 on epoch=279
06/19/2022 22:36:22 - INFO - __main__ - Step 570 Global step 570 Train loss 1.00 on epoch=284
06/19/2022 22:36:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.90 on epoch=289
06/19/2022 22:36:25 - INFO - __main__ - Step 590 Global step 590 Train loss 1.02 on epoch=294
06/19/2022 22:36:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.98 on epoch=299
06/19/2022 22:36:27 - INFO - __main__ - Global step 600 Train loss 0.97 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 22:36:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.89 on epoch=304
06/19/2022 22:36:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.85 on epoch=309
06/19/2022 22:36:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.97 on epoch=314
06/19/2022 22:36:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.84 on epoch=319
06/19/2022 22:36:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.85 on epoch=324
06/19/2022 22:36:33 - INFO - __main__ - Global step 650 Train loss 0.88 Classification-F1 0.3043478260869565 on epoch=324
06/19/2022 22:36:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.91 on epoch=329
06/19/2022 22:36:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.90 on epoch=334
06/19/2022 22:36:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.82 on epoch=339
06/19/2022 22:36:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.87 on epoch=344
06/19/2022 22:36:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.74 on epoch=349
06/19/2022 22:36:40 - INFO - __main__ - Global step 700 Train loss 0.84 Classification-F1 0.36374269005847953 on epoch=349
06/19/2022 22:36:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.80 on epoch=354
06/19/2022 22:36:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.82 on epoch=359
06/19/2022 22:36:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.70 on epoch=364
06/19/2022 22:36:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.86 on epoch=369
06/19/2022 22:36:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.75 on epoch=374
06/19/2022 22:36:47 - INFO - __main__ - Global step 750 Train loss 0.79 Classification-F1 0.3816425120772947 on epoch=374
06/19/2022 22:36:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.75 on epoch=379
06/19/2022 22:36:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.71 on epoch=384
06/19/2022 22:36:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.79 on epoch=389
06/19/2022 22:36:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.80 on epoch=394
06/19/2022 22:36:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.75 on epoch=399
06/19/2022 22:36:53 - INFO - __main__ - Global step 800 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 22:36:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.75 on epoch=404
06/19/2022 22:36:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.77 on epoch=409
06/19/2022 22:36:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.65 on epoch=414
06/19/2022 22:36:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.70 on epoch=419
06/19/2022 22:37:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.71 on epoch=424
06/19/2022 22:37:00 - INFO - __main__ - Global step 850 Train loss 0.72 Classification-F1 0.3992490613266583 on epoch=424
06/19/2022 22:37:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.70 on epoch=429
06/19/2022 22:37:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.70 on epoch=434
06/19/2022 22:37:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.66 on epoch=439
06/19/2022 22:37:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.77 on epoch=444
06/19/2022 22:37:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.66 on epoch=449
06/19/2022 22:37:07 - INFO - __main__ - Global step 900 Train loss 0.70 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 22:37:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.71 on epoch=454
06/19/2022 22:37:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.70 on epoch=459
06/19/2022 22:37:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.77 on epoch=464
06/19/2022 22:37:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.75 on epoch=469
06/19/2022 22:37:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.68 on epoch=474
06/19/2022 22:37:14 - INFO - __main__ - Global step 950 Train loss 0.72 Classification-F1 0.28888888888888886 on epoch=474
06/19/2022 22:37:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.63 on epoch=479
06/19/2022 22:37:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.70 on epoch=484
06/19/2022 22:37:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.70 on epoch=489
06/19/2022 22:37:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.63 on epoch=494
06/19/2022 22:37:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.70 on epoch=499
06/19/2022 22:37:22 - INFO - __main__ - Global step 1000 Train loss 0.67 Classification-F1 0.3191489361702127 on epoch=499
06/19/2022 22:37:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.65 on epoch=504
06/19/2022 22:37:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.64 on epoch=509
06/19/2022 22:37:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.59 on epoch=514
06/19/2022 22:37:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.66 on epoch=519
06/19/2022 22:37:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.53 on epoch=524
06/19/2022 22:37:29 - INFO - __main__ - Global step 1050 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 22:37:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.69 on epoch=529
06/19/2022 22:37:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.66 on epoch=534
06/19/2022 22:37:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.70 on epoch=539
06/19/2022 22:37:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.59 on epoch=544
06/19/2022 22:37:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.58 on epoch=549
06/19/2022 22:37:36 - INFO - __main__ - Global step 1100 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 22:37:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.63 on epoch=554
06/19/2022 22:37:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.65 on epoch=559
06/19/2022 22:37:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.62 on epoch=564
06/19/2022 22:37:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.71 on epoch=569
06/19/2022 22:37:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.61 on epoch=574
06/19/2022 22:37:43 - INFO - __main__ - Global step 1150 Train loss 0.64 Classification-F1 0.36374269005847953 on epoch=574
06/19/2022 22:37:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.55 on epoch=579
06/19/2022 22:37:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.56 on epoch=584
06/19/2022 22:37:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.61 on epoch=589
06/19/2022 22:37:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.53 on epoch=594
06/19/2022 22:37:49 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.65 on epoch=599
06/19/2022 22:37:49 - INFO - __main__ - Global step 1200 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 22:37:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.63 on epoch=604
06/19/2022 22:37:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.58 on epoch=609
06/19/2022 22:37:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.63 on epoch=614
06/19/2022 22:37:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.60 on epoch=619
06/19/2022 22:37:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.57 on epoch=624
06/19/2022 22:37:56 - INFO - __main__ - Global step 1250 Train loss 0.60 Classification-F1 0.4458874458874459 on epoch=624
06/19/2022 22:37:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.61 on epoch=629
06/19/2022 22:37:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.48 on epoch=634
06/19/2022 22:38:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.58 on epoch=639
06/19/2022 22:38:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.60 on epoch=644
06/19/2022 22:38:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=649
06/19/2022 22:38:03 - INFO - __main__ - Global step 1300 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 22:38:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.55 on epoch=654
06/19/2022 22:38:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.60 on epoch=659
06/19/2022 22:38:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.59 on epoch=664
06/19/2022 22:38:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.52 on epoch=669
06/19/2022 22:38:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.56 on epoch=674
06/19/2022 22:38:10 - INFO - __main__ - Global step 1350 Train loss 0.56 Classification-F1 0.5134502923976608 on epoch=674
06/19/2022 22:38:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.55 on epoch=679
06/19/2022 22:38:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.50 on epoch=684
06/19/2022 22:38:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.63 on epoch=689
06/19/2022 22:38:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.51 on epoch=694
06/19/2022 22:38:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=699
06/19/2022 22:38:16 - INFO - __main__ - Global step 1400 Train loss 0.53 Classification-F1 0.4385964912280702 on epoch=699
06/19/2022 22:38:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.55 on epoch=704
06/19/2022 22:38:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.55 on epoch=709
06/19/2022 22:38:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.57 on epoch=714
06/19/2022 22:38:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.49 on epoch=719
06/19/2022 22:38:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.51 on epoch=724
06/19/2022 22:38:23 - INFO - __main__ - Global step 1450 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 22:38:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.52 on epoch=729
06/19/2022 22:38:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.57 on epoch=734
06/19/2022 22:38:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.60 on epoch=739
06/19/2022 22:38:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.47 on epoch=744
06/19/2022 22:38:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=749
06/19/2022 22:38:30 - INFO - __main__ - Global step 1500 Train loss 0.52 Classification-F1 0.3992490613266583 on epoch=749
06/19/2022 22:38:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.57 on epoch=754
06/19/2022 22:38:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.48 on epoch=759
06/19/2022 22:38:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.46 on epoch=764
06/19/2022 22:38:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=769
06/19/2022 22:38:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.62 on epoch=774
06/19/2022 22:38:36 - INFO - __main__ - Global step 1550 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 22:38:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.56 on epoch=779
06/19/2022 22:38:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.52 on epoch=784
06/19/2022 22:38:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.51 on epoch=789
06/19/2022 22:38:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.59 on epoch=794
06/19/2022 22:38:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.51 on epoch=799
06/19/2022 22:38:43 - INFO - __main__ - Global step 1600 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 22:38:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.55 on epoch=804
06/19/2022 22:38:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.49 on epoch=809
06/19/2022 22:38:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.53 on epoch=814
06/19/2022 22:38:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.52 on epoch=819
06/19/2022 22:38:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.50 on epoch=824
06/19/2022 22:38:50 - INFO - __main__ - Global step 1650 Train loss 0.52 Classification-F1 0.4589371980676329 on epoch=824
06/19/2022 22:38:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.54 on epoch=829
06/19/2022 22:38:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=834
06/19/2022 22:38:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=839
06/19/2022 22:38:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.54 on epoch=844
06/19/2022 22:38:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.59 on epoch=849
06/19/2022 22:38:57 - INFO - __main__ - Global step 1700 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=849
06/19/2022 22:38:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.48 on epoch=854
06/19/2022 22:38:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.49 on epoch=859
06/19/2022 22:39:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.47 on epoch=864
06/19/2022 22:39:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.54 on epoch=869
06/19/2022 22:39:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.49 on epoch=874
06/19/2022 22:39:03 - INFO - __main__ - Global step 1750 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 22:39:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.50 on epoch=879
06/19/2022 22:39:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.46 on epoch=884
06/19/2022 22:39:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=889
06/19/2022 22:39:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.45 on epoch=894
06/19/2022 22:39:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.51 on epoch=899
06/19/2022 22:39:10 - INFO - __main__ - Global step 1800 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 22:39:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.49 on epoch=904
06/19/2022 22:39:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.47 on epoch=909
06/19/2022 22:39:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.47 on epoch=914
06/19/2022 22:39:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.50 on epoch=919
06/19/2022 22:39:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.51 on epoch=924
06/19/2022 22:39:17 - INFO - __main__ - Global step 1850 Train loss 0.49 Classification-F1 0.6190476190476191 on epoch=924
06/19/2022 22:39:17 - INFO - __main__ - Saving model with best Classification-F1: 0.5636363636363637 -> 0.6190476190476191 on epoch=924, global_step=1850
06/19/2022 22:39:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.60 on epoch=929
06/19/2022 22:39:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.51 on epoch=934
06/19/2022 22:39:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.50 on epoch=939
06/19/2022 22:39:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.50 on epoch=944
06/19/2022 22:39:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=949
06/19/2022 22:39:23 - INFO - __main__ - Global step 1900 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 22:39:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.49 on epoch=954
06/19/2022 22:39:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.48 on epoch=959
06/19/2022 22:39:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.49 on epoch=964
06/19/2022 22:39:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=969
06/19/2022 22:39:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.53 on epoch=974
06/19/2022 22:39:30 - INFO - __main__ - Global step 1950 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 22:39:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
06/19/2022 22:39:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.49 on epoch=984
06/19/2022 22:39:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.49 on epoch=989
06/19/2022 22:39:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.48 on epoch=994
06/19/2022 22:39:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=999
06/19/2022 22:39:37 - INFO - __main__ - Global step 2000 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 22:39:37 - INFO - __main__ - save last model!
06/19/2022 22:39:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 22:39:37 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 22:39:37 - INFO - __main__ - Printing 3 examples
06/19/2022 22:39:37 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 22:39:37 - INFO - __main__ - ['0']
06/19/2022 22:39:37 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 22:39:37 - INFO - __main__ - ['1']
06/19/2022 22:39:37 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 22:39:37 - INFO - __main__ - ['1']
06/19/2022 22:39:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:39:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:39:37 - INFO - __main__ - Printing 3 examples
06/19/2022 22:39:37 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 22:39:37 - INFO - __main__ - ['1']
06/19/2022 22:39:37 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 22:39:37 - INFO - __main__ - ['1']
06/19/2022 22:39:37 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 22:39:37 - INFO - __main__ - ['1']
06/19/2022 22:39:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:39:37 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:39:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:39:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:39:37 - INFO - __main__ - Printing 3 examples
06/19/2022 22:39:37 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 22:39:37 - INFO - __main__ - ['1']
06/19/2022 22:39:37 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 22:39:37 - INFO - __main__ - ['1']
06/19/2022 22:39:37 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 22:39:37 - INFO - __main__ - ['1']
06/19/2022 22:39:37 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:39:38 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:39:38 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:39:41 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:39:43 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:39:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:39:43 - INFO - __main__ - Starting training!
06/19/2022 22:39:49 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 22:41:14 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_42_0.5_8_predictions.txt
06/19/2022 22:41:14 - INFO - __main__ - Classification-F1 on test data: 0.3089
06/19/2022 22:41:14 - INFO - __main__ - prefix=paws_16_42, lr=0.5, bsz=8, dev_performance=0.6190476190476191, test_performance=0.3088984902782056
06/19/2022 22:41:14 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.4, bsz=8 ...
06/19/2022 22:41:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:41:15 - INFO - __main__ - Printing 3 examples
06/19/2022 22:41:15 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 22:41:15 - INFO - __main__ - ['1']
06/19/2022 22:41:15 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 22:41:15 - INFO - __main__ - ['1']
06/19/2022 22:41:15 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 22:41:15 - INFO - __main__ - ['1']
06/19/2022 22:41:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:41:15 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:41:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:41:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:41:15 - INFO - __main__ - Printing 3 examples
06/19/2022 22:41:15 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 22:41:15 - INFO - __main__ - ['1']
06/19/2022 22:41:15 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 22:41:15 - INFO - __main__ - ['1']
06/19/2022 22:41:15 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 22:41:15 - INFO - __main__ - ['1']
06/19/2022 22:41:15 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:41:15 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:41:15 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:41:21 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:41:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:41:21 - INFO - __main__ - Starting training!
06/19/2022 22:41:22 - INFO - __main__ - Step 10 Global step 10 Train loss 5.95 on epoch=4
06/19/2022 22:41:24 - INFO - __main__ - Step 20 Global step 20 Train loss 5.38 on epoch=9
06/19/2022 22:41:25 - INFO - __main__ - Step 30 Global step 30 Train loss 4.88 on epoch=14
06/19/2022 22:41:26 - INFO - __main__ - Step 40 Global step 40 Train loss 4.33 on epoch=19
06/19/2022 22:41:27 - INFO - __main__ - Step 50 Global step 50 Train loss 4.01 on epoch=24
06/19/2022 22:41:33 - INFO - __main__ - Global step 50 Train loss 4.91 Classification-F1 0.0 on epoch=24
06/19/2022 22:41:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 22:41:34 - INFO - __main__ - Step 60 Global step 60 Train loss 3.86 on epoch=29
06/19/2022 22:41:36 - INFO - __main__ - Step 70 Global step 70 Train loss 3.80 on epoch=34
06/19/2022 22:41:37 - INFO - __main__ - Step 80 Global step 80 Train loss 3.60 on epoch=39
06/19/2022 22:41:38 - INFO - __main__ - Step 90 Global step 90 Train loss 3.48 on epoch=44
06/19/2022 22:41:39 - INFO - __main__ - Step 100 Global step 100 Train loss 3.49 on epoch=49
06/19/2022 22:41:43 - INFO - __main__ - Global step 100 Train loss 3.64 Classification-F1 0.023529411764705882 on epoch=49
06/19/2022 22:41:43 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.023529411764705882 on epoch=49, global_step=100
06/19/2022 22:41:44 - INFO - __main__ - Step 110 Global step 110 Train loss 3.26 on epoch=54
06/19/2022 22:41:45 - INFO - __main__ - Step 120 Global step 120 Train loss 3.11 on epoch=59
06/19/2022 22:41:47 - INFO - __main__ - Step 130 Global step 130 Train loss 2.96 on epoch=64
06/19/2022 22:41:48 - INFO - __main__ - Step 140 Global step 140 Train loss 3.00 on epoch=69
06/19/2022 22:41:49 - INFO - __main__ - Step 150 Global step 150 Train loss 2.85 on epoch=74
06/19/2022 22:41:51 - INFO - __main__ - Global step 150 Train loss 3.04 Classification-F1 0.14222222222222222 on epoch=74
06/19/2022 22:41:51 - INFO - __main__ - Saving model with best Classification-F1: 0.023529411764705882 -> 0.14222222222222222 on epoch=74, global_step=150
06/19/2022 22:41:52 - INFO - __main__ - Step 160 Global step 160 Train loss 2.81 on epoch=79
06/19/2022 22:41:53 - INFO - __main__ - Step 170 Global step 170 Train loss 2.78 on epoch=84
06/19/2022 22:41:54 - INFO - __main__ - Step 180 Global step 180 Train loss 2.59 on epoch=89
06/19/2022 22:41:56 - INFO - __main__ - Step 190 Global step 190 Train loss 2.54 on epoch=94
06/19/2022 22:41:57 - INFO - __main__ - Step 200 Global step 200 Train loss 2.55 on epoch=99
06/19/2022 22:41:59 - INFO - __main__ - Global step 200 Train loss 2.65 Classification-F1 0.19607843137254904 on epoch=99
06/19/2022 22:41:59 - INFO - __main__ - Saving model with best Classification-F1: 0.14222222222222222 -> 0.19607843137254904 on epoch=99, global_step=200
06/19/2022 22:42:00 - INFO - __main__ - Step 210 Global step 210 Train loss 2.40 on epoch=104
06/19/2022 22:42:02 - INFO - __main__ - Step 220 Global step 220 Train loss 2.35 on epoch=109
06/19/2022 22:42:03 - INFO - __main__ - Step 230 Global step 230 Train loss 2.33 on epoch=114
06/19/2022 22:42:04 - INFO - __main__ - Step 240 Global step 240 Train loss 2.21 on epoch=119
06/19/2022 22:42:05 - INFO - __main__ - Step 250 Global step 250 Train loss 2.19 on epoch=124
06/19/2022 22:42:06 - INFO - __main__ - Global step 250 Train loss 2.30 Classification-F1 0.4589371980676329 on epoch=124
06/19/2022 22:42:06 - INFO - __main__ - Saving model with best Classification-F1: 0.19607843137254904 -> 0.4589371980676329 on epoch=124, global_step=250
06/19/2022 22:42:07 - INFO - __main__ - Step 260 Global step 260 Train loss 2.05 on epoch=129
06/19/2022 22:42:09 - INFO - __main__ - Step 270 Global step 270 Train loss 2.04 on epoch=134
06/19/2022 22:42:10 - INFO - __main__ - Step 280 Global step 280 Train loss 1.99 on epoch=139
06/19/2022 22:42:11 - INFO - __main__ - Step 290 Global step 290 Train loss 1.89 on epoch=144
06/19/2022 22:42:12 - INFO - __main__ - Step 300 Global step 300 Train loss 1.76 on epoch=149
06/19/2022 22:42:13 - INFO - __main__ - Global step 300 Train loss 1.94 Classification-F1 0.4920634920634921 on epoch=149
06/19/2022 22:42:13 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.4920634920634921 on epoch=149, global_step=300
06/19/2022 22:42:14 - INFO - __main__ - Step 310 Global step 310 Train loss 1.74 on epoch=154
06/19/2022 22:42:15 - INFO - __main__ - Step 320 Global step 320 Train loss 1.75 on epoch=159
06/19/2022 22:42:16 - INFO - __main__ - Step 330 Global step 330 Train loss 1.75 on epoch=164
06/19/2022 22:42:18 - INFO - __main__ - Step 340 Global step 340 Train loss 1.74 on epoch=169
06/19/2022 22:42:19 - INFO - __main__ - Step 350 Global step 350 Train loss 1.58 on epoch=174
06/19/2022 22:42:19 - INFO - __main__ - Global step 350 Train loss 1.71 Classification-F1 0.4817813765182186 on epoch=174
06/19/2022 22:42:21 - INFO - __main__ - Step 360 Global step 360 Train loss 1.72 on epoch=179
06/19/2022 22:42:22 - INFO - __main__ - Step 370 Global step 370 Train loss 1.46 on epoch=184
06/19/2022 22:42:23 - INFO - __main__ - Step 380 Global step 380 Train loss 1.44 on epoch=189
06/19/2022 22:42:24 - INFO - __main__ - Step 390 Global step 390 Train loss 1.39 on epoch=194
06/19/2022 22:42:26 - INFO - __main__ - Step 400 Global step 400 Train loss 1.44 on epoch=199
06/19/2022 22:42:26 - INFO - __main__ - Global step 400 Train loss 1.49 Classification-F1 0.5151515151515151 on epoch=199
06/19/2022 22:42:26 - INFO - __main__ - Saving model with best Classification-F1: 0.4920634920634921 -> 0.5151515151515151 on epoch=199, global_step=400
06/19/2022 22:42:27 - INFO - __main__ - Step 410 Global step 410 Train loss 1.30 on epoch=204
06/19/2022 22:42:28 - INFO - __main__ - Step 420 Global step 420 Train loss 1.44 on epoch=209
06/19/2022 22:42:30 - INFO - __main__ - Step 430 Global step 430 Train loss 1.36 on epoch=214
06/19/2022 22:42:31 - INFO - __main__ - Step 440 Global step 440 Train loss 1.42 on epoch=219
06/19/2022 22:42:32 - INFO - __main__ - Step 450 Global step 450 Train loss 1.34 on epoch=224
06/19/2022 22:42:33 - INFO - __main__ - Global step 450 Train loss 1.37 Classification-F1 0.49090909090909085 on epoch=224
06/19/2022 22:42:34 - INFO - __main__ - Step 460 Global step 460 Train loss 1.26 on epoch=229
06/19/2022 22:42:35 - INFO - __main__ - Step 470 Global step 470 Train loss 1.30 on epoch=234
06/19/2022 22:42:36 - INFO - __main__ - Step 480 Global step 480 Train loss 1.29 on epoch=239
06/19/2022 22:42:37 - INFO - __main__ - Step 490 Global step 490 Train loss 1.22 on epoch=244
06/19/2022 22:42:39 - INFO - __main__ - Step 500 Global step 500 Train loss 1.15 on epoch=249
06/19/2022 22:42:39 - INFO - __main__ - Global step 500 Train loss 1.25 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 22:42:40 - INFO - __main__ - Step 510 Global step 510 Train loss 1.10 on epoch=254
06/19/2022 22:42:41 - INFO - __main__ - Step 520 Global step 520 Train loss 1.03 on epoch=259
06/19/2022 22:42:43 - INFO - __main__ - Step 530 Global step 530 Train loss 1.08 on epoch=264
06/19/2022 22:42:44 - INFO - __main__ - Step 540 Global step 540 Train loss 1.02 on epoch=269
06/19/2022 22:42:45 - INFO - __main__ - Step 550 Global step 550 Train loss 1.04 on epoch=274
06/19/2022 22:42:46 - INFO - __main__ - Global step 550 Train loss 1.05 Classification-F1 0.4589371980676329 on epoch=274
06/19/2022 22:42:47 - INFO - __main__ - Step 560 Global step 560 Train loss 1.13 on epoch=279
06/19/2022 22:42:48 - INFO - __main__ - Step 570 Global step 570 Train loss 1.04 on epoch=284
06/19/2022 22:42:49 - INFO - __main__ - Step 580 Global step 580 Train loss 1.01 on epoch=289
06/19/2022 22:42:50 - INFO - __main__ - Step 590 Global step 590 Train loss 1.04 on epoch=294
06/19/2022 22:42:52 - INFO - __main__ - Step 600 Global step 600 Train loss 1.04 on epoch=299
06/19/2022 22:42:52 - INFO - __main__ - Global step 600 Train loss 1.05 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 22:42:53 - INFO - __main__ - Step 610 Global step 610 Train loss 1.06 on epoch=304
06/19/2022 22:42:55 - INFO - __main__ - Step 620 Global step 620 Train loss 1.03 on epoch=309
06/19/2022 22:42:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.92 on epoch=314
06/19/2022 22:42:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.93 on epoch=319
06/19/2022 22:42:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.89 on epoch=324
06/19/2022 22:42:59 - INFO - __main__ - Global step 650 Train loss 0.97 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 22:43:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.96 on epoch=329
06/19/2022 22:43:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.88 on epoch=334
06/19/2022 22:43:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.94 on epoch=339
06/19/2022 22:43:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.87 on epoch=344
06/19/2022 22:43:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.98 on epoch=349
06/19/2022 22:43:05 - INFO - __main__ - Global step 700 Train loss 0.93 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 22:43:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.89 on epoch=354
06/19/2022 22:43:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.97 on epoch=359
06/19/2022 22:43:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.87 on epoch=364
06/19/2022 22:43:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.85 on epoch=369
06/19/2022 22:43:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.82 on epoch=374
06/19/2022 22:43:12 - INFO - __main__ - Global step 750 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 22:43:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.81 on epoch=379
06/19/2022 22:43:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.80 on epoch=384
06/19/2022 22:43:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.83 on epoch=389
06/19/2022 22:43:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.90 on epoch=394
06/19/2022 22:43:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.90 on epoch=399
06/19/2022 22:43:18 - INFO - __main__ - Global step 800 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 22:43:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.80 on epoch=404
06/19/2022 22:43:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.81 on epoch=409
06/19/2022 22:43:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.74 on epoch=414
06/19/2022 22:43:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.77 on epoch=419
06/19/2022 22:43:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.80 on epoch=424
06/19/2022 22:43:25 - INFO - __main__ - Global step 850 Train loss 0.78 Classification-F1 0.4589371980676329 on epoch=424
06/19/2022 22:43:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.83 on epoch=429
06/19/2022 22:43:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.78 on epoch=434
06/19/2022 22:43:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.78 on epoch=439
06/19/2022 22:43:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.77 on epoch=444
06/19/2022 22:43:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.69 on epoch=449
06/19/2022 22:43:31 - INFO - __main__ - Global step 900 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 22:43:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.84 on epoch=454
06/19/2022 22:43:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.75 on epoch=459
06/19/2022 22:43:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.86 on epoch=464
06/19/2022 22:43:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.77 on epoch=469
06/19/2022 22:43:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.72 on epoch=474
06/19/2022 22:43:38 - INFO - __main__ - Global step 950 Train loss 0.79 Classification-F1 0.4589371980676329 on epoch=474
06/19/2022 22:43:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.70 on epoch=479
06/19/2022 22:43:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.76 on epoch=484
06/19/2022 22:43:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.75 on epoch=489
06/19/2022 22:43:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.71 on epoch=494
06/19/2022 22:43:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.70 on epoch=499
06/19/2022 22:43:44 - INFO - __main__ - Global step 1000 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 22:43:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.67 on epoch=504
06/19/2022 22:43:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.78 on epoch=509
06/19/2022 22:43:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.72 on epoch=514
06/19/2022 22:43:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.67 on epoch=519
06/19/2022 22:43:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.64 on epoch=524
06/19/2022 22:43:51 - INFO - __main__ - Global step 1050 Train loss 0.70 Classification-F1 0.3992490613266583 on epoch=524
06/19/2022 22:43:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.68 on epoch=529
06/19/2022 22:43:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.78 on epoch=534
06/19/2022 22:43:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.70 on epoch=539
06/19/2022 22:43:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.64 on epoch=544
06/19/2022 22:43:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.67 on epoch=549
06/19/2022 22:43:57 - INFO - __main__ - Global step 1100 Train loss 0.70 Classification-F1 0.539313399778516 on epoch=549
06/19/2022 22:43:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.539313399778516 on epoch=549, global_step=1100
06/19/2022 22:43:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.70 on epoch=554
06/19/2022 22:44:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.64 on epoch=559
06/19/2022 22:44:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.72 on epoch=564
06/19/2022 22:44:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.68 on epoch=569
06/19/2022 22:44:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.71 on epoch=574
06/19/2022 22:44:04 - INFO - __main__ - Global step 1150 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 22:44:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.69 on epoch=579
06/19/2022 22:44:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.68 on epoch=584
06/19/2022 22:44:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.73 on epoch=589
06/19/2022 22:44:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.64 on epoch=594
06/19/2022 22:44:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.57 on epoch=599
06/19/2022 22:44:10 - INFO - __main__ - Global step 1200 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 22:44:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.64 on epoch=604
06/19/2022 22:44:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.67 on epoch=609
06/19/2022 22:44:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.64 on epoch=614
06/19/2022 22:44:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.68 on epoch=619
06/19/2022 22:44:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.61 on epoch=624
06/19/2022 22:44:17 - INFO - __main__ - Global step 1250 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 22:44:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.63 on epoch=629
06/19/2022 22:44:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.63 on epoch=634
06/19/2022 22:44:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.65 on epoch=639
06/19/2022 22:44:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.71 on epoch=644
06/19/2022 22:44:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.56 on epoch=649
06/19/2022 22:44:23 - INFO - __main__ - Global step 1300 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 22:44:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.69 on epoch=654
06/19/2022 22:44:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.64 on epoch=659
06/19/2022 22:44:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.60 on epoch=664
06/19/2022 22:44:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.66 on epoch=669
06/19/2022 22:44:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.64 on epoch=674
06/19/2022 22:44:30 - INFO - __main__ - Global step 1350 Train loss 0.65 Classification-F1 0.3454545454545454 on epoch=674
06/19/2022 22:44:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.58 on epoch=679
06/19/2022 22:44:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.60 on epoch=684
06/19/2022 22:44:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.58 on epoch=689
06/19/2022 22:44:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.63 on epoch=694
06/19/2022 22:44:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.64 on epoch=699
06/19/2022 22:44:36 - INFO - __main__ - Global step 1400 Train loss 0.61 Classification-F1 0.5076923076923077 on epoch=699
06/19/2022 22:44:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.60 on epoch=704
06/19/2022 22:44:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.52 on epoch=709
06/19/2022 22:44:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.50 on epoch=714
06/19/2022 22:44:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.56 on epoch=719
06/19/2022 22:44:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.65 on epoch=724
06/19/2022 22:44:43 - INFO - __main__ - Global step 1450 Train loss 0.57 Classification-F1 0.6559139784946237 on epoch=724
06/19/2022 22:44:43 - INFO - __main__ - Saving model with best Classification-F1: 0.539313399778516 -> 0.6559139784946237 on epoch=724, global_step=1450
06/19/2022 22:44:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.63 on epoch=729
06/19/2022 22:44:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.64 on epoch=734
06/19/2022 22:44:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.70 on epoch=739
06/19/2022 22:44:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.57 on epoch=744
06/19/2022 22:44:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.60 on epoch=749
06/19/2022 22:44:50 - INFO - __main__ - Global step 1500 Train loss 0.63 Classification-F1 0.5588547189819725 on epoch=749
06/19/2022 22:44:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.51 on epoch=754
06/19/2022 22:44:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.51 on epoch=759
06/19/2022 22:44:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.56 on epoch=764
06/19/2022 22:44:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.54 on epoch=769
06/19/2022 22:44:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.59 on epoch=774
06/19/2022 22:44:56 - INFO - __main__ - Global step 1550 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 22:44:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.52 on epoch=779
06/19/2022 22:44:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.63 on epoch=784
06/19/2022 22:45:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.58 on epoch=789
06/19/2022 22:45:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.59 on epoch=794
06/19/2022 22:45:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.64 on epoch=799
06/19/2022 22:45:03 - INFO - __main__ - Global step 1600 Train loss 0.59 Classification-F1 0.5151515151515151 on epoch=799
06/19/2022 22:45:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.61 on epoch=804
06/19/2022 22:45:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.50 on epoch=809
06/19/2022 22:45:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.56 on epoch=814
06/19/2022 22:45:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.58 on epoch=819
06/19/2022 22:45:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.53 on epoch=824
06/19/2022 22:45:09 - INFO - __main__ - Global step 1650 Train loss 0.56 Classification-F1 0.46843853820598 on epoch=824
06/19/2022 22:45:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.47 on epoch=829
06/19/2022 22:45:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.62 on epoch=834
06/19/2022 22:45:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.50 on epoch=839
06/19/2022 22:45:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.51 on epoch=844
06/19/2022 22:45:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.52 on epoch=849
06/19/2022 22:45:16 - INFO - __main__ - Global step 1700 Train loss 0.53 Classification-F1 0.4181818181818182 on epoch=849
06/19/2022 22:45:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.56 on epoch=854
06/19/2022 22:45:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.56 on epoch=859
06/19/2022 22:45:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.52 on epoch=864
06/19/2022 22:45:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=869
06/19/2022 22:45:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.59 on epoch=874
06/19/2022 22:45:22 - INFO - __main__ - Global step 1750 Train loss 0.55 Classification-F1 0.39756367663344405 on epoch=874
06/19/2022 22:45:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.63 on epoch=879
06/19/2022 22:45:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.52 on epoch=884
06/19/2022 22:45:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.61 on epoch=889
06/19/2022 22:45:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.50 on epoch=894
06/19/2022 22:45:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.58 on epoch=899
06/19/2022 22:45:29 - INFO - __main__ - Global step 1800 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 22:45:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.46 on epoch=904
06/19/2022 22:45:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.56 on epoch=909
06/19/2022 22:45:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.49 on epoch=914
06/19/2022 22:45:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.57 on epoch=919
06/19/2022 22:45:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.47 on epoch=924
06/19/2022 22:45:35 - INFO - __main__ - Global step 1850 Train loss 0.51 Classification-F1 0.4666666666666667 on epoch=924
06/19/2022 22:45:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.51 on epoch=929
06/19/2022 22:45:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.61 on epoch=934
06/19/2022 22:45:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.50 on epoch=939
06/19/2022 22:45:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.56 on epoch=944
06/19/2022 22:45:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.52 on epoch=949
06/19/2022 22:45:42 - INFO - __main__ - Global step 1900 Train loss 0.54 Classification-F1 0.49090909090909085 on epoch=949
06/19/2022 22:45:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.56 on epoch=954
06/19/2022 22:45:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.54 on epoch=959
06/19/2022 22:45:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.54 on epoch=964
06/19/2022 22:45:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.50 on epoch=969
06/19/2022 22:45:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.44 on epoch=974
06/19/2022 22:45:48 - INFO - __main__ - Global step 1950 Train loss 0.52 Classification-F1 0.3992490613266583 on epoch=974
06/19/2022 22:45:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.58 on epoch=979
06/19/2022 22:45:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.53 on epoch=984
06/19/2022 22:45:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.50 on epoch=989
06/19/2022 22:45:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.54 on epoch=994
06/19/2022 22:45:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.52 on epoch=999
06/19/2022 22:45:55 - INFO - __main__ - Global step 2000 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 22:45:55 - INFO - __main__ - save last model!
06/19/2022 22:45:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 22:45:55 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 22:45:55 - INFO - __main__ - Printing 3 examples
06/19/2022 22:45:55 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 22:45:55 - INFO - __main__ - ['0']
06/19/2022 22:45:55 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 22:45:55 - INFO - __main__ - ['1']
06/19/2022 22:45:55 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 22:45:55 - INFO - __main__ - ['1']
06/19/2022 22:45:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:45:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:45:56 - INFO - __main__ - Printing 3 examples
06/19/2022 22:45:56 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 22:45:56 - INFO - __main__ - ['1']
06/19/2022 22:45:56 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 22:45:56 - INFO - __main__ - ['1']
06/19/2022 22:45:56 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 22:45:56 - INFO - __main__ - ['1']
06/19/2022 22:45:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:45:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:45:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:45:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:45:56 - INFO - __main__ - Printing 3 examples
06/19/2022 22:45:56 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 22:45:56 - INFO - __main__ - ['1']
06/19/2022 22:45:56 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 22:45:56 - INFO - __main__ - ['1']
06/19/2022 22:45:56 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 22:45:56 - INFO - __main__ - ['1']
06/19/2022 22:45:56 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:45:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:45:56 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:45:59 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:46:01 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:46:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:46:02 - INFO - __main__ - Starting training!
06/19/2022 22:46:07 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 22:47:32 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_42_0.4_8_predictions.txt
06/19/2022 22:47:32 - INFO - __main__ - Classification-F1 on test data: 0.3221
06/19/2022 22:47:32 - INFO - __main__ - prefix=paws_16_42, lr=0.4, bsz=8, dev_performance=0.6559139784946237, test_performance=0.3220957520643551
06/19/2022 22:47:32 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.3, bsz=8 ...
06/19/2022 22:47:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:47:33 - INFO - __main__ - Printing 3 examples
06/19/2022 22:47:33 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 22:47:33 - INFO - __main__ - ['1']
06/19/2022 22:47:33 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 22:47:33 - INFO - __main__ - ['1']
06/19/2022 22:47:33 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 22:47:33 - INFO - __main__ - ['1']
06/19/2022 22:47:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:47:33 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:47:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:47:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:47:33 - INFO - __main__ - Printing 3 examples
06/19/2022 22:47:33 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 22:47:33 - INFO - __main__ - ['1']
06/19/2022 22:47:33 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 22:47:33 - INFO - __main__ - ['1']
06/19/2022 22:47:33 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 22:47:33 - INFO - __main__ - ['1']
06/19/2022 22:47:33 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:47:33 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:47:33 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:47:38 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:47:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:47:39 - INFO - __main__ - Starting training!
06/19/2022 22:47:40 - INFO - __main__ - Step 10 Global step 10 Train loss 5.91 on epoch=4
06/19/2022 22:47:41 - INFO - __main__ - Step 20 Global step 20 Train loss 5.58 on epoch=9
06/19/2022 22:47:43 - INFO - __main__ - Step 30 Global step 30 Train loss 5.22 on epoch=14
06/19/2022 22:47:44 - INFO - __main__ - Step 40 Global step 40 Train loss 4.89 on epoch=19
06/19/2022 22:47:45 - INFO - __main__ - Step 50 Global step 50 Train loss 4.58 on epoch=24
06/19/2022 22:47:49 - INFO - __main__ - Global step 50 Train loss 5.23 Classification-F1 0.0 on epoch=24
06/19/2022 22:47:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 22:47:50 - INFO - __main__ - Step 60 Global step 60 Train loss 4.32 on epoch=29
06/19/2022 22:47:51 - INFO - __main__ - Step 70 Global step 70 Train loss 4.27 on epoch=34
06/19/2022 22:47:52 - INFO - __main__ - Step 80 Global step 80 Train loss 4.11 on epoch=39
06/19/2022 22:47:54 - INFO - __main__ - Step 90 Global step 90 Train loss 3.95 on epoch=44
06/19/2022 22:47:55 - INFO - __main__ - Step 100 Global step 100 Train loss 3.90 on epoch=49
06/19/2022 22:47:56 - INFO - __main__ - Global step 100 Train loss 4.11 Classification-F1 0.0 on epoch=49
06/19/2022 22:47:57 - INFO - __main__ - Step 110 Global step 110 Train loss 3.88 on epoch=54
06/19/2022 22:47:58 - INFO - __main__ - Step 120 Global step 120 Train loss 3.74 on epoch=59
06/19/2022 22:47:59 - INFO - __main__ - Step 130 Global step 130 Train loss 3.61 on epoch=64
06/19/2022 22:48:01 - INFO - __main__ - Step 140 Global step 140 Train loss 3.44 on epoch=69
06/19/2022 22:48:02 - INFO - __main__ - Step 150 Global step 150 Train loss 3.39 on epoch=74
06/19/2022 22:48:09 - INFO - __main__ - Global step 150 Train loss 3.62 Classification-F1 0.0 on epoch=74
06/19/2022 22:48:10 - INFO - __main__ - Step 160 Global step 160 Train loss 3.51 on epoch=79
06/19/2022 22:48:11 - INFO - __main__ - Step 170 Global step 170 Train loss 3.21 on epoch=84
06/19/2022 22:48:13 - INFO - __main__ - Step 180 Global step 180 Train loss 3.32 on epoch=89
06/19/2022 22:48:14 - INFO - __main__ - Step 190 Global step 190 Train loss 3.18 on epoch=94
06/19/2022 22:48:15 - INFO - __main__ - Step 200 Global step 200 Train loss 3.12 on epoch=99
06/19/2022 22:48:17 - INFO - __main__ - Global step 200 Train loss 3.27 Classification-F1 0.05263157894736842 on epoch=99
06/19/2022 22:48:17 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.05263157894736842 on epoch=99, global_step=200
06/19/2022 22:48:18 - INFO - __main__ - Step 210 Global step 210 Train loss 2.95 on epoch=104
06/19/2022 22:48:19 - INFO - __main__ - Step 220 Global step 220 Train loss 2.98 on epoch=109
06/19/2022 22:48:21 - INFO - __main__ - Step 230 Global step 230 Train loss 2.78 on epoch=114
06/19/2022 22:48:22 - INFO - __main__ - Step 240 Global step 240 Train loss 2.95 on epoch=119
06/19/2022 22:48:23 - INFO - __main__ - Step 250 Global step 250 Train loss 2.65 on epoch=124
06/19/2022 22:48:24 - INFO - __main__ - Global step 250 Train loss 2.86 Classification-F1 0.3333333333333333 on epoch=124
06/19/2022 22:48:24 - INFO - __main__ - Saving model with best Classification-F1: 0.05263157894736842 -> 0.3333333333333333 on epoch=124, global_step=250
06/19/2022 22:48:25 - INFO - __main__ - Step 260 Global step 260 Train loss 2.66 on epoch=129
06/19/2022 22:48:26 - INFO - __main__ - Step 270 Global step 270 Train loss 2.44 on epoch=134
06/19/2022 22:48:27 - INFO - __main__ - Step 280 Global step 280 Train loss 2.44 on epoch=139
06/19/2022 22:48:29 - INFO - __main__ - Step 290 Global step 290 Train loss 2.34 on epoch=144
06/19/2022 22:48:30 - INFO - __main__ - Step 300 Global step 300 Train loss 2.37 on epoch=149
06/19/2022 22:48:30 - INFO - __main__ - Global step 300 Train loss 2.45 Classification-F1 0.3333333333333333 on epoch=149
06/19/2022 22:48:32 - INFO - __main__ - Step 310 Global step 310 Train loss 2.39 on epoch=154
06/19/2022 22:48:33 - INFO - __main__ - Step 320 Global step 320 Train loss 2.43 on epoch=159
06/19/2022 22:48:34 - INFO - __main__ - Step 330 Global step 330 Train loss 2.27 on epoch=164
06/19/2022 22:48:36 - INFO - __main__ - Step 340 Global step 340 Train loss 2.25 on epoch=169
06/19/2022 22:48:37 - INFO - __main__ - Step 350 Global step 350 Train loss 2.21 on epoch=174
06/19/2022 22:48:37 - INFO - __main__ - Global step 350 Train loss 2.31 Classification-F1 0.3333333333333333 on epoch=174
06/19/2022 22:48:39 - INFO - __main__ - Step 360 Global step 360 Train loss 2.18 on epoch=179
06/19/2022 22:48:40 - INFO - __main__ - Step 370 Global step 370 Train loss 2.20 on epoch=184
06/19/2022 22:48:41 - INFO - __main__ - Step 380 Global step 380 Train loss 2.19 on epoch=189
06/19/2022 22:48:42 - INFO - __main__ - Step 390 Global step 390 Train loss 2.22 on epoch=194
06/19/2022 22:48:44 - INFO - __main__ - Step 400 Global step 400 Train loss 2.03 on epoch=199
06/19/2022 22:48:44 - INFO - __main__ - Global step 400 Train loss 2.16 Classification-F1 0.5933528836754642 on epoch=199
06/19/2022 22:48:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5933528836754642 on epoch=199, global_step=400
06/19/2022 22:48:46 - INFO - __main__ - Step 410 Global step 410 Train loss 2.02 on epoch=204
06/19/2022 22:48:47 - INFO - __main__ - Step 420 Global step 420 Train loss 2.05 on epoch=209
06/19/2022 22:48:48 - INFO - __main__ - Step 430 Global step 430 Train loss 2.00 on epoch=214
06/19/2022 22:48:49 - INFO - __main__ - Step 440 Global step 440 Train loss 2.12 on epoch=219
06/19/2022 22:48:51 - INFO - __main__ - Step 450 Global step 450 Train loss 1.99 on epoch=224
06/19/2022 22:48:51 - INFO - __main__ - Global step 450 Train loss 2.04 Classification-F1 0.4375 on epoch=224
06/19/2022 22:48:52 - INFO - __main__ - Step 460 Global step 460 Train loss 2.04 on epoch=229
06/19/2022 22:48:54 - INFO - __main__ - Step 470 Global step 470 Train loss 1.95 on epoch=234
06/19/2022 22:48:55 - INFO - __main__ - Step 480 Global step 480 Train loss 1.95 on epoch=239
06/19/2022 22:48:56 - INFO - __main__ - Step 490 Global step 490 Train loss 1.93 on epoch=244
06/19/2022 22:48:58 - INFO - __main__ - Step 500 Global step 500 Train loss 2.03 on epoch=249
06/19/2022 22:48:58 - INFO - __main__ - Global step 500 Train loss 1.98 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 22:48:59 - INFO - __main__ - Step 510 Global step 510 Train loss 1.92 on epoch=254
06/19/2022 22:49:00 - INFO - __main__ - Step 520 Global step 520 Train loss 1.98 on epoch=259
06/19/2022 22:49:02 - INFO - __main__ - Step 530 Global step 530 Train loss 1.90 on epoch=264
06/19/2022 22:49:03 - INFO - __main__ - Step 540 Global step 540 Train loss 1.90 on epoch=269
06/19/2022 22:49:04 - INFO - __main__ - Step 550 Global step 550 Train loss 1.87 on epoch=274
06/19/2022 22:49:05 - INFO - __main__ - Global step 550 Train loss 1.91 Classification-F1 0.75 on epoch=274
06/19/2022 22:49:05 - INFO - __main__ - Saving model with best Classification-F1: 0.5933528836754642 -> 0.75 on epoch=274, global_step=550
06/19/2022 22:49:06 - INFO - __main__ - Step 560 Global step 560 Train loss 1.85 on epoch=279
06/19/2022 22:49:07 - INFO - __main__ - Step 570 Global step 570 Train loss 1.67 on epoch=284
06/19/2022 22:49:08 - INFO - __main__ - Step 580 Global step 580 Train loss 1.69 on epoch=289
06/19/2022 22:49:10 - INFO - __main__ - Step 590 Global step 590 Train loss 1.75 on epoch=294
06/19/2022 22:49:11 - INFO - __main__ - Step 600 Global step 600 Train loss 1.78 on epoch=299
06/19/2022 22:49:11 - INFO - __main__ - Global step 600 Train loss 1.75 Classification-F1 0.3816425120772947 on epoch=299
06/19/2022 22:49:13 - INFO - __main__ - Step 610 Global step 610 Train loss 1.76 on epoch=304
06/19/2022 22:49:14 - INFO - __main__ - Step 620 Global step 620 Train loss 1.66 on epoch=309
06/19/2022 22:49:15 - INFO - __main__ - Step 630 Global step 630 Train loss 1.67 on epoch=314
06/19/2022 22:49:16 - INFO - __main__ - Step 640 Global step 640 Train loss 1.57 on epoch=319
06/19/2022 22:49:18 - INFO - __main__ - Step 650 Global step 650 Train loss 1.69 on epoch=324
06/19/2022 22:49:18 - INFO - __main__ - Global step 650 Train loss 1.67 Classification-F1 0.46843853820598 on epoch=324
06/19/2022 22:49:19 - INFO - __main__ - Step 660 Global step 660 Train loss 1.72 on epoch=329
06/19/2022 22:49:21 - INFO - __main__ - Step 670 Global step 670 Train loss 1.48 on epoch=334
06/19/2022 22:49:22 - INFO - __main__ - Step 680 Global step 680 Train loss 1.47 on epoch=339
06/19/2022 22:49:23 - INFO - __main__ - Step 690 Global step 690 Train loss 1.44 on epoch=344
06/19/2022 22:49:24 - INFO - __main__ - Step 700 Global step 700 Train loss 1.61 on epoch=349
06/19/2022 22:49:25 - INFO - __main__ - Global step 700 Train loss 1.54 Classification-F1 0.625 on epoch=349
06/19/2022 22:49:26 - INFO - __main__ - Step 710 Global step 710 Train loss 1.40 on epoch=354
06/19/2022 22:49:27 - INFO - __main__ - Step 720 Global step 720 Train loss 1.60 on epoch=359
06/19/2022 22:49:29 - INFO - __main__ - Step 730 Global step 730 Train loss 1.54 on epoch=364
06/19/2022 22:49:30 - INFO - __main__ - Step 740 Global step 740 Train loss 1.45 on epoch=369
06/19/2022 22:49:31 - INFO - __main__ - Step 750 Global step 750 Train loss 1.30 on epoch=374
06/19/2022 22:49:32 - INFO - __main__ - Global step 750 Train loss 1.46 Classification-F1 0.5607843137254902 on epoch=374
06/19/2022 22:49:33 - INFO - __main__ - Step 760 Global step 760 Train loss 1.42 on epoch=379
06/19/2022 22:49:34 - INFO - __main__ - Step 770 Global step 770 Train loss 1.56 on epoch=384
06/19/2022 22:49:35 - INFO - __main__ - Step 780 Global step 780 Train loss 1.33 on epoch=389
06/19/2022 22:49:37 - INFO - __main__ - Step 790 Global step 790 Train loss 1.29 on epoch=394
06/19/2022 22:49:38 - INFO - __main__ - Step 800 Global step 800 Train loss 1.42 on epoch=399
06/19/2022 22:49:38 - INFO - __main__ - Global step 800 Train loss 1.40 Classification-F1 0.4666666666666667 on epoch=399
06/19/2022 22:49:40 - INFO - __main__ - Step 810 Global step 810 Train loss 1.26 on epoch=404
06/19/2022 22:49:41 - INFO - __main__ - Step 820 Global step 820 Train loss 1.23 on epoch=409
06/19/2022 22:49:42 - INFO - __main__ - Step 830 Global step 830 Train loss 1.40 on epoch=414
06/19/2022 22:49:43 - INFO - __main__ - Step 840 Global step 840 Train loss 1.38 on epoch=419
06/19/2022 22:49:45 - INFO - __main__ - Step 850 Global step 850 Train loss 1.17 on epoch=424
06/19/2022 22:49:45 - INFO - __main__ - Global step 850 Train loss 1.29 Classification-F1 0.4554554554554554 on epoch=424
06/19/2022 22:49:46 - INFO - __main__ - Step 860 Global step 860 Train loss 1.19 on epoch=429
06/19/2022 22:49:48 - INFO - __main__ - Step 870 Global step 870 Train loss 1.11 on epoch=434
06/19/2022 22:49:49 - INFO - __main__ - Step 880 Global step 880 Train loss 1.28 on epoch=439
06/19/2022 22:49:50 - INFO - __main__ - Step 890 Global step 890 Train loss 1.23 on epoch=444
06/19/2022 22:49:51 - INFO - __main__ - Step 900 Global step 900 Train loss 1.23 on epoch=449
06/19/2022 22:49:52 - INFO - __main__ - Global step 900 Train loss 1.21 Classification-F1 0.3816425120772947 on epoch=449
06/19/2022 22:49:53 - INFO - __main__ - Step 910 Global step 910 Train loss 1.17 on epoch=454
06/19/2022 22:49:54 - INFO - __main__ - Step 920 Global step 920 Train loss 1.20 on epoch=459
06/19/2022 22:49:56 - INFO - __main__ - Step 930 Global step 930 Train loss 1.24 on epoch=464
06/19/2022 22:49:57 - INFO - __main__ - Step 940 Global step 940 Train loss 1.06 on epoch=469
06/19/2022 22:49:58 - INFO - __main__ - Step 950 Global step 950 Train loss 1.29 on epoch=474
06/19/2022 22:49:58 - INFO - __main__ - Global step 950 Train loss 1.19 Classification-F1 0.5076923076923077 on epoch=474
06/19/2022 22:50:00 - INFO - __main__ - Step 960 Global step 960 Train loss 1.15 on epoch=479
06/19/2022 22:50:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.99 on epoch=484
06/19/2022 22:50:02 - INFO - __main__ - Step 980 Global step 980 Train loss 1.05 on epoch=489
06/19/2022 22:50:04 - INFO - __main__ - Step 990 Global step 990 Train loss 1.06 on epoch=494
06/19/2022 22:50:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.12 on epoch=499
06/19/2022 22:50:05 - INFO - __main__ - Global step 1000 Train loss 1.07 Classification-F1 0.3816425120772947 on epoch=499
06/19/2022 22:50:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.10 on epoch=504
06/19/2022 22:50:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.08 on epoch=509
06/19/2022 22:50:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.01 on epoch=514
06/19/2022 22:50:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.97 on epoch=519
06/19/2022 22:50:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.99 on epoch=524
06/19/2022 22:50:12 - INFO - __main__ - Global step 1050 Train loss 1.03 Classification-F1 0.3816425120772947 on epoch=524
06/19/2022 22:50:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.02 on epoch=529
06/19/2022 22:50:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.96 on epoch=534
06/19/2022 22:50:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.11 on epoch=539
06/19/2022 22:50:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.93 on epoch=544
06/19/2022 22:50:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.87 on epoch=549
06/19/2022 22:50:18 - INFO - __main__ - Global step 1100 Train loss 0.98 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 22:50:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.08 on epoch=554
06/19/2022 22:50:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.06 on epoch=559
06/19/2022 22:50:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.89 on epoch=564
06/19/2022 22:50:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.96 on epoch=569
06/19/2022 22:50:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.90 on epoch=574
06/19/2022 22:50:25 - INFO - __main__ - Global step 1150 Train loss 0.98 Classification-F1 0.3191489361702127 on epoch=574
06/19/2022 22:50:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.01 on epoch=579
06/19/2022 22:50:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.91 on epoch=584
06/19/2022 22:50:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.93 on epoch=589
06/19/2022 22:50:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.99 on epoch=594
06/19/2022 22:50:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.86 on epoch=599
06/19/2022 22:50:32 - INFO - __main__ - Global step 1200 Train loss 0.94 Classification-F1 0.6000000000000001 on epoch=599
06/19/2022 22:50:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.02 on epoch=604
06/19/2022 22:50:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.89 on epoch=609
06/19/2022 22:50:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.01 on epoch=614
06/19/2022 22:50:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.00 on epoch=619
06/19/2022 22:50:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.85 on epoch=624
06/19/2022 22:50:39 - INFO - __main__ - Global step 1250 Train loss 0.96 Classification-F1 0.3454545454545454 on epoch=624
06/19/2022 22:50:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.88 on epoch=629
06/19/2022 22:50:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.94 on epoch=634
06/19/2022 22:50:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.83 on epoch=639
06/19/2022 22:50:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.94 on epoch=644
06/19/2022 22:50:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.83 on epoch=649
06/19/2022 22:50:45 - INFO - __main__ - Global step 1300 Train loss 0.88 Classification-F1 0.5076923076923077 on epoch=649
06/19/2022 22:50:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=654
06/19/2022 22:50:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.85 on epoch=659
06/19/2022 22:50:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.88 on epoch=664
06/19/2022 22:50:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.93 on epoch=669
06/19/2022 22:50:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.87 on epoch=674
06/19/2022 22:50:52 - INFO - __main__ - Global step 1350 Train loss 0.86 Classification-F1 0.5465587044534412 on epoch=674
06/19/2022 22:50:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.84 on epoch=679
06/19/2022 22:50:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.94 on epoch=684
06/19/2022 22:50:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.75 on epoch=689
06/19/2022 22:50:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.90 on epoch=694
06/19/2022 22:50:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.81 on epoch=699
06/19/2022 22:50:59 - INFO - __main__ - Global step 1400 Train loss 0.85 Classification-F1 0.46843853820598 on epoch=699
06/19/2022 22:51:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.90 on epoch=704
06/19/2022 22:51:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.78 on epoch=709
06/19/2022 22:51:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.83 on epoch=714
06/19/2022 22:51:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.76 on epoch=719
06/19/2022 22:51:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.75 on epoch=724
06/19/2022 22:51:06 - INFO - __main__ - Global step 1450 Train loss 0.80 Classification-F1 0.4385964912280702 on epoch=724
06/19/2022 22:51:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.69 on epoch=729
06/19/2022 22:51:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.75 on epoch=734
06/19/2022 22:51:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.72 on epoch=739
06/19/2022 22:51:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.74 on epoch=744
06/19/2022 22:51:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.82 on epoch=749
06/19/2022 22:51:12 - INFO - __main__ - Global step 1500 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 22:51:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.76 on epoch=754
06/19/2022 22:51:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.78 on epoch=759
06/19/2022 22:51:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.75 on epoch=764
06/19/2022 22:51:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.72 on epoch=769
06/19/2022 22:51:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.72 on epoch=774
06/19/2022 22:51:19 - INFO - __main__ - Global step 1550 Train loss 0.75 Classification-F1 0.3191489361702127 on epoch=774
06/19/2022 22:51:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.81 on epoch=779
06/19/2022 22:51:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.71 on epoch=784
06/19/2022 22:51:23 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.75 on epoch=789
06/19/2022 22:51:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.71 on epoch=794
06/19/2022 22:51:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.73 on epoch=799
06/19/2022 22:51:26 - INFO - __main__ - Global step 1600 Train loss 0.74 Classification-F1 0.5151515151515151 on epoch=799
06/19/2022 22:51:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.76 on epoch=804
06/19/2022 22:51:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.72 on epoch=809
06/19/2022 22:51:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.67 on epoch=814
06/19/2022 22:51:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.73 on epoch=819
06/19/2022 22:51:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.72 on epoch=824
06/19/2022 22:51:33 - INFO - __main__ - Global step 1650 Train loss 0.72 Classification-F1 0.4458874458874459 on epoch=824
06/19/2022 22:51:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.68 on epoch=829
06/19/2022 22:51:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.69 on epoch=834
06/19/2022 22:51:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.70 on epoch=839
06/19/2022 22:51:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.75 on epoch=844
06/19/2022 22:51:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.67 on epoch=849
06/19/2022 22:51:39 - INFO - __main__ - Global step 1700 Train loss 0.70 Classification-F1 0.36374269005847953 on epoch=849
06/19/2022 22:51:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.76 on epoch=854
06/19/2022 22:51:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.72 on epoch=859
06/19/2022 22:51:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.69 on epoch=864
06/19/2022 22:51:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.75 on epoch=869
06/19/2022 22:51:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.61 on epoch=874
06/19/2022 22:51:46 - INFO - __main__ - Global step 1750 Train loss 0.71 Classification-F1 0.5134502923976608 on epoch=874
06/19/2022 22:51:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.65 on epoch=879
06/19/2022 22:51:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.70 on epoch=884
06/19/2022 22:51:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.78 on epoch=889
06/19/2022 22:51:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.71 on epoch=894
06/19/2022 22:51:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.67 on epoch=899
06/19/2022 22:51:53 - INFO - __main__ - Global step 1800 Train loss 0.70 Classification-F1 0.4231177094379639 on epoch=899
06/19/2022 22:51:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.79 on epoch=904
06/19/2022 22:51:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.67 on epoch=909
06/19/2022 22:51:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.67 on epoch=914
06/19/2022 22:51:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.76 on epoch=919
06/19/2022 22:51:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.65 on epoch=924
06/19/2022 22:51:59 - INFO - __main__ - Global step 1850 Train loss 0.71 Classification-F1 0.6559139784946237 on epoch=924
06/19/2022 22:52:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.74 on epoch=929
06/19/2022 22:52:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.80 on epoch=934
06/19/2022 22:52:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.61 on epoch=939
06/19/2022 22:52:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.64 on epoch=944
06/19/2022 22:52:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.67 on epoch=949
06/19/2022 22:52:06 - INFO - __main__ - Global step 1900 Train loss 0.69 Classification-F1 0.4920634920634921 on epoch=949
06/19/2022 22:52:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.68 on epoch=954
06/19/2022 22:52:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.59 on epoch=959
06/19/2022 22:52:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.72 on epoch=964
06/19/2022 22:52:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.63 on epoch=969
06/19/2022 22:52:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.64 on epoch=974
06/19/2022 22:52:12 - INFO - __main__ - Global step 1950 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 22:52:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.69 on epoch=979
06/19/2022 22:52:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.83 on epoch=984
06/19/2022 22:52:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.60 on epoch=989
06/19/2022 22:52:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.64 on epoch=994
06/19/2022 22:52:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.71 on epoch=999
06/19/2022 22:52:19 - INFO - __main__ - Global step 2000 Train loss 0.70 Classification-F1 0.4980392156862745 on epoch=999
06/19/2022 22:52:19 - INFO - __main__ - save last model!
06/19/2022 22:52:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 22:52:19 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 22:52:19 - INFO - __main__ - Printing 3 examples
06/19/2022 22:52:19 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 22:52:19 - INFO - __main__ - ['0']
06/19/2022 22:52:19 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 22:52:19 - INFO - __main__ - ['1']
06/19/2022 22:52:19 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 22:52:19 - INFO - __main__ - ['1']
06/19/2022 22:52:19 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:52:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:52:20 - INFO - __main__ - Printing 3 examples
06/19/2022 22:52:20 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 22:52:20 - INFO - __main__ - ['1']
06/19/2022 22:52:20 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 22:52:20 - INFO - __main__ - ['1']
06/19/2022 22:52:20 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 22:52:20 - INFO - __main__ - ['1']
06/19/2022 22:52:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:52:20 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:52:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:52:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:52:20 - INFO - __main__ - Printing 3 examples
06/19/2022 22:52:20 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 22:52:20 - INFO - __main__ - ['1']
06/19/2022 22:52:20 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 22:52:20 - INFO - __main__ - ['1']
06/19/2022 22:52:20 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 22:52:20 - INFO - __main__ - ['1']
06/19/2022 22:52:20 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:52:20 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:52:20 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:52:23 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:52:25 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:52:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:52:26 - INFO - __main__ - Starting training!
06/19/2022 22:52:31 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 22:53:58 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_42_0.3_8_predictions.txt
06/19/2022 22:53:58 - INFO - __main__ - Classification-F1 on test data: 0.4987
06/19/2022 22:53:59 - INFO - __main__ - prefix=paws_16_42, lr=0.3, bsz=8, dev_performance=0.75, test_performance=0.49872732936424197
06/19/2022 22:53:59 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.2, bsz=8 ...
06/19/2022 22:54:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:54:00 - INFO - __main__ - Printing 3 examples
06/19/2022 22:54:00 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/19/2022 22:54:00 - INFO - __main__ - ['1']
06/19/2022 22:54:00 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/19/2022 22:54:00 - INFO - __main__ - ['1']
06/19/2022 22:54:00 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/19/2022 22:54:00 - INFO - __main__ - ['1']
06/19/2022 22:54:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:54:00 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:54:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:54:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:54:00 - INFO - __main__ - Printing 3 examples
06/19/2022 22:54:00 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/19/2022 22:54:00 - INFO - __main__ - ['1']
06/19/2022 22:54:00 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/19/2022 22:54:00 - INFO - __main__ - ['1']
06/19/2022 22:54:00 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/19/2022 22:54:00 - INFO - __main__ - ['1']
06/19/2022 22:54:00 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:54:00 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:54:00 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:54:05 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:54:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:54:05 - INFO - __main__ - Starting training!
06/19/2022 22:54:07 - INFO - __main__ - Step 10 Global step 10 Train loss 6.00 on epoch=4
06/19/2022 22:54:08 - INFO - __main__ - Step 20 Global step 20 Train loss 5.79 on epoch=9
06/19/2022 22:54:10 - INFO - __main__ - Step 30 Global step 30 Train loss 5.51 on epoch=14
06/19/2022 22:54:11 - INFO - __main__ - Step 40 Global step 40 Train loss 5.35 on epoch=19
06/19/2022 22:54:12 - INFO - __main__ - Step 50 Global step 50 Train loss 5.03 on epoch=24
06/19/2022 22:54:15 - INFO - __main__ - Global step 50 Train loss 5.54 Classification-F1 0.0 on epoch=24
06/19/2022 22:54:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 22:54:17 - INFO - __main__ - Step 60 Global step 60 Train loss 4.73 on epoch=29
06/19/2022 22:54:18 - INFO - __main__ - Step 70 Global step 70 Train loss 4.47 on epoch=34
06/19/2022 22:54:19 - INFO - __main__ - Step 80 Global step 80 Train loss 4.36 on epoch=39
06/19/2022 22:54:20 - INFO - __main__ - Step 90 Global step 90 Train loss 4.23 on epoch=44
06/19/2022 22:54:22 - INFO - __main__ - Step 100 Global step 100 Train loss 4.13 on epoch=49
06/19/2022 22:54:23 - INFO - __main__ - Global step 100 Train loss 4.38 Classification-F1 0.0 on epoch=49
06/19/2022 22:54:24 - INFO - __main__ - Step 110 Global step 110 Train loss 3.95 on epoch=54
06/19/2022 22:54:25 - INFO - __main__ - Step 120 Global step 120 Train loss 3.96 on epoch=59
06/19/2022 22:54:27 - INFO - __main__ - Step 130 Global step 130 Train loss 3.86 on epoch=64
06/19/2022 22:54:28 - INFO - __main__ - Step 140 Global step 140 Train loss 3.82 on epoch=69
06/19/2022 22:54:29 - INFO - __main__ - Step 150 Global step 150 Train loss 3.77 on epoch=74
06/19/2022 22:54:31 - INFO - __main__ - Global step 150 Train loss 3.87 Classification-F1 0.0 on epoch=74
06/19/2022 22:54:32 - INFO - __main__ - Step 160 Global step 160 Train loss 3.71 on epoch=79
06/19/2022 22:54:33 - INFO - __main__ - Step 170 Global step 170 Train loss 3.69 on epoch=84
06/19/2022 22:54:34 - INFO - __main__ - Step 180 Global step 180 Train loss 3.50 on epoch=89
06/19/2022 22:54:36 - INFO - __main__ - Step 190 Global step 190 Train loss 3.55 on epoch=94
06/19/2022 22:54:37 - INFO - __main__ - Step 200 Global step 200 Train loss 3.41 on epoch=99
06/19/2022 22:54:48 - INFO - __main__ - Global step 200 Train loss 3.57 Classification-F1 0.0 on epoch=99
06/19/2022 22:54:49 - INFO - __main__ - Step 210 Global step 210 Train loss 3.31 on epoch=104
06/19/2022 22:54:51 - INFO - __main__ - Step 220 Global step 220 Train loss 3.17 on epoch=109
06/19/2022 22:54:52 - INFO - __main__ - Step 230 Global step 230 Train loss 3.36 on epoch=114
06/19/2022 22:54:53 - INFO - __main__ - Step 240 Global step 240 Train loss 3.18 on epoch=119
06/19/2022 22:54:54 - INFO - __main__ - Step 250 Global step 250 Train loss 3.00 on epoch=124
06/19/2022 22:55:01 - INFO - __main__ - Global step 250 Train loss 3.20 Classification-F1 0.014035087719298244 on epoch=124
06/19/2022 22:55:01 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.014035087719298244 on epoch=124, global_step=250
06/19/2022 22:55:02 - INFO - __main__ - Step 260 Global step 260 Train loss 3.02 on epoch=129
06/19/2022 22:55:03 - INFO - __main__ - Step 270 Global step 270 Train loss 3.16 on epoch=134
06/19/2022 22:55:04 - INFO - __main__ - Step 280 Global step 280 Train loss 3.03 on epoch=139
06/19/2022 22:55:06 - INFO - __main__ - Step 290 Global step 290 Train loss 2.92 on epoch=144
06/19/2022 22:55:07 - INFO - __main__ - Step 300 Global step 300 Train loss 2.80 on epoch=149
06/19/2022 22:55:09 - INFO - __main__ - Global step 300 Train loss 2.98 Classification-F1 0.0611111111111111 on epoch=149
06/19/2022 22:55:09 - INFO - __main__ - Saving model with best Classification-F1: 0.014035087719298244 -> 0.0611111111111111 on epoch=149, global_step=300
06/19/2022 22:55:10 - INFO - __main__ - Step 310 Global step 310 Train loss 2.80 on epoch=154
06/19/2022 22:55:12 - INFO - __main__ - Step 320 Global step 320 Train loss 2.71 on epoch=159
06/19/2022 22:55:13 - INFO - __main__ - Step 330 Global step 330 Train loss 2.70 on epoch=164
06/19/2022 22:55:14 - INFO - __main__ - Step 340 Global step 340 Train loss 2.70 on epoch=169
06/19/2022 22:55:15 - INFO - __main__ - Step 350 Global step 350 Train loss 2.60 on epoch=174
06/19/2022 22:55:17 - INFO - __main__ - Global step 350 Train loss 2.70 Classification-F1 0.14444444444444443 on epoch=174
06/19/2022 22:55:17 - INFO - __main__ - Saving model with best Classification-F1: 0.0611111111111111 -> 0.14444444444444443 on epoch=174, global_step=350
06/19/2022 22:55:18 - INFO - __main__ - Step 360 Global step 360 Train loss 2.61 on epoch=179
06/19/2022 22:55:20 - INFO - __main__ - Step 370 Global step 370 Train loss 2.51 on epoch=184
06/19/2022 22:55:21 - INFO - __main__ - Step 380 Global step 380 Train loss 2.58 on epoch=189
06/19/2022 22:55:22 - INFO - __main__ - Step 390 Global step 390 Train loss 2.45 on epoch=194
06/19/2022 22:55:23 - INFO - __main__ - Step 400 Global step 400 Train loss 2.33 on epoch=199
06/19/2022 22:55:25 - INFO - __main__ - Global step 400 Train loss 2.50 Classification-F1 0.21276595744680848 on epoch=199
06/19/2022 22:55:25 - INFO - __main__ - Saving model with best Classification-F1: 0.14444444444444443 -> 0.21276595744680848 on epoch=199, global_step=400
06/19/2022 22:55:26 - INFO - __main__ - Step 410 Global step 410 Train loss 2.35 on epoch=204
06/19/2022 22:55:27 - INFO - __main__ - Step 420 Global step 420 Train loss 2.37 on epoch=209
06/19/2022 22:55:28 - INFO - __main__ - Step 430 Global step 430 Train loss 2.38 on epoch=214
06/19/2022 22:55:30 - INFO - __main__ - Step 440 Global step 440 Train loss 2.46 on epoch=219
06/19/2022 22:55:31 - INFO - __main__ - Step 450 Global step 450 Train loss 2.18 on epoch=224
06/19/2022 22:55:34 - INFO - __main__ - Global step 450 Train loss 2.35 Classification-F1 0.18 on epoch=224
06/19/2022 22:55:35 - INFO - __main__ - Step 460 Global step 460 Train loss 2.19 on epoch=229
06/19/2022 22:55:36 - INFO - __main__ - Step 470 Global step 470 Train loss 2.22 on epoch=234
06/19/2022 22:55:37 - INFO - __main__ - Step 480 Global step 480 Train loss 2.08 on epoch=239
06/19/2022 22:55:39 - INFO - __main__ - Step 490 Global step 490 Train loss 2.11 on epoch=244
06/19/2022 22:55:40 - INFO - __main__ - Step 500 Global step 500 Train loss 2.03 on epoch=249
06/19/2022 22:55:42 - INFO - __main__ - Global step 500 Train loss 2.13 Classification-F1 0.2686486486486487 on epoch=249
06/19/2022 22:55:42 - INFO - __main__ - Saving model with best Classification-F1: 0.21276595744680848 -> 0.2686486486486487 on epoch=249, global_step=500
06/19/2022 22:55:43 - INFO - __main__ - Step 510 Global step 510 Train loss 2.00 on epoch=254
06/19/2022 22:55:44 - INFO - __main__ - Step 520 Global step 520 Train loss 2.04 on epoch=259
06/19/2022 22:55:46 - INFO - __main__ - Step 530 Global step 530 Train loss 2.04 on epoch=264
06/19/2022 22:55:47 - INFO - __main__ - Step 540 Global step 540 Train loss 1.82 on epoch=269
06/19/2022 22:55:48 - INFO - __main__ - Step 550 Global step 550 Train loss 1.89 on epoch=274
06/19/2022 22:55:50 - INFO - __main__ - Global step 550 Train loss 1.96 Classification-F1 0.6559139784946237 on epoch=274
06/19/2022 22:55:50 - INFO - __main__ - Saving model with best Classification-F1: 0.2686486486486487 -> 0.6559139784946237 on epoch=274, global_step=550
06/19/2022 22:55:52 - INFO - __main__ - Step 560 Global step 560 Train loss 1.80 on epoch=279
06/19/2022 22:55:53 - INFO - __main__ - Step 570 Global step 570 Train loss 1.81 on epoch=284
06/19/2022 22:55:54 - INFO - __main__ - Step 580 Global step 580 Train loss 1.89 on epoch=289
06/19/2022 22:55:55 - INFO - __main__ - Step 590 Global step 590 Train loss 1.85 on epoch=294
06/19/2022 22:55:57 - INFO - __main__ - Step 600 Global step 600 Train loss 1.78 on epoch=299
06/19/2022 22:55:58 - INFO - __main__ - Global step 600 Train loss 1.83 Classification-F1 0.4909862142099682 on epoch=299
06/19/2022 22:55:59 - INFO - __main__ - Step 610 Global step 610 Train loss 1.81 on epoch=304
06/19/2022 22:56:01 - INFO - __main__ - Step 620 Global step 620 Train loss 1.86 on epoch=309
06/19/2022 22:56:02 - INFO - __main__ - Step 630 Global step 630 Train loss 1.74 on epoch=314
06/19/2022 22:56:03 - INFO - __main__ - Step 640 Global step 640 Train loss 1.77 on epoch=319
06/19/2022 22:56:04 - INFO - __main__ - Step 650 Global step 650 Train loss 1.67 on epoch=324
06/19/2022 22:56:05 - INFO - __main__ - Global step 650 Train loss 1.77 Classification-F1 0.39999999999999997 on epoch=324
06/19/2022 22:56:06 - INFO - __main__ - Step 660 Global step 660 Train loss 1.70 on epoch=329
06/19/2022 22:56:08 - INFO - __main__ - Step 670 Global step 670 Train loss 1.65 on epoch=334
06/19/2022 22:56:09 - INFO - __main__ - Step 680 Global step 680 Train loss 1.63 on epoch=339
06/19/2022 22:56:10 - INFO - __main__ - Step 690 Global step 690 Train loss 1.55 on epoch=344
06/19/2022 22:56:11 - INFO - __main__ - Step 700 Global step 700 Train loss 1.58 on epoch=349
06/19/2022 22:56:13 - INFO - __main__ - Global step 700 Train loss 1.62 Classification-F1 0.3816425120772947 on epoch=349
06/19/2022 22:56:14 - INFO - __main__ - Step 710 Global step 710 Train loss 1.50 on epoch=354
06/19/2022 22:56:15 - INFO - __main__ - Step 720 Global step 720 Train loss 1.57 on epoch=359
06/19/2022 22:56:17 - INFO - __main__ - Step 730 Global step 730 Train loss 1.49 on epoch=364
06/19/2022 22:56:18 - INFO - __main__ - Step 740 Global step 740 Train loss 1.49 on epoch=369
06/19/2022 22:56:19 - INFO - __main__ - Step 750 Global step 750 Train loss 1.29 on epoch=374
06/19/2022 22:56:19 - INFO - __main__ - Global step 750 Train loss 1.47 Classification-F1 0.3191489361702127 on epoch=374
06/19/2022 22:56:21 - INFO - __main__ - Step 760 Global step 760 Train loss 1.40 on epoch=379
06/19/2022 22:56:22 - INFO - __main__ - Step 770 Global step 770 Train loss 1.32 on epoch=384
06/19/2022 22:56:23 - INFO - __main__ - Step 780 Global step 780 Train loss 1.48 on epoch=389
06/19/2022 22:56:24 - INFO - __main__ - Step 790 Global step 790 Train loss 1.45 on epoch=394
06/19/2022 22:56:26 - INFO - __main__ - Step 800 Global step 800 Train loss 1.40 on epoch=399
06/19/2022 22:56:26 - INFO - __main__ - Global step 800 Train loss 1.41 Classification-F1 0.4385964912280702 on epoch=399
06/19/2022 22:56:27 - INFO - __main__ - Step 810 Global step 810 Train loss 1.25 on epoch=404
06/19/2022 22:56:28 - INFO - __main__ - Step 820 Global step 820 Train loss 1.24 on epoch=409
06/19/2022 22:56:30 - INFO - __main__ - Step 830 Global step 830 Train loss 1.34 on epoch=414
06/19/2022 22:56:31 - INFO - __main__ - Step 840 Global step 840 Train loss 1.30 on epoch=419
06/19/2022 22:56:32 - INFO - __main__ - Step 850 Global step 850 Train loss 1.20 on epoch=424
06/19/2022 22:56:32 - INFO - __main__ - Global step 850 Train loss 1.27 Classification-F1 0.3816425120772947 on epoch=424
06/19/2022 22:56:34 - INFO - __main__ - Step 860 Global step 860 Train loss 1.32 on epoch=429
06/19/2022 22:56:35 - INFO - __main__ - Step 870 Global step 870 Train loss 1.22 on epoch=434
06/19/2022 22:56:36 - INFO - __main__ - Step 880 Global step 880 Train loss 1.30 on epoch=439
06/19/2022 22:56:38 - INFO - __main__ - Step 890 Global step 890 Train loss 1.27 on epoch=444
06/19/2022 22:56:39 - INFO - __main__ - Step 900 Global step 900 Train loss 1.17 on epoch=449
06/19/2022 22:56:39 - INFO - __main__ - Global step 900 Train loss 1.25 Classification-F1 0.4589371980676329 on epoch=449
06/19/2022 22:56:40 - INFO - __main__ - Step 910 Global step 910 Train loss 1.23 on epoch=454
06/19/2022 22:56:42 - INFO - __main__ - Step 920 Global step 920 Train loss 1.24 on epoch=459
06/19/2022 22:56:43 - INFO - __main__ - Step 930 Global step 930 Train loss 1.29 on epoch=464
06/19/2022 22:56:44 - INFO - __main__ - Step 940 Global step 940 Train loss 1.14 on epoch=469
06/19/2022 22:56:45 - INFO - __main__ - Step 950 Global step 950 Train loss 1.23 on epoch=474
06/19/2022 22:56:46 - INFO - __main__ - Global step 950 Train loss 1.23 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 22:56:47 - INFO - __main__ - Step 960 Global step 960 Train loss 1.16 on epoch=479
06/19/2022 22:56:48 - INFO - __main__ - Step 970 Global step 970 Train loss 1.06 on epoch=484
06/19/2022 22:56:49 - INFO - __main__ - Step 980 Global step 980 Train loss 1.14 on epoch=489
06/19/2022 22:56:51 - INFO - __main__ - Step 990 Global step 990 Train loss 1.19 on epoch=494
06/19/2022 22:56:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.13 on epoch=499
06/19/2022 22:56:52 - INFO - __main__ - Global step 1000 Train loss 1.14 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 22:56:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.07 on epoch=504
06/19/2022 22:56:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.18 on epoch=509
06/19/2022 22:56:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.11 on epoch=514
06/19/2022 22:56:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.94 on epoch=519
06/19/2022 22:56:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.12 on epoch=524
06/19/2022 22:56:59 - INFO - __main__ - Global step 1050 Train loss 1.08 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 22:57:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.98 on epoch=529
06/19/2022 22:57:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.91 on epoch=534
06/19/2022 22:57:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.03 on epoch=539
06/19/2022 22:57:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.06 on epoch=544
06/19/2022 22:57:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.01 on epoch=549
06/19/2022 22:57:05 - INFO - __main__ - Global step 1100 Train loss 1.00 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 22:57:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.02 on epoch=554
06/19/2022 22:57:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.98 on epoch=559
06/19/2022 22:57:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.04 on epoch=564
06/19/2022 22:57:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.06 on epoch=569
06/19/2022 22:57:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.92 on epoch=574
06/19/2022 22:57:12 - INFO - __main__ - Global step 1150 Train loss 1.00 Classification-F1 0.539313399778516 on epoch=574
06/19/2022 22:57:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.03 on epoch=579
06/19/2022 22:57:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.90 on epoch=584
06/19/2022 22:57:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.04 on epoch=589
06/19/2022 22:57:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.05 on epoch=594
06/19/2022 22:57:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.95 on epoch=599
06/19/2022 22:57:19 - INFO - __main__ - Global step 1200 Train loss 0.99 Classification-F1 0.3816425120772947 on epoch=599
06/19/2022 22:57:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.92 on epoch=604
06/19/2022 22:57:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.00 on epoch=609
06/19/2022 22:57:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.94 on epoch=614
06/19/2022 22:57:24 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.97 on epoch=619
06/19/2022 22:57:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.02 on epoch=624
06/19/2022 22:57:25 - INFO - __main__ - Global step 1250 Train loss 0.97 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 22:57:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.92 on epoch=629
06/19/2022 22:57:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.96 on epoch=634
06/19/2022 22:57:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.95 on epoch=639
06/19/2022 22:57:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.91 on epoch=644
06/19/2022 22:57:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.01 on epoch=649
06/19/2022 22:57:32 - INFO - __main__ - Global step 1300 Train loss 0.95 Classification-F1 0.3191489361702127 on epoch=649
06/19/2022 22:57:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.84 on epoch=654
06/19/2022 22:57:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.85 on epoch=659
06/19/2022 22:57:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.85 on epoch=664
06/19/2022 22:57:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.92 on epoch=669
06/19/2022 22:57:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.85 on epoch=674
06/19/2022 22:57:39 - INFO - __main__ - Global step 1350 Train loss 0.86 Classification-F1 0.3816425120772947 on epoch=674
06/19/2022 22:57:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.01 on epoch=679
06/19/2022 22:57:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.83 on epoch=684
06/19/2022 22:57:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.88 on epoch=689
06/19/2022 22:57:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.88 on epoch=694
06/19/2022 22:57:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.77 on epoch=699
06/19/2022 22:57:46 - INFO - __main__ - Global step 1400 Train loss 0.88 Classification-F1 0.28888888888888886 on epoch=699
06/19/2022 22:57:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.96 on epoch=704
06/19/2022 22:57:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.88 on epoch=709
06/19/2022 22:57:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.84 on epoch=714
06/19/2022 22:57:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.87 on epoch=719
06/19/2022 22:57:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.88 on epoch=724
06/19/2022 22:57:52 - INFO - __main__ - Global step 1450 Train loss 0.89 Classification-F1 0.3816425120772947 on epoch=724
06/19/2022 22:57:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.94 on epoch=729
06/19/2022 22:57:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.77 on epoch=734
06/19/2022 22:57:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.75 on epoch=739
06/19/2022 22:57:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.76 on epoch=744
06/19/2022 22:57:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.70 on epoch=749
06/19/2022 22:57:59 - INFO - __main__ - Global step 1500 Train loss 0.78 Classification-F1 0.3992490613266583 on epoch=749
06/19/2022 22:58:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.82 on epoch=754
06/19/2022 22:58:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.82 on epoch=759
06/19/2022 22:58:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.86 on epoch=764
06/19/2022 22:58:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.75 on epoch=769
06/19/2022 22:58:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.82 on epoch=774
06/19/2022 22:58:06 - INFO - __main__ - Global step 1550 Train loss 0.81 Classification-F1 0.3992490613266583 on epoch=774
06/19/2022 22:58:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.81 on epoch=779
06/19/2022 22:58:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.80 on epoch=784
06/19/2022 22:58:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.83 on epoch=789
06/19/2022 22:58:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.82 on epoch=794
06/19/2022 22:58:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.85 on epoch=799
06/19/2022 22:58:12 - INFO - __main__ - Global step 1600 Train loss 0.82 Classification-F1 0.3992490613266583 on epoch=799
06/19/2022 22:58:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.77 on epoch=804
06/19/2022 22:58:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.76 on epoch=809
06/19/2022 22:58:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.68 on epoch=814
06/19/2022 22:58:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.70 on epoch=819
06/19/2022 22:58:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.75 on epoch=824
06/19/2022 22:58:19 - INFO - __main__ - Global step 1650 Train loss 0.73 Classification-F1 0.3992490613266583 on epoch=824
06/19/2022 22:58:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.71 on epoch=829
06/19/2022 22:58:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.71 on epoch=834
06/19/2022 22:58:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.81 on epoch=839
06/19/2022 22:58:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.71 on epoch=844
06/19/2022 22:58:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.68 on epoch=849
06/19/2022 22:58:26 - INFO - __main__ - Global step 1700 Train loss 0.72 Classification-F1 0.3992490613266583 on epoch=849
06/19/2022 22:58:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.74 on epoch=854
06/19/2022 22:58:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.81 on epoch=859
06/19/2022 22:58:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.77 on epoch=864
06/19/2022 22:58:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.78 on epoch=869
06/19/2022 22:58:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.75 on epoch=874
06/19/2022 22:58:33 - INFO - __main__ - Global step 1750 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=874
06/19/2022 22:58:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.74 on epoch=879
06/19/2022 22:58:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.74 on epoch=884
06/19/2022 22:58:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.75 on epoch=889
06/19/2022 22:58:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.76 on epoch=894
06/19/2022 22:58:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.68 on epoch=899
06/19/2022 22:58:39 - INFO - __main__ - Global step 1800 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 22:58:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.74 on epoch=904
06/19/2022 22:58:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.71 on epoch=909
06/19/2022 22:58:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.77 on epoch=914
06/19/2022 22:58:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.63 on epoch=919
06/19/2022 22:58:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.77 on epoch=924
06/19/2022 22:58:46 - INFO - __main__ - Global step 1850 Train loss 0.73 Classification-F1 0.3992490613266583 on epoch=924
06/19/2022 22:58:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.69 on epoch=929
06/19/2022 22:58:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.73 on epoch=934
06/19/2022 22:58:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.60 on epoch=939
06/19/2022 22:58:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.67 on epoch=944
06/19/2022 22:58:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.66 on epoch=949
06/19/2022 22:58:53 - INFO - __main__ - Global step 1900 Train loss 0.67 Classification-F1 0.3992490613266583 on epoch=949
06/19/2022 22:58:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.70 on epoch=954
06/19/2022 22:58:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.64 on epoch=959
06/19/2022 22:58:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.65 on epoch=964
06/19/2022 22:58:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.77 on epoch=969
06/19/2022 22:58:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.67 on epoch=974
06/19/2022 22:58:59 - INFO - __main__ - Global step 1950 Train loss 0.68 Classification-F1 0.3992490613266583 on epoch=974
06/19/2022 22:59:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.66 on epoch=979
06/19/2022 22:59:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.73 on epoch=984
06/19/2022 22:59:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.66 on epoch=989
06/19/2022 22:59:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.73 on epoch=994
06/19/2022 22:59:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.63 on epoch=999
06/19/2022 22:59:06 - INFO - __main__ - Global step 2000 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 22:59:06 - INFO - __main__ - save last model!
06/19/2022 22:59:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 22:59:06 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 22:59:06 - INFO - __main__ - Printing 3 examples
06/19/2022 22:59:06 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 22:59:06 - INFO - __main__ - ['0']
06/19/2022 22:59:06 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 22:59:06 - INFO - __main__ - ['1']
06/19/2022 22:59:06 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 22:59:06 - INFO - __main__ - ['1']
06/19/2022 22:59:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:59:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:59:07 - INFO - __main__ - Printing 3 examples
06/19/2022 22:59:07 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 22:59:07 - INFO - __main__ - ['0']
06/19/2022 22:59:07 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 22:59:07 - INFO - __main__ - ['0']
06/19/2022 22:59:07 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 22:59:07 - INFO - __main__ - ['0']
06/19/2022 22:59:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:59:07 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:59:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:59:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:59:07 - INFO - __main__ - Printing 3 examples
06/19/2022 22:59:07 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 22:59:07 - INFO - __main__ - ['0']
06/19/2022 22:59:07 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 22:59:07 - INFO - __main__ - ['0']
06/19/2022 22:59:07 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 22:59:07 - INFO - __main__ - ['0']
06/19/2022 22:59:07 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:59:07 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:59:07 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:59:11 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:59:13 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 22:59:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 22:59:13 - INFO - __main__ - Starting training!
06/19/2022 22:59:19 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 23:00:45 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_42_0.2_8_predictions.txt
06/19/2022 23:00:45 - INFO - __main__ - Classification-F1 on test data: 0.3120
06/19/2022 23:00:45 - INFO - __main__ - prefix=paws_16_42, lr=0.2, bsz=8, dev_performance=0.6559139784946237, test_performance=0.31195517839733855
06/19/2022 23:00:45 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.5, bsz=8 ...
06/19/2022 23:00:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:00:46 - INFO - __main__ - Printing 3 examples
06/19/2022 23:00:46 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 23:00:46 - INFO - __main__ - ['0']
06/19/2022 23:00:46 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 23:00:46 - INFO - __main__ - ['0']
06/19/2022 23:00:46 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 23:00:46 - INFO - __main__ - ['0']
06/19/2022 23:00:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 23:00:46 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:00:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 23:00:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:00:46 - INFO - __main__ - Printing 3 examples
06/19/2022 23:00:46 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 23:00:46 - INFO - __main__ - ['0']
06/19/2022 23:00:46 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 23:00:46 - INFO - __main__ - ['0']
06/19/2022 23:00:46 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 23:00:46 - INFO - __main__ - ['0']
06/19/2022 23:00:46 - INFO - __main__ - Tokenizing Input ...
06/19/2022 23:00:46 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:00:46 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 23:00:51 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 23:00:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 23:00:52 - INFO - __main__ - Starting training!
06/19/2022 23:00:53 - INFO - __main__ - Step 10 Global step 10 Train loss 5.68 on epoch=4
06/19/2022 23:00:54 - INFO - __main__ - Step 20 Global step 20 Train loss 5.15 on epoch=9
06/19/2022 23:00:56 - INFO - __main__ - Step 30 Global step 30 Train loss 4.53 on epoch=14
06/19/2022 23:00:57 - INFO - __main__ - Step 40 Global step 40 Train loss 4.13 on epoch=19
06/19/2022 23:00:58 - INFO - __main__ - Step 50 Global step 50 Train loss 3.92 on epoch=24
06/19/2022 23:00:59 - INFO - __main__ - Global step 50 Train loss 4.68 Classification-F1 0.0 on epoch=24
06/19/2022 23:00:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 23:01:01 - INFO - __main__ - Step 60 Global step 60 Train loss 3.72 on epoch=29
06/19/2022 23:01:02 - INFO - __main__ - Step 70 Global step 70 Train loss 3.54 on epoch=34
06/19/2022 23:01:03 - INFO - __main__ - Step 80 Global step 80 Train loss 3.55 on epoch=39
06/19/2022 23:01:04 - INFO - __main__ - Step 90 Global step 90 Train loss 3.42 on epoch=44
06/19/2022 23:01:06 - INFO - __main__ - Step 100 Global step 100 Train loss 3.22 on epoch=49
06/19/2022 23:01:08 - INFO - __main__ - Global step 100 Train loss 3.49 Classification-F1 0.0 on epoch=49
06/19/2022 23:01:09 - INFO - __main__ - Step 110 Global step 110 Train loss 3.20 on epoch=54
06/19/2022 23:01:10 - INFO - __main__ - Step 120 Global step 120 Train loss 3.05 on epoch=59
06/19/2022 23:01:12 - INFO - __main__ - Step 130 Global step 130 Train loss 2.97 on epoch=64
06/19/2022 23:01:13 - INFO - __main__ - Step 140 Global step 140 Train loss 2.98 on epoch=69
06/19/2022 23:01:14 - INFO - __main__ - Step 150 Global step 150 Train loss 2.79 on epoch=74
06/19/2022 23:01:16 - INFO - __main__ - Global step 150 Train loss 3.00 Classification-F1 0.21276595744680848 on epoch=74
06/19/2022 23:01:16 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.21276595744680848 on epoch=74, global_step=150
06/19/2022 23:01:17 - INFO - __main__ - Step 160 Global step 160 Train loss 2.57 on epoch=79
06/19/2022 23:01:18 - INFO - __main__ - Step 170 Global step 170 Train loss 2.52 on epoch=84
06/19/2022 23:01:19 - INFO - __main__ - Step 180 Global step 180 Train loss 2.28 on epoch=89
06/19/2022 23:01:20 - INFO - __main__ - Step 190 Global step 190 Train loss 2.16 on epoch=94
06/19/2022 23:01:22 - INFO - __main__ - Step 200 Global step 200 Train loss 2.11 on epoch=99
06/19/2022 23:01:24 - INFO - __main__ - Global step 200 Train loss 2.33 Classification-F1 0.3333333333333333 on epoch=99
06/19/2022 23:01:24 - INFO - __main__ - Saving model with best Classification-F1: 0.21276595744680848 -> 0.3333333333333333 on epoch=99, global_step=200
06/19/2022 23:01:25 - INFO - __main__ - Step 210 Global step 210 Train loss 1.96 on epoch=104
06/19/2022 23:01:26 - INFO - __main__ - Step 220 Global step 220 Train loss 1.99 on epoch=109
06/19/2022 23:01:27 - INFO - __main__ - Step 230 Global step 230 Train loss 1.75 on epoch=114
06/19/2022 23:01:29 - INFO - __main__ - Step 240 Global step 240 Train loss 1.94 on epoch=119
06/19/2022 23:01:30 - INFO - __main__ - Step 250 Global step 250 Train loss 1.80 on epoch=124
06/19/2022 23:01:31 - INFO - __main__ - Global step 250 Train loss 1.89 Classification-F1 0.39756367663344405 on epoch=124
06/19/2022 23:01:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.39756367663344405 on epoch=124, global_step=250
06/19/2022 23:01:33 - INFO - __main__ - Step 260 Global step 260 Train loss 1.90 on epoch=129
06/19/2022 23:01:34 - INFO - __main__ - Step 270 Global step 270 Train loss 1.63 on epoch=134
06/19/2022 23:01:35 - INFO - __main__ - Step 280 Global step 280 Train loss 1.62 on epoch=139
06/19/2022 23:01:36 - INFO - __main__ - Step 290 Global step 290 Train loss 1.62 on epoch=144
06/19/2022 23:01:38 - INFO - __main__ - Step 300 Global step 300 Train loss 1.52 on epoch=149
06/19/2022 23:01:38 - INFO - __main__ - Global step 300 Train loss 1.66 Classification-F1 0.4181818181818182 on epoch=149
06/19/2022 23:01:38 - INFO - __main__ - Saving model with best Classification-F1: 0.39756367663344405 -> 0.4181818181818182 on epoch=149, global_step=300
06/19/2022 23:01:39 - INFO - __main__ - Step 310 Global step 310 Train loss 1.48 on epoch=154
06/19/2022 23:01:40 - INFO - __main__ - Step 320 Global step 320 Train loss 1.49 on epoch=159
06/19/2022 23:01:42 - INFO - __main__ - Step 330 Global step 330 Train loss 1.49 on epoch=164
06/19/2022 23:01:43 - INFO - __main__ - Step 340 Global step 340 Train loss 1.36 on epoch=169
06/19/2022 23:01:44 - INFO - __main__ - Step 350 Global step 350 Train loss 1.38 on epoch=174
06/19/2022 23:01:44 - INFO - __main__ - Global step 350 Train loss 1.44 Classification-F1 0.4920634920634921 on epoch=174
06/19/2022 23:01:44 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.4920634920634921 on epoch=174, global_step=350
06/19/2022 23:01:46 - INFO - __main__ - Step 360 Global step 360 Train loss 1.38 on epoch=179
06/19/2022 23:01:47 - INFO - __main__ - Step 370 Global step 370 Train loss 1.28 on epoch=184
06/19/2022 23:01:48 - INFO - __main__ - Step 380 Global step 380 Train loss 1.23 on epoch=189
06/19/2022 23:01:49 - INFO - __main__ - Step 390 Global step 390 Train loss 1.38 on epoch=194
06/19/2022 23:01:51 - INFO - __main__ - Step 400 Global step 400 Train loss 1.29 on epoch=199
06/19/2022 23:01:51 - INFO - __main__ - Global step 400 Train loss 1.31 Classification-F1 0.3107692307692308 on epoch=199
06/19/2022 23:01:52 - INFO - __main__ - Step 410 Global step 410 Train loss 1.23 on epoch=204
06/19/2022 23:01:53 - INFO - __main__ - Step 420 Global step 420 Train loss 1.11 on epoch=209
06/19/2022 23:01:55 - INFO - __main__ - Step 430 Global step 430 Train loss 1.30 on epoch=214
06/19/2022 23:01:56 - INFO - __main__ - Step 440 Global step 440 Train loss 1.16 on epoch=219
06/19/2022 23:01:57 - INFO - __main__ - Step 450 Global step 450 Train loss 1.22 on epoch=224
06/19/2022 23:01:57 - INFO - __main__ - Global step 450 Train loss 1.21 Classification-F1 0.5307917888563051 on epoch=224
06/19/2022 23:01:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4920634920634921 -> 0.5307917888563051 on epoch=224, global_step=450
06/19/2022 23:01:59 - INFO - __main__ - Step 460 Global step 460 Train loss 1.15 on epoch=229
06/19/2022 23:02:00 - INFO - __main__ - Step 470 Global step 470 Train loss 1.01 on epoch=234
06/19/2022 23:02:01 - INFO - __main__ - Step 480 Global step 480 Train loss 1.14 on epoch=239
06/19/2022 23:02:02 - INFO - __main__ - Step 490 Global step 490 Train loss 1.00 on epoch=244
06/19/2022 23:02:03 - INFO - __main__ - Step 500 Global step 500 Train loss 1.17 on epoch=249
06/19/2022 23:02:04 - INFO - __main__ - Global step 500 Train loss 1.09 Classification-F1 0.46843853820598 on epoch=249
06/19/2022 23:02:05 - INFO - __main__ - Step 510 Global step 510 Train loss 1.10 on epoch=254
06/19/2022 23:02:06 - INFO - __main__ - Step 520 Global step 520 Train loss 1.02 on epoch=259
06/19/2022 23:02:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.95 on epoch=264
06/19/2022 23:02:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.94 on epoch=269
06/19/2022 23:02:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.99 on epoch=274
06/19/2022 23:02:10 - INFO - __main__ - Global step 550 Train loss 1.00 Classification-F1 0.3992490613266583 on epoch=274
06/19/2022 23:02:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.84 on epoch=279
06/19/2022 23:02:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.90 on epoch=284
06/19/2022 23:02:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.93 on epoch=289
06/19/2022 23:02:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.78 on epoch=294
06/19/2022 23:02:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.77 on epoch=299
06/19/2022 23:02:17 - INFO - __main__ - Global step 600 Train loss 0.85 Classification-F1 0.4682306940371457 on epoch=299
06/19/2022 23:02:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.85 on epoch=304
06/19/2022 23:02:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.88 on epoch=309
06/19/2022 23:02:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.81 on epoch=314
06/19/2022 23:02:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.82 on epoch=319
06/19/2022 23:02:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.89 on epoch=324
06/19/2022 23:02:23 - INFO - __main__ - Global step 650 Train loss 0.85 Classification-F1 0.6875 on epoch=324
06/19/2022 23:02:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5307917888563051 -> 0.6875 on epoch=324, global_step=650
06/19/2022 23:02:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.71 on epoch=329
06/19/2022 23:02:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.76 on epoch=334
06/19/2022 23:02:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.87 on epoch=339
06/19/2022 23:02:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.80 on epoch=344
06/19/2022 23:02:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.74 on epoch=349
06/19/2022 23:02:29 - INFO - __main__ - Global step 700 Train loss 0.78 Classification-F1 0.4666666666666667 on epoch=349
06/19/2022 23:02:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.78 on epoch=354
06/19/2022 23:02:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.78 on epoch=359
06/19/2022 23:02:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.68 on epoch=364
06/19/2022 23:02:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.78 on epoch=369
06/19/2022 23:02:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.68 on epoch=374
06/19/2022 23:02:36 - INFO - __main__ - Global step 750 Train loss 0.74 Classification-F1 0.3816425120772947 on epoch=374
06/19/2022 23:02:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.69 on epoch=379
06/19/2022 23:02:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.66 on epoch=384
06/19/2022 23:02:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.72 on epoch=389
06/19/2022 23:02:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.67 on epoch=394
06/19/2022 23:02:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.68 on epoch=399
06/19/2022 23:02:42 - INFO - __main__ - Global step 800 Train loss 0.69 Classification-F1 0.3522267206477733 on epoch=399
06/19/2022 23:02:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.64 on epoch=404
06/19/2022 23:02:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.59 on epoch=409
06/19/2022 23:02:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.72 on epoch=414
06/19/2022 23:02:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.64 on epoch=419
06/19/2022 23:02:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.68 on epoch=424
06/19/2022 23:02:49 - INFO - __main__ - Global step 850 Train loss 0.66 Classification-F1 0.3992490613266583 on epoch=424
06/19/2022 23:02:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.71 on epoch=429
06/19/2022 23:02:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.74 on epoch=434
06/19/2022 23:02:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.68 on epoch=439
06/19/2022 23:02:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.62 on epoch=444
06/19/2022 23:02:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.67 on epoch=449
06/19/2022 23:02:55 - INFO - __main__ - Global step 900 Train loss 0.68 Classification-F1 0.539313399778516 on epoch=449
06/19/2022 23:02:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.72 on epoch=454
06/19/2022 23:02:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.68 on epoch=459
06/19/2022 23:02:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.60 on epoch=464
06/19/2022 23:03:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.66 on epoch=469
06/19/2022 23:03:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.64 on epoch=474
06/19/2022 23:03:02 - INFO - __main__ - Global step 950 Train loss 0.66 Classification-F1 0.28888888888888886 on epoch=474
06/19/2022 23:03:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.61 on epoch=479
06/19/2022 23:03:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.67 on epoch=484
06/19/2022 23:03:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.63 on epoch=489
06/19/2022 23:03:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.59 on epoch=494
06/19/2022 23:03:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.58 on epoch=499
06/19/2022 23:03:08 - INFO - __main__ - Global step 1000 Train loss 0.62 Classification-F1 0.5307917888563051 on epoch=499
06/19/2022 23:03:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.64 on epoch=504
06/19/2022 23:03:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.63 on epoch=509
06/19/2022 23:03:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.63 on epoch=514
06/19/2022 23:03:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.60 on epoch=519
06/19/2022 23:03:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.60 on epoch=524
06/19/2022 23:03:15 - INFO - __main__ - Global step 1050 Train loss 0.62 Classification-F1 0.464039408866995 on epoch=524
06/19/2022 23:03:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.56 on epoch=529
06/19/2022 23:03:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.66 on epoch=534
06/19/2022 23:03:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.58 on epoch=539
06/19/2022 23:03:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.55 on epoch=544
06/19/2022 23:03:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.52 on epoch=549
06/19/2022 23:03:21 - INFO - __main__ - Global step 1100 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 23:03:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.57 on epoch=554
06/19/2022 23:03:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.53 on epoch=559
06/19/2022 23:03:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.60 on epoch=564
06/19/2022 23:03:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=569
06/19/2022 23:03:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.50 on epoch=574
06/19/2022 23:03:28 - INFO - __main__ - Global step 1150 Train loss 0.53 Classification-F1 0.4458874458874459 on epoch=574
06/19/2022 23:03:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=579
06/19/2022 23:03:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.55 on epoch=584
06/19/2022 23:03:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.59 on epoch=589
06/19/2022 23:03:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.61 on epoch=594
06/19/2022 23:03:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.64 on epoch=599
06/19/2022 23:03:34 - INFO - __main__ - Global step 1200 Train loss 0.58 Classification-F1 0.4420512820512821 on epoch=599
06/19/2022 23:03:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.51 on epoch=604
06/19/2022 23:03:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=609
06/19/2022 23:03:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.53 on epoch=614
06/19/2022 23:03:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=619
06/19/2022 23:03:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.56 on epoch=624
06/19/2022 23:03:41 - INFO - __main__ - Global step 1250 Train loss 0.51 Classification-F1 0.5333333333333333 on epoch=624
06/19/2022 23:03:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.53 on epoch=629
06/19/2022 23:03:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.50 on epoch=634
06/19/2022 23:03:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.58 on epoch=639
06/19/2022 23:03:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.53 on epoch=644
06/19/2022 23:03:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.50 on epoch=649
06/19/2022 23:03:47 - INFO - __main__ - Global step 1300 Train loss 0.53 Classification-F1 0.3191489361702127 on epoch=649
06/19/2022 23:03:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=654
06/19/2022 23:03:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=659
06/19/2022 23:03:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.50 on epoch=664
06/19/2022 23:03:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.58 on epoch=669
06/19/2022 23:03:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.56 on epoch=674
06/19/2022 23:03:54 - INFO - __main__ - Global step 1350 Train loss 0.52 Classification-F1 0.3816425120772947 on epoch=674
06/19/2022 23:03:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.52 on epoch=679
06/19/2022 23:03:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.57 on epoch=684
06/19/2022 23:03:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.51 on epoch=689
06/19/2022 23:03:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.48 on epoch=694
06/19/2022 23:04:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=699
06/19/2022 23:04:00 - INFO - __main__ - Global step 1400 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 23:04:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.52 on epoch=704
06/19/2022 23:04:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.55 on epoch=709
06/19/2022 23:04:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=714
06/19/2022 23:04:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.61 on epoch=719
06/19/2022 23:04:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.53 on epoch=724
06/19/2022 23:04:07 - INFO - __main__ - Global step 1450 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 23:04:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.56 on epoch=729
06/19/2022 23:04:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.57 on epoch=734
06/19/2022 23:04:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.54 on epoch=739
06/19/2022 23:04:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.47 on epoch=744
06/19/2022 23:04:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.51 on epoch=749
06/19/2022 23:04:13 - INFO - __main__ - Global step 1500 Train loss 0.53 Classification-F1 0.3043478260869565 on epoch=749
06/19/2022 23:04:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.55 on epoch=754
06/19/2022 23:04:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.47 on epoch=759
06/19/2022 23:04:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.48 on epoch=764
06/19/2022 23:04:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.53 on epoch=769
06/19/2022 23:04:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.53 on epoch=774
06/19/2022 23:04:20 - INFO - __main__ - Global step 1550 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 23:04:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.51 on epoch=779
06/19/2022 23:04:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.51 on epoch=784
06/19/2022 23:04:23 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=789
06/19/2022 23:04:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=794
06/19/2022 23:04:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=799
06/19/2022 23:04:26 - INFO - __main__ - Global step 1600 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=799
06/19/2022 23:04:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=804
06/19/2022 23:04:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.47 on epoch=809
06/19/2022 23:04:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.52 on epoch=814
06/19/2022 23:04:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.49 on epoch=819
06/19/2022 23:04:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=824
06/19/2022 23:04:33 - INFO - __main__ - Global step 1650 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=824
06/19/2022 23:04:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.58 on epoch=829
06/19/2022 23:04:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.49 on epoch=834
06/19/2022 23:04:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=839
06/19/2022 23:04:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.44 on epoch=844
06/19/2022 23:04:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=849
06/19/2022 23:04:39 - INFO - __main__ - Global step 1700 Train loss 0.48 Classification-F1 0.4589371980676329 on epoch=849
06/19/2022 23:04:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.54 on epoch=854
06/19/2022 23:04:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=859
06/19/2022 23:04:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.45 on epoch=864
06/19/2022 23:04:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.46 on epoch=869
06/19/2022 23:04:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.46 on epoch=874
06/19/2022 23:04:45 - INFO - __main__ - Global step 1750 Train loss 0.47 Classification-F1 0.464039408866995 on epoch=874
06/19/2022 23:04:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=879
06/19/2022 23:04:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.46 on epoch=884
06/19/2022 23:04:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.47 on epoch=889
06/19/2022 23:04:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.52 on epoch=894
06/19/2022 23:04:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.48 on epoch=899
06/19/2022 23:04:52 - INFO - __main__ - Global step 1800 Train loss 0.47 Classification-F1 0.5901477832512315 on epoch=899
06/19/2022 23:04:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=904
06/19/2022 23:04:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.53 on epoch=909
06/19/2022 23:04:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=914
06/19/2022 23:04:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.51 on epoch=919
06/19/2022 23:04:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.44 on epoch=924
06/19/2022 23:04:58 - INFO - __main__ - Global step 1850 Train loss 0.47 Classification-F1 0.3992490613266583 on epoch=924
06/19/2022 23:05:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.55 on epoch=929
06/19/2022 23:05:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.46 on epoch=934
06/19/2022 23:05:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=939
06/19/2022 23:05:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=944
06/19/2022 23:05:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=949
06/19/2022 23:05:05 - INFO - __main__ - Global step 1900 Train loss 0.46 Classification-F1 0.3043478260869565 on epoch=949
06/19/2022 23:05:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.51 on epoch=954
06/19/2022 23:05:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.52 on epoch=959
06/19/2022 23:05:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=964
06/19/2022 23:05:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.47 on epoch=969
06/19/2022 23:05:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.47 on epoch=974
06/19/2022 23:05:11 - INFO - __main__ - Global step 1950 Train loss 0.48 Classification-F1 0.36374269005847953 on epoch=974
06/19/2022 23:05:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
06/19/2022 23:05:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.52 on epoch=984
06/19/2022 23:05:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=989
06/19/2022 23:05:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.50 on epoch=994
06/19/2022 23:05:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=999
06/19/2022 23:05:18 - INFO - __main__ - Global step 2000 Train loss 0.46 Classification-F1 0.5134502923976608 on epoch=999
06/19/2022 23:05:18 - INFO - __main__ - save last model!
06/19/2022 23:05:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 23:05:18 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 23:05:18 - INFO - __main__ - Printing 3 examples
06/19/2022 23:05:18 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 23:05:18 - INFO - __main__ - ['0']
06/19/2022 23:05:18 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 23:05:18 - INFO - __main__ - ['1']
06/19/2022 23:05:18 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 23:05:18 - INFO - __main__ - ['1']
06/19/2022 23:05:18 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 23:05:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:05:18 - INFO - __main__ - Printing 3 examples
06/19/2022 23:05:18 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 23:05:18 - INFO - __main__ - ['0']
06/19/2022 23:05:18 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 23:05:18 - INFO - __main__ - ['0']
06/19/2022 23:05:18 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 23:05:18 - INFO - __main__ - ['0']
06/19/2022 23:05:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 23:05:18 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:05:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 23:05:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:05:18 - INFO - __main__ - Printing 3 examples
06/19/2022 23:05:18 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 23:05:18 - INFO - __main__ - ['0']
06/19/2022 23:05:18 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 23:05:18 - INFO - __main__ - ['0']
06/19/2022 23:05:18 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 23:05:18 - INFO - __main__ - ['0']
06/19/2022 23:05:18 - INFO - __main__ - Tokenizing Input ...
06/19/2022 23:05:18 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:05:18 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 23:05:22 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:05:24 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 23:05:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 23:05:24 - INFO - __main__ - Starting training!
06/19/2022 23:05:30 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 23:06:56 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_87_0.5_8_predictions.txt
06/19/2022 23:06:56 - INFO - __main__ - Classification-F1 on test data: 0.4087
06/19/2022 23:06:57 - INFO - __main__ - prefix=paws_16_87, lr=0.5, bsz=8, dev_performance=0.6875, test_performance=0.4087410066827034
06/19/2022 23:06:57 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.4, bsz=8 ...
06/19/2022 23:06:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:06:58 - INFO - __main__ - Printing 3 examples
06/19/2022 23:06:58 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 23:06:58 - INFO - __main__ - ['0']
06/19/2022 23:06:58 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 23:06:58 - INFO - __main__ - ['0']
06/19/2022 23:06:58 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 23:06:58 - INFO - __main__ - ['0']
06/19/2022 23:06:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 23:06:58 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:06:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 23:06:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:06:58 - INFO - __main__ - Printing 3 examples
06/19/2022 23:06:58 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 23:06:58 - INFO - __main__ - ['0']
06/19/2022 23:06:58 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 23:06:58 - INFO - __main__ - ['0']
06/19/2022 23:06:58 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 23:06:58 - INFO - __main__ - ['0']
06/19/2022 23:06:58 - INFO - __main__ - Tokenizing Input ...
06/19/2022 23:06:58 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:06:58 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 23:07:03 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 23:07:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 23:07:04 - INFO - __main__ - Starting training!
06/19/2022 23:07:05 - INFO - __main__ - Step 10 Global step 10 Train loss 5.65 on epoch=4
06/19/2022 23:07:06 - INFO - __main__ - Step 20 Global step 20 Train loss 5.40 on epoch=9
06/19/2022 23:07:07 - INFO - __main__ - Step 30 Global step 30 Train loss 4.73 on epoch=14
06/19/2022 23:07:09 - INFO - __main__ - Step 40 Global step 40 Train loss 4.38 on epoch=19
06/19/2022 23:07:10 - INFO - __main__ - Step 50 Global step 50 Train loss 4.15 on epoch=24
06/19/2022 23:07:11 - INFO - __main__ - Global step 50 Train loss 4.86 Classification-F1 0.0 on epoch=24
06/19/2022 23:07:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 23:07:12 - INFO - __main__ - Step 60 Global step 60 Train loss 3.99 on epoch=29
06/19/2022 23:07:13 - INFO - __main__ - Step 70 Global step 70 Train loss 3.68 on epoch=34
06/19/2022 23:07:15 - INFO - __main__ - Step 80 Global step 80 Train loss 3.76 on epoch=39
06/19/2022 23:07:16 - INFO - __main__ - Step 90 Global step 90 Train loss 3.65 on epoch=44
06/19/2022 23:07:17 - INFO - __main__ - Step 100 Global step 100 Train loss 3.58 on epoch=49
06/19/2022 23:07:19 - INFO - __main__ - Global step 100 Train loss 3.73 Classification-F1 0.0 on epoch=49
06/19/2022 23:07:20 - INFO - __main__ - Step 110 Global step 110 Train loss 3.40 on epoch=54
06/19/2022 23:07:21 - INFO - __main__ - Step 120 Global step 120 Train loss 3.15 on epoch=59
06/19/2022 23:07:22 - INFO - __main__ - Step 130 Global step 130 Train loss 3.07 on epoch=64
06/19/2022 23:07:24 - INFO - __main__ - Step 140 Global step 140 Train loss 2.98 on epoch=69
06/19/2022 23:07:25 - INFO - __main__ - Step 150 Global step 150 Train loss 2.99 on epoch=74
06/19/2022 23:07:26 - INFO - __main__ - Global step 150 Train loss 3.12 Classification-F1 0.09032258064516129 on epoch=74
06/19/2022 23:07:26 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.09032258064516129 on epoch=74, global_step=150
06/19/2022 23:07:27 - INFO - __main__ - Step 160 Global step 160 Train loss 2.63 on epoch=79
06/19/2022 23:07:28 - INFO - __main__ - Step 170 Global step 170 Train loss 2.61 on epoch=84
06/19/2022 23:07:29 - INFO - __main__ - Step 180 Global step 180 Train loss 2.53 on epoch=89
06/19/2022 23:07:31 - INFO - __main__ - Step 190 Global step 190 Train loss 2.31 on epoch=94
06/19/2022 23:07:32 - INFO - __main__ - Step 200 Global step 200 Train loss 2.20 on epoch=99
06/19/2022 23:07:32 - INFO - __main__ - Global step 200 Train loss 2.45 Classification-F1 0.3043478260869565 on epoch=99
06/19/2022 23:07:32 - INFO - __main__ - Saving model with best Classification-F1: 0.09032258064516129 -> 0.3043478260869565 on epoch=99, global_step=200
06/19/2022 23:07:34 - INFO - __main__ - Step 210 Global step 210 Train loss 2.16 on epoch=104
06/19/2022 23:07:35 - INFO - __main__ - Step 220 Global step 220 Train loss 2.21 on epoch=109
06/19/2022 23:07:36 - INFO - __main__ - Step 230 Global step 230 Train loss 1.97 on epoch=114
06/19/2022 23:07:37 - INFO - __main__ - Step 240 Global step 240 Train loss 1.94 on epoch=119
06/19/2022 23:07:38 - INFO - __main__ - Step 250 Global step 250 Train loss 2.08 on epoch=124
06/19/2022 23:07:40 - INFO - __main__ - Global step 250 Train loss 2.07 Classification-F1 0.4554554554554554 on epoch=124
06/19/2022 23:07:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3043478260869565 -> 0.4554554554554554 on epoch=124, global_step=250
06/19/2022 23:07:41 - INFO - __main__ - Step 260 Global step 260 Train loss 1.93 on epoch=129
06/19/2022 23:07:42 - INFO - __main__ - Step 270 Global step 270 Train loss 1.76 on epoch=134
06/19/2022 23:07:43 - INFO - __main__ - Step 280 Global step 280 Train loss 1.72 on epoch=139
06/19/2022 23:07:45 - INFO - __main__ - Step 290 Global step 290 Train loss 1.71 on epoch=144
06/19/2022 23:07:46 - INFO - __main__ - Step 300 Global step 300 Train loss 1.74 on epoch=149
06/19/2022 23:07:46 - INFO - __main__ - Global step 300 Train loss 1.77 Classification-F1 0.2873806998939555 on epoch=149
06/19/2022 23:07:47 - INFO - __main__ - Step 310 Global step 310 Train loss 1.65 on epoch=154
06/19/2022 23:07:49 - INFO - __main__ - Step 320 Global step 320 Train loss 1.50 on epoch=159
06/19/2022 23:07:50 - INFO - __main__ - Step 330 Global step 330 Train loss 1.52 on epoch=164
06/19/2022 23:07:51 - INFO - __main__ - Step 340 Global step 340 Train loss 1.35 on epoch=169
06/19/2022 23:07:52 - INFO - __main__ - Step 350 Global step 350 Train loss 1.45 on epoch=174
06/19/2022 23:07:53 - INFO - __main__ - Global step 350 Train loss 1.50 Classification-F1 0.4385964912280702 on epoch=174
06/19/2022 23:07:54 - INFO - __main__ - Step 360 Global step 360 Train loss 1.32 on epoch=179
06/19/2022 23:07:55 - INFO - __main__ - Step 370 Global step 370 Train loss 1.43 on epoch=184
06/19/2022 23:07:56 - INFO - __main__ - Step 380 Global step 380 Train loss 1.41 on epoch=189
06/19/2022 23:07:58 - INFO - __main__ - Step 390 Global step 390 Train loss 1.34 on epoch=194
06/19/2022 23:07:59 - INFO - __main__ - Step 400 Global step 400 Train loss 1.21 on epoch=199
06/19/2022 23:07:59 - INFO - __main__ - Global step 400 Train loss 1.34 Classification-F1 0.3333333333333333 on epoch=199
06/19/2022 23:08:00 - INFO - __main__ - Step 410 Global step 410 Train loss 1.44 on epoch=204
06/19/2022 23:08:02 - INFO - __main__ - Step 420 Global step 420 Train loss 1.28 on epoch=209
06/19/2022 23:08:03 - INFO - __main__ - Step 430 Global step 430 Train loss 1.25 on epoch=214
06/19/2022 23:08:04 - INFO - __main__ - Step 440 Global step 440 Train loss 1.31 on epoch=219
06/19/2022 23:08:05 - INFO - __main__ - Step 450 Global step 450 Train loss 1.25 on epoch=224
06/19/2022 23:08:06 - INFO - __main__ - Global step 450 Train loss 1.30 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 23:08:07 - INFO - __main__ - Step 460 Global step 460 Train loss 1.37 on epoch=229
06/19/2022 23:08:08 - INFO - __main__ - Step 470 Global step 470 Train loss 1.13 on epoch=234
06/19/2022 23:08:09 - INFO - __main__ - Step 480 Global step 480 Train loss 1.34 on epoch=239
06/19/2022 23:08:10 - INFO - __main__ - Step 490 Global step 490 Train loss 1.02 on epoch=244
06/19/2022 23:08:12 - INFO - __main__ - Step 500 Global step 500 Train loss 1.11 on epoch=249
06/19/2022 23:08:12 - INFO - __main__ - Global step 500 Train loss 1.19 Classification-F1 0.3333333333333333 on epoch=249
06/19/2022 23:08:13 - INFO - __main__ - Step 510 Global step 510 Train loss 1.14 on epoch=254
06/19/2022 23:08:14 - INFO - __main__ - Step 520 Global step 520 Train loss 1.06 on epoch=259
06/19/2022 23:08:16 - INFO - __main__ - Step 530 Global step 530 Train loss 1.09 on epoch=264
06/19/2022 23:08:17 - INFO - __main__ - Step 540 Global step 540 Train loss 1.15 on epoch=269
06/19/2022 23:08:18 - INFO - __main__ - Step 550 Global step 550 Train loss 1.16 on epoch=274
06/19/2022 23:08:18 - INFO - __main__ - Global step 550 Train loss 1.12 Classification-F1 0.4589371980676329 on epoch=274
06/19/2022 23:08:18 - INFO - __main__ - Saving model with best Classification-F1: 0.4554554554554554 -> 0.4589371980676329 on epoch=274, global_step=550
06/19/2022 23:08:20 - INFO - __main__ - Step 560 Global step 560 Train loss 1.01 on epoch=279
06/19/2022 23:08:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.96 on epoch=284
06/19/2022 23:08:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.87 on epoch=289
06/19/2022 23:08:23 - INFO - __main__ - Step 590 Global step 590 Train loss 1.01 on epoch=294
06/19/2022 23:08:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.89 on epoch=299
06/19/2022 23:08:25 - INFO - __main__ - Global step 600 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=299
06/19/2022 23:08:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.85 on epoch=304
06/19/2022 23:08:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.85 on epoch=309
06/19/2022 23:08:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.86 on epoch=314
06/19/2022 23:08:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.93 on epoch=319
06/19/2022 23:08:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.82 on epoch=324
06/19/2022 23:08:31 - INFO - __main__ - Global step 650 Train loss 0.86 Classification-F1 0.3333333333333333 on epoch=324
06/19/2022 23:08:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.93 on epoch=329
06/19/2022 23:08:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.94 on epoch=334
06/19/2022 23:08:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.83 on epoch=339
06/19/2022 23:08:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.93 on epoch=344
06/19/2022 23:08:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.76 on epoch=349
06/19/2022 23:08:38 - INFO - __main__ - Global step 700 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=349
06/19/2022 23:08:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.79 on epoch=354
06/19/2022 23:08:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.78 on epoch=359
06/19/2022 23:08:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.78 on epoch=364
06/19/2022 23:08:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.84 on epoch=369
06/19/2022 23:08:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.86 on epoch=374
06/19/2022 23:08:44 - INFO - __main__ - Global step 750 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=374
06/19/2022 23:08:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.78 on epoch=379
06/19/2022 23:08:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.77 on epoch=384
06/19/2022 23:08:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.70 on epoch=389
06/19/2022 23:08:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.75 on epoch=394
06/19/2022 23:08:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.82 on epoch=399
06/19/2022 23:08:51 - INFO - __main__ - Global step 800 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=399
06/19/2022 23:08:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.72 on epoch=404
06/19/2022 23:08:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.76 on epoch=409
06/19/2022 23:08:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.83 on epoch=414
06/19/2022 23:08:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.74 on epoch=419
06/19/2022 23:08:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.69 on epoch=424
06/19/2022 23:08:57 - INFO - __main__ - Global step 850 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=424
06/19/2022 23:08:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.82 on epoch=429
06/19/2022 23:09:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.66 on epoch=434
06/19/2022 23:09:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.70 on epoch=439
06/19/2022 23:09:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.75 on epoch=444
06/19/2022 23:09:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.71 on epoch=449
06/19/2022 23:09:04 - INFO - __main__ - Global step 900 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=449
06/19/2022 23:09:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.79 on epoch=454
06/19/2022 23:09:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.70 on epoch=459
06/19/2022 23:09:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.74 on epoch=464
06/19/2022 23:09:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.73 on epoch=469
06/19/2022 23:09:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.73 on epoch=474
06/19/2022 23:09:10 - INFO - __main__ - Global step 950 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=474
06/19/2022 23:09:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.64 on epoch=479
06/19/2022 23:09:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.62 on epoch=484
06/19/2022 23:09:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.75 on epoch=489
06/19/2022 23:09:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.56 on epoch=494
06/19/2022 23:09:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.67 on epoch=499
06/19/2022 23:09:16 - INFO - __main__ - Global step 1000 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=499
06/19/2022 23:09:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.67 on epoch=504
06/19/2022 23:09:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.61 on epoch=509
06/19/2022 23:09:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.70 on epoch=514
06/19/2022 23:09:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.66 on epoch=519
06/19/2022 23:09:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.65 on epoch=524
06/19/2022 23:09:23 - INFO - __main__ - Global step 1050 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=524
06/19/2022 23:09:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.65 on epoch=529
06/19/2022 23:09:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.66 on epoch=534
06/19/2022 23:09:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.59 on epoch=539
06/19/2022 23:09:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.71 on epoch=544
06/19/2022 23:09:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.54 on epoch=549
06/19/2022 23:09:29 - INFO - __main__ - Global step 1100 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=549
06/19/2022 23:09:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.57 on epoch=554
06/19/2022 23:09:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.55 on epoch=559
06/19/2022 23:09:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.54 on epoch=564
06/19/2022 23:09:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.64 on epoch=569
06/19/2022 23:09:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.54 on epoch=574
06/19/2022 23:09:36 - INFO - __main__ - Global step 1150 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 23:09:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.59 on epoch=579
06/19/2022 23:09:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.55 on epoch=584
06/19/2022 23:09:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.60 on epoch=589
06/19/2022 23:09:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.63 on epoch=594
06/19/2022 23:09:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.63 on epoch=599
06/19/2022 23:09:42 - INFO - __main__ - Global step 1200 Train loss 0.60 Classification-F1 0.46843853820598 on epoch=599
06/19/2022 23:09:42 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.46843853820598 on epoch=599, global_step=1200
06/19/2022 23:09:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.57 on epoch=604
06/19/2022 23:09:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.49 on epoch=609
06/19/2022 23:09:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.54 on epoch=614
06/19/2022 23:09:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.55 on epoch=619
06/19/2022 23:09:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.62 on epoch=624
06/19/2022 23:09:48 - INFO - __main__ - Global step 1250 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 23:09:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.64 on epoch=629
06/19/2022 23:09:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.61 on epoch=634
06/19/2022 23:09:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.56 on epoch=639
06/19/2022 23:09:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.57 on epoch=644
06/19/2022 23:09:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.55 on epoch=649
06/19/2022 23:09:55 - INFO - __main__ - Global step 1300 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=649
06/19/2022 23:09:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=654
06/19/2022 23:09:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.49 on epoch=659
06/19/2022 23:09:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.55 on epoch=664
06/19/2022 23:10:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.60 on epoch=669
06/19/2022 23:10:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.51 on epoch=674
06/19/2022 23:10:01 - INFO - __main__ - Global step 1350 Train loss 0.53 Classification-F1 0.3992490613266583 on epoch=674
06/19/2022 23:10:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.55 on epoch=679
06/19/2022 23:10:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.64 on epoch=684
06/19/2022 23:10:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=689
06/19/2022 23:10:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.50 on epoch=694
06/19/2022 23:10:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.59 on epoch=699
06/19/2022 23:10:08 - INFO - __main__ - Global step 1400 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=699
06/19/2022 23:10:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.52 on epoch=704
06/19/2022 23:10:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.52 on epoch=709
06/19/2022 23:10:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.56 on epoch=714
06/19/2022 23:10:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.60 on epoch=719
06/19/2022 23:10:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.59 on epoch=724
06/19/2022 23:10:14 - INFO - __main__ - Global step 1450 Train loss 0.56 Classification-F1 0.5465587044534412 on epoch=724
06/19/2022 23:10:14 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5465587044534412 on epoch=724, global_step=1450
06/19/2022 23:10:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
06/19/2022 23:10:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=734
06/19/2022 23:10:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.62 on epoch=739
06/19/2022 23:10:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.57 on epoch=744
06/19/2022 23:10:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.58 on epoch=749
06/19/2022 23:10:21 - INFO - __main__ - Global step 1500 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 23:10:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.55 on epoch=754
06/19/2022 23:10:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.59 on epoch=759
06/19/2022 23:10:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.55 on epoch=764
06/19/2022 23:10:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.52 on epoch=769
06/19/2022 23:10:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=774
06/19/2022 23:10:27 - INFO - __main__ - Global step 1550 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 23:10:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.52 on epoch=779
06/19/2022 23:10:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=784
06/19/2022 23:10:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=789
06/19/2022 23:10:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.53 on epoch=794
06/19/2022 23:10:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.56 on epoch=799
06/19/2022 23:10:33 - INFO - __main__ - Global step 1600 Train loss 0.52 Classification-F1 0.6532019704433498 on epoch=799
06/19/2022 23:10:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5465587044534412 -> 0.6532019704433498 on epoch=799, global_step=1600
06/19/2022 23:10:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.58 on epoch=804
06/19/2022 23:10:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.51 on epoch=809
06/19/2022 23:10:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.57 on epoch=814
06/19/2022 23:10:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=819
06/19/2022 23:10:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=824
06/19/2022 23:10:40 - INFO - __main__ - Global step 1650 Train loss 0.51 Classification-F1 0.5636363636363637 on epoch=824
06/19/2022 23:10:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.51 on epoch=829
06/19/2022 23:10:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.48 on epoch=834
06/19/2022 23:10:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.54 on epoch=839
06/19/2022 23:10:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.53 on epoch=844
06/19/2022 23:10:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.51 on epoch=849
06/19/2022 23:10:46 - INFO - __main__ - Global step 1700 Train loss 0.52 Classification-F1 0.6536796536796536 on epoch=849
06/19/2022 23:10:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6532019704433498 -> 0.6536796536796536 on epoch=849, global_step=1700
06/19/2022 23:10:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=854
06/19/2022 23:10:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.52 on epoch=859
06/19/2022 23:10:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.50 on epoch=864
06/19/2022 23:10:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.47 on epoch=869
06/19/2022 23:10:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=874
06/19/2022 23:10:53 - INFO - __main__ - Global step 1750 Train loss 0.48 Classification-F1 0.3992490613266583 on epoch=874
06/19/2022 23:10:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=879
06/19/2022 23:10:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.48 on epoch=884
06/19/2022 23:10:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.51 on epoch=889
06/19/2022 23:10:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=894
06/19/2022 23:10:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.53 on epoch=899
06/19/2022 23:10:59 - INFO - __main__ - Global step 1800 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 23:11:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=904
06/19/2022 23:11:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.49 on epoch=909
06/19/2022 23:11:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.48 on epoch=914
06/19/2022 23:11:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.48 on epoch=919
06/19/2022 23:11:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.51 on epoch=924
06/19/2022 23:11:06 - INFO - __main__ - Global step 1850 Train loss 0.47 Classification-F1 0.3992490613266583 on epoch=924
06/19/2022 23:11:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.48 on epoch=929
06/19/2022 23:11:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.52 on epoch=934
06/19/2022 23:11:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=939
06/19/2022 23:11:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.50 on epoch=944
06/19/2022 23:11:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=949
06/19/2022 23:11:12 - INFO - __main__ - Global step 1900 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=949
06/19/2022 23:11:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.53 on epoch=954
06/19/2022 23:11:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.48 on epoch=959
06/19/2022 23:11:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=964
06/19/2022 23:11:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.50 on epoch=969
06/19/2022 23:11:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.48 on epoch=974
06/19/2022 23:11:19 - INFO - __main__ - Global step 1950 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 23:11:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.49 on epoch=979
06/19/2022 23:11:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.48 on epoch=984
06/19/2022 23:11:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.48 on epoch=989
06/19/2022 23:11:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.44 on epoch=994
06/19/2022 23:11:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=999
06/19/2022 23:11:25 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 23:11:25 - INFO - __main__ - save last model!
06/19/2022 23:11:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 23:11:25 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 23:11:25 - INFO - __main__ - Printing 3 examples
06/19/2022 23:11:25 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 23:11:25 - INFO - __main__ - ['0']
06/19/2022 23:11:25 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 23:11:25 - INFO - __main__ - ['1']
06/19/2022 23:11:25 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 23:11:25 - INFO - __main__ - ['1']
06/19/2022 23:11:25 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 23:11:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:11:26 - INFO - __main__ - Printing 3 examples
06/19/2022 23:11:26 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 23:11:26 - INFO - __main__ - ['0']
06/19/2022 23:11:26 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 23:11:26 - INFO - __main__ - ['0']
06/19/2022 23:11:26 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 23:11:26 - INFO - __main__ - ['0']
06/19/2022 23:11:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 23:11:26 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:11:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 23:11:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:11:26 - INFO - __main__ - Printing 3 examples
06/19/2022 23:11:26 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 23:11:26 - INFO - __main__ - ['0']
06/19/2022 23:11:26 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 23:11:26 - INFO - __main__ - ['0']
06/19/2022 23:11:26 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 23:11:26 - INFO - __main__ - ['0']
06/19/2022 23:11:26 - INFO - __main__ - Tokenizing Input ...
06/19/2022 23:11:26 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:11:26 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 23:11:29 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:11:31 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 23:11:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 23:11:32 - INFO - __main__ - Starting training!
06/19/2022 23:11:37 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 23:13:01 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_87_0.4_8_predictions.txt
06/19/2022 23:13:01 - INFO - __main__ - Classification-F1 on test data: 0.3081
06/19/2022 23:13:02 - INFO - __main__ - prefix=paws_16_87, lr=0.4, bsz=8, dev_performance=0.6536796536796536, test_performance=0.30808093970432643
06/19/2022 23:13:02 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.3, bsz=8 ...
06/19/2022 23:13:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:13:02 - INFO - __main__ - Printing 3 examples
06/19/2022 23:13:02 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 23:13:02 - INFO - __main__ - ['0']
06/19/2022 23:13:02 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 23:13:02 - INFO - __main__ - ['0']
06/19/2022 23:13:02 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 23:13:02 - INFO - __main__ - ['0']
06/19/2022 23:13:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 23:13:02 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:13:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 23:13:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:13:03 - INFO - __main__ - Printing 3 examples
06/19/2022 23:13:03 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 23:13:03 - INFO - __main__ - ['0']
06/19/2022 23:13:03 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 23:13:03 - INFO - __main__ - ['0']
06/19/2022 23:13:03 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 23:13:03 - INFO - __main__ - ['0']
06/19/2022 23:13:03 - INFO - __main__ - Tokenizing Input ...
06/19/2022 23:13:03 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:13:03 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 23:13:08 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 23:13:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 23:13:08 - INFO - __main__ - Starting training!
06/19/2022 23:13:10 - INFO - __main__ - Step 10 Global step 10 Train loss 5.62 on epoch=4
06/19/2022 23:13:11 - INFO - __main__ - Step 20 Global step 20 Train loss 5.45 on epoch=9
06/19/2022 23:13:12 - INFO - __main__ - Step 30 Global step 30 Train loss 5.22 on epoch=14
06/19/2022 23:13:13 - INFO - __main__ - Step 40 Global step 40 Train loss 4.91 on epoch=19
06/19/2022 23:13:15 - INFO - __main__ - Step 50 Global step 50 Train loss 4.36 on epoch=24
06/19/2022 23:13:16 - INFO - __main__ - Global step 50 Train loss 5.11 Classification-F1 0.0 on epoch=24
06/19/2022 23:13:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 23:13:17 - INFO - __main__ - Step 60 Global step 60 Train loss 4.21 on epoch=29
06/19/2022 23:13:18 - INFO - __main__ - Step 70 Global step 70 Train loss 4.22 on epoch=34
06/19/2022 23:13:19 - INFO - __main__ - Step 80 Global step 80 Train loss 3.84 on epoch=39
06/19/2022 23:13:21 - INFO - __main__ - Step 90 Global step 90 Train loss 3.73 on epoch=44
06/19/2022 23:13:22 - INFO - __main__ - Step 100 Global step 100 Train loss 3.63 on epoch=49
06/19/2022 23:13:25 - INFO - __main__ - Global step 100 Train loss 3.92 Classification-F1 0.0 on epoch=49
06/19/2022 23:13:26 - INFO - __main__ - Step 110 Global step 110 Train loss 3.65 on epoch=54
06/19/2022 23:13:27 - INFO - __main__ - Step 120 Global step 120 Train loss 3.41 on epoch=59
06/19/2022 23:13:28 - INFO - __main__ - Step 130 Global step 130 Train loss 3.39 on epoch=64
06/19/2022 23:13:30 - INFO - __main__ - Step 140 Global step 140 Train loss 3.35 on epoch=69
06/19/2022 23:13:31 - INFO - __main__ - Step 150 Global step 150 Train loss 3.18 on epoch=74
06/19/2022 23:13:33 - INFO - __main__ - Global step 150 Train loss 3.40 Classification-F1 0.034482758620689655 on epoch=74
06/19/2022 23:13:33 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.034482758620689655 on epoch=74, global_step=150
06/19/2022 23:13:34 - INFO - __main__ - Step 160 Global step 160 Train loss 3.22 on epoch=79
06/19/2022 23:13:35 - INFO - __main__ - Step 170 Global step 170 Train loss 3.09 on epoch=84
06/19/2022 23:13:37 - INFO - __main__ - Step 180 Global step 180 Train loss 2.95 on epoch=89
06/19/2022 23:13:38 - INFO - __main__ - Step 190 Global step 190 Train loss 2.98 on epoch=94
06/19/2022 23:13:39 - INFO - __main__ - Step 200 Global step 200 Train loss 3.00 on epoch=99
06/19/2022 23:13:45 - INFO - __main__ - Global step 200 Train loss 3.05 Classification-F1 0.09302325581395347 on epoch=99
06/19/2022 23:13:45 - INFO - __main__ - Saving model with best Classification-F1: 0.034482758620689655 -> 0.09302325581395347 on epoch=99, global_step=200
06/19/2022 23:13:46 - INFO - __main__ - Step 210 Global step 210 Train loss 2.81 on epoch=104
06/19/2022 23:13:48 - INFO - __main__ - Step 220 Global step 220 Train loss 2.97 on epoch=109
06/19/2022 23:13:49 - INFO - __main__ - Step 230 Global step 230 Train loss 2.84 on epoch=114
06/19/2022 23:13:50 - INFO - __main__ - Step 240 Global step 240 Train loss 2.86 on epoch=119
06/19/2022 23:13:51 - INFO - __main__ - Step 250 Global step 250 Train loss 2.70 on epoch=124
06/19/2022 23:13:53 - INFO - __main__ - Global step 250 Train loss 2.84 Classification-F1 0.13636363636363635 on epoch=124
06/19/2022 23:13:53 - INFO - __main__ - Saving model with best Classification-F1: 0.09302325581395347 -> 0.13636363636363635 on epoch=124, global_step=250
06/19/2022 23:13:54 - INFO - __main__ - Step 260 Global step 260 Train loss 2.77 on epoch=129
06/19/2022 23:13:56 - INFO - __main__ - Step 270 Global step 270 Train loss 2.65 on epoch=134
06/19/2022 23:13:57 - INFO - __main__ - Step 280 Global step 280 Train loss 2.52 on epoch=139
06/19/2022 23:13:58 - INFO - __main__ - Step 290 Global step 290 Train loss 2.54 on epoch=144
06/19/2022 23:13:59 - INFO - __main__ - Step 300 Global step 300 Train loss 2.77 on epoch=149
06/19/2022 23:14:02 - INFO - __main__ - Global step 300 Train loss 2.65 Classification-F1 0.06625074272133097 on epoch=149
06/19/2022 23:14:03 - INFO - __main__ - Step 310 Global step 310 Train loss 2.69 on epoch=154
06/19/2022 23:14:04 - INFO - __main__ - Step 320 Global step 320 Train loss 2.55 on epoch=159
06/19/2022 23:14:05 - INFO - __main__ - Step 330 Global step 330 Train loss 2.62 on epoch=164
06/19/2022 23:14:07 - INFO - __main__ - Step 340 Global step 340 Train loss 2.62 on epoch=169
06/19/2022 23:14:08 - INFO - __main__ - Step 350 Global step 350 Train loss 2.45 on epoch=174
06/19/2022 23:14:10 - INFO - __main__ - Global step 350 Train loss 2.59 Classification-F1 0.04729729729729729 on epoch=174
06/19/2022 23:14:11 - INFO - __main__ - Step 360 Global step 360 Train loss 2.35 on epoch=179
06/19/2022 23:14:12 - INFO - __main__ - Step 370 Global step 370 Train loss 2.48 on epoch=184
06/19/2022 23:14:14 - INFO - __main__ - Step 380 Global step 380 Train loss 2.48 on epoch=189
06/19/2022 23:14:15 - INFO - __main__ - Step 390 Global step 390 Train loss 2.52 on epoch=194
06/19/2022 23:14:16 - INFO - __main__ - Step 400 Global step 400 Train loss 2.42 on epoch=199
06/19/2022 23:14:18 - INFO - __main__ - Global step 400 Train loss 2.45 Classification-F1 0.12640692640692638 on epoch=199
06/19/2022 23:14:19 - INFO - __main__ - Step 410 Global step 410 Train loss 2.48 on epoch=204
06/19/2022 23:14:20 - INFO - __main__ - Step 420 Global step 420 Train loss 2.39 on epoch=209
06/19/2022 23:14:22 - INFO - __main__ - Step 430 Global step 430 Train loss 2.40 on epoch=214
06/19/2022 23:14:23 - INFO - __main__ - Step 440 Global step 440 Train loss 2.61 on epoch=219
06/19/2022 23:14:24 - INFO - __main__ - Step 450 Global step 450 Train loss 2.58 on epoch=224
06/19/2022 23:14:26 - INFO - __main__ - Global step 450 Train loss 2.49 Classification-F1 0.0909090909090909 on epoch=224
06/19/2022 23:14:27 - INFO - __main__ - Step 460 Global step 460 Train loss 2.72 on epoch=229
06/19/2022 23:14:28 - INFO - __main__ - Step 470 Global step 470 Train loss 2.53 on epoch=234
06/19/2022 23:14:30 - INFO - __main__ - Step 480 Global step 480 Train loss 2.47 on epoch=239
06/19/2022 23:14:31 - INFO - __main__ - Step 490 Global step 490 Train loss 2.33 on epoch=244
06/19/2022 23:14:32 - INFO - __main__ - Step 500 Global step 500 Train loss 2.32 on epoch=249
06/19/2022 23:14:33 - INFO - __main__ - Global step 500 Train loss 2.47 Classification-F1 0.3273273273273273 on epoch=249
06/19/2022 23:14:33 - INFO - __main__ - Saving model with best Classification-F1: 0.13636363636363635 -> 0.3273273273273273 on epoch=249, global_step=500
06/19/2022 23:14:34 - INFO - __main__ - Step 510 Global step 510 Train loss 2.29 on epoch=254
06/19/2022 23:14:35 - INFO - __main__ - Step 520 Global step 520 Train loss 2.24 on epoch=259
06/19/2022 23:14:37 - INFO - __main__ - Step 530 Global step 530 Train loss 2.15 on epoch=264
06/19/2022 23:14:38 - INFO - __main__ - Step 540 Global step 540 Train loss 2.10 on epoch=269
06/19/2022 23:14:39 - INFO - __main__ - Step 550 Global step 550 Train loss 2.19 on epoch=274
06/19/2022 23:14:40 - INFO - __main__ - Global step 550 Train loss 2.19 Classification-F1 0.3454545454545454 on epoch=274
06/19/2022 23:14:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3273273273273273 -> 0.3454545454545454 on epoch=274, global_step=550
06/19/2022 23:14:41 - INFO - __main__ - Step 560 Global step 560 Train loss 2.20 on epoch=279
06/19/2022 23:14:42 - INFO - __main__ - Step 570 Global step 570 Train loss 2.14 on epoch=284
06/19/2022 23:14:44 - INFO - __main__ - Step 580 Global step 580 Train loss 2.08 on epoch=289
06/19/2022 23:14:45 - INFO - __main__ - Step 590 Global step 590 Train loss 2.14 on epoch=294
06/19/2022 23:14:46 - INFO - __main__ - Step 600 Global step 600 Train loss 2.12 on epoch=299
06/19/2022 23:14:46 - INFO - __main__ - Global step 600 Train loss 2.14 Classification-F1 0.4458874458874459 on epoch=299
06/19/2022 23:14:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3454545454545454 -> 0.4458874458874459 on epoch=299, global_step=600
06/19/2022 23:14:48 - INFO - __main__ - Step 610 Global step 610 Train loss 2.15 on epoch=304
06/19/2022 23:14:49 - INFO - __main__ - Step 620 Global step 620 Train loss 2.04 on epoch=309
06/19/2022 23:14:50 - INFO - __main__ - Step 630 Global step 630 Train loss 2.09 on epoch=314
06/19/2022 23:14:51 - INFO - __main__ - Step 640 Global step 640 Train loss 2.10 on epoch=319
06/19/2022 23:14:53 - INFO - __main__ - Step 650 Global step 650 Train loss 1.97 on epoch=324
06/19/2022 23:14:53 - INFO - __main__ - Global step 650 Train loss 2.07 Classification-F1 0.4817813765182186 on epoch=324
06/19/2022 23:14:53 - INFO - __main__ - Saving model with best Classification-F1: 0.4458874458874459 -> 0.4817813765182186 on epoch=324, global_step=650
06/19/2022 23:14:54 - INFO - __main__ - Step 660 Global step 660 Train loss 1.97 on epoch=329
06/19/2022 23:14:56 - INFO - __main__ - Step 670 Global step 670 Train loss 2.08 on epoch=334
06/19/2022 23:14:57 - INFO - __main__ - Step 680 Global step 680 Train loss 1.97 on epoch=339
06/19/2022 23:14:58 - INFO - __main__ - Step 690 Global step 690 Train loss 1.91 on epoch=344
06/19/2022 23:14:59 - INFO - __main__ - Step 700 Global step 700 Train loss 1.89 on epoch=349
06/19/2022 23:15:00 - INFO - __main__ - Global step 700 Train loss 1.97 Classification-F1 0.4682306940371457 on epoch=349
06/19/2022 23:15:01 - INFO - __main__ - Step 710 Global step 710 Train loss 1.89 on epoch=354
06/19/2022 23:15:02 - INFO - __main__ - Step 720 Global step 720 Train loss 1.85 on epoch=359
06/19/2022 23:15:04 - INFO - __main__ - Step 730 Global step 730 Train loss 1.80 on epoch=364
06/19/2022 23:15:05 - INFO - __main__ - Step 740 Global step 740 Train loss 1.91 on epoch=369
06/19/2022 23:15:06 - INFO - __main__ - Step 750 Global step 750 Train loss 1.79 on epoch=374
06/19/2022 23:15:07 - INFO - __main__ - Global step 750 Train loss 1.85 Classification-F1 0.4375 on epoch=374
06/19/2022 23:15:08 - INFO - __main__ - Step 760 Global step 760 Train loss 1.76 on epoch=379
06/19/2022 23:15:09 - INFO - __main__ - Step 770 Global step 770 Train loss 1.76 on epoch=384
06/19/2022 23:15:10 - INFO - __main__ - Step 780 Global step 780 Train loss 1.85 on epoch=389
06/19/2022 23:15:12 - INFO - __main__ - Step 790 Global step 790 Train loss 1.84 on epoch=394
06/19/2022 23:15:13 - INFO - __main__ - Step 800 Global step 800 Train loss 1.76 on epoch=399
06/19/2022 23:15:13 - INFO - __main__ - Global step 800 Train loss 1.79 Classification-F1 0.6000000000000001 on epoch=399
06/19/2022 23:15:13 - INFO - __main__ - Saving model with best Classification-F1: 0.4817813765182186 -> 0.6000000000000001 on epoch=399, global_step=800
06/19/2022 23:15:15 - INFO - __main__ - Step 810 Global step 810 Train loss 1.78 on epoch=404
06/19/2022 23:15:16 - INFO - __main__ - Step 820 Global step 820 Train loss 1.75 on epoch=409
06/19/2022 23:15:17 - INFO - __main__ - Step 830 Global step 830 Train loss 1.84 on epoch=414
06/19/2022 23:15:18 - INFO - __main__ - Step 840 Global step 840 Train loss 1.69 on epoch=419
06/19/2022 23:15:20 - INFO - __main__ - Step 850 Global step 850 Train loss 1.70 on epoch=424
06/19/2022 23:15:20 - INFO - __main__ - Global step 850 Train loss 1.75 Classification-F1 0.3073593073593074 on epoch=424
06/19/2022 23:15:21 - INFO - __main__ - Step 860 Global step 860 Train loss 1.56 on epoch=429
06/19/2022 23:15:23 - INFO - __main__ - Step 870 Global step 870 Train loss 1.59 on epoch=434
06/19/2022 23:15:24 - INFO - __main__ - Step 880 Global step 880 Train loss 1.63 on epoch=439
06/19/2022 23:15:25 - INFO - __main__ - Step 890 Global step 890 Train loss 1.70 on epoch=444
06/19/2022 23:15:26 - INFO - __main__ - Step 900 Global step 900 Train loss 1.77 on epoch=449
06/19/2022 23:15:27 - INFO - __main__ - Global step 900 Train loss 1.65 Classification-F1 0.4231177094379639 on epoch=449
06/19/2022 23:15:28 - INFO - __main__ - Step 910 Global step 910 Train loss 1.57 on epoch=454
06/19/2022 23:15:29 - INFO - __main__ - Step 920 Global step 920 Train loss 1.55 on epoch=459
06/19/2022 23:15:31 - INFO - __main__ - Step 930 Global step 930 Train loss 1.72 on epoch=464
06/19/2022 23:15:32 - INFO - __main__ - Step 940 Global step 940 Train loss 1.57 on epoch=469
06/19/2022 23:15:33 - INFO - __main__ - Step 950 Global step 950 Train loss 1.41 on epoch=474
06/19/2022 23:15:34 - INFO - __main__ - Global step 950 Train loss 1.56 Classification-F1 0.3191489361702127 on epoch=474
06/19/2022 23:15:35 - INFO - __main__ - Step 960 Global step 960 Train loss 1.56 on epoch=479
06/19/2022 23:15:36 - INFO - __main__ - Step 970 Global step 970 Train loss 1.56 on epoch=484
06/19/2022 23:15:37 - INFO - __main__ - Step 980 Global step 980 Train loss 1.58 on epoch=489
06/19/2022 23:15:39 - INFO - __main__ - Step 990 Global step 990 Train loss 1.46 on epoch=494
06/19/2022 23:15:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.35 on epoch=499
06/19/2022 23:15:40 - INFO - __main__ - Global step 1000 Train loss 1.50 Classification-F1 0.36374269005847953 on epoch=499
06/19/2022 23:15:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.43 on epoch=504
06/19/2022 23:15:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.52 on epoch=509
06/19/2022 23:15:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.28 on epoch=514
06/19/2022 23:15:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.27 on epoch=519
06/19/2022 23:15:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.39 on epoch=524
06/19/2022 23:15:47 - INFO - __main__ - Global step 1050 Train loss 1.38 Classification-F1 0.3454545454545454 on epoch=524
06/19/2022 23:15:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.37 on epoch=529
06/19/2022 23:15:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.29 on epoch=534
06/19/2022 23:15:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.15 on epoch=539
06/19/2022 23:15:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.32 on epoch=544
06/19/2022 23:15:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.20 on epoch=549
06/19/2022 23:15:54 - INFO - __main__ - Global step 1100 Train loss 1.27 Classification-F1 0.36374269005847953 on epoch=549
06/19/2022 23:15:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.21 on epoch=554
06/19/2022 23:15:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.16 on epoch=559
06/19/2022 23:15:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.13 on epoch=564
06/19/2022 23:15:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.16 on epoch=569
06/19/2022 23:16:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.19 on epoch=574
06/19/2022 23:16:00 - INFO - __main__ - Global step 1150 Train loss 1.17 Classification-F1 0.3043478260869565 on epoch=574
06/19/2022 23:16:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.13 on epoch=579
06/19/2022 23:16:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.11 on epoch=584
06/19/2022 23:16:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.04 on epoch=589
06/19/2022 23:16:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.17 on epoch=594
06/19/2022 23:16:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.05 on epoch=599
06/19/2022 23:16:07 - INFO - __main__ - Global step 1200 Train loss 1.10 Classification-F1 0.3266888150609081 on epoch=599
06/19/2022 23:16:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.00 on epoch=604
06/19/2022 23:16:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.02 on epoch=609
06/19/2022 23:16:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.99 on epoch=614
06/19/2022 23:16:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.07 on epoch=619
06/19/2022 23:16:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.03 on epoch=624
06/19/2022 23:16:14 - INFO - __main__ - Global step 1250 Train loss 1.02 Classification-F1 0.5270935960591133 on epoch=624
06/19/2022 23:16:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.08 on epoch=629
06/19/2022 23:16:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.97 on epoch=634
06/19/2022 23:16:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.90 on epoch=639
06/19/2022 23:16:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.99 on epoch=644
06/19/2022 23:16:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.98 on epoch=649
06/19/2022 23:16:20 - INFO - __main__ - Global step 1300 Train loss 0.98 Classification-F1 0.5134502923976608 on epoch=649
06/19/2022 23:16:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.98 on epoch=654
06/19/2022 23:16:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.95 on epoch=659
06/19/2022 23:16:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.99 on epoch=664
06/19/2022 23:16:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.00 on epoch=669
06/19/2022 23:16:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.92 on epoch=674
06/19/2022 23:16:27 - INFO - __main__ - Global step 1350 Train loss 0.97 Classification-F1 0.36374269005847953 on epoch=674
06/19/2022 23:16:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.90 on epoch=679
06/19/2022 23:16:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.92 on epoch=684
06/19/2022 23:16:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.94 on epoch=689
06/19/2022 23:16:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.89 on epoch=694
06/19/2022 23:16:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.93 on epoch=699
06/19/2022 23:16:34 - INFO - __main__ - Global step 1400 Train loss 0.91 Classification-F1 0.4181818181818182 on epoch=699
06/19/2022 23:16:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.89 on epoch=704
06/19/2022 23:16:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.86 on epoch=709
06/19/2022 23:16:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.95 on epoch=714
06/19/2022 23:16:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.83 on epoch=719
06/19/2022 23:16:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.97 on epoch=724
06/19/2022 23:16:40 - INFO - __main__ - Global step 1450 Train loss 0.90 Classification-F1 0.39756367663344405 on epoch=724
06/19/2022 23:16:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.89 on epoch=729
06/19/2022 23:16:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.90 on epoch=734
06/19/2022 23:16:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.75 on epoch=739
06/19/2022 23:16:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.78 on epoch=744
06/19/2022 23:16:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.84 on epoch=749
06/19/2022 23:16:47 - INFO - __main__ - Global step 1500 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 23:16:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.81 on epoch=754
06/19/2022 23:16:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.78 on epoch=759
06/19/2022 23:16:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.91 on epoch=764
06/19/2022 23:16:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.85 on epoch=769
06/19/2022 23:16:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.75 on epoch=774
06/19/2022 23:16:54 - INFO - __main__ - Global step 1550 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=774
06/19/2022 23:16:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.75 on epoch=779
06/19/2022 23:16:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.80 on epoch=784
06/19/2022 23:16:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.75 on epoch=789
06/19/2022 23:16:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.74 on epoch=794
06/19/2022 23:17:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.76 on epoch=799
06/19/2022 23:17:00 - INFO - __main__ - Global step 1600 Train loss 0.76 Classification-F1 0.4181818181818182 on epoch=799
06/19/2022 23:17:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.78 on epoch=804
06/19/2022 23:17:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.80 on epoch=809
06/19/2022 23:17:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.74 on epoch=814
06/19/2022 23:17:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.74 on epoch=819
06/19/2022 23:17:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.83 on epoch=824
06/19/2022 23:17:07 - INFO - __main__ - Global step 1650 Train loss 0.78 Classification-F1 0.25581395348837205 on epoch=824
06/19/2022 23:17:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.74 on epoch=829
06/19/2022 23:17:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.74 on epoch=834
06/19/2022 23:17:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.79 on epoch=839
06/19/2022 23:17:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.80 on epoch=844
06/19/2022 23:17:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.72 on epoch=849
06/19/2022 23:17:14 - INFO - __main__ - Global step 1700 Train loss 0.76 Classification-F1 0.28888888888888886 on epoch=849
06/19/2022 23:17:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.81 on epoch=854
06/19/2022 23:17:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.69 on epoch=859
06/19/2022 23:17:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.83 on epoch=864
06/19/2022 23:17:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.70 on epoch=869
06/19/2022 23:17:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.79 on epoch=874
06/19/2022 23:17:21 - INFO - __main__ - Global step 1750 Train loss 0.77 Classification-F1 0.3992490613266583 on epoch=874
06/19/2022 23:17:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.79 on epoch=879
06/19/2022 23:17:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.79 on epoch=884
06/19/2022 23:17:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.68 on epoch=889
06/19/2022 23:17:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.74 on epoch=894
06/19/2022 23:17:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.84 on epoch=899
06/19/2022 23:17:27 - INFO - __main__ - Global step 1800 Train loss 0.77 Classification-F1 0.4385964912280702 on epoch=899
06/19/2022 23:17:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.77 on epoch=904
06/19/2022 23:17:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.70 on epoch=909
06/19/2022 23:17:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.73 on epoch=914
06/19/2022 23:17:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.63 on epoch=919
06/19/2022 23:17:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.69 on epoch=924
06/19/2022 23:17:34 - INFO - __main__ - Global step 1850 Train loss 0.70 Classification-F1 0.5134502923976608 on epoch=924
06/19/2022 23:17:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.71 on epoch=929
06/19/2022 23:17:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.73 on epoch=934
06/19/2022 23:17:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.75 on epoch=939
06/19/2022 23:17:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.67 on epoch=944
06/19/2022 23:17:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.69 on epoch=949
06/19/2022 23:17:41 - INFO - __main__ - Global step 1900 Train loss 0.71 Classification-F1 0.4385964912280702 on epoch=949
06/19/2022 23:17:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.66 on epoch=954
06/19/2022 23:17:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.62 on epoch=959
06/19/2022 23:17:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.69 on epoch=964
06/19/2022 23:17:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.67 on epoch=969
06/19/2022 23:17:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.71 on epoch=974
06/19/2022 23:17:47 - INFO - __main__ - Global step 1950 Train loss 0.67 Classification-F1 0.3992490613266583 on epoch=974
06/19/2022 23:17:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.60 on epoch=979
06/19/2022 23:17:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.73 on epoch=984
06/19/2022 23:17:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.69 on epoch=989
06/19/2022 23:17:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.61 on epoch=994
06/19/2022 23:17:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.68 on epoch=999
06/19/2022 23:17:54 - INFO - __main__ - Global step 2000 Train loss 0.66 Classification-F1 0.4385964912280702 on epoch=999
06/19/2022 23:17:54 - INFO - __main__ - save last model!
06/19/2022 23:17:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 23:17:54 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 23:17:54 - INFO - __main__ - Printing 3 examples
06/19/2022 23:17:54 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 23:17:54 - INFO - __main__ - ['0']
06/19/2022 23:17:54 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 23:17:54 - INFO - __main__ - ['1']
06/19/2022 23:17:54 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 23:17:54 - INFO - __main__ - ['1']
06/19/2022 23:17:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 23:17:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:17:55 - INFO - __main__ - Printing 3 examples
06/19/2022 23:17:55 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 23:17:55 - INFO - __main__ - ['0']
06/19/2022 23:17:55 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 23:17:55 - INFO - __main__ - ['0']
06/19/2022 23:17:55 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 23:17:55 - INFO - __main__ - ['0']
06/19/2022 23:17:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 23:17:55 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:17:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 23:17:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:17:55 - INFO - __main__ - Printing 3 examples
06/19/2022 23:17:55 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 23:17:55 - INFO - __main__ - ['0']
06/19/2022 23:17:55 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 23:17:55 - INFO - __main__ - ['0']
06/19/2022 23:17:55 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 23:17:55 - INFO - __main__ - ['0']
06/19/2022 23:17:55 - INFO - __main__ - Tokenizing Input ...
06/19/2022 23:17:55 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:17:55 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 23:17:58 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:18:00 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 23:18:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 23:18:01 - INFO - __main__ - Starting training!
06/19/2022 23:18:06 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 23:19:32 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_87_0.3_8_predictions.txt
06/19/2022 23:19:32 - INFO - __main__ - Classification-F1 on test data: 0.3875
06/19/2022 23:19:32 - INFO - __main__ - prefix=paws_16_87, lr=0.3, bsz=8, dev_performance=0.6000000000000001, test_performance=0.3875040202034258
06/19/2022 23:19:32 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.2, bsz=8 ...
06/19/2022 23:19:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:19:33 - INFO - __main__ - Printing 3 examples
06/19/2022 23:19:33 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/19/2022 23:19:33 - INFO - __main__ - ['0']
06/19/2022 23:19:33 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/19/2022 23:19:33 - INFO - __main__ - ['0']
06/19/2022 23:19:33 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/19/2022 23:19:33 - INFO - __main__ - ['0']
06/19/2022 23:19:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 23:19:33 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:19:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 23:19:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 23:19:33 - INFO - __main__ - Printing 3 examples
06/19/2022 23:19:33 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/19/2022 23:19:33 - INFO - __main__ - ['0']
06/19/2022 23:19:33 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/19/2022 23:19:33 - INFO - __main__ - ['0']
06/19/2022 23:19:33 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/19/2022 23:19:33 - INFO - __main__ - ['0']
06/19/2022 23:19:33 - INFO - __main__ - Tokenizing Input ...
06/19/2022 23:19:33 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:19:33 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 23:19:39 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 23:19:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 23:19:39 - INFO - __main__ - Starting training!
06/19/2022 23:19:41 - INFO - __main__ - Step 10 Global step 10 Train loss 5.69 on epoch=4
06/19/2022 23:19:42 - INFO - __main__ - Step 20 Global step 20 Train loss 5.53 on epoch=9
06/19/2022 23:19:43 - INFO - __main__ - Step 30 Global step 30 Train loss 5.28 on epoch=14
06/19/2022 23:19:45 - INFO - __main__ - Step 40 Global step 40 Train loss 4.84 on epoch=19
06/19/2022 23:19:46 - INFO - __main__ - Step 50 Global step 50 Train loss 4.61 on epoch=24
06/19/2022 23:19:49 - INFO - __main__ - Global step 50 Train loss 5.19 Classification-F1 0.0 on epoch=24
06/19/2022 23:19:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 23:19:50 - INFO - __main__ - Step 60 Global step 60 Train loss 4.51 on epoch=29
06/19/2022 23:19:51 - INFO - __main__ - Step 70 Global step 70 Train loss 4.29 on epoch=34
06/19/2022 23:19:53 - INFO - __main__ - Step 80 Global step 80 Train loss 4.11 on epoch=39
06/19/2022 23:19:54 - INFO - __main__ - Step 90 Global step 90 Train loss 4.01 on epoch=44
06/19/2022 23:19:55 - INFO - __main__ - Step 100 Global step 100 Train loss 3.94 on epoch=49
06/19/2022 23:19:56 - INFO - __main__ - Global step 100 Train loss 4.17 Classification-F1 0.0 on epoch=49
06/19/2022 23:19:58 - INFO - __main__ - Step 110 Global step 110 Train loss 3.93 on epoch=54
06/19/2022 23:19:59 - INFO - __main__ - Step 120 Global step 120 Train loss 3.86 on epoch=59
06/19/2022 23:20:00 - INFO - __main__ - Step 130 Global step 130 Train loss 3.80 on epoch=64
06/19/2022 23:20:01 - INFO - __main__ - Step 140 Global step 140 Train loss 3.52 on epoch=69
06/19/2022 23:20:03 - INFO - __main__ - Step 150 Global step 150 Train loss 3.53 on epoch=74
06/19/2022 23:20:04 - INFO - __main__ - Global step 150 Train loss 3.73 Classification-F1 0.0 on epoch=74
06/19/2022 23:20:05 - INFO - __main__ - Step 160 Global step 160 Train loss 3.58 on epoch=79
06/19/2022 23:20:07 - INFO - __main__ - Step 170 Global step 170 Train loss 3.54 on epoch=84
06/19/2022 23:20:08 - INFO - __main__ - Step 180 Global step 180 Train loss 3.35 on epoch=89
06/19/2022 23:20:09 - INFO - __main__ - Step 190 Global step 190 Train loss 3.44 on epoch=94
06/19/2022 23:20:11 - INFO - __main__ - Step 200 Global step 200 Train loss 3.37 on epoch=99
06/19/2022 23:20:17 - INFO - __main__ - Global step 200 Train loss 3.46 Classification-F1 0.0 on epoch=99
06/19/2022 23:20:18 - INFO - __main__ - Step 210 Global step 210 Train loss 3.24 on epoch=104
06/19/2022 23:20:19 - INFO - __main__ - Step 220 Global step 220 Train loss 3.20 on epoch=109
06/19/2022 23:20:20 - INFO - __main__ - Step 230 Global step 230 Train loss 3.07 on epoch=114
06/19/2022 23:20:22 - INFO - __main__ - Step 240 Global step 240 Train loss 3.10 on epoch=119
06/19/2022 23:20:23 - INFO - __main__ - Step 250 Global step 250 Train loss 3.08 on epoch=124
06/19/2022 23:20:25 - INFO - __main__ - Global step 250 Train loss 3.14 Classification-F1 0.09600000000000002 on epoch=124
06/19/2022 23:20:25 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.09600000000000002 on epoch=124, global_step=250
06/19/2022 23:20:26 - INFO - __main__ - Step 260 Global step 260 Train loss 3.03 on epoch=129
06/19/2022 23:20:27 - INFO - __main__ - Step 270 Global step 270 Train loss 3.09 on epoch=134
06/19/2022 23:20:29 - INFO - __main__ - Step 280 Global step 280 Train loss 3.03 on epoch=139
06/19/2022 23:20:30 - INFO - __main__ - Step 290 Global step 290 Train loss 2.84 on epoch=144
06/19/2022 23:20:31 - INFO - __main__ - Step 300 Global step 300 Train loss 2.84 on epoch=149
06/19/2022 23:20:33 - INFO - __main__ - Global step 300 Train loss 2.97 Classification-F1 0.15217391304347824 on epoch=149
06/19/2022 23:20:33 - INFO - __main__ - Saving model with best Classification-F1: 0.09600000000000002 -> 0.15217391304347824 on epoch=149, global_step=300
06/19/2022 23:20:34 - INFO - __main__ - Step 310 Global step 310 Train loss 2.84 on epoch=154
06/19/2022 23:20:35 - INFO - __main__ - Step 320 Global step 320 Train loss 2.78 on epoch=159
06/19/2022 23:20:37 - INFO - __main__ - Step 330 Global step 330 Train loss 2.75 on epoch=164
06/19/2022 23:20:38 - INFO - __main__ - Step 340 Global step 340 Train loss 2.65 on epoch=169
06/19/2022 23:20:39 - INFO - __main__ - Step 350 Global step 350 Train loss 2.57 on epoch=174
06/19/2022 23:20:41 - INFO - __main__ - Global step 350 Train loss 2.72 Classification-F1 0.16304347826086957 on epoch=174
06/19/2022 23:20:41 - INFO - __main__ - Saving model with best Classification-F1: 0.15217391304347824 -> 0.16304347826086957 on epoch=174, global_step=350
06/19/2022 23:20:43 - INFO - __main__ - Step 360 Global step 360 Train loss 2.49 on epoch=179
06/19/2022 23:20:44 - INFO - __main__ - Step 370 Global step 370 Train loss 2.38 on epoch=184
06/19/2022 23:20:45 - INFO - __main__ - Step 380 Global step 380 Train loss 2.37 on epoch=189
06/19/2022 23:20:46 - INFO - __main__ - Step 390 Global step 390 Train loss 2.34 on epoch=194
06/19/2022 23:20:48 - INFO - __main__ - Step 400 Global step 400 Train loss 2.45 on epoch=199
06/19/2022 23:20:50 - INFO - __main__ - Global step 400 Train loss 2.41 Classification-F1 0.21276595744680848 on epoch=199
06/19/2022 23:20:50 - INFO - __main__ - Saving model with best Classification-F1: 0.16304347826086957 -> 0.21276595744680848 on epoch=199, global_step=400
06/19/2022 23:20:51 - INFO - __main__ - Step 410 Global step 410 Train loss 2.30 on epoch=204
06/19/2022 23:20:52 - INFO - __main__ - Step 420 Global step 420 Train loss 2.22 on epoch=209
06/19/2022 23:20:53 - INFO - __main__ - Step 430 Global step 430 Train loss 2.29 on epoch=214
06/19/2022 23:20:55 - INFO - __main__ - Step 440 Global step 440 Train loss 2.16 on epoch=219
06/19/2022 23:20:56 - INFO - __main__ - Step 450 Global step 450 Train loss 2.15 on epoch=224
06/19/2022 23:20:58 - INFO - __main__ - Global step 450 Train loss 2.22 Classification-F1 0.3333333333333333 on epoch=224
06/19/2022 23:20:58 - INFO - __main__ - Saving model with best Classification-F1: 0.21276595744680848 -> 0.3333333333333333 on epoch=224, global_step=450
06/19/2022 23:20:59 - INFO - __main__ - Step 460 Global step 460 Train loss 2.19 on epoch=229
06/19/2022 23:21:01 - INFO - __main__ - Step 470 Global step 470 Train loss 2.17 on epoch=234
06/19/2022 23:21:02 - INFO - __main__ - Step 480 Global step 480 Train loss 2.01 on epoch=239
06/19/2022 23:21:03 - INFO - __main__ - Step 490 Global step 490 Train loss 1.87 on epoch=244
06/19/2022 23:21:04 - INFO - __main__ - Step 500 Global step 500 Train loss 1.90 on epoch=249
06/19/2022 23:21:09 - INFO - __main__ - Global step 500 Train loss 2.03 Classification-F1 0.3992490613266583 on epoch=249
06/19/2022 23:21:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=249, global_step=500
06/19/2022 23:21:10 - INFO - __main__ - Step 510 Global step 510 Train loss 1.96 on epoch=254
06/19/2022 23:21:11 - INFO - __main__ - Step 520 Global step 520 Train loss 1.96 on epoch=259
06/19/2022 23:21:12 - INFO - __main__ - Step 530 Global step 530 Train loss 1.93 on epoch=264
06/19/2022 23:21:14 - INFO - __main__ - Step 540 Global step 540 Train loss 1.85 on epoch=269
06/19/2022 23:21:15 - INFO - __main__ - Step 550 Global step 550 Train loss 1.84 on epoch=274
06/19/2022 23:21:16 - INFO - __main__ - Global step 550 Train loss 1.91 Classification-F1 0.4980392156862745 on epoch=274
06/19/2022 23:21:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4980392156862745 on epoch=274, global_step=550
06/19/2022 23:21:17 - INFO - __main__ - Step 560 Global step 560 Train loss 1.96 on epoch=279
06/19/2022 23:21:19 - INFO - __main__ - Step 570 Global step 570 Train loss 1.86 on epoch=284
06/19/2022 23:21:20 - INFO - __main__ - Step 580 Global step 580 Train loss 1.69 on epoch=289
06/19/2022 23:21:21 - INFO - __main__ - Step 590 Global step 590 Train loss 1.82 on epoch=294
06/19/2022 23:21:22 - INFO - __main__ - Step 600 Global step 600 Train loss 1.81 on epoch=299
06/19/2022 23:21:23 - INFO - __main__ - Global step 600 Train loss 1.83 Classification-F1 0.6559139784946237 on epoch=299
06/19/2022 23:21:24 - INFO - __main__ - Saving model with best Classification-F1: 0.4980392156862745 -> 0.6559139784946237 on epoch=299, global_step=600
06/19/2022 23:21:25 - INFO - __main__ - Step 610 Global step 610 Train loss 1.68 on epoch=304
06/19/2022 23:21:26 - INFO - __main__ - Step 620 Global step 620 Train loss 1.88 on epoch=309
06/19/2022 23:21:27 - INFO - __main__ - Step 630 Global step 630 Train loss 1.67 on epoch=314
06/19/2022 23:21:29 - INFO - __main__ - Step 640 Global step 640 Train loss 1.60 on epoch=319
06/19/2022 23:21:30 - INFO - __main__ - Step 650 Global step 650 Train loss 1.56 on epoch=324
06/19/2022 23:21:30 - INFO - __main__ - Global step 650 Train loss 1.68 Classification-F1 0.3454545454545454 on epoch=324
06/19/2022 23:21:32 - INFO - __main__ - Step 660 Global step 660 Train loss 1.51 on epoch=329
06/19/2022 23:21:33 - INFO - __main__ - Step 670 Global step 670 Train loss 1.54 on epoch=334
06/19/2022 23:21:34 - INFO - __main__ - Step 680 Global step 680 Train loss 1.39 on epoch=339
06/19/2022 23:21:36 - INFO - __main__ - Step 690 Global step 690 Train loss 1.49 on epoch=344
06/19/2022 23:21:37 - INFO - __main__ - Step 700 Global step 700 Train loss 1.58 on epoch=349
06/19/2022 23:21:37 - INFO - __main__ - Global step 700 Train loss 1.50 Classification-F1 0.4666666666666667 on epoch=349
06/19/2022 23:21:38 - INFO - __main__ - Step 710 Global step 710 Train loss 1.38 on epoch=354
06/19/2022 23:21:40 - INFO - __main__ - Step 720 Global step 720 Train loss 1.55 on epoch=359
06/19/2022 23:21:41 - INFO - __main__ - Step 730 Global step 730 Train loss 1.49 on epoch=364
06/19/2022 23:21:42 - INFO - __main__ - Step 740 Global step 740 Train loss 1.44 on epoch=369
06/19/2022 23:21:44 - INFO - __main__ - Step 750 Global step 750 Train loss 1.51 on epoch=374
06/19/2022 23:21:44 - INFO - __main__ - Global step 750 Train loss 1.47 Classification-F1 0.3552492046659597 on epoch=374
06/19/2022 23:21:45 - INFO - __main__ - Step 760 Global step 760 Train loss 1.42 on epoch=379
06/19/2022 23:21:46 - INFO - __main__ - Step 770 Global step 770 Train loss 1.35 on epoch=384
06/19/2022 23:21:48 - INFO - __main__ - Step 780 Global step 780 Train loss 1.36 on epoch=389
06/19/2022 23:21:49 - INFO - __main__ - Step 790 Global step 790 Train loss 1.37 on epoch=394
06/19/2022 23:21:50 - INFO - __main__ - Step 800 Global step 800 Train loss 1.42 on epoch=399
06/19/2022 23:21:51 - INFO - __main__ - Global step 800 Train loss 1.38 Classification-F1 0.41700404858299595 on epoch=399
06/19/2022 23:21:52 - INFO - __main__ - Step 810 Global step 810 Train loss 1.28 on epoch=404
06/19/2022 23:21:53 - INFO - __main__ - Step 820 Global step 820 Train loss 1.25 on epoch=409
06/19/2022 23:21:55 - INFO - __main__ - Step 830 Global step 830 Train loss 1.39 on epoch=414
06/19/2022 23:21:56 - INFO - __main__ - Step 840 Global step 840 Train loss 1.31 on epoch=419
06/19/2022 23:21:57 - INFO - __main__ - Step 850 Global step 850 Train loss 1.27 on epoch=424
06/19/2022 23:21:58 - INFO - __main__ - Global step 850 Train loss 1.30 Classification-F1 0.4458874458874459 on epoch=424
06/19/2022 23:21:59 - INFO - __main__ - Step 860 Global step 860 Train loss 1.29 on epoch=429
06/19/2022 23:22:00 - INFO - __main__ - Step 870 Global step 870 Train loss 1.12 on epoch=434
06/19/2022 23:22:01 - INFO - __main__ - Step 880 Global step 880 Train loss 1.29 on epoch=439
06/19/2022 23:22:03 - INFO - __main__ - Step 890 Global step 890 Train loss 1.20 on epoch=444
06/19/2022 23:22:04 - INFO - __main__ - Step 900 Global step 900 Train loss 1.17 on epoch=449
06/19/2022 23:22:04 - INFO - __main__ - Global step 900 Train loss 1.21 Classification-F1 0.3125 on epoch=449
06/19/2022 23:22:05 - INFO - __main__ - Step 910 Global step 910 Train loss 1.24 on epoch=454
06/19/2022 23:22:07 - INFO - __main__ - Step 920 Global step 920 Train loss 1.08 on epoch=459
06/19/2022 23:22:08 - INFO - __main__ - Step 930 Global step 930 Train loss 1.12 on epoch=464
06/19/2022 23:22:09 - INFO - __main__ - Step 940 Global step 940 Train loss 1.16 on epoch=469
06/19/2022 23:22:11 - INFO - __main__ - Step 950 Global step 950 Train loss 1.13 on epoch=474
06/19/2022 23:22:11 - INFO - __main__ - Global step 950 Train loss 1.15 Classification-F1 0.3073593073593074 on epoch=474
06/19/2022 23:22:12 - INFO - __main__ - Step 960 Global step 960 Train loss 1.08 on epoch=479
06/19/2022 23:22:13 - INFO - __main__ - Step 970 Global step 970 Train loss 1.23 on epoch=484
06/19/2022 23:22:15 - INFO - __main__ - Step 980 Global step 980 Train loss 1.10 on epoch=489
06/19/2022 23:22:16 - INFO - __main__ - Step 990 Global step 990 Train loss 1.11 on epoch=494
06/19/2022 23:22:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.04 on epoch=499
06/19/2022 23:22:18 - INFO - __main__ - Global step 1000 Train loss 1.11 Classification-F1 0.4385964912280702 on epoch=499
06/19/2022 23:22:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.13 on epoch=504
06/19/2022 23:22:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.03 on epoch=509
06/19/2022 23:22:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.03 on epoch=514
06/19/2022 23:22:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.03 on epoch=519
06/19/2022 23:22:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.97 on epoch=524
06/19/2022 23:22:24 - INFO - __main__ - Global step 1050 Train loss 1.04 Classification-F1 0.4589371980676329 on epoch=524
06/19/2022 23:22:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.02 on epoch=529
06/19/2022 23:22:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.01 on epoch=534
06/19/2022 23:22:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.07 on epoch=539
06/19/2022 23:22:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.94 on epoch=544
06/19/2022 23:22:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.99 on epoch=549
06/19/2022 23:22:31 - INFO - __main__ - Global step 1100 Train loss 1.01 Classification-F1 0.3191489361702127 on epoch=549
06/19/2022 23:22:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.03 on epoch=554
06/19/2022 23:22:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.98 on epoch=559
06/19/2022 23:22:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.96 on epoch=564
06/19/2022 23:22:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.91 on epoch=569
06/19/2022 23:22:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.90 on epoch=574
06/19/2022 23:22:38 - INFO - __main__ - Global step 1150 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=574
06/19/2022 23:22:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.88 on epoch=579
06/19/2022 23:22:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.98 on epoch=584
06/19/2022 23:22:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.89 on epoch=589
06/19/2022 23:22:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.92 on epoch=594
06/19/2022 23:22:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.98 on epoch=599
06/19/2022 23:22:45 - INFO - __main__ - Global step 1200 Train loss 0.93 Classification-F1 0.3333333333333333 on epoch=599
06/19/2022 23:22:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.97 on epoch=604
06/19/2022 23:22:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.88 on epoch=609
06/19/2022 23:22:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.90 on epoch=614
06/19/2022 23:22:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=619
06/19/2022 23:22:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.93 on epoch=624
06/19/2022 23:22:51 - INFO - __main__ - Global step 1250 Train loss 0.90 Classification-F1 0.3333333333333333 on epoch=624
06/19/2022 23:22:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.89 on epoch=629
06/19/2022 23:22:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.84 on epoch=634
06/19/2022 23:22:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.81 on epoch=639
06/19/2022 23:22:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.82 on epoch=644
06/19/2022 23:22:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.90 on epoch=649
06/19/2022 23:22:58 - INFO - __main__ - Global step 1300 Train loss 0.85 Classification-F1 0.3992490613266583 on epoch=649
06/19/2022 23:22:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.70 on epoch=654
06/19/2022 23:23:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.75 on epoch=659
06/19/2022 23:23:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.78 on epoch=664
06/19/2022 23:23:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.89 on epoch=669
06/19/2022 23:23:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.77 on epoch=674
06/19/2022 23:23:05 - INFO - __main__ - Global step 1350 Train loss 0.78 Classification-F1 0.4554554554554554 on epoch=674
06/19/2022 23:23:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.71 on epoch=679
06/19/2022 23:23:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.74 on epoch=684
06/19/2022 23:23:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.84 on epoch=689
06/19/2022 23:23:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.85 on epoch=694
06/19/2022 23:23:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.82 on epoch=699
06/19/2022 23:23:11 - INFO - __main__ - Global step 1400 Train loss 0.79 Classification-F1 0.3992490613266583 on epoch=699
06/19/2022 23:23:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.73 on epoch=704
06/19/2022 23:23:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.78 on epoch=709
06/19/2022 23:23:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.84 on epoch=714
06/19/2022 23:23:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.74 on epoch=719
06/19/2022 23:23:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.81 on epoch=724
06/19/2022 23:23:18 - INFO - __main__ - Global step 1450 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=724
06/19/2022 23:23:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.73 on epoch=729
06/19/2022 23:23:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.79 on epoch=734
06/19/2022 23:23:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.81 on epoch=739
06/19/2022 23:23:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.71 on epoch=744
06/19/2022 23:23:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.76 on epoch=749
06/19/2022 23:23:25 - INFO - __main__ - Global step 1500 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=749
06/19/2022 23:23:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.84 on epoch=754
06/19/2022 23:23:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.67 on epoch=759
06/19/2022 23:23:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.81 on epoch=764
06/19/2022 23:23:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.70 on epoch=769
06/19/2022 23:23:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.78 on epoch=774
06/19/2022 23:23:31 - INFO - __main__ - Global step 1550 Train loss 0.76 Classification-F1 0.4589371980676329 on epoch=774
06/19/2022 23:23:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.73 on epoch=779
06/19/2022 23:23:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.78 on epoch=784
06/19/2022 23:23:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.79 on epoch=789
06/19/2022 23:23:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.75 on epoch=794
06/19/2022 23:23:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.64 on epoch=799
06/19/2022 23:23:38 - INFO - __main__ - Global step 1600 Train loss 0.74 Classification-F1 0.3816425120772947 on epoch=799
06/19/2022 23:23:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.74 on epoch=804
06/19/2022 23:23:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.66 on epoch=809
06/19/2022 23:23:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.75 on epoch=814
06/19/2022 23:23:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.77 on epoch=819
06/19/2022 23:23:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.73 on epoch=824
06/19/2022 23:23:45 - INFO - __main__ - Global step 1650 Train loss 0.73 Classification-F1 0.4458874458874459 on epoch=824
06/19/2022 23:23:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.67 on epoch=829
06/19/2022 23:23:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.70 on epoch=834
06/19/2022 23:23:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.72 on epoch=839
06/19/2022 23:23:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.69 on epoch=844
06/19/2022 23:23:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.69 on epoch=849
06/19/2022 23:23:52 - INFO - __main__ - Global step 1700 Train loss 0.69 Classification-F1 0.36374269005847953 on epoch=849
06/19/2022 23:23:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.69 on epoch=854
06/19/2022 23:23:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.76 on epoch=859
06/19/2022 23:23:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.65 on epoch=864
06/19/2022 23:23:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.61 on epoch=869
06/19/2022 23:23:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.80 on epoch=874
06/19/2022 23:23:58 - INFO - __main__ - Global step 1750 Train loss 0.70 Classification-F1 0.3992490613266583 on epoch=874
06/19/2022 23:24:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.77 on epoch=879
06/19/2022 23:24:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.69 on epoch=884
06/19/2022 23:24:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.72 on epoch=889
06/19/2022 23:24:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.72 on epoch=894
06/19/2022 23:24:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.68 on epoch=899
06/19/2022 23:24:05 - INFO - __main__ - Global step 1800 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=899
06/19/2022 23:24:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.68 on epoch=904
06/19/2022 23:24:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.61 on epoch=909
06/19/2022 23:24:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.60 on epoch=914
06/19/2022 23:24:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.62 on epoch=919
06/19/2022 23:24:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.55 on epoch=924
06/19/2022 23:24:12 - INFO - __main__ - Global step 1850 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=924
06/19/2022 23:24:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.65 on epoch=929
06/19/2022 23:24:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.64 on epoch=934
06/19/2022 23:24:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.69 on epoch=939
06/19/2022 23:24:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.67 on epoch=944
06/19/2022 23:24:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.64 on epoch=949
06/19/2022 23:24:18 - INFO - __main__ - Global step 1900 Train loss 0.66 Classification-F1 0.3191489361702127 on epoch=949
06/19/2022 23:24:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.66 on epoch=954
06/19/2022 23:24:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.73 on epoch=959
06/19/2022 23:24:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.67 on epoch=964
06/19/2022 23:24:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.66 on epoch=969
06/19/2022 23:24:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.64 on epoch=974
06/19/2022 23:24:25 - INFO - __main__ - Global step 1950 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=974
06/19/2022 23:24:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.74 on epoch=979
06/19/2022 23:24:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.58 on epoch=984
06/19/2022 23:24:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.76 on epoch=989
06/19/2022 23:24:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.63 on epoch=994
06/19/2022 23:24:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.61 on epoch=999
06/19/2022 23:24:32 - INFO - __main__ - Global step 2000 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=999
06/19/2022 23:24:32 - INFO - __main__ - save last model!
06/19/2022 23:24:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 23:24:32 - INFO - __main__ - Start tokenizing ... 8000 instances
06/19/2022 23:24:32 - INFO - __main__ - Printing 3 examples
06/19/2022 23:24:32 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/19/2022 23:24:32 - INFO - __main__ - ['0']
06/19/2022 23:24:32 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/19/2022 23:24:32 - INFO - __main__ - ['1']
06/19/2022 23:24:32 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/19/2022 23:24:32 - INFO - __main__ - ['1']
06/19/2022 23:24:32 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 23:24:36 - INFO - __main__ - Tokenizing Output ...
06/19/2022 23:24:44 - INFO - __main__ - Loaded 8000 examples from test data
06/19/2022 23:26:09 - INFO - __main__ - Saved prediction in models/T5-base-maml-nopara2para-3e-5-2-5000-5e-1/singletask-paws/paws_16_87_0.2_8_predictions.txt
06/19/2022 23:26:09 - INFO - __main__ - Classification-F1 on test data: 0.3251
06/19/2022 23:26:09 - INFO - __main__ - prefix=paws_16_87, lr=0.2, bsz=8, dev_performance=0.6559139784946237, test_performance=0.3250948287367326
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (88172): No such process
