05/21/2022 21:36:59 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-20-up128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-up128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20-128shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/21/2022 21:36:59 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-up128shot/singletask-emo
05/21/2022 21:36:59 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-20-up128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-up128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20-128shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/21/2022 21:36:59 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-up128shot/singletask-emo
05/21/2022 21:36:59 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:36:59 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:36:59 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:36:59 - INFO - __main__ - Using 2 gpus
05/21/2022 21:36:59 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:36:59 - INFO - __main__ - Using 2 gpus
05/21/2022 21:36:59 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/21/2022 21:36:59 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/21/2022 21:37:04 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
05/21/2022 21:37:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 21:37:05 - INFO - __main__ - Printing 3 examples
05/21/2022 21:37:05 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 21:37:05 - INFO - __main__ - ['others']
05/21/2022 21:37:05 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 21:37:05 - INFO - __main__ - ['others']
05/21/2022 21:37:05 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 21:37:05 - INFO - __main__ - ['others']
05/21/2022 21:37:05 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:37:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 21:37:05 - INFO - __main__ - Printing 3 examples
05/21/2022 21:37:05 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 21:37:05 - INFO - __main__ - ['others']
05/21/2022 21:37:05 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 21:37:05 - INFO - __main__ - ['others']
05/21/2022 21:37:05 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 21:37:05 - INFO - __main__ - ['others']
05/21/2022 21:37:05 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:37:05 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:37:05 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:37:05 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 21:37:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 21:37:05 - INFO - __main__ - Printing 3 examples
05/21/2022 21:37:05 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 21:37:05 - INFO - __main__ - ['others']
05/21/2022 21:37:05 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 21:37:05 - INFO - __main__ - ['others']
05/21/2022 21:37:05 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 21:37:05 - INFO - __main__ - ['others']
05/21/2022 21:37:05 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:37:05 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 21:37:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 21:37:05 - INFO - __main__ - Printing 3 examples
05/21/2022 21:37:05 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 21:37:05 - INFO - __main__ - ['others']
05/21/2022 21:37:05 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 21:37:05 - INFO - __main__ - ['others']
05/21/2022 21:37:05 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 21:37:05 - INFO - __main__ - ['others']
05/21/2022 21:37:05 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:37:05 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:37:05 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:37:05 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 21:37:05 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 21:37:23 - INFO - __main__ - load prompt embedding from ckpt
06/01/2022 01:26:44 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-20-up128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-up128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20-128shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/01/2022 01:26:44 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-up128shot/singletask-emo
06/01/2022 01:26:44 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-20-up128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-up128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20-128shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/01/2022 01:26:44 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-up128shot/singletask-emo
06/01/2022 01:26:46 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/01/2022 01:26:46 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/01/2022 01:26:46 - INFO - __main__ - args.device: cuda:0
06/01/2022 01:26:46 - INFO - __main__ - Using 2 gpus
06/01/2022 01:26:46 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
06/01/2022 01:26:46 - INFO - __main__ - args.device: cuda:1
06/01/2022 01:26:46 - INFO - __main__ - Using 2 gpus
06/01/2022 01:26:46 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
06/01/2022 01:26:50 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
06/01/2022 01:26:51 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 01:26:51 - INFO - __main__ - Printing 3 examples
06/01/2022 01:26:51 - INFO - __main__ -  [emo] how cause yes am listening
06/01/2022 01:26:51 - INFO - __main__ - ['others']
06/01/2022 01:26:51 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/01/2022 01:26:51 - INFO - __main__ - ['others']
06/01/2022 01:26:51 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/01/2022 01:26:51 - INFO - __main__ - ['others']
06/01/2022 01:26:51 - INFO - __main__ - Tokenizing Input ...
06/01/2022 01:26:51 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 01:26:51 - INFO - __main__ - Printing 3 examples
06/01/2022 01:26:51 - INFO - __main__ -  [emo] how cause yes am listening
06/01/2022 01:26:51 - INFO - __main__ - ['others']
06/01/2022 01:26:51 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/01/2022 01:26:51 - INFO - __main__ - ['others']
06/01/2022 01:26:51 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/01/2022 01:26:51 - INFO - __main__ - ['others']
06/01/2022 01:26:51 - INFO - __main__ - Tokenizing Input ...
06/01/2022 01:26:51 - INFO - __main__ - Tokenizing Output ...
06/01/2022 01:26:51 - INFO - __main__ - Tokenizing Output ...
06/01/2022 01:26:52 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 01:26:52 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 01:26:52 - INFO - __main__ - Printing 3 examples
06/01/2022 01:26:52 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/01/2022 01:26:52 - INFO - __main__ - ['others']
06/01/2022 01:26:52 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/01/2022 01:26:52 - INFO - __main__ - ['others']
06/01/2022 01:26:52 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/01/2022 01:26:52 - INFO - __main__ - ['others']
06/01/2022 01:26:52 - INFO - __main__ - Tokenizing Input ...
06/01/2022 01:26:52 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 01:26:52 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 01:26:52 - INFO - __main__ - Printing 3 examples
06/01/2022 01:26:52 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/01/2022 01:26:52 - INFO - __main__ - ['others']
06/01/2022 01:26:52 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/01/2022 01:26:52 - INFO - __main__ - ['others']
06/01/2022 01:26:52 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/01/2022 01:26:52 - INFO - __main__ - ['others']
06/01/2022 01:26:52 - INFO - __main__ - Tokenizing Input ...
06/01/2022 01:26:52 - INFO - __main__ - Tokenizing Output ...
06/01/2022 01:26:52 - INFO - __main__ - Tokenizing Output ...
06/01/2022 01:26:52 - INFO - __main__ - Loaded 64 examples from dev data
06/01/2022 01:26:52 - INFO - __main__ - Loaded 64 examples from dev data
06/01/2022 01:27:09 - INFO - __main__ - load prompt embedding from ckpt
06/01/2022 01:27:09 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 19:33:08 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-20-up128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-up128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20-128shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/14/2022 19:33:08 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-up128shot/singletask-emo
06/14/2022 19:33:08 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-20-up128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-up128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20-128shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/14/2022 19:33:08 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-up128shot/singletask-emo
06/14/2022 19:33:09 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/14/2022 19:33:09 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/14/2022 19:33:09 - INFO - __main__ - args.device: cuda:0
06/14/2022 19:33:09 - INFO - __main__ - Using 2 gpus
06/14/2022 19:33:09 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
06/14/2022 19:33:09 - INFO - __main__ - args.device: cuda:1
06/14/2022 19:33:09 - INFO - __main__ - Using 2 gpus
06/14/2022 19:33:09 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
06/14/2022 19:33:14 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
06/14/2022 19:33:15 - INFO - __main__ - Start tokenizing ... 64 instances
06/14/2022 19:33:15 - INFO - __main__ - Printing 3 examples
06/14/2022 19:33:15 - INFO - __main__ -  [emo] how cause yes am listening
06/14/2022 19:33:15 - INFO - __main__ - ['others']
06/14/2022 19:33:15 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/14/2022 19:33:15 - INFO - __main__ - ['others']
06/14/2022 19:33:15 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/14/2022 19:33:15 - INFO - __main__ - ['others']
06/14/2022 19:33:15 - INFO - __main__ - Tokenizing Input ...
06/14/2022 19:33:15 - INFO - __main__ - Tokenizing Output ...
06/14/2022 19:33:15 - INFO - __main__ - Start tokenizing ... 64 instances
06/14/2022 19:33:15 - INFO - __main__ - Printing 3 examples
06/14/2022 19:33:15 - INFO - __main__ -  [emo] how cause yes am listening
06/14/2022 19:33:15 - INFO - __main__ - ['others']
06/14/2022 19:33:15 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/14/2022 19:33:15 - INFO - __main__ - ['others']
06/14/2022 19:33:15 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/14/2022 19:33:15 - INFO - __main__ - ['others']
06/14/2022 19:33:15 - INFO - __main__ - Tokenizing Input ...
06/14/2022 19:33:15 - INFO - __main__ - Tokenizing Output ...
06/14/2022 19:33:15 - INFO - __main__ - Loaded 64 examples from train data
06/14/2022 19:33:15 - INFO - __main__ - Start tokenizing ... 64 instances
06/14/2022 19:33:15 - INFO - __main__ - Printing 3 examples
06/14/2022 19:33:15 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/14/2022 19:33:15 - INFO - __main__ - ['others']
06/14/2022 19:33:15 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/14/2022 19:33:15 - INFO - __main__ - ['others']
06/14/2022 19:33:15 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/14/2022 19:33:15 - INFO - __main__ - ['others']
06/14/2022 19:33:15 - INFO - __main__ - Tokenizing Input ...
06/14/2022 19:33:15 - INFO - __main__ - Loaded 64 examples from train data
06/14/2022 19:33:15 - INFO - __main__ - Start tokenizing ... 64 instances
06/14/2022 19:33:15 - INFO - __main__ - Printing 3 examples
06/14/2022 19:33:15 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/14/2022 19:33:15 - INFO - __main__ - ['others']
06/14/2022 19:33:15 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/14/2022 19:33:15 - INFO - __main__ - ['others']
06/14/2022 19:33:15 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/14/2022 19:33:15 - INFO - __main__ - ['others']
06/14/2022 19:33:15 - INFO - __main__ - Tokenizing Input ...
06/14/2022 19:33:15 - INFO - __main__ - Tokenizing Output ...
06/14/2022 19:33:15 - INFO - __main__ - Tokenizing Output ...
06/14/2022 19:33:15 - INFO - __main__ - Loaded 64 examples from dev data
06/14/2022 19:33:15 - INFO - __main__ - Loaded 64 examples from dev data
06/14/2022 19:33:33 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 19:33:33 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 07:34:50 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-3-up128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-3-128shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/23/2022 07:34:50 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-3-up128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-3-128shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
06/23/2022 07:34:50 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo
06/23/2022 07:34:50 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo
06/23/2022 07:34:51 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/23/2022 07:34:51 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/23/2022 07:34:51 - INFO - __main__ - args.device: cuda:1
06/23/2022 07:34:51 - INFO - __main__ - args.device: cuda:0
06/23/2022 07:34:51 - INFO - __main__ - Using 2 gpus
06/23/2022 07:34:51 - INFO - __main__ - Using 2 gpus
06/23/2022 07:34:51 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
06/23/2022 07:34:51 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
06/23/2022 07:34:55 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
06/23/2022 07:34:56 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 07:34:56 - INFO - __main__ - Printing 3 examples
06/23/2022 07:34:56 - INFO - __main__ -  [emo] how cause yes am listening
06/23/2022 07:34:56 - INFO - __main__ - ['others']
06/23/2022 07:34:56 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/23/2022 07:34:56 - INFO - __main__ - ['others']
06/23/2022 07:34:56 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/23/2022 07:34:56 - INFO - __main__ - ['others']
06/23/2022 07:34:56 - INFO - __main__ - Tokenizing Input ...
06/23/2022 07:34:56 - INFO - __main__ - Tokenizing Output ...
06/23/2022 07:34:56 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 07:34:56 - INFO - __main__ - Printing 3 examples
06/23/2022 07:34:56 - INFO - __main__ -  [emo] how cause yes am listening
06/23/2022 07:34:56 - INFO - __main__ - ['others']
06/23/2022 07:34:56 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/23/2022 07:34:56 - INFO - __main__ - ['others']
06/23/2022 07:34:56 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/23/2022 07:34:56 - INFO - __main__ - ['others']
06/23/2022 07:34:56 - INFO - __main__ - Tokenizing Input ...
06/23/2022 07:34:56 - INFO - __main__ - Tokenizing Output ...
06/23/2022 07:34:56 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 07:34:56 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 07:34:57 - INFO - __main__ - Printing 3 examples
06/23/2022 07:34:57 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/23/2022 07:34:57 - INFO - __main__ - ['others']
06/23/2022 07:34:57 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/23/2022 07:34:57 - INFO - __main__ - ['others']
06/23/2022 07:34:57 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/23/2022 07:34:57 - INFO - __main__ - ['others']
06/23/2022 07:34:57 - INFO - __main__ - Tokenizing Input ...
06/23/2022 07:34:57 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 07:34:57 - INFO - __main__ - Tokenizing Output ...
06/23/2022 07:34:57 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 07:34:57 - INFO - __main__ - Printing 3 examples
06/23/2022 07:34:57 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/23/2022 07:34:57 - INFO - __main__ - ['others']
06/23/2022 07:34:57 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/23/2022 07:34:57 - INFO - __main__ - ['others']
06/23/2022 07:34:57 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/23/2022 07:34:57 - INFO - __main__ - ['others']
06/23/2022 07:34:57 - INFO - __main__ - Tokenizing Input ...
06/23/2022 07:34:57 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 07:34:57 - INFO - __main__ - Tokenizing Output ...
06/23/2022 07:34:57 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 07:35:15 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 07:35:15 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 07:35:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 07:35:15 - INFO - __main__ - Starting training!
06/23/2022 07:35:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 07:35:20 - INFO - __main__ - Starting training!
06/23/2022 07:35:24 - INFO - __main__ - Step 10 Global step 10 Train loss 4.12 on epoch=2
06/23/2022 07:35:26 - INFO - __main__ - Step 20 Global step 20 Train loss 2.90 on epoch=4
06/23/2022 07:35:29 - INFO - __main__ - Step 30 Global step 30 Train loss 2.27 on epoch=7
06/23/2022 07:35:31 - INFO - __main__ - Step 40 Global step 40 Train loss 1.70 on epoch=9
06/23/2022 07:35:34 - INFO - __main__ - Step 50 Global step 50 Train loss 1.50 on epoch=12
06/23/2022 07:35:35 - INFO - __main__ - Global step 50 Train loss 2.50 Classification-F1 0.21843853820598005 on epoch=12
06/23/2022 07:35:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.21843853820598005 on epoch=12, global_step=50
06/23/2022 07:35:37 - INFO - __main__ - Step 60 Global step 60 Train loss 1.17 on epoch=14
06/23/2022 07:35:40 - INFO - __main__ - Step 70 Global step 70 Train loss 1.02 on epoch=17
06/23/2022 07:35:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.98 on epoch=19
06/23/2022 07:35:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=22
06/23/2022 07:35:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.88 on epoch=24
06/23/2022 07:35:48 - INFO - __main__ - Global step 100 Train loss 0.99 Classification-F1 0.5550131962580663 on epoch=24
06/23/2022 07:35:48 - INFO - __main__ - Saving model with best Classification-F1: 0.21843853820598005 -> 0.5550131962580663 on epoch=24, global_step=100
06/23/2022 07:35:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.86 on epoch=27
06/23/2022 07:35:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.76 on epoch=29
06/23/2022 07:35:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.81 on epoch=32
06/23/2022 07:35:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.70 on epoch=34
06/23/2022 07:36:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.74 on epoch=37
06/23/2022 07:36:01 - INFO - __main__ - Global step 150 Train loss 0.77 Classification-F1 0.6171130952380952 on epoch=37
06/23/2022 07:36:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5550131962580663 -> 0.6171130952380952 on epoch=37, global_step=150
06/23/2022 07:36:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.70 on epoch=39
06/23/2022 07:36:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.67 on epoch=42
06/23/2022 07:36:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.65 on epoch=44
06/23/2022 07:36:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=47
06/23/2022 07:36:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.57 on epoch=49
06/23/2022 07:36:14 - INFO - __main__ - Global step 200 Train loss 0.63 Classification-F1 0.6440860215053763 on epoch=49
06/23/2022 07:36:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6171130952380952 -> 0.6440860215053763 on epoch=49, global_step=200
06/23/2022 07:36:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.69 on epoch=52
06/23/2022 07:36:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.61 on epoch=54
06/23/2022 07:36:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.63 on epoch=57
06/23/2022 07:36:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=59
06/23/2022 07:36:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.61 on epoch=62
06/23/2022 07:36:27 - INFO - __main__ - Global step 250 Train loss 0.62 Classification-F1 0.6440860215053763 on epoch=62
06/23/2022 07:36:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.57 on epoch=64
06/23/2022 07:36:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.54 on epoch=67
06/23/2022 07:36:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=69
06/23/2022 07:36:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.56 on epoch=72
06/23/2022 07:36:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=74
06/23/2022 07:36:41 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.628035178035178 on epoch=74
06/23/2022 07:36:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=77
06/23/2022 07:36:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=79
06/23/2022 07:36:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.57 on epoch=82
06/23/2022 07:36:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=84
06/23/2022 07:36:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=87
06/23/2022 07:36:54 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.6870915032679739 on epoch=87
06/23/2022 07:36:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6440860215053763 -> 0.6870915032679739 on epoch=87, global_step=350
06/23/2022 07:36:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=89
06/23/2022 07:36:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.55 on epoch=92
06/23/2022 07:37:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=94
06/23/2022 07:37:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=97
06/23/2022 07:37:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=99
06/23/2022 07:37:07 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.649896978021978 on epoch=99
06/23/2022 07:37:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=102
06/23/2022 07:37:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=104
06/23/2022 07:37:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=107
06/23/2022 07:37:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=109
06/23/2022 07:37:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=112
06/23/2022 07:37:20 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.6938259109311741 on epoch=112
06/23/2022 07:37:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6870915032679739 -> 0.6938259109311741 on epoch=112, global_step=450
06/23/2022 07:37:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=114
06/23/2022 07:37:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.35 on epoch=117
06/23/2022 07:37:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=119
06/23/2022 07:37:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=122
06/23/2022 07:37:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=124
06/23/2022 07:37:34 - INFO - __main__ - Global step 500 Train loss 0.36 Classification-F1 0.6796650717703349 on epoch=124
06/23/2022 07:37:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=127
06/23/2022 07:37:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=129
06/23/2022 07:37:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=132
06/23/2022 07:37:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=134
06/23/2022 07:37:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=137
06/23/2022 07:37:47 - INFO - __main__ - Global step 550 Train loss 0.30 Classification-F1 0.7309518620002491 on epoch=137
06/23/2022 07:37:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6938259109311741 -> 0.7309518620002491 on epoch=137, global_step=550
06/23/2022 07:37:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=139
06/23/2022 07:37:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.35 on epoch=142
06/23/2022 07:37:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=144
06/23/2022 07:37:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=147
06/23/2022 07:37:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.28 on epoch=149
06/23/2022 07:38:00 - INFO - __main__ - Global step 600 Train loss 0.29 Classification-F1 0.6501305067481538 on epoch=149
06/23/2022 07:38:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=152
06/23/2022 07:38:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=154
06/23/2022 07:38:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=157
06/23/2022 07:38:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=159
06/23/2022 07:38:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=162
06/23/2022 07:38:13 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.7142857142857143 on epoch=162
06/23/2022 07:38:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=164
06/23/2022 07:38:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.14 on epoch=167
06/23/2022 07:38:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=169
06/23/2022 07:38:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=172
06/23/2022 07:38:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=174
06/23/2022 07:38:27 - INFO - __main__ - Global step 700 Train loss 0.18 Classification-F1 0.7287037037037036 on epoch=174
06/23/2022 07:38:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=177
06/23/2022 07:38:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=179
06/23/2022 07:38:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=182
06/23/2022 07:38:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=184
06/23/2022 07:38:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=187
06/23/2022 07:38:40 - INFO - __main__ - Global step 750 Train loss 0.17 Classification-F1 0.7313508814523013 on epoch=187
06/23/2022 07:38:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7309518620002491 -> 0.7313508814523013 on epoch=187, global_step=750
06/23/2022 07:38:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=189
06/23/2022 07:38:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.16 on epoch=192
06/23/2022 07:38:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.14 on epoch=194
06/23/2022 07:38:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=197
06/23/2022 07:38:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=199
06/23/2022 07:38:53 - INFO - __main__ - Global step 800 Train loss 0.13 Classification-F1 0.7128033205619413 on epoch=199
06/23/2022 07:38:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=202
06/23/2022 07:38:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=204
06/23/2022 07:39:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=207
06/23/2022 07:39:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=209
06/23/2022 07:39:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=212
06/23/2022 07:39:07 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.7332125953829807 on epoch=212
06/23/2022 07:39:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7313508814523013 -> 0.7332125953829807 on epoch=212, global_step=850
06/23/2022 07:39:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=214
06/23/2022 07:39:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.11 on epoch=217
06/23/2022 07:39:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=219
06/23/2022 07:39:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=222
06/23/2022 07:39:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=224
06/23/2022 07:39:20 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.7307351179055033 on epoch=224
06/23/2022 07:39:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=227
06/23/2022 07:39:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=229
06/23/2022 07:39:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=232
06/23/2022 07:39:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=234
06/23/2022 07:39:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=237
06/23/2022 07:39:33 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.7135902636916835 on epoch=237
06/23/2022 07:39:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=239
06/23/2022 07:39:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
06/23/2022 07:39:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=244
06/23/2022 07:39:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
06/23/2022 07:39:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=249
06/23/2022 07:39:47 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.7340005760368663 on epoch=249
06/23/2022 07:39:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7332125953829807 -> 0.7340005760368663 on epoch=249, global_step=1000
06/23/2022 07:39:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=252
06/23/2022 07:39:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
06/23/2022 07:39:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=257
06/23/2022 07:39:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=259
06/23/2022 07:39:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=262
06/23/2022 07:40:00 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.6987821565407772 on epoch=262
06/23/2022 07:40:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
06/23/2022 07:40:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
06/23/2022 07:40:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=269
06/23/2022 07:40:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
06/23/2022 07:40:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
06/23/2022 07:40:13 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.712037037037037 on epoch=274
06/23/2022 07:40:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=277
06/23/2022 07:40:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=279
06/23/2022 07:40:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
06/23/2022 07:40:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=284
06/23/2022 07:40:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
06/23/2022 07:40:27 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.728238779202268 on epoch=287
06/23/2022 07:40:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
06/23/2022 07:40:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=292
06/23/2022 07:40:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
06/23/2022 07:40:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=297
06/23/2022 07:40:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=299
06/23/2022 07:40:40 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.7492831541218637 on epoch=299
06/23/2022 07:40:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7340005760368663 -> 0.7492831541218637 on epoch=299, global_step=1200
06/23/2022 07:40:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=302
06/23/2022 07:40:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
06/23/2022 07:40:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=307
06/23/2022 07:40:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
06/23/2022 07:40:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
06/23/2022 07:40:53 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.7018804112554113 on epoch=312
06/23/2022 07:40:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
06/23/2022 07:40:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
06/23/2022 07:41:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
06/23/2022 07:41:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
06/23/2022 07:41:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
06/23/2022 07:41:06 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.7042328042328042 on epoch=324
06/23/2022 07:41:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
06/23/2022 07:41:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
06/23/2022 07:41:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
06/23/2022 07:41:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=334
06/23/2022 07:41:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
06/23/2022 07:41:20 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.7506465517241379 on epoch=337
06/23/2022 07:41:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7492831541218637 -> 0.7506465517241379 on epoch=337, global_step=1350
06/23/2022 07:41:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
06/23/2022 07:41:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
06/23/2022 07:41:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
06/23/2022 07:41:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
06/23/2022 07:41:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
06/23/2022 07:41:33 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.6623563218390804 on epoch=349
06/23/2022 07:41:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
06/23/2022 07:41:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=354
06/23/2022 07:41:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
06/23/2022 07:41:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=359
06/23/2022 07:41:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=362
06/23/2022 07:41:46 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.6683982683982684 on epoch=362
06/23/2022 07:41:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
06/23/2022 07:41:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=367
06/23/2022 07:41:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
06/23/2022 07:41:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
06/23/2022 07:41:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
06/23/2022 07:42:00 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.738979448656868 on epoch=374
06/23/2022 07:42:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
06/23/2022 07:42:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
06/23/2022 07:42:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/23/2022 07:42:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
06/23/2022 07:42:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
06/23/2022 07:42:13 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.6859243697478992 on epoch=387
06/23/2022 07:42:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
06/23/2022 07:42:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
06/23/2022 07:42:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
06/23/2022 07:42:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
06/23/2022 07:42:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/23/2022 07:42:26 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6998792620496475 on epoch=399
06/23/2022 07:42:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/23/2022 07:42:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
06/23/2022 07:42:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
06/23/2022 07:42:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=409
06/23/2022 07:42:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
06/23/2022 07:42:40 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6918346918346918 on epoch=412
06/23/2022 07:42:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=414
06/23/2022 07:42:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
06/23/2022 07:42:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/23/2022 07:42:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
06/23/2022 07:42:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
06/23/2022 07:42:53 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.7340232683982684 on epoch=424
06/23/2022 07:42:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
06/23/2022 07:42:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/23/2022 07:43:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=432
06/23/2022 07:43:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
06/23/2022 07:43:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
06/23/2022 07:43:06 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.7090996168582375 on epoch=437
06/23/2022 07:43:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
06/23/2022 07:43:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=442
06/23/2022 07:43:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
06/23/2022 07:43:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
06/23/2022 07:43:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=449
06/23/2022 07:43:20 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.7142174432497013 on epoch=449
06/23/2022 07:43:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=452
06/23/2022 07:43:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
06/23/2022 07:43:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
06/23/2022 07:43:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
06/23/2022 07:43:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
06/23/2022 07:43:33 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.7491747835497835 on epoch=462
06/23/2022 07:43:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
06/23/2022 07:43:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
06/23/2022 07:43:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/23/2022 07:43:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
06/23/2022 07:43:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
06/23/2022 07:43:47 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7142174432497013 on epoch=474
06/23/2022 07:43:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
06/23/2022 07:43:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
06/23/2022 07:43:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/23/2022 07:43:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
06/23/2022 07:43:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
06/23/2022 07:44:00 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7488182773109244 on epoch=487
06/23/2022 07:44:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
06/23/2022 07:44:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/23/2022 07:44:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
06/23/2022 07:44:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
06/23/2022 07:44:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
06/23/2022 07:44:13 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7013616557734204 on epoch=499
06/23/2022 07:44:16 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/23/2022 07:44:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
06/23/2022 07:44:21 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
06/23/2022 07:44:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
06/23/2022 07:44:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
06/23/2022 07:44:27 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7166373000813228 on epoch=512
06/23/2022 07:44:29 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/23/2022 07:44:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=517
06/23/2022 07:44:34 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
06/23/2022 07:44:37 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=522
06/23/2022 07:44:39 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
06/23/2022 07:44:40 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7216303304538598 on epoch=524
06/23/2022 07:44:42 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
06/23/2022 07:44:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/23/2022 07:44:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/23/2022 07:44:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
06/23/2022 07:44:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/23/2022 07:44:53 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7340232683982684 on epoch=537
06/23/2022 07:44:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/23/2022 07:44:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
06/23/2022 07:45:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
06/23/2022 07:45:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/23/2022 07:45:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
06/23/2022 07:45:07 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7050381263616556 on epoch=549
06/23/2022 07:45:09 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/23/2022 07:45:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
06/23/2022 07:45:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
06/23/2022 07:45:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/23/2022 07:45:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
06/23/2022 07:45:20 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7183982683982684 on epoch=562
06/23/2022 07:45:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
06/23/2022 07:45:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
06/23/2022 07:45:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
06/23/2022 07:45:30 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
06/23/2022 07:45:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/23/2022 07:45:33 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6952380952380952 on epoch=574
06/23/2022 07:45:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
06/23/2022 07:45:38 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/23/2022 07:45:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/23/2022 07:45:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
06/23/2022 07:45:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/23/2022 07:45:47 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6748949579831933 on epoch=587
06/23/2022 07:45:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/23/2022 07:45:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
06/23/2022 07:45:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/23/2022 07:45:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
06/23/2022 07:45:59 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/23/2022 07:46:00 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.726002748151541 on epoch=599
06/23/2022 07:46:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
06/23/2022 07:46:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/23/2022 07:46:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/23/2022 07:46:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
06/23/2022 07:46:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
06/23/2022 07:46:13 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.726002748151541 on epoch=612
06/23/2022 07:46:16 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/23/2022 07:46:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
06/23/2022 07:46:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/23/2022 07:46:23 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/23/2022 07:46:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/23/2022 07:46:26 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6859243697478992 on epoch=624
06/23/2022 07:46:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/23/2022 07:46:31 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/23/2022 07:46:34 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=632
06/23/2022 07:46:36 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
06/23/2022 07:46:39 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/23/2022 07:46:40 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.674025974025974 on epoch=637
06/23/2022 07:46:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/23/2022 07:46:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/23/2022 07:46:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=644
06/23/2022 07:46:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/23/2022 07:46:52 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/23/2022 07:46:53 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7183982683982684 on epoch=649
06/23/2022 07:46:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/23/2022 07:46:58 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/23/2022 07:47:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
06/23/2022 07:47:03 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/23/2022 07:47:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
06/23/2022 07:47:06 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6956896551724138 on epoch=662
06/23/2022 07:47:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=664
06/23/2022 07:47:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
06/23/2022 07:47:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.09 on epoch=669
06/23/2022 07:47:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
06/23/2022 07:47:19 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/23/2022 07:47:20 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.668495018975332 on epoch=674
06/23/2022 07:47:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/23/2022 07:47:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/23/2022 07:47:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/23/2022 07:47:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=684
06/23/2022 07:47:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/23/2022 07:47:33 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6949628127112915 on epoch=687
06/23/2022 07:47:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=689
06/23/2022 07:47:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/23/2022 07:47:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
06/23/2022 07:47:43 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/23/2022 07:47:46 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/23/2022 07:47:47 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7261932202348024 on epoch=699
06/23/2022 07:47:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/23/2022 07:47:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/23/2022 07:47:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/23/2022 07:47:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/23/2022 07:47:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/23/2022 07:48:00 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6954545454545454 on epoch=712
06/23/2022 07:48:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/23/2022 07:48:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/23/2022 07:48:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/23/2022 07:48:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/23/2022 07:48:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/23/2022 07:48:13 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.711207739238885 on epoch=724
06/23/2022 07:48:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/23/2022 07:48:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/23/2022 07:48:21 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/23/2022 07:48:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/23/2022 07:48:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/23/2022 07:48:26 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7218855218855219 on epoch=737
06/23/2022 07:48:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/23/2022 07:48:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/23/2022 07:48:34 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/23/2022 07:48:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/23/2022 07:48:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=749
06/23/2022 07:48:40 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6596288515406162 on epoch=749
06/23/2022 07:48:40 - INFO - __main__ - save last model!
06/23/2022 07:48:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 07:48:40 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 07:48:40 - INFO - __main__ - Printing 3 examples
06/23/2022 07:48:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 07:48:40 - INFO - __main__ - ['others']
06/23/2022 07:48:40 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 07:48:40 - INFO - __main__ - ['others']
06/23/2022 07:48:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 07:48:40 - INFO - __main__ - ['others']
06/23/2022 07:48:40 - INFO - __main__ - Tokenizing Input ...
06/23/2022 07:48:40 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 07:48:40 - INFO - __main__ - Printing 3 examples
06/23/2022 07:48:40 - INFO - __main__ -  [emo] how cause yes am listening
06/23/2022 07:48:40 - INFO - __main__ - ['others']
06/23/2022 07:48:40 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/23/2022 07:48:40 - INFO - __main__ - ['others']
06/23/2022 07:48:40 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/23/2022 07:48:40 - INFO - __main__ - ['others']
06/23/2022 07:48:40 - INFO - __main__ - Tokenizing Input ...
06/23/2022 07:48:40 - INFO - __main__ - Tokenizing Output ...
06/23/2022 07:48:40 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 07:48:40 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 07:48:40 - INFO - __main__ - Printing 3 examples
06/23/2022 07:48:40 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/23/2022 07:48:40 - INFO - __main__ - ['others']
06/23/2022 07:48:40 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/23/2022 07:48:40 - INFO - __main__ - ['others']
06/23/2022 07:48:40 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/23/2022 07:48:40 - INFO - __main__ - ['others']
06/23/2022 07:48:40 - INFO - __main__ - Tokenizing Input ...
06/23/2022 07:48:40 - INFO - __main__ - Tokenizing Output ...
06/23/2022 07:48:41 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 07:48:42 - INFO - __main__ - Tokenizing Output ...
06/23/2022 07:48:47 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 07:48:55 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 07:48:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 07:48:56 - INFO - __main__ - Starting training!
06/23/2022 07:50:15 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_100_0.5_8_predictions.txt
06/23/2022 07:50:15 - INFO - __main__ - Classification-F1 on test data: 0.2475
06/23/2022 07:50:15 - INFO - __main__ - prefix=emo_16_100, lr=0.5, bsz=8, dev_performance=0.7506465517241379, test_performance=0.24750597265727342
06/23/2022 07:50:15 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.4, bsz=8 ...
06/23/2022 07:50:16 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 07:50:16 - INFO - __main__ - Printing 3 examples
06/23/2022 07:50:16 - INFO - __main__ -  [emo] how cause yes am listening
06/23/2022 07:50:16 - INFO - __main__ - ['others']
06/23/2022 07:50:16 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/23/2022 07:50:16 - INFO - __main__ - ['others']
06/23/2022 07:50:16 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/23/2022 07:50:16 - INFO - __main__ - ['others']
06/23/2022 07:50:16 - INFO - __main__ - Tokenizing Input ...
06/23/2022 07:50:16 - INFO - __main__ - Tokenizing Output ...
06/23/2022 07:50:17 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 07:50:17 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 07:50:17 - INFO - __main__ - Printing 3 examples
06/23/2022 07:50:17 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/23/2022 07:50:17 - INFO - __main__ - ['others']
06/23/2022 07:50:17 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/23/2022 07:50:17 - INFO - __main__ - ['others']
06/23/2022 07:50:17 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/23/2022 07:50:17 - INFO - __main__ - ['others']
06/23/2022 07:50:17 - INFO - __main__ - Tokenizing Input ...
06/23/2022 07:50:17 - INFO - __main__ - Tokenizing Output ...
06/23/2022 07:50:17 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 07:50:33 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 07:50:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 07:50:33 - INFO - __main__ - Starting training!
06/23/2022 07:50:36 - INFO - __main__ - Step 10 Global step 10 Train loss 4.29 on epoch=2
06/23/2022 07:50:39 - INFO - __main__ - Step 20 Global step 20 Train loss 3.02 on epoch=4
06/23/2022 07:50:41 - INFO - __main__ - Step 30 Global step 30 Train loss 2.55 on epoch=7
06/23/2022 07:50:44 - INFO - __main__ - Step 40 Global step 40 Train loss 2.07 on epoch=9
06/23/2022 07:50:46 - INFO - __main__ - Step 50 Global step 50 Train loss 1.79 on epoch=12
06/23/2022 07:50:47 - INFO - __main__ - Global step 50 Train loss 2.74 Classification-F1 0.21101190476190473 on epoch=12
06/23/2022 07:50:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.21101190476190473 on epoch=12, global_step=50
06/23/2022 07:50:50 - INFO - __main__ - Step 60 Global step 60 Train loss 1.33 on epoch=14
06/23/2022 07:50:52 - INFO - __main__ - Step 70 Global step 70 Train loss 1.19 on epoch=17
06/23/2022 07:50:54 - INFO - __main__ - Step 80 Global step 80 Train loss 1.05 on epoch=19
06/23/2022 07:50:57 - INFO - __main__ - Step 90 Global step 90 Train loss 1.03 on epoch=22
06/23/2022 07:50:59 - INFO - __main__ - Step 100 Global step 100 Train loss 0.84 on epoch=24
06/23/2022 07:51:00 - INFO - __main__ - Global step 100 Train loss 1.09 Classification-F1 0.5175070028011204 on epoch=24
06/23/2022 07:51:00 - INFO - __main__ - Saving model with best Classification-F1: 0.21101190476190473 -> 0.5175070028011204 on epoch=24, global_step=100
06/23/2022 07:51:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.75 on epoch=27
06/23/2022 07:51:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.80 on epoch=29
06/23/2022 07:51:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.81 on epoch=32
06/23/2022 07:51:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.73 on epoch=34
06/23/2022 07:51:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.73 on epoch=37
06/23/2022 07:51:13 - INFO - __main__ - Global step 150 Train loss 0.76 Classification-F1 0.5385565210932858 on epoch=37
06/23/2022 07:51:13 - INFO - __main__ - Saving model with best Classification-F1: 0.5175070028011204 -> 0.5385565210932858 on epoch=37, global_step=150
06/23/2022 07:51:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.73 on epoch=39
06/23/2022 07:51:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.83 on epoch=42
06/23/2022 07:51:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.68 on epoch=44
06/23/2022 07:51:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.62 on epoch=47
06/23/2022 07:51:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.65 on epoch=49
06/23/2022 07:51:27 - INFO - __main__ - Global step 200 Train loss 0.70 Classification-F1 0.5950558213716108 on epoch=49
06/23/2022 07:51:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5385565210932858 -> 0.5950558213716108 on epoch=49, global_step=200
06/23/2022 07:51:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.65 on epoch=52
06/23/2022 07:51:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.67 on epoch=54
06/23/2022 07:51:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.63 on epoch=57
06/23/2022 07:51:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.64 on epoch=59
06/23/2022 07:51:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.73 on epoch=62
06/23/2022 07:51:40 - INFO - __main__ - Global step 250 Train loss 0.66 Classification-F1 0.6399611608587765 on epoch=62
06/23/2022 07:51:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5950558213716108 -> 0.6399611608587765 on epoch=62, global_step=250
06/23/2022 07:51:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.62 on epoch=64
06/23/2022 07:51:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.68 on epoch=67
06/23/2022 07:51:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.57 on epoch=69
06/23/2022 07:51:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.74 on epoch=72
06/23/2022 07:51:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=74
06/23/2022 07:51:53 - INFO - __main__ - Global step 300 Train loss 0.62 Classification-F1 0.6795207716260347 on epoch=74
06/23/2022 07:51:53 - INFO - __main__ - Saving model with best Classification-F1: 0.6399611608587765 -> 0.6795207716260347 on epoch=74, global_step=300
06/23/2022 07:51:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=77
06/23/2022 07:51:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=79
06/23/2022 07:52:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=82
06/23/2022 07:52:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=84
06/23/2022 07:52:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.57 on epoch=87
06/23/2022 07:52:06 - INFO - __main__ - Global step 350 Train loss 0.52 Classification-F1 0.6932997557997558 on epoch=87
06/23/2022 07:52:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6795207716260347 -> 0.6932997557997558 on epoch=87, global_step=350
06/23/2022 07:52:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=89
06/23/2022 07:52:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=92
06/23/2022 07:52:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.54 on epoch=94
06/23/2022 07:52:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=97
06/23/2022 07:52:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=99
06/23/2022 07:52:19 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.6654430193129884 on epoch=99
06/23/2022 07:52:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=102
06/23/2022 07:52:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=104
06/23/2022 07:52:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=107
06/23/2022 07:52:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=109
06/23/2022 07:52:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=112
06/23/2022 07:52:32 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.673175212113831 on epoch=112
06/23/2022 07:52:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=114
06/23/2022 07:52:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=117
06/23/2022 07:52:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=119
06/23/2022 07:52:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=122
06/23/2022 07:52:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=124
06/23/2022 07:52:45 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.7016659810777457 on epoch=124
06/23/2022 07:52:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6932997557997558 -> 0.7016659810777457 on epoch=124, global_step=500
06/23/2022 07:52:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=127
06/23/2022 07:52:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=129
06/23/2022 07:52:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=132
06/23/2022 07:52:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=134
06/23/2022 07:52:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=137
06/23/2022 07:52:59 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.6212993746949604 on epoch=137
06/23/2022 07:53:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=139
06/23/2022 07:53:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.29 on epoch=142
06/23/2022 07:53:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=144
06/23/2022 07:53:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=147
06/23/2022 07:53:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=149
06/23/2022 07:53:12 - INFO - __main__ - Global step 600 Train loss 0.30 Classification-F1 0.6560350673253899 on epoch=149
06/23/2022 07:53:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=152
06/23/2022 07:53:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=154
06/23/2022 07:53:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=157
06/23/2022 07:53:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=159
06/23/2022 07:53:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.28 on epoch=162
06/23/2022 07:53:25 - INFO - __main__ - Global step 650 Train loss 0.32 Classification-F1 0.7015684162423292 on epoch=162
06/23/2022 07:53:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=164
06/23/2022 07:53:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=167
06/23/2022 07:53:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=169
06/23/2022 07:53:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=172
06/23/2022 07:53:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=174
06/23/2022 07:53:38 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.6653793574846206 on epoch=174
06/23/2022 07:53:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=177
06/23/2022 07:53:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=179
06/23/2022 07:53:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=182
06/23/2022 07:53:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=184
06/23/2022 07:53:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=187
06/23/2022 07:53:51 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.6661895096677705 on epoch=187
06/23/2022 07:53:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=189
06/23/2022 07:53:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=192
06/23/2022 07:53:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=194
06/23/2022 07:54:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=197
06/23/2022 07:54:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=199
06/23/2022 07:54:04 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.6953048150698407 on epoch=199
06/23/2022 07:54:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=202
06/23/2022 07:54:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=204
06/23/2022 07:54:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=207
06/23/2022 07:54:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=209
06/23/2022 07:54:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=212
06/23/2022 07:54:17 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.6942102407844769 on epoch=212
06/23/2022 07:54:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=214
06/23/2022 07:54:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=217
06/23/2022 07:54:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=219
06/23/2022 07:54:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=222
06/23/2022 07:54:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=224
06/23/2022 07:54:30 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.7009169884169885 on epoch=224
06/23/2022 07:54:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=227
06/23/2022 07:54:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=229
06/23/2022 07:54:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=232
06/23/2022 07:54:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=234
06/23/2022 07:54:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=237
06/23/2022 07:54:43 - INFO - __main__ - Global step 950 Train loss 0.16 Classification-F1 0.7218381180223286 on epoch=237
06/23/2022 07:54:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7016659810777457 -> 0.7218381180223286 on epoch=237, global_step=950
06/23/2022 07:54:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=239
06/23/2022 07:54:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=242
06/23/2022 07:54:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=244
06/23/2022 07:54:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=247
06/23/2022 07:54:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=249
06/23/2022 07:54:56 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.7158750615973617 on epoch=249
06/23/2022 07:54:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=252
06/23/2022 07:55:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.10 on epoch=254
06/23/2022 07:55:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.13 on epoch=257
06/23/2022 07:55:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=259
06/23/2022 07:55:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=262
06/23/2022 07:55:09 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.7283068783068782 on epoch=262
06/23/2022 07:55:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7218381180223286 -> 0.7283068783068782 on epoch=262, global_step=1050
06/23/2022 07:55:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=264
06/23/2022 07:55:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=267
06/23/2022 07:55:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.11 on epoch=269
06/23/2022 07:55:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
06/23/2022 07:55:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=274
06/23/2022 07:55:23 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.746937386569873 on epoch=274
06/23/2022 07:55:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7283068783068782 -> 0.746937386569873 on epoch=274, global_step=1100
06/23/2022 07:55:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=277
06/23/2022 07:55:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
06/23/2022 07:55:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=282
06/23/2022 07:55:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=284
06/23/2022 07:55:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=287
06/23/2022 07:55:36 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.7324363156259708 on epoch=287
06/23/2022 07:55:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=289
06/23/2022 07:55:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=292
06/23/2022 07:55:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=294
06/23/2022 07:55:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=297
06/23/2022 07:55:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=299
06/23/2022 07:55:49 - INFO - __main__ - Global step 1200 Train loss 0.11 Classification-F1 0.6930675495103806 on epoch=299
06/23/2022 07:55:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
06/23/2022 07:55:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=304
06/23/2022 07:55:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=307
06/23/2022 07:55:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=309
06/23/2022 07:56:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.15 on epoch=312
06/23/2022 07:56:02 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.6921116138763198 on epoch=312
06/23/2022 07:56:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.10 on epoch=314
06/23/2022 07:56:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=317
06/23/2022 07:56:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
06/23/2022 07:56:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
06/23/2022 07:56:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
06/23/2022 07:56:15 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.7135515346722243 on epoch=324
06/23/2022 07:56:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
06/23/2022 07:56:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=329
06/23/2022 07:56:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
06/23/2022 07:56:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=334
06/23/2022 07:56:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
06/23/2022 07:56:28 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.7153293449177764 on epoch=337
06/23/2022 07:56:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=339
06/23/2022 07:56:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
06/23/2022 07:56:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
06/23/2022 07:56:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
06/23/2022 07:56:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
06/23/2022 07:56:41 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6986111111111111 on epoch=349
06/23/2022 07:56:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
06/23/2022 07:56:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=354
06/23/2022 07:56:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
06/23/2022 07:56:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
06/23/2022 07:56:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=362
06/23/2022 07:56:54 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.69940424295263 on epoch=362
06/23/2022 07:56:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
06/23/2022 07:56:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
06/23/2022 07:57:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
06/23/2022 07:57:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=372
06/23/2022 07:57:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
06/23/2022 07:57:07 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.7230029943011688 on epoch=374
06/23/2022 07:57:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
06/23/2022 07:57:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
06/23/2022 07:57:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
06/23/2022 07:57:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
06/23/2022 07:57:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
06/23/2022 07:57:20 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7002222687706559 on epoch=387
06/23/2022 07:57:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
06/23/2022 07:57:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=392
06/23/2022 07:57:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
06/23/2022 07:57:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=397
06/23/2022 07:57:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
06/23/2022 07:57:33 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.7119107744107743 on epoch=399
06/23/2022 07:57:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
06/23/2022 07:57:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=404
06/23/2022 07:57:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
06/23/2022 07:57:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
06/23/2022 07:57:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
06/23/2022 07:57:46 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.7 on epoch=412
06/23/2022 07:57:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
06/23/2022 07:57:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
06/23/2022 07:57:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/23/2022 07:57:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
06/23/2022 07:57:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
06/23/2022 07:57:59 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.6948004038521279 on epoch=424
06/23/2022 07:58:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
06/23/2022 07:58:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
06/23/2022 07:58:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/23/2022 07:58:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=434
06/23/2022 07:58:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
06/23/2022 07:58:12 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.6821236559139784 on epoch=437
06/23/2022 07:58:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=439
06/23/2022 07:58:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
06/23/2022 07:58:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
06/23/2022 07:58:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
06/23/2022 07:58:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
06/23/2022 07:58:26 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.6821236559139784 on epoch=449
06/23/2022 07:58:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
06/23/2022 07:58:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
06/23/2022 07:58:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/23/2022 07:58:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
06/23/2022 07:58:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=462
06/23/2022 07:58:39 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.7340073529411765 on epoch=462
06/23/2022 07:58:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=464
06/23/2022 07:58:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/23/2022 07:58:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/23/2022 07:58:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
06/23/2022 07:58:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
06/23/2022 07:58:52 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7166666666666668 on epoch=474
06/23/2022 07:58:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=477
06/23/2022 07:58:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=479
06/23/2022 07:58:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
06/23/2022 07:59:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=484
06/23/2022 07:59:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
06/23/2022 07:59:05 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.7285714285714285 on epoch=487
06/23/2022 07:59:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
06/23/2022 07:59:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
06/23/2022 07:59:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
06/23/2022 07:59:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=497
06/23/2022 07:59:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
06/23/2022 07:59:18 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.701379173290938 on epoch=499
06/23/2022 07:59:20 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
06/23/2022 07:59:23 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
06/23/2022 07:59:25 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
06/23/2022 07:59:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
06/23/2022 07:59:30 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.10 on epoch=512
06/23/2022 07:59:31 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.7145474137931034 on epoch=512
06/23/2022 07:59:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=514
06/23/2022 07:59:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/23/2022 07:59:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
06/23/2022 07:59:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
06/23/2022 07:59:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
06/23/2022 07:59:44 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7316430020283976 on epoch=524
06/23/2022 07:59:46 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
06/23/2022 07:59:48 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/23/2022 07:59:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
06/23/2022 07:59:53 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=534
06/23/2022 07:59:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/23/2022 07:59:57 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.7193014705882352 on epoch=537
06/23/2022 07:59:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=539
06/23/2022 08:00:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
06/23/2022 08:00:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
06/23/2022 08:00:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
06/23/2022 08:00:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
06/23/2022 08:00:10 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6986111111111111 on epoch=549
06/23/2022 08:00:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
06/23/2022 08:00:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
06/23/2022 08:00:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=557
06/23/2022 08:00:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/23/2022 08:00:22 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
06/23/2022 08:00:23 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6970238095238095 on epoch=562
06/23/2022 08:00:25 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
06/23/2022 08:00:28 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
06/23/2022 08:00:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
06/23/2022 08:00:33 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=572
06/23/2022 08:00:35 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
06/23/2022 08:00:36 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.717559004857392 on epoch=574
06/23/2022 08:00:39 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/23/2022 08:00:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=579
06/23/2022 08:00:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/23/2022 08:00:46 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
06/23/2022 08:00:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
06/23/2022 08:00:49 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.730701754385965 on epoch=587
06/23/2022 08:00:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
06/23/2022 08:00:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/23/2022 08:00:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
06/23/2022 08:00:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=597
06/23/2022 08:01:01 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
06/23/2022 08:01:02 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.7117845117845119 on epoch=599
06/23/2022 08:01:05 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/23/2022 08:01:07 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=604
06/23/2022 08:01:09 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=607
06/23/2022 08:01:12 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
06/23/2022 08:01:14 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
06/23/2022 08:01:15 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.7164650537634408 on epoch=612
06/23/2022 08:01:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/23/2022 08:01:20 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=617
06/23/2022 08:01:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
06/23/2022 08:01:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/23/2022 08:01:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
06/23/2022 08:01:28 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7286572622779519 on epoch=624
06/23/2022 08:01:31 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
06/23/2022 08:01:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
06/23/2022 08:01:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/23/2022 08:01:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
06/23/2022 08:01:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
06/23/2022 08:01:41 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7296150718351857 on epoch=637
06/23/2022 08:01:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/23/2022 08:01:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
06/23/2022 08:01:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/23/2022 08:01:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/23/2022 08:01:54 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/23/2022 08:01:55 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7282438282438284 on epoch=649
06/23/2022 08:01:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/23/2022 08:01:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/23/2022 08:02:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/23/2022 08:02:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
06/23/2022 08:02:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
06/23/2022 08:02:08 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7166666666666668 on epoch=662
06/23/2022 08:02:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
06/23/2022 08:02:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/23/2022 08:02:15 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
06/23/2022 08:02:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/23/2022 08:02:20 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/23/2022 08:02:21 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7129177564661436 on epoch=674
06/23/2022 08:02:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=677
06/23/2022 08:02:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
06/23/2022 08:02:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
06/23/2022 08:02:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/23/2022 08:02:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.09 on epoch=687
06/23/2022 08:02:34 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6960768398268399 on epoch=687
06/23/2022 08:02:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
06/23/2022 08:02:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=692
06/23/2022 08:02:42 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/23/2022 08:02:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/23/2022 08:02:46 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/23/2022 08:02:47 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7094504957408183 on epoch=699
06/23/2022 08:02:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/23/2022 08:02:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/23/2022 08:02:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/23/2022 08:02:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/23/2022 08:03:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/23/2022 08:03:00 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7094504957408183 on epoch=712
06/23/2022 08:03:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/23/2022 08:03:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=717
06/23/2022 08:03:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=719
06/23/2022 08:03:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/23/2022 08:03:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/23/2022 08:03:14 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.712375859434683 on epoch=724
06/23/2022 08:03:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/23/2022 08:03:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=729
06/23/2022 08:03:21 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=732
06/23/2022 08:03:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
06/23/2022 08:03:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=737
06/23/2022 08:03:27 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.6964093701996927 on epoch=737
06/23/2022 08:03:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/23/2022 08:03:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=742
06/23/2022 08:03:34 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/23/2022 08:03:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/23/2022 08:03:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
06/23/2022 08:03:40 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6917525074545947 on epoch=749
06/23/2022 08:03:40 - INFO - __main__ - save last model!
06/23/2022 08:03:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 08:03:40 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 08:03:40 - INFO - __main__ - Printing 3 examples
06/23/2022 08:03:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 08:03:40 - INFO - __main__ - ['others']
06/23/2022 08:03:40 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 08:03:40 - INFO - __main__ - ['others']
06/23/2022 08:03:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 08:03:40 - INFO - __main__ - ['others']
06/23/2022 08:03:40 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:03:41 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:03:41 - INFO - __main__ - Printing 3 examples
06/23/2022 08:03:41 - INFO - __main__ -  [emo] how cause yes am listening
06/23/2022 08:03:41 - INFO - __main__ - ['others']
06/23/2022 08:03:41 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/23/2022 08:03:41 - INFO - __main__ - ['others']
06/23/2022 08:03:41 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/23/2022 08:03:41 - INFO - __main__ - ['others']
06/23/2022 08:03:41 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:03:41 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:03:41 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 08:03:41 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:03:41 - INFO - __main__ - Printing 3 examples
06/23/2022 08:03:41 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/23/2022 08:03:41 - INFO - __main__ - ['others']
06/23/2022 08:03:41 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/23/2022 08:03:41 - INFO - __main__ - ['others']
06/23/2022 08:03:41 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/23/2022 08:03:41 - INFO - __main__ - ['others']
06/23/2022 08:03:41 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:03:41 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:03:41 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 08:03:43 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:03:48 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 08:03:59 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 08:04:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 08:04:00 - INFO - __main__ - Starting training!
06/23/2022 08:05:11 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_100_0.4_8_predictions.txt
06/23/2022 08:05:11 - INFO - __main__ - Classification-F1 on test data: 0.1904
06/23/2022 08:05:12 - INFO - __main__ - prefix=emo_16_100, lr=0.4, bsz=8, dev_performance=0.746937386569873, test_performance=0.1904288427887987
06/23/2022 08:05:12 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.3, bsz=8 ...
06/23/2022 08:05:12 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:05:12 - INFO - __main__ - Printing 3 examples
06/23/2022 08:05:12 - INFO - __main__ -  [emo] how cause yes am listening
06/23/2022 08:05:12 - INFO - __main__ - ['others']
06/23/2022 08:05:12 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/23/2022 08:05:12 - INFO - __main__ - ['others']
06/23/2022 08:05:12 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/23/2022 08:05:12 - INFO - __main__ - ['others']
06/23/2022 08:05:12 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:05:12 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:05:13 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 08:05:13 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:05:13 - INFO - __main__ - Printing 3 examples
06/23/2022 08:05:13 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/23/2022 08:05:13 - INFO - __main__ - ['others']
06/23/2022 08:05:13 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/23/2022 08:05:13 - INFO - __main__ - ['others']
06/23/2022 08:05:13 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/23/2022 08:05:13 - INFO - __main__ - ['others']
06/23/2022 08:05:13 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:05:13 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:05:13 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 08:05:32 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 08:05:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 08:05:33 - INFO - __main__ - Starting training!
06/23/2022 08:05:35 - INFO - __main__ - Step 10 Global step 10 Train loss 4.30 on epoch=2
06/23/2022 08:05:38 - INFO - __main__ - Step 20 Global step 20 Train loss 3.41 on epoch=4
06/23/2022 08:05:40 - INFO - __main__ - Step 30 Global step 30 Train loss 3.03 on epoch=7
06/23/2022 08:05:43 - INFO - __main__ - Step 40 Global step 40 Train loss 2.35 on epoch=9
06/23/2022 08:05:45 - INFO - __main__ - Step 50 Global step 50 Train loss 2.12 on epoch=12
06/23/2022 08:05:46 - INFO - __main__ - Global step 50 Train loss 3.04 Classification-F1 0.04615384615384615 on epoch=12
06/23/2022 08:05:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.04615384615384615 on epoch=12, global_step=50
06/23/2022 08:05:49 - INFO - __main__ - Step 60 Global step 60 Train loss 1.69 on epoch=14
06/23/2022 08:05:51 - INFO - __main__ - Step 70 Global step 70 Train loss 1.59 on epoch=17
06/23/2022 08:05:54 - INFO - __main__ - Step 80 Global step 80 Train loss 1.28 on epoch=19
06/23/2022 08:05:56 - INFO - __main__ - Step 90 Global step 90 Train loss 1.15 on epoch=22
06/23/2022 08:05:59 - INFO - __main__ - Step 100 Global step 100 Train loss 1.04 on epoch=24
06/23/2022 08:06:00 - INFO - __main__ - Global step 100 Train loss 1.35 Classification-F1 0.48578477443609025 on epoch=24
06/23/2022 08:06:00 - INFO - __main__ - Saving model with best Classification-F1: 0.04615384615384615 -> 0.48578477443609025 on epoch=24, global_step=100
06/23/2022 08:06:02 - INFO - __main__ - Step 110 Global step 110 Train loss 1.02 on epoch=27
06/23/2022 08:06:04 - INFO - __main__ - Step 120 Global step 120 Train loss 0.88 on epoch=29
06/23/2022 08:06:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.91 on epoch=32
06/23/2022 08:06:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.83 on epoch=34
06/23/2022 08:06:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.92 on epoch=37
06/23/2022 08:06:13 - INFO - __main__ - Global step 150 Train loss 0.91 Classification-F1 0.4878589152782701 on epoch=37
06/23/2022 08:06:13 - INFO - __main__ - Saving model with best Classification-F1: 0.48578477443609025 -> 0.4878589152782701 on epoch=37, global_step=150
06/23/2022 08:06:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.78 on epoch=39
06/23/2022 08:06:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.74 on epoch=42
06/23/2022 08:06:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.69 on epoch=44
06/23/2022 08:06:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.80 on epoch=47
06/23/2022 08:06:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.59 on epoch=49
06/23/2022 08:06:26 - INFO - __main__ - Global step 200 Train loss 0.72 Classification-F1 0.5552532326725875 on epoch=49
06/23/2022 08:06:26 - INFO - __main__ - Saving model with best Classification-F1: 0.4878589152782701 -> 0.5552532326725875 on epoch=49, global_step=200
06/23/2022 08:06:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.77 on epoch=52
06/23/2022 08:06:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.69 on epoch=54
06/23/2022 08:06:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.77 on epoch=57
06/23/2022 08:06:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.80 on epoch=59
06/23/2022 08:06:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.74 on epoch=62
06/23/2022 08:06:39 - INFO - __main__ - Global step 250 Train loss 0.75 Classification-F1 0.5787736568986569 on epoch=62
06/23/2022 08:06:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5552532326725875 -> 0.5787736568986569 on epoch=62, global_step=250
06/23/2022 08:06:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.62 on epoch=64
06/23/2022 08:06:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.63 on epoch=67
06/23/2022 08:06:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.62 on epoch=69
06/23/2022 08:06:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.62 on epoch=72
06/23/2022 08:06:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=74
06/23/2022 08:06:52 - INFO - __main__ - Global step 300 Train loss 0.61 Classification-F1 0.5468939998480011 on epoch=74
06/23/2022 08:06:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.71 on epoch=77
06/23/2022 08:06:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.64 on epoch=79
06/23/2022 08:06:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.67 on epoch=82
06/23/2022 08:07:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.60 on epoch=84
06/23/2022 08:07:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=87
06/23/2022 08:07:05 - INFO - __main__ - Global step 350 Train loss 0.64 Classification-F1 0.6602045194508008 on epoch=87
06/23/2022 08:07:05 - INFO - __main__ - Saving model with best Classification-F1: 0.5787736568986569 -> 0.6602045194508008 on epoch=87, global_step=350
06/23/2022 08:07:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.60 on epoch=89
06/23/2022 08:07:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.63 on epoch=92
06/23/2022 08:07:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=94
06/23/2022 08:07:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.60 on epoch=97
06/23/2022 08:07:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=99
06/23/2022 08:07:18 - INFO - __main__ - Global step 400 Train loss 0.58 Classification-F1 0.6393575174825175 on epoch=99
06/23/2022 08:07:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.62 on epoch=102
06/23/2022 08:07:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.55 on epoch=104
06/23/2022 08:07:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.55 on epoch=107
06/23/2022 08:07:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.53 on epoch=109
06/23/2022 08:07:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.59 on epoch=112
06/23/2022 08:07:31 - INFO - __main__ - Global step 450 Train loss 0.57 Classification-F1 0.6798704954954955 on epoch=112
06/23/2022 08:07:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6602045194508008 -> 0.6798704954954955 on epoch=112, global_step=450
06/23/2022 08:07:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.57 on epoch=114
06/23/2022 08:07:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=117
06/23/2022 08:07:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=119
06/23/2022 08:07:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=122
06/23/2022 08:07:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=124
06/23/2022 08:07:44 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.7167292961410607 on epoch=124
06/23/2022 08:07:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6798704954954955 -> 0.7167292961410607 on epoch=124, global_step=500
06/23/2022 08:07:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=127
06/23/2022 08:07:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=129
06/23/2022 08:07:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=132
06/23/2022 08:07:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=134
06/23/2022 08:07:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=137
06/23/2022 08:07:57 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.7233075466771118 on epoch=137
06/23/2022 08:07:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7167292961410607 -> 0.7233075466771118 on epoch=137, global_step=550
06/23/2022 08:08:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=139
06/23/2022 08:08:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.49 on epoch=142
06/23/2022 08:08:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=144
06/23/2022 08:08:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=147
06/23/2022 08:08:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=149
06/23/2022 08:08:11 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.6889724310776943 on epoch=149
06/23/2022 08:08:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=152
06/23/2022 08:08:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=154
06/23/2022 08:08:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.33 on epoch=157
06/23/2022 08:08:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.31 on epoch=159
06/23/2022 08:08:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=162
06/23/2022 08:08:24 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.7166827953272965 on epoch=162
06/23/2022 08:08:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=164
06/23/2022 08:08:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=167
06/23/2022 08:08:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=169
06/23/2022 08:08:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=172
06/23/2022 08:08:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=174
06/23/2022 08:08:37 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.6932997557997558 on epoch=174
06/23/2022 08:08:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=177
06/23/2022 08:08:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=179
06/23/2022 08:08:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=182
06/23/2022 08:08:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=184
06/23/2022 08:08:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=187
06/23/2022 08:08:50 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.6650104267751327 on epoch=187
06/23/2022 08:08:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=189
06/23/2022 08:08:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=192
06/23/2022 08:08:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=194
06/23/2022 08:09:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=197
06/23/2022 08:09:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=199
06/23/2022 08:09:03 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.6797163091280738 on epoch=199
06/23/2022 08:09:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=202
06/23/2022 08:09:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=204
06/23/2022 08:09:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=207
06/23/2022 08:09:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=209
06/23/2022 08:09:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=212
06/23/2022 08:09:16 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.7238992707742707 on epoch=212
06/23/2022 08:09:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7233075466771118 -> 0.7238992707742707 on epoch=212, global_step=850
06/23/2022 08:09:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.29 on epoch=214
06/23/2022 08:09:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=217
06/23/2022 08:09:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=219
06/23/2022 08:09:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=222
06/23/2022 08:09:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=224
06/23/2022 08:09:29 - INFO - __main__ - Global step 900 Train loss 0.24 Classification-F1 0.6942396942396942 on epoch=224
06/23/2022 08:09:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.27 on epoch=227
06/23/2022 08:09:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=229
06/23/2022 08:09:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=232
06/23/2022 08:09:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=234
06/23/2022 08:09:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.27 on epoch=237
06/23/2022 08:09:42 - INFO - __main__ - Global step 950 Train loss 0.25 Classification-F1 0.717296494355318 on epoch=237
06/23/2022 08:09:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=239
06/23/2022 08:09:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=242
06/23/2022 08:09:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=244
06/23/2022 08:09:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.27 on epoch=247
06/23/2022 08:09:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=249
06/23/2022 08:09:55 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.7014905814905815 on epoch=249
06/23/2022 08:09:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=252
06/23/2022 08:10:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=254
06/23/2022 08:10:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=257
06/23/2022 08:10:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=259
06/23/2022 08:10:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=262
06/23/2022 08:10:08 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.7571008508508509 on epoch=262
06/23/2022 08:10:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7238992707742707 -> 0.7571008508508509 on epoch=262, global_step=1050
06/23/2022 08:10:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=264
06/23/2022 08:10:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=267
06/23/2022 08:10:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=269
06/23/2022 08:10:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=272
06/23/2022 08:10:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=274
06/23/2022 08:10:21 - INFO - __main__ - Global step 1100 Train loss 0.17 Classification-F1 0.7798037658802177 on epoch=274
06/23/2022 08:10:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7571008508508509 -> 0.7798037658802177 on epoch=274, global_step=1100
06/23/2022 08:10:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=277
06/23/2022 08:10:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=279
06/23/2022 08:10:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=282
06/23/2022 08:10:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=284
06/23/2022 08:10:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=287
06/23/2022 08:10:34 - INFO - __main__ - Global step 1150 Train loss 0.16 Classification-F1 0.7608089826839827 on epoch=287
06/23/2022 08:10:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=289
06/23/2022 08:10:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=292
06/23/2022 08:10:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=294
06/23/2022 08:10:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.11 on epoch=297
06/23/2022 08:10:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=299
06/23/2022 08:10:47 - INFO - __main__ - Global step 1200 Train loss 0.12 Classification-F1 0.7774375454765947 on epoch=299
06/23/2022 08:10:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=302
06/23/2022 08:10:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=304
06/23/2022 08:10:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=307
06/23/2022 08:10:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
06/23/2022 08:10:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=312
06/23/2022 08:11:00 - INFO - __main__ - Global step 1250 Train loss 0.12 Classification-F1 0.7491401802656547 on epoch=312
06/23/2022 08:11:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=314
06/23/2022 08:11:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=317
06/23/2022 08:11:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=319
06/23/2022 08:11:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=322
06/23/2022 08:11:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=324
06/23/2022 08:11:13 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.7600903755137627 on epoch=324
06/23/2022 08:11:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=327
06/23/2022 08:11:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
06/23/2022 08:11:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=332
06/23/2022 08:11:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=334
06/23/2022 08:11:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=337
06/23/2022 08:11:26 - INFO - __main__ - Global step 1350 Train loss 0.11 Classification-F1 0.7310693138279345 on epoch=337
06/23/2022 08:11:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=339
06/23/2022 08:11:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
06/23/2022 08:11:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=344
06/23/2022 08:11:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=347
06/23/2022 08:11:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=349
06/23/2022 08:11:39 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.7635886591478697 on epoch=349
06/23/2022 08:11:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=352
06/23/2022 08:11:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=354
06/23/2022 08:11:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=357
06/23/2022 08:11:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=359
06/23/2022 08:11:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=362
06/23/2022 08:11:52 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.7627676377676378 on epoch=362
06/23/2022 08:11:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=364
06/23/2022 08:11:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
06/23/2022 08:12:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=369
06/23/2022 08:12:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=372
06/23/2022 08:12:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=374
06/23/2022 08:12:06 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.7479151919641351 on epoch=374
06/23/2022 08:12:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
06/23/2022 08:12:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=379
06/23/2022 08:12:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=382
06/23/2022 08:12:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=384
06/23/2022 08:12:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=387
06/23/2022 08:12:19 - INFO - __main__ - Global step 1550 Train loss 0.08 Classification-F1 0.7490301724137931 on epoch=387
06/23/2022 08:12:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=389
06/23/2022 08:12:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=392
06/23/2022 08:12:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=394
06/23/2022 08:12:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=397
06/23/2022 08:12:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=399
06/23/2022 08:12:32 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.7822219620240812 on epoch=399
06/23/2022 08:12:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7798037658802177 -> 0.7822219620240812 on epoch=399, global_step=1600
06/23/2022 08:12:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=402
06/23/2022 08:12:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=404
06/23/2022 08:12:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=407
06/23/2022 08:12:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=409
06/23/2022 08:12:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
06/23/2022 08:12:45 - INFO - __main__ - Global step 1650 Train loss 0.07 Classification-F1 0.7349750384024578 on epoch=412
06/23/2022 08:12:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=414
06/23/2022 08:12:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=417
06/23/2022 08:12:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=419
06/23/2022 08:12:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=422
06/23/2022 08:12:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=424
06/23/2022 08:12:58 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.7479151919641351 on epoch=424
06/23/2022 08:13:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
06/23/2022 08:13:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
06/23/2022 08:13:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
06/23/2022 08:13:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=434
06/23/2022 08:13:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=437
06/23/2022 08:13:11 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.7479151919641351 on epoch=437
06/23/2022 08:13:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=439
06/23/2022 08:13:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=442
06/23/2022 08:13:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=444
06/23/2022 08:13:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
06/23/2022 08:13:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=449
06/23/2022 08:13:24 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.7479151919641351 on epoch=449
06/23/2022 08:13:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
06/23/2022 08:13:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=454
06/23/2022 08:13:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
06/23/2022 08:13:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
06/23/2022 08:13:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=462
06/23/2022 08:13:37 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.7490301724137931 on epoch=462
06/23/2022 08:13:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=464
06/23/2022 08:13:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/23/2022 08:13:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=469
06/23/2022 08:13:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=472
06/23/2022 08:13:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
06/23/2022 08:13:50 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7487781036168134 on epoch=474
06/23/2022 08:13:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=477
06/23/2022 08:13:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=479
06/23/2022 08:13:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=482
06/23/2022 08:14:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
06/23/2022 08:14:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=487
06/23/2022 08:14:03 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.746969696969697 on epoch=487
06/23/2022 08:14:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
06/23/2022 08:14:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=492
06/23/2022 08:14:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
06/23/2022 08:14:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
06/23/2022 08:14:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
06/23/2022 08:14:16 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.7622041920216363 on epoch=499
06/23/2022 08:14:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=502
06/23/2022 08:14:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=504
06/23/2022 08:14:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
06/23/2022 08:14:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
06/23/2022 08:14:29 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
06/23/2022 08:14:30 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.7622041920216363 on epoch=512
06/23/2022 08:14:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/23/2022 08:14:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
06/23/2022 08:14:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.08 on epoch=519
06/23/2022 08:14:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
06/23/2022 08:14:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=524
06/23/2022 08:14:43 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.7335227272727274 on epoch=524
06/23/2022 08:14:45 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=527
06/23/2022 08:14:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
06/23/2022 08:14:50 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=532
06/23/2022 08:14:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
06/23/2022 08:14:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/23/2022 08:14:56 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.7641423724915186 on epoch=537
06/23/2022 08:14:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
06/23/2022 08:15:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
06/23/2022 08:15:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
06/23/2022 08:15:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
06/23/2022 08:15:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
06/23/2022 08:15:09 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7479151919641351 on epoch=549
06/23/2022 08:15:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
06/23/2022 08:15:14 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
06/23/2022 08:15:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
06/23/2022 08:15:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=559
06/23/2022 08:15:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
06/23/2022 08:15:22 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.7784313725490196 on epoch=562
06/23/2022 08:15:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
06/23/2022 08:15:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=567
06/23/2022 08:15:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
06/23/2022 08:15:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
06/23/2022 08:15:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=574
06/23/2022 08:15:35 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.7784313725490196 on epoch=574
06/23/2022 08:15:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/23/2022 08:15:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/23/2022 08:15:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=582
06/23/2022 08:15:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=584
06/23/2022 08:15:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=587
06/23/2022 08:15:48 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7622041920216363 on epoch=587
06/23/2022 08:15:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
06/23/2022 08:15:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.09 on epoch=592
06/23/2022 08:15:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/23/2022 08:15:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=597
06/23/2022 08:16:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
06/23/2022 08:16:01 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.7784313725490196 on epoch=599
06/23/2022 08:16:03 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=602
06/23/2022 08:16:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/23/2022 08:16:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
06/23/2022 08:16:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
06/23/2022 08:16:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/23/2022 08:16:14 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7784313725490196 on epoch=612
06/23/2022 08:16:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/23/2022 08:16:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
06/23/2022 08:16:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.11 on epoch=619
06/23/2022 08:16:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
06/23/2022 08:16:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=624
06/23/2022 08:16:27 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7456904978595746 on epoch=624
06/23/2022 08:16:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
06/23/2022 08:16:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
06/23/2022 08:16:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
06/23/2022 08:16:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
06/23/2022 08:16:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
06/23/2022 08:16:41 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7481060606060607 on epoch=637
06/23/2022 08:16:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/23/2022 08:16:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=642
06/23/2022 08:16:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
06/23/2022 08:16:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
06/23/2022 08:16:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/23/2022 08:16:54 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7488850195503421 on epoch=649
06/23/2022 08:16:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
06/23/2022 08:16:58 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/23/2022 08:17:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/23/2022 08:17:03 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
06/23/2022 08:17:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
06/23/2022 08:17:07 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7463488843813386 on epoch=662
06/23/2022 08:17:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
06/23/2022 08:17:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
06/23/2022 08:17:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/23/2022 08:17:16 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=672
06/23/2022 08:17:19 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.12 on epoch=674
06/23/2022 08:17:20 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.7456904978595746 on epoch=674
06/23/2022 08:17:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
06/23/2022 08:17:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
06/23/2022 08:17:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
06/23/2022 08:17:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/23/2022 08:17:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
06/23/2022 08:17:33 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7342680840664713 on epoch=687
06/23/2022 08:17:35 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=689
06/23/2022 08:17:37 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=692
06/23/2022 08:17:40 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/23/2022 08:17:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
06/23/2022 08:17:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/23/2022 08:17:46 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7474174347158219 on epoch=699
06/23/2022 08:17:48 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/23/2022 08:17:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/23/2022 08:17:53 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
06/23/2022 08:17:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=709
06/23/2022 08:17:58 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
06/23/2022 08:17:59 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7470588235294118 on epoch=712
06/23/2022 08:18:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/23/2022 08:18:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/23/2022 08:18:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/23/2022 08:18:08 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
06/23/2022 08:18:11 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
06/23/2022 08:18:12 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7347187110257187 on epoch=724
06/23/2022 08:18:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
06/23/2022 08:18:17 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
06/23/2022 08:18:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
06/23/2022 08:18:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/23/2022 08:18:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=737
06/23/2022 08:18:25 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7564814814814814 on epoch=737
06/23/2022 08:18:27 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
06/23/2022 08:18:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/23/2022 08:18:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=744
06/23/2022 08:18:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/23/2022 08:18:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
06/23/2022 08:18:38 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.7295844231328104 on epoch=749
06/23/2022 08:18:38 - INFO - __main__ - save last model!
06/23/2022 08:18:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 08:18:38 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 08:18:38 - INFO - __main__ - Printing 3 examples
06/23/2022 08:18:38 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 08:18:38 - INFO - __main__ - ['others']
06/23/2022 08:18:38 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 08:18:38 - INFO - __main__ - ['others']
06/23/2022 08:18:38 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 08:18:38 - INFO - __main__ - ['others']
06/23/2022 08:18:38 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:18:38 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:18:38 - INFO - __main__ - Printing 3 examples
06/23/2022 08:18:38 - INFO - __main__ -  [emo] how cause yes am listening
06/23/2022 08:18:38 - INFO - __main__ - ['others']
06/23/2022 08:18:38 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/23/2022 08:18:38 - INFO - __main__ - ['others']
06/23/2022 08:18:38 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/23/2022 08:18:38 - INFO - __main__ - ['others']
06/23/2022 08:18:38 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:18:38 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:18:38 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 08:18:38 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:18:38 - INFO - __main__ - Printing 3 examples
06/23/2022 08:18:38 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/23/2022 08:18:38 - INFO - __main__ - ['others']
06/23/2022 08:18:38 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/23/2022 08:18:38 - INFO - __main__ - ['others']
06/23/2022 08:18:38 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/23/2022 08:18:38 - INFO - __main__ - ['others']
06/23/2022 08:18:38 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:18:38 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:18:38 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 08:18:40 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:18:45 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 08:18:57 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 08:18:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 08:18:57 - INFO - __main__ - Starting training!
06/23/2022 08:20:10 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_100_0.3_8_predictions.txt
06/23/2022 08:20:10 - INFO - __main__ - Classification-F1 on test data: 0.2301
06/23/2022 08:20:11 - INFO - __main__ - prefix=emo_16_100, lr=0.3, bsz=8, dev_performance=0.7822219620240812, test_performance=0.23014322597081135
06/23/2022 08:20:11 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.2, bsz=8 ...
06/23/2022 08:20:12 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:20:12 - INFO - __main__ - Printing 3 examples
06/23/2022 08:20:12 - INFO - __main__ -  [emo] how cause yes am listening
06/23/2022 08:20:12 - INFO - __main__ - ['others']
06/23/2022 08:20:12 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/23/2022 08:20:12 - INFO - __main__ - ['others']
06/23/2022 08:20:12 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/23/2022 08:20:12 - INFO - __main__ - ['others']
06/23/2022 08:20:12 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:20:12 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:20:12 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 08:20:12 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:20:12 - INFO - __main__ - Printing 3 examples
06/23/2022 08:20:12 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/23/2022 08:20:12 - INFO - __main__ - ['others']
06/23/2022 08:20:12 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/23/2022 08:20:12 - INFO - __main__ - ['others']
06/23/2022 08:20:12 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/23/2022 08:20:12 - INFO - __main__ - ['others']
06/23/2022 08:20:12 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:20:12 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:20:12 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 08:20:31 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 08:20:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 08:20:32 - INFO - __main__ - Starting training!
06/23/2022 08:20:35 - INFO - __main__ - Step 10 Global step 10 Train loss 4.39 on epoch=2
06/23/2022 08:20:37 - INFO - __main__ - Step 20 Global step 20 Train loss 3.51 on epoch=4
06/23/2022 08:20:39 - INFO - __main__ - Step 30 Global step 30 Train loss 3.24 on epoch=7
06/23/2022 08:20:42 - INFO - __main__ - Step 40 Global step 40 Train loss 2.77 on epoch=9
06/23/2022 08:20:44 - INFO - __main__ - Step 50 Global step 50 Train loss 2.74 on epoch=12
06/23/2022 08:20:45 - INFO - __main__ - Global step 50 Train loss 3.33 Classification-F1 0.008547008547008546 on epoch=12
06/23/2022 08:20:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.008547008547008546 on epoch=12, global_step=50
06/23/2022 08:20:48 - INFO - __main__ - Step 60 Global step 60 Train loss 2.47 on epoch=14
06/23/2022 08:20:50 - INFO - __main__ - Step 70 Global step 70 Train loss 2.24 on epoch=17
06/23/2022 08:20:52 - INFO - __main__ - Step 80 Global step 80 Train loss 1.82 on epoch=19
06/23/2022 08:20:55 - INFO - __main__ - Step 90 Global step 90 Train loss 1.86 on epoch=22
06/23/2022 08:20:57 - INFO - __main__ - Step 100 Global step 100 Train loss 1.49 on epoch=24
06/23/2022 08:20:58 - INFO - __main__ - Global step 100 Train loss 1.98 Classification-F1 0.29084156408634587 on epoch=24
06/23/2022 08:20:58 - INFO - __main__ - Saving model with best Classification-F1: 0.008547008547008546 -> 0.29084156408634587 on epoch=24, global_step=100
06/23/2022 08:21:01 - INFO - __main__ - Step 110 Global step 110 Train loss 1.43 on epoch=27
06/23/2022 08:21:03 - INFO - __main__ - Step 120 Global step 120 Train loss 1.19 on epoch=29
06/23/2022 08:21:05 - INFO - __main__ - Step 130 Global step 130 Train loss 1.18 on epoch=32
06/23/2022 08:21:08 - INFO - __main__ - Step 140 Global step 140 Train loss 1.01 on epoch=34
06/23/2022 08:21:10 - INFO - __main__ - Step 150 Global step 150 Train loss 1.05 on epoch=37
06/23/2022 08:21:11 - INFO - __main__ - Global step 150 Train loss 1.17 Classification-F1 0.4874405576183962 on epoch=37
06/23/2022 08:21:11 - INFO - __main__ - Saving model with best Classification-F1: 0.29084156408634587 -> 0.4874405576183962 on epoch=37, global_step=150
06/23/2022 08:21:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.88 on epoch=39
06/23/2022 08:21:16 - INFO - __main__ - Step 170 Global step 170 Train loss 1.03 on epoch=42
06/23/2022 08:21:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.84 on epoch=44
06/23/2022 08:21:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.81 on epoch=47
06/23/2022 08:21:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=49
06/23/2022 08:21:24 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.5075980392156862 on epoch=49
06/23/2022 08:21:24 - INFO - __main__ - Saving model with best Classification-F1: 0.4874405576183962 -> 0.5075980392156862 on epoch=49, global_step=200
06/23/2022 08:21:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.80 on epoch=52
06/23/2022 08:21:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.78 on epoch=54
06/23/2022 08:21:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.73 on epoch=57
06/23/2022 08:21:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.81 on epoch=59
06/23/2022 08:21:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.78 on epoch=62
06/23/2022 08:21:37 - INFO - __main__ - Global step 250 Train loss 0.78 Classification-F1 0.5494883040935672 on epoch=62
06/23/2022 08:21:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5075980392156862 -> 0.5494883040935672 on epoch=62, global_step=250
06/23/2022 08:21:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.75 on epoch=64
06/23/2022 08:21:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.71 on epoch=67
06/23/2022 08:21:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.72 on epoch=69
06/23/2022 08:21:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.74 on epoch=72
06/23/2022 08:21:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.76 on epoch=74
06/23/2022 08:21:50 - INFO - __main__ - Global step 300 Train loss 0.74 Classification-F1 0.55007215007215 on epoch=74
06/23/2022 08:21:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5494883040935672 -> 0.55007215007215 on epoch=74, global_step=300
06/23/2022 08:21:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.64 on epoch=77
06/23/2022 08:21:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.80 on epoch=79
06/23/2022 08:21:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.74 on epoch=82
06/23/2022 08:22:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.62 on epoch=84
06/23/2022 08:22:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.59 on epoch=87
06/23/2022 08:22:03 - INFO - __main__ - Global step 350 Train loss 0.68 Classification-F1 0.6089285714285714 on epoch=87
06/23/2022 08:22:03 - INFO - __main__ - Saving model with best Classification-F1: 0.55007215007215 -> 0.6089285714285714 on epoch=87, global_step=350
06/23/2022 08:22:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.62 on epoch=89
06/23/2022 08:22:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.67 on epoch=92
06/23/2022 08:22:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.59 on epoch=94
06/23/2022 08:22:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.72 on epoch=97
06/23/2022 08:22:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.71 on epoch=99
06/23/2022 08:22:16 - INFO - __main__ - Global step 400 Train loss 0.66 Classification-F1 0.6065312377722708 on epoch=99
06/23/2022 08:22:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.74 on epoch=102
06/23/2022 08:22:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.68 on epoch=104
06/23/2022 08:22:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.63 on epoch=107
06/23/2022 08:22:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.66 on epoch=109
06/23/2022 08:22:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.69 on epoch=112
06/23/2022 08:22:29 - INFO - __main__ - Global step 450 Train loss 0.68 Classification-F1 0.6183284457478005 on epoch=112
06/23/2022 08:22:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6089285714285714 -> 0.6183284457478005 on epoch=112, global_step=450
06/23/2022 08:22:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.60 on epoch=114
06/23/2022 08:22:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.56 on epoch=117
06/23/2022 08:22:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.57 on epoch=119
06/23/2022 08:22:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.59 on epoch=122
06/23/2022 08:22:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.54 on epoch=124
06/23/2022 08:22:42 - INFO - __main__ - Global step 500 Train loss 0.57 Classification-F1 0.6304115853658536 on epoch=124
06/23/2022 08:22:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6183284457478005 -> 0.6304115853658536 on epoch=124, global_step=500
06/23/2022 08:22:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.65 on epoch=127
06/23/2022 08:22:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.62 on epoch=129
06/23/2022 08:22:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.61 on epoch=132
06/23/2022 08:22:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.53 on epoch=134
06/23/2022 08:22:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.68 on epoch=137
06/23/2022 08:22:55 - INFO - __main__ - Global step 550 Train loss 0.62 Classification-F1 0.653422619047619 on epoch=137
06/23/2022 08:22:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6304115853658536 -> 0.653422619047619 on epoch=137, global_step=550
06/23/2022 08:22:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.54 on epoch=139
06/23/2022 08:23:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.58 on epoch=142
06/23/2022 08:23:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.53 on epoch=144
06/23/2022 08:23:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=147
06/23/2022 08:23:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=149
06/23/2022 08:23:08 - INFO - __main__ - Global step 600 Train loss 0.53 Classification-F1 0.653422619047619 on epoch=149
06/23/2022 08:23:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.58 on epoch=152
06/23/2022 08:23:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.60 on epoch=154
06/23/2022 08:23:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.62 on epoch=157
06/23/2022 08:23:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=159
06/23/2022 08:23:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.54 on epoch=162
06/23/2022 08:23:21 - INFO - __main__ - Global step 650 Train loss 0.57 Classification-F1 0.6798704954954955 on epoch=162
06/23/2022 08:23:21 - INFO - __main__ - Saving model with best Classification-F1: 0.653422619047619 -> 0.6798704954954955 on epoch=162, global_step=650
06/23/2022 08:23:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=164
06/23/2022 08:23:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.56 on epoch=167
06/23/2022 08:23:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.60 on epoch=169
06/23/2022 08:23:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.56 on epoch=172
06/23/2022 08:23:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.57 on epoch=174
06/23/2022 08:23:34 - INFO - __main__ - Global step 700 Train loss 0.57 Classification-F1 0.6877258158508158 on epoch=174
06/23/2022 08:23:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6798704954954955 -> 0.6877258158508158 on epoch=174, global_step=700
06/23/2022 08:23:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.51 on epoch=177
06/23/2022 08:23:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=179
06/23/2022 08:23:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=182
06/23/2022 08:23:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=184
06/23/2022 08:23:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=187
06/23/2022 08:23:47 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.6877258158508158 on epoch=187
06/23/2022 08:23:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=189
06/23/2022 08:23:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.54 on epoch=192
06/23/2022 08:23:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=194
06/23/2022 08:23:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=197
06/23/2022 08:23:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.50 on epoch=199
06/23/2022 08:24:00 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.6652014652014653 on epoch=199
06/23/2022 08:24:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=202
06/23/2022 08:24:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=204
06/23/2022 08:24:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.49 on epoch=207
06/23/2022 08:24:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=209
06/23/2022 08:24:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=212
06/23/2022 08:24:13 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.6795207716260347 on epoch=212
06/23/2022 08:24:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=214
06/23/2022 08:24:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=217
06/23/2022 08:24:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.32 on epoch=219
06/23/2022 08:24:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=222
06/23/2022 08:24:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=224
06/23/2022 08:24:26 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.6939510939510939 on epoch=224
06/23/2022 08:24:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6877258158508158 -> 0.6939510939510939 on epoch=224, global_step=900
06/23/2022 08:24:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=227
06/23/2022 08:24:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=229
06/23/2022 08:24:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=232
06/23/2022 08:24:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=234
06/23/2022 08:24:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=237
06/23/2022 08:24:39 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.6796653796653797 on epoch=237
06/23/2022 08:24:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=239
06/23/2022 08:24:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=242
06/23/2022 08:24:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=244
06/23/2022 08:24:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=247
06/23/2022 08:24:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.31 on epoch=249
06/23/2022 08:24:52 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.649896978021978 on epoch=249
06/23/2022 08:24:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=252
06/23/2022 08:24:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=254
06/23/2022 08:24:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=257
06/23/2022 08:25:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=259
06/23/2022 08:25:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=262
06/23/2022 08:25:04 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.6862148268398268 on epoch=262
06/23/2022 08:25:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.27 on epoch=264
06/23/2022 08:25:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=267
06/23/2022 08:25:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=269
06/23/2022 08:25:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=272
06/23/2022 08:25:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=274
06/23/2022 08:25:18 - INFO - __main__ - Global step 1100 Train loss 0.32 Classification-F1 0.6646020646020645 on epoch=274
06/23/2022 08:25:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=277
06/23/2022 08:25:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.31 on epoch=279
06/23/2022 08:25:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=282
06/23/2022 08:25:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=284
06/23/2022 08:25:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=287
06/23/2022 08:25:30 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.6654939487756826 on epoch=287
06/23/2022 08:25:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=289
06/23/2022 08:25:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=292
06/23/2022 08:25:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=294
06/23/2022 08:25:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=297
06/23/2022 08:25:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.30 on epoch=299
06/23/2022 08:25:43 - INFO - __main__ - Global step 1200 Train loss 0.27 Classification-F1 0.7021116138763198 on epoch=299
06/23/2022 08:25:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6939510939510939 -> 0.7021116138763198 on epoch=299, global_step=1200
06/23/2022 08:25:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=302
06/23/2022 08:25:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=304
06/23/2022 08:25:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.29 on epoch=307
06/23/2022 08:25:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=309
06/23/2022 08:25:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.28 on epoch=312
06/23/2022 08:25:56 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.7078656597774244 on epoch=312
06/23/2022 08:25:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7021116138763198 -> 0.7078656597774244 on epoch=312, global_step=1250
06/23/2022 08:25:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=314
06/23/2022 08:26:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=317
06/23/2022 08:26:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=319
06/23/2022 08:26:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.29 on epoch=322
06/23/2022 08:26:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=324
06/23/2022 08:26:09 - INFO - __main__ - Global step 1300 Train loss 0.24 Classification-F1 0.7015684162423292 on epoch=324
06/23/2022 08:26:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=327
06/23/2022 08:26:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.27 on epoch=329
06/23/2022 08:26:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=332
06/23/2022 08:26:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=334
06/23/2022 08:26:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=337
06/23/2022 08:26:22 - INFO - __main__ - Global step 1350 Train loss 0.23 Classification-F1 0.7463352007469654 on epoch=337
06/23/2022 08:26:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7078656597774244 -> 0.7463352007469654 on epoch=337, global_step=1350
06/23/2022 08:26:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=339
06/23/2022 08:26:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=342
06/23/2022 08:26:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=344
06/23/2022 08:26:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.25 on epoch=347
06/23/2022 08:26:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=349
06/23/2022 08:26:35 - INFO - __main__ - Global step 1400 Train loss 0.23 Classification-F1 0.6944444444444444 on epoch=349
06/23/2022 08:26:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.26 on epoch=352
06/23/2022 08:26:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=354
06/23/2022 08:26:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=357
06/23/2022 08:26:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=359
06/23/2022 08:26:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=362
06/23/2022 08:26:48 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.7127332144979204 on epoch=362
06/23/2022 08:26:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=364
06/23/2022 08:26:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.23 on epoch=367
06/23/2022 08:26:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=369
06/23/2022 08:26:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=372
06/23/2022 08:27:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=374
06/23/2022 08:27:01 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.7463352007469654 on epoch=374
06/23/2022 08:27:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=377
06/23/2022 08:27:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=379
06/23/2022 08:27:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=382
06/23/2022 08:27:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=384
06/23/2022 08:27:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=387
06/23/2022 08:27:14 - INFO - __main__ - Global step 1550 Train loss 0.17 Classification-F1 0.7227375762859634 on epoch=387
06/23/2022 08:27:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=389
06/23/2022 08:27:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=392
06/23/2022 08:27:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.15 on epoch=394
06/23/2022 08:27:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.26 on epoch=397
06/23/2022 08:27:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=399
06/23/2022 08:27:27 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.693524531024531 on epoch=399
06/23/2022 08:27:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=402
06/23/2022 08:27:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.15 on epoch=404
06/23/2022 08:27:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.16 on epoch=407
06/23/2022 08:27:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=409
06/23/2022 08:27:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.15 on epoch=412
06/23/2022 08:27:40 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.7458359687961206 on epoch=412
06/23/2022 08:27:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=414
06/23/2022 08:27:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=417
06/23/2022 08:27:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=419
06/23/2022 08:27:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.12 on epoch=422
06/23/2022 08:27:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=424
06/23/2022 08:27:53 - INFO - __main__ - Global step 1700 Train loss 0.15 Classification-F1 0.761111111111111 on epoch=424
06/23/2022 08:27:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7463352007469654 -> 0.761111111111111 on epoch=424, global_step=1700
06/23/2022 08:27:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=427
06/23/2022 08:27:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=429
06/23/2022 08:28:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=432
06/23/2022 08:28:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.16 on epoch=434
06/23/2022 08:28:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=437
06/23/2022 08:28:06 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.7578898527174389 on epoch=437
06/23/2022 08:28:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.15 on epoch=439
06/23/2022 08:28:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.12 on epoch=442
06/23/2022 08:28:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=444
06/23/2022 08:28:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=447
06/23/2022 08:28:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=449
06/23/2022 08:28:19 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.761111111111111 on epoch=449
06/23/2022 08:28:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=452
06/23/2022 08:28:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.14 on epoch=454
06/23/2022 08:28:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=457
06/23/2022 08:28:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=459
06/23/2022 08:28:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.11 on epoch=462
06/23/2022 08:28:32 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.7418389853873725 on epoch=462
06/23/2022 08:28:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=464
06/23/2022 08:28:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=467
06/23/2022 08:28:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=469
06/23/2022 08:28:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=472
06/23/2022 08:28:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=474
06/23/2022 08:28:45 - INFO - __main__ - Global step 1900 Train loss 0.13 Classification-F1 0.7439335887611749 on epoch=474
06/23/2022 08:28:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=477
06/23/2022 08:28:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=479
06/23/2022 08:28:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=482
06/23/2022 08:28:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=484
06/23/2022 08:28:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=487
06/23/2022 08:28:58 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.7640104317589105 on epoch=487
06/23/2022 08:28:58 - INFO - __main__ - Saving model with best Classification-F1: 0.761111111111111 -> 0.7640104317589105 on epoch=487, global_step=1950
06/23/2022 08:29:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=489
06/23/2022 08:29:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=492
06/23/2022 08:29:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=494
06/23/2022 08:29:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=497
06/23/2022 08:29:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=499
06/23/2022 08:29:11 - INFO - __main__ - Global step 2000 Train loss 0.09 Classification-F1 0.7578898527174389 on epoch=499
06/23/2022 08:29:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=502
06/23/2022 08:29:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=504
06/23/2022 08:29:19 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=507
06/23/2022 08:29:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=509
06/23/2022 08:29:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.11 on epoch=512
06/23/2022 08:29:24 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.7949081624728177 on epoch=512
06/23/2022 08:29:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7640104317589105 -> 0.7949081624728177 on epoch=512, global_step=2050
06/23/2022 08:29:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=514
06/23/2022 08:29:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.12 on epoch=517
06/23/2022 08:29:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=519
06/23/2022 08:29:34 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.11 on epoch=522
06/23/2022 08:29:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=524
06/23/2022 08:29:37 - INFO - __main__ - Global step 2100 Train loss 0.08 Classification-F1 0.778982744499986 on epoch=524
06/23/2022 08:29:40 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=527
06/23/2022 08:29:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.14 on epoch=529
06/23/2022 08:29:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=532
06/23/2022 08:29:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=534
06/23/2022 08:29:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=537
06/23/2022 08:29:50 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.7598455598455598 on epoch=537
06/23/2022 08:29:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=539
06/23/2022 08:29:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=542
06/23/2022 08:29:58 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.12 on epoch=544
06/23/2022 08:30:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.10 on epoch=547
06/23/2022 08:30:03 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=549
06/23/2022 08:30:04 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.7949081624728177 on epoch=549
06/23/2022 08:30:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=552
06/23/2022 08:30:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=554
06/23/2022 08:30:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=557
06/23/2022 08:30:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=559
06/23/2022 08:30:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=562
06/23/2022 08:30:17 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.734269781144781 on epoch=562
06/23/2022 08:30:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=564
06/23/2022 08:30:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
06/23/2022 08:30:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=569
06/23/2022 08:30:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=572
06/23/2022 08:30:29 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=574
06/23/2022 08:30:30 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.7430603016809912 on epoch=574
06/23/2022 08:30:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=577
06/23/2022 08:30:34 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.15 on epoch=579
06/23/2022 08:30:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=582
06/23/2022 08:30:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=584
06/23/2022 08:30:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=587
06/23/2022 08:30:43 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.7472628878878879 on epoch=587
06/23/2022 08:30:45 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=589
06/23/2022 08:30:47 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=592
06/23/2022 08:30:50 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=594
06/23/2022 08:30:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=597
06/23/2022 08:30:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
06/23/2022 08:30:56 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.7774274201758988 on epoch=599
06/23/2022 08:30:58 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=602
06/23/2022 08:31:00 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=604
06/23/2022 08:31:03 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=607
06/23/2022 08:31:05 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.13 on epoch=609
06/23/2022 08:31:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
06/23/2022 08:31:09 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.7612994225897451 on epoch=612
06/23/2022 08:31:11 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.09 on epoch=614
06/23/2022 08:31:13 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
06/23/2022 08:31:16 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=619
06/23/2022 08:31:18 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=622
06/23/2022 08:31:21 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=624
06/23/2022 08:31:22 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.760436907495731 on epoch=624
06/23/2022 08:31:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=627
06/23/2022 08:31:26 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
06/23/2022 08:31:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=632
06/23/2022 08:31:31 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
06/23/2022 08:31:34 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
06/23/2022 08:31:35 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.7612994225897451 on epoch=637
06/23/2022 08:31:37 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=639
06/23/2022 08:31:39 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=642
06/23/2022 08:31:42 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
06/23/2022 08:31:44 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
06/23/2022 08:31:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=649
06/23/2022 08:31:48 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.7950159210935073 on epoch=649
06/23/2022 08:31:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7949081624728177 -> 0.7950159210935073 on epoch=649, global_step=2600
06/23/2022 08:31:50 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=652
06/23/2022 08:31:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.12 on epoch=654
06/23/2022 08:31:55 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=657
06/23/2022 08:31:57 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=659
06/23/2022 08:31:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
06/23/2022 08:32:00 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.7480008417508417 on epoch=662
06/23/2022 08:32:03 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
06/23/2022 08:32:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.10 on epoch=667
06/23/2022 08:32:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=669
06/23/2022 08:32:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
06/23/2022 08:32:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
06/23/2022 08:32:13 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.7468023051514512 on epoch=674
06/23/2022 08:32:16 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=677
06/23/2022 08:32:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
06/23/2022 08:32:21 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=682
06/23/2022 08:32:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=684
06/23/2022 08:32:26 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=687
06/23/2022 08:32:27 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.7833974477814856 on epoch=687
06/23/2022 08:32:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=689
06/23/2022 08:32:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=692
06/23/2022 08:32:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=694
06/23/2022 08:32:36 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=697
06/23/2022 08:32:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
06/23/2022 08:32:40 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.7480008417508417 on epoch=699
06/23/2022 08:32:42 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
06/23/2022 08:32:44 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=704
06/23/2022 08:32:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=707
06/23/2022 08:32:49 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=709
06/23/2022 08:32:52 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=712
06/23/2022 08:32:53 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.7833974477814856 on epoch=712
06/23/2022 08:32:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
06/23/2022 08:32:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=717
06/23/2022 08:33:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
06/23/2022 08:33:02 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=722
06/23/2022 08:33:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
06/23/2022 08:33:06 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.7296296296296295 on epoch=724
06/23/2022 08:33:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=727
06/23/2022 08:33:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
06/23/2022 08:33:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=732
06/23/2022 08:33:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=734
06/23/2022 08:33:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
06/23/2022 08:33:19 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.7468023051514512 on epoch=737
06/23/2022 08:33:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
06/23/2022 08:33:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=742
06/23/2022 08:33:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=744
06/23/2022 08:33:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=747
06/23/2022 08:33:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
06/23/2022 08:33:32 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.7833974477814856 on epoch=749
06/23/2022 08:33:32 - INFO - __main__ - save last model!
06/23/2022 08:33:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 08:33:32 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 08:33:32 - INFO - __main__ - Printing 3 examples
06/23/2022 08:33:32 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 08:33:32 - INFO - __main__ - ['others']
06/23/2022 08:33:32 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 08:33:32 - INFO - __main__ - ['others']
06/23/2022 08:33:32 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 08:33:32 - INFO - __main__ - ['others']
06/23/2022 08:33:32 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:33:32 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:33:32 - INFO - __main__ - Printing 3 examples
06/23/2022 08:33:32 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/23/2022 08:33:32 - INFO - __main__ - ['others']
06/23/2022 08:33:32 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/23/2022 08:33:32 - INFO - __main__ - ['others']
06/23/2022 08:33:32 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/23/2022 08:33:32 - INFO - __main__ - ['others']
06/23/2022 08:33:32 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:33:32 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:33:32 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 08:33:32 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:33:32 - INFO - __main__ - Printing 3 examples
06/23/2022 08:33:32 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/23/2022 08:33:32 - INFO - __main__ - ['others']
06/23/2022 08:33:32 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/23/2022 08:33:32 - INFO - __main__ - ['others']
06/23/2022 08:33:32 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/23/2022 08:33:32 - INFO - __main__ - ['others']
06/23/2022 08:33:32 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:33:32 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:33:32 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 08:33:34 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:33:39 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 08:33:51 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 08:33:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 08:33:52 - INFO - __main__ - Starting training!
06/23/2022 08:35:04 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_100_0.2_8_predictions.txt
06/23/2022 08:35:04 - INFO - __main__ - Classification-F1 on test data: 0.2111
06/23/2022 08:35:04 - INFO - __main__ - prefix=emo_16_100, lr=0.2, bsz=8, dev_performance=0.7950159210935073, test_performance=0.21113721506441593
06/23/2022 08:35:04 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.5, bsz=8 ...
06/23/2022 08:35:05 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:35:05 - INFO - __main__ - Printing 3 examples
06/23/2022 08:35:05 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/23/2022 08:35:05 - INFO - __main__ - ['others']
06/23/2022 08:35:05 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/23/2022 08:35:05 - INFO - __main__ - ['others']
06/23/2022 08:35:05 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/23/2022 08:35:05 - INFO - __main__ - ['others']
06/23/2022 08:35:05 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:35:05 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:35:05 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 08:35:05 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:35:05 - INFO - __main__ - Printing 3 examples
06/23/2022 08:35:05 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/23/2022 08:35:05 - INFO - __main__ - ['others']
06/23/2022 08:35:05 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/23/2022 08:35:05 - INFO - __main__ - ['others']
06/23/2022 08:35:05 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/23/2022 08:35:05 - INFO - __main__ - ['others']
06/23/2022 08:35:05 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:35:05 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:35:05 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 08:35:21 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 08:35:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 08:35:22 - INFO - __main__ - Starting training!
06/23/2022 08:35:25 - INFO - __main__ - Step 10 Global step 10 Train loss 4.01 on epoch=2
06/23/2022 08:35:27 - INFO - __main__ - Step 20 Global step 20 Train loss 2.94 on epoch=4
06/23/2022 08:35:30 - INFO - __main__ - Step 30 Global step 30 Train loss 2.53 on epoch=7
06/23/2022 08:35:32 - INFO - __main__ - Step 40 Global step 40 Train loss 1.79 on epoch=9
06/23/2022 08:35:34 - INFO - __main__ - Step 50 Global step 50 Train loss 1.38 on epoch=12
06/23/2022 08:35:35 - INFO - __main__ - Global step 50 Train loss 2.53 Classification-F1 0.15758701845658368 on epoch=12
06/23/2022 08:35:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15758701845658368 on epoch=12, global_step=50
06/23/2022 08:35:38 - INFO - __main__ - Step 60 Global step 60 Train loss 1.17 on epoch=14
06/23/2022 08:35:40 - INFO - __main__ - Step 70 Global step 70 Train loss 1.01 on epoch=17
06/23/2022 08:35:43 - INFO - __main__ - Step 80 Global step 80 Train loss 0.88 on epoch=19
06/23/2022 08:35:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.73 on epoch=22
06/23/2022 08:35:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.63 on epoch=24
06/23/2022 08:35:49 - INFO - __main__ - Global step 100 Train loss 0.89 Classification-F1 0.5220116141168772 on epoch=24
06/23/2022 08:35:49 - INFO - __main__ - Saving model with best Classification-F1: 0.15758701845658368 -> 0.5220116141168772 on epoch=24, global_step=100
06/23/2022 08:35:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.73 on epoch=27
06/23/2022 08:35:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.65 on epoch=29
06/23/2022 08:35:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.70 on epoch=32
06/23/2022 08:35:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=34
06/23/2022 08:36:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=37
06/23/2022 08:36:02 - INFO - __main__ - Global step 150 Train loss 0.65 Classification-F1 0.6177880044939936 on epoch=37
06/23/2022 08:36:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5220116141168772 -> 0.6177880044939936 on epoch=37, global_step=150
06/23/2022 08:36:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=39
06/23/2022 08:36:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.68 on epoch=42
06/23/2022 08:36:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=44
06/23/2022 08:36:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.69 on epoch=47
06/23/2022 08:36:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=49
06/23/2022 08:36:15 - INFO - __main__ - Global step 200 Train loss 0.56 Classification-F1 0.6717661149825784 on epoch=49
06/23/2022 08:36:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6177880044939936 -> 0.6717661149825784 on epoch=49, global_step=200
06/23/2022 08:36:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=52
06/23/2022 08:36:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=54
06/23/2022 08:36:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=57
06/23/2022 08:36:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=59
06/23/2022 08:36:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=62
06/23/2022 08:36:29 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.7590690559440559 on epoch=62
06/23/2022 08:36:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6717661149825784 -> 0.7590690559440559 on epoch=62, global_step=250
06/23/2022 08:36:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=64
06/23/2022 08:36:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.31 on epoch=67
06/23/2022 08:36:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.36 on epoch=69
06/23/2022 08:36:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=72
06/23/2022 08:36:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=74
06/23/2022 08:36:42 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.7378139483294617 on epoch=74
06/23/2022 08:36:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=77
06/23/2022 08:36:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=79
06/23/2022 08:36:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=82
06/23/2022 08:36:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=84
06/23/2022 08:36:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=87
06/23/2022 08:36:55 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.7581900452488688 on epoch=87
06/23/2022 08:36:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=89
06/23/2022 08:37:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.32 on epoch=92
06/23/2022 08:37:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=94
06/23/2022 08:37:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=97
06/23/2022 08:37:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=99
06/23/2022 08:37:08 - INFO - __main__ - Global step 400 Train loss 0.29 Classification-F1 0.8106442577030812 on epoch=99
06/23/2022 08:37:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7590690559440559 -> 0.8106442577030812 on epoch=99, global_step=400
06/23/2022 08:37:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=102
06/23/2022 08:37:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=104
06/23/2022 08:37:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=107
06/23/2022 08:37:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=109
06/23/2022 08:37:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=112
06/23/2022 08:37:21 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.7784059719543591 on epoch=112
06/23/2022 08:37:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.19 on epoch=114
06/23/2022 08:37:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=117
06/23/2022 08:37:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=119
06/23/2022 08:37:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=122
06/23/2022 08:37:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=124
06/23/2022 08:37:35 - INFO - __main__ - Global step 500 Train loss 0.21 Classification-F1 0.7217503217503218 on epoch=124
06/23/2022 08:37:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=127
06/23/2022 08:37:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=129
06/23/2022 08:37:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=132
06/23/2022 08:37:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=134
06/23/2022 08:37:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=137
06/23/2022 08:37:48 - INFO - __main__ - Global step 550 Train loss 0.19 Classification-F1 0.7968029688364373 on epoch=137
06/23/2022 08:37:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=139
06/23/2022 08:37:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=142
06/23/2022 08:37:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=144
06/23/2022 08:37:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=147
06/23/2022 08:38:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=149
06/23/2022 08:38:01 - INFO - __main__ - Global step 600 Train loss 0.17 Classification-F1 0.7614665800149671 on epoch=149
06/23/2022 08:38:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=152
06/23/2022 08:38:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=154
06/23/2022 08:38:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=157
06/23/2022 08:38:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=159
06/23/2022 08:38:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=162
06/23/2022 08:38:15 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.7222222222222223 on epoch=162
06/23/2022 08:38:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=164
06/23/2022 08:38:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=167
06/23/2022 08:38:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=169
06/23/2022 08:38:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=172
06/23/2022 08:38:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=174
06/23/2022 08:38:28 - INFO - __main__ - Global step 700 Train loss 0.14 Classification-F1 0.7222222222222223 on epoch=174
06/23/2022 08:38:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=177
06/23/2022 08:38:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.13 on epoch=179
06/23/2022 08:38:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=182
06/23/2022 08:38:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=184
06/23/2022 08:38:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=187
06/23/2022 08:38:41 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.7789877061935885 on epoch=187
06/23/2022 08:38:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=189
06/23/2022 08:38:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=192
06/23/2022 08:38:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=194
06/23/2022 08:38:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=197
06/23/2022 08:38:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=199
06/23/2022 08:38:55 - INFO - __main__ - Global step 800 Train loss 0.09 Classification-F1 0.7937662520205214 on epoch=199
06/23/2022 08:38:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=202
06/23/2022 08:38:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=204
06/23/2022 08:39:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=207
06/23/2022 08:39:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=209
06/23/2022 08:39:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=212
06/23/2022 08:39:08 - INFO - __main__ - Global step 850 Train loss 0.06 Classification-F1 0.7580312407898615 on epoch=212
06/23/2022 08:39:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=214
06/23/2022 08:39:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=217
06/23/2022 08:39:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=219
06/23/2022 08:39:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=222
06/23/2022 08:39:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=224
06/23/2022 08:39:21 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.7583471487066814 on epoch=224
06/23/2022 08:39:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=227
06/23/2022 08:39:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=229
06/23/2022 08:39:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=232
06/23/2022 08:39:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=234
06/23/2022 08:39:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=237
06/23/2022 08:39:35 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.7564327485380117 on epoch=237
06/23/2022 08:39:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=239
06/23/2022 08:39:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=242
06/23/2022 08:39:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
06/23/2022 08:39:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=247
06/23/2022 08:39:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
06/23/2022 08:39:48 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.7378779610316114 on epoch=249
06/23/2022 08:39:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=252
06/23/2022 08:39:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=254
06/23/2022 08:39:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=257
06/23/2022 08:39:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=259
06/23/2022 08:40:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=262
06/23/2022 08:40:01 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.8265533486121722 on epoch=262
06/23/2022 08:40:01 - INFO - __main__ - Saving model with best Classification-F1: 0.8106442577030812 -> 0.8265533486121722 on epoch=262, global_step=1050
06/23/2022 08:40:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=264
06/23/2022 08:40:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
06/23/2022 08:40:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
06/23/2022 08:40:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
06/23/2022 08:40:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=274
06/23/2022 08:40:15 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.7937662520205214 on epoch=274
06/23/2022 08:40:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
06/23/2022 08:40:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
06/23/2022 08:40:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=282
06/23/2022 08:40:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=284
06/23/2022 08:40:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
06/23/2022 08:40:28 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.7937662520205214 on epoch=287
06/23/2022 08:40:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
06/23/2022 08:40:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
06/23/2022 08:40:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
06/23/2022 08:40:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
06/23/2022 08:40:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
06/23/2022 08:40:41 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.7951954887438758 on epoch=299
06/23/2022 08:40:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
06/23/2022 08:40:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=304
06/23/2022 08:40:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
06/23/2022 08:40:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=309
06/23/2022 08:40:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
06/23/2022 08:40:54 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.8277732683982684 on epoch=312
06/23/2022 08:40:54 - INFO - __main__ - Saving model with best Classification-F1: 0.8265533486121722 -> 0.8277732683982684 on epoch=312, global_step=1250
06/23/2022 08:40:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
06/23/2022 08:40:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
06/23/2022 08:41:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
06/23/2022 08:41:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
06/23/2022 08:41:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
06/23/2022 08:41:08 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.7866658363432558 on epoch=324
06/23/2022 08:41:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
06/23/2022 08:41:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
06/23/2022 08:41:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=332
06/23/2022 08:41:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
06/23/2022 08:41:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
06/23/2022 08:41:21 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.8265533486121722 on epoch=337
06/23/2022 08:41:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
06/23/2022 08:41:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
06/23/2022 08:41:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=344
06/23/2022 08:41:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=347
06/23/2022 08:41:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
06/23/2022 08:41:34 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.7720023767082591 on epoch=349
06/23/2022 08:41:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=352
06/23/2022 08:41:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
06/23/2022 08:41:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
06/23/2022 08:41:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
06/23/2022 08:41:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
06/23/2022 08:41:48 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.8112572223246667 on epoch=362
06/23/2022 08:41:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
06/23/2022 08:41:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
06/23/2022 08:41:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
06/23/2022 08:41:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
06/23/2022 08:42:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
06/23/2022 08:42:01 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.7906348553407376 on epoch=374
06/23/2022 08:42:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
06/23/2022 08:42:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
06/23/2022 08:42:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/23/2022 08:42:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
06/23/2022 08:42:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=387
06/23/2022 08:42:15 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.8124313209963933 on epoch=387
06/23/2022 08:42:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
06/23/2022 08:42:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
06/23/2022 08:42:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
06/23/2022 08:42:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
06/23/2022 08:42:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/23/2022 08:42:28 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.7515331890331891 on epoch=399
06/23/2022 08:42:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
06/23/2022 08:42:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
06/23/2022 08:42:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
06/23/2022 08:42:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
06/23/2022 08:42:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
06/23/2022 08:42:41 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7434966607292584 on epoch=412
06/23/2022 08:42:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
06/23/2022 08:42:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
06/23/2022 08:42:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/23/2022 08:42:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
06/23/2022 08:42:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
06/23/2022 08:42:55 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.7912346028154851 on epoch=424
06/23/2022 08:42:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
06/23/2022 08:43:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
06/23/2022 08:43:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
06/23/2022 08:43:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=434
06/23/2022 08:43:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
06/23/2022 08:43:08 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.7515331890331891 on epoch=437
06/23/2022 08:43:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
06/23/2022 08:43:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
06/23/2022 08:43:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
06/23/2022 08:43:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
06/23/2022 08:43:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
06/23/2022 08:43:22 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7864583333333333 on epoch=449
06/23/2022 08:43:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
06/23/2022 08:43:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
06/23/2022 08:43:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
06/23/2022 08:43:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
06/23/2022 08:43:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
06/23/2022 08:43:35 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.8115301724137931 on epoch=462
06/23/2022 08:43:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
06/23/2022 08:43:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/23/2022 08:43:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
06/23/2022 08:43:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
06/23/2022 08:43:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
06/23/2022 08:43:48 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.7866522366522366 on epoch=474
06/23/2022 08:43:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
06/23/2022 08:43:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
06/23/2022 08:43:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
06/23/2022 08:43:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/23/2022 08:44:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
06/23/2022 08:44:02 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7439939939939939 on epoch=487
06/23/2022 08:44:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
06/23/2022 08:44:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
06/23/2022 08:44:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
06/23/2022 08:44:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/23/2022 08:44:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
06/23/2022 08:44:15 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.8067082590612003 on epoch=499
06/23/2022 08:44:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
06/23/2022 08:44:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
06/23/2022 08:44:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
06/23/2022 08:44:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/23/2022 08:44:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
06/23/2022 08:44:29 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.8251712543991956 on epoch=512
06/23/2022 08:44:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/23/2022 08:44:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/23/2022 08:44:36 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
06/23/2022 08:44:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
06/23/2022 08:44:41 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
06/23/2022 08:44:42 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.8278231625005819 on epoch=524
06/23/2022 08:44:42 - INFO - __main__ - Saving model with best Classification-F1: 0.8277732683982684 -> 0.8278231625005819 on epoch=524, global_step=2100
06/23/2022 08:44:45 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
06/23/2022 08:44:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=529
06/23/2022 08:44:50 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
06/23/2022 08:44:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
06/23/2022 08:44:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/23/2022 08:44:56 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7912346028154851 on epoch=537
06/23/2022 08:44:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/23/2022 08:45:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
06/23/2022 08:45:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
06/23/2022 08:45:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/23/2022 08:45:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/23/2022 08:45:09 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.8125270362130763 on epoch=549
06/23/2022 08:45:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=552
06/23/2022 08:45:14 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
06/23/2022 08:45:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
06/23/2022 08:45:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
06/23/2022 08:45:22 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
06/23/2022 08:45:23 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7972444581280789 on epoch=562
06/23/2022 08:45:25 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
06/23/2022 08:45:28 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
06/23/2022 08:45:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
06/23/2022 08:45:33 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/23/2022 08:45:35 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
06/23/2022 08:45:36 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.8128904878904878 on epoch=574
06/23/2022 08:45:39 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/23/2022 08:45:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.07 on epoch=579
06/23/2022 08:45:44 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
06/23/2022 08:45:46 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=584
06/23/2022 08:45:49 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
06/23/2022 08:45:50 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.843987191650854 on epoch=587
06/23/2022 08:45:50 - INFO - __main__ - Saving model with best Classification-F1: 0.8278231625005819 -> 0.843987191650854 on epoch=587, global_step=2350
06/23/2022 08:45:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
06/23/2022 08:45:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/23/2022 08:45:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/23/2022 08:46:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/23/2022 08:46:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/23/2022 08:46:03 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7963980039550118 on epoch=599
06/23/2022 08:46:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=602
06/23/2022 08:46:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/23/2022 08:46:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=607
06/23/2022 08:46:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/23/2022 08:46:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
06/23/2022 08:46:17 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7963980039550118 on epoch=612
06/23/2022 08:46:19 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/23/2022 08:46:22 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/23/2022 08:46:24 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/23/2022 08:46:27 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
06/23/2022 08:46:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
06/23/2022 08:46:30 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7942911255411256 on epoch=624
06/23/2022 08:46:33 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/23/2022 08:46:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/23/2022 08:46:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/23/2022 08:46:40 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
06/23/2022 08:46:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/23/2022 08:46:44 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.8286585015237767 on epoch=637
06/23/2022 08:46:46 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/23/2022 08:46:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/23/2022 08:46:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/23/2022 08:46:54 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=647
06/23/2022 08:46:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/23/2022 08:46:57 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.8115301724137931 on epoch=649
06/23/2022 08:47:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/23/2022 08:47:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/23/2022 08:47:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/23/2022 08:47:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/23/2022 08:47:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/23/2022 08:47:10 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.8100273569023568 on epoch=662
06/23/2022 08:47:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/23/2022 08:47:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/23/2022 08:47:18 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/23/2022 08:47:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/23/2022 08:47:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/23/2022 08:47:24 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.8268262987012986 on epoch=674
06/23/2022 08:47:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/23/2022 08:47:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/23/2022 08:47:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
06/23/2022 08:47:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
06/23/2022 08:47:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/23/2022 08:47:37 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7917637917637917 on epoch=687
06/23/2022 08:47:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/23/2022 08:47:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
06/23/2022 08:47:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=694
06/23/2022 08:47:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
06/23/2022 08:47:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/23/2022 08:47:51 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7768308080808081 on epoch=699
06/23/2022 08:47:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=702
06/23/2022 08:47:56 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
06/23/2022 08:47:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/23/2022 08:48:01 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/23/2022 08:48:03 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/23/2022 08:48:04 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7906348553407376 on epoch=712
06/23/2022 08:48:07 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/23/2022 08:48:09 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/23/2022 08:48:12 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
06/23/2022 08:48:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
06/23/2022 08:48:17 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
06/23/2022 08:48:18 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7937662520205214 on epoch=724
06/23/2022 08:48:20 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/23/2022 08:48:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/23/2022 08:48:25 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/23/2022 08:48:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/23/2022 08:48:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
06/23/2022 08:48:31 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.8094276094276094 on epoch=737
06/23/2022 08:48:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/23/2022 08:48:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/23/2022 08:48:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/23/2022 08:48:41 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
06/23/2022 08:48:44 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/23/2022 08:48:45 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.8094276094276094 on epoch=749
06/23/2022 08:48:45 - INFO - __main__ - save last model!
06/23/2022 08:48:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 08:48:45 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 08:48:45 - INFO - __main__ - Printing 3 examples
06/23/2022 08:48:45 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 08:48:45 - INFO - __main__ - ['others']
06/23/2022 08:48:45 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 08:48:45 - INFO - __main__ - ['others']
06/23/2022 08:48:45 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 08:48:45 - INFO - __main__ - ['others']
06/23/2022 08:48:45 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:48:45 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:48:45 - INFO - __main__ - Printing 3 examples
06/23/2022 08:48:45 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/23/2022 08:48:45 - INFO - __main__ - ['others']
06/23/2022 08:48:45 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/23/2022 08:48:45 - INFO - __main__ - ['others']
06/23/2022 08:48:45 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/23/2022 08:48:45 - INFO - __main__ - ['others']
06/23/2022 08:48:45 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:48:45 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:48:45 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 08:48:45 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:48:45 - INFO - __main__ - Printing 3 examples
06/23/2022 08:48:45 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/23/2022 08:48:45 - INFO - __main__ - ['others']
06/23/2022 08:48:45 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/23/2022 08:48:45 - INFO - __main__ - ['others']
06/23/2022 08:48:45 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/23/2022 08:48:45 - INFO - __main__ - ['others']
06/23/2022 08:48:45 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:48:45 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:48:45 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 08:48:47 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:48:52 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 08:49:01 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 08:49:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 08:49:01 - INFO - __main__ - Starting training!
06/23/2022 08:50:06 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_13_0.5_8_predictions.txt
06/23/2022 08:50:06 - INFO - __main__ - Classification-F1 on test data: 0.2821
06/23/2022 08:50:07 - INFO - __main__ - prefix=emo_16_13, lr=0.5, bsz=8, dev_performance=0.843987191650854, test_performance=0.28212873232485913
06/23/2022 08:50:07 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.4, bsz=8 ...
06/23/2022 08:50:07 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:50:07 - INFO - __main__ - Printing 3 examples
06/23/2022 08:50:07 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/23/2022 08:50:07 - INFO - __main__ - ['others']
06/23/2022 08:50:07 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/23/2022 08:50:07 - INFO - __main__ - ['others']
06/23/2022 08:50:08 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/23/2022 08:50:08 - INFO - __main__ - ['others']
06/23/2022 08:50:08 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:50:08 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:50:08 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 08:50:08 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 08:50:08 - INFO - __main__ - Printing 3 examples
06/23/2022 08:50:08 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/23/2022 08:50:08 - INFO - __main__ - ['others']
06/23/2022 08:50:08 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/23/2022 08:50:08 - INFO - __main__ - ['others']
06/23/2022 08:50:08 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/23/2022 08:50:08 - INFO - __main__ - ['others']
06/23/2022 08:50:08 - INFO - __main__ - Tokenizing Input ...
06/23/2022 08:50:08 - INFO - __main__ - Tokenizing Output ...
06/23/2022 08:50:08 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 08:50:23 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 08:50:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 08:50:24 - INFO - __main__ - Starting training!
06/23/2022 08:50:27 - INFO - __main__ - Step 10 Global step 10 Train loss 4.42 on epoch=2
06/23/2022 08:50:29 - INFO - __main__ - Step 20 Global step 20 Train loss 3.40 on epoch=4
06/23/2022 08:50:32 - INFO - __main__ - Step 30 Global step 30 Train loss 2.68 on epoch=7
06/23/2022 08:50:34 - INFO - __main__ - Step 40 Global step 40 Train loss 2.02 on epoch=9
06/23/2022 08:50:37 - INFO - __main__ - Step 50 Global step 50 Train loss 1.86 on epoch=12
06/23/2022 08:50:38 - INFO - __main__ - Global step 50 Train loss 2.88 Classification-F1 0.12268248992386925 on epoch=12
06/23/2022 08:50:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.12268248992386925 on epoch=12, global_step=50
06/23/2022 08:50:40 - INFO - __main__ - Step 60 Global step 60 Train loss 1.52 on epoch=14
06/23/2022 08:50:43 - INFO - __main__ - Step 70 Global step 70 Train loss 1.18 on epoch=17
06/23/2022 08:50:45 - INFO - __main__ - Step 80 Global step 80 Train loss 1.00 on epoch=19
06/23/2022 08:50:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.82 on epoch=22
06/23/2022 08:50:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.82 on epoch=24
06/23/2022 08:50:51 - INFO - __main__ - Global step 100 Train loss 1.07 Classification-F1 0.4883458646616542 on epoch=24
06/23/2022 08:50:51 - INFO - __main__ - Saving model with best Classification-F1: 0.12268248992386925 -> 0.4883458646616542 on epoch=24, global_step=100
06/23/2022 08:50:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.79 on epoch=27
06/23/2022 08:50:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.69 on epoch=29
06/23/2022 08:50:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.77 on epoch=32
06/23/2022 08:51:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.69 on epoch=34
06/23/2022 08:51:03 - INFO - __main__ - Step 150 Global step 150 Train loss 0.69 on epoch=37
06/23/2022 08:51:04 - INFO - __main__ - Global step 150 Train loss 0.73 Classification-F1 0.6468817204301076 on epoch=37
06/23/2022 08:51:04 - INFO - __main__ - Saving model with best Classification-F1: 0.4883458646616542 -> 0.6468817204301076 on epoch=37, global_step=150
06/23/2022 08:51:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.72 on epoch=39
06/23/2022 08:51:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.65 on epoch=42
06/23/2022 08:51:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.67 on epoch=44
06/23/2022 08:51:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.65 on epoch=47
06/23/2022 08:51:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.53 on epoch=49
06/23/2022 08:51:17 - INFO - __main__ - Global step 200 Train loss 0.64 Classification-F1 0.5523989898989898 on epoch=49
06/23/2022 08:51:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.55 on epoch=52
06/23/2022 08:51:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=54
06/23/2022 08:51:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=57
06/23/2022 08:51:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=59
06/23/2022 08:51:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.52 on epoch=62
06/23/2022 08:51:31 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.680950040950041 on epoch=62
06/23/2022 08:51:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6468817204301076 -> 0.680950040950041 on epoch=62, global_step=250
06/23/2022 08:51:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=64
06/23/2022 08:51:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=67
06/23/2022 08:51:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=69
06/23/2022 08:51:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=72
06/23/2022 08:51:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=74
06/23/2022 08:51:44 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.716210445268158 on epoch=74
06/23/2022 08:51:44 - INFO - __main__ - Saving model with best Classification-F1: 0.680950040950041 -> 0.716210445268158 on epoch=74, global_step=300
06/23/2022 08:51:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=77
06/23/2022 08:51:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=79
06/23/2022 08:51:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=82
06/23/2022 08:51:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=84
06/23/2022 08:51:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=87
06/23/2022 08:51:57 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.7628878878878879 on epoch=87
06/23/2022 08:51:57 - INFO - __main__ - Saving model with best Classification-F1: 0.716210445268158 -> 0.7628878878878879 on epoch=87, global_step=350
06/23/2022 08:51:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=89
06/23/2022 08:52:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=92
06/23/2022 08:52:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=94
06/23/2022 08:52:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=97
06/23/2022 08:52:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=99
06/23/2022 08:52:10 - INFO - __main__ - Global step 400 Train loss 0.31 Classification-F1 0.7454453441295547 on epoch=99
06/23/2022 08:52:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.29 on epoch=102
06/23/2022 08:52:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.32 on epoch=104
06/23/2022 08:52:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=107
06/23/2022 08:52:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.29 on epoch=109
06/23/2022 08:52:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.32 on epoch=112
06/23/2022 08:52:23 - INFO - __main__ - Global step 450 Train loss 0.29 Classification-F1 0.7961822660098522 on epoch=112
06/23/2022 08:52:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7628878878878879 -> 0.7961822660098522 on epoch=112, global_step=450
06/23/2022 08:52:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=114
06/23/2022 08:52:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=117
06/23/2022 08:52:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=119
06/23/2022 08:52:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.19 on epoch=122
06/23/2022 08:52:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=124
06/23/2022 08:52:37 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.7961822660098522 on epoch=124
06/23/2022 08:52:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=127
06/23/2022 08:52:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=129
06/23/2022 08:52:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=132
06/23/2022 08:52:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=134
06/23/2022 08:52:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=137
06/23/2022 08:52:50 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.7797619047619048 on epoch=137
06/23/2022 08:52:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=139
06/23/2022 08:52:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=142
06/23/2022 08:52:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=144
06/23/2022 08:52:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.17 on epoch=147
06/23/2022 08:53:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=149
06/23/2022 08:53:03 - INFO - __main__ - Global step 600 Train loss 0.19 Classification-F1 0.7961822660098522 on epoch=149
06/23/2022 08:53:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=152
06/23/2022 08:53:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=154
06/23/2022 08:53:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=157
06/23/2022 08:53:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=159
06/23/2022 08:53:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=162
06/23/2022 08:53:16 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.795279988828376 on epoch=162
06/23/2022 08:53:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=164
06/23/2022 08:53:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=167
06/23/2022 08:53:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=169
06/23/2022 08:53:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=172
06/23/2022 08:53:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=174
06/23/2022 08:53:29 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.8104380039863911 on epoch=174
06/23/2022 08:53:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7961822660098522 -> 0.8104380039863911 on epoch=174, global_step=700
06/23/2022 08:53:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=177
06/23/2022 08:53:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=179
06/23/2022 08:53:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=182
06/23/2022 08:53:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=184
06/23/2022 08:53:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=187
06/23/2022 08:53:42 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.795279988828376 on epoch=187
06/23/2022 08:53:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=189
06/23/2022 08:53:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.09 on epoch=192
06/23/2022 08:53:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=194
06/23/2022 08:53:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=197
06/23/2022 08:53:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=199
06/23/2022 08:53:56 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.7954391433992952 on epoch=199
06/23/2022 08:53:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=202
06/23/2022 08:54:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=204
06/23/2022 08:54:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=207
06/23/2022 08:54:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=209
06/23/2022 08:54:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=212
06/23/2022 08:54:09 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.8117003500763235 on epoch=212
06/23/2022 08:54:09 - INFO - __main__ - Saving model with best Classification-F1: 0.8104380039863911 -> 0.8117003500763235 on epoch=212, global_step=850
06/23/2022 08:54:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=214
06/23/2022 08:54:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=217
06/23/2022 08:54:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=219
06/23/2022 08:54:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=222
06/23/2022 08:54:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=224
06/23/2022 08:54:22 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.795279988828376 on epoch=224
06/23/2022 08:54:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=227
06/23/2022 08:54:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=229
06/23/2022 08:54:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=232
06/23/2022 08:54:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=234
06/23/2022 08:54:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=237
06/23/2022 08:54:36 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.8117003500763235 on epoch=237
06/23/2022 08:54:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=239
06/23/2022 08:54:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
06/23/2022 08:54:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=244
06/23/2022 08:54:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=247
06/23/2022 08:54:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
06/23/2022 08:54:49 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.7954391433992952 on epoch=249
06/23/2022 08:54:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=252
06/23/2022 08:54:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=254
06/23/2022 08:54:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=257
06/23/2022 08:54:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=259
06/23/2022 08:55:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=262
06/23/2022 08:55:02 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.8277729860272556 on epoch=262
06/23/2022 08:55:02 - INFO - __main__ - Saving model with best Classification-F1: 0.8117003500763235 -> 0.8277729860272556 on epoch=262, global_step=1050
06/23/2022 08:55:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
06/23/2022 08:55:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=267
06/23/2022 08:55:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=269
06/23/2022 08:55:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
06/23/2022 08:55:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=274
06/23/2022 08:55:15 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.7954391433992952 on epoch=274
06/23/2022 08:55:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
06/23/2022 08:55:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=279
06/23/2022 08:55:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
06/23/2022 08:55:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
06/23/2022 08:55:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
06/23/2022 08:55:29 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.825944170771757 on epoch=287
06/23/2022 08:55:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=289
06/23/2022 08:55:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=292
06/23/2022 08:55:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
06/23/2022 08:55:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
06/23/2022 08:55:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
06/23/2022 08:55:42 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.7954391433992952 on epoch=299
06/23/2022 08:55:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.09 on epoch=302
06/23/2022 08:55:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
06/23/2022 08:55:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
06/23/2022 08:55:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=309
06/23/2022 08:55:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
06/23/2022 08:55:55 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.8107142857142856 on epoch=312
06/23/2022 08:55:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
06/23/2022 08:56:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
06/23/2022 08:56:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
06/23/2022 08:56:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
06/23/2022 08:56:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
06/23/2022 08:56:08 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.8286137234413096 on epoch=324
06/23/2022 08:56:08 - INFO - __main__ - Saving model with best Classification-F1: 0.8277729860272556 -> 0.8286137234413096 on epoch=324, global_step=1300
06/23/2022 08:56:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
06/23/2022 08:56:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
06/23/2022 08:56:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
06/23/2022 08:56:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=334
06/23/2022 08:56:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
06/23/2022 08:56:22 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.8274847870182555 on epoch=337
06/23/2022 08:56:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
06/23/2022 08:56:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
06/23/2022 08:56:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
06/23/2022 08:56:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
06/23/2022 08:56:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
06/23/2022 08:56:35 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.7789877061935885 on epoch=349
06/23/2022 08:56:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
06/23/2022 08:56:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
06/23/2022 08:56:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
06/23/2022 08:56:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
06/23/2022 08:56:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
06/23/2022 08:56:48 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.8117003500763235 on epoch=362
06/23/2022 08:56:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
06/23/2022 08:56:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
06/23/2022 08:56:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=369
06/23/2022 08:56:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
06/23/2022 08:57:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
06/23/2022 08:57:01 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.8114612511671334 on epoch=374
06/23/2022 08:57:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
06/23/2022 08:57:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=379
06/23/2022 08:57:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/23/2022 08:57:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
06/23/2022 08:57:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
06/23/2022 08:57:14 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.7929492585564692 on epoch=387
06/23/2022 08:57:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
06/23/2022 08:57:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
06/23/2022 08:57:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
06/23/2022 08:57:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
06/23/2022 08:57:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/23/2022 08:57:27 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.8270646189510287 on epoch=399
06/23/2022 08:57:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
06/23/2022 08:57:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
06/23/2022 08:57:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
06/23/2022 08:57:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/23/2022 08:57:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
06/23/2022 08:57:41 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7816194581280789 on epoch=412
06/23/2022 08:57:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
06/23/2022 08:57:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
06/23/2022 08:57:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/23/2022 08:57:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
06/23/2022 08:57:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
06/23/2022 08:57:54 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.812121212121212 on epoch=424
06/23/2022 08:57:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
06/23/2022 08:57:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
06/23/2022 08:58:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
06/23/2022 08:58:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
06/23/2022 08:58:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
06/23/2022 08:58:07 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.8106442577030812 on epoch=437
06/23/2022 08:58:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
06/23/2022 08:58:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
06/23/2022 08:58:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=444
06/23/2022 08:58:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=447
06/23/2022 08:58:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
06/23/2022 08:58:20 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.7935185185185184 on epoch=449
06/23/2022 08:58:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
06/23/2022 08:58:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
06/23/2022 08:58:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/23/2022 08:58:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=459
06/23/2022 08:58:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
06/23/2022 08:58:33 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.7955375253549696 on epoch=462
06/23/2022 08:58:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/23/2022 08:58:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/23/2022 08:58:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/23/2022 08:58:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
06/23/2022 08:58:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
06/23/2022 08:58:46 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.8259259259259258 on epoch=474
06/23/2022 08:58:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
06/23/2022 08:58:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
06/23/2022 08:58:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
06/23/2022 08:58:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
06/23/2022 08:58:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
06/23/2022 08:58:59 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.8106442577030812 on epoch=487
06/23/2022 08:59:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
06/23/2022 08:59:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/23/2022 08:59:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
06/23/2022 08:59:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/23/2022 08:59:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
06/23/2022 08:59:13 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.8106442577030812 on epoch=499
06/23/2022 08:59:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/23/2022 08:59:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
06/23/2022 08:59:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
06/23/2022 08:59:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
06/23/2022 08:59:25 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/23/2022 08:59:26 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.8106442577030812 on epoch=512
06/23/2022 08:59:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/23/2022 08:59:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
06/23/2022 08:59:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
06/23/2022 08:59:36 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
06/23/2022 08:59:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/23/2022 08:59:39 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.7906606606606605 on epoch=524
06/23/2022 08:59:42 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
06/23/2022 08:59:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
06/23/2022 08:59:46 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
06/23/2022 08:59:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
06/23/2022 08:59:51 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/23/2022 08:59:52 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.8270646189510287 on epoch=537
06/23/2022 08:59:55 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
06/23/2022 08:59:57 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/23/2022 08:59:59 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/23/2022 09:00:02 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/23/2022 09:00:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
06/23/2022 09:00:05 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.8270646189510287 on epoch=549
06/23/2022 09:00:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
06/23/2022 09:00:10 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/23/2022 09:00:13 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
06/23/2022 09:00:15 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
06/23/2022 09:00:18 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
06/23/2022 09:00:18 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7800324675324675 on epoch=562
06/23/2022 09:00:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
06/23/2022 09:00:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
06/23/2022 09:00:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
06/23/2022 09:00:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=572
06/23/2022 09:00:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/23/2022 09:00:32 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.8107142857142856 on epoch=574
06/23/2022 09:00:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/23/2022 09:00:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/23/2022 09:00:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/23/2022 09:00:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
06/23/2022 09:00:44 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
06/23/2022 09:00:45 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.796279479533749 on epoch=587
06/23/2022 09:00:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=589
06/23/2022 09:00:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/23/2022 09:00:52 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/23/2022 09:00:55 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
06/23/2022 09:00:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
06/23/2022 09:00:58 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7967986314760509 on epoch=599
06/23/2022 09:01:00 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
06/23/2022 09:01:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/23/2022 09:01:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/23/2022 09:01:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
06/23/2022 09:01:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/23/2022 09:01:11 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.8107142857142856 on epoch=612
06/23/2022 09:01:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/23/2022 09:01:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/23/2022 09:01:19 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
06/23/2022 09:01:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/23/2022 09:01:23 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/23/2022 09:01:24 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.8085239085239084 on epoch=624
06/23/2022 09:01:27 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/23/2022 09:01:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/23/2022 09:01:32 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/23/2022 09:01:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
06/23/2022 09:01:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/23/2022 09:01:38 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.796279479533749 on epoch=637
06/23/2022 09:01:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/23/2022 09:01:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/23/2022 09:01:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/23/2022 09:01:47 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/23/2022 09:01:50 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/23/2022 09:01:51 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7789877061935885 on epoch=649
06/23/2022 09:01:53 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/23/2022 09:01:56 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/23/2022 09:01:58 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=657
06/23/2022 09:02:01 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/23/2022 09:02:03 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
06/23/2022 09:02:04 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7640621905327787 on epoch=662
06/23/2022 09:02:06 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
06/23/2022 09:02:09 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/23/2022 09:02:11 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/23/2022 09:02:14 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/23/2022 09:02:16 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/23/2022 09:02:17 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.7937662520205214 on epoch=674
06/23/2022 09:02:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/23/2022 09:02:22 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=679
06/23/2022 09:02:25 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
06/23/2022 09:02:27 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/23/2022 09:02:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
06/23/2022 09:02:31 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.8131944444444443 on epoch=687
06/23/2022 09:02:33 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
06/23/2022 09:02:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/23/2022 09:02:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/23/2022 09:02:40 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/23/2022 09:02:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
06/23/2022 09:02:44 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.8114612511671334 on epoch=699
06/23/2022 09:02:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
06/23/2022 09:02:48 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/23/2022 09:02:51 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=707
06/23/2022 09:02:53 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/23/2022 09:02:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/23/2022 09:02:57 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.775355468903856 on epoch=712
06/23/2022 09:02:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/23/2022 09:03:02 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/23/2022 09:03:04 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/23/2022 09:03:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/23/2022 09:03:09 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/23/2022 09:03:10 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7786854722338593 on epoch=724
06/23/2022 09:03:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/23/2022 09:03:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/23/2022 09:03:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/23/2022 09:03:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
06/23/2022 09:03:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
06/23/2022 09:03:23 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.812121212121212 on epoch=737
06/23/2022 09:03:25 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/23/2022 09:03:28 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
06/23/2022 09:03:30 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/23/2022 09:03:33 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/23/2022 09:03:35 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/23/2022 09:03:36 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.812121212121212 on epoch=749
06/23/2022 09:03:36 - INFO - __main__ - save last model!
06/23/2022 09:03:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 09:03:36 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 09:03:36 - INFO - __main__ - Printing 3 examples
06/23/2022 09:03:36 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 09:03:36 - INFO - __main__ - ['others']
06/23/2022 09:03:36 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 09:03:36 - INFO - __main__ - ['others']
06/23/2022 09:03:36 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 09:03:36 - INFO - __main__ - ['others']
06/23/2022 09:03:36 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:03:37 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:03:37 - INFO - __main__ - Printing 3 examples
06/23/2022 09:03:37 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/23/2022 09:03:37 - INFO - __main__ - ['others']
06/23/2022 09:03:37 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/23/2022 09:03:37 - INFO - __main__ - ['others']
06/23/2022 09:03:37 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/23/2022 09:03:37 - INFO - __main__ - ['others']
06/23/2022 09:03:37 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:03:37 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:03:37 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 09:03:37 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:03:37 - INFO - __main__ - Printing 3 examples
06/23/2022 09:03:37 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/23/2022 09:03:37 - INFO - __main__ - ['others']
06/23/2022 09:03:37 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/23/2022 09:03:37 - INFO - __main__ - ['others']
06/23/2022 09:03:37 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/23/2022 09:03:37 - INFO - __main__ - ['others']
06/23/2022 09:03:37 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:03:37 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:03:37 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 09:03:39 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:03:44 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 09:03:52 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 09:03:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 09:03:53 - INFO - __main__ - Starting training!
06/23/2022 09:04:58 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_13_0.4_8_predictions.txt
06/23/2022 09:04:58 - INFO - __main__ - Classification-F1 on test data: 0.2962
06/23/2022 09:04:59 - INFO - __main__ - prefix=emo_16_13, lr=0.4, bsz=8, dev_performance=0.8286137234413096, test_performance=0.29618875653906285
06/23/2022 09:04:59 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.3, bsz=8 ...
06/23/2022 09:05:00 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:05:00 - INFO - __main__ - Printing 3 examples
06/23/2022 09:05:00 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/23/2022 09:05:00 - INFO - __main__ - ['others']
06/23/2022 09:05:00 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/23/2022 09:05:00 - INFO - __main__ - ['others']
06/23/2022 09:05:00 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/23/2022 09:05:00 - INFO - __main__ - ['others']
06/23/2022 09:05:00 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:05:00 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:05:00 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 09:05:00 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:05:00 - INFO - __main__ - Printing 3 examples
06/23/2022 09:05:00 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/23/2022 09:05:00 - INFO - __main__ - ['others']
06/23/2022 09:05:00 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/23/2022 09:05:00 - INFO - __main__ - ['others']
06/23/2022 09:05:00 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/23/2022 09:05:00 - INFO - __main__ - ['others']
06/23/2022 09:05:00 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:05:00 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:05:00 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 09:05:15 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 09:05:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 09:05:16 - INFO - __main__ - Starting training!
06/23/2022 09:05:19 - INFO - __main__ - Step 10 Global step 10 Train loss 4.78 on epoch=2
06/23/2022 09:05:21 - INFO - __main__ - Step 20 Global step 20 Train loss 3.51 on epoch=4
06/23/2022 09:05:24 - INFO - __main__ - Step 30 Global step 30 Train loss 2.89 on epoch=7
06/23/2022 09:05:26 - INFO - __main__ - Step 40 Global step 40 Train loss 2.54 on epoch=9
06/23/2022 09:05:29 - INFO - __main__ - Step 50 Global step 50 Train loss 2.35 on epoch=12
06/23/2022 09:05:30 - INFO - __main__ - Global step 50 Train loss 3.21 Classification-F1 0.0202020202020202 on epoch=12
06/23/2022 09:05:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0202020202020202 on epoch=12, global_step=50
06/23/2022 09:05:32 - INFO - __main__ - Step 60 Global step 60 Train loss 1.84 on epoch=14
06/23/2022 09:05:35 - INFO - __main__ - Step 70 Global step 70 Train loss 1.59 on epoch=17
06/23/2022 09:05:37 - INFO - __main__ - Step 80 Global step 80 Train loss 1.16 on epoch=19
06/23/2022 09:05:40 - INFO - __main__ - Step 90 Global step 90 Train loss 1.18 on epoch=22
06/23/2022 09:05:42 - INFO - __main__ - Step 100 Global step 100 Train loss 1.04 on epoch=24
06/23/2022 09:05:43 - INFO - __main__ - Global step 100 Train loss 1.36 Classification-F1 0.37708234550339814 on epoch=24
06/23/2022 09:05:43 - INFO - __main__ - Saving model with best Classification-F1: 0.0202020202020202 -> 0.37708234550339814 on epoch=24, global_step=100
06/23/2022 09:05:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.98 on epoch=27
06/23/2022 09:05:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.90 on epoch=29
06/23/2022 09:05:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.94 on epoch=32
06/23/2022 09:05:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.80 on epoch=34
06/23/2022 09:05:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.79 on epoch=37
06/23/2022 09:05:56 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.49070512820512824 on epoch=37
06/23/2022 09:05:56 - INFO - __main__ - Saving model with best Classification-F1: 0.37708234550339814 -> 0.49070512820512824 on epoch=37, global_step=150
06/23/2022 09:05:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.65 on epoch=39
06/23/2022 09:06:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.72 on epoch=42
06/23/2022 09:06:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.63 on epoch=44
06/23/2022 09:06:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.80 on epoch=47
06/23/2022 09:06:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.71 on epoch=49
06/23/2022 09:06:09 - INFO - __main__ - Global step 200 Train loss 0.70 Classification-F1 0.4741715107568766 on epoch=49
06/23/2022 09:06:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.61 on epoch=52
06/23/2022 09:06:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.64 on epoch=54
06/23/2022 09:06:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.59 on epoch=57
06/23/2022 09:06:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=59
06/23/2022 09:06:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.58 on epoch=62
06/23/2022 09:06:22 - INFO - __main__ - Global step 250 Train loss 0.58 Classification-F1 0.6988393923877795 on epoch=62
06/23/2022 09:06:22 - INFO - __main__ - Saving model with best Classification-F1: 0.49070512820512824 -> 0.6988393923877795 on epoch=62, global_step=250
06/23/2022 09:06:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.63 on epoch=64
06/23/2022 09:06:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.56 on epoch=67
06/23/2022 09:06:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=69
06/23/2022 09:06:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=72
06/23/2022 09:06:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.59 on epoch=74
06/23/2022 09:06:36 - INFO - __main__ - Global step 300 Train loss 0.54 Classification-F1 0.7110653574068209 on epoch=74
06/23/2022 09:06:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6988393923877795 -> 0.7110653574068209 on epoch=74, global_step=300
06/23/2022 09:06:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.60 on epoch=77
06/23/2022 09:06:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=79
06/23/2022 09:06:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=82
06/23/2022 09:06:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=84
06/23/2022 09:06:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=87
06/23/2022 09:06:49 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.7884652981427175 on epoch=87
06/23/2022 09:06:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7110653574068209 -> 0.7884652981427175 on epoch=87, global_step=350
06/23/2022 09:06:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=89
06/23/2022 09:06:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=92
06/23/2022 09:06:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=94
06/23/2022 09:06:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=97
06/23/2022 09:07:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.53 on epoch=99
06/23/2022 09:07:02 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.7531782172041669 on epoch=99
06/23/2022 09:07:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=102
06/23/2022 09:07:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=104
06/23/2022 09:07:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=107
06/23/2022 09:07:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.32 on epoch=109
06/23/2022 09:07:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=112
06/23/2022 09:07:15 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.7763276970707312 on epoch=112
06/23/2022 09:07:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=114
06/23/2022 09:07:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=117
06/23/2022 09:07:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=119
06/23/2022 09:07:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.33 on epoch=122
06/23/2022 09:07:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=124
06/23/2022 09:07:28 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.7581900452488688 on epoch=124
06/23/2022 09:07:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=127
06/23/2022 09:07:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.30 on epoch=129
06/23/2022 09:07:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=132
06/23/2022 09:07:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=134
06/23/2022 09:07:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=137
06/23/2022 09:07:42 - INFO - __main__ - Global step 550 Train loss 0.33 Classification-F1 0.7273076923076923 on epoch=137
06/23/2022 09:07:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=139
06/23/2022 09:07:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=142
06/23/2022 09:07:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=144
06/23/2022 09:07:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=147
06/23/2022 09:07:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.28 on epoch=149
06/23/2022 09:07:55 - INFO - __main__ - Global step 600 Train loss 0.26 Classification-F1 0.7428257763741636 on epoch=149
06/23/2022 09:07:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=152
06/23/2022 09:08:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=154
06/23/2022 09:08:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=157
06/23/2022 09:08:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=159
06/23/2022 09:08:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=162
06/23/2022 09:08:08 - INFO - __main__ - Global step 650 Train loss 0.24 Classification-F1 0.7797619047619048 on epoch=162
06/23/2022 09:08:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=164
06/23/2022 09:08:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=167
06/23/2022 09:08:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=169
06/23/2022 09:08:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=172
06/23/2022 09:08:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=174
06/23/2022 09:08:21 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.8431372549019608 on epoch=174
06/23/2022 09:08:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7884652981427175 -> 0.8431372549019608 on epoch=174, global_step=700
06/23/2022 09:08:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=177
06/23/2022 09:08:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=179
06/23/2022 09:08:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.18 on epoch=182
06/23/2022 09:08:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=184
06/23/2022 09:08:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=187
06/23/2022 09:08:34 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.8122549019607843 on epoch=187
06/23/2022 09:08:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.17 on epoch=189
06/23/2022 09:08:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=192
06/23/2022 09:08:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=194
06/23/2022 09:08:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=197
06/23/2022 09:08:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.14 on epoch=199
06/23/2022 09:08:47 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.8122549019607843 on epoch=199
06/23/2022 09:08:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.13 on epoch=202
06/23/2022 09:08:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=204
06/23/2022 09:08:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=207
06/23/2022 09:08:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=209
06/23/2022 09:08:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=212
06/23/2022 09:09:00 - INFO - __main__ - Global step 850 Train loss 0.17 Classification-F1 0.8277729860272556 on epoch=212
06/23/2022 09:09:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.16 on epoch=214
06/23/2022 09:09:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.19 on epoch=217
06/23/2022 09:09:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=219
06/23/2022 09:09:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=222
06/23/2022 09:09:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=224
06/23/2022 09:09:13 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.795279988828376 on epoch=224
06/23/2022 09:09:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=227
06/23/2022 09:09:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=229
06/23/2022 09:09:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=232
06/23/2022 09:09:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.14 on epoch=234
06/23/2022 09:09:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.15 on epoch=237
06/23/2022 09:09:27 - INFO - __main__ - Global step 950 Train loss 0.12 Classification-F1 0.8277729860272556 on epoch=237
06/23/2022 09:09:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=239
06/23/2022 09:09:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=242
06/23/2022 09:09:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=244
06/23/2022 09:09:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=247
06/23/2022 09:09:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=249
06/23/2022 09:09:40 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.8435972629521017 on epoch=249
06/23/2022 09:09:40 - INFO - __main__ - Saving model with best Classification-F1: 0.8431372549019608 -> 0.8435972629521017 on epoch=249, global_step=1000
06/23/2022 09:09:42 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=252
06/23/2022 09:09:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=254
06/23/2022 09:09:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=257
06/23/2022 09:09:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=259
06/23/2022 09:09:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=262
06/23/2022 09:09:53 - INFO - __main__ - Global step 1050 Train loss 0.11 Classification-F1 0.8286642516244034 on epoch=262
06/23/2022 09:09:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=264
06/23/2022 09:09:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=267
06/23/2022 09:10:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
06/23/2022 09:10:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=272
06/23/2022 09:10:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
06/23/2022 09:10:06 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.7605044834646353 on epoch=274
06/23/2022 09:10:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=277
06/23/2022 09:10:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=279
06/23/2022 09:10:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=282
06/23/2022 09:10:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=284
06/23/2022 09:10:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
06/23/2022 09:10:19 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.778243376203528 on epoch=287
06/23/2022 09:10:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
06/23/2022 09:10:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=292
06/23/2022 09:10:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=294
06/23/2022 09:10:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
06/23/2022 09:10:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=299
06/23/2022 09:10:33 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.8286642516244034 on epoch=299
06/23/2022 09:10:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=302
06/23/2022 09:10:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=304
06/23/2022 09:10:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
06/23/2022 09:10:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=309
06/23/2022 09:10:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
06/23/2022 09:10:46 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.7954391433992952 on epoch=312
06/23/2022 09:10:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=314
06/23/2022 09:10:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=317
06/23/2022 09:10:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
06/23/2022 09:10:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
06/23/2022 09:10:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
06/23/2022 09:10:59 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.8277729860272556 on epoch=324
06/23/2022 09:11:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=327
06/23/2022 09:11:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=329
06/23/2022 09:11:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
06/23/2022 09:11:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=334
06/23/2022 09:11:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=337
06/23/2022 09:11:12 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.8106442577030812 on epoch=337
06/23/2022 09:11:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=339
06/23/2022 09:11:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
06/23/2022 09:11:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=344
06/23/2022 09:11:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
06/23/2022 09:11:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
06/23/2022 09:11:25 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.8106442577030812 on epoch=349
06/23/2022 09:11:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=352
06/23/2022 09:11:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
06/23/2022 09:11:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
06/23/2022 09:11:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
06/23/2022 09:11:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=362
06/23/2022 09:11:39 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.778243376203528 on epoch=362
06/23/2022 09:11:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
06/23/2022 09:11:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
06/23/2022 09:11:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
06/23/2022 09:11:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
06/23/2022 09:11:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
06/23/2022 09:11:52 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.8117003500763235 on epoch=374
06/23/2022 09:11:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
06/23/2022 09:11:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
06/23/2022 09:11:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
06/23/2022 09:12:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
06/23/2022 09:12:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=387
06/23/2022 09:12:05 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.8117003500763235 on epoch=387
06/23/2022 09:12:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
06/23/2022 09:12:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=392
06/23/2022 09:12:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
06/23/2022 09:12:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
06/23/2022 09:12:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/23/2022 09:12:18 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.8270646189510287 on epoch=399
06/23/2022 09:12:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/23/2022 09:12:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
06/23/2022 09:12:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=407
06/23/2022 09:12:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
06/23/2022 09:12:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
06/23/2022 09:12:31 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.8106442577030812 on epoch=412
06/23/2022 09:12:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
06/23/2022 09:12:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
06/23/2022 09:12:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
06/23/2022 09:12:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
06/23/2022 09:12:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
06/23/2022 09:12:44 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.8270646189510287 on epoch=424
06/23/2022 09:12:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
06/23/2022 09:12:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/23/2022 09:12:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/23/2022 09:12:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
06/23/2022 09:12:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=437
06/23/2022 09:12:57 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.8274847870182555 on epoch=437
06/23/2022 09:12:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
06/23/2022 09:13:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=442
06/23/2022 09:13:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=444
06/23/2022 09:13:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
06/23/2022 09:13:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
06/23/2022 09:13:10 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.8274847870182555 on epoch=449
06/23/2022 09:13:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=452
06/23/2022 09:13:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=454
06/23/2022 09:13:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
06/23/2022 09:13:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
06/23/2022 09:13:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
06/23/2022 09:13:23 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.8117003500763235 on epoch=462
06/23/2022 09:13:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=464
06/23/2022 09:13:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=467
06/23/2022 09:13:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/23/2022 09:13:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=472
06/23/2022 09:13:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
06/23/2022 09:13:36 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.8270646189510287 on epoch=474
06/23/2022 09:13:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
06/23/2022 09:13:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
06/23/2022 09:13:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/23/2022 09:13:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/23/2022 09:13:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
06/23/2022 09:13:49 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.8115844038764931 on epoch=487
06/23/2022 09:13:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
06/23/2022 09:13:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
06/23/2022 09:13:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=494
06/23/2022 09:13:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
06/23/2022 09:14:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
06/23/2022 09:14:02 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.8270646189510287 on epoch=499
06/23/2022 09:14:05 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
06/23/2022 09:14:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
06/23/2022 09:14:10 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
06/23/2022 09:14:12 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/23/2022 09:14:14 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
06/23/2022 09:14:15 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.8270646189510287 on epoch=512
06/23/2022 09:14:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/23/2022 09:14:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
06/23/2022 09:14:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
06/23/2022 09:14:25 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
06/23/2022 09:14:28 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
06/23/2022 09:14:28 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.8270646189510287 on epoch=524
06/23/2022 09:14:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
06/23/2022 09:14:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
06/23/2022 09:14:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
06/23/2022 09:14:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
06/23/2022 09:14:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/23/2022 09:14:41 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.8270646189510287 on epoch=537
06/23/2022 09:14:44 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
06/23/2022 09:14:46 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
06/23/2022 09:14:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
06/23/2022 09:14:51 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/23/2022 09:14:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/23/2022 09:14:55 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.8274847870182555 on epoch=549
06/23/2022 09:14:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
06/23/2022 09:14:59 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
06/23/2022 09:15:02 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/23/2022 09:15:04 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/23/2022 09:15:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
06/23/2022 09:15:08 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.8270646189510287 on epoch=562
06/23/2022 09:15:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
06/23/2022 09:15:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
06/23/2022 09:15:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/23/2022 09:15:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
06/23/2022 09:15:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/23/2022 09:15:21 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.8438466983517039 on epoch=574
06/23/2022 09:15:21 - INFO - __main__ - Saving model with best Classification-F1: 0.8435972629521017 -> 0.8438466983517039 on epoch=574, global_step=2300
06/23/2022 09:15:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/23/2022 09:15:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/23/2022 09:15:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
06/23/2022 09:15:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=584
06/23/2022 09:15:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/23/2022 09:15:34 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.8274847870182555 on epoch=587
06/23/2022 09:15:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/23/2022 09:15:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/23/2022 09:15:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/23/2022 09:15:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
06/23/2022 09:15:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/23/2022 09:15:47 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.8128904878904878 on epoch=599
06/23/2022 09:15:50 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/23/2022 09:15:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/23/2022 09:15:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/23/2022 09:15:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
06/23/2022 09:15:59 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/23/2022 09:16:00 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.8286137234413096 on epoch=612
06/23/2022 09:16:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/23/2022 09:16:05 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/23/2022 09:16:08 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/23/2022 09:16:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/23/2022 09:16:13 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=624
06/23/2022 09:16:13 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.8286137234413096 on epoch=624
06/23/2022 09:16:16 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
06/23/2022 09:16:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=629
06/23/2022 09:16:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/23/2022 09:16:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/23/2022 09:16:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
06/23/2022 09:16:27 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.8270646189510287 on epoch=637
06/23/2022 09:16:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/23/2022 09:16:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/23/2022 09:16:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/23/2022 09:16:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=647
06/23/2022 09:16:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/23/2022 09:16:40 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.8270646189510287 on epoch=649
06/23/2022 09:16:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/23/2022 09:16:45 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
06/23/2022 09:16:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=657
06/23/2022 09:16:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
06/23/2022 09:16:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=662
06/23/2022 09:16:53 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.7944170771756978 on epoch=662
06/23/2022 09:16:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=664
06/23/2022 09:16:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/23/2022 09:17:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=669
06/23/2022 09:17:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/23/2022 09:17:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/23/2022 09:17:06 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.8270646189510287 on epoch=674
06/23/2022 09:17:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/23/2022 09:17:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/23/2022 09:17:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
06/23/2022 09:17:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/23/2022 09:17:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
06/23/2022 09:17:19 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.8270646189510287 on epoch=687
06/23/2022 09:17:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/23/2022 09:17:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
06/23/2022 09:17:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
06/23/2022 09:17:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=697
06/23/2022 09:17:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=699
06/23/2022 09:17:32 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.8108374384236453 on epoch=699
06/23/2022 09:17:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/23/2022 09:17:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/23/2022 09:17:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=707
06/23/2022 09:17:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/23/2022 09:17:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/23/2022 09:17:45 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.8270646189510287 on epoch=712
06/23/2022 09:17:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/23/2022 09:17:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
06/23/2022 09:17:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/23/2022 09:17:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
06/23/2022 09:17:57 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
06/23/2022 09:17:58 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.8286137234413096 on epoch=724
06/23/2022 09:18:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/23/2022 09:18:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/23/2022 09:18:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
06/23/2022 09:18:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/23/2022 09:18:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/23/2022 09:18:11 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.8274847870182555 on epoch=737
06/23/2022 09:18:14 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/23/2022 09:18:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=742
06/23/2022 09:18:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=744
06/23/2022 09:18:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
06/23/2022 09:18:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=749
06/23/2022 09:18:25 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.8274847870182555 on epoch=749
06/23/2022 09:18:25 - INFO - __main__ - save last model!
06/23/2022 09:18:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 09:18:25 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 09:18:25 - INFO - __main__ - Printing 3 examples
06/23/2022 09:18:25 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 09:18:25 - INFO - __main__ - ['others']
06/23/2022 09:18:25 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 09:18:25 - INFO - __main__ - ['others']
06/23/2022 09:18:25 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 09:18:25 - INFO - __main__ - ['others']
06/23/2022 09:18:25 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:18:25 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:18:25 - INFO - __main__ - Printing 3 examples
06/23/2022 09:18:25 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/23/2022 09:18:25 - INFO - __main__ - ['others']
06/23/2022 09:18:25 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/23/2022 09:18:25 - INFO - __main__ - ['others']
06/23/2022 09:18:25 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/23/2022 09:18:25 - INFO - __main__ - ['others']
06/23/2022 09:18:25 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:18:25 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:18:25 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 09:18:25 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:18:25 - INFO - __main__ - Printing 3 examples
06/23/2022 09:18:25 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/23/2022 09:18:25 - INFO - __main__ - ['others']
06/23/2022 09:18:25 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/23/2022 09:18:25 - INFO - __main__ - ['others']
06/23/2022 09:18:25 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/23/2022 09:18:25 - INFO - __main__ - ['others']
06/23/2022 09:18:25 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:18:25 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:18:25 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 09:18:27 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:18:32 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 09:18:41 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 09:18:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 09:18:41 - INFO - __main__ - Starting training!
06/23/2022 09:19:46 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_13_0.3_8_predictions.txt
06/23/2022 09:19:46 - INFO - __main__ - Classification-F1 on test data: 0.2342
06/23/2022 09:19:46 - INFO - __main__ - prefix=emo_16_13, lr=0.3, bsz=8, dev_performance=0.8438466983517039, test_performance=0.23419263322787093
06/23/2022 09:19:46 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.2, bsz=8 ...
06/23/2022 09:19:47 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:19:47 - INFO - __main__ - Printing 3 examples
06/23/2022 09:19:47 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/23/2022 09:19:47 - INFO - __main__ - ['others']
06/23/2022 09:19:47 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/23/2022 09:19:47 - INFO - __main__ - ['others']
06/23/2022 09:19:47 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/23/2022 09:19:47 - INFO - __main__ - ['others']
06/23/2022 09:19:47 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:19:47 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:19:47 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 09:19:47 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:19:47 - INFO - __main__ - Printing 3 examples
06/23/2022 09:19:47 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/23/2022 09:19:47 - INFO - __main__ - ['others']
06/23/2022 09:19:47 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/23/2022 09:19:47 - INFO - __main__ - ['others']
06/23/2022 09:19:47 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/23/2022 09:19:47 - INFO - __main__ - ['others']
06/23/2022 09:19:47 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:19:47 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:19:47 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 09:20:03 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 09:20:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 09:20:04 - INFO - __main__ - Starting training!
06/23/2022 09:20:07 - INFO - __main__ - Step 10 Global step 10 Train loss 4.56 on epoch=2
06/23/2022 09:20:09 - INFO - __main__ - Step 20 Global step 20 Train loss 4.01 on epoch=4
06/23/2022 09:20:11 - INFO - __main__ - Step 30 Global step 30 Train loss 3.09 on epoch=7
06/23/2022 09:20:14 - INFO - __main__ - Step 40 Global step 40 Train loss 3.05 on epoch=9
06/23/2022 09:20:16 - INFO - __main__ - Step 50 Global step 50 Train loss 2.76 on epoch=12
06/23/2022 09:20:17 - INFO - __main__ - Global step 50 Train loss 3.49 Classification-F1 0.017543859649122806 on epoch=12
06/23/2022 09:20:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.017543859649122806 on epoch=12, global_step=50
06/23/2022 09:20:20 - INFO - __main__ - Step 60 Global step 60 Train loss 2.45 on epoch=14
06/23/2022 09:20:22 - INFO - __main__ - Step 70 Global step 70 Train loss 2.35 on epoch=17
06/23/2022 09:20:25 - INFO - __main__ - Step 80 Global step 80 Train loss 1.95 on epoch=19
06/23/2022 09:20:27 - INFO - __main__ - Step 90 Global step 90 Train loss 1.97 on epoch=22
06/23/2022 09:20:29 - INFO - __main__ - Step 100 Global step 100 Train loss 1.53 on epoch=24
06/23/2022 09:20:30 - INFO - __main__ - Global step 100 Train loss 2.05 Classification-F1 0.14402103273071015 on epoch=24
06/23/2022 09:20:30 - INFO - __main__ - Saving model with best Classification-F1: 0.017543859649122806 -> 0.14402103273071015 on epoch=24, global_step=100
06/23/2022 09:20:33 - INFO - __main__ - Step 110 Global step 110 Train loss 1.47 on epoch=27
06/23/2022 09:20:35 - INFO - __main__ - Step 120 Global step 120 Train loss 1.20 on epoch=29
06/23/2022 09:20:38 - INFO - __main__ - Step 130 Global step 130 Train loss 1.13 on epoch=32
06/23/2022 09:20:40 - INFO - __main__ - Step 140 Global step 140 Train loss 1.12 on epoch=34
06/23/2022 09:20:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.97 on epoch=37
06/23/2022 09:20:43 - INFO - __main__ - Global step 150 Train loss 1.18 Classification-F1 0.376984126984127 on epoch=37
06/23/2022 09:20:43 - INFO - __main__ - Saving model with best Classification-F1: 0.14402103273071015 -> 0.376984126984127 on epoch=37, global_step=150
06/23/2022 09:20:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=39
06/23/2022 09:20:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.99 on epoch=42
06/23/2022 09:20:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.77 on epoch=44
06/23/2022 09:20:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.93 on epoch=47
06/23/2022 09:20:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=49
06/23/2022 09:20:56 - INFO - __main__ - Global step 200 Train loss 0.90 Classification-F1 0.47437882035405254 on epoch=49
06/23/2022 09:20:57 - INFO - __main__ - Saving model with best Classification-F1: 0.376984126984127 -> 0.47437882035405254 on epoch=49, global_step=200
06/23/2022 09:20:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.79 on epoch=52
06/23/2022 09:21:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.80 on epoch=54
06/23/2022 09:21:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.67 on epoch=57
06/23/2022 09:21:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.67 on epoch=59
06/23/2022 09:21:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.69 on epoch=62
06/23/2022 09:21:10 - INFO - __main__ - Global step 250 Train loss 0.72 Classification-F1 0.5220116141168772 on epoch=62
06/23/2022 09:21:10 - INFO - __main__ - Saving model with best Classification-F1: 0.47437882035405254 -> 0.5220116141168772 on epoch=62, global_step=250
06/23/2022 09:21:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.67 on epoch=64
06/23/2022 09:21:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.66 on epoch=67
06/23/2022 09:21:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.70 on epoch=69
06/23/2022 09:21:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.74 on epoch=72
06/23/2022 09:21:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.71 on epoch=74
06/23/2022 09:21:23 - INFO - __main__ - Global step 300 Train loss 0.70 Classification-F1 0.5193770299985878 on epoch=74
06/23/2022 09:21:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.61 on epoch=77
06/23/2022 09:21:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.63 on epoch=79
06/23/2022 09:21:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.65 on epoch=82
06/23/2022 09:21:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.59 on epoch=84
06/23/2022 09:21:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.56 on epoch=87
06/23/2022 09:21:36 - INFO - __main__ - Global step 350 Train loss 0.61 Classification-F1 0.6469272844272844 on epoch=87
06/23/2022 09:21:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5220116141168772 -> 0.6469272844272844 on epoch=87, global_step=350
06/23/2022 09:21:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=89
06/23/2022 09:21:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.62 on epoch=92
06/23/2022 09:21:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.55 on epoch=94
06/23/2022 09:21:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.58 on epoch=97
06/23/2022 09:21:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=99
06/23/2022 09:21:49 - INFO - __main__ - Global step 400 Train loss 0.55 Classification-F1 0.5935013262599469 on epoch=99
06/23/2022 09:21:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.60 on epoch=102
06/23/2022 09:21:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=104
06/23/2022 09:21:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.57 on epoch=107
06/23/2022 09:21:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.53 on epoch=109
06/23/2022 09:22:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.58 on epoch=112
06/23/2022 09:22:02 - INFO - __main__ - Global step 450 Train loss 0.56 Classification-F1 0.7397686100131753 on epoch=112
06/23/2022 09:22:02 - INFO - __main__ - Saving model with best Classification-F1: 0.6469272844272844 -> 0.7397686100131753 on epoch=112, global_step=450
06/23/2022 09:22:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=114
06/23/2022 09:22:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=117
06/23/2022 09:22:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=119
06/23/2022 09:22:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=122
06/23/2022 09:22:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=124
06/23/2022 09:22:15 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.7378139483294617 on epoch=124
06/23/2022 09:22:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=127
06/23/2022 09:22:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=129
06/23/2022 09:22:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=132
06/23/2022 09:22:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=134
06/23/2022 09:22:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=137
06/23/2022 09:22:28 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.7434440559440559 on epoch=137
06/23/2022 09:22:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7397686100131753 -> 0.7434440559440559 on epoch=137, global_step=550
06/23/2022 09:22:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=139
06/23/2022 09:22:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=142
06/23/2022 09:22:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=144
06/23/2022 09:22:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=147
06/23/2022 09:22:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=149
06/23/2022 09:22:41 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.7571847507331378 on epoch=149
06/23/2022 09:22:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7434440559440559 -> 0.7571847507331378 on epoch=149, global_step=600
06/23/2022 09:22:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=152
06/23/2022 09:22:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=154
06/23/2022 09:22:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=157
06/23/2022 09:22:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=159
06/23/2022 09:22:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=162
06/23/2022 09:22:54 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.7601282051282051 on epoch=162
06/23/2022 09:22:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7571847507331378 -> 0.7601282051282051 on epoch=162, global_step=650
06/23/2022 09:22:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=164
06/23/2022 09:22:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=167
06/23/2022 09:23:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=169
06/23/2022 09:23:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=172
06/23/2022 09:23:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=174
06/23/2022 09:23:07 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.7571847507331378 on epoch=174
06/23/2022 09:23:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=177
06/23/2022 09:23:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=179
06/23/2022 09:23:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=182
06/23/2022 09:23:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=184
06/23/2022 09:23:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=187
06/23/2022 09:23:20 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.79491991991992 on epoch=187
06/23/2022 09:23:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7601282051282051 -> 0.79491991991992 on epoch=187, global_step=750
06/23/2022 09:23:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=189
06/23/2022 09:23:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=192
06/23/2022 09:23:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=194
06/23/2022 09:23:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=197
06/23/2022 09:23:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=199
06/23/2022 09:23:33 - INFO - __main__ - Global step 800 Train loss 0.27 Classification-F1 0.7601282051282051 on epoch=199
06/23/2022 09:23:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=202
06/23/2022 09:23:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=204
06/23/2022 09:23:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.31 on epoch=207
06/23/2022 09:23:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=209
06/23/2022 09:23:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=212
06/23/2022 09:23:46 - INFO - __main__ - Global step 850 Train loss 0.28 Classification-F1 0.7628878878878879 on epoch=212
06/23/2022 09:23:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=214
06/23/2022 09:23:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=217
06/23/2022 09:23:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=219
06/23/2022 09:23:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=222
06/23/2022 09:23:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=224
06/23/2022 09:23:59 - INFO - __main__ - Global step 900 Train loss 0.28 Classification-F1 0.777834008097166 on epoch=224
06/23/2022 09:24:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=227
06/23/2022 09:24:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=229
06/23/2022 09:24:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.28 on epoch=232
06/23/2022 09:24:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=234
06/23/2022 09:24:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=237
06/23/2022 09:24:13 - INFO - __main__ - Global step 950 Train loss 0.26 Classification-F1 0.7756462891946763 on epoch=237
06/23/2022 09:24:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=239
06/23/2022 09:24:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=242
06/23/2022 09:24:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.28 on epoch=244
06/23/2022 09:24:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=247
06/23/2022 09:24:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=249
06/23/2022 09:24:26 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.7756462891946763 on epoch=249
06/23/2022 09:24:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=252
06/23/2022 09:24:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.18 on epoch=254
06/23/2022 09:24:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=257
06/23/2022 09:24:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=259
06/23/2022 09:24:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=262
06/23/2022 09:24:39 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.7933520921636372 on epoch=262
06/23/2022 09:24:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=264
06/23/2022 09:24:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=267
06/23/2022 09:24:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=269
06/23/2022 09:24:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=272
06/23/2022 09:24:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=274
06/23/2022 09:24:52 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.795279988828376 on epoch=274
06/23/2022 09:24:52 - INFO - __main__ - Saving model with best Classification-F1: 0.79491991991992 -> 0.795279988828376 on epoch=274, global_step=1100
06/23/2022 09:24:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=277
06/23/2022 09:24:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=279
06/23/2022 09:24:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=282
06/23/2022 09:25:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=284
06/23/2022 09:25:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.15 on epoch=287
06/23/2022 09:25:05 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.795279988828376 on epoch=287
06/23/2022 09:25:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=289
06/23/2022 09:25:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=292
06/23/2022 09:25:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=294
06/23/2022 09:25:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=297
06/23/2022 09:25:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=299
06/23/2022 09:25:18 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.7933520921636372 on epoch=299
06/23/2022 09:25:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=302
06/23/2022 09:25:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=304
06/23/2022 09:25:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=307
06/23/2022 09:25:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=309
06/23/2022 09:25:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=312
06/23/2022 09:25:31 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.7784059719543591 on epoch=312
06/23/2022 09:25:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=314
06/23/2022 09:25:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=317
06/23/2022 09:25:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=319
06/23/2022 09:25:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=322
06/23/2022 09:25:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=324
06/23/2022 09:25:44 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.8117003500763235 on epoch=324
06/23/2022 09:25:44 - INFO - __main__ - Saving model with best Classification-F1: 0.795279988828376 -> 0.8117003500763235 on epoch=324, global_step=1300
06/23/2022 09:25:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=327
06/23/2022 09:25:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=329
06/23/2022 09:25:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=332
06/23/2022 09:25:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=334
06/23/2022 09:25:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=337
06/23/2022 09:25:57 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.7784059719543591 on epoch=337
06/23/2022 09:26:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.12 on epoch=339
06/23/2022 09:26:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.14 on epoch=342
06/23/2022 09:26:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=344
06/23/2022 09:26:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=347
06/23/2022 09:26:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=349
06/23/2022 09:26:10 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.7784059719543591 on epoch=349
06/23/2022 09:26:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=352
06/23/2022 09:26:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=354
06/23/2022 09:26:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.13 on epoch=357
06/23/2022 09:26:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=359
06/23/2022 09:26:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=362
06/23/2022 09:26:24 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.795279988828376 on epoch=362
06/23/2022 09:26:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=364
06/23/2022 09:26:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=367
06/23/2022 09:26:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=369
06/23/2022 09:26:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=372
06/23/2022 09:26:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=374
06/23/2022 09:26:37 - INFO - __main__ - Global step 1500 Train loss 0.14 Classification-F1 0.795279988828376 on epoch=374
06/23/2022 09:26:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=377
06/23/2022 09:26:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.10 on epoch=379
06/23/2022 09:26:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.11 on epoch=382
06/23/2022 09:26:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=384
06/23/2022 09:26:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=387
06/23/2022 09:26:50 - INFO - __main__ - Global step 1550 Train loss 0.14 Classification-F1 0.8106442577030812 on epoch=387
06/23/2022 09:26:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=389
06/23/2022 09:26:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=392
06/23/2022 09:26:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=394
06/23/2022 09:27:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=397
06/23/2022 09:27:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=399
06/23/2022 09:27:03 - INFO - __main__ - Global step 1600 Train loss 0.12 Classification-F1 0.7954391433992952 on epoch=399
06/23/2022 09:27:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=402
06/23/2022 09:27:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.17 on epoch=404
06/23/2022 09:27:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=407
06/23/2022 09:27:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=409
06/23/2022 09:27:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=412
06/23/2022 09:27:16 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.7954391433992952 on epoch=412
06/23/2022 09:27:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=414
06/23/2022 09:27:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=417
06/23/2022 09:27:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=419
06/23/2022 09:27:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=422
06/23/2022 09:27:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=424
06/23/2022 09:27:29 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.8107142857142856 on epoch=424
06/23/2022 09:27:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=427
06/23/2022 09:27:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=429
06/23/2022 09:27:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=432
06/23/2022 09:27:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=434
06/23/2022 09:27:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=437
06/23/2022 09:27:42 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.8270646189510287 on epoch=437
06/23/2022 09:27:42 - INFO - __main__ - Saving model with best Classification-F1: 0.8117003500763235 -> 0.8270646189510287 on epoch=437, global_step=1750
06/23/2022 09:27:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=439
06/23/2022 09:27:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=442
06/23/2022 09:27:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=444
06/23/2022 09:27:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=447
06/23/2022 09:27:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=449
06/23/2022 09:27:55 - INFO - __main__ - Global step 1800 Train loss 0.09 Classification-F1 0.8107142857142856 on epoch=449
06/23/2022 09:27:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.18 on epoch=452
06/23/2022 09:28:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
06/23/2022 09:28:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=457
06/23/2022 09:28:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=459
06/23/2022 09:28:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=462
06/23/2022 09:28:09 - INFO - __main__ - Global step 1850 Train loss 0.10 Classification-F1 0.8107142857142856 on epoch=462
06/23/2022 09:28:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=464
06/23/2022 09:28:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=467
06/23/2022 09:28:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
06/23/2022 09:28:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=472
06/23/2022 09:28:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=474
06/23/2022 09:28:22 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.8107142857142856 on epoch=474
06/23/2022 09:28:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=477
06/23/2022 09:28:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=479
06/23/2022 09:28:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=482
06/23/2022 09:28:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=484
06/23/2022 09:28:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=487
06/23/2022 09:28:35 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.7954391433992952 on epoch=487
06/23/2022 09:28:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
06/23/2022 09:28:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=492
06/23/2022 09:28:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=494
06/23/2022 09:28:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
06/23/2022 09:28:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=499
06/23/2022 09:28:48 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.8107142857142856 on epoch=499
06/23/2022 09:28:51 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=502
06/23/2022 09:28:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=504
06/23/2022 09:28:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=507
06/23/2022 09:28:58 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
06/23/2022 09:29:00 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=512
06/23/2022 09:29:01 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.8107142857142856 on epoch=512
06/23/2022 09:29:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=514
06/23/2022 09:29:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=517
06/23/2022 09:29:08 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=519
06/23/2022 09:29:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=522
06/23/2022 09:29:13 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
06/23/2022 09:29:14 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.8107142857142856 on epoch=524
06/23/2022 09:29:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=527
06/23/2022 09:29:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=529
06/23/2022 09:29:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
06/23/2022 09:29:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=534
06/23/2022 09:29:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
06/23/2022 09:29:27 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.8107142857142856 on epoch=537
06/23/2022 09:29:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
06/23/2022 09:29:32 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=542
06/23/2022 09:29:35 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=544
06/23/2022 09:29:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=547
06/23/2022 09:29:40 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
06/23/2022 09:29:40 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.7517465626161278 on epoch=549
06/23/2022 09:29:43 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
06/23/2022 09:29:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.19 on epoch=554
06/23/2022 09:29:48 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=557
06/23/2022 09:29:50 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
06/23/2022 09:29:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
06/23/2022 09:29:54 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.8107142857142856 on epoch=562
06/23/2022 09:29:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=564
06/23/2022 09:29:58 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
06/23/2022 09:30:01 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
06/23/2022 09:30:03 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=572
06/23/2022 09:30:06 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
06/23/2022 09:30:07 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.8107142857142856 on epoch=574
06/23/2022 09:30:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
06/23/2022 09:30:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
06/23/2022 09:30:14 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
06/23/2022 09:30:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=584
06/23/2022 09:30:19 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=587
06/23/2022 09:30:20 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7930192865676736 on epoch=587
06/23/2022 09:30:22 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=589
06/23/2022 09:30:25 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
06/23/2022 09:30:27 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=594
06/23/2022 09:30:29 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.15 on epoch=597
06/23/2022 09:30:32 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=599
06/23/2022 09:30:33 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.8107142857142856 on epoch=599
06/23/2022 09:30:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
06/23/2022 09:30:38 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=604
06/23/2022 09:30:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=607
06/23/2022 09:30:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=609
06/23/2022 09:30:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
06/23/2022 09:30:46 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.8107142857142856 on epoch=612
06/23/2022 09:30:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=614
06/23/2022 09:30:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.13 on epoch=617
06/23/2022 09:30:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=619
06/23/2022 09:30:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
06/23/2022 09:30:58 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
06/23/2022 09:30:59 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.8107142857142856 on epoch=624
06/23/2022 09:31:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
06/23/2022 09:31:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
06/23/2022 09:31:06 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
06/23/2022 09:31:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
06/23/2022 09:31:11 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
06/23/2022 09:31:12 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.8439393939393939 on epoch=637
06/23/2022 09:31:12 - INFO - __main__ - Saving model with best Classification-F1: 0.8270646189510287 -> 0.8439393939393939 on epoch=637, global_step=2550
06/23/2022 09:31:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
06/23/2022 09:31:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=642
06/23/2022 09:31:19 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
06/23/2022 09:31:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/23/2022 09:31:24 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/23/2022 09:31:25 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.8107142857142856 on epoch=649
06/23/2022 09:31:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=652
06/23/2022 09:31:30 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=654
06/23/2022 09:31:32 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=657
06/23/2022 09:31:35 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
06/23/2022 09:31:37 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
06/23/2022 09:31:38 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.8107142857142856 on epoch=662
06/23/2022 09:31:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
06/23/2022 09:31:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=667
06/23/2022 09:31:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
06/23/2022 09:31:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
06/23/2022 09:31:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=674
06/23/2022 09:31:51 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.8107142857142856 on epoch=674
06/23/2022 09:31:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
06/23/2022 09:31:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
06/23/2022 09:31:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
06/23/2022 09:32:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=684
06/23/2022 09:32:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=687
06/23/2022 09:32:05 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.8107142857142856 on epoch=687
06/23/2022 09:32:07 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
06/23/2022 09:32:09 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
06/23/2022 09:32:12 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=694
06/23/2022 09:32:14 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
06/23/2022 09:32:17 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/23/2022 09:32:18 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.8085239085239084 on epoch=699
06/23/2022 09:32:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=702
06/23/2022 09:32:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=704
06/23/2022 09:32:25 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=707
06/23/2022 09:32:28 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=709
06/23/2022 09:32:30 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
06/23/2022 09:32:31 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.7935185185185184 on epoch=712
06/23/2022 09:32:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
06/23/2022 09:32:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=717
06/23/2022 09:32:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
06/23/2022 09:32:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
06/23/2022 09:32:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
06/23/2022 09:32:44 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.8286137234413096 on epoch=724
06/23/2022 09:32:47 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
06/23/2022 09:32:49 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
06/23/2022 09:32:52 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
06/23/2022 09:32:54 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/23/2022 09:32:56 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=737
06/23/2022 09:32:57 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.8114612511671334 on epoch=737
06/23/2022 09:33:00 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
06/23/2022 09:33:02 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
06/23/2022 09:33:05 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=744
06/23/2022 09:33:07 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
06/23/2022 09:33:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
06/23/2022 09:33:11 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.8107142857142856 on epoch=749
06/23/2022 09:33:11 - INFO - __main__ - save last model!
06/23/2022 09:33:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 09:33:11 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 09:33:11 - INFO - __main__ - Printing 3 examples
06/23/2022 09:33:11 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 09:33:11 - INFO - __main__ - ['others']
06/23/2022 09:33:11 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 09:33:11 - INFO - __main__ - ['others']
06/23/2022 09:33:11 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 09:33:11 - INFO - __main__ - ['others']
06/23/2022 09:33:11 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:33:11 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:33:11 - INFO - __main__ - Printing 3 examples
06/23/2022 09:33:11 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/23/2022 09:33:11 - INFO - __main__ - ['sad']
06/23/2022 09:33:11 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/23/2022 09:33:11 - INFO - __main__ - ['sad']
06/23/2022 09:33:11 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/23/2022 09:33:11 - INFO - __main__ - ['sad']
06/23/2022 09:33:11 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:33:11 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:33:11 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 09:33:11 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:33:11 - INFO - __main__ - Printing 3 examples
06/23/2022 09:33:11 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/23/2022 09:33:11 - INFO - __main__ - ['sad']
06/23/2022 09:33:11 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/23/2022 09:33:11 - INFO - __main__ - ['sad']
06/23/2022 09:33:11 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/23/2022 09:33:11 - INFO - __main__ - ['sad']
06/23/2022 09:33:11 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:33:11 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:33:11 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 09:33:13 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:33:18 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 09:33:26 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 09:33:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 09:33:27 - INFO - __main__ - Starting training!
06/23/2022 09:34:32 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_13_0.2_8_predictions.txt
06/23/2022 09:34:32 - INFO - __main__ - Classification-F1 on test data: 0.2671
06/23/2022 09:34:32 - INFO - __main__ - prefix=emo_16_13, lr=0.2, bsz=8, dev_performance=0.8439393939393939, test_performance=0.2670711304702276
06/23/2022 09:34:32 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.5, bsz=8 ...
06/23/2022 09:34:33 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:34:33 - INFO - __main__ - Printing 3 examples
06/23/2022 09:34:33 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/23/2022 09:34:33 - INFO - __main__ - ['sad']
06/23/2022 09:34:33 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/23/2022 09:34:33 - INFO - __main__ - ['sad']
06/23/2022 09:34:33 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/23/2022 09:34:33 - INFO - __main__ - ['sad']
06/23/2022 09:34:33 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:34:33 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:34:33 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 09:34:33 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:34:33 - INFO - __main__ - Printing 3 examples
06/23/2022 09:34:33 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/23/2022 09:34:33 - INFO - __main__ - ['sad']
06/23/2022 09:34:33 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/23/2022 09:34:33 - INFO - __main__ - ['sad']
06/23/2022 09:34:33 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/23/2022 09:34:33 - INFO - __main__ - ['sad']
06/23/2022 09:34:33 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:34:33 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:34:33 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 09:34:49 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 09:34:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 09:34:49 - INFO - __main__ - Starting training!
06/23/2022 09:34:52 - INFO - __main__ - Step 10 Global step 10 Train loss 4.50 on epoch=2
06/23/2022 09:34:55 - INFO - __main__ - Step 20 Global step 20 Train loss 3.43 on epoch=4
06/23/2022 09:34:57 - INFO - __main__ - Step 30 Global step 30 Train loss 2.71 on epoch=7
06/23/2022 09:35:00 - INFO - __main__ - Step 40 Global step 40 Train loss 1.94 on epoch=9
06/23/2022 09:35:02 - INFO - __main__ - Step 50 Global step 50 Train loss 1.53 on epoch=12
06/23/2022 09:35:03 - INFO - __main__ - Global step 50 Train loss 2.82 Classification-F1 0.33544598886849086 on epoch=12
06/23/2022 09:35:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.33544598886849086 on epoch=12, global_step=50
06/23/2022 09:35:06 - INFO - __main__ - Step 60 Global step 60 Train loss 1.18 on epoch=14
06/23/2022 09:35:08 - INFO - __main__ - Step 70 Global step 70 Train loss 1.01 on epoch=17
06/23/2022 09:35:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.78 on epoch=19
06/23/2022 09:35:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.82 on epoch=22
06/23/2022 09:35:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.66 on epoch=24
06/23/2022 09:35:16 - INFO - __main__ - Global step 100 Train loss 0.89 Classification-F1 0.5745098039215687 on epoch=24
06/23/2022 09:35:16 - INFO - __main__ - Saving model with best Classification-F1: 0.33544598886849086 -> 0.5745098039215687 on epoch=24, global_step=100
06/23/2022 09:35:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.69 on epoch=27
06/23/2022 09:35:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=29
06/23/2022 09:35:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=32
06/23/2022 09:35:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.59 on epoch=34
06/23/2022 09:35:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.64 on epoch=37
06/23/2022 09:35:29 - INFO - __main__ - Global step 150 Train loss 0.62 Classification-F1 0.5896126159284054 on epoch=37
06/23/2022 09:35:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5745098039215687 -> 0.5896126159284054 on epoch=37, global_step=150
06/23/2022 09:35:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=39
06/23/2022 09:35:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=42
06/23/2022 09:35:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=44
06/23/2022 09:35:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=47
06/23/2022 09:35:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=49
06/23/2022 09:35:42 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.6269276131505234 on epoch=49
06/23/2022 09:35:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5896126159284054 -> 0.6269276131505234 on epoch=49, global_step=200
06/23/2022 09:35:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=52
06/23/2022 09:35:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.32 on epoch=54
06/23/2022 09:35:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=57
06/23/2022 09:35:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=59
06/23/2022 09:35:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=62
06/23/2022 09:35:55 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.6299270904534062 on epoch=62
06/23/2022 09:35:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6269276131505234 -> 0.6299270904534062 on epoch=62, global_step=250
06/23/2022 09:35:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=64
06/23/2022 09:36:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=67
06/23/2022 09:36:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=69
06/23/2022 09:36:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=72
06/23/2022 09:36:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=74
06/23/2022 09:36:08 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.6424603174603174 on epoch=74
06/23/2022 09:36:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6299270904534062 -> 0.6424603174603174 on epoch=74, global_step=300
06/23/2022 09:36:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=77
06/23/2022 09:36:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=79
06/23/2022 09:36:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=82
06/23/2022 09:36:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=84
06/23/2022 09:36:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.34 on epoch=87
06/23/2022 09:36:21 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.6149997924191473 on epoch=87
06/23/2022 09:36:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=89
06/23/2022 09:36:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=92
06/23/2022 09:36:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=94
06/23/2022 09:36:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=97
06/23/2022 09:36:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=99
06/23/2022 09:36:34 - INFO - __main__ - Global step 400 Train loss 0.29 Classification-F1 0.6147755753018911 on epoch=99
06/23/2022 09:36:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=102
06/23/2022 09:36:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=104
06/23/2022 09:36:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=107
06/23/2022 09:36:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=109
06/23/2022 09:36:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=112
06/23/2022 09:36:47 - INFO - __main__ - Global step 450 Train loss 0.25 Classification-F1 0.6000804375804375 on epoch=112
06/23/2022 09:36:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=114
06/23/2022 09:36:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=117
06/23/2022 09:36:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=119
06/23/2022 09:36:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=122
06/23/2022 09:37:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=124
06/23/2022 09:37:01 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.6670419363917817 on epoch=124
06/23/2022 09:37:01 - INFO - __main__ - Saving model with best Classification-F1: 0.6424603174603174 -> 0.6670419363917817 on epoch=124, global_step=500
06/23/2022 09:37:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=127
06/23/2022 09:37:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.18 on epoch=129
06/23/2022 09:37:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=132
06/23/2022 09:37:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=134
06/23/2022 09:37:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=137
06/23/2022 09:37:14 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.6377840257296964 on epoch=137
06/23/2022 09:37:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=139
06/23/2022 09:37:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=142
06/23/2022 09:37:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=144
06/23/2022 09:37:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=147
06/23/2022 09:37:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=149
06/23/2022 09:37:27 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.6522214627477786 on epoch=149
06/23/2022 09:37:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.16 on epoch=152
06/23/2022 09:37:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=154
06/23/2022 09:37:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=157
06/23/2022 09:37:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=159
06/23/2022 09:37:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=162
06/23/2022 09:37:40 - INFO - __main__ - Global step 650 Train loss 0.15 Classification-F1 0.6516857766857767 on epoch=162
06/23/2022 09:37:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=164
06/23/2022 09:37:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=167
06/23/2022 09:37:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=169
06/23/2022 09:37:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=172
06/23/2022 09:37:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=174
06/23/2022 09:37:53 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.6884633994608418 on epoch=174
06/23/2022 09:37:53 - INFO - __main__ - Saving model with best Classification-F1: 0.6670419363917817 -> 0.6884633994608418 on epoch=174, global_step=700
06/23/2022 09:37:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=177
06/23/2022 09:37:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=179
06/23/2022 09:38:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=182
06/23/2022 09:38:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=184
06/23/2022 09:38:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=187
06/23/2022 09:38:06 - INFO - __main__ - Global step 750 Train loss 0.10 Classification-F1 0.6522214627477786 on epoch=187
06/23/2022 09:38:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=189
06/23/2022 09:38:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=192
06/23/2022 09:38:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=194
06/23/2022 09:38:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=197
06/23/2022 09:38:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=199
06/23/2022 09:38:20 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.6527445507708667 on epoch=199
06/23/2022 09:38:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=202
06/23/2022 09:38:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=204
06/23/2022 09:38:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=207
06/23/2022 09:38:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=209
06/23/2022 09:38:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=212
06/23/2022 09:38:33 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.6738899613899614 on epoch=212
06/23/2022 09:38:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.11 on epoch=214
06/23/2022 09:38:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=217
06/23/2022 09:38:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=219
06/23/2022 09:38:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=222
06/23/2022 09:38:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
06/23/2022 09:38:46 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.7025869768197135 on epoch=224
06/23/2022 09:38:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6884633994608418 -> 0.7025869768197135 on epoch=224, global_step=900
06/23/2022 09:38:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=227
06/23/2022 09:38:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=229
06/23/2022 09:38:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=232
06/23/2022 09:38:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=234
06/23/2022 09:38:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=237
06/23/2022 09:38:59 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.6753345210568211 on epoch=237
06/23/2022 09:39:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=239
06/23/2022 09:39:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
06/23/2022 09:39:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=244
06/23/2022 09:39:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=247
06/23/2022 09:39:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=249
06/23/2022 09:39:12 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.6753345210568211 on epoch=249
06/23/2022 09:39:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=252
06/23/2022 09:39:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=254
06/23/2022 09:39:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=257
06/23/2022 09:39:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
06/23/2022 09:39:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
06/23/2022 09:39:25 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.7090728715728717 on epoch=262
06/23/2022 09:39:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7025869768197135 -> 0.7090728715728717 on epoch=262, global_step=1050
06/23/2022 09:39:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
06/23/2022 09:39:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
06/23/2022 09:39:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=269
06/23/2022 09:39:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=272
06/23/2022 09:39:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
06/23/2022 09:39:38 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.6527445507708667 on epoch=274
06/23/2022 09:39:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
06/23/2022 09:39:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
06/23/2022 09:39:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
06/23/2022 09:39:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=284
06/23/2022 09:39:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=287
06/23/2022 09:39:51 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.688794440968354 on epoch=287
06/23/2022 09:39:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=289
06/23/2022 09:39:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=292
06/23/2022 09:39:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=294
06/23/2022 09:40:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
06/23/2022 09:40:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
06/23/2022 09:40:04 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.6956129516613389 on epoch=299
06/23/2022 09:40:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
06/23/2022 09:40:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
06/23/2022 09:40:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
06/23/2022 09:40:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
06/23/2022 09:40:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
06/23/2022 09:40:18 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.6673729778992937 on epoch=312
06/23/2022 09:40:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
06/23/2022 09:40:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
06/23/2022 09:40:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
06/23/2022 09:40:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
06/23/2022 09:40:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
06/23/2022 09:40:31 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.653913057987761 on epoch=324
06/23/2022 09:40:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
06/23/2022 09:40:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
06/23/2022 09:40:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
06/23/2022 09:40:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
06/23/2022 09:40:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
06/23/2022 09:40:44 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.6672274857758729 on epoch=337
06/23/2022 09:40:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
06/23/2022 09:40:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=342
06/23/2022 09:40:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
06/23/2022 09:40:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
06/23/2022 09:40:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
06/23/2022 09:40:57 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6672274857758729 on epoch=349
06/23/2022 09:40:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
06/23/2022 09:41:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
06/23/2022 09:41:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
06/23/2022 09:41:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
06/23/2022 09:41:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=362
06/23/2022 09:41:10 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.6672274857758729 on epoch=362
06/23/2022 09:41:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
06/23/2022 09:41:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
06/23/2022 09:41:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
06/23/2022 09:41:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
06/23/2022 09:41:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
06/23/2022 09:41:23 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.6672274857758729 on epoch=374
06/23/2022 09:41:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
06/23/2022 09:41:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
06/23/2022 09:41:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/23/2022 09:41:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
06/23/2022 09:41:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
06/23/2022 09:41:36 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.6672274857758729 on epoch=387
06/23/2022 09:41:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
06/23/2022 09:41:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
06/23/2022 09:41:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
06/23/2022 09:41:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
06/23/2022 09:41:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
06/23/2022 09:41:50 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6672274857758729 on epoch=399
06/23/2022 09:41:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
06/23/2022 09:41:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
06/23/2022 09:41:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
06/23/2022 09:41:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/23/2022 09:42:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
06/23/2022 09:42:03 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.6887492515768149 on epoch=412
06/23/2022 09:42:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
06/23/2022 09:42:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
06/23/2022 09:42:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
06/23/2022 09:42:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
06/23/2022 09:42:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
06/23/2022 09:42:16 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.7631200022091862 on epoch=424
06/23/2022 09:42:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7090728715728717 -> 0.7631200022091862 on epoch=424, global_step=1700
06/23/2022 09:42:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
06/23/2022 09:42:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
06/23/2022 09:42:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
06/23/2022 09:42:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
06/23/2022 09:42:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=437
06/23/2022 09:42:29 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.6672274857758729 on epoch=437
06/23/2022 09:42:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
06/23/2022 09:42:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
06/23/2022 09:42:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
06/23/2022 09:42:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
06/23/2022 09:42:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
06/23/2022 09:42:42 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.6539130579877609 on epoch=449
06/23/2022 09:42:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
06/23/2022 09:42:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
06/23/2022 09:42:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/23/2022 09:42:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
06/23/2022 09:42:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
06/23/2022 09:42:55 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6904775176514306 on epoch=462
06/23/2022 09:42:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/23/2022 09:43:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/23/2022 09:43:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
06/23/2022 09:43:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
06/23/2022 09:43:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
06/23/2022 09:43:08 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7105994152046784 on epoch=474
06/23/2022 09:43:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
06/23/2022 09:43:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
06/23/2022 09:43:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/23/2022 09:43:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
06/23/2022 09:43:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
06/23/2022 09:43:21 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7297447447447447 on epoch=487
06/23/2022 09:43:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
06/23/2022 09:43:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=492
06/23/2022 09:43:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
06/23/2022 09:43:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
06/23/2022 09:43:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
06/23/2022 09:43:34 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6753345210568211 on epoch=499
06/23/2022 09:43:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
06/23/2022 09:43:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=504
06/23/2022 09:43:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
06/23/2022 09:43:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
06/23/2022 09:43:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
06/23/2022 09:43:47 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.7162312312312312 on epoch=512
06/23/2022 09:43:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/23/2022 09:43:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
06/23/2022 09:43:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
06/23/2022 09:43:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
06/23/2022 09:44:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/23/2022 09:44:01 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6672274857758729 on epoch=524
06/23/2022 09:44:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
06/23/2022 09:44:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/23/2022 09:44:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/23/2022 09:44:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
06/23/2022 09:44:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/23/2022 09:44:14 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.6887492515768149 on epoch=537
06/23/2022 09:44:16 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/23/2022 09:44:19 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/23/2022 09:44:21 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/23/2022 09:44:24 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/23/2022 09:44:26 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/23/2022 09:44:27 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.7091264651748523 on epoch=549
06/23/2022 09:44:30 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/23/2022 09:44:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
06/23/2022 09:44:34 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
06/23/2022 09:44:37 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
06/23/2022 09:44:39 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/23/2022 09:44:40 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6439493374977245 on epoch=562
06/23/2022 09:44:43 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=564
06/23/2022 09:44:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
06/23/2022 09:44:48 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/23/2022 09:44:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/23/2022 09:44:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
06/23/2022 09:44:54 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6887492515768149 on epoch=574
06/23/2022 09:44:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/23/2022 09:44:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/23/2022 09:45:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/23/2022 09:45:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
06/23/2022 09:45:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
06/23/2022 09:45:07 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.6887492515768149 on epoch=587
06/23/2022 09:45:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
06/23/2022 09:45:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/23/2022 09:45:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/23/2022 09:45:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/23/2022 09:45:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/23/2022 09:45:20 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.6663378288378289 on epoch=599
06/23/2022 09:45:22 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
06/23/2022 09:45:25 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/23/2022 09:45:27 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/23/2022 09:45:30 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/23/2022 09:45:32 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/23/2022 09:45:33 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6887492515768149 on epoch=612
06/23/2022 09:45:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
06/23/2022 09:45:38 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/23/2022 09:45:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/23/2022 09:45:43 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
06/23/2022 09:45:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/23/2022 09:45:46 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6878595946387709 on epoch=624
06/23/2022 09:45:49 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/23/2022 09:45:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/23/2022 09:45:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/23/2022 09:45:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=634
06/23/2022 09:45:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/23/2022 09:45:59 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6672274857758729 on epoch=637
06/23/2022 09:46:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/23/2022 09:46:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/23/2022 09:46:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/23/2022 09:46:09 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/23/2022 09:46:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/23/2022 09:46:12 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7174973588353586 on epoch=649
06/23/2022 09:46:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=652
06/23/2022 09:46:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/23/2022 09:46:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=657
06/23/2022 09:46:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/23/2022 09:46:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/23/2022 09:46:26 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6672274857758729 on epoch=662
06/23/2022 09:46:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
06/23/2022 09:46:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/23/2022 09:46:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/23/2022 09:46:36 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/23/2022 09:46:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/23/2022 09:46:39 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7093065324601827 on epoch=674
06/23/2022 09:46:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=677
06/23/2022 09:46:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/23/2022 09:46:46 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/23/2022 09:46:49 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/23/2022 09:46:51 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
06/23/2022 09:46:52 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7093065324601827 on epoch=687
06/23/2022 09:46:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
06/23/2022 09:46:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/23/2022 09:46:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/23/2022 09:47:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
06/23/2022 09:47:04 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/23/2022 09:47:05 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6672274857758729 on epoch=699
06/23/2022 09:47:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/23/2022 09:47:10 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=704
06/23/2022 09:47:13 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/23/2022 09:47:15 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/23/2022 09:47:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/23/2022 09:47:18 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6806235431235431 on epoch=712
06/23/2022 09:47:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/23/2022 09:47:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/23/2022 09:47:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/23/2022 09:47:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/23/2022 09:47:30 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/23/2022 09:47:31 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6672274857758729 on epoch=724
06/23/2022 09:47:34 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/23/2022 09:47:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/23/2022 09:47:39 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/23/2022 09:47:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/23/2022 09:47:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/23/2022 09:47:44 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6753345210568211 on epoch=737
06/23/2022 09:47:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/23/2022 09:47:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/23/2022 09:47:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/23/2022 09:47:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
06/23/2022 09:47:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/23/2022 09:47:58 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6809086779675015 on epoch=749
06/23/2022 09:47:58 - INFO - __main__ - save last model!
06/23/2022 09:47:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 09:47:58 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 09:47:58 - INFO - __main__ - Printing 3 examples
06/23/2022 09:47:58 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 09:47:58 - INFO - __main__ - ['others']
06/23/2022 09:47:58 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 09:47:58 - INFO - __main__ - ['others']
06/23/2022 09:47:58 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 09:47:58 - INFO - __main__ - ['others']
06/23/2022 09:47:58 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:47:58 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:47:58 - INFO - __main__ - Printing 3 examples
06/23/2022 09:47:58 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/23/2022 09:47:58 - INFO - __main__ - ['sad']
06/23/2022 09:47:58 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/23/2022 09:47:58 - INFO - __main__ - ['sad']
06/23/2022 09:47:58 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/23/2022 09:47:58 - INFO - __main__ - ['sad']
06/23/2022 09:47:58 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:47:58 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:47:58 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 09:47:58 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:47:58 - INFO - __main__ - Printing 3 examples
06/23/2022 09:47:58 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/23/2022 09:47:58 - INFO - __main__ - ['sad']
06/23/2022 09:47:58 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/23/2022 09:47:58 - INFO - __main__ - ['sad']
06/23/2022 09:47:58 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/23/2022 09:47:58 - INFO - __main__ - ['sad']
06/23/2022 09:47:58 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:47:58 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:47:58 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 09:48:00 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:48:05 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 09:48:13 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 09:48:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 09:48:14 - INFO - __main__ - Starting training!
06/23/2022 09:49:25 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_21_0.5_8_predictions.txt
06/23/2022 09:49:25 - INFO - __main__ - Classification-F1 on test data: 0.2141
06/23/2022 09:49:25 - INFO - __main__ - prefix=emo_16_21, lr=0.5, bsz=8, dev_performance=0.7631200022091862, test_performance=0.21414526570040754
06/23/2022 09:49:25 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.4, bsz=8 ...
06/23/2022 09:49:26 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:49:26 - INFO - __main__ - Printing 3 examples
06/23/2022 09:49:26 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/23/2022 09:49:26 - INFO - __main__ - ['sad']
06/23/2022 09:49:26 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/23/2022 09:49:26 - INFO - __main__ - ['sad']
06/23/2022 09:49:26 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/23/2022 09:49:26 - INFO - __main__ - ['sad']
06/23/2022 09:49:26 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:49:26 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:49:27 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 09:49:27 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 09:49:27 - INFO - __main__ - Printing 3 examples
06/23/2022 09:49:27 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/23/2022 09:49:27 - INFO - __main__ - ['sad']
06/23/2022 09:49:27 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/23/2022 09:49:27 - INFO - __main__ - ['sad']
06/23/2022 09:49:27 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/23/2022 09:49:27 - INFO - __main__ - ['sad']
06/23/2022 09:49:27 - INFO - __main__ - Tokenizing Input ...
06/23/2022 09:49:27 - INFO - __main__ - Tokenizing Output ...
06/23/2022 09:49:27 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 09:49:42 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 09:49:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 09:49:43 - INFO - __main__ - Starting training!
06/23/2022 09:49:46 - INFO - __main__ - Step 10 Global step 10 Train loss 4.59 on epoch=2
06/23/2022 09:49:48 - INFO - __main__ - Step 20 Global step 20 Train loss 3.43 on epoch=4
06/23/2022 09:49:50 - INFO - __main__ - Step 30 Global step 30 Train loss 2.88 on epoch=7
06/23/2022 09:49:53 - INFO - __main__ - Step 40 Global step 40 Train loss 2.27 on epoch=9
06/23/2022 09:49:55 - INFO - __main__ - Step 50 Global step 50 Train loss 2.02 on epoch=12
06/23/2022 09:49:56 - INFO - __main__ - Global step 50 Train loss 3.04 Classification-F1 0.1400352733686067 on epoch=12
06/23/2022 09:49:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1400352733686067 on epoch=12, global_step=50
06/23/2022 09:49:59 - INFO - __main__ - Step 60 Global step 60 Train loss 1.54 on epoch=14
06/23/2022 09:50:01 - INFO - __main__ - Step 70 Global step 70 Train loss 1.19 on epoch=17
06/23/2022 09:50:04 - INFO - __main__ - Step 80 Global step 80 Train loss 0.90 on epoch=19
06/23/2022 09:50:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.88 on epoch=22
06/23/2022 09:50:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.85 on epoch=24
06/23/2022 09:50:09 - INFO - __main__ - Global step 100 Train loss 1.07 Classification-F1 0.5722774244833068 on epoch=24
06/23/2022 09:50:10 - INFO - __main__ - Saving model with best Classification-F1: 0.1400352733686067 -> 0.5722774244833068 on epoch=24, global_step=100
06/23/2022 09:50:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.75 on epoch=27
06/23/2022 09:50:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.74 on epoch=29
06/23/2022 09:50:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.75 on epoch=32
06/23/2022 09:50:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.73 on epoch=34
06/23/2022 09:50:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.70 on epoch=37
06/23/2022 09:50:22 - INFO - __main__ - Global step 150 Train loss 0.73 Classification-F1 0.6149305555555555 on epoch=37
06/23/2022 09:50:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5722774244833068 -> 0.6149305555555555 on epoch=37, global_step=150
06/23/2022 09:50:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=39
06/23/2022 09:50:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=42
06/23/2022 09:50:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=44
06/23/2022 09:50:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=47
06/23/2022 09:50:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=49
06/23/2022 09:50:35 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.5905007949125596 on epoch=49
06/23/2022 09:50:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.61 on epoch=52
06/23/2022 09:50:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=54
06/23/2022 09:50:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=57
06/23/2022 09:50:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=59
06/23/2022 09:50:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=62
06/23/2022 09:50:49 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.6299270904534062 on epoch=62
06/23/2022 09:50:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6149305555555555 -> 0.6299270904534062 on epoch=62, global_step=250
06/23/2022 09:50:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=64
06/23/2022 09:50:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=67
06/23/2022 09:50:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.35 on epoch=69
06/23/2022 09:50:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=72
06/23/2022 09:51:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=74
06/23/2022 09:51:02 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.5913288288288289 on epoch=74
06/23/2022 09:51:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=77
06/23/2022 09:51:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=79
06/23/2022 09:51:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=82
06/23/2022 09:51:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=84
06/23/2022 09:51:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=87
06/23/2022 09:51:15 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.6018241167434715 on epoch=87
06/23/2022 09:51:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=89
06/23/2022 09:51:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=92
06/23/2022 09:51:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=94
06/23/2022 09:51:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=97
06/23/2022 09:51:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=99
06/23/2022 09:51:28 - INFO - __main__ - Global step 400 Train loss 0.32 Classification-F1 0.5901386530843238 on epoch=99
06/23/2022 09:51:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=102
06/23/2022 09:51:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=104
06/23/2022 09:51:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=107
06/23/2022 09:51:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=109
06/23/2022 09:51:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=112
06/23/2022 09:51:41 - INFO - __main__ - Global step 450 Train loss 0.31 Classification-F1 0.5770815170008718 on epoch=112
06/23/2022 09:51:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.31 on epoch=114
06/23/2022 09:51:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=117
06/23/2022 09:51:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=119
06/23/2022 09:51:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=122
06/23/2022 09:51:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=124
06/23/2022 09:51:54 - INFO - __main__ - Global step 500 Train loss 0.28 Classification-F1 0.5901386530843238 on epoch=124
06/23/2022 09:51:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=127
06/23/2022 09:51:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=129
06/23/2022 09:52:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=132
06/23/2022 09:52:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=134
06/23/2022 09:52:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=137
06/23/2022 09:52:07 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.6015432394889102 on epoch=137
06/23/2022 09:52:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=139
06/23/2022 09:52:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=142
06/23/2022 09:52:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=144
06/23/2022 09:52:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=147
06/23/2022 09:52:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=149
06/23/2022 09:52:20 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.6015432394889102 on epoch=149
06/23/2022 09:52:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=152
06/23/2022 09:52:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=154
06/23/2022 09:52:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=157
06/23/2022 09:52:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=159
06/23/2022 09:52:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=162
06/23/2022 09:52:33 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.6015432394889102 on epoch=162
06/23/2022 09:52:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=164
06/23/2022 09:52:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=167
06/23/2022 09:52:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=169
06/23/2022 09:52:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=172
06/23/2022 09:52:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=174
06/23/2022 09:52:46 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.5901386530843238 on epoch=174
06/23/2022 09:52:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=177
06/23/2022 09:52:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=179
06/23/2022 09:52:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=182
06/23/2022 09:52:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=184
06/23/2022 09:52:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=187
06/23/2022 09:52:59 - INFO - __main__ - Global step 750 Train loss 0.17 Classification-F1 0.6147627132346998 on epoch=187
06/23/2022 09:53:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.15 on epoch=189
06/23/2022 09:53:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=192
06/23/2022 09:53:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=194
06/23/2022 09:53:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.16 on epoch=197
06/23/2022 09:53:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=199
06/23/2022 09:53:12 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.6147627132346998 on epoch=199
06/23/2022 09:53:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=202
06/23/2022 09:53:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=204
06/23/2022 09:53:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=207
06/23/2022 09:53:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=209
06/23/2022 09:53:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=212
06/23/2022 09:53:25 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.6377840257296963 on epoch=212
06/23/2022 09:53:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6299270904534062 -> 0.6377840257296963 on epoch=212, global_step=850
06/23/2022 09:53:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=214
06/23/2022 09:53:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=217
06/23/2022 09:53:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=219
06/23/2022 09:53:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=222
06/23/2022 09:53:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=224
06/23/2022 09:53:38 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.6162907268170427 on epoch=224
06/23/2022 09:53:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=227
06/23/2022 09:53:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=229
06/23/2022 09:53:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=232
06/23/2022 09:53:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=234
06/23/2022 09:53:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=237
06/23/2022 09:53:51 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.6293859649122808 on epoch=237
06/23/2022 09:53:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=239
06/23/2022 09:53:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=242
06/23/2022 09:53:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=244
06/23/2022 09:54:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=247
06/23/2022 09:54:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=249
06/23/2022 09:54:04 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.6741660138399268 on epoch=249
06/23/2022 09:54:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6377840257296963 -> 0.6741660138399268 on epoch=249, global_step=1000
06/23/2022 09:54:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
06/23/2022 09:54:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=254
06/23/2022 09:54:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
06/23/2022 09:54:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
06/23/2022 09:54:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=262
06/23/2022 09:54:17 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.653913057987761 on epoch=262
06/23/2022 09:54:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=264
06/23/2022 09:54:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=267
06/23/2022 09:54:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=269
06/23/2022 09:54:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=272
06/23/2022 09:54:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
06/23/2022 09:54:31 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.6294664024927183 on epoch=274
06/23/2022 09:54:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=277
06/23/2022 09:54:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
06/23/2022 09:54:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
06/23/2022 09:54:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=284
06/23/2022 09:54:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
06/23/2022 09:54:44 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.6294664024927183 on epoch=287
06/23/2022 09:54:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
06/23/2022 09:54:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
06/23/2022 09:54:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
06/23/2022 09:54:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
06/23/2022 09:54:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
06/23/2022 09:54:57 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.6527445507708667 on epoch=299
06/23/2022 09:54:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
06/23/2022 09:55:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=304
06/23/2022 09:55:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
06/23/2022 09:55:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=309
06/23/2022 09:55:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=312
06/23/2022 09:55:10 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.6377840257296964 on epoch=312
06/23/2022 09:55:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
06/23/2022 09:55:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
06/23/2022 09:55:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=319
06/23/2022 09:55:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=322
06/23/2022 09:55:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
06/23/2022 09:55:23 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.6944444444444445 on epoch=324
06/23/2022 09:55:23 - INFO - __main__ - Saving model with best Classification-F1: 0.6741660138399268 -> 0.6944444444444445 on epoch=324, global_step=1300
06/23/2022 09:55:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
06/23/2022 09:55:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=329
06/23/2022 09:55:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
06/23/2022 09:55:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
06/23/2022 09:55:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
06/23/2022 09:55:36 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.6393120393120393 on epoch=337
06/23/2022 09:55:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
06/23/2022 09:55:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=342
06/23/2022 09:55:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
06/23/2022 09:55:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
06/23/2022 09:55:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
06/23/2022 09:55:49 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.6046052631578949 on epoch=349
06/23/2022 09:55:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
06/23/2022 09:55:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
06/23/2022 09:55:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
06/23/2022 09:55:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
06/23/2022 09:56:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=362
06/23/2022 09:56:02 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.6393120393120393 on epoch=362
06/23/2022 09:56:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
06/23/2022 09:56:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
06/23/2022 09:56:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
06/23/2022 09:56:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
06/23/2022 09:56:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
06/23/2022 09:56:15 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.6609903381642512 on epoch=374
06/23/2022 09:56:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
06/23/2022 09:56:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
06/23/2022 09:56:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
06/23/2022 09:56:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
06/23/2022 09:56:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
06/23/2022 09:56:28 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7091264651748523 on epoch=387
06/23/2022 09:56:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6944444444444445 -> 0.7091264651748523 on epoch=387, global_step=1550
06/23/2022 09:56:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=389
06/23/2022 09:56:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
06/23/2022 09:56:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
06/23/2022 09:56:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
06/23/2022 09:56:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/23/2022 09:56:41 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.5901386530843238 on epoch=399
06/23/2022 09:56:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
06/23/2022 09:56:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
06/23/2022 09:56:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
06/23/2022 09:56:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/23/2022 09:56:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
06/23/2022 09:56:55 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.660496423196652 on epoch=412
06/23/2022 09:56:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
06/23/2022 09:56:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
06/23/2022 09:57:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
06/23/2022 09:57:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
06/23/2022 09:57:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
06/23/2022 09:57:08 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.6609903381642512 on epoch=424
06/23/2022 09:57:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
06/23/2022 09:57:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
06/23/2022 09:57:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
06/23/2022 09:57:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=434
06/23/2022 09:57:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
06/23/2022 09:57:21 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.6609903381642512 on epoch=437
06/23/2022 09:57:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
06/23/2022 09:57:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
06/23/2022 09:57:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
06/23/2022 09:57:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
06/23/2022 09:57:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
06/23/2022 09:57:34 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6609903381642512 on epoch=449
06/23/2022 09:57:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
06/23/2022 09:57:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
06/23/2022 09:57:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
06/23/2022 09:57:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
06/23/2022 09:57:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
06/23/2022 09:57:47 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6393120393120393 on epoch=462
06/23/2022 09:57:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/23/2022 09:57:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/23/2022 09:57:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/23/2022 09:57:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
06/23/2022 09:57:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
06/23/2022 09:58:00 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6393120393120393 on epoch=474
06/23/2022 09:58:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
06/23/2022 09:58:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=479
06/23/2022 09:58:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=482
06/23/2022 09:58:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/23/2022 09:58:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
06/23/2022 09:58:13 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.653913057987761 on epoch=487
06/23/2022 09:58:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
06/23/2022 09:58:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
06/23/2022 09:58:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
06/23/2022 09:58:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
06/23/2022 09:58:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
06/23/2022 09:58:26 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6294664024927183 on epoch=499
06/23/2022 09:58:29 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
06/23/2022 09:58:31 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
06/23/2022 09:58:34 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
06/23/2022 09:58:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/23/2022 09:58:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
06/23/2022 09:58:39 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6169871794871795 on epoch=512
06/23/2022 09:58:42 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
06/23/2022 09:58:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/23/2022 09:58:47 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
06/23/2022 09:58:49 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=522
06/23/2022 09:58:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/23/2022 09:58:53 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.6753345210568211 on epoch=524
06/23/2022 09:58:55 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
06/23/2022 09:58:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/23/2022 09:59:00 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
06/23/2022 09:59:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
06/23/2022 09:59:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/23/2022 09:59:06 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6294664024927183 on epoch=537
06/23/2022 09:59:08 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=539
06/23/2022 09:59:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
06/23/2022 09:59:13 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/23/2022 09:59:15 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/23/2022 09:59:18 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/23/2022 09:59:19 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.6956129516613389 on epoch=549
06/23/2022 09:59:21 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/23/2022 09:59:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/23/2022 09:59:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
06/23/2022 09:59:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/23/2022 09:59:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/23/2022 09:59:32 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.6755913568399727 on epoch=562
06/23/2022 09:59:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
06/23/2022 09:59:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=567
06/23/2022 09:59:39 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/23/2022 09:59:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
06/23/2022 09:59:44 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
06/23/2022 09:59:45 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.668112714987715 on epoch=574
06/23/2022 09:59:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/23/2022 09:59:50 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
06/23/2022 09:59:52 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
06/23/2022 09:59:55 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
06/23/2022 09:59:57 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/23/2022 09:59:58 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.653913057987761 on epoch=587
06/23/2022 10:00:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/23/2022 10:00:03 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/23/2022 10:00:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/23/2022 10:00:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
06/23/2022 10:00:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/23/2022 10:00:11 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6435591435591436 on epoch=599
06/23/2022 10:00:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/23/2022 10:00:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/23/2022 10:00:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/23/2022 10:00:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
06/23/2022 10:00:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
06/23/2022 10:00:24 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6393120393120393 on epoch=612
06/23/2022 10:00:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/23/2022 10:00:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
06/23/2022 10:00:32 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/23/2022 10:00:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/23/2022 10:00:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/23/2022 10:00:37 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6897910138399268 on epoch=624
06/23/2022 10:00:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/23/2022 10:00:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/23/2022 10:00:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/23/2022 10:00:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/23/2022 10:00:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/23/2022 10:00:51 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.653913057987761 on epoch=637
06/23/2022 10:00:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/23/2022 10:00:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
06/23/2022 10:00:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/23/2022 10:01:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/23/2022 10:01:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/23/2022 10:01:04 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6393120393120393 on epoch=649
06/23/2022 10:01:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/23/2022 10:01:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/23/2022 10:01:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
06/23/2022 10:01:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/23/2022 10:01:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/23/2022 10:01:17 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7010065237651444 on epoch=662
06/23/2022 10:01:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
06/23/2022 10:01:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/23/2022 10:01:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/23/2022 10:01:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
06/23/2022 10:01:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/23/2022 10:01:30 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.660496423196652 on epoch=674
06/23/2022 10:01:32 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/23/2022 10:01:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/23/2022 10:01:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
06/23/2022 10:01:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/23/2022 10:01:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
06/23/2022 10:01:43 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7162312312312312 on epoch=687
06/23/2022 10:01:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7091264651748523 -> 0.7162312312312312 on epoch=687, global_step=2750
06/23/2022 10:01:46 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/23/2022 10:01:48 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/23/2022 10:01:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
06/23/2022 10:01:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=697
06/23/2022 10:01:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/23/2022 10:01:56 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.653913057987761 on epoch=699
06/23/2022 10:01:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/23/2022 10:02:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/23/2022 10:02:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
06/23/2022 10:02:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/23/2022 10:02:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/23/2022 10:02:09 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6771630898633185 on epoch=712
06/23/2022 10:02:12 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/23/2022 10:02:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/23/2022 10:02:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/23/2022 10:02:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
06/23/2022 10:02:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
06/23/2022 10:02:22 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.653913057987761 on epoch=724
06/23/2022 10:02:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/23/2022 10:02:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/23/2022 10:02:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
06/23/2022 10:02:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/23/2022 10:02:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
06/23/2022 10:02:36 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.668112714987715 on epoch=737
06/23/2022 10:02:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
06/23/2022 10:02:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/23/2022 10:02:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/23/2022 10:02:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/23/2022 10:02:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/23/2022 10:02:49 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7162312312312312 on epoch=749
06/23/2022 10:02:49 - INFO - __main__ - save last model!
06/23/2022 10:02:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 10:02:49 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 10:02:49 - INFO - __main__ - Printing 3 examples
06/23/2022 10:02:49 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 10:02:49 - INFO - __main__ - ['others']
06/23/2022 10:02:49 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 10:02:49 - INFO - __main__ - ['others']
06/23/2022 10:02:49 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 10:02:49 - INFO - __main__ - ['others']
06/23/2022 10:02:49 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:02:49 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:02:49 - INFO - __main__ - Printing 3 examples
06/23/2022 10:02:49 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/23/2022 10:02:49 - INFO - __main__ - ['sad']
06/23/2022 10:02:49 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/23/2022 10:02:49 - INFO - __main__ - ['sad']
06/23/2022 10:02:49 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/23/2022 10:02:49 - INFO - __main__ - ['sad']
06/23/2022 10:02:49 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:02:49 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:02:49 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 10:02:49 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:02:49 - INFO - __main__ - Printing 3 examples
06/23/2022 10:02:49 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/23/2022 10:02:49 - INFO - __main__ - ['sad']
06/23/2022 10:02:49 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/23/2022 10:02:49 - INFO - __main__ - ['sad']
06/23/2022 10:02:49 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/23/2022 10:02:49 - INFO - __main__ - ['sad']
06/23/2022 10:02:49 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:02:49 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:02:49 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 10:02:51 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:02:56 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 10:03:04 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 10:03:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 10:03:05 - INFO - __main__ - Starting training!
06/23/2022 10:04:14 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_21_0.4_8_predictions.txt
06/23/2022 10:04:14 - INFO - __main__ - Classification-F1 on test data: 0.2052
06/23/2022 10:04:14 - INFO - __main__ - prefix=emo_16_21, lr=0.4, bsz=8, dev_performance=0.7162312312312312, test_performance=0.20523762378869487
06/23/2022 10:04:14 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.3, bsz=8 ...
06/23/2022 10:04:15 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:04:15 - INFO - __main__ - Printing 3 examples
06/23/2022 10:04:15 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/23/2022 10:04:15 - INFO - __main__ - ['sad']
06/23/2022 10:04:15 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/23/2022 10:04:15 - INFO - __main__ - ['sad']
06/23/2022 10:04:15 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/23/2022 10:04:15 - INFO - __main__ - ['sad']
06/23/2022 10:04:15 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:04:15 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:04:15 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 10:04:15 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:04:15 - INFO - __main__ - Printing 3 examples
06/23/2022 10:04:15 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/23/2022 10:04:15 - INFO - __main__ - ['sad']
06/23/2022 10:04:15 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/23/2022 10:04:15 - INFO - __main__ - ['sad']
06/23/2022 10:04:15 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/23/2022 10:04:15 - INFO - __main__ - ['sad']
06/23/2022 10:04:15 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:04:15 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:04:15 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 10:04:31 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 10:04:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 10:04:31 - INFO - __main__ - Starting training!
06/23/2022 10:04:34 - INFO - __main__ - Step 10 Global step 10 Train loss 4.68 on epoch=2
06/23/2022 10:04:37 - INFO - __main__ - Step 20 Global step 20 Train loss 3.75 on epoch=4
06/23/2022 10:04:39 - INFO - __main__ - Step 30 Global step 30 Train loss 3.22 on epoch=7
06/23/2022 10:04:41 - INFO - __main__ - Step 40 Global step 40 Train loss 2.61 on epoch=9
06/23/2022 10:04:44 - INFO - __main__ - Step 50 Global step 50 Train loss 2.46 on epoch=12
06/23/2022 10:04:45 - INFO - __main__ - Global step 50 Train loss 3.34 Classification-F1 0.05634278002699055 on epoch=12
06/23/2022 10:04:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.05634278002699055 on epoch=12, global_step=50
06/23/2022 10:04:47 - INFO - __main__ - Step 60 Global step 60 Train loss 1.95 on epoch=14
06/23/2022 10:04:50 - INFO - __main__ - Step 70 Global step 70 Train loss 1.80 on epoch=17
06/23/2022 10:04:52 - INFO - __main__ - Step 80 Global step 80 Train loss 1.39 on epoch=19
06/23/2022 10:04:55 - INFO - __main__ - Step 90 Global step 90 Train loss 1.13 on epoch=22
06/23/2022 10:04:57 - INFO - __main__ - Step 100 Global step 100 Train loss 1.07 on epoch=24
06/23/2022 10:04:58 - INFO - __main__ - Global step 100 Train loss 1.47 Classification-F1 0.5230962020435705 on epoch=24
06/23/2022 10:04:58 - INFO - __main__ - Saving model with best Classification-F1: 0.05634278002699055 -> 0.5230962020435705 on epoch=24, global_step=100
06/23/2022 10:05:00 - INFO - __main__ - Step 110 Global step 110 Train loss 1.02 on epoch=27
06/23/2022 10:05:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.99 on epoch=29
06/23/2022 10:05:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.85 on epoch=32
06/23/2022 10:05:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.76 on epoch=34
06/23/2022 10:05:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.66 on epoch=37
06/23/2022 10:05:11 - INFO - __main__ - Global step 150 Train loss 0.86 Classification-F1 0.5551587301587302 on epoch=37
06/23/2022 10:05:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5230962020435705 -> 0.5551587301587302 on epoch=37, global_step=150
06/23/2022 10:05:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.71 on epoch=39
06/23/2022 10:05:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.64 on epoch=42
06/23/2022 10:05:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.61 on epoch=44
06/23/2022 10:05:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.62 on epoch=47
06/23/2022 10:05:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.64 on epoch=49
06/23/2022 10:05:24 - INFO - __main__ - Global step 200 Train loss 0.65 Classification-F1 0.5909232540811489 on epoch=49
06/23/2022 10:05:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5551587301587302 -> 0.5909232540811489 on epoch=49, global_step=200
06/23/2022 10:05:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.66 on epoch=52
06/23/2022 10:05:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=54
06/23/2022 10:05:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.63 on epoch=57
06/23/2022 10:05:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=59
06/23/2022 10:05:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.57 on epoch=62
06/23/2022 10:05:37 - INFO - __main__ - Global step 250 Train loss 0.57 Classification-F1 0.5892616155774051 on epoch=62
06/23/2022 10:05:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=64
06/23/2022 10:05:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.57 on epoch=67
06/23/2022 10:05:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=69
06/23/2022 10:05:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.58 on epoch=72
06/23/2022 10:05:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=74
06/23/2022 10:05:50 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.5459884766914896 on epoch=74
06/23/2022 10:05:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=77
06/23/2022 10:05:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=79
06/23/2022 10:05:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=82
06/23/2022 10:06:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=84
06/23/2022 10:06:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=87
06/23/2022 10:06:03 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.5905950305143853 on epoch=87
06/23/2022 10:06:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=89
06/23/2022 10:06:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=92
06/23/2022 10:06:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=94
06/23/2022 10:06:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=97
06/23/2022 10:06:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=99
06/23/2022 10:06:16 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.6048423423423424 on epoch=99
06/23/2022 10:06:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5909232540811489 -> 0.6048423423423424 on epoch=99, global_step=400
06/23/2022 10:06:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=102
06/23/2022 10:06:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=104
06/23/2022 10:06:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=107
06/23/2022 10:06:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=109
06/23/2022 10:06:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=112
06/23/2022 10:06:29 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.6149997924191473 on epoch=112
06/23/2022 10:06:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6048423423423424 -> 0.6149997924191473 on epoch=112, global_step=450
06/23/2022 10:06:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=114
06/23/2022 10:06:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=117
06/23/2022 10:06:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=119
06/23/2022 10:06:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.32 on epoch=122
06/23/2022 10:06:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=124
06/23/2022 10:06:42 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.6512975392432099 on epoch=124
06/23/2022 10:06:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6149997924191473 -> 0.6512975392432099 on epoch=124, global_step=500
06/23/2022 10:06:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=127
06/23/2022 10:06:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=129
06/23/2022 10:06:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.30 on epoch=132
06/23/2022 10:06:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.27 on epoch=134
06/23/2022 10:06:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=137
06/23/2022 10:06:55 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.5878205128205128 on epoch=137
06/23/2022 10:06:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.31 on epoch=139
06/23/2022 10:07:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.33 on epoch=142
06/23/2022 10:07:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=144
06/23/2022 10:07:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=147
06/23/2022 10:07:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=149
06/23/2022 10:07:08 - INFO - __main__ - Global step 600 Train loss 0.30 Classification-F1 0.6233585858585858 on epoch=149
06/23/2022 10:07:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=152
06/23/2022 10:07:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.30 on epoch=154
06/23/2022 10:07:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=157
06/23/2022 10:07:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=159
06/23/2022 10:07:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=162
06/23/2022 10:07:21 - INFO - __main__ - Global step 650 Train loss 0.29 Classification-F1 0.6233585858585858 on epoch=162
06/23/2022 10:07:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=164
06/23/2022 10:07:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.33 on epoch=167
06/23/2022 10:07:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=169
06/23/2022 10:07:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=172
06/23/2022 10:07:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=174
06/23/2022 10:07:34 - INFO - __main__ - Global step 700 Train loss 0.27 Classification-F1 0.6000804375804375 on epoch=174
06/23/2022 10:07:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.35 on epoch=177
06/23/2022 10:07:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=179
06/23/2022 10:07:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=182
06/23/2022 10:07:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=184
06/23/2022 10:07:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=187
06/23/2022 10:07:47 - INFO - __main__ - Global step 750 Train loss 0.24 Classification-F1 0.6233585858585858 on epoch=187
06/23/2022 10:07:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=189
06/23/2022 10:07:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=192
06/23/2022 10:07:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=194
06/23/2022 10:07:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=197
06/23/2022 10:07:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=199
06/23/2022 10:08:00 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.6000804375804375 on epoch=199
06/23/2022 10:08:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=202
06/23/2022 10:08:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=204
06/23/2022 10:08:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=207
06/23/2022 10:08:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=209
06/23/2022 10:08:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=212
06/23/2022 10:08:13 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.6659024455077087 on epoch=212
06/23/2022 10:08:13 - INFO - __main__ - Saving model with best Classification-F1: 0.6512975392432099 -> 0.6659024455077087 on epoch=212, global_step=850
06/23/2022 10:08:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=214
06/23/2022 10:08:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=217
06/23/2022 10:08:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=219
06/23/2022 10:08:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=222
06/23/2022 10:08:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=224
06/23/2022 10:08:26 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.6668372918372919 on epoch=224
06/23/2022 10:08:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6659024455077087 -> 0.6668372918372919 on epoch=224, global_step=900
06/23/2022 10:08:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=227
06/23/2022 10:08:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=229
06/23/2022 10:08:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=232
06/23/2022 10:08:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=234
06/23/2022 10:08:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=237
06/23/2022 10:08:39 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.6177846577227383 on epoch=237
06/23/2022 10:08:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=239
06/23/2022 10:08:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=242
06/23/2022 10:08:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=244
06/23/2022 10:08:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=247
06/23/2022 10:08:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=249
06/23/2022 10:08:52 - INFO - __main__ - Global step 1000 Train loss 0.16 Classification-F1 0.6423423423423423 on epoch=249
06/23/2022 10:08:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=252
06/23/2022 10:08:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=254
06/23/2022 10:08:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=257
06/23/2022 10:09:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=259
06/23/2022 10:09:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.15 on epoch=262
06/23/2022 10:09:05 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.6668372918372919 on epoch=262
06/23/2022 10:09:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=264
06/23/2022 10:09:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=267
06/23/2022 10:09:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=269
06/23/2022 10:09:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=272
06/23/2022 10:09:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=274
06/23/2022 10:09:18 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.6423423423423423 on epoch=274
06/23/2022 10:09:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.16 on epoch=277
06/23/2022 10:09:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=279
06/23/2022 10:09:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=282
06/23/2022 10:09:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=284
06/23/2022 10:09:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=287
06/23/2022 10:09:31 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.6887492515768149 on epoch=287
06/23/2022 10:09:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6668372918372919 -> 0.6887492515768149 on epoch=287, global_step=1150
06/23/2022 10:09:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=289
06/23/2022 10:09:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=292
06/23/2022 10:09:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
06/23/2022 10:09:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=297
06/23/2022 10:09:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=299
06/23/2022 10:09:44 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.7027914764824174 on epoch=299
06/23/2022 10:09:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6887492515768149 -> 0.7027914764824174 on epoch=299, global_step=1200
06/23/2022 10:09:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.09 on epoch=302
06/23/2022 10:09:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=304
06/23/2022 10:09:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=307
06/23/2022 10:09:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=309
06/23/2022 10:09:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=312
06/23/2022 10:09:58 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.653913057987761 on epoch=312
06/23/2022 10:10:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=314
06/23/2022 10:10:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=317
06/23/2022 10:10:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=319
06/23/2022 10:10:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=322
06/23/2022 10:10:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
06/23/2022 10:10:11 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.653913057987761 on epoch=324
06/23/2022 10:10:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=327
06/23/2022 10:10:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
06/23/2022 10:10:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
06/23/2022 10:10:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
06/23/2022 10:10:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
06/23/2022 10:10:24 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.6820588210390842 on epoch=337
06/23/2022 10:10:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
06/23/2022 10:10:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
06/23/2022 10:10:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=344
06/23/2022 10:10:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
06/23/2022 10:10:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
06/23/2022 10:10:37 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6879984051036682 on epoch=349
06/23/2022 10:10:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
06/23/2022 10:10:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
06/23/2022 10:10:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
06/23/2022 10:10:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=359
06/23/2022 10:10:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
06/23/2022 10:10:50 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.6583881578947369 on epoch=362
06/23/2022 10:10:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
06/23/2022 10:10:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
06/23/2022 10:10:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
06/23/2022 10:11:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
06/23/2022 10:11:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
06/23/2022 10:11:03 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.6808821808821809 on epoch=374
06/23/2022 10:11:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=377
06/23/2022 10:11:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
06/23/2022 10:11:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=382
06/23/2022 10:11:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
06/23/2022 10:11:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
06/23/2022 10:11:16 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.7181436071238703 on epoch=387
06/23/2022 10:11:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7027914764824174 -> 0.7181436071238703 on epoch=387, global_step=1550
06/23/2022 10:11:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
06/23/2022 10:11:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=392
06/23/2022 10:11:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
06/23/2022 10:11:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
06/23/2022 10:11:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/23/2022 10:11:30 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.7034845946387709 on epoch=399
06/23/2022 10:11:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=402
06/23/2022 10:11:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
06/23/2022 10:11:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
06/23/2022 10:11:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
06/23/2022 10:11:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=412
06/23/2022 10:11:43 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.6718013686763686 on epoch=412
06/23/2022 10:11:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
06/23/2022 10:11:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
06/23/2022 10:11:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/23/2022 10:11:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=422
06/23/2022 10:11:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
06/23/2022 10:11:56 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.7181436071238703 on epoch=424
06/23/2022 10:11:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
06/23/2022 10:12:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
06/23/2022 10:12:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
06/23/2022 10:12:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
06/23/2022 10:12:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=437
06/23/2022 10:12:09 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.7175438596491228 on epoch=437
06/23/2022 10:12:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
06/23/2022 10:12:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
06/23/2022 10:12:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
06/23/2022 10:12:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
06/23/2022 10:12:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
06/23/2022 10:12:22 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.7175438596491228 on epoch=449
06/23/2022 10:12:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
06/23/2022 10:12:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
06/23/2022 10:12:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
06/23/2022 10:12:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
06/23/2022 10:12:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=462
06/23/2022 10:12:35 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.7027914764824174 on epoch=462
06/23/2022 10:12:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
06/23/2022 10:12:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
06/23/2022 10:12:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
06/23/2022 10:12:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
06/23/2022 10:12:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
06/23/2022 10:12:49 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.6602172304209656 on epoch=474
06/23/2022 10:12:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
06/23/2022 10:12:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
06/23/2022 10:12:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/23/2022 10:12:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/23/2022 10:13:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
06/23/2022 10:13:02 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7048782838348794 on epoch=487
06/23/2022 10:13:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
06/23/2022 10:13:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
06/23/2022 10:13:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
06/23/2022 10:13:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
06/23/2022 10:13:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
06/23/2022 10:13:15 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7034845946387709 on epoch=499
06/23/2022 10:13:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=502
06/23/2022 10:13:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
06/23/2022 10:13:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
06/23/2022 10:13:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/23/2022 10:13:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/23/2022 10:13:29 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.7048782838348794 on epoch=512
06/23/2022 10:13:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/23/2022 10:13:33 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/23/2022 10:13:36 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
06/23/2022 10:13:38 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
06/23/2022 10:13:41 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
06/23/2022 10:13:42 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.671546052631579 on epoch=524
06/23/2022 10:13:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
06/23/2022 10:13:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/23/2022 10:13:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
06/23/2022 10:13:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
06/23/2022 10:13:54 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/23/2022 10:13:55 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6583881578947369 on epoch=537
06/23/2022 10:13:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
06/23/2022 10:14:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
06/23/2022 10:14:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
06/23/2022 10:14:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
06/23/2022 10:14:07 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
06/23/2022 10:14:08 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6566735466116271 on epoch=549
06/23/2022 10:14:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
06/23/2022 10:14:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
06/23/2022 10:14:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/23/2022 10:14:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/23/2022 10:14:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
06/23/2022 10:14:22 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6326121794871795 on epoch=562
06/23/2022 10:14:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
06/23/2022 10:14:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
06/23/2022 10:14:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
06/23/2022 10:14:32 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/23/2022 10:14:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
06/23/2022 10:14:35 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.725043657788223 on epoch=574
06/23/2022 10:14:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7181436071238703 -> 0.725043657788223 on epoch=574, global_step=2300
06/23/2022 10:14:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/23/2022 10:14:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/23/2022 10:14:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/23/2022 10:14:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
06/23/2022 10:14:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/23/2022 10:14:48 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6602172304209656 on epoch=587
06/23/2022 10:14:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
06/23/2022 10:14:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/23/2022 10:14:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/23/2022 10:14:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
06/23/2022 10:15:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/23/2022 10:15:01 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7043535012285012 on epoch=599
06/23/2022 10:15:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
06/23/2022 10:15:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/23/2022 10:15:09 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/23/2022 10:15:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/23/2022 10:15:14 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
06/23/2022 10:15:15 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.696104242979243 on epoch=612
06/23/2022 10:15:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/23/2022 10:15:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
06/23/2022 10:15:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
06/23/2022 10:15:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
06/23/2022 10:15:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/23/2022 10:15:28 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6803442097559744 on epoch=624
06/23/2022 10:15:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
06/23/2022 10:15:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/23/2022 10:15:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/23/2022 10:15:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
06/23/2022 10:15:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/23/2022 10:15:41 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7043535012285012 on epoch=637
06/23/2022 10:15:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
06/23/2022 10:15:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
06/23/2022 10:15:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/23/2022 10:15:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/23/2022 10:15:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/23/2022 10:15:54 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6808821808821809 on epoch=649
06/23/2022 10:15:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/23/2022 10:15:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/23/2022 10:16:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/23/2022 10:16:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/23/2022 10:16:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
06/23/2022 10:16:07 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7043535012285012 on epoch=662
06/23/2022 10:16:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/23/2022 10:16:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/23/2022 10:16:15 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/23/2022 10:16:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/23/2022 10:16:19 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
06/23/2022 10:16:20 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6572115177378335 on epoch=674
06/23/2022 10:16:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/23/2022 10:16:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
06/23/2022 10:16:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
06/23/2022 10:16:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/23/2022 10:16:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
06/23/2022 10:16:34 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7174023892773893 on epoch=687
06/23/2022 10:16:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=689
06/23/2022 10:16:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/23/2022 10:16:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/23/2022 10:16:43 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=697
06/23/2022 10:16:46 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/23/2022 10:16:47 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6572115177378335 on epoch=699
06/23/2022 10:16:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
06/23/2022 10:16:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/23/2022 10:16:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=707
06/23/2022 10:16:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/23/2022 10:16:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
06/23/2022 10:17:00 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6572115177378335 on epoch=712
06/23/2022 10:17:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/23/2022 10:17:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/23/2022 10:17:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/23/2022 10:17:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
06/23/2022 10:17:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/23/2022 10:17:13 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6572115177378335 on epoch=724
06/23/2022 10:17:15 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/23/2022 10:17:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=729
06/23/2022 10:17:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/23/2022 10:17:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/23/2022 10:17:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
06/23/2022 10:17:26 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6824144398157556 on epoch=737
06/23/2022 10:17:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/23/2022 10:17:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
06/23/2022 10:17:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=744
06/23/2022 10:17:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/23/2022 10:17:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/23/2022 10:17:39 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.6572115177378335 on epoch=749
06/23/2022 10:17:39 - INFO - __main__ - save last model!
06/23/2022 10:17:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 10:17:39 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 10:17:39 - INFO - __main__ - Printing 3 examples
06/23/2022 10:17:39 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 10:17:39 - INFO - __main__ - ['others']
06/23/2022 10:17:39 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 10:17:39 - INFO - __main__ - ['others']
06/23/2022 10:17:39 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 10:17:39 - INFO - __main__ - ['others']
06/23/2022 10:17:39 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:17:39 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:17:39 - INFO - __main__ - Printing 3 examples
06/23/2022 10:17:39 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/23/2022 10:17:39 - INFO - __main__ - ['sad']
06/23/2022 10:17:39 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/23/2022 10:17:39 - INFO - __main__ - ['sad']
06/23/2022 10:17:39 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/23/2022 10:17:39 - INFO - __main__ - ['sad']
06/23/2022 10:17:39 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:17:39 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:17:40 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 10:17:40 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:17:40 - INFO - __main__ - Printing 3 examples
06/23/2022 10:17:40 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/23/2022 10:17:40 - INFO - __main__ - ['sad']
06/23/2022 10:17:40 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/23/2022 10:17:40 - INFO - __main__ - ['sad']
06/23/2022 10:17:40 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/23/2022 10:17:40 - INFO - __main__ - ['sad']
06/23/2022 10:17:40 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:17:40 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:17:40 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 10:17:41 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:17:47 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 10:17:55 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 10:17:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 10:17:56 - INFO - __main__ - Starting training!
06/23/2022 10:19:06 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_21_0.3_8_predictions.txt
06/23/2022 10:19:06 - INFO - __main__ - Classification-F1 on test data: 0.3134
06/23/2022 10:19:06 - INFO - __main__ - prefix=emo_16_21, lr=0.3, bsz=8, dev_performance=0.725043657788223, test_performance=0.31344093577014415
06/23/2022 10:19:06 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.2, bsz=8 ...
06/23/2022 10:19:07 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:19:07 - INFO - __main__ - Printing 3 examples
06/23/2022 10:19:07 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/23/2022 10:19:07 - INFO - __main__ - ['sad']
06/23/2022 10:19:07 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/23/2022 10:19:07 - INFO - __main__ - ['sad']
06/23/2022 10:19:07 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/23/2022 10:19:07 - INFO - __main__ - ['sad']
06/23/2022 10:19:07 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:19:07 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:19:07 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 10:19:07 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:19:07 - INFO - __main__ - Printing 3 examples
06/23/2022 10:19:07 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/23/2022 10:19:07 - INFO - __main__ - ['sad']
06/23/2022 10:19:07 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/23/2022 10:19:07 - INFO - __main__ - ['sad']
06/23/2022 10:19:07 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/23/2022 10:19:07 - INFO - __main__ - ['sad']
06/23/2022 10:19:07 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:19:07 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:19:07 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 10:19:23 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 10:19:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 10:19:23 - INFO - __main__ - Starting training!
06/23/2022 10:19:26 - INFO - __main__ - Step 10 Global step 10 Train loss 4.52 on epoch=2
06/23/2022 10:19:29 - INFO - __main__ - Step 20 Global step 20 Train loss 3.87 on epoch=4
06/23/2022 10:19:31 - INFO - __main__ - Step 30 Global step 30 Train loss 3.42 on epoch=7
06/23/2022 10:19:33 - INFO - __main__ - Step 40 Global step 40 Train loss 3.16 on epoch=9
06/23/2022 10:19:36 - INFO - __main__ - Step 50 Global step 50 Train loss 2.81 on epoch=12
06/23/2022 10:19:37 - INFO - __main__ - Global step 50 Train loss 3.56 Classification-F1 0.0 on epoch=12
06/23/2022 10:19:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
06/23/2022 10:19:39 - INFO - __main__ - Step 60 Global step 60 Train loss 2.54 on epoch=14
06/23/2022 10:19:42 - INFO - __main__ - Step 70 Global step 70 Train loss 2.40 on epoch=17
06/23/2022 10:19:44 - INFO - __main__ - Step 80 Global step 80 Train loss 2.01 on epoch=19
06/23/2022 10:19:47 - INFO - __main__ - Step 90 Global step 90 Train loss 1.83 on epoch=22
06/23/2022 10:19:49 - INFO - __main__ - Step 100 Global step 100 Train loss 1.55 on epoch=24
06/23/2022 10:19:50 - INFO - __main__ - Global step 100 Train loss 2.07 Classification-F1 0.2658856037579442 on epoch=24
06/23/2022 10:19:50 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.2658856037579442 on epoch=24, global_step=100
06/23/2022 10:19:53 - INFO - __main__ - Step 110 Global step 110 Train loss 1.50 on epoch=27
06/23/2022 10:19:55 - INFO - __main__ - Step 120 Global step 120 Train loss 1.31 on epoch=29
06/23/2022 10:19:57 - INFO - __main__ - Step 130 Global step 130 Train loss 1.24 on epoch=32
06/23/2022 10:20:00 - INFO - __main__ - Step 140 Global step 140 Train loss 1.21 on epoch=34
06/23/2022 10:20:02 - INFO - __main__ - Step 150 Global step 150 Train loss 1.09 on epoch=37
06/23/2022 10:20:03 - INFO - __main__ - Global step 150 Train loss 1.27 Classification-F1 0.5120614035087719 on epoch=37
06/23/2022 10:20:03 - INFO - __main__ - Saving model with best Classification-F1: 0.2658856037579442 -> 0.5120614035087719 on epoch=37, global_step=150
06/23/2022 10:20:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.87 on epoch=39
06/23/2022 10:20:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.83 on epoch=42
06/23/2022 10:20:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.79 on epoch=44
06/23/2022 10:20:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.72 on epoch=47
06/23/2022 10:20:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.78 on epoch=49
06/23/2022 10:20:16 - INFO - __main__ - Global step 200 Train loss 0.80 Classification-F1 0.5510626725797004 on epoch=49
06/23/2022 10:20:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5120614035087719 -> 0.5510626725797004 on epoch=49, global_step=200
06/23/2022 10:20:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.75 on epoch=52
06/23/2022 10:20:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.77 on epoch=54
06/23/2022 10:20:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.71 on epoch=57
06/23/2022 10:20:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.68 on epoch=59
06/23/2022 10:20:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.64 on epoch=62
06/23/2022 10:20:29 - INFO - __main__ - Global step 250 Train loss 0.71 Classification-F1 0.5797314794747401 on epoch=62
06/23/2022 10:20:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5510626725797004 -> 0.5797314794747401 on epoch=62, global_step=250
06/23/2022 10:20:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.65 on epoch=64
06/23/2022 10:20:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.72 on epoch=67
06/23/2022 10:20:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.60 on epoch=69
06/23/2022 10:20:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.63 on epoch=72
06/23/2022 10:20:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.63 on epoch=74
06/23/2022 10:20:42 - INFO - __main__ - Global step 300 Train loss 0.65 Classification-F1 0.5881673881673881 on epoch=74
06/23/2022 10:20:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5797314794747401 -> 0.5881673881673881 on epoch=74, global_step=300
06/23/2022 10:20:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.60 on epoch=77
06/23/2022 10:20:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.62 on epoch=79
06/23/2022 10:20:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.76 on epoch=82
06/23/2022 10:20:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=84
06/23/2022 10:20:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.55 on epoch=87
06/23/2022 10:20:55 - INFO - __main__ - Global step 350 Train loss 0.60 Classification-F1 0.5892616155774051 on epoch=87
06/23/2022 10:20:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5881673881673881 -> 0.5892616155774051 on epoch=87, global_step=350
06/23/2022 10:20:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=89
06/23/2022 10:21:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.54 on epoch=92
06/23/2022 10:21:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=94
06/23/2022 10:21:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=97
06/23/2022 10:21:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=99
06/23/2022 10:21:08 - INFO - __main__ - Global step 400 Train loss 0.51 Classification-F1 0.5599206349206349 on epoch=99
06/23/2022 10:21:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=102
06/23/2022 10:21:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=104
06/23/2022 10:21:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.52 on epoch=107
06/23/2022 10:21:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=109
06/23/2022 10:21:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=112
06/23/2022 10:21:21 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.6320124320124321 on epoch=112
06/23/2022 10:21:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5892616155774051 -> 0.6320124320124321 on epoch=112, global_step=450
06/23/2022 10:21:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=114
06/23/2022 10:21:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=117
06/23/2022 10:21:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=119
06/23/2022 10:21:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=122
06/23/2022 10:21:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=124
06/23/2022 10:21:34 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.61869800422432 on epoch=124
06/23/2022 10:21:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.42 on epoch=127
06/23/2022 10:21:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=129
06/23/2022 10:21:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=132
06/23/2022 10:21:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=134
06/23/2022 10:21:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=137
06/23/2022 10:21:47 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.61869800422432 on epoch=137
06/23/2022 10:21:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=139
06/23/2022 10:21:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=142
06/23/2022 10:21:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=144
06/23/2022 10:21:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=147
06/23/2022 10:21:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=149
06/23/2022 10:22:00 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.6051844907108066 on epoch=149
06/23/2022 10:22:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=152
06/23/2022 10:22:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.35 on epoch=154
06/23/2022 10:22:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.30 on epoch=157
06/23/2022 10:22:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=159
06/23/2022 10:22:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=162
06/23/2022 10:22:13 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.6299270904534062 on epoch=162
06/23/2022 10:22:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=164
06/23/2022 10:22:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=167
06/23/2022 10:22:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=169
06/23/2022 10:22:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=172
06/23/2022 10:22:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=174
06/23/2022 10:22:26 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.6299270904534062 on epoch=174
06/23/2022 10:22:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=177
06/23/2022 10:22:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=179
06/23/2022 10:22:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=182
06/23/2022 10:22:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.27 on epoch=184
06/23/2022 10:22:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=187
06/23/2022 10:22:39 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.6299270904534062 on epoch=187
06/23/2022 10:22:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=189
06/23/2022 10:22:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.26 on epoch=192
06/23/2022 10:22:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=194
06/23/2022 10:22:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=197
06/23/2022 10:22:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.31 on epoch=199
06/23/2022 10:22:52 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.6426242972295604 on epoch=199
06/23/2022 10:22:52 - INFO - __main__ - Saving model with best Classification-F1: 0.6320124320124321 -> 0.6426242972295604 on epoch=199, global_step=800
06/23/2022 10:22:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=202
06/23/2022 10:22:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=204
06/23/2022 10:22:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=207
06/23/2022 10:23:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.30 on epoch=209
06/23/2022 10:23:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=212
06/23/2022 10:23:05 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.6435591435591436 on epoch=212
06/23/2022 10:23:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6426242972295604 -> 0.6435591435591436 on epoch=212, global_step=850
06/23/2022 10:23:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.29 on epoch=214
06/23/2022 10:23:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=217
06/23/2022 10:23:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=219
06/23/2022 10:23:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.30 on epoch=222
06/23/2022 10:23:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=224
06/23/2022 10:23:18 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.6294664024927183 on epoch=224
06/23/2022 10:23:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.30 on epoch=227
06/23/2022 10:23:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=229
06/23/2022 10:23:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.28 on epoch=232
06/23/2022 10:23:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=234
06/23/2022 10:23:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=237
06/23/2022 10:23:31 - INFO - __main__ - Global step 950 Train loss 0.27 Classification-F1 0.612781954887218 on epoch=237
06/23/2022 10:23:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=239
06/23/2022 10:23:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=242
06/23/2022 10:23:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=244
06/23/2022 10:23:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=247
06/23/2022 10:23:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=249
06/23/2022 10:23:44 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.6426242972295604 on epoch=249
06/23/2022 10:23:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=252
06/23/2022 10:23:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=254
06/23/2022 10:23:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=257
06/23/2022 10:23:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=259
06/23/2022 10:23:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=262
06/23/2022 10:23:57 - INFO - __main__ - Global step 1050 Train loss 0.26 Classification-F1 0.6569738740791373 on epoch=262
06/23/2022 10:23:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6435591435591436 -> 0.6569738740791373 on epoch=262, global_step=1050
06/23/2022 10:24:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=264
06/23/2022 10:24:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=267
06/23/2022 10:24:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.26 on epoch=269
06/23/2022 10:24:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=272
06/23/2022 10:24:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=274
06/23/2022 10:24:10 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.6426242972295604 on epoch=274
06/23/2022 10:24:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=277
06/23/2022 10:24:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=279
06/23/2022 10:24:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=282
06/23/2022 10:24:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.25 on epoch=284
06/23/2022 10:24:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=287
06/23/2022 10:24:23 - INFO - __main__ - Global step 1150 Train loss 0.23 Classification-F1 0.6509419204665384 on epoch=287
06/23/2022 10:24:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=289
06/23/2022 10:24:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=292
06/23/2022 10:24:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.22 on epoch=294
06/23/2022 10:24:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=297
06/23/2022 10:24:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=299
06/23/2022 10:24:36 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.6509419204665384 on epoch=299
06/23/2022 10:24:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=302
06/23/2022 10:24:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=304
06/23/2022 10:24:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=307
06/23/2022 10:24:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=309
06/23/2022 10:24:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=312
06/23/2022 10:24:49 - INFO - __main__ - Global step 1250 Train loss 0.19 Classification-F1 0.6727190023122701 on epoch=312
06/23/2022 10:24:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6569738740791373 -> 0.6727190023122701 on epoch=312, global_step=1250
06/23/2022 10:24:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=314
06/23/2022 10:24:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=317
06/23/2022 10:24:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=319
06/23/2022 10:24:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=322
06/23/2022 10:25:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=324
06/23/2022 10:25:03 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.7019769129743554 on epoch=324
06/23/2022 10:25:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6727190023122701 -> 0.7019769129743554 on epoch=324, global_step=1300
06/23/2022 10:25:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=327
06/23/2022 10:25:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=329
06/23/2022 10:25:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=332
06/23/2022 10:25:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=334
06/23/2022 10:25:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.10 on epoch=337
06/23/2022 10:25:16 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.667070952724603 on epoch=337
06/23/2022 10:25:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=339
06/23/2022 10:25:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=342
06/23/2022 10:25:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=344
06/23/2022 10:25:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=347
06/23/2022 10:25:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=349
06/23/2022 10:25:29 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.6876795273534404 on epoch=349
06/23/2022 10:25:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=352
06/23/2022 10:25:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=354
06/23/2022 10:25:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=357
06/23/2022 10:25:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=359
06/23/2022 10:25:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=362
06/23/2022 10:25:42 - INFO - __main__ - Global step 1450 Train loss 0.18 Classification-F1 0.6876795273534404 on epoch=362
06/23/2022 10:25:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=364
06/23/2022 10:25:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.12 on epoch=367
06/23/2022 10:25:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=369
06/23/2022 10:25:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=372
06/23/2022 10:25:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=374
06/23/2022 10:25:55 - INFO - __main__ - Global step 1500 Train loss 0.14 Classification-F1 0.701907146313657 on epoch=374
06/23/2022 10:25:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=377
06/23/2022 10:25:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=379
06/23/2022 10:26:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=382
06/23/2022 10:26:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.11 on epoch=384
06/23/2022 10:26:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.15 on epoch=387
06/23/2022 10:26:08 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.6880222401961533 on epoch=387
06/23/2022 10:26:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=389
06/23/2022 10:26:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=392
06/23/2022 10:26:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=394
06/23/2022 10:26:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=397
06/23/2022 10:26:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=399
06/23/2022 10:26:21 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.6888480345703347 on epoch=399
06/23/2022 10:26:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=402
06/23/2022 10:26:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=404
06/23/2022 10:26:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=407
06/23/2022 10:26:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=409
06/23/2022 10:26:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=412
06/23/2022 10:26:34 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.6755913568399727 on epoch=412
06/23/2022 10:26:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=414
06/23/2022 10:26:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=417
06/23/2022 10:26:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=419
06/23/2022 10:26:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=422
06/23/2022 10:26:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=424
06/23/2022 10:26:47 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.667070952724603 on epoch=424
06/23/2022 10:26:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=427
06/23/2022 10:26:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.10 on epoch=429
06/23/2022 10:26:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=432
06/23/2022 10:26:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=434
06/23/2022 10:26:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=437
06/23/2022 10:27:00 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.7033045273534404 on epoch=437
06/23/2022 10:27:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7019769129743554 -> 0.7033045273534404 on epoch=437, global_step=1750
06/23/2022 10:27:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=439
06/23/2022 10:27:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.09 on epoch=442
06/23/2022 10:27:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=444
06/23/2022 10:27:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
06/23/2022 10:27:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=449
06/23/2022 10:27:13 - INFO - __main__ - Global step 1800 Train loss 0.09 Classification-F1 0.6816197316197316 on epoch=449
06/23/2022 10:27:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=452
06/23/2022 10:27:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=454
06/23/2022 10:27:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
06/23/2022 10:27:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=459
06/23/2022 10:27:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=462
06/23/2022 10:27:26 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.7033045273534404 on epoch=462
06/23/2022 10:27:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
06/23/2022 10:27:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=467
06/23/2022 10:27:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=469
06/23/2022 10:27:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=472
06/23/2022 10:27:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=474
06/23/2022 10:27:39 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.6897910138399268 on epoch=474
06/23/2022 10:27:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=477
06/23/2022 10:27:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=479
06/23/2022 10:27:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.18 on epoch=482
06/23/2022 10:27:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=484
06/23/2022 10:27:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=487
06/23/2022 10:27:53 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.7166424893756129 on epoch=487
06/23/2022 10:27:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7033045273534404 -> 0.7166424893756129 on epoch=487, global_step=1950
06/23/2022 10:27:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
06/23/2022 10:27:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=492
06/23/2022 10:28:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=494
06/23/2022 10:28:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
06/23/2022 10:28:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
06/23/2022 10:28:06 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.7033045273534404 on epoch=499
06/23/2022 10:28:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.10 on epoch=502
06/23/2022 10:28:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=504
06/23/2022 10:28:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=507
06/23/2022 10:28:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
06/23/2022 10:28:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
06/23/2022 10:28:19 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.651917081328846 on epoch=512
06/23/2022 10:28:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=514
06/23/2022 10:28:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=517
06/23/2022 10:28:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=519
06/23/2022 10:28:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=522
06/23/2022 10:28:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=524
06/23/2022 10:28:32 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.668112714987715 on epoch=524
06/23/2022 10:28:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=527
06/23/2022 10:28:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=529
06/23/2022 10:28:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=532
06/23/2022 10:28:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=534
06/23/2022 10:28:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=537
06/23/2022 10:28:45 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.6746267018006148 on epoch=537
06/23/2022 10:28:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
06/23/2022 10:28:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=542
06/23/2022 10:28:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
06/23/2022 10:28:55 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=547
06/23/2022 10:28:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
06/23/2022 10:28:58 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.6472578569352763 on epoch=549
06/23/2022 10:29:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=552
06/23/2022 10:29:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
06/23/2022 10:29:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=557
06/23/2022 10:29:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=559
06/23/2022 10:29:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
06/23/2022 10:29:11 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.6897910138399268 on epoch=562
06/23/2022 10:29:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
06/23/2022 10:29:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
06/23/2022 10:29:19 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
06/23/2022 10:29:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=572
06/23/2022 10:29:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
06/23/2022 10:29:24 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.6897910138399268 on epoch=574
06/23/2022 10:29:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=577
06/23/2022 10:29:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=579
06/23/2022 10:29:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=582
06/23/2022 10:29:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=584
06/23/2022 10:29:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
06/23/2022 10:29:37 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.7033045273534404 on epoch=587
06/23/2022 10:29:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
06/23/2022 10:29:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
06/23/2022 10:29:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
06/23/2022 10:29:47 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
06/23/2022 10:29:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=599
06/23/2022 10:29:50 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.652948402948403 on epoch=599
06/23/2022 10:29:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
06/23/2022 10:29:55 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/23/2022 10:29:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/23/2022 10:30:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
06/23/2022 10:30:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=612
06/23/2022 10:30:04 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.6735953801810579 on epoch=612
06/23/2022 10:30:06 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=614
06/23/2022 10:30:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=617
06/23/2022 10:30:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=619
06/23/2022 10:30:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=622
06/23/2022 10:30:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=624
06/23/2022 10:30:17 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.6450914024927183 on epoch=624
06/23/2022 10:30:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.08 on epoch=627
06/23/2022 10:30:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
06/23/2022 10:30:24 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
06/23/2022 10:30:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
06/23/2022 10:30:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=637
06/23/2022 10:30:30 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.6309425524595804 on epoch=637
06/23/2022 10:30:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/23/2022 10:30:35 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
06/23/2022 10:30:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=644
06/23/2022 10:30:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
06/23/2022 10:30:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
06/23/2022 10:30:43 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.7033045273534404 on epoch=649
06/23/2022 10:30:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
06/23/2022 10:30:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/23/2022 10:30:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=657
06/23/2022 10:30:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=659
06/23/2022 10:30:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
06/23/2022 10:30:56 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.6831999849826674 on epoch=662
06/23/2022 10:30:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
06/23/2022 10:31:01 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=667
06/23/2022 10:31:03 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
06/23/2022 10:31:06 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
06/23/2022 10:31:08 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/23/2022 10:31:09 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.6450914024927183 on epoch=674
06/23/2022 10:31:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
06/23/2022 10:31:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=679
06/23/2022 10:31:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=682
06/23/2022 10:31:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
06/23/2022 10:31:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
06/23/2022 10:31:22 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.6472578569352763 on epoch=687
06/23/2022 10:31:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=689
06/23/2022 10:31:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
06/23/2022 10:31:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/23/2022 10:31:32 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
06/23/2022 10:31:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
06/23/2022 10:31:36 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.7033045273534404 on epoch=699
06/23/2022 10:31:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
06/23/2022 10:31:40 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=704
06/23/2022 10:31:43 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/23/2022 10:31:45 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/23/2022 10:31:48 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
06/23/2022 10:31:49 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7049770668283992 on epoch=712
06/23/2022 10:31:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
06/23/2022 10:31:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/23/2022 10:31:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/23/2022 10:31:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
06/23/2022 10:32:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
06/23/2022 10:32:02 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.6831999849826674 on epoch=724
06/23/2022 10:32:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=727
06/23/2022 10:32:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
06/23/2022 10:32:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=732
06/23/2022 10:32:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/23/2022 10:32:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
06/23/2022 10:32:15 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.7048782838348794 on epoch=737
06/23/2022 10:32:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=739
06/23/2022 10:32:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
06/23/2022 10:32:22 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=744
06/23/2022 10:32:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
06/23/2022 10:32:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=749
06/23/2022 10:32:28 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7034845946387709 on epoch=749
06/23/2022 10:32:28 - INFO - __main__ - save last model!
06/23/2022 10:32:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 10:32:28 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 10:32:28 - INFO - __main__ - Printing 3 examples
06/23/2022 10:32:28 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 10:32:28 - INFO - __main__ - ['others']
06/23/2022 10:32:28 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 10:32:28 - INFO - __main__ - ['others']
06/23/2022 10:32:28 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 10:32:28 - INFO - __main__ - ['others']
06/23/2022 10:32:28 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:32:28 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:32:28 - INFO - __main__ - Printing 3 examples
06/23/2022 10:32:28 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/23/2022 10:32:28 - INFO - __main__ - ['happy']
06/23/2022 10:32:28 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/23/2022 10:32:28 - INFO - __main__ - ['happy']
06/23/2022 10:32:28 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/23/2022 10:32:28 - INFO - __main__ - ['happy']
06/23/2022 10:32:28 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:32:28 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:32:28 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 10:32:29 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:32:29 - INFO - __main__ - Printing 3 examples
06/23/2022 10:32:29 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/23/2022 10:32:29 - INFO - __main__ - ['happy']
06/23/2022 10:32:29 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/23/2022 10:32:29 - INFO - __main__ - ['happy']
06/23/2022 10:32:29 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/23/2022 10:32:29 - INFO - __main__ - ['happy']
06/23/2022 10:32:29 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:32:29 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:32:29 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 10:32:30 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:32:36 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 10:32:44 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 10:32:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 10:32:45 - INFO - __main__ - Starting training!
06/23/2022 10:33:49 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_21_0.2_8_predictions.txt
06/23/2022 10:33:49 - INFO - __main__ - Classification-F1 on test data: 0.2264
06/23/2022 10:33:50 - INFO - __main__ - prefix=emo_16_21, lr=0.2, bsz=8, dev_performance=0.7166424893756129, test_performance=0.22642401630529044
06/23/2022 10:33:50 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.5, bsz=8 ...
06/23/2022 10:33:51 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:33:51 - INFO - __main__ - Printing 3 examples
06/23/2022 10:33:51 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/23/2022 10:33:51 - INFO - __main__ - ['happy']
06/23/2022 10:33:51 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/23/2022 10:33:51 - INFO - __main__ - ['happy']
06/23/2022 10:33:51 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/23/2022 10:33:51 - INFO - __main__ - ['happy']
06/23/2022 10:33:51 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:33:51 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:33:51 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 10:33:51 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:33:51 - INFO - __main__ - Printing 3 examples
06/23/2022 10:33:51 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/23/2022 10:33:51 - INFO - __main__ - ['happy']
06/23/2022 10:33:51 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/23/2022 10:33:51 - INFO - __main__ - ['happy']
06/23/2022 10:33:51 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/23/2022 10:33:51 - INFO - __main__ - ['happy']
06/23/2022 10:33:51 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:33:51 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:33:51 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 10:34:06 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 10:34:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 10:34:07 - INFO - __main__ - Starting training!
06/23/2022 10:34:10 - INFO - __main__ - Step 10 Global step 10 Train loss 3.97 on epoch=2
06/23/2022 10:34:12 - INFO - __main__ - Step 20 Global step 20 Train loss 3.25 on epoch=4
06/23/2022 10:34:15 - INFO - __main__ - Step 30 Global step 30 Train loss 2.25 on epoch=7
06/23/2022 10:34:17 - INFO - __main__ - Step 40 Global step 40 Train loss 1.86 on epoch=9
06/23/2022 10:34:20 - INFO - __main__ - Step 50 Global step 50 Train loss 1.56 on epoch=12
06/23/2022 10:34:21 - INFO - __main__ - Global step 50 Train loss 2.58 Classification-F1 0.4618872549019608 on epoch=12
06/23/2022 10:34:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4618872549019608 on epoch=12, global_step=50
06/23/2022 10:34:23 - INFO - __main__ - Step 60 Global step 60 Train loss 1.35 on epoch=14
06/23/2022 10:34:26 - INFO - __main__ - Step 70 Global step 70 Train loss 1.08 on epoch=17
06/23/2022 10:34:28 - INFO - __main__ - Step 80 Global step 80 Train loss 1.00 on epoch=19
06/23/2022 10:34:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.86 on epoch=22
06/23/2022 10:34:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.73 on epoch=24
06/23/2022 10:34:34 - INFO - __main__ - Global step 100 Train loss 1.00 Classification-F1 0.45707070707070707 on epoch=24
06/23/2022 10:34:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.69 on epoch=27
06/23/2022 10:34:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.74 on epoch=29
06/23/2022 10:34:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.68 on epoch=32
06/23/2022 10:34:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.67 on epoch=34
06/23/2022 10:34:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.70 on epoch=37
06/23/2022 10:34:47 - INFO - __main__ - Global step 150 Train loss 0.70 Classification-F1 0.5079185520361991 on epoch=37
06/23/2022 10:34:47 - INFO - __main__ - Saving model with best Classification-F1: 0.4618872549019608 -> 0.5079185520361991 on epoch=37, global_step=150
06/23/2022 10:34:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.56 on epoch=39
06/23/2022 10:34:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.63 on epoch=42
06/23/2022 10:34:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.58 on epoch=44
06/23/2022 10:34:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.64 on epoch=47
06/23/2022 10:35:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=49
06/23/2022 10:35:00 - INFO - __main__ - Global step 200 Train loss 0.59 Classification-F1 0.5583579699433359 on epoch=49
06/23/2022 10:35:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5079185520361991 -> 0.5583579699433359 on epoch=49, global_step=200
06/23/2022 10:35:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=52
06/23/2022 10:35:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=54
06/23/2022 10:35:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=57
06/23/2022 10:35:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=59
06/23/2022 10:35:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=62
06/23/2022 10:35:14 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.6943042787032556 on epoch=62
06/23/2022 10:35:14 - INFO - __main__ - Saving model with best Classification-F1: 0.5583579699433359 -> 0.6943042787032556 on epoch=62, global_step=250
06/23/2022 10:35:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=64
06/23/2022 10:35:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=67
06/23/2022 10:35:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=69
06/23/2022 10:35:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=72
06/23/2022 10:35:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=74
06/23/2022 10:35:27 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.7512885481635481 on epoch=74
06/23/2022 10:35:27 - INFO - __main__ - Saving model with best Classification-F1: 0.6943042787032556 -> 0.7512885481635481 on epoch=74, global_step=300
06/23/2022 10:35:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=77
06/23/2022 10:35:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=79
06/23/2022 10:35:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=82
06/23/2022 10:35:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=84
06/23/2022 10:35:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=87
06/23/2022 10:35:41 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.6411733161733162 on epoch=87
06/23/2022 10:35:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=89
06/23/2022 10:35:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=92
06/23/2022 10:35:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=94
06/23/2022 10:35:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=97
06/23/2022 10:35:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=99
06/23/2022 10:35:54 - INFO - __main__ - Global step 400 Train loss 0.31 Classification-F1 0.7654930936180936 on epoch=99
06/23/2022 10:35:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7512885481635481 -> 0.7654930936180936 on epoch=99, global_step=400
06/23/2022 10:35:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=102
06/23/2022 10:35:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=104
06/23/2022 10:36:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=107
06/23/2022 10:36:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=109
06/23/2022 10:36:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.17 on epoch=112
06/23/2022 10:36:07 - INFO - __main__ - Global step 450 Train loss 0.28 Classification-F1 0.7249908456805009 on epoch=112
06/23/2022 10:36:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=114
06/23/2022 10:36:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=117
06/23/2022 10:36:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.17 on epoch=119
06/23/2022 10:36:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=122
06/23/2022 10:36:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.18 on epoch=124
06/23/2022 10:36:21 - INFO - __main__ - Global step 500 Train loss 0.19 Classification-F1 0.7183431325811749 on epoch=124
06/23/2022 10:36:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=127
06/23/2022 10:36:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.17 on epoch=129
06/23/2022 10:36:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=132
06/23/2022 10:36:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.16 on epoch=134
06/23/2022 10:36:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.14 on epoch=137
06/23/2022 10:36:34 - INFO - __main__ - Global step 550 Train loss 0.17 Classification-F1 0.7492288961038961 on epoch=137
06/23/2022 10:36:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.13 on epoch=139
06/23/2022 10:36:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=142
06/23/2022 10:36:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.10 on epoch=144
06/23/2022 10:36:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.09 on epoch=147
06/23/2022 10:36:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=149
06/23/2022 10:36:47 - INFO - __main__ - Global step 600 Train loss 0.13 Classification-F1 0.7942760942760942 on epoch=149
06/23/2022 10:36:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7654930936180936 -> 0.7942760942760942 on epoch=149, global_step=600
06/23/2022 10:36:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=152
06/23/2022 10:36:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=154
06/23/2022 10:36:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.12 on epoch=157
06/23/2022 10:36:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=159
06/23/2022 10:37:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=162
06/23/2022 10:37:01 - INFO - __main__ - Global step 650 Train loss 0.13 Classification-F1 0.7941925381263616 on epoch=162
06/23/2022 10:37:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.10 on epoch=164
06/23/2022 10:37:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=167
06/23/2022 10:37:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=169
06/23/2022 10:37:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=172
06/23/2022 10:37:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.10 on epoch=174
06/23/2022 10:37:14 - INFO - __main__ - Global step 700 Train loss 0.10 Classification-F1 0.7640356569027315 on epoch=174
06/23/2022 10:37:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=177
06/23/2022 10:37:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=179
06/23/2022 10:37:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=182
06/23/2022 10:37:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=184
06/23/2022 10:37:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=187
06/23/2022 10:37:27 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.8086962057550293 on epoch=187
06/23/2022 10:37:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7942760942760942 -> 0.8086962057550293 on epoch=187, global_step=750
06/23/2022 10:37:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=189
06/23/2022 10:37:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=192
06/23/2022 10:37:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=194
06/23/2022 10:37:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=197
06/23/2022 10:37:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=199
06/23/2022 10:37:41 - INFO - __main__ - Global step 800 Train loss 0.08 Classification-F1 0.8053662024250261 on epoch=199
06/23/2022 10:37:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.13 on epoch=202
06/23/2022 10:37:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=204
06/23/2022 10:37:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=207
06/23/2022 10:37:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=209
06/23/2022 10:37:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=212
06/23/2022 10:37:54 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.8243897306397306 on epoch=212
06/23/2022 10:37:54 - INFO - __main__ - Saving model with best Classification-F1: 0.8086962057550293 -> 0.8243897306397306 on epoch=212, global_step=850
06/23/2022 10:37:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=214
06/23/2022 10:37:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=217
06/23/2022 10:38:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=219
06/23/2022 10:38:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=222
06/23/2022 10:38:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=224
06/23/2022 10:38:07 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.794890873015873 on epoch=224
06/23/2022 10:38:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=227
06/23/2022 10:38:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=229
06/23/2022 10:38:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=232
06/23/2022 10:38:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=234
06/23/2022 10:38:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=237
06/23/2022 10:38:21 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.8081033549783551 on epoch=237
06/23/2022 10:38:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=239
06/23/2022 10:38:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=242
06/23/2022 10:38:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=244
06/23/2022 10:38:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=247
06/23/2022 10:38:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=249
06/23/2022 10:38:34 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.8094426406926407 on epoch=249
06/23/2022 10:38:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=252
06/23/2022 10:38:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=254
06/23/2022 10:38:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=257
06/23/2022 10:38:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=259
06/23/2022 10:38:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=262
06/23/2022 10:38:47 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.8234020881079704 on epoch=262
06/23/2022 10:38:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
06/23/2022 10:38:52 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=267
06/23/2022 10:38:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=269
06/23/2022 10:38:57 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=272
06/23/2022 10:39:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
06/23/2022 10:39:01 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.8081033549783551 on epoch=274
06/23/2022 10:39:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
06/23/2022 10:39:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
06/23/2022 10:39:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=282
06/23/2022 10:39:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=284
06/23/2022 10:39:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
06/23/2022 10:39:14 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.8052584670231729 on epoch=287
06/23/2022 10:39:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
06/23/2022 10:39:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
06/23/2022 10:39:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
06/23/2022 10:39:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
06/23/2022 10:39:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
06/23/2022 10:39:27 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.8231227975060993 on epoch=299
06/23/2022 10:39:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=302
06/23/2022 10:39:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
06/23/2022 10:39:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
06/23/2022 10:39:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=309
06/23/2022 10:39:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
06/23/2022 10:39:41 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.8052584670231729 on epoch=312
06/23/2022 10:39:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
06/23/2022 10:39:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
06/23/2022 10:39:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
06/23/2022 10:39:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
06/23/2022 10:39:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
06/23/2022 10:39:54 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.8052584670231729 on epoch=324
06/23/2022 10:39:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
06/23/2022 10:39:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
06/23/2022 10:40:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
06/23/2022 10:40:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=334
06/23/2022 10:40:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
06/23/2022 10:40:07 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.8260464669738864 on epoch=337
06/23/2022 10:40:07 - INFO - __main__ - Saving model with best Classification-F1: 0.8243897306397306 -> 0.8260464669738864 on epoch=337, global_step=1350
06/23/2022 10:40:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=339
06/23/2022 10:40:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
06/23/2022 10:40:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
06/23/2022 10:40:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
06/23/2022 10:40:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
06/23/2022 10:40:20 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.8081033549783551 on epoch=349
06/23/2022 10:40:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=352
06/23/2022 10:40:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
06/23/2022 10:40:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
06/23/2022 10:40:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
06/23/2022 10:40:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
06/23/2022 10:40:34 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.8081033549783551 on epoch=362
06/23/2022 10:40:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
06/23/2022 10:40:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
06/23/2022 10:40:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
06/23/2022 10:40:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
06/23/2022 10:40:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
06/23/2022 10:40:47 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.8258158866995075 on epoch=374
06/23/2022 10:40:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=377
06/23/2022 10:40:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
06/23/2022 10:40:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/23/2022 10:40:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
06/23/2022 10:41:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
06/23/2022 10:41:00 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.8085768398268399 on epoch=387
06/23/2022 10:41:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
06/23/2022 10:41:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
06/23/2022 10:41:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
06/23/2022 10:41:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
06/23/2022 10:41:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/23/2022 10:41:14 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.841421568627451 on epoch=399
06/23/2022 10:41:14 - INFO - __main__ - Saving model with best Classification-F1: 0.8260464669738864 -> 0.841421568627451 on epoch=399, global_step=1600
06/23/2022 10:41:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/23/2022 10:41:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
06/23/2022 10:41:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
06/23/2022 10:41:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
06/23/2022 10:41:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
06/23/2022 10:41:27 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7944412084498292 on epoch=412
06/23/2022 10:41:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=414
06/23/2022 10:41:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
06/23/2022 10:41:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/23/2022 10:41:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
06/23/2022 10:41:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
06/23/2022 10:41:41 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.8237920168067228 on epoch=424
06/23/2022 10:41:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
06/23/2022 10:41:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
06/23/2022 10:41:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/23/2022 10:41:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
06/23/2022 10:41:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
06/23/2022 10:41:54 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.8237920168067228 on epoch=437
06/23/2022 10:41:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
06/23/2022 10:41:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
06/23/2022 10:42:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
06/23/2022 10:42:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
06/23/2022 10:42:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
06/23/2022 10:42:08 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.8237920168067228 on epoch=449
06/23/2022 10:42:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
06/23/2022 10:42:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
06/23/2022 10:42:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
06/23/2022 10:42:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
06/23/2022 10:42:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
06/23/2022 10:42:21 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7900432900432901 on epoch=462
06/23/2022 10:42:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
06/23/2022 10:42:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/23/2022 10:42:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/23/2022 10:42:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
06/23/2022 10:42:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
06/23/2022 10:42:34 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7928683385579938 on epoch=474
06/23/2022 10:42:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
06/23/2022 10:42:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
06/23/2022 10:42:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
06/23/2022 10:42:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/23/2022 10:42:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
06/23/2022 10:42:48 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.8084981388652134 on epoch=487
06/23/2022 10:42:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
06/23/2022 10:42:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/23/2022 10:42:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
06/23/2022 10:42:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
06/23/2022 10:43:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
06/23/2022 10:43:01 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.8081033549783551 on epoch=499
06/23/2022 10:43:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
06/23/2022 10:43:06 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
06/23/2022 10:43:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
06/23/2022 10:43:11 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
06/23/2022 10:43:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/23/2022 10:43:14 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.8096590909090909 on epoch=512
06/23/2022 10:43:17 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
06/23/2022 10:43:19 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
06/23/2022 10:43:22 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
06/23/2022 10:43:24 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
06/23/2022 10:43:27 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=524
06/23/2022 10:43:28 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.8085768398268399 on epoch=524
06/23/2022 10:43:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
06/23/2022 10:43:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/23/2022 10:43:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/23/2022 10:43:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
06/23/2022 10:43:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/23/2022 10:43:41 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.7952865347018573 on epoch=537
06/23/2022 10:43:44 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
06/23/2022 10:43:46 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
06/23/2022 10:43:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/23/2022 10:43:51 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
06/23/2022 10:43:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
06/23/2022 10:43:54 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.8085745073891626 on epoch=549
06/23/2022 10:43:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/23/2022 10:43:59 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
06/23/2022 10:44:02 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/23/2022 10:44:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
06/23/2022 10:44:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/23/2022 10:44:08 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.7801350195503421 on epoch=562
06/23/2022 10:44:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
06/23/2022 10:44:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
06/23/2022 10:44:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/23/2022 10:44:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
06/23/2022 10:44:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/23/2022 10:44:21 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.8085745073891626 on epoch=574
06/23/2022 10:44:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/23/2022 10:44:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=579
06/23/2022 10:44:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/23/2022 10:44:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
06/23/2022 10:44:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
06/23/2022 10:44:35 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.8085745073891626 on epoch=587
06/23/2022 10:44:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
06/23/2022 10:44:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/23/2022 10:44:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/23/2022 10:44:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
06/23/2022 10:44:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/23/2022 10:44:49 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.8085745073891626 on epoch=599
06/23/2022 10:44:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/23/2022 10:44:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
06/23/2022 10:44:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/23/2022 10:44:59 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/23/2022 10:45:01 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/23/2022 10:45:02 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7944240196078431 on epoch=612
06/23/2022 10:45:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=614
06/23/2022 10:45:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/23/2022 10:45:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
06/23/2022 10:45:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/23/2022 10:45:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/23/2022 10:45:16 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7944240196078431 on epoch=624
06/23/2022 10:45:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/23/2022 10:45:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/23/2022 10:45:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/23/2022 10:45:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/23/2022 10:45:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=637
06/23/2022 10:45:29 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.8085768398268399 on epoch=637
06/23/2022 10:45:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/23/2022 10:45:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/23/2022 10:45:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/23/2022 10:45:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/23/2022 10:45:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/23/2022 10:45:42 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.8085745073891626 on epoch=649
06/23/2022 10:45:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/23/2022 10:45:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/23/2022 10:45:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/23/2022 10:45:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/23/2022 10:45:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
06/23/2022 10:45:56 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.8085745073891626 on epoch=662
06/23/2022 10:45:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/23/2022 10:46:01 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/23/2022 10:46:03 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
06/23/2022 10:46:06 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
06/23/2022 10:46:08 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/23/2022 10:46:09 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.8085768398268399 on epoch=674
06/23/2022 10:46:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/23/2022 10:46:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/23/2022 10:46:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/23/2022 10:46:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/23/2022 10:46:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/23/2022 10:46:23 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7942887931034482 on epoch=687
06/23/2022 10:46:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/23/2022 10:46:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/23/2022 10:46:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/23/2022 10:46:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/23/2022 10:46:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/23/2022 10:46:37 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.8085768398268399 on epoch=699
06/23/2022 10:46:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/23/2022 10:46:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/23/2022 10:46:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/23/2022 10:46:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/23/2022 10:46:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
06/23/2022 10:46:50 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.8085768398268399 on epoch=712
06/23/2022 10:46:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/23/2022 10:46:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/23/2022 10:46:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/23/2022 10:47:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/23/2022 10:47:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/23/2022 10:47:03 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.8085745073891626 on epoch=724
06/23/2022 10:47:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/23/2022 10:47:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/23/2022 10:47:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
06/23/2022 10:47:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=734
06/23/2022 10:47:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/23/2022 10:47:17 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.8085745073891626 on epoch=737
06/23/2022 10:47:19 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/23/2022 10:47:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
06/23/2022 10:47:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/23/2022 10:47:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/23/2022 10:47:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
06/23/2022 10:47:30 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.8046656162464986 on epoch=749
06/23/2022 10:47:30 - INFO - __main__ - save last model!
06/23/2022 10:47:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 10:47:30 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 10:47:30 - INFO - __main__ - Printing 3 examples
06/23/2022 10:47:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 10:47:30 - INFO - __main__ - ['others']
06/23/2022 10:47:30 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 10:47:30 - INFO - __main__ - ['others']
06/23/2022 10:47:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 10:47:30 - INFO - __main__ - ['others']
06/23/2022 10:47:30 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:47:32 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:47:33 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:47:33 - INFO - __main__ - Printing 3 examples
06/23/2022 10:47:33 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/23/2022 10:47:33 - INFO - __main__ - ['happy']
06/23/2022 10:47:33 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/23/2022 10:47:33 - INFO - __main__ - ['happy']
06/23/2022 10:47:33 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/23/2022 10:47:33 - INFO - __main__ - ['happy']
06/23/2022 10:47:33 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:47:33 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:47:33 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 10:47:33 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:47:33 - INFO - __main__ - Printing 3 examples
06/23/2022 10:47:33 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/23/2022 10:47:33 - INFO - __main__ - ['happy']
06/23/2022 10:47:33 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/23/2022 10:47:33 - INFO - __main__ - ['happy']
06/23/2022 10:47:33 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/23/2022 10:47:33 - INFO - __main__ - ['happy']
06/23/2022 10:47:33 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:47:33 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:47:33 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 10:47:38 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 10:47:49 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 10:47:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 10:47:49 - INFO - __main__ - Starting training!
06/23/2022 10:48:58 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_42_0.5_8_predictions.txt
06/23/2022 10:48:58 - INFO - __main__ - Classification-F1 on test data: 0.2804
06/23/2022 10:48:58 - INFO - __main__ - prefix=emo_16_42, lr=0.5, bsz=8, dev_performance=0.841421568627451, test_performance=0.28036710557547045
06/23/2022 10:48:58 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.4, bsz=8 ...
06/23/2022 10:48:59 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:48:59 - INFO - __main__ - Printing 3 examples
06/23/2022 10:48:59 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/23/2022 10:48:59 - INFO - __main__ - ['happy']
06/23/2022 10:48:59 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/23/2022 10:48:59 - INFO - __main__ - ['happy']
06/23/2022 10:48:59 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/23/2022 10:48:59 - INFO - __main__ - ['happy']
06/23/2022 10:48:59 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:48:59 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:48:59 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 10:48:59 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 10:48:59 - INFO - __main__ - Printing 3 examples
06/23/2022 10:48:59 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/23/2022 10:48:59 - INFO - __main__ - ['happy']
06/23/2022 10:48:59 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/23/2022 10:48:59 - INFO - __main__ - ['happy']
06/23/2022 10:48:59 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/23/2022 10:48:59 - INFO - __main__ - ['happy']
06/23/2022 10:48:59 - INFO - __main__ - Tokenizing Input ...
06/23/2022 10:48:59 - INFO - __main__ - Tokenizing Output ...
06/23/2022 10:48:59 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 10:49:15 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 10:49:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 10:49:16 - INFO - __main__ - Starting training!
06/23/2022 10:49:19 - INFO - __main__ - Step 10 Global step 10 Train loss 4.17 on epoch=2
06/23/2022 10:49:21 - INFO - __main__ - Step 20 Global step 20 Train loss 3.28 on epoch=4
06/23/2022 10:49:24 - INFO - __main__ - Step 30 Global step 30 Train loss 2.67 on epoch=7
06/23/2022 10:49:26 - INFO - __main__ - Step 40 Global step 40 Train loss 2.45 on epoch=9
06/23/2022 10:49:29 - INFO - __main__ - Step 50 Global step 50 Train loss 1.88 on epoch=12
06/23/2022 10:49:30 - INFO - __main__ - Global step 50 Train loss 2.89 Classification-F1 0.13213213213213212 on epoch=12
06/23/2022 10:49:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13213213213213212 on epoch=12, global_step=50
06/23/2022 10:49:32 - INFO - __main__ - Step 60 Global step 60 Train loss 1.56 on epoch=14
06/23/2022 10:49:35 - INFO - __main__ - Step 70 Global step 70 Train loss 1.24 on epoch=17
06/23/2022 10:49:37 - INFO - __main__ - Step 80 Global step 80 Train loss 0.95 on epoch=19
06/23/2022 10:49:40 - INFO - __main__ - Step 90 Global step 90 Train loss 1.01 on epoch=22
06/23/2022 10:49:42 - INFO - __main__ - Step 100 Global step 100 Train loss 0.91 on epoch=24
06/23/2022 10:49:43 - INFO - __main__ - Global step 100 Train loss 1.13 Classification-F1 0.4753899835796388 on epoch=24
06/23/2022 10:49:43 - INFO - __main__ - Saving model with best Classification-F1: 0.13213213213213212 -> 0.4753899835796388 on epoch=24, global_step=100
06/23/2022 10:49:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.77 on epoch=27
06/23/2022 10:49:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.81 on epoch=29
06/23/2022 10:49:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.66 on epoch=32
06/23/2022 10:49:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.84 on epoch=34
06/23/2022 10:49:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.67 on epoch=37
06/23/2022 10:49:56 - INFO - __main__ - Global step 150 Train loss 0.75 Classification-F1 0.5085348506401137 on epoch=37
06/23/2022 10:49:56 - INFO - __main__ - Saving model with best Classification-F1: 0.4753899835796388 -> 0.5085348506401137 on epoch=37, global_step=150
06/23/2022 10:49:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.69 on epoch=39
06/23/2022 10:50:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.68 on epoch=42
06/23/2022 10:50:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.63 on epoch=44
06/23/2022 10:50:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.57 on epoch=47
06/23/2022 10:50:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.68 on epoch=49
06/23/2022 10:50:09 - INFO - __main__ - Global step 200 Train loss 0.65 Classification-F1 0.5704080604995239 on epoch=49
06/23/2022 10:50:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5085348506401137 -> 0.5704080604995239 on epoch=49, global_step=200
06/23/2022 10:50:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.59 on epoch=52
06/23/2022 10:50:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.59 on epoch=54
06/23/2022 10:50:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=57
06/23/2022 10:50:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.62 on epoch=59
06/23/2022 10:50:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=62
06/23/2022 10:50:22 - INFO - __main__ - Global step 250 Train loss 0.56 Classification-F1 0.7001305282555282 on epoch=62
06/23/2022 10:50:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5704080604995239 -> 0.7001305282555282 on epoch=62, global_step=250
06/23/2022 10:50:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=64
06/23/2022 10:50:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=67
06/23/2022 10:50:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=69
06/23/2022 10:50:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=72
06/23/2022 10:50:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=74
06/23/2022 10:50:36 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.7412443693693693 on epoch=74
06/23/2022 10:50:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7001305282555282 -> 0.7412443693693693 on epoch=74, global_step=300
06/23/2022 10:50:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=77
06/23/2022 10:50:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=79
06/23/2022 10:50:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=82
06/23/2022 10:50:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=84
06/23/2022 10:50:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=87
06/23/2022 10:50:49 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.7412443693693693 on epoch=87
06/23/2022 10:50:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.31 on epoch=89
06/23/2022 10:50:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=92
06/23/2022 10:50:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=94
06/23/2022 10:50:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=97
06/23/2022 10:51:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.31 on epoch=99
06/23/2022 10:51:02 - INFO - __main__ - Global step 400 Train loss 0.33 Classification-F1 0.7738280235693599 on epoch=99
06/23/2022 10:51:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7412443693693693 -> 0.7738280235693599 on epoch=99, global_step=400
06/23/2022 10:51:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.31 on epoch=102
06/23/2022 10:51:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=104
06/23/2022 10:51:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.29 on epoch=107
06/23/2022 10:51:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=109
06/23/2022 10:51:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=112
06/23/2022 10:51:15 - INFO - __main__ - Global step 450 Train loss 0.31 Classification-F1 0.75461233729485 on epoch=112
06/23/2022 10:51:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.29 on epoch=114
06/23/2022 10:51:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=117
06/23/2022 10:51:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=119
06/23/2022 10:51:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=122
06/23/2022 10:51:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=124
06/23/2022 10:51:28 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.7628878878878879 on epoch=124
06/23/2022 10:51:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=127
06/23/2022 10:51:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.35 on epoch=129
06/23/2022 10:51:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=132
06/23/2022 10:51:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=134
06/23/2022 10:51:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=137
06/23/2022 10:51:41 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.791344537815126 on epoch=137
06/23/2022 10:51:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7738280235693599 -> 0.791344537815126 on epoch=137, global_step=550
06/23/2022 10:51:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=139
06/23/2022 10:51:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=142
06/23/2022 10:51:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=144
06/23/2022 10:51:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=147
06/23/2022 10:51:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.13 on epoch=149
06/23/2022 10:51:54 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.791344537815126 on epoch=149
06/23/2022 10:51:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=152
06/23/2022 10:51:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=154
06/23/2022 10:52:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=157
06/23/2022 10:52:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=159
06/23/2022 10:52:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=162
06/23/2022 10:52:07 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.8287878787878787 on epoch=162
06/23/2022 10:52:07 - INFO - __main__ - Saving model with best Classification-F1: 0.791344537815126 -> 0.8287878787878787 on epoch=162, global_step=650
06/23/2022 10:52:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=164
06/23/2022 10:52:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.14 on epoch=167
06/23/2022 10:52:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=169
06/23/2022 10:52:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=172
06/23/2022 10:52:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=174
06/23/2022 10:52:20 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.8279461279461279 on epoch=174
06/23/2022 10:52:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=177
06/23/2022 10:52:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=179
06/23/2022 10:52:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=182
06/23/2022 10:52:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=184
06/23/2022 10:52:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.18 on epoch=187
06/23/2022 10:52:33 - INFO - __main__ - Global step 750 Train loss 0.17 Classification-F1 0.8255895191379062 on epoch=187
06/23/2022 10:52:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=189
06/23/2022 10:52:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=192
06/23/2022 10:52:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.14 on epoch=194
06/23/2022 10:52:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=197
06/23/2022 10:52:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=199
06/23/2022 10:52:46 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.8284986016350114 on epoch=199
06/23/2022 10:52:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=202
06/23/2022 10:52:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=204
06/23/2022 10:52:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=207
06/23/2022 10:52:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=209
06/23/2022 10:52:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=212
06/23/2022 10:52:59 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.8086254703901763 on epoch=212
06/23/2022 10:53:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=214
06/23/2022 10:53:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=217
06/23/2022 10:53:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=219
06/23/2022 10:53:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=222
06/23/2022 10:53:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=224
06/23/2022 10:53:12 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.8121482683982684 on epoch=224
06/23/2022 10:53:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=227
06/23/2022 10:53:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=229
06/23/2022 10:53:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=232
06/23/2022 10:53:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=234
06/23/2022 10:53:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=237
06/23/2022 10:53:25 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.8200720847779672 on epoch=237
06/23/2022 10:53:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=239
06/23/2022 10:53:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=242
06/23/2022 10:53:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=244
06/23/2022 10:53:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=247
06/23/2022 10:53:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=249
06/23/2022 10:53:38 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.8115301724137931 on epoch=249
06/23/2022 10:53:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=252
06/23/2022 10:53:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=254
06/23/2022 10:53:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=257
06/23/2022 10:53:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
06/23/2022 10:53:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=262
06/23/2022 10:53:51 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.8200720847779672 on epoch=262
06/23/2022 10:53:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
06/23/2022 10:53:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=267
06/23/2022 10:53:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=269
06/23/2022 10:54:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=272
06/23/2022 10:54:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=274
06/23/2022 10:54:05 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.8203983516483516 on epoch=274
06/23/2022 10:54:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=277
06/23/2022 10:54:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
06/23/2022 10:54:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=282
06/23/2022 10:54:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
06/23/2022 10:54:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
06/23/2022 10:54:18 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.8053733900508094 on epoch=287
06/23/2022 10:54:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
06/23/2022 10:54:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
06/23/2022 10:54:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=294
06/23/2022 10:54:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
06/23/2022 10:54:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
06/23/2022 10:54:31 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.805366202425026 on epoch=299
06/23/2022 10:54:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
06/23/2022 10:54:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
06/23/2022 10:54:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=307
06/23/2022 10:54:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=309
06/23/2022 10:54:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=312
06/23/2022 10:54:44 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.8237133237133237 on epoch=312
06/23/2022 10:54:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=314
06/23/2022 10:54:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=317
06/23/2022 10:54:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
06/23/2022 10:54:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
06/23/2022 10:54:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
06/23/2022 10:54:57 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.8257202001035019 on epoch=324
06/23/2022 10:55:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
06/23/2022 10:55:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=329
06/23/2022 10:55:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
06/23/2022 10:55:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
06/23/2022 10:55:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
06/23/2022 10:55:11 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.8201357466063349 on epoch=337
06/23/2022 10:55:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
06/23/2022 10:55:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
06/23/2022 10:55:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
06/23/2022 10:55:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
06/23/2022 10:55:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=349
06/23/2022 10:55:24 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.8087647306397306 on epoch=349
06/23/2022 10:55:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
06/23/2022 10:55:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
06/23/2022 10:55:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=357
06/23/2022 10:55:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
06/23/2022 10:55:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
06/23/2022 10:55:37 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.8081033549783551 on epoch=362
06/23/2022 10:55:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
06/23/2022 10:55:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
06/23/2022 10:55:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
06/23/2022 10:55:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
06/23/2022 10:55:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
06/23/2022 10:55:50 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.8081033549783551 on epoch=374
06/23/2022 10:55:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
06/23/2022 10:55:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
06/23/2022 10:55:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
06/23/2022 10:56:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
06/23/2022 10:56:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
06/23/2022 10:56:04 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7907239819004526 on epoch=387
06/23/2022 10:56:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=389
06/23/2022 10:56:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=392
06/23/2022 10:56:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
06/23/2022 10:56:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
06/23/2022 10:56:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/23/2022 10:56:17 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.8084981388652134 on epoch=399
06/23/2022 10:56:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/23/2022 10:56:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
06/23/2022 10:56:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
06/23/2022 10:56:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/23/2022 10:56:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
06/23/2022 10:56:30 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.8415854978354979 on epoch=412
06/23/2022 10:56:30 - INFO - __main__ - Saving model with best Classification-F1: 0.8287878787878787 -> 0.8415854978354979 on epoch=412, global_step=1650
06/23/2022 10:56:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
06/23/2022 10:56:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
06/23/2022 10:56:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/23/2022 10:56:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
06/23/2022 10:56:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
06/23/2022 10:56:43 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.8237133237133237 on epoch=424
06/23/2022 10:56:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
06/23/2022 10:56:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
06/23/2022 10:56:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
06/23/2022 10:56:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
06/23/2022 10:56:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
06/23/2022 10:56:56 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.8203983516483516 on epoch=437
06/23/2022 10:56:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
06/23/2022 10:57:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
06/23/2022 10:57:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=444
06/23/2022 10:57:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
06/23/2022 10:57:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
06/23/2022 10:57:09 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7935574871058743 on epoch=449
06/23/2022 10:57:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
06/23/2022 10:57:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
06/23/2022 10:57:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/23/2022 10:57:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
06/23/2022 10:57:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
06/23/2022 10:57:22 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.7907239819004526 on epoch=462
06/23/2022 10:57:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/23/2022 10:57:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=467
06/23/2022 10:57:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/23/2022 10:57:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
06/23/2022 10:57:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
06/23/2022 10:57:35 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7788388082505731 on epoch=474
06/23/2022 10:57:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
06/23/2022 10:57:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
06/23/2022 10:57:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
06/23/2022 10:57:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
06/23/2022 10:57:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
06/23/2022 10:57:48 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.8237133237133237 on epoch=487
06/23/2022 10:57:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
06/23/2022 10:57:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/23/2022 10:57:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
06/23/2022 10:57:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/23/2022 10:58:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
06/23/2022 10:58:01 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7907239819004526 on epoch=499
06/23/2022 10:58:04 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
06/23/2022 10:58:06 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
06/23/2022 10:58:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
06/23/2022 10:58:11 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
06/23/2022 10:58:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
06/23/2022 10:58:14 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7810361681329424 on epoch=512
06/23/2022 10:58:17 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=514
06/23/2022 10:58:19 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
06/23/2022 10:58:21 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
06/23/2022 10:58:24 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
06/23/2022 10:58:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/23/2022 10:58:27 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.8252612330198538 on epoch=524
06/23/2022 10:58:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
06/23/2022 10:58:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
06/23/2022 10:58:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
06/23/2022 10:58:37 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
06/23/2022 10:58:39 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/23/2022 10:58:40 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.8053733900508094 on epoch=537
06/23/2022 10:58:43 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/23/2022 10:58:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/23/2022 10:58:48 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/23/2022 10:58:50 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/23/2022 10:58:52 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
06/23/2022 10:58:53 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.7744307400379508 on epoch=549
06/23/2022 10:58:56 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/23/2022 10:58:58 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/23/2022 10:59:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
06/23/2022 10:59:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
06/23/2022 10:59:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/23/2022 10:59:06 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.8252612330198538 on epoch=562
06/23/2022 10:59:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
06/23/2022 10:59:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
06/23/2022 10:59:13 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=569
06/23/2022 10:59:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/23/2022 10:59:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=574
06/23/2022 10:59:19 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7948924731182797 on epoch=574
06/23/2022 10:59:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/23/2022 10:59:24 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/23/2022 10:59:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/23/2022 10:59:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
06/23/2022 10:59:31 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
06/23/2022 10:59:32 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.8388091347650171 on epoch=587
06/23/2022 10:59:35 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
06/23/2022 10:59:37 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
06/23/2022 10:59:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/23/2022 10:59:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/23/2022 10:59:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=599
06/23/2022 10:59:45 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.8252612330198538 on epoch=599
06/23/2022 10:59:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/23/2022 10:59:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/23/2022 10:59:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/23/2022 10:59:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
06/23/2022 10:59:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
06/23/2022 10:59:58 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.8277176660936394 on epoch=612
06/23/2022 11:00:01 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/23/2022 11:00:03 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
06/23/2022 11:00:06 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/23/2022 11:00:08 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
06/23/2022 11:00:11 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/23/2022 11:00:11 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.8053733900508094 on epoch=624
06/23/2022 11:00:14 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
06/23/2022 11:00:16 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=629
06/23/2022 11:00:19 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/23/2022 11:00:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/23/2022 11:00:24 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
06/23/2022 11:00:25 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.8053733900508094 on epoch=637
06/23/2022 11:00:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
06/23/2022 11:00:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/23/2022 11:00:32 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
06/23/2022 11:00:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.26 on epoch=647
06/23/2022 11:00:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/23/2022 11:00:38 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.8203983516483516 on epoch=649
06/23/2022 11:00:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/23/2022 11:00:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/23/2022 11:00:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/23/2022 11:00:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/23/2022 11:00:50 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/23/2022 11:00:51 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.8203983516483516 on epoch=662
06/23/2022 11:00:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/23/2022 11:00:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/23/2022 11:00:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/23/2022 11:01:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
06/23/2022 11:01:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/23/2022 11:01:04 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.8053733900508094 on epoch=674
06/23/2022 11:01:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
06/23/2022 11:01:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/23/2022 11:01:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/23/2022 11:01:14 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/23/2022 11:01:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/23/2022 11:01:17 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.8053733900508094 on epoch=687
06/23/2022 11:01:20 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/23/2022 11:01:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
06/23/2022 11:01:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/23/2022 11:01:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/23/2022 11:01:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/23/2022 11:01:30 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.8203983516483516 on epoch=699
06/23/2022 11:01:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/23/2022 11:01:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=704
06/23/2022 11:01:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
06/23/2022 11:01:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/23/2022 11:01:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/23/2022 11:01:44 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7948924731182797 on epoch=712
06/23/2022 11:01:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/23/2022 11:01:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/23/2022 11:01:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/23/2022 11:01:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/23/2022 11:01:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
06/23/2022 11:01:57 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.8384828678946326 on epoch=724
06/23/2022 11:01:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/23/2022 11:02:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=729
06/23/2022 11:02:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/23/2022 11:02:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/23/2022 11:02:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/23/2022 11:02:10 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.8074929971988796 on epoch=737
06/23/2022 11:02:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/23/2022 11:02:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/23/2022 11:02:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/23/2022 11:02:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/23/2022 11:02:22 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/23/2022 11:02:23 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.825473292765382 on epoch=749
06/23/2022 11:02:23 - INFO - __main__ - save last model!
06/23/2022 11:02:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 11:02:23 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 11:02:23 - INFO - __main__ - Printing 3 examples
06/23/2022 11:02:23 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 11:02:23 - INFO - __main__ - ['others']
06/23/2022 11:02:23 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 11:02:23 - INFO - __main__ - ['others']
06/23/2022 11:02:23 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 11:02:23 - INFO - __main__ - ['others']
06/23/2022 11:02:24 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:02:24 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:02:24 - INFO - __main__ - Printing 3 examples
06/23/2022 11:02:24 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/23/2022 11:02:24 - INFO - __main__ - ['happy']
06/23/2022 11:02:24 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/23/2022 11:02:24 - INFO - __main__ - ['happy']
06/23/2022 11:02:24 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/23/2022 11:02:24 - INFO - __main__ - ['happy']
06/23/2022 11:02:24 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:02:24 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:02:24 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 11:02:24 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:02:24 - INFO - __main__ - Printing 3 examples
06/23/2022 11:02:24 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/23/2022 11:02:24 - INFO - __main__ - ['happy']
06/23/2022 11:02:24 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/23/2022 11:02:24 - INFO - __main__ - ['happy']
06/23/2022 11:02:24 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/23/2022 11:02:24 - INFO - __main__ - ['happy']
06/23/2022 11:02:24 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:02:24 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:02:24 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 11:02:26 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:02:31 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 11:02:42 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 11:02:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 11:02:43 - INFO - __main__ - Starting training!
06/23/2022 11:03:56 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_42_0.4_8_predictions.txt
06/23/2022 11:03:56 - INFO - __main__ - Classification-F1 on test data: 0.2087
06/23/2022 11:03:57 - INFO - __main__ - prefix=emo_16_42, lr=0.4, bsz=8, dev_performance=0.8415854978354979, test_performance=0.20872164188101128
06/23/2022 11:03:57 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.3, bsz=8 ...
06/23/2022 11:03:58 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:03:58 - INFO - __main__ - Printing 3 examples
06/23/2022 11:03:58 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/23/2022 11:03:58 - INFO - __main__ - ['happy']
06/23/2022 11:03:58 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/23/2022 11:03:58 - INFO - __main__ - ['happy']
06/23/2022 11:03:58 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/23/2022 11:03:58 - INFO - __main__ - ['happy']
06/23/2022 11:03:58 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:03:58 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:03:58 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 11:03:58 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:03:58 - INFO - __main__ - Printing 3 examples
06/23/2022 11:03:58 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/23/2022 11:03:58 - INFO - __main__ - ['happy']
06/23/2022 11:03:58 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/23/2022 11:03:58 - INFO - __main__ - ['happy']
06/23/2022 11:03:58 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/23/2022 11:03:58 - INFO - __main__ - ['happy']
06/23/2022 11:03:58 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:03:58 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:03:58 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 11:04:17 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 11:04:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 11:04:18 - INFO - __main__ - Starting training!
06/23/2022 11:04:21 - INFO - __main__ - Step 10 Global step 10 Train loss 4.44 on epoch=2
06/23/2022 11:04:23 - INFO - __main__ - Step 20 Global step 20 Train loss 3.58 on epoch=4
06/23/2022 11:04:26 - INFO - __main__ - Step 30 Global step 30 Train loss 2.94 on epoch=7
06/23/2022 11:04:28 - INFO - __main__ - Step 40 Global step 40 Train loss 2.68 on epoch=9
06/23/2022 11:04:31 - INFO - __main__ - Step 50 Global step 50 Train loss 2.36 on epoch=12
06/23/2022 11:04:31 - INFO - __main__ - Global step 50 Train loss 3.20 Classification-F1 0.0 on epoch=12
06/23/2022 11:04:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
06/23/2022 11:04:34 - INFO - __main__ - Step 60 Global step 60 Train loss 2.13 on epoch=14
06/23/2022 11:04:36 - INFO - __main__ - Step 70 Global step 70 Train loss 1.65 on epoch=17
06/23/2022 11:04:39 - INFO - __main__ - Step 80 Global step 80 Train loss 1.56 on epoch=19
06/23/2022 11:04:41 - INFO - __main__ - Step 90 Global step 90 Train loss 1.11 on epoch=22
06/23/2022 11:04:44 - INFO - __main__ - Step 100 Global step 100 Train loss 1.07 on epoch=24
06/23/2022 11:04:45 - INFO - __main__ - Global step 100 Train loss 1.50 Classification-F1 0.4947163733500942 on epoch=24
06/23/2022 11:04:45 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.4947163733500942 on epoch=24, global_step=100
06/23/2022 11:04:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.95 on epoch=27
06/23/2022 11:04:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.92 on epoch=29
06/23/2022 11:04:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.85 on epoch=32
06/23/2022 11:04:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.86 on epoch=34
06/23/2022 11:04:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.76 on epoch=37
06/23/2022 11:04:58 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.5508904374758034 on epoch=37
06/23/2022 11:04:58 - INFO - __main__ - Saving model with best Classification-F1: 0.4947163733500942 -> 0.5508904374758034 on epoch=37, global_step=150
06/23/2022 11:05:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.82 on epoch=39
06/23/2022 11:05:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.68 on epoch=42
06/23/2022 11:05:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.72 on epoch=44
06/23/2022 11:05:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.60 on epoch=47
06/23/2022 11:05:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.80 on epoch=49
06/23/2022 11:05:11 - INFO - __main__ - Global step 200 Train loss 0.72 Classification-F1 0.5031024531024532 on epoch=49
06/23/2022 11:05:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.64 on epoch=52
06/23/2022 11:05:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.65 on epoch=54
06/23/2022 11:05:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=57
06/23/2022 11:05:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.64 on epoch=59
06/23/2022 11:05:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.60 on epoch=62
06/23/2022 11:05:24 - INFO - __main__ - Global step 250 Train loss 0.62 Classification-F1 0.5426767676767676 on epoch=62
06/23/2022 11:05:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.61 on epoch=64
06/23/2022 11:05:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.65 on epoch=67
06/23/2022 11:05:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.65 on epoch=69
06/23/2022 11:05:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.61 on epoch=72
06/23/2022 11:05:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=74
06/23/2022 11:05:37 - INFO - __main__ - Global step 300 Train loss 0.64 Classification-F1 0.6865144659262306 on epoch=74
06/23/2022 11:05:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5508904374758034 -> 0.6865144659262306 on epoch=74, global_step=300
06/23/2022 11:05:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.57 on epoch=77
06/23/2022 11:05:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.60 on epoch=79
06/23/2022 11:05:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.58 on epoch=82
06/23/2022 11:05:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=84
06/23/2022 11:05:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.54 on epoch=87
06/23/2022 11:05:50 - INFO - __main__ - Global step 350 Train loss 0.56 Classification-F1 0.653453947368421 on epoch=87
06/23/2022 11:05:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=89
06/23/2022 11:05:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=92
06/23/2022 11:05:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=94
06/23/2022 11:06:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=97
06/23/2022 11:06:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=99
06/23/2022 11:06:03 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.7790066071316072 on epoch=99
06/23/2022 11:06:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6865144659262306 -> 0.7790066071316072 on epoch=99, global_step=400
06/23/2022 11:06:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=102
06/23/2022 11:06:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=104
06/23/2022 11:06:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=107
06/23/2022 11:06:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=109
06/23/2022 11:06:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=112
06/23/2022 11:06:16 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.7315675552517658 on epoch=112
06/23/2022 11:06:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=114
06/23/2022 11:06:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=117
06/23/2022 11:06:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=119
06/23/2022 11:06:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=122
06/23/2022 11:06:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=124
06/23/2022 11:06:29 - INFO - __main__ - Global step 500 Train loss 0.35 Classification-F1 0.7804135828329376 on epoch=124
06/23/2022 11:06:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7790066071316072 -> 0.7804135828329376 on epoch=124, global_step=500
06/23/2022 11:06:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=127
06/23/2022 11:06:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=129
06/23/2022 11:06:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=132
06/23/2022 11:06:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=134
06/23/2022 11:06:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=137
06/23/2022 11:06:42 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.7077294685990339 on epoch=137
06/23/2022 11:06:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=139
06/23/2022 11:06:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=142
06/23/2022 11:06:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.30 on epoch=144
06/23/2022 11:06:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.31 on epoch=147
06/23/2022 11:06:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=149
06/23/2022 11:06:55 - INFO - __main__ - Global step 600 Train loss 0.30 Classification-F1 0.7469934640522877 on epoch=149
06/23/2022 11:06:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.32 on epoch=152
06/23/2022 11:07:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=154
06/23/2022 11:07:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=157
06/23/2022 11:07:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=159
06/23/2022 11:07:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=162
06/23/2022 11:07:09 - INFO - __main__ - Global step 650 Train loss 0.27 Classification-F1 0.7441919191919192 on epoch=162
06/23/2022 11:07:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=164
06/23/2022 11:07:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=167
06/23/2022 11:07:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=169
06/23/2022 11:07:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.28 on epoch=172
06/23/2022 11:07:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=174
06/23/2022 11:07:22 - INFO - __main__ - Global step 700 Train loss 0.25 Classification-F1 0.7960497835497835 on epoch=174
06/23/2022 11:07:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7804135828329376 -> 0.7960497835497835 on epoch=174, global_step=700
06/23/2022 11:07:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=177
06/23/2022 11:07:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=179
06/23/2022 11:07:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.28 on epoch=182
06/23/2022 11:07:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=184
06/23/2022 11:07:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=187
06/23/2022 11:07:35 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.7815444359562006 on epoch=187
06/23/2022 11:07:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=189
06/23/2022 11:07:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=192
06/23/2022 11:07:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=194
06/23/2022 11:07:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=197
06/23/2022 11:07:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=199
06/23/2022 11:07:48 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.8116747835497835 on epoch=199
06/23/2022 11:07:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7960497835497835 -> 0.8116747835497835 on epoch=199, global_step=800
06/23/2022 11:07:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=202
06/23/2022 11:07:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.15 on epoch=204
06/23/2022 11:07:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=207
06/23/2022 11:07:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=209
06/23/2022 11:08:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=212
06/23/2022 11:08:01 - INFO - __main__ - Global step 850 Train loss 0.17 Classification-F1 0.8265533486121722 on epoch=212
06/23/2022 11:08:01 - INFO - __main__ - Saving model with best Classification-F1: 0.8116747835497835 -> 0.8265533486121722 on epoch=212, global_step=850
06/23/2022 11:08:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=214
06/23/2022 11:08:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.15 on epoch=217
06/23/2022 11:08:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.15 on epoch=219
06/23/2022 11:08:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=222
06/23/2022 11:08:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=224
06/23/2022 11:08:14 - INFO - __main__ - Global step 900 Train loss 0.17 Classification-F1 0.8114018334606571 on epoch=224
06/23/2022 11:08:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=227
06/23/2022 11:08:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=229
06/23/2022 11:08:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=232
06/23/2022 11:08:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.14 on epoch=234
06/23/2022 11:08:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=237
06/23/2022 11:08:27 - INFO - __main__ - Global step 950 Train loss 0.13 Classification-F1 0.8415854978354979 on epoch=237
06/23/2022 11:08:27 - INFO - __main__ - Saving model with best Classification-F1: 0.8265533486121722 -> 0.8415854978354979 on epoch=237, global_step=950
06/23/2022 11:08:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=239
06/23/2022 11:08:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=242
06/23/2022 11:08:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=244
06/23/2022 11:08:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.11 on epoch=247
06/23/2022 11:08:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=249
06/23/2022 11:08:40 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.8241032524120759 on epoch=249
06/23/2022 11:08:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=252
06/23/2022 11:08:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.10 on epoch=254
06/23/2022 11:08:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=257
06/23/2022 11:08:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=259
06/23/2022 11:08:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=262
06/23/2022 11:08:53 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.8259604978354979 on epoch=262
06/23/2022 11:08:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=264
06/23/2022 11:08:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=267
06/23/2022 11:09:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=269
06/23/2022 11:09:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=272
06/23/2022 11:09:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.11 on epoch=274
06/23/2022 11:09:07 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.7752103658175765 on epoch=274
06/23/2022 11:09:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.13 on epoch=277
06/23/2022 11:09:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=279
06/23/2022 11:09:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=282
06/23/2022 11:09:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.10 on epoch=284
06/23/2022 11:09:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
06/23/2022 11:09:20 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.7904176093514329 on epoch=287
06/23/2022 11:09:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
06/23/2022 11:09:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=292
06/23/2022 11:09:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.11 on epoch=294
06/23/2022 11:09:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=297
06/23/2022 11:09:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
06/23/2022 11:09:33 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.7752103658175765 on epoch=299
06/23/2022 11:09:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=302
06/23/2022 11:09:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=304
06/23/2022 11:09:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=307
06/23/2022 11:09:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
06/23/2022 11:09:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=312
06/23/2022 11:09:46 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.8201357466063349 on epoch=312
06/23/2022 11:09:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
06/23/2022 11:09:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
06/23/2022 11:09:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.09 on epoch=319
06/23/2022 11:09:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=322
06/23/2022 11:09:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=324
06/23/2022 11:09:59 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.8053662024250261 on epoch=324
06/23/2022 11:10:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
06/23/2022 11:10:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
06/23/2022 11:10:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
06/23/2022 11:10:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=334
06/23/2022 11:10:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
06/23/2022 11:10:12 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.8201357466063349 on epoch=337
06/23/2022 11:10:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
06/23/2022 11:10:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
06/23/2022 11:10:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=344
06/23/2022 11:10:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
06/23/2022 11:10:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
06/23/2022 11:10:26 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.7904176093514329 on epoch=349
06/23/2022 11:10:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
06/23/2022 11:10:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=354
06/23/2022 11:10:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
06/23/2022 11:10:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=359
06/23/2022 11:10:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
06/23/2022 11:10:39 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.8053662024250261 on epoch=362
06/23/2022 11:10:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
06/23/2022 11:10:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
06/23/2022 11:10:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=369
06/23/2022 11:10:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=372
06/23/2022 11:10:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=374
06/23/2022 11:10:52 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.7904176093514329 on epoch=374
06/23/2022 11:10:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
06/23/2022 11:10:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
06/23/2022 11:10:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
06/23/2022 11:11:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
06/23/2022 11:11:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
06/23/2022 11:11:05 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.8201357466063349 on epoch=387
06/23/2022 11:11:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
06/23/2022 11:11:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
06/23/2022 11:11:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=394
06/23/2022 11:11:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
06/23/2022 11:11:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
06/23/2022 11:11:18 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.8352235999294823 on epoch=399
06/23/2022 11:11:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/23/2022 11:11:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
06/23/2022 11:11:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
06/23/2022 11:11:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/23/2022 11:11:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
06/23/2022 11:11:31 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.835955710955711 on epoch=412
06/23/2022 11:11:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
06/23/2022 11:11:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
06/23/2022 11:11:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/23/2022 11:11:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=422
06/23/2022 11:11:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
06/23/2022 11:11:44 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.8056526806526807 on epoch=424
06/23/2022 11:11:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
06/23/2022 11:11:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/23/2022 11:11:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/23/2022 11:11:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
06/23/2022 11:11:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=437
06/23/2022 11:11:57 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.835844155844156 on epoch=437
06/23/2022 11:12:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=439
06/23/2022 11:12:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.09 on epoch=442
06/23/2022 11:12:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=444
06/23/2022 11:12:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
06/23/2022 11:12:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=449
06/23/2022 11:12:10 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.8207484917043741 on epoch=449
06/23/2022 11:12:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=452
06/23/2022 11:12:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
06/23/2022 11:12:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/23/2022 11:12:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
06/23/2022 11:12:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
06/23/2022 11:12:23 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.8352235999294823 on epoch=462
06/23/2022 11:12:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/23/2022 11:12:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
06/23/2022 11:12:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
06/23/2022 11:12:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
06/23/2022 11:12:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
06/23/2022 11:12:37 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.8352235999294823 on epoch=474
06/23/2022 11:12:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
06/23/2022 11:12:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
06/23/2022 11:12:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
06/23/2022 11:12:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
06/23/2022 11:12:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
06/23/2022 11:12:50 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.8200720847779672 on epoch=487
06/23/2022 11:12:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
06/23/2022 11:12:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/23/2022 11:12:57 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
06/23/2022 11:13:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
06/23/2022 11:13:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
06/23/2022 11:13:03 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.8047733516483517 on epoch=499
06/23/2022 11:13:05 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=502
06/23/2022 11:13:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
06/23/2022 11:13:10 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
06/23/2022 11:13:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/23/2022 11:13:15 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=512
06/23/2022 11:13:16 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.8053662024250261 on epoch=512
06/23/2022 11:13:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/23/2022 11:13:21 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
06/23/2022 11:13:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
06/23/2022 11:13:26 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
06/23/2022 11:13:28 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=524
06/23/2022 11:13:29 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.8352235999294823 on epoch=524
06/23/2022 11:13:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=527
06/23/2022 11:13:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
06/23/2022 11:13:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
06/23/2022 11:13:39 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
06/23/2022 11:13:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
06/23/2022 11:13:42 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.7761149433475409 on epoch=537
06/23/2022 11:13:44 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/23/2022 11:13:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=542
06/23/2022 11:13:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
06/23/2022 11:13:52 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=547
06/23/2022 11:13:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/23/2022 11:13:55 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.8053662024250261 on epoch=549
06/23/2022 11:13:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/23/2022 11:14:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
06/23/2022 11:14:02 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
06/23/2022 11:14:04 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/23/2022 11:14:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
06/23/2022 11:14:08 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7761149433475409 on epoch=562
06/23/2022 11:14:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
06/23/2022 11:14:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
06/23/2022 11:14:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
06/23/2022 11:14:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
06/23/2022 11:14:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.09 on epoch=574
06/23/2022 11:14:21 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.7809831593927894 on epoch=574
06/23/2022 11:14:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/23/2022 11:14:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/23/2022 11:14:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/23/2022 11:14:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
06/23/2022 11:14:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/23/2022 11:14:34 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.8259352526276968 on epoch=587
06/23/2022 11:14:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/23/2022 11:14:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
06/23/2022 11:14:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/23/2022 11:14:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
06/23/2022 11:14:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
06/23/2022 11:14:47 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.8200720847779672 on epoch=599
06/23/2022 11:14:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
06/23/2022 11:14:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/23/2022 11:14:54 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/23/2022 11:14:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/23/2022 11:14:59 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
06/23/2022 11:15:00 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.8200720847779672 on epoch=612
06/23/2022 11:15:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/23/2022 11:15:05 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
06/23/2022 11:15:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
06/23/2022 11:15:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
06/23/2022 11:15:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.11 on epoch=624
06/23/2022 11:15:13 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.8234020881079704 on epoch=624
06/23/2022 11:15:16 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/23/2022 11:15:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/23/2022 11:15:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
06/23/2022 11:15:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
06/23/2022 11:15:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
06/23/2022 11:15:26 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.8415854978354979 on epoch=637
06/23/2022 11:15:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/23/2022 11:15:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=642
06/23/2022 11:15:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
06/23/2022 11:15:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.10 on epoch=647
06/23/2022 11:15:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/23/2022 11:15:39 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.841114619829123 on epoch=649
06/23/2022 11:15:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/23/2022 11:15:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/23/2022 11:15:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
06/23/2022 11:15:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/23/2022 11:15:51 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/23/2022 11:15:52 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.841114619829123 on epoch=662
06/23/2022 11:15:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=664
06/23/2022 11:15:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/23/2022 11:16:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
06/23/2022 11:16:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=672
06/23/2022 11:16:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
06/23/2022 11:16:05 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.8203983516483516 on epoch=674
06/23/2022 11:16:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=677
06/23/2022 11:16:10 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/23/2022 11:16:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=682
06/23/2022 11:16:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/23/2022 11:16:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
06/23/2022 11:16:19 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.8264087374761817 on epoch=687
06/23/2022 11:16:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
06/23/2022 11:16:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/23/2022 11:16:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/23/2022 11:16:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/23/2022 11:16:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=699
06/23/2022 11:16:32 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.841114619829123 on epoch=699
06/23/2022 11:16:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=702
06/23/2022 11:16:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
06/23/2022 11:16:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/23/2022 11:16:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/23/2022 11:16:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=712
06/23/2022 11:16:45 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.7899597338935574 on epoch=712
06/23/2022 11:16:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/23/2022 11:16:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/23/2022 11:16:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/23/2022 11:16:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
06/23/2022 11:16:57 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/23/2022 11:16:58 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.804915514592934 on epoch=724
06/23/2022 11:17:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/23/2022 11:17:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=729
06/23/2022 11:17:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/23/2022 11:17:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/23/2022 11:17:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/23/2022 11:17:11 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.804915514592934 on epoch=737
06/23/2022 11:17:14 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
06/23/2022 11:17:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
06/23/2022 11:17:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
06/23/2022 11:17:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=747
06/23/2022 11:17:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=749
06/23/2022 11:17:24 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.8384828678946326 on epoch=749
06/23/2022 11:17:24 - INFO - __main__ - save last model!
06/23/2022 11:17:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 11:17:24 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 11:17:24 - INFO - __main__ - Printing 3 examples
06/23/2022 11:17:24 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 11:17:24 - INFO - __main__ - ['others']
06/23/2022 11:17:24 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 11:17:24 - INFO - __main__ - ['others']
06/23/2022 11:17:24 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 11:17:24 - INFO - __main__ - ['others']
06/23/2022 11:17:24 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:17:25 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:17:25 - INFO - __main__ - Printing 3 examples
06/23/2022 11:17:25 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/23/2022 11:17:25 - INFO - __main__ - ['happy']
06/23/2022 11:17:25 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/23/2022 11:17:25 - INFO - __main__ - ['happy']
06/23/2022 11:17:25 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/23/2022 11:17:25 - INFO - __main__ - ['happy']
06/23/2022 11:17:25 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:17:25 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:17:25 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 11:17:25 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:17:25 - INFO - __main__ - Printing 3 examples
06/23/2022 11:17:25 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/23/2022 11:17:25 - INFO - __main__ - ['happy']
06/23/2022 11:17:25 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/23/2022 11:17:25 - INFO - __main__ - ['happy']
06/23/2022 11:17:25 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/23/2022 11:17:25 - INFO - __main__ - ['happy']
06/23/2022 11:17:25 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:17:25 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:17:25 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 11:17:27 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:17:32 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 11:17:44 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 11:17:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 11:17:45 - INFO - __main__ - Starting training!
06/23/2022 11:18:47 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_42_0.3_8_predictions.txt
06/23/2022 11:18:47 - INFO - __main__ - Classification-F1 on test data: 0.2470
06/23/2022 11:18:47 - INFO - __main__ - prefix=emo_16_42, lr=0.3, bsz=8, dev_performance=0.8415854978354979, test_performance=0.24704082367986177
06/23/2022 11:18:47 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.2, bsz=8 ...
06/23/2022 11:18:48 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:18:48 - INFO - __main__ - Printing 3 examples
06/23/2022 11:18:48 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/23/2022 11:18:48 - INFO - __main__ - ['happy']
06/23/2022 11:18:48 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/23/2022 11:18:48 - INFO - __main__ - ['happy']
06/23/2022 11:18:48 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/23/2022 11:18:48 - INFO - __main__ - ['happy']
06/23/2022 11:18:48 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:18:48 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:18:48 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 11:18:48 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:18:48 - INFO - __main__ - Printing 3 examples
06/23/2022 11:18:48 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/23/2022 11:18:48 - INFO - __main__ - ['happy']
06/23/2022 11:18:48 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/23/2022 11:18:48 - INFO - __main__ - ['happy']
06/23/2022 11:18:48 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/23/2022 11:18:48 - INFO - __main__ - ['happy']
06/23/2022 11:18:48 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:18:48 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:18:48 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 11:19:07 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 11:19:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 11:19:08 - INFO - __main__ - Starting training!
06/23/2022 11:19:11 - INFO - __main__ - Step 10 Global step 10 Train loss 5.02 on epoch=2
06/23/2022 11:19:13 - INFO - __main__ - Step 20 Global step 20 Train loss 4.18 on epoch=4
06/23/2022 11:19:16 - INFO - __main__ - Step 30 Global step 30 Train loss 3.34 on epoch=7
06/23/2022 11:19:18 - INFO - __main__ - Step 40 Global step 40 Train loss 3.04 on epoch=9
06/23/2022 11:19:20 - INFO - __main__ - Step 50 Global step 50 Train loss 2.81 on epoch=12
06/23/2022 11:19:21 - INFO - __main__ - Global step 50 Train loss 3.68 Classification-F1 0.0 on epoch=12
06/23/2022 11:19:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
06/23/2022 11:19:24 - INFO - __main__ - Step 60 Global step 60 Train loss 2.67 on epoch=14
06/23/2022 11:19:26 - INFO - __main__ - Step 70 Global step 70 Train loss 2.08 on epoch=17
06/23/2022 11:19:29 - INFO - __main__ - Step 80 Global step 80 Train loss 2.03 on epoch=19
06/23/2022 11:19:31 - INFO - __main__ - Step 90 Global step 90 Train loss 1.79 on epoch=22
06/23/2022 11:19:34 - INFO - __main__ - Step 100 Global step 100 Train loss 1.82 on epoch=24
06/23/2022 11:19:34 - INFO - __main__ - Global step 100 Train loss 2.08 Classification-F1 0.36646153846153845 on epoch=24
06/23/2022 11:19:35 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.36646153846153845 on epoch=24, global_step=100
06/23/2022 11:19:37 - INFO - __main__ - Step 110 Global step 110 Train loss 1.25 on epoch=27
06/23/2022 11:19:39 - INFO - __main__ - Step 120 Global step 120 Train loss 1.29 on epoch=29
06/23/2022 11:19:42 - INFO - __main__ - Step 130 Global step 130 Train loss 1.24 on epoch=32
06/23/2022 11:19:44 - INFO - __main__ - Step 140 Global step 140 Train loss 1.16 on epoch=34
06/23/2022 11:19:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.80 on epoch=37
06/23/2022 11:19:47 - INFO - __main__ - Global step 150 Train loss 1.15 Classification-F1 0.48127108457094114 on epoch=37
06/23/2022 11:19:48 - INFO - __main__ - Saving model with best Classification-F1: 0.36646153846153845 -> 0.48127108457094114 on epoch=37, global_step=150
06/23/2022 11:19:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.95 on epoch=39
06/23/2022 11:19:52 - INFO - __main__ - Step 170 Global step 170 Train loss 1.00 on epoch=42
06/23/2022 11:19:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=44
06/23/2022 11:19:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=47
06/23/2022 11:20:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.73 on epoch=49
06/23/2022 11:20:01 - INFO - __main__ - Global step 200 Train loss 0.89 Classification-F1 0.507940632940633 on epoch=49
06/23/2022 11:20:01 - INFO - __main__ - Saving model with best Classification-F1: 0.48127108457094114 -> 0.507940632940633 on epoch=49, global_step=200
06/23/2022 11:20:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.78 on epoch=52
06/23/2022 11:20:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.81 on epoch=54
06/23/2022 11:20:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.92 on epoch=57
06/23/2022 11:20:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.83 on epoch=59
06/23/2022 11:20:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.60 on epoch=62
06/23/2022 11:20:14 - INFO - __main__ - Global step 250 Train loss 0.79 Classification-F1 0.5364114114114115 on epoch=62
06/23/2022 11:20:14 - INFO - __main__ - Saving model with best Classification-F1: 0.507940632940633 -> 0.5364114114114115 on epoch=62, global_step=250
06/23/2022 11:20:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.67 on epoch=64
06/23/2022 11:20:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.67 on epoch=67
06/23/2022 11:20:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.68 on epoch=69
06/23/2022 11:20:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.72 on epoch=72
06/23/2022 11:20:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.60 on epoch=74
06/23/2022 11:20:27 - INFO - __main__ - Global step 300 Train loss 0.67 Classification-F1 0.4946585753615883 on epoch=74
06/23/2022 11:20:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.63 on epoch=77
06/23/2022 11:20:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.63 on epoch=79
06/23/2022 11:20:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.57 on epoch=82
06/23/2022 11:20:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.57 on epoch=84
06/23/2022 11:20:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.53 on epoch=87
06/23/2022 11:20:40 - INFO - __main__ - Global step 350 Train loss 0.59 Classification-F1 0.5211538461538462 on epoch=87
06/23/2022 11:20:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.63 on epoch=89
06/23/2022 11:20:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.58 on epoch=92
06/23/2022 11:20:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.67 on epoch=94
06/23/2022 11:20:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=97
06/23/2022 11:20:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.68 on epoch=99
06/23/2022 11:20:53 - INFO - __main__ - Global step 400 Train loss 0.61 Classification-F1 0.5991410818713451 on epoch=99
06/23/2022 11:20:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5364114114114115 -> 0.5991410818713451 on epoch=99, global_step=400
06/23/2022 11:20:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.54 on epoch=102
06/23/2022 11:20:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.57 on epoch=104
06/23/2022 11:21:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.55 on epoch=107
06/23/2022 11:21:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.63 on epoch=109
06/23/2022 11:21:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.53 on epoch=112
06/23/2022 11:21:06 - INFO - __main__ - Global step 450 Train loss 0.56 Classification-F1 0.5684241202899645 on epoch=112
06/23/2022 11:21:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.60 on epoch=114
06/23/2022 11:21:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=117
06/23/2022 11:21:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.58 on epoch=119
06/23/2022 11:21:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=122
06/23/2022 11:21:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=124
06/23/2022 11:21:19 - INFO - __main__ - Global step 500 Train loss 0.52 Classification-F1 0.7211960601906254 on epoch=124
06/23/2022 11:21:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5991410818713451 -> 0.7211960601906254 on epoch=124, global_step=500
06/23/2022 11:21:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=127
06/23/2022 11:21:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.53 on epoch=129
06/23/2022 11:21:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=132
06/23/2022 11:21:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=134
06/23/2022 11:21:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=137
06/23/2022 11:21:32 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.7310150104267751 on epoch=137
06/23/2022 11:21:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7211960601906254 -> 0.7310150104267751 on epoch=137, global_step=550
06/23/2022 11:21:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=139
06/23/2022 11:21:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=142
06/23/2022 11:21:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=144
06/23/2022 11:21:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=147
06/23/2022 11:21:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=149
06/23/2022 11:21:45 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.7604600556438792 on epoch=149
06/23/2022 11:21:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7310150104267751 -> 0.7604600556438792 on epoch=149, global_step=600
06/23/2022 11:21:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=152
06/23/2022 11:21:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=154
06/23/2022 11:21:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=157
06/23/2022 11:21:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=159
06/23/2022 11:21:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=162
06/23/2022 11:21:58 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.7211960601906254 on epoch=162
06/23/2022 11:22:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=164
06/23/2022 11:22:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=167
06/23/2022 11:22:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=169
06/23/2022 11:22:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=172
06/23/2022 11:22:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=174
06/23/2022 11:22:11 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.7412443693693693 on epoch=174
06/23/2022 11:22:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=177
06/23/2022 11:22:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=179
06/23/2022 11:22:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=182
06/23/2022 11:22:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=184
06/23/2022 11:22:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=187
06/23/2022 11:22:24 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.7373746600089311 on epoch=187
06/23/2022 11:22:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=189
06/23/2022 11:22:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=192
06/23/2022 11:22:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.31 on epoch=194
06/23/2022 11:22:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=197
06/23/2022 11:22:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=199
06/23/2022 11:22:37 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.7494212962962963 on epoch=199
06/23/2022 11:22:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=202
06/23/2022 11:22:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=204
06/23/2022 11:22:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=207
06/23/2022 11:22:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.29 on epoch=209
06/23/2022 11:22:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=212
06/23/2022 11:22:50 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.7792137056842939 on epoch=212
06/23/2022 11:22:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7604600556438792 -> 0.7792137056842939 on epoch=212, global_step=850
06/23/2022 11:22:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.33 on epoch=214
06/23/2022 11:22:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=217
06/23/2022 11:22:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=219
06/23/2022 11:23:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=222
06/23/2022 11:23:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=224
06/23/2022 11:23:03 - INFO - __main__ - Global step 900 Train loss 0.29 Classification-F1 0.7647019919078741 on epoch=224
06/23/2022 11:23:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=227
06/23/2022 11:23:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=229
06/23/2022 11:23:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.31 on epoch=232
06/23/2022 11:23:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=234
06/23/2022 11:23:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=237
06/23/2022 11:23:16 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.7812941960000783 on epoch=237
06/23/2022 11:23:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7792137056842939 -> 0.7812941960000783 on epoch=237, global_step=950
06/23/2022 11:23:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=239
06/23/2022 11:23:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=242
06/23/2022 11:23:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=244
06/23/2022 11:23:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=247
06/23/2022 11:23:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=249
06/23/2022 11:23:29 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.780564263322884 on epoch=249
06/23/2022 11:23:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=252
06/23/2022 11:23:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=254
06/23/2022 11:23:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=257
06/23/2022 11:23:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=259
06/23/2022 11:23:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=262
06/23/2022 11:23:42 - INFO - __main__ - Global step 1050 Train loss 0.24 Classification-F1 0.7782587782587782 on epoch=262
06/23/2022 11:23:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=264
06/23/2022 11:23:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=267
06/23/2022 11:23:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=269
06/23/2022 11:23:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=272
06/23/2022 11:23:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=274
06/23/2022 11:23:55 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.7508506183340968 on epoch=274
06/23/2022 11:23:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=277
06/23/2022 11:24:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.17 on epoch=279
06/23/2022 11:24:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=282
06/23/2022 11:24:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=284
06/23/2022 11:24:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=287
06/23/2022 11:24:09 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.7802801724137931 on epoch=287
06/23/2022 11:24:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=289
06/23/2022 11:24:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=292
06/23/2022 11:24:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=294
06/23/2022 11:24:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=297
06/23/2022 11:24:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=299
06/23/2022 11:24:22 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.7802801724137931 on epoch=299
06/23/2022 11:24:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.15 on epoch=302
06/23/2022 11:24:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=304
06/23/2022 11:24:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=307
06/23/2022 11:24:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=309
06/23/2022 11:24:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=312
06/23/2022 11:24:35 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.7901190476190476 on epoch=312
06/23/2022 11:24:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7812941960000783 -> 0.7901190476190476 on epoch=312, global_step=1250
06/23/2022 11:24:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.14 on epoch=314
06/23/2022 11:24:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=317
06/23/2022 11:24:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=319
06/23/2022 11:24:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=322
06/23/2022 11:24:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=324
06/23/2022 11:24:48 - INFO - __main__ - Global step 1300 Train loss 0.17 Classification-F1 0.7956322223246666 on epoch=324
06/23/2022 11:24:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7901190476190476 -> 0.7956322223246666 on epoch=324, global_step=1300
06/23/2022 11:24:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=327
06/23/2022 11:24:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=329
06/23/2022 11:24:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.16 on epoch=332
06/23/2022 11:24:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=334
06/23/2022 11:25:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=337
06/23/2022 11:25:01 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.8097319347319348 on epoch=337
06/23/2022 11:25:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7956322223246666 -> 0.8097319347319348 on epoch=337, global_step=1350
06/23/2022 11:25:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=339
06/23/2022 11:25:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=342
06/23/2022 11:25:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=344
06/23/2022 11:25:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=347
06/23/2022 11:25:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=349
06/23/2022 11:25:14 - INFO - __main__ - Global step 1400 Train loss 0.14 Classification-F1 0.7804528551232406 on epoch=349
06/23/2022 11:25:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=352
06/23/2022 11:25:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=354
06/23/2022 11:25:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=357
06/23/2022 11:25:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=359
06/23/2022 11:25:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=362
06/23/2022 11:25:27 - INFO - __main__ - Global step 1450 Train loss 0.10 Classification-F1 0.7639069264069265 on epoch=362
06/23/2022 11:25:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=364
06/23/2022 11:25:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=367
06/23/2022 11:25:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=369
06/23/2022 11:25:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.12 on epoch=372
06/23/2022 11:25:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=374
06/23/2022 11:25:40 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.7932734204793027 on epoch=374
06/23/2022 11:25:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.12 on epoch=377
06/23/2022 11:25:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=379
06/23/2022 11:25:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=382
06/23/2022 11:25:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=384
06/23/2022 11:25:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=387
06/23/2022 11:25:53 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.825017139723022 on epoch=387
06/23/2022 11:25:53 - INFO - __main__ - Saving model with best Classification-F1: 0.8097319347319348 -> 0.825017139723022 on epoch=387, global_step=1550
06/23/2022 11:25:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=389
06/23/2022 11:25:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.09 on epoch=392
06/23/2022 11:26:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
06/23/2022 11:26:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=397
06/23/2022 11:26:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=399
06/23/2022 11:26:06 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.7932734204793027 on epoch=399
06/23/2022 11:26:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=402
06/23/2022 11:26:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
06/23/2022 11:26:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
06/23/2022 11:26:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.15 on epoch=409
06/23/2022 11:26:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=412
06/23/2022 11:26:19 - INFO - __main__ - Global step 1650 Train loss 0.08 Classification-F1 0.7778361344537815 on epoch=412
06/23/2022 11:26:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=414
06/23/2022 11:26:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
06/23/2022 11:26:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=419
06/23/2022 11:26:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=422
06/23/2022 11:26:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=424
06/23/2022 11:26:32 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.7778361344537815 on epoch=424
06/23/2022 11:26:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
06/23/2022 11:26:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
06/23/2022 11:26:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=432
06/23/2022 11:26:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=434
06/23/2022 11:26:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=437
06/23/2022 11:26:45 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.7932734204793027 on epoch=437
06/23/2022 11:26:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=439
06/23/2022 11:26:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=442
06/23/2022 11:26:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=444
06/23/2022 11:26:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
06/23/2022 11:26:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=449
06/23/2022 11:26:58 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.7748626373626373 on epoch=449
06/23/2022 11:27:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=452
06/23/2022 11:27:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=454
06/23/2022 11:27:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
06/23/2022 11:27:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=459
06/23/2022 11:27:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
06/23/2022 11:27:12 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.7748626373626373 on epoch=462
06/23/2022 11:27:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=464
06/23/2022 11:27:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=467
06/23/2022 11:27:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=469
06/23/2022 11:27:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
06/23/2022 11:27:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=474
06/23/2022 11:27:25 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.7970232447817838 on epoch=474
06/23/2022 11:27:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=477
06/23/2022 11:27:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=479
06/23/2022 11:27:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=482
06/23/2022 11:27:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
06/23/2022 11:27:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
06/23/2022 11:27:38 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.7627960275019098 on epoch=487
06/23/2022 11:27:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
06/23/2022 11:27:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=492
06/23/2022 11:27:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=494
06/23/2022 11:27:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=497
06/23/2022 11:27:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
06/23/2022 11:27:51 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.7627960275019098 on epoch=499
06/23/2022 11:27:53 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=502
06/23/2022 11:27:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=504
06/23/2022 11:27:58 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
06/23/2022 11:28:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
06/23/2022 11:28:03 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
06/23/2022 11:28:04 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.7745007680491551 on epoch=512
06/23/2022 11:28:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
06/23/2022 11:28:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
06/23/2022 11:28:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
06/23/2022 11:28:13 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.16 on epoch=522
06/23/2022 11:28:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=524
06/23/2022 11:28:17 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.7778361344537815 on epoch=524
06/23/2022 11:28:19 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
06/23/2022 11:28:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
06/23/2022 11:28:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=532
06/23/2022 11:28:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=534
06/23/2022 11:28:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
06/23/2022 11:28:30 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.8081670168067228 on epoch=537
06/23/2022 11:28:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
06/23/2022 11:28:35 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=542
06/23/2022 11:28:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=544
06/23/2022 11:28:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
06/23/2022 11:28:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
06/23/2022 11:28:43 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.7927080509623204 on epoch=549
06/23/2022 11:28:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
06/23/2022 11:28:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
06/23/2022 11:28:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=557
06/23/2022 11:28:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
06/23/2022 11:28:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=562
06/23/2022 11:28:56 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.8081670168067228 on epoch=562
06/23/2022 11:28:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
06/23/2022 11:29:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=567
06/23/2022 11:29:03 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
06/23/2022 11:29:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=572
06/23/2022 11:29:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=574
06/23/2022 11:29:09 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7927080509623204 on epoch=574
06/23/2022 11:29:11 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
06/23/2022 11:29:14 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
06/23/2022 11:29:16 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
06/23/2022 11:29:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
06/23/2022 11:29:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
06/23/2022 11:29:22 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.775355468903856 on epoch=587
06/23/2022 11:29:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=589
06/23/2022 11:29:27 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
06/23/2022 11:29:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=594
06/23/2022 11:29:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=597
06/23/2022 11:29:34 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
06/23/2022 11:29:35 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.7745007680491551 on epoch=599
06/23/2022 11:29:37 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=602
06/23/2022 11:29:40 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.07 on epoch=604
06/23/2022 11:29:42 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
06/23/2022 11:29:45 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
06/23/2022 11:29:47 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=612
06/23/2022 11:29:48 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.7745007680491551 on epoch=612
06/23/2022 11:29:50 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=614
06/23/2022 11:29:53 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
06/23/2022 11:29:55 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
06/23/2022 11:29:57 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=622
06/23/2022 11:30:00 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
06/23/2022 11:30:01 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.790214687273511 on epoch=624
06/23/2022 11:30:03 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=627
06/23/2022 11:30:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=629
06/23/2022 11:30:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
06/23/2022 11:30:10 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=634
06/23/2022 11:30:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.08 on epoch=637
06/23/2022 11:30:14 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.7927080509623204 on epoch=637
06/23/2022 11:30:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
06/23/2022 11:30:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
06/23/2022 11:30:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
06/23/2022 11:30:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/23/2022 11:30:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=649
06/23/2022 11:30:27 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7755088049205696 on epoch=649
06/23/2022 11:30:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/23/2022 11:30:32 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
06/23/2022 11:30:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/23/2022 11:30:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
06/23/2022 11:30:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
06/23/2022 11:30:40 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.8053733900508094 on epoch=662
06/23/2022 11:30:42 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
06/23/2022 11:30:45 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=667
06/23/2022 11:30:47 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
06/23/2022 11:30:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
06/23/2022 11:30:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
06/23/2022 11:30:53 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.8237769855416914 on epoch=674
06/23/2022 11:30:55 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
06/23/2022 11:30:58 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/23/2022 11:31:00 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
06/23/2022 11:31:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=684
06/23/2022 11:31:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=687
06/23/2022 11:31:06 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.7930990578049403 on epoch=687
06/23/2022 11:31:08 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
06/23/2022 11:31:10 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=692
06/23/2022 11:31:13 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=694
06/23/2022 11:31:15 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
06/23/2022 11:31:18 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=699
06/23/2022 11:31:19 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.8074929971988796 on epoch=699
06/23/2022 11:31:21 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=702
06/23/2022 11:31:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
06/23/2022 11:31:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=707
06/23/2022 11:31:28 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/23/2022 11:31:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=712
06/23/2022 11:31:32 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.8388091347650171 on epoch=712
06/23/2022 11:31:32 - INFO - __main__ - Saving model with best Classification-F1: 0.825017139723022 -> 0.8388091347650171 on epoch=712, global_step=2850
06/23/2022 11:31:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
06/23/2022 11:31:37 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=717
06/23/2022 11:31:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
06/23/2022 11:31:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.10 on epoch=722
06/23/2022 11:31:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
06/23/2022 11:31:45 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.8081033549783551 on epoch=724
06/23/2022 11:31:47 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=727
06/23/2022 11:31:50 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
06/23/2022 11:31:52 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
06/23/2022 11:31:54 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/23/2022 11:31:57 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
06/23/2022 11:31:58 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.8074929971988796 on epoch=737
06/23/2022 11:32:00 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
06/23/2022 11:32:03 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
06/23/2022 11:32:05 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
06/23/2022 11:32:07 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
06/23/2022 11:32:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
06/23/2022 11:32:11 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.7904176093514329 on epoch=749
06/23/2022 11:32:11 - INFO - __main__ - save last model!
06/23/2022 11:32:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 11:32:11 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 11:32:11 - INFO - __main__ - Printing 3 examples
06/23/2022 11:32:11 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 11:32:11 - INFO - __main__ - ['others']
06/23/2022 11:32:11 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 11:32:11 - INFO - __main__ - ['others']
06/23/2022 11:32:11 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 11:32:11 - INFO - __main__ - ['others']
06/23/2022 11:32:11 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:32:11 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:32:11 - INFO - __main__ - Printing 3 examples
06/23/2022 11:32:11 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/23/2022 11:32:11 - INFO - __main__ - ['others']
06/23/2022 11:32:11 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/23/2022 11:32:11 - INFO - __main__ - ['others']
06/23/2022 11:32:11 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/23/2022 11:32:11 - INFO - __main__ - ['others']
06/23/2022 11:32:11 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:32:11 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:32:11 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 11:32:11 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:32:11 - INFO - __main__ - Printing 3 examples
06/23/2022 11:32:11 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/23/2022 11:32:11 - INFO - __main__ - ['others']
06/23/2022 11:32:11 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/23/2022 11:32:11 - INFO - __main__ - ['others']
06/23/2022 11:32:11 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/23/2022 11:32:11 - INFO - __main__ - ['others']
06/23/2022 11:32:11 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:32:11 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:32:12 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 11:32:13 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:32:18 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 11:32:30 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 11:32:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 11:32:31 - INFO - __main__ - Starting training!
06/23/2022 11:33:33 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_42_0.2_8_predictions.txt
06/23/2022 11:33:33 - INFO - __main__ - Classification-F1 on test data: 0.2216
06/23/2022 11:33:33 - INFO - __main__ - prefix=emo_16_42, lr=0.2, bsz=8, dev_performance=0.8388091347650171, test_performance=0.22156809407591388
06/23/2022 11:33:33 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.5, bsz=8 ...
06/23/2022 11:33:34 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:33:34 - INFO - __main__ - Printing 3 examples
06/23/2022 11:33:34 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/23/2022 11:33:34 - INFO - __main__ - ['others']
06/23/2022 11:33:34 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/23/2022 11:33:34 - INFO - __main__ - ['others']
06/23/2022 11:33:34 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/23/2022 11:33:34 - INFO - __main__ - ['others']
06/23/2022 11:33:34 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:33:34 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:33:34 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 11:33:34 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:33:34 - INFO - __main__ - Printing 3 examples
06/23/2022 11:33:34 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/23/2022 11:33:34 - INFO - __main__ - ['others']
06/23/2022 11:33:34 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/23/2022 11:33:34 - INFO - __main__ - ['others']
06/23/2022 11:33:34 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/23/2022 11:33:34 - INFO - __main__ - ['others']
06/23/2022 11:33:34 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:33:35 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:33:35 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 11:33:53 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 11:33:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 11:33:54 - INFO - __main__ - Starting training!
06/23/2022 11:33:57 - INFO - __main__ - Step 10 Global step 10 Train loss 3.98 on epoch=2
06/23/2022 11:34:00 - INFO - __main__ - Step 20 Global step 20 Train loss 3.06 on epoch=4
06/23/2022 11:34:02 - INFO - __main__ - Step 30 Global step 30 Train loss 2.22 on epoch=7
06/23/2022 11:34:05 - INFO - __main__ - Step 40 Global step 40 Train loss 1.64 on epoch=9
06/23/2022 11:34:07 - INFO - __main__ - Step 50 Global step 50 Train loss 1.43 on epoch=12
06/23/2022 11:34:08 - INFO - __main__ - Global step 50 Train loss 2.47 Classification-F1 0.18583308583308583 on epoch=12
06/23/2022 11:34:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18583308583308583 on epoch=12, global_step=50
06/23/2022 11:34:10 - INFO - __main__ - Step 60 Global step 60 Train loss 1.04 on epoch=14
06/23/2022 11:34:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.79 on epoch=17
06/23/2022 11:34:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.78 on epoch=19
06/23/2022 11:34:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.65 on epoch=22
06/23/2022 11:34:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.77 on epoch=24
06/23/2022 11:34:21 - INFO - __main__ - Global step 100 Train loss 0.81 Classification-F1 0.5102657004830918 on epoch=24
06/23/2022 11:34:21 - INFO - __main__ - Saving model with best Classification-F1: 0.18583308583308583 -> 0.5102657004830918 on epoch=24, global_step=100
06/23/2022 11:34:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.68 on epoch=27
06/23/2022 11:34:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.59 on epoch=29
06/23/2022 11:34:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.63 on epoch=32
06/23/2022 11:34:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=34
06/23/2022 11:34:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.57 on epoch=37
06/23/2022 11:34:34 - INFO - __main__ - Global step 150 Train loss 0.60 Classification-F1 0.5333333333333333 on epoch=37
06/23/2022 11:34:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5102657004830918 -> 0.5333333333333333 on epoch=37, global_step=150
06/23/2022 11:34:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=39
06/23/2022 11:34:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=42
06/23/2022 11:34:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.59 on epoch=44
06/23/2022 11:34:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=47
06/23/2022 11:34:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=49
06/23/2022 11:34:47 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.5546594982078853 on epoch=49
06/23/2022 11:34:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5333333333333333 -> 0.5546594982078853 on epoch=49, global_step=200
06/23/2022 11:34:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=52
06/23/2022 11:34:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=54
06/23/2022 11:34:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.33 on epoch=57
06/23/2022 11:34:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=59
06/23/2022 11:34:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=62
06/23/2022 11:35:00 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.6606437969924812 on epoch=62
06/23/2022 11:35:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5546594982078853 -> 0.6606437969924812 on epoch=62, global_step=250
06/23/2022 11:35:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=64
06/23/2022 11:35:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=67
06/23/2022 11:35:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=69
06/23/2022 11:35:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=72
06/23/2022 11:35:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=74
06/23/2022 11:35:13 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.645490407924244 on epoch=74
06/23/2022 11:35:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=77
06/23/2022 11:35:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=79
06/23/2022 11:35:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=82
06/23/2022 11:35:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=84
06/23/2022 11:35:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=87
06/23/2022 11:35:26 - INFO - __main__ - Global step 350 Train loss 0.34 Classification-F1 0.6606437969924812 on epoch=87
06/23/2022 11:35:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=89
06/23/2022 11:35:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=92
06/23/2022 11:35:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=94
06/23/2022 11:35:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=97
06/23/2022 11:35:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=99
06/23/2022 11:35:39 - INFO - __main__ - Global step 400 Train loss 0.29 Classification-F1 0.6857788791791661 on epoch=99
06/23/2022 11:35:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6606437969924812 -> 0.6857788791791661 on epoch=99, global_step=400
06/23/2022 11:35:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=102
06/23/2022 11:35:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=104
06/23/2022 11:35:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
06/23/2022 11:35:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=109
06/23/2022 11:35:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=112
06/23/2022 11:35:52 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.6704268292682927 on epoch=112
06/23/2022 11:35:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.19 on epoch=114
06/23/2022 11:35:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=117
06/23/2022 11:35:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=119
06/23/2022 11:36:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=122
06/23/2022 11:36:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=124
06/23/2022 11:36:04 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.6450187969924812 on epoch=124
06/23/2022 11:36:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.17 on epoch=127
06/23/2022 11:36:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=129
06/23/2022 11:36:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.17 on epoch=132
06/23/2022 11:36:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.15 on epoch=134
06/23/2022 11:36:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.10 on epoch=137
06/23/2022 11:36:17 - INFO - __main__ - Global step 550 Train loss 0.16 Classification-F1 0.7084293394777267 on epoch=137
06/23/2022 11:36:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6857788791791661 -> 0.7084293394777267 on epoch=137, global_step=550
06/23/2022 11:36:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=139
06/23/2022 11:36:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=142
06/23/2022 11:36:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=144
06/23/2022 11:36:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.16 on epoch=147
06/23/2022 11:36:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=149
06/23/2022 11:36:30 - INFO - __main__ - Global step 600 Train loss 0.15 Classification-F1 0.7156362766836225 on epoch=149
06/23/2022 11:36:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7084293394777267 -> 0.7156362766836225 on epoch=149, global_step=600
06/23/2022 11:36:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=152
06/23/2022 11:36:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=154
06/23/2022 11:36:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=157
06/23/2022 11:36:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=159
06/23/2022 11:36:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=162
06/23/2022 11:36:43 - INFO - __main__ - Global step 650 Train loss 0.14 Classification-F1 0.7147073647073647 on epoch=162
06/23/2022 11:36:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=164
06/23/2022 11:36:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=167
06/23/2022 11:36:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=169
06/23/2022 11:36:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=172
06/23/2022 11:36:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=174
06/23/2022 11:36:56 - INFO - __main__ - Global step 700 Train loss 0.13 Classification-F1 0.6925836550836552 on epoch=174
06/23/2022 11:36:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=177
06/23/2022 11:37:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=179
06/23/2022 11:37:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=182
06/23/2022 11:37:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=184
06/23/2022 11:37:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=187
06/23/2022 11:37:09 - INFO - __main__ - Global step 750 Train loss 0.10 Classification-F1 0.7392570664629489 on epoch=187
06/23/2022 11:37:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7156362766836225 -> 0.7392570664629489 on epoch=187, global_step=750
06/23/2022 11:37:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=189
06/23/2022 11:37:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=192
06/23/2022 11:37:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=194
06/23/2022 11:37:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=197
06/23/2022 11:37:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=199
06/23/2022 11:37:22 - INFO - __main__ - Global step 800 Train loss 0.09 Classification-F1 0.7147073647073647 on epoch=199
06/23/2022 11:37:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=202
06/23/2022 11:37:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=204
06/23/2022 11:37:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=207
06/23/2022 11:37:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=209
06/23/2022 11:37:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=212
06/23/2022 11:37:35 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.7065915915915917 on epoch=212
06/23/2022 11:37:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=214
06/23/2022 11:37:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=217
06/23/2022 11:37:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=219
06/23/2022 11:37:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=222
06/23/2022 11:37:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=224
06/23/2022 11:37:48 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.7165270293545927 on epoch=224
06/23/2022 11:37:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=227
06/23/2022 11:37:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=229
06/23/2022 11:37:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=232
06/23/2022 11:37:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=234
06/23/2022 11:38:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=237
06/23/2022 11:38:01 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.7613807760866584 on epoch=237
06/23/2022 11:38:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7392570664629489 -> 0.7613807760866584 on epoch=237, global_step=950
06/23/2022 11:38:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=239
06/23/2022 11:38:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
06/23/2022 11:38:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
06/23/2022 11:38:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=247
06/23/2022 11:38:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=249
06/23/2022 11:38:14 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.7210585585585586 on epoch=249
06/23/2022 11:38:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=252
06/23/2022 11:38:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=254
06/23/2022 11:38:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=257
06/23/2022 11:38:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=259
06/23/2022 11:38:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=262
06/23/2022 11:38:26 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.7563261648745521 on epoch=262
06/23/2022 11:38:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
06/23/2022 11:38:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=267
06/23/2022 11:38:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=269
06/23/2022 11:38:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
06/23/2022 11:38:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
06/23/2022 11:38:39 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.8124887626675384 on epoch=274
06/23/2022 11:38:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7613807760866584 -> 0.8124887626675384 on epoch=274, global_step=1100
06/23/2022 11:38:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
06/23/2022 11:38:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=279
06/23/2022 11:38:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
06/23/2022 11:38:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=284
06/23/2022 11:38:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
06/23/2022 11:38:52 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.746082042957043 on epoch=287
06/23/2022 11:38:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
06/23/2022 11:38:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
06/23/2022 11:39:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=294
06/23/2022 11:39:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
06/23/2022 11:39:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
06/23/2022 11:39:05 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.7365659275625345 on epoch=299
06/23/2022 11:39:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
06/23/2022 11:39:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
06/23/2022 11:39:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=307
06/23/2022 11:39:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=309
06/23/2022 11:39:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
06/23/2022 11:39:18 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.7974032966629547 on epoch=312
06/23/2022 11:39:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=314
06/23/2022 11:39:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
06/23/2022 11:39:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
06/23/2022 11:39:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
06/23/2022 11:39:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
06/23/2022 11:39:31 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.7669530483818241 on epoch=324
06/23/2022 11:39:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
06/23/2022 11:39:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
06/23/2022 11:39:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
06/23/2022 11:39:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
06/23/2022 11:39:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
06/23/2022 11:39:44 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.7718551587301588 on epoch=337
06/23/2022 11:39:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
06/23/2022 11:39:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
06/23/2022 11:39:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=344
06/23/2022 11:39:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
06/23/2022 11:39:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
06/23/2022 11:39:57 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.8023054070112894 on epoch=349
06/23/2022 11:39:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
06/23/2022 11:40:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=354
06/23/2022 11:40:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
06/23/2022 11:40:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
06/23/2022 11:40:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
06/23/2022 11:40:10 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.8023054070112894 on epoch=362
06/23/2022 11:40:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=364
06/23/2022 11:40:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
06/23/2022 11:40:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
06/23/2022 11:40:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
06/23/2022 11:40:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
06/23/2022 11:40:23 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7974032966629547 on epoch=374
06/23/2022 11:40:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
06/23/2022 11:40:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
06/23/2022 11:40:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
06/23/2022 11:40:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
06/23/2022 11:40:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=387
06/23/2022 11:40:36 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7765322912381736 on epoch=387
06/23/2022 11:40:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
06/23/2022 11:40:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
06/23/2022 11:40:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
06/23/2022 11:40:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
06/23/2022 11:40:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
06/23/2022 11:40:49 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.7369042429526301 on epoch=399
06/23/2022 11:40:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=402
06/23/2022 11:40:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
06/23/2022 11:40:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
06/23/2022 11:40:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/23/2022 11:41:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
06/23/2022 11:41:02 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7775917374959137 on epoch=412
06/23/2022 11:41:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=414
06/23/2022 11:41:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
06/23/2022 11:41:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=419
06/23/2022 11:41:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
06/23/2022 11:41:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
06/23/2022 11:41:15 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.7563261648745521 on epoch=424
06/23/2022 11:41:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
06/23/2022 11:41:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/23/2022 11:41:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/23/2022 11:41:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
06/23/2022 11:41:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
06/23/2022 11:41:28 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7916177572427572 on epoch=437
06/23/2022 11:41:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
06/23/2022 11:41:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
06/23/2022 11:41:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
06/23/2022 11:41:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
06/23/2022 11:41:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
06/23/2022 11:41:41 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7822517815114396 on epoch=449
06/23/2022 11:41:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
06/23/2022 11:41:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
06/23/2022 11:41:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/23/2022 11:41:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
06/23/2022 11:41:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=462
06/23/2022 11:41:54 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.8172303319362143 on epoch=462
06/23/2022 11:41:54 - INFO - __main__ - Saving model with best Classification-F1: 0.8124887626675384 -> 0.8172303319362143 on epoch=462, global_step=1850
06/23/2022 11:41:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
06/23/2022 11:41:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
06/23/2022 11:42:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
06/23/2022 11:42:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
06/23/2022 11:42:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
06/23/2022 11:42:07 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7765322912381736 on epoch=474
06/23/2022 11:42:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
06/23/2022 11:42:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
06/23/2022 11:42:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
06/23/2022 11:42:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
06/23/2022 11:42:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
06/23/2022 11:42:20 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.746082042957043 on epoch=487
06/23/2022 11:42:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
06/23/2022 11:42:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
06/23/2022 11:42:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=494
06/23/2022 11:42:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/23/2022 11:42:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.14 on epoch=499
06/23/2022 11:42:33 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.7613807760866584 on epoch=499
06/23/2022 11:42:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
06/23/2022 11:42:37 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
06/23/2022 11:42:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
06/23/2022 11:42:42 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
06/23/2022 11:42:45 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
06/23/2022 11:42:46 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7305530491014363 on epoch=512
06/23/2022 11:42:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
06/23/2022 11:42:51 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
06/23/2022 11:42:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
06/23/2022 11:42:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
06/23/2022 11:42:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
06/23/2022 11:42:59 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7305530491014363 on epoch=524
06/23/2022 11:43:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
06/23/2022 11:43:04 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
06/23/2022 11:43:06 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=532
06/23/2022 11:43:08 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=534
06/23/2022 11:43:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
06/23/2022 11:43:12 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.7514240545262174 on epoch=537
06/23/2022 11:43:14 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/23/2022 11:43:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/23/2022 11:43:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
06/23/2022 11:43:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/23/2022 11:43:24 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
06/23/2022 11:43:25 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7305530491014363 on epoch=549
06/23/2022 11:43:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/23/2022 11:43:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/23/2022 11:43:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
06/23/2022 11:43:34 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
06/23/2022 11:43:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/23/2022 11:43:38 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7765322912381736 on epoch=562
06/23/2022 11:43:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
06/23/2022 11:43:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
06/23/2022 11:43:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/23/2022 11:43:47 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/23/2022 11:43:50 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
06/23/2022 11:43:51 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7669530483818241 on epoch=574
06/23/2022 11:43:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/23/2022 11:43:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/23/2022 11:43:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=582
06/23/2022 11:44:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
06/23/2022 11:44:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
06/23/2022 11:44:04 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7765322912381736 on epoch=587
06/23/2022 11:44:06 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/23/2022 11:44:09 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/23/2022 11:44:11 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/23/2022 11:44:13 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/23/2022 11:44:16 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/23/2022 11:44:17 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.746082042957043 on epoch=599
06/23/2022 11:44:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/23/2022 11:44:22 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=604
06/23/2022 11:44:24 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/23/2022 11:44:26 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/23/2022 11:44:29 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
06/23/2022 11:44:30 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.7531523228297422 on epoch=612
06/23/2022 11:44:32 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/23/2022 11:44:35 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=617
06/23/2022 11:44:37 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
06/23/2022 11:44:39 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=622
06/23/2022 11:44:42 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/23/2022 11:44:43 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7310823051312181 on epoch=624
06/23/2022 11:44:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
06/23/2022 11:44:48 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/23/2022 11:44:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
06/23/2022 11:44:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/23/2022 11:44:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/23/2022 11:44:56 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.76250627149133 on epoch=637
06/23/2022 11:44:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/23/2022 11:45:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/23/2022 11:45:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/23/2022 11:45:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/23/2022 11:45:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/23/2022 11:45:09 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.746082042957043 on epoch=649
06/23/2022 11:45:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/23/2022 11:45:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/23/2022 11:45:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/23/2022 11:45:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/23/2022 11:45:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/23/2022 11:45:22 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.7321999418773613 on epoch=662
06/23/2022 11:45:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/23/2022 11:45:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/23/2022 11:45:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/23/2022 11:45:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
06/23/2022 11:45:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/23/2022 11:45:34 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.7310823051312181 on epoch=674
06/23/2022 11:45:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/23/2022 11:45:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/23/2022 11:45:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/23/2022 11:45:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
06/23/2022 11:45:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/23/2022 11:45:47 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7301822573561705 on epoch=687
06/23/2022 11:45:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/23/2022 11:45:52 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/23/2022 11:45:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
06/23/2022 11:45:57 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/23/2022 11:45:59 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/23/2022 11:46:00 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7443693693693694 on epoch=699
06/23/2022 11:46:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/23/2022 11:46:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/23/2022 11:46:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/23/2022 11:46:10 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/23/2022 11:46:12 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/23/2022 11:46:13 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7441945441945441 on epoch=712
06/23/2022 11:46:16 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/23/2022 11:46:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/23/2022 11:46:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/23/2022 11:46:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/23/2022 11:46:25 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/23/2022 11:46:26 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7448940417690417 on epoch=724
06/23/2022 11:46:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/23/2022 11:46:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/23/2022 11:46:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/23/2022 11:46:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/23/2022 11:46:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/23/2022 11:46:39 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7592344062932299 on epoch=737
06/23/2022 11:46:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
06/23/2022 11:46:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/23/2022 11:46:46 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/23/2022 11:46:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/23/2022 11:46:51 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/23/2022 11:46:52 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7443693693693694 on epoch=749
06/23/2022 11:46:52 - INFO - __main__ - save last model!
06/23/2022 11:46:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 11:46:52 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 11:46:52 - INFO - __main__ - Printing 3 examples
06/23/2022 11:46:52 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 11:46:52 - INFO - __main__ - ['others']
06/23/2022 11:46:52 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 11:46:52 - INFO - __main__ - ['others']
06/23/2022 11:46:52 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 11:46:52 - INFO - __main__ - ['others']
06/23/2022 11:46:52 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:46:52 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:46:52 - INFO - __main__ - Printing 3 examples
06/23/2022 11:46:52 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/23/2022 11:46:52 - INFO - __main__ - ['others']
06/23/2022 11:46:52 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/23/2022 11:46:52 - INFO - __main__ - ['others']
06/23/2022 11:46:52 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/23/2022 11:46:52 - INFO - __main__ - ['others']
06/23/2022 11:46:52 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:46:52 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:46:53 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 11:46:53 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:46:53 - INFO - __main__ - Printing 3 examples
06/23/2022 11:46:53 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/23/2022 11:46:53 - INFO - __main__ - ['others']
06/23/2022 11:46:53 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/23/2022 11:46:53 - INFO - __main__ - ['others']
06/23/2022 11:46:53 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/23/2022 11:46:53 - INFO - __main__ - ['others']
06/23/2022 11:46:53 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:46:53 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:46:53 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 11:46:54 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:47:00 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 11:47:11 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 11:47:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 11:47:12 - INFO - __main__ - Starting training!
06/23/2022 11:48:32 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_87_0.5_8_predictions.txt
06/23/2022 11:48:32 - INFO - __main__ - Classification-F1 on test data: 0.1584
06/23/2022 11:48:33 - INFO - __main__ - prefix=emo_16_87, lr=0.5, bsz=8, dev_performance=0.8172303319362143, test_performance=0.15841407441802316
06/23/2022 11:48:33 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.4, bsz=8 ...
06/23/2022 11:48:34 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:48:34 - INFO - __main__ - Printing 3 examples
06/23/2022 11:48:34 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/23/2022 11:48:34 - INFO - __main__ - ['others']
06/23/2022 11:48:34 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/23/2022 11:48:34 - INFO - __main__ - ['others']
06/23/2022 11:48:34 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/23/2022 11:48:34 - INFO - __main__ - ['others']
06/23/2022 11:48:34 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:48:34 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:48:34 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 11:48:34 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 11:48:34 - INFO - __main__ - Printing 3 examples
06/23/2022 11:48:34 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/23/2022 11:48:34 - INFO - __main__ - ['others']
06/23/2022 11:48:34 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/23/2022 11:48:34 - INFO - __main__ - ['others']
06/23/2022 11:48:34 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/23/2022 11:48:34 - INFO - __main__ - ['others']
06/23/2022 11:48:34 - INFO - __main__ - Tokenizing Input ...
06/23/2022 11:48:34 - INFO - __main__ - Tokenizing Output ...
06/23/2022 11:48:34 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 11:48:53 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 11:48:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 11:48:54 - INFO - __main__ - Starting training!
06/23/2022 11:48:57 - INFO - __main__ - Step 10 Global step 10 Train loss 4.19 on epoch=2
06/23/2022 11:48:59 - INFO - __main__ - Step 20 Global step 20 Train loss 3.04 on epoch=4
06/23/2022 11:49:01 - INFO - __main__ - Step 30 Global step 30 Train loss 2.50 on epoch=7
06/23/2022 11:49:04 - INFO - __main__ - Step 40 Global step 40 Train loss 1.87 on epoch=9
06/23/2022 11:49:06 - INFO - __main__ - Step 50 Global step 50 Train loss 1.51 on epoch=12
06/23/2022 11:49:07 - INFO - __main__ - Global step 50 Train loss 2.62 Classification-F1 0.24039361075499807 on epoch=12
06/23/2022 11:49:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.24039361075499807 on epoch=12, global_step=50
06/23/2022 11:49:10 - INFO - __main__ - Step 60 Global step 60 Train loss 1.22 on epoch=14
06/23/2022 11:49:12 - INFO - __main__ - Step 70 Global step 70 Train loss 1.05 on epoch=17
06/23/2022 11:49:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.91 on epoch=19
06/23/2022 11:49:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=22
06/23/2022 11:49:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.76 on epoch=24
06/23/2022 11:49:20 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.5113890664961637 on epoch=24
06/23/2022 11:49:20 - INFO - __main__ - Saving model with best Classification-F1: 0.24039361075499807 -> 0.5113890664961637 on epoch=24, global_step=100
06/23/2022 11:49:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.57 on epoch=27
06/23/2022 11:49:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.71 on epoch=29
06/23/2022 11:49:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.68 on epoch=32
06/23/2022 11:49:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.66 on epoch=34
06/23/2022 11:49:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=37
06/23/2022 11:49:33 - INFO - __main__ - Global step 150 Train loss 0.64 Classification-F1 0.5170658194851743 on epoch=37
06/23/2022 11:49:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5113890664961637 -> 0.5170658194851743 on epoch=37, global_step=150
06/23/2022 11:49:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=39
06/23/2022 11:49:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.58 on epoch=42
06/23/2022 11:49:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=44
06/23/2022 11:49:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=47
06/23/2022 11:49:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.53 on epoch=49
06/23/2022 11:49:46 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.5891437415630963 on epoch=49
06/23/2022 11:49:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5170658194851743 -> 0.5891437415630963 on epoch=49, global_step=200
06/23/2022 11:49:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.55 on epoch=52
06/23/2022 11:49:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=54
06/23/2022 11:49:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=57
06/23/2022 11:49:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=59
06/23/2022 11:49:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=62
06/23/2022 11:49:59 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.6455116285336998 on epoch=62
06/23/2022 11:49:59 - INFO - __main__ - Saving model with best Classification-F1: 0.5891437415630963 -> 0.6455116285336998 on epoch=62, global_step=250
06/23/2022 11:50:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=64
06/23/2022 11:50:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=67
06/23/2022 11:50:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=69
06/23/2022 11:50:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=72
06/23/2022 11:50:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=74
06/23/2022 11:50:12 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.6446117739753087 on epoch=74
06/23/2022 11:50:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=77
06/23/2022 11:50:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=79
06/23/2022 11:50:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=82
06/23/2022 11:50:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=84
06/23/2022 11:50:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=87
06/23/2022 11:50:25 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.6850694444444444 on epoch=87
06/23/2022 11:50:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6455116285336998 -> 0.6850694444444444 on epoch=87, global_step=350
06/23/2022 11:50:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.30 on epoch=89
06/23/2022 11:50:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=92
06/23/2022 11:50:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=94
06/23/2022 11:50:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=97
06/23/2022 11:50:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=99
06/23/2022 11:50:39 - INFO - __main__ - Global step 400 Train loss 0.31 Classification-F1 0.644558180373328 on epoch=99
06/23/2022 11:50:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=102
06/23/2022 11:50:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=104
06/23/2022 11:50:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=107
06/23/2022 11:50:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=109
06/23/2022 11:50:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=112
06/23/2022 11:50:52 - INFO - __main__ - Global step 450 Train loss 0.30 Classification-F1 0.6594436421337897 on epoch=112
06/23/2022 11:50:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=114
06/23/2022 11:50:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=117
06/23/2022 11:51:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=119
06/23/2022 11:51:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=122
06/23/2022 11:51:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=124
06/23/2022 11:51:06 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.6739954098105574 on epoch=124
06/23/2022 11:51:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=127
06/23/2022 11:51:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=129
06/23/2022 11:51:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=132
06/23/2022 11:51:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.14 on epoch=134
06/23/2022 11:51:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=137
06/23/2022 11:51:19 - INFO - __main__ - Global step 550 Train loss 0.19 Classification-F1 0.6739954098105574 on epoch=137
06/23/2022 11:51:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.14 on epoch=139
06/23/2022 11:51:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.17 on epoch=142
06/23/2022 11:51:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=144
06/23/2022 11:51:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=147
06/23/2022 11:51:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=149
06/23/2022 11:51:33 - INFO - __main__ - Global step 600 Train loss 0.16 Classification-F1 0.6742543197468868 on epoch=149
06/23/2022 11:51:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=152
06/23/2022 11:51:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=154
06/23/2022 11:51:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=157
06/23/2022 11:51:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=159
06/23/2022 11:51:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=162
06/23/2022 11:51:46 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.7290584415584416 on epoch=162
06/23/2022 11:51:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6850694444444444 -> 0.7290584415584416 on epoch=162, global_step=650
06/23/2022 11:51:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=164
06/23/2022 11:51:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=167
06/23/2022 11:51:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=169
06/23/2022 11:51:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=172
06/23/2022 11:51:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=174
06/23/2022 11:52:00 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.7034326392477869 on epoch=174
06/23/2022 11:52:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=177
06/23/2022 11:52:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=179
06/23/2022 11:52:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=182
06/23/2022 11:52:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=184
06/23/2022 11:52:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=187
06/23/2022 11:52:13 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.752913752913753 on epoch=187
06/23/2022 11:52:13 - INFO - __main__ - Saving model with best Classification-F1: 0.7290584415584416 -> 0.752913752913753 on epoch=187, global_step=750
06/23/2022 11:52:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=189
06/23/2022 11:52:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=192
06/23/2022 11:52:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=194
06/23/2022 11:52:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=197
06/23/2022 11:52:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.11 on epoch=199
06/23/2022 11:52:27 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.6866154356479435 on epoch=199
06/23/2022 11:52:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=202
06/23/2022 11:52:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=204
06/23/2022 11:52:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=207
06/23/2022 11:52:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=209
06/23/2022 11:52:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=212
06/23/2022 11:52:40 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.671750398724083 on epoch=212
06/23/2022 11:52:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=214
06/23/2022 11:52:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.17 on epoch=217
06/23/2022 11:52:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=219
06/23/2022 11:52:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=222
06/23/2022 11:52:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=224
06/23/2022 11:52:54 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.700337543554007 on epoch=224
06/23/2022 11:52:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=227
06/23/2022 11:52:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=229
06/23/2022 11:53:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=232
06/23/2022 11:53:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=234
06/23/2022 11:53:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=237
06/23/2022 11:53:07 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.7239583333333334 on epoch=237
06/23/2022 11:53:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=239
06/23/2022 11:53:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
06/23/2022 11:53:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
06/23/2022 11:53:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=247
06/23/2022 11:53:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=249
06/23/2022 11:53:20 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.7449874686716793 on epoch=249
06/23/2022 11:53:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
06/23/2022 11:53:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=254
06/23/2022 11:53:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=257
06/23/2022 11:53:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
06/23/2022 11:53:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=262
06/23/2022 11:53:34 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.8165584415584416 on epoch=262
06/23/2022 11:53:34 - INFO - __main__ - Saving model with best Classification-F1: 0.752913752913753 -> 0.8165584415584416 on epoch=262, global_step=1050
06/23/2022 11:53:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=264
06/23/2022 11:53:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
06/23/2022 11:53:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
06/23/2022 11:53:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
06/23/2022 11:53:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=274
06/23/2022 11:53:47 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.6987554112554113 on epoch=274
06/23/2022 11:53:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
06/23/2022 11:53:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=279
06/23/2022 11:53:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=282
06/23/2022 11:53:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=284
06/23/2022 11:54:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
06/23/2022 11:54:01 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.6976107226107227 on epoch=287
06/23/2022 11:54:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
06/23/2022 11:54:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
06/23/2022 11:54:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
06/23/2022 11:54:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
06/23/2022 11:54:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=299
06/23/2022 11:54:14 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.721719070403281 on epoch=299
06/23/2022 11:54:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
06/23/2022 11:54:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=304
06/23/2022 11:54:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=307
06/23/2022 11:54:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
06/23/2022 11:54:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
06/23/2022 11:54:28 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.7443693693693694 on epoch=312
06/23/2022 11:54:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
06/23/2022 11:54:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
06/23/2022 11:54:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
06/23/2022 11:54:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
06/23/2022 11:54:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
06/23/2022 11:54:42 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.7449874686716793 on epoch=324
06/23/2022 11:54:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
06/23/2022 11:54:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=329
06/23/2022 11:54:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
06/23/2022 11:54:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
06/23/2022 11:54:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
06/23/2022 11:54:55 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.7228136446886447 on epoch=337
06/23/2022 11:54:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
06/23/2022 11:55:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=342
06/23/2022 11:55:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
06/23/2022 11:55:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
06/23/2022 11:55:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=349
06/23/2022 11:55:09 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6987554112554113 on epoch=349
06/23/2022 11:55:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=352
06/23/2022 11:55:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
06/23/2022 11:55:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
06/23/2022 11:55:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
06/23/2022 11:55:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
06/23/2022 11:55:22 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.7239583333333334 on epoch=362
06/23/2022 11:55:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
06/23/2022 11:55:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
06/23/2022 11:55:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
06/23/2022 11:55:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
06/23/2022 11:55:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
06/23/2022 11:55:35 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7765322912381736 on epoch=374
06/23/2022 11:55:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
06/23/2022 11:55:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
06/23/2022 11:55:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
06/23/2022 11:55:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
06/23/2022 11:55:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
06/23/2022 11:55:49 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7392570664629489 on epoch=387
06/23/2022 11:55:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
06/23/2022 11:55:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
06/23/2022 11:55:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
06/23/2022 11:55:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
06/23/2022 11:56:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
06/23/2022 11:56:02 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.7669530483818241 on epoch=399
06/23/2022 11:56:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/23/2022 11:56:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
06/23/2022 11:56:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
06/23/2022 11:56:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
06/23/2022 11:56:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
06/23/2022 11:56:16 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.7228136446886447 on epoch=412
06/23/2022 11:56:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
06/23/2022 11:56:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
06/23/2022 11:56:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
06/23/2022 11:56:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
06/23/2022 11:56:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
06/23/2022 11:56:29 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.7749085280045033 on epoch=424
06/23/2022 11:56:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
06/23/2022 11:56:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/23/2022 11:56:37 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=432
06/23/2022 11:56:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
06/23/2022 11:56:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
06/23/2022 11:56:43 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.7377622377622378 on epoch=437
06/23/2022 11:56:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
06/23/2022 11:56:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
06/23/2022 11:56:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
06/23/2022 11:56:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
06/23/2022 11:56:56 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
06/23/2022 11:56:57 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7451903907496013 on epoch=449
06/23/2022 11:56:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
06/23/2022 11:57:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
06/23/2022 11:57:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
06/23/2022 11:57:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
06/23/2022 11:57:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
06/23/2022 11:57:10 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.7867800836550837 on epoch=462
06/23/2022 11:57:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/23/2022 11:57:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
06/23/2022 11:57:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
06/23/2022 11:57:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
06/23/2022 11:57:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
06/23/2022 11:57:24 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7228136446886447 on epoch=474
06/23/2022 11:57:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
06/23/2022 11:57:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
06/23/2022 11:57:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=482
06/23/2022 11:57:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
06/23/2022 11:57:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
06/23/2022 11:57:37 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7228136446886447 on epoch=487
06/23/2022 11:57:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
06/23/2022 11:57:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
06/23/2022 11:57:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
06/23/2022 11:57:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/23/2022 11:57:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
06/23/2022 11:57:51 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6976107226107227 on epoch=499
06/23/2022 11:57:54 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
06/23/2022 11:57:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
06/23/2022 11:57:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=507
06/23/2022 11:58:01 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
06/23/2022 11:58:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/23/2022 11:58:05 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7290584415584416 on epoch=512
06/23/2022 11:58:07 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/23/2022 11:58:10 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/23/2022 11:58:12 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
06/23/2022 11:58:15 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
06/23/2022 11:58:17 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/23/2022 11:58:18 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.682510395010395 on epoch=524
06/23/2022 11:58:21 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
06/23/2022 11:58:23 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/23/2022 11:58:26 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/23/2022 11:58:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
06/23/2022 11:58:31 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/23/2022 11:58:32 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7066187428029533 on epoch=537
06/23/2022 11:58:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
06/23/2022 11:58:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/23/2022 11:58:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
06/23/2022 11:58:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
06/23/2022 11:58:44 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/23/2022 11:58:45 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7299831472157449 on epoch=549
06/23/2022 11:58:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/23/2022 11:58:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/23/2022 11:58:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
06/23/2022 11:58:56 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
06/23/2022 11:58:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/23/2022 11:58:59 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.7292690417690417 on epoch=562
06/23/2022 11:59:02 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
06/23/2022 11:59:04 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
06/23/2022 11:59:07 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
06/23/2022 11:59:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
06/23/2022 11:59:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/23/2022 11:59:13 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7451903907496013 on epoch=574
06/23/2022 11:59:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/23/2022 11:59:18 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/23/2022 11:59:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/23/2022 11:59:23 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
06/23/2022 11:59:26 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
06/23/2022 11:59:27 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7749085280045033 on epoch=587
06/23/2022 11:59:29 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/23/2022 11:59:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
06/23/2022 11:59:34 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/23/2022 11:59:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/23/2022 11:59:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/23/2022 11:59:40 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.721719070403281 on epoch=599
06/23/2022 11:59:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=602
06/23/2022 11:59:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/23/2022 11:59:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/23/2022 11:59:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/23/2022 11:59:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/23/2022 11:59:54 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.773940288646171 on epoch=612
06/23/2022 11:59:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/23/2022 11:59:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
06/23/2022 12:00:01 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/23/2022 12:00:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/23/2022 12:00:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/23/2022 12:00:07 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7271816418875242 on epoch=624
06/23/2022 12:00:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/23/2022 12:00:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
06/23/2022 12:00:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/23/2022 12:00:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=634
06/23/2022 12:00:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/23/2022 12:00:21 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7271816418875242 on epoch=637
06/23/2022 12:00:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/23/2022 12:00:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/23/2022 12:00:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
06/23/2022 12:00:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/23/2022 12:00:34 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/23/2022 12:00:34 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7124757595345831 on epoch=649
06/23/2022 12:00:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/23/2022 12:00:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/23/2022 12:00:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
06/23/2022 12:00:45 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/23/2022 12:00:47 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
06/23/2022 12:00:48 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7512899896800825 on epoch=662
06/23/2022 12:00:51 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/23/2022 12:00:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/23/2022 12:00:56 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
06/23/2022 12:00:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=672
06/23/2022 12:01:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/23/2022 12:01:02 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7512899896800825 on epoch=674
06/23/2022 12:01:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/23/2022 12:01:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/23/2022 12:01:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/23/2022 12:01:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/23/2022 12:01:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/23/2022 12:01:15 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7365841073271414 on epoch=687
06/23/2022 12:01:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
06/23/2022 12:01:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/23/2022 12:01:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/23/2022 12:01:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/23/2022 12:01:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/23/2022 12:01:29 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7813472039814752 on epoch=699
06/23/2022 12:01:31 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/23/2022 12:01:34 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/23/2022 12:01:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/23/2022 12:01:39 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=709
06/23/2022 12:01:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/23/2022 12:01:43 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7271816418875242 on epoch=712
06/23/2022 12:01:45 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/23/2022 12:01:48 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/23/2022 12:01:50 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/23/2022 12:01:53 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/23/2022 12:01:55 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/23/2022 12:01:56 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7813472039814752 on epoch=724
06/23/2022 12:01:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/23/2022 12:02:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/23/2022 12:02:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/23/2022 12:02:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
06/23/2022 12:02:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/23/2022 12:02:10 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7964350573046225 on epoch=737
06/23/2022 12:02:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
06/23/2022 12:02:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/23/2022 12:02:17 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
06/23/2022 12:02:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
06/23/2022 12:02:22 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/23/2022 12:02:23 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.752913752913753 on epoch=749
06/23/2022 12:02:23 - INFO - __main__ - save last model!
06/23/2022 12:02:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 12:02:24 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 12:02:24 - INFO - __main__ - Printing 3 examples
06/23/2022 12:02:24 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 12:02:24 - INFO - __main__ - ['others']
06/23/2022 12:02:24 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 12:02:24 - INFO - __main__ - ['others']
06/23/2022 12:02:24 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 12:02:24 - INFO - __main__ - ['others']
06/23/2022 12:02:24 - INFO - __main__ - Tokenizing Input ...
06/23/2022 12:02:24 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 12:02:24 - INFO - __main__ - Printing 3 examples
06/23/2022 12:02:24 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/23/2022 12:02:24 - INFO - __main__ - ['others']
06/23/2022 12:02:24 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/23/2022 12:02:24 - INFO - __main__ - ['others']
06/23/2022 12:02:24 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/23/2022 12:02:24 - INFO - __main__ - ['others']
06/23/2022 12:02:24 - INFO - __main__ - Tokenizing Input ...
06/23/2022 12:02:24 - INFO - __main__ - Tokenizing Output ...
06/23/2022 12:02:24 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 12:02:24 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 12:02:24 - INFO - __main__ - Printing 3 examples
06/23/2022 12:02:24 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/23/2022 12:02:24 - INFO - __main__ - ['others']
06/23/2022 12:02:24 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/23/2022 12:02:24 - INFO - __main__ - ['others']
06/23/2022 12:02:24 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/23/2022 12:02:24 - INFO - __main__ - ['others']
06/23/2022 12:02:24 - INFO - __main__ - Tokenizing Input ...
06/23/2022 12:02:24 - INFO - __main__ - Tokenizing Output ...
06/23/2022 12:02:24 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 12:02:26 - INFO - __main__ - Tokenizing Output ...
06/23/2022 12:02:31 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 12:02:43 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 12:02:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 12:02:44 - INFO - __main__ - Starting training!
06/23/2022 12:03:49 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_87_0.4_8_predictions.txt
06/23/2022 12:03:50 - INFO - __main__ - Classification-F1 on test data: 0.2428
06/23/2022 12:03:50 - INFO - __main__ - prefix=emo_16_87, lr=0.4, bsz=8, dev_performance=0.8165584415584416, test_performance=0.24280785888695
06/23/2022 12:03:50 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.3, bsz=8 ...
06/23/2022 12:03:51 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 12:03:51 - INFO - __main__ - Printing 3 examples
06/23/2022 12:03:51 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/23/2022 12:03:51 - INFO - __main__ - ['others']
06/23/2022 12:03:51 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/23/2022 12:03:51 - INFO - __main__ - ['others']
06/23/2022 12:03:51 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/23/2022 12:03:51 - INFO - __main__ - ['others']
06/23/2022 12:03:51 - INFO - __main__ - Tokenizing Input ...
06/23/2022 12:03:51 - INFO - __main__ - Tokenizing Output ...
06/23/2022 12:03:51 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 12:03:51 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 12:03:51 - INFO - __main__ - Printing 3 examples
06/23/2022 12:03:51 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/23/2022 12:03:51 - INFO - __main__ - ['others']
06/23/2022 12:03:51 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/23/2022 12:03:51 - INFO - __main__ - ['others']
06/23/2022 12:03:51 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/23/2022 12:03:51 - INFO - __main__ - ['others']
06/23/2022 12:03:51 - INFO - __main__ - Tokenizing Input ...
06/23/2022 12:03:51 - INFO - __main__ - Tokenizing Output ...
06/23/2022 12:03:51 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 12:04:10 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 12:04:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 12:04:11 - INFO - __main__ - Starting training!
06/23/2022 12:04:14 - INFO - __main__ - Step 10 Global step 10 Train loss 4.12 on epoch=2
06/23/2022 12:04:16 - INFO - __main__ - Step 20 Global step 20 Train loss 3.18 on epoch=4
06/23/2022 12:04:19 - INFO - __main__ - Step 30 Global step 30 Train loss 2.79 on epoch=7
06/23/2022 12:04:21 - INFO - __main__ - Step 40 Global step 40 Train loss 2.22 on epoch=9
06/23/2022 12:04:24 - INFO - __main__ - Step 50 Global step 50 Train loss 2.08 on epoch=12
06/23/2022 12:04:25 - INFO - __main__ - Global step 50 Train loss 2.88 Classification-F1 0.023391812865497075 on epoch=12
06/23/2022 12:04:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.023391812865497075 on epoch=12, global_step=50
06/23/2022 12:04:27 - INFO - __main__ - Step 60 Global step 60 Train loss 1.69 on epoch=14
06/23/2022 12:04:30 - INFO - __main__ - Step 70 Global step 70 Train loss 1.47 on epoch=17
06/23/2022 12:04:32 - INFO - __main__ - Step 80 Global step 80 Train loss 1.22 on epoch=19
06/23/2022 12:04:35 - INFO - __main__ - Step 90 Global step 90 Train loss 1.06 on epoch=22
06/23/2022 12:04:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.94 on epoch=24
06/23/2022 12:04:38 - INFO - __main__ - Global step 100 Train loss 1.27 Classification-F1 0.5125 on epoch=24
06/23/2022 12:04:38 - INFO - __main__ - Saving model with best Classification-F1: 0.023391812865497075 -> 0.5125 on epoch=24, global_step=100
06/23/2022 12:04:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.96 on epoch=27
06/23/2022 12:04:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.77 on epoch=29
06/23/2022 12:04:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.74 on epoch=32
06/23/2022 12:04:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.75 on epoch=34
06/23/2022 12:04:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=37
06/23/2022 12:04:52 - INFO - __main__ - Global step 150 Train loss 0.76 Classification-F1 0.5363095238095239 on epoch=37
06/23/2022 12:04:52 - INFO - __main__ - Saving model with best Classification-F1: 0.5125 -> 0.5363095238095239 on epoch=37, global_step=150
06/23/2022 12:04:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.70 on epoch=39
06/23/2022 12:04:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.62 on epoch=42
06/23/2022 12:04:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.58 on epoch=44
06/23/2022 12:05:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.60 on epoch=47
06/23/2022 12:05:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=49
06/23/2022 12:05:05 - INFO - __main__ - Global step 200 Train loss 0.60 Classification-F1 0.5157152285101468 on epoch=49
06/23/2022 12:05:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=52
06/23/2022 12:05:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.55 on epoch=54
06/23/2022 12:05:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.61 on epoch=57
06/23/2022 12:05:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.59 on epoch=59
06/23/2022 12:05:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=62
06/23/2022 12:05:19 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.6032696489593041 on epoch=62
06/23/2022 12:05:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5363095238095239 -> 0.6032696489593041 on epoch=62, global_step=250
06/23/2022 12:05:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=64
06/23/2022 12:05:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=67
06/23/2022 12:05:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=69
06/23/2022 12:05:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=72
06/23/2022 12:05:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=74
06/23/2022 12:05:32 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.5932000543995648 on epoch=74
06/23/2022 12:05:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=77
06/23/2022 12:05:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=79
06/23/2022 12:05:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=82
06/23/2022 12:05:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=84
06/23/2022 12:05:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=87
06/23/2022 12:05:46 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.6709196608095113 on epoch=87
06/23/2022 12:05:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6032696489593041 -> 0.6709196608095113 on epoch=87, global_step=350
06/23/2022 12:05:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=89
06/23/2022 12:05:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=92
06/23/2022 12:05:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=94
06/23/2022 12:05:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=97
06/23/2022 12:05:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=99
06/23/2022 12:05:59 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.6870486930675758 on epoch=99
06/23/2022 12:05:59 - INFO - __main__ - Saving model with best Classification-F1: 0.6709196608095113 -> 0.6870486930675758 on epoch=99, global_step=400
06/23/2022 12:06:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=102
06/23/2022 12:06:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=104
06/23/2022 12:06:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=107
06/23/2022 12:06:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=109
06/23/2022 12:06:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=112
06/23/2022 12:06:12 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.6870486930675758 on epoch=112
06/23/2022 12:06:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=114
06/23/2022 12:06:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=117
06/23/2022 12:06:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=119
06/23/2022 12:06:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=122
06/23/2022 12:06:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.29 on epoch=124
06/23/2022 12:06:26 - INFO - __main__ - Global step 500 Train loss 0.33 Classification-F1 0.6606437969924812 on epoch=124
06/23/2022 12:06:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=127
06/23/2022 12:06:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=129
06/23/2022 12:06:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.29 on epoch=132
06/23/2022 12:06:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=134
06/23/2022 12:06:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=137
06/23/2022 12:06:39 - INFO - __main__ - Global step 550 Train loss 0.29 Classification-F1 0.6616406607917643 on epoch=137
06/23/2022 12:06:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=139
06/23/2022 12:06:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.29 on epoch=142
06/23/2022 12:06:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=144
06/23/2022 12:06:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=147
06/23/2022 12:06:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=149
06/23/2022 12:06:53 - INFO - __main__ - Global step 600 Train loss 0.26 Classification-F1 0.6870486930675758 on epoch=149
06/23/2022 12:06:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=152
06/23/2022 12:06:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=154
06/23/2022 12:07:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=157
06/23/2022 12:07:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=159
06/23/2022 12:07:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=162
06/23/2022 12:07:06 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.6589330212187571 on epoch=162
06/23/2022 12:07:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=164
06/23/2022 12:07:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=167
06/23/2022 12:07:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=169
06/23/2022 12:07:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=172
06/23/2022 12:07:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=174
06/23/2022 12:07:20 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.6603708469033547 on epoch=174
06/23/2022 12:07:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.28 on epoch=177
06/23/2022 12:07:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=179
06/23/2022 12:07:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=182
06/23/2022 12:07:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=184
06/23/2022 12:07:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=187
06/23/2022 12:07:33 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.6594436421337897 on epoch=187
06/23/2022 12:07:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=189
06/23/2022 12:07:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=192
06/23/2022 12:07:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=194
06/23/2022 12:07:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=197
06/23/2022 12:07:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=199
06/23/2022 12:07:47 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.6603708469033547 on epoch=199
06/23/2022 12:07:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=202
06/23/2022 12:07:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.13 on epoch=204
06/23/2022 12:07:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=207
06/23/2022 12:07:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=209
06/23/2022 12:08:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=212
06/23/2022 12:08:00 - INFO - __main__ - Global step 850 Train loss 0.16 Classification-F1 0.6996212121212121 on epoch=212
06/23/2022 12:08:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6870486930675758 -> 0.6996212121212121 on epoch=212, global_step=850
06/23/2022 12:08:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=214
06/23/2022 12:08:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=217
06/23/2022 12:08:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.15 on epoch=219
06/23/2022 12:08:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=222
06/23/2022 12:08:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=224
06/23/2022 12:08:14 - INFO - __main__ - Global step 900 Train loss 0.17 Classification-F1 0.715962543554007 on epoch=224
06/23/2022 12:08:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6996212121212121 -> 0.715962543554007 on epoch=224, global_step=900
06/23/2022 12:08:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=227
06/23/2022 12:08:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=229
06/23/2022 12:08:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=232
06/23/2022 12:08:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=234
06/23/2022 12:08:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=237
06/23/2022 12:08:27 - INFO - __main__ - Global step 950 Train loss 0.14 Classification-F1 0.6857788791791661 on epoch=237
06/23/2022 12:08:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=239
06/23/2022 12:08:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=242
06/23/2022 12:08:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=244
06/23/2022 12:08:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=247
06/23/2022 12:08:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.12 on epoch=249
06/23/2022 12:08:41 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.7009303943306813 on epoch=249
06/23/2022 12:08:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=252
06/23/2022 12:08:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.16 on epoch=254
06/23/2022 12:08:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.13 on epoch=257
06/23/2022 12:08:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=259
06/23/2022 12:08:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=262
06/23/2022 12:08:54 - INFO - __main__ - Global step 1050 Train loss 0.13 Classification-F1 0.715962543554007 on epoch=262
06/23/2022 12:08:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=264
06/23/2022 12:08:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=267
06/23/2022 12:09:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=269
06/23/2022 12:09:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=272
06/23/2022 12:09:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=274
06/23/2022 12:09:08 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.7009303943306813 on epoch=274
06/23/2022 12:09:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=277
06/23/2022 12:09:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
06/23/2022 12:09:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=282
06/23/2022 12:09:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=284
06/23/2022 12:09:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=287
06/23/2022 12:09:21 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.7009375819564647 on epoch=287
06/23/2022 12:09:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=289
06/23/2022 12:09:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
06/23/2022 12:09:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=294
06/23/2022 12:09:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
06/23/2022 12:09:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
06/23/2022 12:09:34 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.7009303943306813 on epoch=299
06/23/2022 12:09:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=302
06/23/2022 12:09:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=304
06/23/2022 12:09:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
06/23/2022 12:09:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
06/23/2022 12:09:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
06/23/2022 12:09:48 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.6996212121212121 on epoch=312
06/23/2022 12:09:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
06/23/2022 12:09:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=317
06/23/2022 12:09:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=319
06/23/2022 12:09:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
06/23/2022 12:10:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=324
06/23/2022 12:10:01 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.74585326953748 on epoch=324
06/23/2022 12:10:01 - INFO - __main__ - Saving model with best Classification-F1: 0.715962543554007 -> 0.74585326953748 on epoch=324, global_step=1300
06/23/2022 12:10:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=327
06/23/2022 12:10:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
06/23/2022 12:10:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
06/23/2022 12:10:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
06/23/2022 12:10:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=337
06/23/2022 12:10:15 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.7466748937337173 on epoch=337
06/23/2022 12:10:15 - INFO - __main__ - Saving model with best Classification-F1: 0.74585326953748 -> 0.7466748937337173 on epoch=337, global_step=1350
06/23/2022 12:10:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=339
06/23/2022 12:10:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
06/23/2022 12:10:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
06/23/2022 12:10:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
06/23/2022 12:10:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
06/23/2022 12:10:28 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.7720720720720721 on epoch=349
06/23/2022 12:10:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7466748937337173 -> 0.7720720720720721 on epoch=349, global_step=1400
06/23/2022 12:10:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=352
06/23/2022 12:10:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=354
06/23/2022 12:10:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=357
06/23/2022 12:10:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
06/23/2022 12:10:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
06/23/2022 12:10:42 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.7867800836550837 on epoch=362
06/23/2022 12:10:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7720720720720721 -> 0.7867800836550837 on epoch=362, global_step=1450
06/23/2022 12:10:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
06/23/2022 12:10:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=367
06/23/2022 12:10:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
06/23/2022 12:10:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
06/23/2022 12:10:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
06/23/2022 12:10:56 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.7315233785822022 on epoch=374
06/23/2022 12:10:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.09 on epoch=377
06/23/2022 12:11:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
06/23/2022 12:11:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
06/23/2022 12:11:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
06/23/2022 12:11:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
06/23/2022 12:11:09 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.737874572433396 on epoch=387
06/23/2022 12:11:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
06/23/2022 12:11:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=392
06/23/2022 12:11:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
06/23/2022 12:11:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
06/23/2022 12:11:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/23/2022 12:11:22 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.74585326953748 on epoch=399
06/23/2022 12:11:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
06/23/2022 12:11:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
06/23/2022 12:11:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
06/23/2022 12:11:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=409
06/23/2022 12:11:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=412
06/23/2022 12:11:36 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.7326488739868737 on epoch=412
06/23/2022 12:11:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
06/23/2022 12:11:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
06/23/2022 12:11:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
06/23/2022 12:11:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
06/23/2022 12:11:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
06/23/2022 12:11:50 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6739954098105574 on epoch=424
06/23/2022 12:11:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
06/23/2022 12:11:55 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/23/2022 12:11:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
06/23/2022 12:12:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
06/23/2022 12:12:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=437
06/23/2022 12:12:03 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.74585326953748 on epoch=437
06/23/2022 12:12:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=439
06/23/2022 12:12:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
06/23/2022 12:12:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
06/23/2022 12:12:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
06/23/2022 12:12:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=449
06/23/2022 12:12:16 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.74585326953748 on epoch=449
06/23/2022 12:12:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=452
06/23/2022 12:12:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
06/23/2022 12:12:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=457
06/23/2022 12:12:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
06/23/2022 12:12:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
06/23/2022 12:12:30 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.761707042957043 on epoch=462
06/23/2022 12:12:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/23/2022 12:12:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=467
06/23/2022 12:12:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
06/23/2022 12:12:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=472
06/23/2022 12:12:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
06/23/2022 12:12:43 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.7466748937337173 on epoch=474
06/23/2022 12:12:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=477
06/23/2022 12:12:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
06/23/2022 12:12:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
06/23/2022 12:12:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
06/23/2022 12:12:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
06/23/2022 12:12:57 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.7466748937337173 on epoch=487
06/23/2022 12:12:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
06/23/2022 12:13:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/23/2022 12:13:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
06/23/2022 12:13:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
06/23/2022 12:13:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=499
06/23/2022 12:13:10 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.709672619047619 on epoch=499
06/23/2022 12:13:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=502
06/23/2022 12:13:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
06/23/2022 12:13:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
06/23/2022 12:13:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
06/23/2022 12:13:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=512
06/23/2022 12:13:24 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.7466748937337173 on epoch=512
06/23/2022 12:13:26 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/23/2022 12:13:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
06/23/2022 12:13:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
06/23/2022 12:13:34 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=522
06/23/2022 12:13:36 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
06/23/2022 12:13:37 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.715962543554007 on epoch=524
06/23/2022 12:13:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
06/23/2022 12:13:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
06/23/2022 12:13:44 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/23/2022 12:13:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
06/23/2022 12:13:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
06/23/2022 12:13:50 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7466748937337173 on epoch=537
06/23/2022 12:13:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
06/23/2022 12:13:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=542
06/23/2022 12:13:58 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
06/23/2022 12:14:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
06/23/2022 12:14:03 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
06/23/2022 12:14:04 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7466748937337173 on epoch=549
06/23/2022 12:14:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
06/23/2022 12:14:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/23/2022 12:14:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
06/23/2022 12:14:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/23/2022 12:14:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/23/2022 12:14:17 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7466748937337173 on epoch=562
06/23/2022 12:14:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
06/23/2022 12:14:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
06/23/2022 12:14:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/23/2022 12:14:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
06/23/2022 12:14:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
06/23/2022 12:14:31 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7466748937337173 on epoch=574
06/23/2022 12:14:33 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
06/23/2022 12:14:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/23/2022 12:14:38 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
06/23/2022 12:14:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=584
06/23/2022 12:14:43 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
06/23/2022 12:14:44 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.806336898395722 on epoch=587
06/23/2022 12:14:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7867800836550837 -> 0.806336898395722 on epoch=587, global_step=2350
06/23/2022 12:14:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=589
06/23/2022 12:14:49 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
06/23/2022 12:14:52 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
06/23/2022 12:14:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
06/23/2022 12:14:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/23/2022 12:14:58 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7466748937337173 on epoch=599
06/23/2022 12:15:00 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
06/23/2022 12:15:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/23/2022 12:15:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/23/2022 12:15:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/23/2022 12:15:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
06/23/2022 12:15:11 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7466748937337173 on epoch=612
06/23/2022 12:15:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=614
06/23/2022 12:15:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
06/23/2022 12:15:19 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
06/23/2022 12:15:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=622
06/23/2022 12:15:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/23/2022 12:15:25 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.7466748937337173 on epoch=624
06/23/2022 12:15:27 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
06/23/2022 12:15:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
06/23/2022 12:15:32 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/23/2022 12:15:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
06/23/2022 12:15:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
06/23/2022 12:15:38 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.7466748937337173 on epoch=637
06/23/2022 12:15:41 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/23/2022 12:15:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
06/23/2022 12:15:46 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
06/23/2022 12:15:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
06/23/2022 12:15:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/23/2022 12:15:52 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7466748937337173 on epoch=649
06/23/2022 12:15:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/23/2022 12:15:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/23/2022 12:15:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=657
06/23/2022 12:16:02 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
06/23/2022 12:16:04 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
06/23/2022 12:16:05 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7910804881393118 on epoch=662
06/23/2022 12:16:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
06/23/2022 12:16:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/23/2022 12:16:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/23/2022 12:16:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
06/23/2022 12:16:17 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/23/2022 12:16:18 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7724480095068331 on epoch=674
06/23/2022 12:16:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/23/2022 12:16:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/23/2022 12:16:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=682
06/23/2022 12:16:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/23/2022 12:16:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
06/23/2022 12:16:32 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7675458991584985 on epoch=687
06/23/2022 12:16:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/23/2022 12:16:37 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
06/23/2022 12:16:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/23/2022 12:16:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
06/23/2022 12:16:44 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/23/2022 12:16:45 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7466748937337173 on epoch=699
06/23/2022 12:16:48 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
06/23/2022 12:16:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
06/23/2022 12:16:53 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/23/2022 12:16:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/23/2022 12:16:58 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/23/2022 12:16:59 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7466748937337173 on epoch=712
06/23/2022 12:17:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
06/23/2022 12:17:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/23/2022 12:17:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/23/2022 12:17:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/23/2022 12:17:11 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/23/2022 12:17:12 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7466748937337173 on epoch=724
06/23/2022 12:17:15 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
06/23/2022 12:17:17 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/23/2022 12:17:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
06/23/2022 12:17:22 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/23/2022 12:17:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
06/23/2022 12:17:26 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7466820813595008 on epoch=737
06/23/2022 12:17:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/23/2022 12:17:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
06/23/2022 12:17:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=744
06/23/2022 12:17:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=747
06/23/2022 12:17:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/23/2022 12:17:39 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7392570664629489 on epoch=749
06/23/2022 12:17:39 - INFO - __main__ - save last model!
06/23/2022 12:17:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 12:17:39 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 12:17:39 - INFO - __main__ - Printing 3 examples
06/23/2022 12:17:39 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 12:17:39 - INFO - __main__ - ['others']
06/23/2022 12:17:39 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 12:17:39 - INFO - __main__ - ['others']
06/23/2022 12:17:39 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 12:17:39 - INFO - __main__ - ['others']
06/23/2022 12:17:39 - INFO - __main__ - Tokenizing Input ...
06/23/2022 12:17:40 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 12:17:40 - INFO - __main__ - Printing 3 examples
06/23/2022 12:17:40 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/23/2022 12:17:40 - INFO - __main__ - ['others']
06/23/2022 12:17:40 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/23/2022 12:17:40 - INFO - __main__ - ['others']
06/23/2022 12:17:40 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/23/2022 12:17:40 - INFO - __main__ - ['others']
06/23/2022 12:17:40 - INFO - __main__ - Tokenizing Input ...
06/23/2022 12:17:40 - INFO - __main__ - Tokenizing Output ...
06/23/2022 12:17:40 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 12:17:40 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 12:17:40 - INFO - __main__ - Printing 3 examples
06/23/2022 12:17:40 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/23/2022 12:17:40 - INFO - __main__ - ['others']
06/23/2022 12:17:40 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/23/2022 12:17:40 - INFO - __main__ - ['others']
06/23/2022 12:17:40 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/23/2022 12:17:40 - INFO - __main__ - ['others']
06/23/2022 12:17:40 - INFO - __main__ - Tokenizing Input ...
06/23/2022 12:17:40 - INFO - __main__ - Tokenizing Output ...
06/23/2022 12:17:40 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 12:17:41 - INFO - __main__ - Tokenizing Output ...
06/23/2022 12:17:47 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 12:17:59 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 12:18:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 12:18:00 - INFO - __main__ - Starting training!
06/23/2022 12:19:00 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_87_0.3_8_predictions.txt
06/23/2022 12:19:00 - INFO - __main__ - Classification-F1 on test data: 0.1526
06/23/2022 12:19:01 - INFO - __main__ - prefix=emo_16_87, lr=0.3, bsz=8, dev_performance=0.806336898395722, test_performance=0.15259144354801024
06/23/2022 12:19:01 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.2, bsz=8 ...
06/23/2022 12:19:02 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 12:19:02 - INFO - __main__ - Printing 3 examples
06/23/2022 12:19:02 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/23/2022 12:19:02 - INFO - __main__ - ['others']
06/23/2022 12:19:02 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/23/2022 12:19:02 - INFO - __main__ - ['others']
06/23/2022 12:19:02 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/23/2022 12:19:02 - INFO - __main__ - ['others']
06/23/2022 12:19:02 - INFO - __main__ - Tokenizing Input ...
06/23/2022 12:19:02 - INFO - __main__ - Tokenizing Output ...
06/23/2022 12:19:02 - INFO - __main__ - Loaded 64 examples from train data
06/23/2022 12:19:02 - INFO - __main__ - Start tokenizing ... 64 instances
06/23/2022 12:19:02 - INFO - __main__ - Printing 3 examples
06/23/2022 12:19:02 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/23/2022 12:19:02 - INFO - __main__ - ['others']
06/23/2022 12:19:02 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/23/2022 12:19:02 - INFO - __main__ - ['others']
06/23/2022 12:19:02 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/23/2022 12:19:02 - INFO - __main__ - ['others']
06/23/2022 12:19:02 - INFO - __main__ - Tokenizing Input ...
06/23/2022 12:19:02 - INFO - __main__ - Tokenizing Output ...
06/23/2022 12:19:02 - INFO - __main__ - Loaded 64 examples from dev data
06/23/2022 12:19:21 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 12:19:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/23/2022 12:19:22 - INFO - __main__ - Starting training!
06/23/2022 12:19:25 - INFO - __main__ - Step 10 Global step 10 Train loss 4.40 on epoch=2
06/23/2022 12:19:27 - INFO - __main__ - Step 20 Global step 20 Train loss 3.65 on epoch=4
06/23/2022 12:19:30 - INFO - __main__ - Step 30 Global step 30 Train loss 2.99 on epoch=7
06/23/2022 12:19:32 - INFO - __main__ - Step 40 Global step 40 Train loss 2.87 on epoch=9
06/23/2022 12:19:35 - INFO - __main__ - Step 50 Global step 50 Train loss 2.48 on epoch=12
06/23/2022 12:19:36 - INFO - __main__ - Global step 50 Train loss 3.28 Classification-F1 0.0 on epoch=12
06/23/2022 12:19:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
06/23/2022 12:19:38 - INFO - __main__ - Step 60 Global step 60 Train loss 2.26 on epoch=14
06/23/2022 12:19:41 - INFO - __main__ - Step 70 Global step 70 Train loss 2.01 on epoch=17
06/23/2022 12:19:43 - INFO - __main__ - Step 80 Global step 80 Train loss 1.78 on epoch=19
06/23/2022 12:19:46 - INFO - __main__ - Step 90 Global step 90 Train loss 1.54 on epoch=22
06/23/2022 12:19:48 - INFO - __main__ - Step 100 Global step 100 Train loss 1.37 on epoch=24
06/23/2022 12:19:49 - INFO - __main__ - Global step 100 Train loss 1.79 Classification-F1 0.11860670194003527 on epoch=24
06/23/2022 12:19:49 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.11860670194003527 on epoch=24, global_step=100
06/23/2022 12:19:52 - INFO - __main__ - Step 110 Global step 110 Train loss 1.12 on epoch=27
06/23/2022 12:19:54 - INFO - __main__ - Step 120 Global step 120 Train loss 1.09 on epoch=29
06/23/2022 12:19:57 - INFO - __main__ - Step 130 Global step 130 Train loss 1.07 on epoch=32
06/23/2022 12:20:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.80 on epoch=34
06/23/2022 12:20:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.75 on epoch=37
06/23/2022 12:20:03 - INFO - __main__ - Global step 150 Train loss 0.96 Classification-F1 0.49643640350877194 on epoch=37
06/23/2022 12:20:03 - INFO - __main__ - Saving model with best Classification-F1: 0.11860670194003527 -> 0.49643640350877194 on epoch=37, global_step=150
06/23/2022 12:20:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.77 on epoch=39
06/23/2022 12:20:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=42
06/23/2022 12:20:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.72 on epoch=44
06/23/2022 12:20:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.57 on epoch=47
06/23/2022 12:20:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.74 on epoch=49
06/23/2022 12:20:17 - INFO - __main__ - Global step 200 Train loss 0.73 Classification-F1 0.4942640692640693 on epoch=49
06/23/2022 12:20:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.73 on epoch=52
06/23/2022 12:20:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=54
06/23/2022 12:20:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.59 on epoch=57
06/23/2022 12:20:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=59
06/23/2022 12:20:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.66 on epoch=62
06/23/2022 12:20:30 - INFO - __main__ - Global step 250 Train loss 0.61 Classification-F1 0.5471927948714341 on epoch=62
06/23/2022 12:20:30 - INFO - __main__ - Saving model with best Classification-F1: 0.49643640350877194 -> 0.5471927948714341 on epoch=62, global_step=250
06/23/2022 12:20:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=64
06/23/2022 12:20:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.72 on epoch=67
06/23/2022 12:20:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.62 on epoch=69
06/23/2022 12:20:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=72
06/23/2022 12:20:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.57 on epoch=74
06/23/2022 12:20:43 - INFO - __main__ - Global step 300 Train loss 0.59 Classification-F1 0.4957623722700862 on epoch=74
06/23/2022 12:20:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=77
06/23/2022 12:20:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=79
06/23/2022 12:20:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=82
06/23/2022 12:20:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=84
06/23/2022 12:20:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=87
06/23/2022 12:20:57 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.5626444988440092 on epoch=87
06/23/2022 12:20:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5471927948714341 -> 0.5626444988440092 on epoch=87, global_step=350
06/23/2022 12:20:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=89
06/23/2022 12:21:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=92
06/23/2022 12:21:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=94
06/23/2022 12:21:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=97
06/23/2022 12:21:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=99
06/23/2022 12:21:11 - INFO - __main__ - Global step 400 Train loss 0.51 Classification-F1 0.5391891891891892 on epoch=99
06/23/2022 12:21:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=102
06/23/2022 12:21:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=104
06/23/2022 12:21:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=107
06/23/2022 12:21:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=109
06/23/2022 12:21:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=112
06/23/2022 12:21:24 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.5723888326829503 on epoch=112
06/23/2022 12:21:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5626444988440092 -> 0.5723888326829503 on epoch=112, global_step=450
06/23/2022 12:21:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.54 on epoch=114
06/23/2022 12:21:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=117
06/23/2022 12:21:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=119
06/23/2022 12:21:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=122
06/23/2022 12:21:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=124
06/23/2022 12:21:37 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.5878591417016464 on epoch=124
06/23/2022 12:21:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5723888326829503 -> 0.5878591417016464 on epoch=124, global_step=500
06/23/2022 12:21:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=127
06/23/2022 12:21:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=129
06/23/2022 12:21:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=132
06/23/2022 12:21:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=134
06/23/2022 12:21:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=137
06/23/2022 12:21:51 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.6456649645504134 on epoch=137
06/23/2022 12:21:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5878591417016464 -> 0.6456649645504134 on epoch=137, global_step=550
06/23/2022 12:21:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=139
06/23/2022 12:21:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=142
06/23/2022 12:21:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=144
06/23/2022 12:22:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=147
06/23/2022 12:22:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=149
06/23/2022 12:22:05 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.6606437969924812 on epoch=149
06/23/2022 12:22:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6456649645504134 -> 0.6606437969924812 on epoch=149, global_step=600
06/23/2022 12:22:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=152
06/23/2022 12:22:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=154
06/23/2022 12:22:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=157
06/23/2022 12:22:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=159
06/23/2022 12:22:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=162
06/23/2022 12:22:18 - INFO - __main__ - Global step 650 Train loss 0.34 Classification-F1 0.6860518292682927 on epoch=162
06/23/2022 12:22:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6606437969924812 -> 0.6860518292682927 on epoch=162, global_step=650
06/23/2022 12:22:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=164
06/23/2022 12:22:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=167
06/23/2022 12:22:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=169
06/23/2022 12:22:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=172
06/23/2022 12:22:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=174
06/23/2022 12:22:31 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.6606437969924812 on epoch=174
06/23/2022 12:22:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=177
06/23/2022 12:22:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=179
06/23/2022 12:22:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.27 on epoch=182
06/23/2022 12:22:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=184
06/23/2022 12:22:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=187
06/23/2022 12:22:45 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.6857788791791661 on epoch=187
06/23/2022 12:22:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=189
06/23/2022 12:22:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.26 on epoch=192
06/23/2022 12:22:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=194
06/23/2022 12:22:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=197
06/23/2022 12:22:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=199
06/23/2022 12:22:58 - INFO - __main__ - Global step 800 Train loss 0.30 Classification-F1 0.6456649645504134 on epoch=199
06/23/2022 12:23:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=202
06/23/2022 12:23:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=204
06/23/2022 12:23:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=207
06/23/2022 12:23:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.29 on epoch=209
06/23/2022 12:23:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=212
06/23/2022 12:23:12 - INFO - __main__ - Global step 850 Train loss 0.27 Classification-F1 0.6603708469033547 on epoch=212
06/23/2022 12:23:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=214
06/23/2022 12:23:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.27 on epoch=217
06/23/2022 12:23:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=219
06/23/2022 12:23:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.17 on epoch=222
06/23/2022 12:23:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=224
06/23/2022 12:23:25 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.6589330212187571 on epoch=224
06/23/2022 12:23:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=227
06/23/2022 12:23:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.28 on epoch=229
06/23/2022 12:23:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=232
06/23/2022 12:23:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=234
06/23/2022 12:23:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=237
06/23/2022 12:23:39 - INFO - __main__ - Global step 950 Train loss 0.27 Classification-F1 0.6589330212187571 on epoch=237
06/23/2022 12:23:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=239
06/23/2022 12:23:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=242
06/23/2022 12:23:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=244
06/23/2022 12:23:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=247
06/23/2022 12:23:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=249
06/23/2022 12:23:52 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.6589330212187571 on epoch=249
06/23/2022 12:23:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.17 on epoch=252
06/23/2022 12:23:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=254
06/23/2022 12:24:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=257
06/23/2022 12:24:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.26 on epoch=259
06/23/2022 12:24:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=262
06/23/2022 12:24:06 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.6589330212187571 on epoch=262
06/23/2022 12:24:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=264
06/23/2022 12:24:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=267
06/23/2022 12:24:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=269
06/23/2022 12:24:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=272
06/23/2022 12:24:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=274
06/23/2022 12:24:19 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.6845588235294118 on epoch=274
06/23/2022 12:24:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=277
06/23/2022 12:24:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=279
06/23/2022 12:24:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=282
06/23/2022 12:24:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=284
06/23/2022 12:24:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=287
06/23/2022 12:24:32 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.6845588235294118 on epoch=287
06/23/2022 12:24:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=289
06/23/2022 12:24:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=292
06/23/2022 12:24:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=294
06/23/2022 12:24:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=297
06/23/2022 12:24:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=299
06/23/2022 12:24:46 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.6589330212187571 on epoch=299
06/23/2022 12:24:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.15 on epoch=302
06/23/2022 12:24:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=304
06/23/2022 12:24:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.10 on epoch=307
06/23/2022 12:24:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=309
06/23/2022 12:24:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=312
06/23/2022 12:24:59 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.6845588235294118 on epoch=312
06/23/2022 12:25:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.14 on epoch=314
06/23/2022 12:25:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=317
06/23/2022 12:25:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=319
06/23/2022 12:25:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.09 on epoch=322
06/23/2022 12:25:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=324
06/23/2022 12:25:13 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.6833333333333333 on epoch=324
06/23/2022 12:25:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=327
06/23/2022 12:25:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
06/23/2022 12:25:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=332
06/23/2022 12:25:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=334
06/23/2022 12:25:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=337
06/23/2022 12:25:26 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.7145833333333333 on epoch=337
06/23/2022 12:25:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6860518292682927 -> 0.7145833333333333 on epoch=337, global_step=1350
06/23/2022 12:25:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=339
06/23/2022 12:25:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=342
06/23/2022 12:25:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=344
06/23/2022 12:25:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=347
06/23/2022 12:25:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.13 on epoch=349
06/23/2022 12:25:40 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.6833333333333333 on epoch=349
06/23/2022 12:25:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=352
06/23/2022 12:25:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=354
06/23/2022 12:25:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=357
06/23/2022 12:25:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=359
06/23/2022 12:25:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=362
06/23/2022 12:25:53 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.7378258995906055 on epoch=362
06/23/2022 12:25:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7145833333333333 -> 0.7378258995906055 on epoch=362, global_step=1450
06/23/2022 12:25:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=364
06/23/2022 12:25:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=367
06/23/2022 12:26:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=369
06/23/2022 12:26:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=372
06/23/2022 12:26:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=374
06/23/2022 12:26:07 - INFO - __main__ - Global step 1500 Train loss 0.10 Classification-F1 0.7071886446886447 on epoch=374
06/23/2022 12:26:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=377
06/23/2022 12:26:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=379
06/23/2022 12:26:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=382
06/23/2022 12:26:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=384
06/23/2022 12:26:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=387
06/23/2022 12:26:20 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.6996212121212121 on epoch=387
06/23/2022 12:26:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=389
06/23/2022 12:26:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=392
06/23/2022 12:26:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=394
06/23/2022 12:26:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=397
06/23/2022 12:26:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=399
06/23/2022 12:26:34 - INFO - __main__ - Global step 1600 Train loss 0.11 Classification-F1 0.7384386446886447 on epoch=399
06/23/2022 12:26:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7378258995906055 -> 0.7384386446886447 on epoch=399, global_step=1600
06/23/2022 12:26:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=402
06/23/2022 12:26:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.15 on epoch=404
06/23/2022 12:26:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=407
06/23/2022 12:26:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=409
06/23/2022 12:26:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=412
06/23/2022 12:26:47 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.7226107226107227 on epoch=412
06/23/2022 12:26:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=414
06/23/2022 12:26:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=417
06/23/2022 12:26:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=419
06/23/2022 12:26:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
06/23/2022 12:27:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=424
06/23/2022 12:27:01 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.7384386446886447 on epoch=424
06/23/2022 12:27:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=427
06/23/2022 12:27:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
06/23/2022 12:27:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=432
06/23/2022 12:27:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=434
06/23/2022 12:27:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=437
06/23/2022 12:27:15 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.752913752913753 on epoch=437
06/23/2022 12:27:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7384386446886447 -> 0.752913752913753 on epoch=437, global_step=1750
06/23/2022 12:27:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=439
06/23/2022 12:27:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=442
06/23/2022 12:27:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=444
06/23/2022 12:27:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=447
06/23/2022 12:27:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=449
06/23/2022 12:27:28 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.7378258995906055 on epoch=449
06/23/2022 12:27:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=452
06/23/2022 12:27:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=454
06/23/2022 12:27:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=457
06/23/2022 12:27:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=459
06/23/2022 12:27:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=462
06/23/2022 12:27:41 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.7295653907496013 on epoch=462
06/23/2022 12:27:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=464
06/23/2022 12:27:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=467
06/23/2022 12:27:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
06/23/2022 12:27:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=472
06/23/2022 12:27:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=474
06/23/2022 12:27:55 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.6987554112554113 on epoch=474
06/23/2022 12:27:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=477
06/23/2022 12:28:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
06/23/2022 12:28:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=482
06/23/2022 12:28:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=484
06/23/2022 12:28:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=487
06/23/2022 12:28:08 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.7602026456515621 on epoch=487
06/23/2022 12:28:09 - INFO - __main__ - Saving model with best Classification-F1: 0.752913752913753 -> 0.7602026456515621 on epoch=487, global_step=1950
06/23/2022 12:28:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=489
06/23/2022 12:28:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=492
06/23/2022 12:28:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
06/23/2022 12:28:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=497
06/23/2022 12:28:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=499
06/23/2022 12:28:22 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.7752904989747095 on epoch=499
06/23/2022 12:28:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7602026456515621 -> 0.7752904989747095 on epoch=499, global_step=2000
06/23/2022 12:28:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
06/23/2022 12:28:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=504
06/23/2022 12:28:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=507
06/23/2022 12:28:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=509
06/23/2022 12:28:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=512
06/23/2022 12:28:36 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.6577075310226786 on epoch=512
06/23/2022 12:28:38 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.11 on epoch=514
06/23/2022 12:28:41 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
06/23/2022 12:28:43 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.08 on epoch=519
06/23/2022 12:28:46 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
06/23/2022 12:28:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
06/23/2022 12:28:49 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.6987554112554113 on epoch=524
06/23/2022 12:28:52 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=527
06/23/2022 12:28:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=529
06/23/2022 12:28:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
06/23/2022 12:28:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=534
06/23/2022 12:29:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=537
06/23/2022 12:29:03 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.7813472039814752 on epoch=537
06/23/2022 12:29:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7752904989747095 -> 0.7813472039814752 on epoch=537, global_step=2150
06/23/2022 12:29:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=539
06/23/2022 12:29:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
06/23/2022 12:29:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=544
06/23/2022 12:29:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
06/23/2022 12:29:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
06/23/2022 12:29:16 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.7138541149576803 on epoch=549
06/23/2022 12:29:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
06/23/2022 12:29:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=554
06/23/2022 12:29:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
06/23/2022 12:29:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=559
06/23/2022 12:29:29 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
06/23/2022 12:29:30 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.7071886446886447 on epoch=562
06/23/2022 12:29:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=564
06/23/2022 12:29:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=567
06/23/2022 12:29:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=569
06/23/2022 12:29:40 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=572
06/23/2022 12:29:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
06/23/2022 12:29:43 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7602026456515621 on epoch=574
06/23/2022 12:29:46 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
06/23/2022 12:29:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
06/23/2022 12:29:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
06/23/2022 12:29:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=584
06/23/2022 12:29:56 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
06/23/2022 12:29:57 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.7378258995906055 on epoch=587
06/23/2022 12:29:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=589
06/23/2022 12:30:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
06/23/2022 12:30:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
06/23/2022 12:30:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
06/23/2022 12:30:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
06/23/2022 12:30:10 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.7520221007063113 on epoch=599
06/23/2022 12:30:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=602
06/23/2022 12:30:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=604
06/23/2022 12:30:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/23/2022 12:30:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
06/23/2022 12:30:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=612
06/23/2022 12:30:24 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.7226107226107227 on epoch=612
06/23/2022 12:30:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
06/23/2022 12:30:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
06/23/2022 12:30:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
06/23/2022 12:30:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=622
06/23/2022 12:30:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
06/23/2022 12:30:37 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.7226107226107227 on epoch=624
06/23/2022 12:30:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=627
06/23/2022 12:30:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=629
06/23/2022 12:30:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
06/23/2022 12:30:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
06/23/2022 12:30:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
06/23/2022 12:30:51 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.6977698771816419 on epoch=637
06/23/2022 12:30:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=639
06/23/2022 12:30:56 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
06/23/2022 12:30:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
06/23/2022 12:31:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/23/2022 12:31:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/23/2022 12:31:04 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7449874686716793 on epoch=649
06/23/2022 12:31:07 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
06/23/2022 12:31:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=654
06/23/2022 12:31:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=657
06/23/2022 12:31:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
06/23/2022 12:31:17 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
06/23/2022 12:31:18 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.7226107226107227 on epoch=662
06/23/2022 12:31:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
06/23/2022 12:31:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/23/2022 12:31:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
06/23/2022 12:31:28 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
06/23/2022 12:31:30 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
06/23/2022 12:31:31 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6987554112554113 on epoch=674
06/23/2022 12:31:34 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
06/23/2022 12:31:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
06/23/2022 12:31:39 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
06/23/2022 12:31:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/23/2022 12:31:44 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/23/2022 12:31:45 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6987554112554113 on epoch=687
06/23/2022 12:31:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=689
06/23/2022 12:31:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=692
06/23/2022 12:31:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
06/23/2022 12:31:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
06/23/2022 12:31:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
06/23/2022 12:31:58 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7139705882352941 on epoch=699
06/23/2022 12:32:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
06/23/2022 12:32:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=704
06/23/2022 12:32:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/23/2022 12:32:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
06/23/2022 12:32:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=712
06/23/2022 12:32:12 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6987554112554113 on epoch=712
06/23/2022 12:32:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=714
06/23/2022 12:32:17 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/23/2022 12:32:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
06/23/2022 12:32:22 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=722
06/23/2022 12:32:25 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
06/23/2022 12:32:25 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.7226107226107227 on epoch=724
06/23/2022 12:32:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/23/2022 12:32:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=729
06/23/2022 12:32:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/23/2022 12:32:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
06/23/2022 12:32:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
06/23/2022 12:32:39 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.752913752913753 on epoch=737
06/23/2022 12:32:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/23/2022 12:32:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
06/23/2022 12:32:46 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
06/23/2022 12:32:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
06/23/2022 12:32:51 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
06/23/2022 12:32:52 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7226107226107227 on epoch=749
06/23/2022 12:32:52 - INFO - __main__ - save last model!
06/23/2022 12:32:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 12:32:52 - INFO - __main__ - Start tokenizing ... 5509 instances
06/23/2022 12:32:52 - INFO - __main__ - Printing 3 examples
06/23/2022 12:32:52 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/23/2022 12:32:52 - INFO - __main__ - ['others']
06/23/2022 12:32:52 - INFO - __main__ -  [emo] what you like very little things ok
06/23/2022 12:32:52 - INFO - __main__ - ['others']
06/23/2022 12:32:52 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/23/2022 12:32:52 - INFO - __main__ - ['others']
06/23/2022 12:32:52 - INFO - __main__ - Tokenizing Input ...
06/23/2022 12:32:55 - INFO - __main__ - Tokenizing Output ...
06/23/2022 12:33:00 - INFO - __main__ - Loaded 5509 examples from test data
06/23/2022 12:34:13 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-3-up128shot/singletask-emo/emo_16_87_0.2_8_predictions.txt
06/23/2022 12:34:13 - INFO - __main__ - Classification-F1 on test data: 0.1616
06/23/2022 12:34:13 - INFO - __main__ - prefix=emo_16_87, lr=0.2, bsz=8, dev_performance=0.7813472039814752, test_performance=0.16159475406393686
