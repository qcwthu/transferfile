05/15/2022 19:00:12 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=False, bsz_list=[4], cache_dir='/data/qin/cache/', checkpoint='None', cuda='4', dataset='nlp_forest_single', debug=False, dev_file='data', do_lowercase=False, do_predict=True, do_train=True, eval_period=50, freeze_embeds=False, gradient_accumulation_steps=2, identifier='T5-large-multitask-nopara2para-5e-1-4-20', learning_rate=0.5, learning_rate_list=[0.5], lm_adapted_path='/data/qin/lm_adapted_t5model/torch_ckpt/large/pytorch_model.bin', local_rank=0, log_step=10, max_grad_norm=1.0, max_input_length=512, max_output_length=128, model='google/t5-v1_1-large', num_beams=4, num_train_epochs=1000.0, output_dir='models/T5-large-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc', predict_batch_size=16, predict_checkpoint='best-model.pt', prefix='', prompt_number=100, quiet=False, seed=42, task_dir='data/glue-mrpc/', task_name='glue-mrpc', test_file='data', total_steps=3000, train_batch_size=4, train_file='data', wait_step=10000000000, warmup_steps=50, weight_decay=1e-05)
05/15/2022 19:00:12 - INFO - __main__ - models/T5-large-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc
06/22/2022 04:04:09 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-multitask-nopara2para-5e-1-4-20', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-nopara2para-5e-1-4-20-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='4,5')
06/22/2022 04:04:09 - INFO - __main__ - models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc
06/22/2022 04:04:09 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-multitask-nopara2para-5e-1-4-20', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-nopara2para-5e-1-4-20-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='4,5')
06/22/2022 04:04:09 - INFO - __main__ - models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc
06/22/2022 04:04:09 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/22/2022 04:04:09 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/22/2022 04:04:09 - INFO - __main__ - args.device: cuda:0
06/22/2022 04:04:09 - INFO - __main__ - Using 2 gpus
06/22/2022 04:04:09 - INFO - __main__ - args.device: cuda:1
06/22/2022 04:04:09 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/22/2022 04:04:09 - INFO - __main__ - Using 2 gpus
06/22/2022 04:04:09 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/22/2022 04:04:14 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.5, bsz=8 ...
06/22/2022 04:04:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:04:15 - INFO - __main__ - Printing 3 examples
06/22/2022 04:04:15 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/22/2022 04:04:15 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:04:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/22/2022 04:04:15 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:04:15 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/22/2022 04:04:15 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:04:15 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:04:15 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:04:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:04:15 - INFO - __main__ - Printing 3 examples
06/22/2022 04:04:15 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/22/2022 04:04:15 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:04:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/22/2022 04:04:15 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:04:15 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/22/2022 04:04:15 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:04:15 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:04:15 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:04:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:04:15 - INFO - __main__ - Printing 3 examples
06/22/2022 04:04:15 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/22/2022 04:04:15 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:04:15 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/22/2022 04:04:15 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:04:15 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:04:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/22/2022 04:04:15 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:04:15 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:04:15 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:04:15 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:04:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:04:15 - INFO - __main__ - Printing 3 examples
06/22/2022 04:04:15 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/22/2022 04:04:15 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:04:15 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/22/2022 04:04:15 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:04:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/22/2022 04:04:15 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:04:15 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:04:15 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:04:15 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:04:15 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:04:21 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:04:21 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:04:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:04:21 - INFO - __main__ - Starting training!
06/22/2022 04:04:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:04:27 - INFO - __main__ - Starting training!
06/22/2022 04:04:29 - INFO - __main__ - Step 10 Global step 10 Train loss 2.97 on epoch=4
06/22/2022 04:04:30 - INFO - __main__ - Step 20 Global step 20 Train loss 1.79 on epoch=9
06/22/2022 04:04:32 - INFO - __main__ - Step 30 Global step 30 Train loss 1.15 on epoch=14
06/22/2022 04:04:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.75 on epoch=19
06/22/2022 04:04:34 - INFO - __main__ - Step 50 Global step 50 Train loss 0.62 on epoch=24
06/22/2022 04:04:35 - INFO - __main__ - Global step 50 Train loss 1.46 ACC 0.46875 on epoch=24
06/22/2022 04:04:35 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.46875 on epoch=24, global_step=50
06/22/2022 04:04:36 - INFO - __main__ - Step 60 Global step 60 Train loss 0.52 on epoch=29
06/22/2022 04:04:37 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=34
06/22/2022 04:04:38 - INFO - __main__ - Step 80 Global step 80 Train loss 0.36 on epoch=39
06/22/2022 04:04:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.33 on epoch=44
06/22/2022 04:04:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.32 on epoch=49
06/22/2022 04:04:41 - INFO - __main__ - Global step 100 Train loss 0.39 ACC 0.46875 on epoch=49
06/22/2022 04:04:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.33 on epoch=54
06/22/2022 04:04:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=59
06/22/2022 04:04:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=64
06/22/2022 04:04:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
06/22/2022 04:04:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=74
06/22/2022 04:04:48 - INFO - __main__ - Global step 150 Train loss 0.28 ACC 0.53125 on epoch=74
06/22/2022 04:04:48 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.53125 on epoch=74, global_step=150
06/22/2022 04:04:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=79
06/22/2022 04:04:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
06/22/2022 04:04:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=89
06/22/2022 04:04:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
06/22/2022 04:04:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
06/22/2022 04:04:55 - INFO - __main__ - Global step 200 Train loss 0.28 ACC 0.5625 on epoch=99
06/22/2022 04:04:55 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=99, global_step=200
06/22/2022 04:04:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
06/22/2022 04:04:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=109
06/22/2022 04:04:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.20 on epoch=114
06/22/2022 04:05:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
06/22/2022 04:05:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.20 on epoch=124
06/22/2022 04:05:02 - INFO - __main__ - Global step 250 Train loss 0.23 ACC 0.46875 on epoch=124
06/22/2022 04:05:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
06/22/2022 04:05:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.21 on epoch=134
06/22/2022 04:05:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=139
06/22/2022 04:05:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.17 on epoch=144
06/22/2022 04:05:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=149
06/22/2022 04:05:08 - INFO - __main__ - Global step 300 Train loss 0.21 ACC 0.59375 on epoch=149
06/22/2022 04:05:08 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=149, global_step=300
06/22/2022 04:05:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.18 on epoch=154
06/22/2022 04:05:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.19 on epoch=159
06/22/2022 04:05:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.15 on epoch=164
06/22/2022 04:05:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.16 on epoch=169
06/22/2022 04:05:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.19 on epoch=174
06/22/2022 04:05:15 - INFO - __main__ - Global step 350 Train loss 0.18 ACC 0.53125 on epoch=174
06/22/2022 04:05:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.19 on epoch=179
06/22/2022 04:05:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.18 on epoch=184
06/22/2022 04:05:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.16 on epoch=189
06/22/2022 04:05:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.17 on epoch=194
06/22/2022 04:05:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.14 on epoch=199
06/22/2022 04:05:22 - INFO - __main__ - Global step 400 Train loss 0.17 ACC 0.6875 on epoch=199
06/22/2022 04:05:22 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.6875 on epoch=199, global_step=400
06/22/2022 04:05:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
06/22/2022 04:05:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=209
06/22/2022 04:05:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
06/22/2022 04:05:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.12 on epoch=219
06/22/2022 04:05:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=224
06/22/2022 04:05:29 - INFO - __main__ - Global step 450 Train loss 0.13 ACC 0.5625 on epoch=224
06/22/2022 04:05:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.10 on epoch=229
06/22/2022 04:05:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.16 on epoch=234
06/22/2022 04:05:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.12 on epoch=239
06/22/2022 04:05:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.09 on epoch=244
06/22/2022 04:05:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=249
06/22/2022 04:05:35 - INFO - __main__ - Global step 500 Train loss 0.12 ACC 0.71875 on epoch=249
06/22/2022 04:05:35 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=249, global_step=500
06/22/2022 04:05:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=254
06/22/2022 04:05:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.09 on epoch=259
06/22/2022 04:05:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
06/22/2022 04:05:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.07 on epoch=269
06/22/2022 04:05:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=274
06/22/2022 04:05:42 - INFO - __main__ - Global step 550 Train loss 0.09 ACC 0.75 on epoch=274
06/22/2022 04:05:42 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.75 on epoch=274, global_step=550
06/22/2022 04:05:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
06/22/2022 04:05:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=284
06/22/2022 04:05:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
06/22/2022 04:05:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
06/22/2022 04:05:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
06/22/2022 04:05:49 - INFO - __main__ - Global step 600 Train loss 0.05 ACC 0.71875 on epoch=299
06/22/2022 04:05:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
06/22/2022 04:05:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
06/22/2022 04:05:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/22/2022 04:05:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
06/22/2022 04:05:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
06/22/2022 04:05:55 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.75 on epoch=324
06/22/2022 04:05:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
06/22/2022 04:05:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
06/22/2022 04:05:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
06/22/2022 04:06:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
06/22/2022 04:06:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
06/22/2022 04:06:02 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.75 on epoch=349
06/22/2022 04:06:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
06/22/2022 04:06:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
06/22/2022 04:06:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
06/22/2022 04:06:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/22/2022 04:06:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
06/22/2022 04:06:09 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.6875 on epoch=374
06/22/2022 04:06:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/22/2022 04:06:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/22/2022 04:06:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
06/22/2022 04:06:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/22/2022 04:06:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
06/22/2022 04:06:16 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.75 on epoch=399
06/22/2022 04:06:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
06/22/2022 04:06:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/22/2022 04:06:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
06/22/2022 04:06:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
06/22/2022 04:06:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
06/22/2022 04:06:22 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.65625 on epoch=424
06/22/2022 04:06:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/22/2022 04:06:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
06/22/2022 04:06:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
06/22/2022 04:06:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
06/22/2022 04:06:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/22/2022 04:06:29 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.78125 on epoch=449
06/22/2022 04:06:29 - INFO - __main__ - Saving model with best ACC: 0.75 -> 0.78125 on epoch=449, global_step=900
06/22/2022 04:06:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/22/2022 04:06:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/22/2022 04:06:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/22/2022 04:06:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/22/2022 04:06:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
06/22/2022 04:06:36 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.625 on epoch=474
06/22/2022 04:06:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/22/2022 04:06:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
06/22/2022 04:06:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/22/2022 04:06:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/22/2022 04:06:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/22/2022 04:06:42 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.65625 on epoch=499
06/22/2022 04:06:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/22/2022 04:06:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/22/2022 04:06:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/22/2022 04:06:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/22/2022 04:06:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/22/2022 04:06:49 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.78125 on epoch=524
06/22/2022 04:06:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/22/2022 04:06:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/22/2022 04:06:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
06/22/2022 04:06:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/22/2022 04:06:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/22/2022 04:06:56 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.71875 on epoch=549
06/22/2022 04:06:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 04:06:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/22/2022 04:06:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/22/2022 04:07:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/22/2022 04:07:02 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/22/2022 04:07:02 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.71875 on epoch=574
06/22/2022 04:07:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/22/2022 04:07:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/22/2022 04:07:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/22/2022 04:07:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/22/2022 04:07:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/22/2022 04:07:09 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.71875 on epoch=599
06/22/2022 04:07:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/22/2022 04:07:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/22/2022 04:07:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 04:07:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/22/2022 04:07:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 04:07:16 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.71875 on epoch=624
06/22/2022 04:07:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 04:07:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 04:07:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 04:07:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/22/2022 04:07:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 04:07:22 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.71875 on epoch=649
06/22/2022 04:07:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 04:07:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 04:07:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 04:07:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 04:07:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/22/2022 04:07:29 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.65625 on epoch=674
06/22/2022 04:07:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 04:07:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 04:07:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 04:07:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 04:07:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 04:07:36 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.71875 on epoch=699
06/22/2022 04:07:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 04:07:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 04:07:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/22/2022 04:07:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 04:07:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/22/2022 04:07:42 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.65625 on epoch=724
06/22/2022 04:07:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 04:07:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 04:07:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
06/22/2022 04:07:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 04:07:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
06/22/2022 04:07:49 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.75 on epoch=749
06/22/2022 04:07:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 04:07:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 04:07:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
06/22/2022 04:07:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 04:07:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 04:07:56 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.71875 on epoch=774
06/22/2022 04:07:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 04:07:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 04:08:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 04:08:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 04:08:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=799
06/22/2022 04:08:03 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.75 on epoch=799
06/22/2022 04:08:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 04:08:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 04:08:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 04:08:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 04:08:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 04:08:09 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.6875 on epoch=824
06/22/2022 04:08:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 04:08:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 04:08:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/22/2022 04:08:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 04:08:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 04:08:16 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.71875 on epoch=849
06/22/2022 04:08:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 04:08:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/22/2022 04:08:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 04:08:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 04:08:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 04:08:23 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.75 on epoch=874
06/22/2022 04:08:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 04:08:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 04:08:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 04:08:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 04:08:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 04:08:29 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.75 on epoch=899
06/22/2022 04:08:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 04:08:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 04:08:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/22/2022 04:08:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 04:08:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 04:08:36 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.78125 on epoch=924
06/22/2022 04:08:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 04:08:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 04:08:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 04:08:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 04:08:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 04:08:43 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.78125 on epoch=949
06/22/2022 04:08:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 04:08:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 04:08:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 04:08:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 04:08:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 04:08:50 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.78125 on epoch=974
06/22/2022 04:08:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 04:08:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 04:08:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
06/22/2022 04:08:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 04:08:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 04:08:56 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.65625 on epoch=999
06/22/2022 04:08:56 - INFO - __main__ - save last model!
06/22/2022 04:08:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 04:08:56 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 04:08:56 - INFO - __main__ - Printing 3 examples
06/22/2022 04:08:56 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 04:08:56 - INFO - __main__ - ['equivalent']
06/22/2022 04:08:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 04:08:56 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:08:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 04:08:56 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:08:56 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:08:56 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:08:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:08:57 - INFO - __main__ - Printing 3 examples
06/22/2022 04:08:57 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/22/2022 04:08:57 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:08:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/22/2022 04:08:57 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:08:57 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/22/2022 04:08:57 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:08:57 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:08:57 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:08:57 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:08:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:08:57 - INFO - __main__ - Printing 3 examples
06/22/2022 04:08:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/22/2022 04:08:57 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:08:57 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/22/2022 04:08:57 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:08:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/22/2022 04:08:57 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:08:57 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:08:57 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:08:57 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:08:57 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 04:09:03 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:09:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:09:03 - INFO - __main__ - Starting training!
06/22/2022 04:09:05 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_100_0.5_8_predictions.txt
06/22/2022 04:09:05 - INFO - __main__ - ACC on test data: 0.6373
06/22/2022 04:09:05 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.5, bsz=8, dev_performance=0.78125, test_performance=0.6372549019607843
06/22/2022 04:09:05 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.4, bsz=8 ...
06/22/2022 04:09:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:09:06 - INFO - __main__ - Printing 3 examples
06/22/2022 04:09:06 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/22/2022 04:09:06 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:09:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/22/2022 04:09:06 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:09:06 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/22/2022 04:09:06 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:09:06 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:09:06 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:09:06 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:09:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:09:06 - INFO - __main__ - Printing 3 examples
06/22/2022 04:09:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/22/2022 04:09:06 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:09:06 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/22/2022 04:09:06 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:09:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/22/2022 04:09:06 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:09:06 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:09:06 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:09:06 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:09:11 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:09:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:09:11 - INFO - __main__ - Starting training!
06/22/2022 04:09:13 - INFO - __main__ - Step 10 Global step 10 Train loss 3.21 on epoch=4
06/22/2022 04:09:14 - INFO - __main__ - Step 20 Global step 20 Train loss 2.15 on epoch=9
06/22/2022 04:09:15 - INFO - __main__ - Step 30 Global step 30 Train loss 1.49 on epoch=14
06/22/2022 04:09:17 - INFO - __main__ - Step 40 Global step 40 Train loss 1.05 on epoch=19
06/22/2022 04:09:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.78 on epoch=24
06/22/2022 04:09:19 - INFO - __main__ - Global step 50 Train loss 1.74 ACC 0.46875 on epoch=24
06/22/2022 04:09:19 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.46875 on epoch=24, global_step=50
06/22/2022 04:09:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.70 on epoch=29
06/22/2022 04:09:21 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=34
06/22/2022 04:09:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=39
06/22/2022 04:09:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=44
06/22/2022 04:09:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=49
06/22/2022 04:09:25 - INFO - __main__ - Global step 100 Train loss 0.51 ACC 0.4375 on epoch=49
06/22/2022 04:09:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=54
06/22/2022 04:09:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.35 on epoch=59
06/22/2022 04:09:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=64
06/22/2022 04:09:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.33 on epoch=69
06/22/2022 04:09:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.29 on epoch=74
06/22/2022 04:09:32 - INFO - __main__ - Global step 150 Train loss 0.34 ACC 0.53125 on epoch=74
06/22/2022 04:09:32 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.53125 on epoch=74, global_step=150
06/22/2022 04:09:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.35 on epoch=79
06/22/2022 04:09:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
06/22/2022 04:09:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=89
06/22/2022 04:09:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=94
06/22/2022 04:09:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.30 on epoch=99
06/22/2022 04:09:39 - INFO - __main__ - Global step 200 Train loss 0.31 ACC 0.4375 on epoch=99
06/22/2022 04:09:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=104
06/22/2022 04:09:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.32 on epoch=109
06/22/2022 04:09:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=114
06/22/2022 04:09:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=119
06/22/2022 04:09:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=124
06/22/2022 04:09:45 - INFO - __main__ - Global step 250 Train loss 0.28 ACC 0.46875 on epoch=124
06/22/2022 04:09:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=129
06/22/2022 04:09:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.28 on epoch=134
06/22/2022 04:09:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=139
06/22/2022 04:09:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=144
06/22/2022 04:09:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.18 on epoch=149
06/22/2022 04:09:52 - INFO - __main__ - Global step 300 Train loss 0.24 ACC 0.5 on epoch=149
06/22/2022 04:09:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
06/22/2022 04:09:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
06/22/2022 04:09:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=164
06/22/2022 04:09:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=169
06/22/2022 04:09:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.20 on epoch=174
06/22/2022 04:09:59 - INFO - __main__ - Global step 350 Train loss 0.21 ACC 0.5 on epoch=174
06/22/2022 04:10:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.20 on epoch=179
06/22/2022 04:10:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.17 on epoch=184
06/22/2022 04:10:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=189
06/22/2022 04:10:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.18 on epoch=194
06/22/2022 04:10:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
06/22/2022 04:10:06 - INFO - __main__ - Global step 400 Train loss 0.20 ACC 0.59375 on epoch=199
06/22/2022 04:10:06 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=199, global_step=400
06/22/2022 04:10:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=204
06/22/2022 04:10:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=209
06/22/2022 04:10:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.18 on epoch=214
06/22/2022 04:10:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.14 on epoch=219
06/22/2022 04:10:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=224
06/22/2022 04:10:12 - INFO - __main__ - Global step 450 Train loss 0.17 ACC 0.5625 on epoch=224
06/22/2022 04:10:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.18 on epoch=229
06/22/2022 04:10:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.12 on epoch=234
06/22/2022 04:10:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.17 on epoch=239
06/22/2022 04:10:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.12 on epoch=244
06/22/2022 04:10:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.14 on epoch=249
06/22/2022 04:10:19 - INFO - __main__ - Global step 500 Train loss 0.15 ACC 0.71875 on epoch=249
06/22/2022 04:10:19 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.71875 on epoch=249, global_step=500
06/22/2022 04:10:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=254
06/22/2022 04:10:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.12 on epoch=259
06/22/2022 04:10:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=264
06/22/2022 04:10:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=269
06/22/2022 04:10:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.12 on epoch=274
06/22/2022 04:10:26 - INFO - __main__ - Global step 550 Train loss 0.12 ACC 0.65625 on epoch=274
06/22/2022 04:10:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=279
06/22/2022 04:10:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=284
06/22/2022 04:10:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
06/22/2022 04:10:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.07 on epoch=294
06/22/2022 04:10:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.08 on epoch=299
06/22/2022 04:10:33 - INFO - __main__ - Global step 600 Train loss 0.08 ACC 0.6875 on epoch=299
06/22/2022 04:10:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=304
06/22/2022 04:10:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=309
06/22/2022 04:10:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=314
06/22/2022 04:10:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
06/22/2022 04:10:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=324
06/22/2022 04:10:39 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.71875 on epoch=324
06/22/2022 04:10:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
06/22/2022 04:10:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
06/22/2022 04:10:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=339
06/22/2022 04:10:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
06/22/2022 04:10:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
06/22/2022 04:10:46 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.6875 on epoch=349
06/22/2022 04:10:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=354
06/22/2022 04:10:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
06/22/2022 04:10:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=364
06/22/2022 04:10:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
06/22/2022 04:10:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
06/22/2022 04:10:53 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.65625 on epoch=374
06/22/2022 04:10:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/22/2022 04:10:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/22/2022 04:10:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
06/22/2022 04:10:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/22/2022 04:10:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
06/22/2022 04:10:59 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.75 on epoch=399
06/22/2022 04:10:59 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.75 on epoch=399, global_step=800
06/22/2022 04:11:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
06/22/2022 04:11:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
06/22/2022 04:11:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/22/2022 04:11:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
06/22/2022 04:11:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/22/2022 04:11:06 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.6875 on epoch=424
06/22/2022 04:11:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/22/2022 04:11:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/22/2022 04:11:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
06/22/2022 04:11:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
06/22/2022 04:11:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/22/2022 04:11:13 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.6875 on epoch=449
06/22/2022 04:11:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/22/2022 04:11:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/22/2022 04:11:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/22/2022 04:11:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/22/2022 04:11:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
06/22/2022 04:11:19 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.65625 on epoch=474
06/22/2022 04:11:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/22/2022 04:11:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/22/2022 04:11:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/22/2022 04:11:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/22/2022 04:11:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
06/22/2022 04:11:26 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.6875 on epoch=499
06/22/2022 04:11:27 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/22/2022 04:11:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/22/2022 04:11:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/22/2022 04:11:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/22/2022 04:11:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/22/2022 04:11:33 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.75 on epoch=524
06/22/2022 04:11:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/22/2022 04:11:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/22/2022 04:11:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
06/22/2022 04:11:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
06/22/2022 04:11:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/22/2022 04:11:40 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.65625 on epoch=549
06/22/2022 04:11:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 04:11:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/22/2022 04:11:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/22/2022 04:11:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/22/2022 04:11:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/22/2022 04:11:46 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.65625 on epoch=574
06/22/2022 04:11:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/22/2022 04:11:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/22/2022 04:11:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/22/2022 04:11:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 04:11:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/22/2022 04:11:53 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.625 on epoch=599
06/22/2022 04:11:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/22/2022 04:11:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/22/2022 04:11:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 04:11:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/22/2022 04:11:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/22/2022 04:12:00 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.75 on epoch=624
06/22/2022 04:12:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/22/2022 04:12:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
06/22/2022 04:12:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 04:12:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 04:12:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 04:12:06 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.8125 on epoch=649
06/22/2022 04:12:06 - INFO - __main__ - Saving model with best ACC: 0.75 -> 0.8125 on epoch=649, global_step=1300
06/22/2022 04:12:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 04:12:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/22/2022 04:12:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 04:12:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 04:12:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 04:12:13 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.75 on epoch=674
06/22/2022 04:12:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 04:12:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 04:12:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 04:12:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 04:12:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 04:12:20 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.6875 on epoch=699
06/22/2022 04:12:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 04:12:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 04:12:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 04:12:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 04:12:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/22/2022 04:12:27 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.78125 on epoch=724
06/22/2022 04:12:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 04:12:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 04:12:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 04:12:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 04:12:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 04:12:33 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.6875 on epoch=749
06/22/2022 04:12:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 04:12:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 04:12:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 04:12:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 04:12:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/22/2022 04:12:40 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.71875 on epoch=774
06/22/2022 04:12:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 04:12:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 04:12:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/22/2022 04:12:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/22/2022 04:12:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 04:12:47 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.65625 on epoch=799
06/22/2022 04:12:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/22/2022 04:12:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
06/22/2022 04:12:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 04:12:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 04:12:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 04:12:54 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.6875 on epoch=824
06/22/2022 04:12:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 04:12:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 04:12:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 04:12:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 04:13:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 04:13:00 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.65625 on epoch=849
06/22/2022 04:13:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 04:13:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 04:13:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 04:13:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 04:13:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 04:13:07 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.6875 on epoch=874
06/22/2022 04:13:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 04:13:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 04:13:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 04:13:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 04:13:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 04:13:14 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.625 on epoch=899
06/22/2022 04:13:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 04:13:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 04:13:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 04:13:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 04:13:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 04:13:21 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.65625 on epoch=924
06/22/2022 04:13:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 04:13:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/22/2022 04:13:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 04:13:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 04:13:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 04:13:27 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.625 on epoch=949
06/22/2022 04:13:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 04:13:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/22/2022 04:13:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/22/2022 04:13:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 04:13:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 04:13:34 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.71875 on epoch=974
06/22/2022 04:13:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 04:13:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 04:13:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 04:13:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 04:13:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 04:13:41 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.75 on epoch=999
06/22/2022 04:13:41 - INFO - __main__ - save last model!
06/22/2022 04:13:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 04:13:41 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 04:13:41 - INFO - __main__ - Printing 3 examples
06/22/2022 04:13:41 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 04:13:41 - INFO - __main__ - ['equivalent']
06/22/2022 04:13:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 04:13:41 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 04:13:41 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:41 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:13:41 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:13:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:13:41 - INFO - __main__ - Printing 3 examples
06/22/2022 04:13:41 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/22/2022 04:13:41 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/22/2022 04:13:41 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:41 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/22/2022 04:13:41 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:41 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:13:41 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:13:41 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:13:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:13:41 - INFO - __main__ - Printing 3 examples
06/22/2022 04:13:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/22/2022 04:13:41 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:41 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/22/2022 04:13:41 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/22/2022 04:13:41 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:41 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:13:41 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:13:41 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:13:41 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 04:13:47 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:13:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:13:47 - INFO - __main__ - Starting training!
06/22/2022 04:13:49 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_100_0.4_8_predictions.txt
06/22/2022 04:13:49 - INFO - __main__ - ACC on test data: 0.5858
06/22/2022 04:13:49 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.4, bsz=8, dev_performance=0.8125, test_performance=0.5857843137254902
06/22/2022 04:13:49 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.3, bsz=8 ...
06/22/2022 04:13:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:13:50 - INFO - __main__ - Printing 3 examples
06/22/2022 04:13:50 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/22/2022 04:13:50 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:50 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/22/2022 04:13:50 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:50 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/22/2022 04:13:50 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:50 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:13:50 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:13:50 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:13:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:13:50 - INFO - __main__ - Printing 3 examples
06/22/2022 04:13:50 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/22/2022 04:13:50 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:50 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/22/2022 04:13:50 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:50 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/22/2022 04:13:50 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:13:50 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:13:50 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:13:51 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:13:56 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:13:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:13:56 - INFO - __main__ - Starting training!
06/22/2022 04:13:58 - INFO - __main__ - Step 10 Global step 10 Train loss 3.28 on epoch=4
06/22/2022 04:13:59 - INFO - __main__ - Step 20 Global step 20 Train loss 2.36 on epoch=9
06/22/2022 04:14:00 - INFO - __main__ - Step 30 Global step 30 Train loss 1.90 on epoch=14
06/22/2022 04:14:01 - INFO - __main__ - Step 40 Global step 40 Train loss 1.44 on epoch=19
06/22/2022 04:14:02 - INFO - __main__ - Step 50 Global step 50 Train loss 1.12 on epoch=24
06/22/2022 04:14:03 - INFO - __main__ - Global step 50 Train loss 2.02 ACC 0.15625 on epoch=24
06/22/2022 04:14:03 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
06/22/2022 04:14:04 - INFO - __main__ - Step 60 Global step 60 Train loss 0.87 on epoch=29
06/22/2022 04:14:06 - INFO - __main__ - Step 70 Global step 70 Train loss 0.69 on epoch=34
06/22/2022 04:14:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=39
06/22/2022 04:14:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=44
06/22/2022 04:14:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=49
06/22/2022 04:14:10 - INFO - __main__ - Global step 100 Train loss 0.64 ACC 0.53125 on epoch=49
06/22/2022 04:14:10 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.53125 on epoch=49, global_step=100
06/22/2022 04:14:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=54
06/22/2022 04:14:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=59
06/22/2022 04:14:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=64
06/22/2022 04:14:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.38 on epoch=69
06/22/2022 04:14:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.33 on epoch=74
06/22/2022 04:14:17 - INFO - __main__ - Global step 150 Train loss 0.39 ACC 0.5 on epoch=74
06/22/2022 04:14:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=79
06/22/2022 04:14:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=84
06/22/2022 04:14:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.30 on epoch=89
06/22/2022 04:14:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.31 on epoch=94
06/22/2022 04:14:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.29 on epoch=99
06/22/2022 04:14:23 - INFO - __main__ - Global step 200 Train loss 0.34 ACC 0.59375 on epoch=99
06/22/2022 04:14:23 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=99, global_step=200
06/22/2022 04:14:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.30 on epoch=104
06/22/2022 04:14:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.31 on epoch=109
06/22/2022 04:14:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
06/22/2022 04:14:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.31 on epoch=119
06/22/2022 04:14:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=124
06/22/2022 04:14:30 - INFO - __main__ - Global step 250 Train loss 0.29 ACC 0.65625 on epoch=124
06/22/2022 04:14:30 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=124, global_step=250
06/22/2022 04:14:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=129
06/22/2022 04:14:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=134
06/22/2022 04:14:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.29 on epoch=139
06/22/2022 04:14:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.30 on epoch=144
06/22/2022 04:14:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
06/22/2022 04:14:37 - INFO - __main__ - Global step 300 Train loss 0.29 ACC 0.5625 on epoch=149
06/22/2022 04:14:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=154
06/22/2022 04:14:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
06/22/2022 04:14:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
06/22/2022 04:14:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=169
06/22/2022 04:14:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
06/22/2022 04:14:43 - INFO - __main__ - Global step 350 Train loss 0.24 ACC 0.4375 on epoch=174
06/22/2022 04:14:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
06/22/2022 04:14:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=184
06/22/2022 04:14:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.27 on epoch=189
06/22/2022 04:14:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=194
06/22/2022 04:14:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=199
06/22/2022 04:14:50 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.5 on epoch=199
06/22/2022 04:14:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.19 on epoch=204
06/22/2022 04:14:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
06/22/2022 04:14:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.20 on epoch=214
06/22/2022 04:14:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=219
06/22/2022 04:14:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=224
06/22/2022 04:14:57 - INFO - __main__ - Global step 450 Train loss 0.21 ACC 0.46875 on epoch=224
06/22/2022 04:14:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=229
06/22/2022 04:14:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=234
06/22/2022 04:15:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
06/22/2022 04:15:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
06/22/2022 04:15:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=249
06/22/2022 04:15:04 - INFO - __main__ - Global step 500 Train loss 0.22 ACC 0.625 on epoch=249
06/22/2022 04:15:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=254
06/22/2022 04:15:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=259
06/22/2022 04:15:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=264
06/22/2022 04:15:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=269
06/22/2022 04:15:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=274
06/22/2022 04:15:10 - INFO - __main__ - Global step 550 Train loss 0.17 ACC 0.625 on epoch=274
06/22/2022 04:15:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=279
06/22/2022 04:15:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=284
06/22/2022 04:15:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=289
06/22/2022 04:15:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.17 on epoch=294
06/22/2022 04:15:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=299
06/22/2022 04:15:17 - INFO - __main__ - Global step 600 Train loss 0.17 ACC 0.6875 on epoch=299
06/22/2022 04:15:17 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=299, global_step=600
06/22/2022 04:15:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=304
06/22/2022 04:15:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=309
06/22/2022 04:15:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.14 on epoch=314
06/22/2022 04:15:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=319
06/22/2022 04:15:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=324
06/22/2022 04:15:24 - INFO - __main__ - Global step 650 Train loss 0.14 ACC 0.59375 on epoch=324
06/22/2022 04:15:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=329
06/22/2022 04:15:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=334
06/22/2022 04:15:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=339
06/22/2022 04:15:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=344
06/22/2022 04:15:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=349
06/22/2022 04:15:31 - INFO - __main__ - Global step 700 Train loss 0.12 ACC 0.6875 on epoch=349
06/22/2022 04:15:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=354
06/22/2022 04:15:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=359
06/22/2022 04:15:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=364
06/22/2022 04:15:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=369
06/22/2022 04:15:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=374
06/22/2022 04:15:37 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.6875 on epoch=374
06/22/2022 04:15:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=379
06/22/2022 04:15:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=384
06/22/2022 04:15:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=389
06/22/2022 04:15:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=394
06/22/2022 04:15:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=399
06/22/2022 04:15:44 - INFO - __main__ - Global step 800 Train loss 0.07 ACC 0.71875 on epoch=399
06/22/2022 04:15:44 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=399, global_step=800
06/22/2022 04:15:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
06/22/2022 04:15:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
06/22/2022 04:15:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=414
06/22/2022 04:15:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
06/22/2022 04:15:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/22/2022 04:15:51 - INFO - __main__ - Global step 850 Train loss 0.05 ACC 0.6875 on epoch=424
06/22/2022 04:15:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
06/22/2022 04:15:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
06/22/2022 04:15:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
06/22/2022 04:15:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
06/22/2022 04:15:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
06/22/2022 04:15:57 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.78125 on epoch=449
06/22/2022 04:15:57 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.78125 on epoch=449, global_step=900
06/22/2022 04:15:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
06/22/2022 04:16:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
06/22/2022 04:16:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
06/22/2022 04:16:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/22/2022 04:16:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
06/22/2022 04:16:04 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.6875 on epoch=474
06/22/2022 04:16:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/22/2022 04:16:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/22/2022 04:16:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
06/22/2022 04:16:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/22/2022 04:16:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
06/22/2022 04:16:11 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.65625 on epoch=499
06/22/2022 04:16:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
06/22/2022 04:16:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
06/22/2022 04:16:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
06/22/2022 04:16:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/22/2022 04:16:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/22/2022 04:16:18 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.65625 on epoch=524
06/22/2022 04:16:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/22/2022 04:16:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/22/2022 04:16:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/22/2022 04:16:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
06/22/2022 04:16:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/22/2022 04:16:24 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.71875 on epoch=549
06/22/2022 04:16:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
06/22/2022 04:16:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/22/2022 04:16:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 04:16:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/22/2022 04:16:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/22/2022 04:16:31 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.6875 on epoch=574
06/22/2022 04:16:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/22/2022 04:16:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/22/2022 04:16:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/22/2022 04:16:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/22/2022 04:16:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/22/2022 04:16:38 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.71875 on epoch=599
06/22/2022 04:16:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/22/2022 04:16:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/22/2022 04:16:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/22/2022 04:16:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/22/2022 04:16:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/22/2022 04:16:44 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.6875 on epoch=624
06/22/2022 04:16:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/22/2022 04:16:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/22/2022 04:16:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/22/2022 04:16:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 04:16:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 04:16:51 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.6875 on epoch=649
06/22/2022 04:16:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/22/2022 04:16:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/22/2022 04:16:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 04:16:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
06/22/2022 04:16:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/22/2022 04:16:58 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.71875 on epoch=674
06/22/2022 04:16:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 04:17:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/22/2022 04:17:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/22/2022 04:17:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 04:17:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 04:17:04 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.625 on epoch=699
06/22/2022 04:17:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 04:17:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
06/22/2022 04:17:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 04:17:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 04:17:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 04:17:11 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.6875 on epoch=724
06/22/2022 04:17:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 04:17:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 04:17:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 04:17:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 04:17:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 04:17:18 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.6875 on epoch=749
06/22/2022 04:17:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 04:17:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 04:17:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/22/2022 04:17:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 04:17:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 04:17:24 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.75 on epoch=774
06/22/2022 04:17:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 04:17:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 04:17:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 04:17:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
06/22/2022 04:17:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 04:17:31 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.78125 on epoch=799
06/22/2022 04:17:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 04:17:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 04:17:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 04:17:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 04:17:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 04:17:38 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.75 on epoch=824
06/22/2022 04:17:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 04:17:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 04:17:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 04:17:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 04:17:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 04:17:44 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.78125 on epoch=849
06/22/2022 04:17:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 04:17:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/22/2022 04:17:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 04:17:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/22/2022 04:17:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 04:17:51 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.71875 on epoch=874
06/22/2022 04:17:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
06/22/2022 04:17:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 04:17:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=889
06/22/2022 04:17:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 04:17:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 04:17:58 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.78125 on epoch=899
06/22/2022 04:17:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 04:18:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 04:18:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 04:18:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=919
06/22/2022 04:18:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 04:18:04 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.75 on epoch=924
06/22/2022 04:18:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 04:18:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 04:18:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 04:18:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 04:18:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 04:18:11 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.78125 on epoch=949
06/22/2022 04:18:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 04:18:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 04:18:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 04:18:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 04:18:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 04:18:18 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.71875 on epoch=974
06/22/2022 04:18:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/22/2022 04:18:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 04:18:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/22/2022 04:18:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 04:18:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 04:18:24 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.71875 on epoch=999
06/22/2022 04:18:24 - INFO - __main__ - save last model!
06/22/2022 04:18:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 04:18:24 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 04:18:24 - INFO - __main__ - Printing 3 examples
06/22/2022 04:18:25 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 04:18:25 - INFO - __main__ - ['equivalent']
06/22/2022 04:18:25 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 04:18:25 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:25 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 04:18:25 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:25 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:18:25 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:18:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:18:25 - INFO - __main__ - Printing 3 examples
06/22/2022 04:18:25 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/22/2022 04:18:25 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:25 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/22/2022 04:18:25 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:25 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/22/2022 04:18:25 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:25 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:18:25 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:18:25 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:18:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:18:25 - INFO - __main__ - Printing 3 examples
06/22/2022 04:18:25 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/22/2022 04:18:25 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:25 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/22/2022 04:18:25 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:25 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/22/2022 04:18:25 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:25 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:18:25 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:18:25 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:18:25 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 04:18:30 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:18:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:18:31 - INFO - __main__ - Starting training!
06/22/2022 04:18:33 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_100_0.3_8_predictions.txt
06/22/2022 04:18:33 - INFO - __main__ - ACC on test data: 0.6201
06/22/2022 04:18:33 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.3, bsz=8, dev_performance=0.78125, test_performance=0.6200980392156863
06/22/2022 04:18:33 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.2, bsz=8 ...
06/22/2022 04:18:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:18:34 - INFO - __main__ - Printing 3 examples
06/22/2022 04:18:34 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/22/2022 04:18:34 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:34 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/22/2022 04:18:34 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:34 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/22/2022 04:18:34 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:34 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:18:34 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:18:34 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:18:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:18:34 - INFO - __main__ - Printing 3 examples
06/22/2022 04:18:34 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/22/2022 04:18:34 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:34 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/22/2022 04:18:34 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:34 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/22/2022 04:18:34 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:18:34 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:18:34 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:18:34 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:18:39 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:18:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:18:40 - INFO - __main__ - Starting training!
06/22/2022 04:18:41 - INFO - __main__ - Step 10 Global step 10 Train loss 3.37 on epoch=4
06/22/2022 04:18:42 - INFO - __main__ - Step 20 Global step 20 Train loss 2.82 on epoch=9
06/22/2022 04:18:44 - INFO - __main__ - Step 30 Global step 30 Train loss 2.28 on epoch=14
06/22/2022 04:18:45 - INFO - __main__ - Step 40 Global step 40 Train loss 1.94 on epoch=19
06/22/2022 04:18:46 - INFO - __main__ - Step 50 Global step 50 Train loss 1.56 on epoch=24
06/22/2022 04:18:47 - INFO - __main__ - Global step 50 Train loss 2.39 ACC 0.0 on epoch=24
06/22/2022 04:18:47 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/22/2022 04:18:48 - INFO - __main__ - Step 60 Global step 60 Train loss 1.29 on epoch=29
06/22/2022 04:18:49 - INFO - __main__ - Step 70 Global step 70 Train loss 1.24 on epoch=34
06/22/2022 04:18:50 - INFO - __main__ - Step 80 Global step 80 Train loss 1.02 on epoch=39
06/22/2022 04:18:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=44
06/22/2022 04:18:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.68 on epoch=49
06/22/2022 04:18:53 - INFO - __main__ - Global step 100 Train loss 1.01 ACC 0.46875 on epoch=49
06/22/2022 04:18:53 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.46875 on epoch=49, global_step=100
06/22/2022 04:18:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.64 on epoch=54
06/22/2022 04:18:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.65 on epoch=59
06/22/2022 04:18:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.66 on epoch=64
06/22/2022 04:18:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=69
06/22/2022 04:19:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=74
06/22/2022 04:19:00 - INFO - __main__ - Global step 150 Train loss 0.60 ACC 0.5 on epoch=74
06/22/2022 04:19:00 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=74, global_step=150
06/22/2022 04:19:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=79
06/22/2022 04:19:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=84
06/22/2022 04:19:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=89
06/22/2022 04:19:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
06/22/2022 04:19:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=99
06/22/2022 04:19:07 - INFO - __main__ - Global step 200 Train loss 0.41 ACC 0.59375 on epoch=99
06/22/2022 04:19:07 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=99, global_step=200
06/22/2022 04:19:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=104
06/22/2022 04:19:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=109
06/22/2022 04:19:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
06/22/2022 04:19:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=119
06/22/2022 04:19:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=124
06/22/2022 04:19:14 - INFO - __main__ - Global step 250 Train loss 0.37 ACC 0.5625 on epoch=124
06/22/2022 04:19:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=129
06/22/2022 04:19:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=134
06/22/2022 04:19:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=139
06/22/2022 04:19:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=144
06/22/2022 04:19:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=149
06/22/2022 04:19:20 - INFO - __main__ - Global step 300 Train loss 0.33 ACC 0.53125 on epoch=149
06/22/2022 04:19:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
06/22/2022 04:19:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=159
06/22/2022 04:19:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=164
06/22/2022 04:19:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
06/22/2022 04:19:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=174
06/22/2022 04:19:27 - INFO - __main__ - Global step 350 Train loss 0.30 ACC 0.59375 on epoch=174
06/22/2022 04:19:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
06/22/2022 04:19:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=184
06/22/2022 04:19:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
06/22/2022 04:19:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=194
06/22/2022 04:19:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
06/22/2022 04:19:34 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.59375 on epoch=199
06/22/2022 04:19:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=204
06/22/2022 04:19:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
06/22/2022 04:19:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=214
06/22/2022 04:19:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=219
06/22/2022 04:19:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/22/2022 04:19:41 - INFO - __main__ - Global step 450 Train loss 0.27 ACC 0.59375 on epoch=224
06/22/2022 04:19:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=229
06/22/2022 04:19:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=234
06/22/2022 04:19:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=239
06/22/2022 04:19:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
06/22/2022 04:19:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
06/22/2022 04:19:47 - INFO - __main__ - Global step 500 Train loss 0.24 ACC 0.53125 on epoch=249
06/22/2022 04:19:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
06/22/2022 04:19:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
06/22/2022 04:19:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=264
06/22/2022 04:19:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=269
06/22/2022 04:19:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=274
06/22/2022 04:19:54 - INFO - __main__ - Global step 550 Train loss 0.23 ACC 0.53125 on epoch=274
06/22/2022 04:19:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=279
06/22/2022 04:19:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
06/22/2022 04:19:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=289
06/22/2022 04:19:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=294
06/22/2022 04:20:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=299
06/22/2022 04:20:01 - INFO - __main__ - Global step 600 Train loss 0.22 ACC 0.65625 on epoch=299
06/22/2022 04:20:01 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=299, global_step=600
06/22/2022 04:20:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=304
06/22/2022 04:20:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=309
06/22/2022 04:20:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
06/22/2022 04:20:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=319
06/22/2022 04:20:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=324
06/22/2022 04:20:08 - INFO - __main__ - Global step 650 Train loss 0.20 ACC 0.53125 on epoch=324
06/22/2022 04:20:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=329
06/22/2022 04:20:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=334
06/22/2022 04:20:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=339
06/22/2022 04:20:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=344
06/22/2022 04:20:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=349
06/22/2022 04:20:14 - INFO - __main__ - Global step 700 Train loss 0.18 ACC 0.5 on epoch=349
06/22/2022 04:20:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=354
06/22/2022 04:20:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=359
06/22/2022 04:20:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=364
06/22/2022 04:20:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=369
06/22/2022 04:20:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=374
06/22/2022 04:20:21 - INFO - __main__ - Global step 750 Train loss 0.16 ACC 0.5625 on epoch=374
06/22/2022 04:20:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.17 on epoch=379
06/22/2022 04:20:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.17 on epoch=384
06/22/2022 04:20:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=389
06/22/2022 04:20:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=394
06/22/2022 04:20:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=399
06/22/2022 04:20:28 - INFO - __main__ - Global step 800 Train loss 0.15 ACC 0.5625 on epoch=399
06/22/2022 04:20:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=404
06/22/2022 04:20:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=409
06/22/2022 04:20:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=414
06/22/2022 04:20:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=419
06/22/2022 04:20:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=424
06/22/2022 04:20:35 - INFO - __main__ - Global step 850 Train loss 0.12 ACC 0.53125 on epoch=424
06/22/2022 04:20:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=429
06/22/2022 04:20:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=434
06/22/2022 04:20:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=439
06/22/2022 04:20:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=444
06/22/2022 04:20:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=449
06/22/2022 04:20:42 - INFO - __main__ - Global step 900 Train loss 0.10 ACC 0.53125 on epoch=449
06/22/2022 04:20:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=454
06/22/2022 04:20:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=459
06/22/2022 04:20:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=464
06/22/2022 04:20:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
06/22/2022 04:20:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=474
06/22/2022 04:20:48 - INFO - __main__ - Global step 950 Train loss 0.08 ACC 0.53125 on epoch=474
06/22/2022 04:20:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
06/22/2022 04:20:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=484
06/22/2022 04:20:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=489
06/22/2022 04:20:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=494
06/22/2022 04:20:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=499
06/22/2022 04:20:55 - INFO - __main__ - Global step 1000 Train loss 0.07 ACC 0.5 on epoch=499
06/22/2022 04:20:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=504
06/22/2022 04:20:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
06/22/2022 04:20:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
06/22/2022 04:21:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
06/22/2022 04:21:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=524
06/22/2022 04:21:02 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.59375 on epoch=524
06/22/2022 04:21:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
06/22/2022 04:21:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=534
06/22/2022 04:21:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
06/22/2022 04:21:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=544
06/22/2022 04:21:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
06/22/2022 04:21:09 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.5625 on epoch=549
06/22/2022 04:21:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=554
06/22/2022 04:21:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
06/22/2022 04:21:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=564
06/22/2022 04:21:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=569
06/22/2022 04:21:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
06/22/2022 04:21:15 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.5625 on epoch=574
06/22/2022 04:21:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/22/2022 04:21:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
06/22/2022 04:21:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=589
06/22/2022 04:21:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
06/22/2022 04:21:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
06/22/2022 04:21:22 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.59375 on epoch=599
06/22/2022 04:21:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
06/22/2022 04:21:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
06/22/2022 04:21:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=614
06/22/2022 04:21:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/22/2022 04:21:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/22/2022 04:21:29 - INFO - __main__ - Global step 1250 Train loss 0.04 ACC 0.625 on epoch=624
06/22/2022 04:21:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
06/22/2022 04:21:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
06/22/2022 04:21:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
06/22/2022 04:21:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
06/22/2022 04:21:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=649
06/22/2022 04:21:36 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.59375 on epoch=649
06/22/2022 04:21:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
06/22/2022 04:21:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
06/22/2022 04:21:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=664
06/22/2022 04:21:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=669
06/22/2022 04:21:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/22/2022 04:21:42 - INFO - __main__ - Global step 1350 Train loss 0.04 ACC 0.46875 on epoch=674
06/22/2022 04:21:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/22/2022 04:21:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
06/22/2022 04:21:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
06/22/2022 04:21:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
06/22/2022 04:21:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/22/2022 04:21:49 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.59375 on epoch=699
06/22/2022 04:21:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 04:21:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
06/22/2022 04:21:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=714
06/22/2022 04:21:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/22/2022 04:21:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
06/22/2022 04:21:56 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.5625 on epoch=724
06/22/2022 04:21:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
06/22/2022 04:21:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
06/22/2022 04:21:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
06/22/2022 04:22:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/22/2022 04:22:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
06/22/2022 04:22:03 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.65625 on epoch=749
06/22/2022 04:22:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/22/2022 04:22:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
06/22/2022 04:22:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
06/22/2022 04:22:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/22/2022 04:22:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/22/2022 04:22:09 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.59375 on epoch=774
06/22/2022 04:22:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
06/22/2022 04:22:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=784
06/22/2022 04:22:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
06/22/2022 04:22:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/22/2022 04:22:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
06/22/2022 04:22:16 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.59375 on epoch=799
06/22/2022 04:22:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/22/2022 04:22:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/22/2022 04:22:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/22/2022 04:22:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/22/2022 04:22:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
06/22/2022 04:22:23 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.59375 on epoch=824
06/22/2022 04:22:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/22/2022 04:22:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
06/22/2022 04:22:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/22/2022 04:22:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=844
06/22/2022 04:22:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/22/2022 04:22:30 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.5625 on epoch=849
06/22/2022 04:22:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/22/2022 04:22:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
06/22/2022 04:22:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
06/22/2022 04:22:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/22/2022 04:22:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/22/2022 04:22:36 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.59375 on epoch=874
06/22/2022 04:22:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/22/2022 04:22:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
06/22/2022 04:22:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
06/22/2022 04:22:41 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/22/2022 04:22:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/22/2022 04:22:43 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.59375 on epoch=899
06/22/2022 04:22:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/22/2022 04:22:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/22/2022 04:22:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/22/2022 04:22:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/22/2022 04:22:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 04:22:50 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.5625 on epoch=924
06/22/2022 04:22:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=929
06/22/2022 04:22:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/22/2022 04:22:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 04:22:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 04:22:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
06/22/2022 04:22:56 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.5625 on epoch=949
06/22/2022 04:22:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/22/2022 04:22:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/22/2022 04:23:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/22/2022 04:23:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/22/2022 04:23:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 04:23:03 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.59375 on epoch=974
06/22/2022 04:23:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/22/2022 04:23:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
06/22/2022 04:23:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/22/2022 04:23:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/22/2022 04:23:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/22/2022 04:23:10 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.6875 on epoch=999
06/22/2022 04:23:10 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=999, global_step=2000
06/22/2022 04:23:10 - INFO - __main__ - save last model!
06/22/2022 04:23:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 04:23:10 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 04:23:10 - INFO - __main__ - Printing 3 examples
06/22/2022 04:23:10 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 04:23:10 - INFO - __main__ - ['equivalent']
06/22/2022 04:23:10 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 04:23:10 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:23:10 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 04:23:10 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:23:10 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:23:10 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:23:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:23:11 - INFO - __main__ - Printing 3 examples
06/22/2022 04:23:11 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/22/2022 04:23:11 - INFO - __main__ - ['equivalent']
06/22/2022 04:23:11 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/22/2022 04:23:11 - INFO - __main__ - ['equivalent']
06/22/2022 04:23:11 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/22/2022 04:23:11 - INFO - __main__ - ['equivalent']
06/22/2022 04:23:11 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:23:11 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:23:11 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:23:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:23:11 - INFO - __main__ - Printing 3 examples
06/22/2022 04:23:11 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/22/2022 04:23:11 - INFO - __main__ - ['equivalent']
06/22/2022 04:23:11 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/22/2022 04:23:11 - INFO - __main__ - ['equivalent']
06/22/2022 04:23:11 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/22/2022 04:23:11 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 04:23:11 - INFO - __main__ - ['equivalent']
06/22/2022 04:23:11 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:23:11 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:23:11 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:23:16 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:23:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:23:16 - INFO - __main__ - Starting training!
06/22/2022 04:23:18 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_100_0.2_8_predictions.txt
06/22/2022 04:23:18 - INFO - __main__ - ACC on test data: 0.6275
06/22/2022 04:23:19 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.2, bsz=8, dev_performance=0.6875, test_performance=0.6274509803921569
06/22/2022 04:23:19 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.5, bsz=8 ...
06/22/2022 04:23:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:23:20 - INFO - __main__ - Printing 3 examples
06/22/2022 04:23:20 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/22/2022 04:23:20 - INFO - __main__ - ['equivalent']
06/22/2022 04:23:20 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/22/2022 04:23:20 - INFO - __main__ - ['equivalent']
06/22/2022 04:23:20 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/22/2022 04:23:20 - INFO - __main__ - ['equivalent']
06/22/2022 04:23:20 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:23:20 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:23:20 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:23:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:23:20 - INFO - __main__ - Printing 3 examples
06/22/2022 04:23:20 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/22/2022 04:23:20 - INFO - __main__ - ['equivalent']
06/22/2022 04:23:20 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/22/2022 04:23:20 - INFO - __main__ - ['equivalent']
06/22/2022 04:23:20 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/22/2022 04:23:20 - INFO - __main__ - ['equivalent']
06/22/2022 04:23:20 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:23:20 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:23:20 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:23:25 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:23:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:23:25 - INFO - __main__ - Starting training!
06/22/2022 04:23:27 - INFO - __main__ - Step 10 Global step 10 Train loss 3.32 on epoch=4
06/22/2022 04:23:28 - INFO - __main__ - Step 20 Global step 20 Train loss 2.24 on epoch=9
06/22/2022 04:23:29 - INFO - __main__ - Step 30 Global step 30 Train loss 1.41 on epoch=14
06/22/2022 04:23:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.93 on epoch=19
06/22/2022 04:23:32 - INFO - __main__ - Step 50 Global step 50 Train loss 0.69 on epoch=24
06/22/2022 04:23:32 - INFO - __main__ - Global step 50 Train loss 1.72 ACC 0.5 on epoch=24
06/22/2022 04:23:32 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/22/2022 04:23:33 - INFO - __main__ - Step 60 Global step 60 Train loss 0.60 on epoch=29
06/22/2022 04:23:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.57 on epoch=34
06/22/2022 04:23:36 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=39
06/22/2022 04:23:37 - INFO - __main__ - Step 90 Global step 90 Train loss 0.37 on epoch=44
06/22/2022 04:23:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.34 on epoch=49
06/22/2022 04:23:39 - INFO - __main__ - Global step 100 Train loss 0.46 ACC 0.40625 on epoch=49
06/22/2022 04:23:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
06/22/2022 04:23:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
06/22/2022 04:23:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=64
06/22/2022 04:23:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.35 on epoch=69
06/22/2022 04:23:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.33 on epoch=74
06/22/2022 04:23:46 - INFO - __main__ - Global step 150 Train loss 0.36 ACC 0.5 on epoch=74
06/22/2022 04:23:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.35 on epoch=79
06/22/2022 04:23:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
06/22/2022 04:23:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.30 on epoch=89
06/22/2022 04:23:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
06/22/2022 04:23:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=99
06/22/2022 04:23:52 - INFO - __main__ - Global step 200 Train loss 0.30 ACC 0.5 on epoch=99
06/22/2022 04:23:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.34 on epoch=104
06/22/2022 04:23:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.32 on epoch=109
06/22/2022 04:23:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
06/22/2022 04:23:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
06/22/2022 04:23:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.33 on epoch=124
06/22/2022 04:23:59 - INFO - __main__ - Global step 250 Train loss 0.30 ACC 0.53125 on epoch=124
06/22/2022 04:23:59 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=124, global_step=250
06/22/2022 04:24:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=129
06/22/2022 04:24:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.24 on epoch=134
06/22/2022 04:24:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=139
06/22/2022 04:24:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=144
06/22/2022 04:24:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.28 on epoch=149
06/22/2022 04:24:06 - INFO - __main__ - Global step 300 Train loss 0.26 ACC 0.5 on epoch=149
06/22/2022 04:24:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=154
06/22/2022 04:24:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
06/22/2022 04:24:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
06/22/2022 04:24:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
06/22/2022 04:24:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
06/22/2022 04:24:13 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.5 on epoch=174
06/22/2022 04:24:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=179
06/22/2022 04:24:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=184
06/22/2022 04:24:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=189
06/22/2022 04:24:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=194
06/22/2022 04:24:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.18 on epoch=199
06/22/2022 04:24:20 - INFO - __main__ - Global step 400 Train loss 0.21 ACC 0.5625 on epoch=199
06/22/2022 04:24:20 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=199, global_step=400
06/22/2022 04:24:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.18 on epoch=204
06/22/2022 04:24:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=209
06/22/2022 04:24:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.15 on epoch=214
06/22/2022 04:24:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=219
06/22/2022 04:24:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.17 on epoch=224
06/22/2022 04:24:26 - INFO - __main__ - Global step 450 Train loss 0.18 ACC 0.59375 on epoch=224
06/22/2022 04:24:26 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=224, global_step=450
06/22/2022 04:24:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.17 on epoch=229
06/22/2022 04:24:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=234
06/22/2022 04:24:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
06/22/2022 04:24:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.09 on epoch=244
06/22/2022 04:24:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=249
06/22/2022 04:24:33 - INFO - __main__ - Global step 500 Train loss 0.13 ACC 0.5 on epoch=249
06/22/2022 04:24:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.11 on epoch=254
06/22/2022 04:24:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=259
06/22/2022 04:24:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
06/22/2022 04:24:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=269
06/22/2022 04:24:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.10 on epoch=274
06/22/2022 04:24:40 - INFO - __main__ - Global step 550 Train loss 0.10 ACC 0.53125 on epoch=274
06/22/2022 04:24:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.11 on epoch=279
06/22/2022 04:24:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=284
06/22/2022 04:24:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.08 on epoch=289
06/22/2022 04:24:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
06/22/2022 04:24:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.10 on epoch=299
06/22/2022 04:24:47 - INFO - __main__ - Global step 600 Train loss 0.09 ACC 0.53125 on epoch=299
06/22/2022 04:24:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
06/22/2022 04:24:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=309
06/22/2022 04:24:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/22/2022 04:24:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
06/22/2022 04:24:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
06/22/2022 04:24:54 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.5 on epoch=324
06/22/2022 04:24:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
06/22/2022 04:24:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
06/22/2022 04:24:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
06/22/2022 04:24:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
06/22/2022 04:25:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
06/22/2022 04:25:00 - INFO - __main__ - Global step 700 Train loss 0.02 ACC 0.5625 on epoch=349
06/22/2022 04:25:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
06/22/2022 04:25:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
06/22/2022 04:25:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/22/2022 04:25:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/22/2022 04:25:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
06/22/2022 04:25:07 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.65625 on epoch=374
06/22/2022 04:25:07 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=374, global_step=750
06/22/2022 04:25:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/22/2022 04:25:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/22/2022 04:25:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
06/22/2022 04:25:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/22/2022 04:25:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
06/22/2022 04:25:14 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.59375 on epoch=399
06/22/2022 04:25:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
06/22/2022 04:25:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/22/2022 04:25:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/22/2022 04:25:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
06/22/2022 04:25:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
06/22/2022 04:25:21 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.625 on epoch=424
06/22/2022 04:25:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
06/22/2022 04:25:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/22/2022 04:25:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
06/22/2022 04:25:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
06/22/2022 04:25:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/22/2022 04:25:27 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.625 on epoch=449
06/22/2022 04:25:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
06/22/2022 04:25:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/22/2022 04:25:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/22/2022 04:25:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/22/2022 04:25:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/22/2022 04:25:34 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.625 on epoch=474
06/22/2022 04:25:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
06/22/2022 04:25:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
06/22/2022 04:25:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
06/22/2022 04:25:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/22/2022 04:25:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/22/2022 04:25:41 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.65625 on epoch=499
06/22/2022 04:25:42 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/22/2022 04:25:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
06/22/2022 04:25:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/22/2022 04:25:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/22/2022 04:25:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/22/2022 04:25:48 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.625 on epoch=524
06/22/2022 04:25:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/22/2022 04:25:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/22/2022 04:25:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/22/2022 04:25:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/22/2022 04:25:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/22/2022 04:25:55 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.59375 on epoch=549
06/22/2022 04:25:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 04:25:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/22/2022 04:25:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/22/2022 04:26:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/22/2022 04:26:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/22/2022 04:26:01 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.5625 on epoch=574
06/22/2022 04:26:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/22/2022 04:26:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/22/2022 04:26:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/22/2022 04:26:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 04:26:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/22/2022 04:26:08 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.59375 on epoch=599
06/22/2022 04:26:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/22/2022 04:26:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/22/2022 04:26:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 04:26:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/22/2022 04:26:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/22/2022 04:26:15 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.59375 on epoch=624
06/22/2022 04:26:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 04:26:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 04:26:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 04:26:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
06/22/2022 04:26:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 04:26:22 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.625 on epoch=649
06/22/2022 04:26:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 04:26:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 04:26:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 04:26:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 04:26:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 04:26:29 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.625 on epoch=674
06/22/2022 04:26:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 04:26:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 04:26:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 04:26:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 04:26:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 04:26:35 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.59375 on epoch=699
06/22/2022 04:26:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 04:26:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 04:26:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 04:26:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 04:26:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 04:26:42 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.59375 on epoch=724
06/22/2022 04:26:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 04:26:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 04:26:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 04:26:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 04:26:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 04:26:49 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.59375 on epoch=749
06/22/2022 04:26:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 04:26:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 04:26:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 04:26:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/22/2022 04:26:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 04:26:56 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.59375 on epoch=774
06/22/2022 04:26:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 04:26:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 04:26:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 04:27:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 04:27:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 04:27:02 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.625 on epoch=799
06/22/2022 04:27:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 04:27:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 04:27:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 04:27:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 04:27:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 04:27:09 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.625 on epoch=824
06/22/2022 04:27:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 04:27:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 04:27:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 04:27:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 04:27:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 04:27:16 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.625 on epoch=849
06/22/2022 04:27:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 04:27:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 04:27:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 04:27:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 04:27:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 04:27:23 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.625 on epoch=874
06/22/2022 04:27:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 04:27:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 04:27:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 04:27:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 04:27:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 04:27:30 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.625 on epoch=899
06/22/2022 04:27:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 04:27:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 04:27:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 04:27:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 04:27:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 04:27:36 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.59375 on epoch=924
06/22/2022 04:27:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 04:27:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 04:27:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 04:27:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 04:27:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 04:27:43 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.59375 on epoch=949
06/22/2022 04:27:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
06/22/2022 04:27:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 04:27:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 04:27:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 04:27:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 04:27:50 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.625 on epoch=974
06/22/2022 04:27:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 04:27:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 04:27:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 04:27:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 04:27:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 04:27:57 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.625 on epoch=999
06/22/2022 04:27:57 - INFO - __main__ - save last model!
06/22/2022 04:27:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 04:27:57 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 04:27:57 - INFO - __main__ - Printing 3 examples
06/22/2022 04:27:57 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 04:27:57 - INFO - __main__ - ['equivalent']
06/22/2022 04:27:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 04:27:57 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:27:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 04:27:57 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:27:57 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:27:57 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:27:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:27:57 - INFO - __main__ - Printing 3 examples
06/22/2022 04:27:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/22/2022 04:27:57 - INFO - __main__ - ['equivalent']
06/22/2022 04:27:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/22/2022 04:27:57 - INFO - __main__ - ['equivalent']
06/22/2022 04:27:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/22/2022 04:27:57 - INFO - __main__ - ['equivalent']
06/22/2022 04:27:57 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:27:57 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:27:57 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:27:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:27:57 - INFO - __main__ - Printing 3 examples
06/22/2022 04:27:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/22/2022 04:27:57 - INFO - __main__ - ['equivalent']
06/22/2022 04:27:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/22/2022 04:27:57 - INFO - __main__ - ['equivalent']
06/22/2022 04:27:57 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/22/2022 04:27:57 - INFO - __main__ - ['equivalent']
06/22/2022 04:27:57 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:27:57 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:27:57 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 04:27:58 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:28:03 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:28:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:28:03 - INFO - __main__ - Starting training!
06/22/2022 04:28:06 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_13_0.5_8_predictions.txt
06/22/2022 04:28:06 - INFO - __main__ - ACC on test data: 0.5368
06/22/2022 04:28:06 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.5, bsz=8, dev_performance=0.65625, test_performance=0.5367647058823529
06/22/2022 04:28:06 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.4, bsz=8 ...
06/22/2022 04:28:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:28:07 - INFO - __main__ - Printing 3 examples
06/22/2022 04:28:07 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/22/2022 04:28:07 - INFO - __main__ - ['equivalent']
06/22/2022 04:28:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/22/2022 04:28:07 - INFO - __main__ - ['equivalent']
06/22/2022 04:28:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/22/2022 04:28:07 - INFO - __main__ - ['equivalent']
06/22/2022 04:28:07 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:28:07 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:28:07 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:28:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:28:07 - INFO - __main__ - Printing 3 examples
06/22/2022 04:28:07 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/22/2022 04:28:07 - INFO - __main__ - ['equivalent']
06/22/2022 04:28:07 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/22/2022 04:28:07 - INFO - __main__ - ['equivalent']
06/22/2022 04:28:07 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/22/2022 04:28:07 - INFO - __main__ - ['equivalent']
06/22/2022 04:28:07 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:28:07 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:28:07 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:28:12 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:28:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:28:12 - INFO - __main__ - Starting training!
06/22/2022 04:28:14 - INFO - __main__ - Step 10 Global step 10 Train loss 3.42 on epoch=4
06/22/2022 04:28:15 - INFO - __main__ - Step 20 Global step 20 Train loss 2.39 on epoch=9
06/22/2022 04:28:16 - INFO - __main__ - Step 30 Global step 30 Train loss 1.76 on epoch=14
06/22/2022 04:28:17 - INFO - __main__ - Step 40 Global step 40 Train loss 1.23 on epoch=19
06/22/2022 04:28:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.88 on epoch=24
06/22/2022 04:28:19 - INFO - __main__ - Global step 50 Train loss 1.93 ACC 0.59375 on epoch=24
06/22/2022 04:28:19 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.59375 on epoch=24, global_step=50
06/22/2022 04:28:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.65 on epoch=29
06/22/2022 04:28:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=34
06/22/2022 04:28:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=39
06/22/2022 04:28:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=44
06/22/2022 04:28:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=49
06/22/2022 04:28:26 - INFO - __main__ - Global step 100 Train loss 0.52 ACC 0.46875 on epoch=49
06/22/2022 04:28:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=54
06/22/2022 04:28:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.36 on epoch=59
06/22/2022 04:28:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.41 on epoch=64
06/22/2022 04:28:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.32 on epoch=69
06/22/2022 04:28:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=74
06/22/2022 04:28:33 - INFO - __main__ - Global step 150 Train loss 0.37 ACC 0.5 on epoch=74
06/22/2022 04:28:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=79
06/22/2022 04:28:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
06/22/2022 04:28:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.34 on epoch=89
06/22/2022 04:28:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
06/22/2022 04:28:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.29 on epoch=99
06/22/2022 04:28:40 - INFO - __main__ - Global step 200 Train loss 0.32 ACC 0.5 on epoch=99
06/22/2022 04:28:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.29 on epoch=104
06/22/2022 04:28:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
06/22/2022 04:28:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.29 on epoch=114
06/22/2022 04:28:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.31 on epoch=119
06/22/2022 04:28:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.33 on epoch=124
06/22/2022 04:28:47 - INFO - __main__ - Global step 250 Train loss 0.31 ACC 0.53125 on epoch=124
06/22/2022 04:28:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=129
06/22/2022 04:28:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
06/22/2022 04:28:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=139
06/22/2022 04:28:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=144
06/22/2022 04:28:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=149
06/22/2022 04:28:54 - INFO - __main__ - Global step 300 Train loss 0.29 ACC 0.5 on epoch=149
06/22/2022 04:28:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
06/22/2022 04:28:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
06/22/2022 04:28:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=164
06/22/2022 04:28:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
06/22/2022 04:29:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
06/22/2022 04:29:00 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.5 on epoch=174
06/22/2022 04:29:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
06/22/2022 04:29:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=184
06/22/2022 04:29:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
06/22/2022 04:29:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
06/22/2022 04:29:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
06/22/2022 04:29:07 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.46875 on epoch=199
06/22/2022 04:29:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=204
06/22/2022 04:29:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=209
06/22/2022 04:29:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
06/22/2022 04:29:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.29 on epoch=219
06/22/2022 04:29:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=224
06/22/2022 04:29:14 - INFO - __main__ - Global step 450 Train loss 0.24 ACC 0.625 on epoch=224
06/22/2022 04:29:14 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=224, global_step=450
06/22/2022 04:29:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.19 on epoch=229
06/22/2022 04:29:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=234
06/22/2022 04:29:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
06/22/2022 04:29:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=244
06/22/2022 04:29:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=249
06/22/2022 04:29:21 - INFO - __main__ - Global step 500 Train loss 0.19 ACC 0.53125 on epoch=249
06/22/2022 04:29:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.17 on epoch=254
06/22/2022 04:29:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=259
06/22/2022 04:29:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=264
06/22/2022 04:29:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=269
06/22/2022 04:29:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=274
06/22/2022 04:29:27 - INFO - __main__ - Global step 550 Train loss 0.16 ACC 0.5 on epoch=274
06/22/2022 04:29:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=279
06/22/2022 04:29:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=284
06/22/2022 04:29:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=289
06/22/2022 04:29:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=294
06/22/2022 04:29:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=299
06/22/2022 04:29:34 - INFO - __main__ - Global step 600 Train loss 0.16 ACC 0.5625 on epoch=299
06/22/2022 04:29:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.11 on epoch=304
06/22/2022 04:29:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=309
06/22/2022 04:29:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=314
06/22/2022 04:29:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=319
06/22/2022 04:29:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=324
06/22/2022 04:29:41 - INFO - __main__ - Global step 650 Train loss 0.09 ACC 0.53125 on epoch=324
06/22/2022 04:29:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=329
06/22/2022 04:29:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
06/22/2022 04:29:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
06/22/2022 04:29:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
06/22/2022 04:29:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
06/22/2022 04:29:48 - INFO - __main__ - Global step 700 Train loss 0.06 ACC 0.53125 on epoch=349
06/22/2022 04:29:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
06/22/2022 04:29:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
06/22/2022 04:29:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
06/22/2022 04:29:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
06/22/2022 04:29:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
06/22/2022 04:29:54 - INFO - __main__ - Global step 750 Train loss 0.05 ACC 0.46875 on epoch=374
06/22/2022 04:29:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
06/22/2022 04:29:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
06/22/2022 04:29:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
06/22/2022 04:29:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
06/22/2022 04:30:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=399
06/22/2022 04:30:01 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.5 on epoch=399
06/22/2022 04:30:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
06/22/2022 04:30:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
06/22/2022 04:30:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
06/22/2022 04:30:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
06/22/2022 04:30:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/22/2022 04:30:08 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.53125 on epoch=424
06/22/2022 04:30:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
06/22/2022 04:30:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
06/22/2022 04:30:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
06/22/2022 04:30:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
06/22/2022 04:30:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/22/2022 04:30:15 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.53125 on epoch=449
06/22/2022 04:30:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/22/2022 04:30:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/22/2022 04:30:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/22/2022 04:30:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
06/22/2022 04:30:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
06/22/2022 04:30:21 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.5625 on epoch=474
06/22/2022 04:30:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/22/2022 04:30:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/22/2022 04:30:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
06/22/2022 04:30:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
06/22/2022 04:30:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/22/2022 04:30:28 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.53125 on epoch=499
06/22/2022 04:30:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/22/2022 04:30:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/22/2022 04:30:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/22/2022 04:30:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/22/2022 04:30:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/22/2022 04:30:35 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.53125 on epoch=524
06/22/2022 04:30:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/22/2022 04:30:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/22/2022 04:30:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/22/2022 04:30:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
06/22/2022 04:30:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/22/2022 04:30:42 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.5625 on epoch=549
06/22/2022 04:30:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/22/2022 04:30:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/22/2022 04:30:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 04:30:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/22/2022 04:30:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/22/2022 04:30:49 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.59375 on epoch=574
06/22/2022 04:30:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/22/2022 04:30:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/22/2022 04:30:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
06/22/2022 04:30:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 04:30:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/22/2022 04:30:55 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.5625 on epoch=599
06/22/2022 04:30:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/22/2022 04:30:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/22/2022 04:30:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/22/2022 04:31:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/22/2022 04:31:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 04:31:02 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.59375 on epoch=624
06/22/2022 04:31:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 04:31:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 04:31:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 04:31:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 04:31:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 04:31:09 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.5625 on epoch=649
06/22/2022 04:31:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/22/2022 04:31:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 04:31:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 04:31:14 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/22/2022 04:31:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 04:31:16 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.625 on epoch=674
06/22/2022 04:31:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
06/22/2022 04:31:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/22/2022 04:31:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 04:31:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 04:31:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 04:31:23 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.5625 on epoch=699
06/22/2022 04:31:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 04:31:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/22/2022 04:31:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 04:31:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 04:31:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 04:31:30 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.625 on epoch=724
06/22/2022 04:31:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 04:31:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 04:31:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 04:31:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 04:31:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 04:31:36 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.5625 on epoch=749
06/22/2022 04:31:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 04:31:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 04:31:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 04:31:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 04:31:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 04:31:43 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.59375 on epoch=774
06/22/2022 04:31:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 04:31:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 04:31:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 04:31:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 04:31:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 04:31:50 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.59375 on epoch=799
06/22/2022 04:31:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 04:31:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 04:31:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 04:31:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 04:31:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 04:31:56 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.5625 on epoch=824
06/22/2022 04:31:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 04:31:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
06/22/2022 04:32:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 04:32:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 04:32:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 04:32:03 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.59375 on epoch=849
06/22/2022 04:32:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 04:32:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
06/22/2022 04:32:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 04:32:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 04:32:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
06/22/2022 04:32:10 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.53125 on epoch=874
06/22/2022 04:32:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/22/2022 04:32:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 04:32:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 04:32:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 04:32:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 04:32:17 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.53125 on epoch=899
06/22/2022 04:32:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 04:32:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 04:32:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 04:32:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 04:32:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 04:32:24 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.53125 on epoch=924
06/22/2022 04:32:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 04:32:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/22/2022 04:32:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
06/22/2022 04:32:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 04:32:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 04:32:31 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.625 on epoch=949
06/22/2022 04:32:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 04:32:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 04:32:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 04:32:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 04:32:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 04:32:37 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.59375 on epoch=974
06/22/2022 04:32:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 04:32:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 04:32:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 04:32:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 04:32:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 04:32:44 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.5 on epoch=999
06/22/2022 04:32:44 - INFO - __main__ - save last model!
06/22/2022 04:32:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 04:32:44 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 04:32:44 - INFO - __main__ - Printing 3 examples
06/22/2022 04:32:44 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 04:32:44 - INFO - __main__ - ['equivalent']
06/22/2022 04:32:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 04:32:44 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:32:44 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 04:32:44 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:32:44 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:32:44 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:32:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:32:45 - INFO - __main__ - Printing 3 examples
06/22/2022 04:32:45 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/22/2022 04:32:45 - INFO - __main__ - ['equivalent']
06/22/2022 04:32:45 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/22/2022 04:32:45 - INFO - __main__ - ['equivalent']
06/22/2022 04:32:45 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/22/2022 04:32:45 - INFO - __main__ - ['equivalent']
06/22/2022 04:32:45 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:32:45 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:32:45 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:32:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:32:45 - INFO - __main__ - Printing 3 examples
06/22/2022 04:32:45 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/22/2022 04:32:45 - INFO - __main__ - ['equivalent']
06/22/2022 04:32:45 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/22/2022 04:32:45 - INFO - __main__ - ['equivalent']
06/22/2022 04:32:45 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/22/2022 04:32:45 - INFO - __main__ - ['equivalent']
06/22/2022 04:32:45 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:32:45 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 04:32:45 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:32:45 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:32:50 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:32:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:32:50 - INFO - __main__ - Starting training!
06/22/2022 04:32:53 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_13_0.4_8_predictions.txt
06/22/2022 04:32:53 - INFO - __main__ - ACC on test data: 0.6593
06/22/2022 04:32:53 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.4, bsz=8, dev_performance=0.625, test_performance=0.6593137254901961
06/22/2022 04:32:53 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.3, bsz=8 ...
06/22/2022 04:32:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:32:54 - INFO - __main__ - Printing 3 examples
06/22/2022 04:32:54 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/22/2022 04:32:54 - INFO - __main__ - ['equivalent']
06/22/2022 04:32:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/22/2022 04:32:54 - INFO - __main__ - ['equivalent']
06/22/2022 04:32:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/22/2022 04:32:54 - INFO - __main__ - ['equivalent']
06/22/2022 04:32:54 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:32:54 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:32:54 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:32:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:32:54 - INFO - __main__ - Printing 3 examples
06/22/2022 04:32:54 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/22/2022 04:32:54 - INFO - __main__ - ['equivalent']
06/22/2022 04:32:54 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/22/2022 04:32:54 - INFO - __main__ - ['equivalent']
06/22/2022 04:32:54 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/22/2022 04:32:54 - INFO - __main__ - ['equivalent']
06/22/2022 04:32:54 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:32:54 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:32:54 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:32:59 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:32:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:32:59 - INFO - __main__ - Starting training!
06/22/2022 04:33:01 - INFO - __main__ - Step 10 Global step 10 Train loss 3.39 on epoch=4
06/22/2022 04:33:02 - INFO - __main__ - Step 20 Global step 20 Train loss 2.66 on epoch=9
06/22/2022 04:33:03 - INFO - __main__ - Step 30 Global step 30 Train loss 2.19 on epoch=14
06/22/2022 04:33:04 - INFO - __main__ - Step 40 Global step 40 Train loss 1.67 on epoch=19
06/22/2022 04:33:06 - INFO - __main__ - Step 50 Global step 50 Train loss 1.36 on epoch=24
06/22/2022 04:33:06 - INFO - __main__ - Global step 50 Train loss 2.25 ACC 0.28125 on epoch=24
06/22/2022 04:33:06 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.28125 on epoch=24, global_step=50
06/22/2022 04:33:08 - INFO - __main__ - Step 60 Global step 60 Train loss 0.86 on epoch=29
06/22/2022 04:33:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.85 on epoch=34
06/22/2022 04:33:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.71 on epoch=39
06/22/2022 04:33:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
06/22/2022 04:33:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=49
06/22/2022 04:33:13 - INFO - __main__ - Global step 100 Train loss 0.68 ACC 0.5625 on epoch=49
06/22/2022 04:33:13 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.5625 on epoch=49, global_step=100
06/22/2022 04:33:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=54
06/22/2022 04:33:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=59
06/22/2022 04:33:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=64
06/22/2022 04:33:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=69
06/22/2022 04:33:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=74
06/22/2022 04:33:20 - INFO - __main__ - Global step 150 Train loss 0.47 ACC 0.59375 on epoch=74
06/22/2022 04:33:20 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=74, global_step=150
06/22/2022 04:33:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=79
06/22/2022 04:33:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=84
06/22/2022 04:33:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=89
06/22/2022 04:33:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
06/22/2022 04:33:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=99
06/22/2022 04:33:27 - INFO - __main__ - Global step 200 Train loss 0.37 ACC 0.40625 on epoch=99
06/22/2022 04:33:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.35 on epoch=104
06/22/2022 04:33:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
06/22/2022 04:33:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.33 on epoch=114
06/22/2022 04:33:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=119
06/22/2022 04:33:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.33 on epoch=124
06/22/2022 04:33:34 - INFO - __main__ - Global step 250 Train loss 0.34 ACC 0.5 on epoch=124
06/22/2022 04:33:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=129
06/22/2022 04:33:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=134
06/22/2022 04:33:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=139
06/22/2022 04:33:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=144
06/22/2022 04:33:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
06/22/2022 04:33:40 - INFO - __main__ - Global step 300 Train loss 0.34 ACC 0.53125 on epoch=149
06/22/2022 04:33:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=154
06/22/2022 04:33:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.30 on epoch=159
06/22/2022 04:33:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
06/22/2022 04:33:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
06/22/2022 04:33:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=174
06/22/2022 04:33:47 - INFO - __main__ - Global step 350 Train loss 0.28 ACC 0.53125 on epoch=174
06/22/2022 04:33:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=179
06/22/2022 04:33:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.28 on epoch=184
06/22/2022 04:33:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=189
06/22/2022 04:33:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=194
06/22/2022 04:33:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=199
06/22/2022 04:33:54 - INFO - __main__ - Global step 400 Train loss 0.30 ACC 0.5 on epoch=199
06/22/2022 04:33:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=204
06/22/2022 04:33:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=209
06/22/2022 04:33:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=214
06/22/2022 04:33:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
06/22/2022 04:34:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
06/22/2022 04:34:01 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.5625 on epoch=224
06/22/2022 04:34:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.30 on epoch=229
06/22/2022 04:34:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=234
06/22/2022 04:34:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=239
06/22/2022 04:34:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=244
06/22/2022 04:34:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
06/22/2022 04:34:08 - INFO - __main__ - Global step 500 Train loss 0.28 ACC 0.5625 on epoch=249
06/22/2022 04:34:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=254
06/22/2022 04:34:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
06/22/2022 04:34:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
06/22/2022 04:34:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=269
06/22/2022 04:34:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=274
06/22/2022 04:34:14 - INFO - __main__ - Global step 550 Train loss 0.24 ACC 0.59375 on epoch=274
06/22/2022 04:34:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
06/22/2022 04:34:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=284
06/22/2022 04:34:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=289
06/22/2022 04:34:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
06/22/2022 04:34:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
06/22/2022 04:34:21 - INFO - __main__ - Global step 600 Train loss 0.25 ACC 0.625 on epoch=299
06/22/2022 04:34:21 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=299, global_step=600
06/22/2022 04:34:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=304
06/22/2022 04:34:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=309
06/22/2022 04:34:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
06/22/2022 04:34:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=319
06/22/2022 04:34:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=324
06/22/2022 04:34:28 - INFO - __main__ - Global step 650 Train loss 0.20 ACC 0.625 on epoch=324
06/22/2022 04:34:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=329
06/22/2022 04:34:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=334
06/22/2022 04:34:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=339
06/22/2022 04:34:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=344
06/22/2022 04:34:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=349
06/22/2022 04:34:35 - INFO - __main__ - Global step 700 Train loss 0.18 ACC 0.59375 on epoch=349
06/22/2022 04:34:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=354
06/22/2022 04:34:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=359
06/22/2022 04:34:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=364
06/22/2022 04:34:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=369
06/22/2022 04:34:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=374
06/22/2022 04:34:42 - INFO - __main__ - Global step 750 Train loss 0.14 ACC 0.625 on epoch=374
06/22/2022 04:34:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=379
06/22/2022 04:34:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=384
06/22/2022 04:34:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=389
06/22/2022 04:34:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=394
06/22/2022 04:34:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=399
06/22/2022 04:34:48 - INFO - __main__ - Global step 800 Train loss 0.13 ACC 0.65625 on epoch=399
06/22/2022 04:34:48 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=399, global_step=800
06/22/2022 04:34:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=404
06/22/2022 04:34:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=409
06/22/2022 04:34:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=414
06/22/2022 04:34:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=419
06/22/2022 04:34:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=424
06/22/2022 04:34:55 - INFO - __main__ - Global step 850 Train loss 0.08 ACC 0.625 on epoch=424
06/22/2022 04:34:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=429
06/22/2022 04:34:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=434
06/22/2022 04:34:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=439
06/22/2022 04:35:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
06/22/2022 04:35:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=449
06/22/2022 04:35:02 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.59375 on epoch=449
06/22/2022 04:35:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
06/22/2022 04:35:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
06/22/2022 04:35:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=464
06/22/2022 04:35:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/22/2022 04:35:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
06/22/2022 04:35:09 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.59375 on epoch=474
06/22/2022 04:35:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=479
06/22/2022 04:35:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/22/2022 04:35:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
06/22/2022 04:35:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
06/22/2022 04:35:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
06/22/2022 04:35:16 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.53125 on epoch=499
06/22/2022 04:35:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
06/22/2022 04:35:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
06/22/2022 04:35:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/22/2022 04:35:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
06/22/2022 04:35:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
06/22/2022 04:35:22 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.59375 on epoch=524
06/22/2022 04:35:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/22/2022 04:35:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/22/2022 04:35:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/22/2022 04:35:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
06/22/2022 04:35:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/22/2022 04:35:29 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.625 on epoch=549
06/22/2022 04:35:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/22/2022 04:35:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
06/22/2022 04:35:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
06/22/2022 04:35:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/22/2022 04:35:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/22/2022 04:35:36 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.625 on epoch=574
06/22/2022 04:35:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
06/22/2022 04:35:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
06/22/2022 04:35:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/22/2022 04:35:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
06/22/2022 04:35:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/22/2022 04:35:42 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.5625 on epoch=599
06/22/2022 04:35:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/22/2022 04:35:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/22/2022 04:35:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/22/2022 04:35:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/22/2022 04:35:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 04:35:49 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.59375 on epoch=624
06/22/2022 04:35:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/22/2022 04:35:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 04:35:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/22/2022 04:35:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/22/2022 04:35:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 04:35:56 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.59375 on epoch=649
06/22/2022 04:35:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
06/22/2022 04:35:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/22/2022 04:36:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 04:36:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/22/2022 04:36:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/22/2022 04:36:03 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.5625 on epoch=674
06/22/2022 04:36:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
06/22/2022 04:36:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 04:36:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/22/2022 04:36:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 04:36:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 04:36:10 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.53125 on epoch=699
06/22/2022 04:36:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 04:36:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 04:36:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/22/2022 04:36:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 04:36:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
06/22/2022 04:36:16 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.59375 on epoch=724
06/22/2022 04:36:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/22/2022 04:36:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 04:36:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/22/2022 04:36:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/22/2022 04:36:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 04:36:23 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.53125 on epoch=749
06/22/2022 04:36:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/22/2022 04:36:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/22/2022 04:36:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/22/2022 04:36:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 04:36:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 04:36:30 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.53125 on epoch=774
06/22/2022 04:36:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 04:36:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/22/2022 04:36:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 04:36:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 04:36:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 04:36:37 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.5625 on epoch=799
06/22/2022 04:36:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/22/2022 04:36:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 04:36:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 04:36:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 04:36:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 04:36:43 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.53125 on epoch=824
06/22/2022 04:36:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 04:36:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/22/2022 04:36:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=839
06/22/2022 04:36:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 04:36:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 04:36:50 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.53125 on epoch=849
06/22/2022 04:36:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 04:36:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 04:36:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/22/2022 04:36:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 04:36:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 04:36:57 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.53125 on epoch=874
06/22/2022 04:36:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 04:36:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 04:37:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 04:37:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 04:37:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 04:37:04 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.65625 on epoch=899
06/22/2022 04:37:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 04:37:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 04:37:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 04:37:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 04:37:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 04:37:10 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.5625 on epoch=924
06/22/2022 04:37:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 04:37:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 04:37:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 04:37:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 04:37:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 04:37:17 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.5625 on epoch=949
06/22/2022 04:37:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 04:37:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 04:37:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 04:37:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 04:37:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 04:37:24 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.625 on epoch=974
06/22/2022 04:37:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 04:37:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 04:37:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/22/2022 04:37:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 04:37:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 04:37:31 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.65625 on epoch=999
06/22/2022 04:37:31 - INFO - __main__ - save last model!
06/22/2022 04:37:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 04:37:31 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 04:37:31 - INFO - __main__ - Printing 3 examples
06/22/2022 04:37:31 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 04:37:31 - INFO - __main__ - ['equivalent']
06/22/2022 04:37:31 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 04:37:31 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:37:31 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 04:37:31 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:37:31 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:37:31 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:37:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:37:31 - INFO - __main__ - Printing 3 examples
06/22/2022 04:37:31 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/22/2022 04:37:31 - INFO - __main__ - ['equivalent']
06/22/2022 04:37:31 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/22/2022 04:37:31 - INFO - __main__ - ['equivalent']
06/22/2022 04:37:31 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/22/2022 04:37:31 - INFO - __main__ - ['equivalent']
06/22/2022 04:37:31 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:37:31 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:37:31 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:37:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:37:31 - INFO - __main__ - Printing 3 examples
06/22/2022 04:37:31 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/22/2022 04:37:31 - INFO - __main__ - ['equivalent']
06/22/2022 04:37:31 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/22/2022 04:37:31 - INFO - __main__ - ['equivalent']
06/22/2022 04:37:31 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/22/2022 04:37:31 - INFO - __main__ - ['equivalent']
06/22/2022 04:37:31 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:37:31 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:37:31 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 04:37:31 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:37:37 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:37:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:37:37 - INFO - __main__ - Starting training!
06/22/2022 04:37:39 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_13_0.3_8_predictions.txt
06/22/2022 04:37:39 - INFO - __main__ - ACC on test data: 0.5588
06/22/2022 04:37:40 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.3, bsz=8, dev_performance=0.65625, test_performance=0.5588235294117647
06/22/2022 04:37:40 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.2, bsz=8 ...
06/22/2022 04:37:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:37:40 - INFO - __main__ - Printing 3 examples
06/22/2022 04:37:40 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/22/2022 04:37:40 - INFO - __main__ - ['equivalent']
06/22/2022 04:37:40 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/22/2022 04:37:40 - INFO - __main__ - ['equivalent']
06/22/2022 04:37:40 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/22/2022 04:37:40 - INFO - __main__ - ['equivalent']
06/22/2022 04:37:40 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:37:41 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:37:41 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:37:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:37:41 - INFO - __main__ - Printing 3 examples
06/22/2022 04:37:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/22/2022 04:37:41 - INFO - __main__ - ['equivalent']
06/22/2022 04:37:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/22/2022 04:37:41 - INFO - __main__ - ['equivalent']
06/22/2022 04:37:41 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/22/2022 04:37:41 - INFO - __main__ - ['equivalent']
06/22/2022 04:37:41 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:37:41 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:37:41 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:37:46 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:37:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:37:46 - INFO - __main__ - Starting training!
06/22/2022 04:37:48 - INFO - __main__ - Step 10 Global step 10 Train loss 3.74 on epoch=4
06/22/2022 04:37:49 - INFO - __main__ - Step 20 Global step 20 Train loss 3.28 on epoch=9
06/22/2022 04:37:50 - INFO - __main__ - Step 30 Global step 30 Train loss 2.52 on epoch=14
06/22/2022 04:37:51 - INFO - __main__ - Step 40 Global step 40 Train loss 2.21 on epoch=19
06/22/2022 04:37:52 - INFO - __main__ - Step 50 Global step 50 Train loss 1.72 on epoch=24
06/22/2022 04:37:53 - INFO - __main__ - Global step 50 Train loss 2.69 ACC 0.0 on epoch=24
06/22/2022 04:37:53 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/22/2022 04:37:54 - INFO - __main__ - Step 60 Global step 60 Train loss 1.51 on epoch=29
06/22/2022 04:37:56 - INFO - __main__ - Step 70 Global step 70 Train loss 1.35 on epoch=34
06/22/2022 04:37:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.98 on epoch=39
06/22/2022 04:37:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.88 on epoch=44
06/22/2022 04:37:59 - INFO - __main__ - Step 100 Global step 100 Train loss 0.78 on epoch=49
06/22/2022 04:38:00 - INFO - __main__ - Global step 100 Train loss 1.10 ACC 0.375 on epoch=49
06/22/2022 04:38:00 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.375 on epoch=49, global_step=100
06/22/2022 04:38:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.62 on epoch=54
06/22/2022 04:38:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.68 on epoch=59
06/22/2022 04:38:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=64
06/22/2022 04:38:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=69
06/22/2022 04:38:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=74
06/22/2022 04:38:07 - INFO - __main__ - Global step 150 Train loss 0.60 ACC 0.53125 on epoch=74
06/22/2022 04:38:07 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.53125 on epoch=74, global_step=150
06/22/2022 04:38:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=79
06/22/2022 04:38:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=84
06/22/2022 04:38:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=89
06/22/2022 04:38:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=94
06/22/2022 04:38:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=99
06/22/2022 04:38:14 - INFO - __main__ - Global step 200 Train loss 0.45 ACC 0.53125 on epoch=99
06/22/2022 04:38:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=104
06/22/2022 04:38:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=109
06/22/2022 04:38:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
06/22/2022 04:38:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=119
06/22/2022 04:38:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=124
06/22/2022 04:38:21 - INFO - __main__ - Global step 250 Train loss 0.41 ACC 0.46875 on epoch=124
06/22/2022 04:38:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=129
06/22/2022 04:38:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=134
06/22/2022 04:38:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=139
06/22/2022 04:38:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=144
06/22/2022 04:38:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.33 on epoch=149
06/22/2022 04:38:27 - INFO - __main__ - Global step 300 Train loss 0.39 ACC 0.40625 on epoch=149
06/22/2022 04:38:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=154
06/22/2022 04:38:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=159
06/22/2022 04:38:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=164
06/22/2022 04:38:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=169
06/22/2022 04:38:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.32 on epoch=174
06/22/2022 04:38:34 - INFO - __main__ - Global step 350 Train loss 0.35 ACC 0.5 on epoch=174
06/22/2022 04:38:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=179
06/22/2022 04:38:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=184
06/22/2022 04:38:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.32 on epoch=189
06/22/2022 04:38:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=194
06/22/2022 04:38:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.31 on epoch=199
06/22/2022 04:38:41 - INFO - __main__ - Global step 400 Train loss 0.33 ACC 0.5 on epoch=199
06/22/2022 04:38:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=204
06/22/2022 04:38:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.30 on epoch=209
06/22/2022 04:38:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=214
06/22/2022 04:38:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=219
06/22/2022 04:38:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
06/22/2022 04:38:48 - INFO - __main__ - Global step 450 Train loss 0.30 ACC 0.5 on epoch=224
06/22/2022 04:38:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=229
06/22/2022 04:38:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=234
06/22/2022 04:38:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=239
06/22/2022 04:38:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=244
06/22/2022 04:38:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=249
06/22/2022 04:38:55 - INFO - __main__ - Global step 500 Train loss 0.30 ACC 0.46875 on epoch=249
06/22/2022 04:38:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
06/22/2022 04:38:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=259
06/22/2022 04:38:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.29 on epoch=264
06/22/2022 04:38:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=269
06/22/2022 04:39:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=274
06/22/2022 04:39:01 - INFO - __main__ - Global step 550 Train loss 0.27 ACC 0.5 on epoch=274
06/22/2022 04:39:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
06/22/2022 04:39:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=284
06/22/2022 04:39:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=289
06/22/2022 04:39:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=294
06/22/2022 04:39:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.28 on epoch=299
06/22/2022 04:39:08 - INFO - __main__ - Global step 600 Train loss 0.27 ACC 0.5 on epoch=299
06/22/2022 04:39:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=304
06/22/2022 04:39:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=309
06/22/2022 04:39:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=314
06/22/2022 04:39:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=319
06/22/2022 04:39:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=324
06/22/2022 04:39:15 - INFO - __main__ - Global step 650 Train loss 0.25 ACC 0.53125 on epoch=324
06/22/2022 04:39:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=329
06/22/2022 04:39:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=334
06/22/2022 04:39:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
06/22/2022 04:39:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=344
06/22/2022 04:39:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=349
06/22/2022 04:39:22 - INFO - __main__ - Global step 700 Train loss 0.26 ACC 0.46875 on epoch=349
06/22/2022 04:39:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=354
06/22/2022 04:39:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=359
06/22/2022 04:39:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=364
06/22/2022 04:39:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=369
06/22/2022 04:39:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=374
06/22/2022 04:39:29 - INFO - __main__ - Global step 750 Train loss 0.26 ACC 0.53125 on epoch=374
06/22/2022 04:39:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=379
06/22/2022 04:39:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.27 on epoch=384
06/22/2022 04:39:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
06/22/2022 04:39:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=394
06/22/2022 04:39:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=399
06/22/2022 04:39:35 - INFO - __main__ - Global step 800 Train loss 0.24 ACC 0.5625 on epoch=399
06/22/2022 04:39:35 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=399, global_step=800
06/22/2022 04:39:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=404
06/22/2022 04:39:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=409
06/22/2022 04:39:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=414
06/22/2022 04:39:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=419
06/22/2022 04:39:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=424
06/22/2022 04:39:42 - INFO - __main__ - Global step 850 Train loss 0.23 ACC 0.5 on epoch=424
06/22/2022 04:39:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=429
06/22/2022 04:39:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.27 on epoch=434
06/22/2022 04:39:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=439
06/22/2022 04:39:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=444
06/22/2022 04:39:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=449
06/22/2022 04:39:49 - INFO - __main__ - Global step 900 Train loss 0.23 ACC 0.53125 on epoch=449
06/22/2022 04:39:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=454
06/22/2022 04:39:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=459
06/22/2022 04:39:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=464
06/22/2022 04:39:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.19 on epoch=469
06/22/2022 04:39:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=474
06/22/2022 04:39:56 - INFO - __main__ - Global step 950 Train loss 0.21 ACC 0.46875 on epoch=474
06/22/2022 04:39:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=479
06/22/2022 04:39:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=484
06/22/2022 04:39:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=489
06/22/2022 04:40:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=494
06/22/2022 04:40:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=499
06/22/2022 04:40:03 - INFO - __main__ - Global step 1000 Train loss 0.17 ACC 0.46875 on epoch=499
06/22/2022 04:40:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=504
06/22/2022 04:40:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=509
06/22/2022 04:40:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=514
06/22/2022 04:40:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.14 on epoch=519
06/22/2022 04:40:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=524
06/22/2022 04:40:09 - INFO - __main__ - Global step 1050 Train loss 0.16 ACC 0.53125 on epoch=524
06/22/2022 04:40:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=529
06/22/2022 04:40:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=534
06/22/2022 04:40:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=539
06/22/2022 04:40:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=544
06/22/2022 04:40:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=549
06/22/2022 04:40:16 - INFO - __main__ - Global step 1100 Train loss 0.11 ACC 0.53125 on epoch=549
06/22/2022 04:40:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=554
06/22/2022 04:40:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=559
06/22/2022 04:40:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=564
06/22/2022 04:40:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=569
06/22/2022 04:40:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=574
06/22/2022 04:40:23 - INFO - __main__ - Global step 1150 Train loss 0.10 ACC 0.53125 on epoch=574
06/22/2022 04:40:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=579
06/22/2022 04:40:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=584
06/22/2022 04:40:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=589
06/22/2022 04:40:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=594
06/22/2022 04:40:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=599
06/22/2022 04:40:30 - INFO - __main__ - Global step 1200 Train loss 0.09 ACC 0.53125 on epoch=599
06/22/2022 04:40:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
06/22/2022 04:40:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=609
06/22/2022 04:40:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
06/22/2022 04:40:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=619
06/22/2022 04:40:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
06/22/2022 04:40:37 - INFO - __main__ - Global step 1250 Train loss 0.07 ACC 0.53125 on epoch=624
06/22/2022 04:40:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=629
06/22/2022 04:40:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=634
06/22/2022 04:40:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=639
06/22/2022 04:40:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
06/22/2022 04:40:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
06/22/2022 04:40:44 - INFO - __main__ - Global step 1300 Train loss 0.05 ACC 0.53125 on epoch=649
06/22/2022 04:40:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=654
06/22/2022 04:40:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
06/22/2022 04:40:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=664
06/22/2022 04:40:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
06/22/2022 04:40:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=674
06/22/2022 04:40:50 - INFO - __main__ - Global step 1350 Train loss 0.06 ACC 0.53125 on epoch=674
06/22/2022 04:40:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
06/22/2022 04:40:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
06/22/2022 04:40:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=689
06/22/2022 04:40:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
06/22/2022 04:40:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=699
06/22/2022 04:40:57 - INFO - __main__ - Global step 1400 Train loss 0.04 ACC 0.53125 on epoch=699
06/22/2022 04:40:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
06/22/2022 04:41:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
06/22/2022 04:41:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
06/22/2022 04:41:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
06/22/2022 04:41:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
06/22/2022 04:41:04 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.5625 on epoch=724
06/22/2022 04:41:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
06/22/2022 04:41:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=734
06/22/2022 04:41:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
06/22/2022 04:41:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=744
06/22/2022 04:41:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
06/22/2022 04:41:11 - INFO - __main__ - Global step 1500 Train loss 0.04 ACC 0.5 on epoch=749
06/22/2022 04:41:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
06/22/2022 04:41:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
06/22/2022 04:41:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
06/22/2022 04:41:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/22/2022 04:41:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=774
06/22/2022 04:41:18 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.5625 on epoch=774
06/22/2022 04:41:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/22/2022 04:41:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
06/22/2022 04:41:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
06/22/2022 04:41:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
06/22/2022 04:41:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
06/22/2022 04:41:25 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.53125 on epoch=799
06/22/2022 04:41:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
06/22/2022 04:41:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
06/22/2022 04:41:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/22/2022 04:41:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
06/22/2022 04:41:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
06/22/2022 04:41:31 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.53125 on epoch=824
06/22/2022 04:41:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
06/22/2022 04:41:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/22/2022 04:41:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/22/2022 04:41:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/22/2022 04:41:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
06/22/2022 04:41:38 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.5625 on epoch=849
06/22/2022 04:41:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/22/2022 04:41:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/22/2022 04:41:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/22/2022 04:41:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
06/22/2022 04:41:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
06/22/2022 04:41:45 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.53125 on epoch=874
06/22/2022 04:41:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/22/2022 04:41:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=884
06/22/2022 04:41:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
06/22/2022 04:41:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
06/22/2022 04:41:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/22/2022 04:41:52 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.5625 on epoch=899
06/22/2022 04:41:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/22/2022 04:41:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=909
06/22/2022 04:41:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/22/2022 04:41:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/22/2022 04:41:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/22/2022 04:41:58 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.53125 on epoch=924
06/22/2022 04:42:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/22/2022 04:42:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/22/2022 04:42:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/22/2022 04:42:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
06/22/2022 04:42:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 04:42:05 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.5625 on epoch=949
06/22/2022 04:42:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
06/22/2022 04:42:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
06/22/2022 04:42:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/22/2022 04:42:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/22/2022 04:42:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/22/2022 04:42:12 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.53125 on epoch=974
06/22/2022 04:42:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/22/2022 04:42:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 04:42:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/22/2022 04:42:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 04:42:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/22/2022 04:42:19 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.53125 on epoch=999
06/22/2022 04:42:19 - INFO - __main__ - save last model!
06/22/2022 04:42:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 04:42:19 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 04:42:19 - INFO - __main__ - Printing 3 examples
06/22/2022 04:42:19 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 04:42:19 - INFO - __main__ - ['equivalent']
06/22/2022 04:42:19 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 04:42:19 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:42:19 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 04:42:19 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:42:19 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:42:19 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:42:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:42:19 - INFO - __main__ - Printing 3 examples
06/22/2022 04:42:19 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/22/2022 04:42:19 - INFO - __main__ - ['equivalent']
06/22/2022 04:42:19 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/22/2022 04:42:19 - INFO - __main__ - ['equivalent']
06/22/2022 04:42:19 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/22/2022 04:42:19 - INFO - __main__ - ['equivalent']
06/22/2022 04:42:19 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:42:19 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:42:19 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:42:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:42:19 - INFO - __main__ - Printing 3 examples
06/22/2022 04:42:19 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/22/2022 04:42:19 - INFO - __main__ - ['equivalent']
06/22/2022 04:42:19 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/22/2022 04:42:19 - INFO - __main__ - ['equivalent']
06/22/2022 04:42:19 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/22/2022 04:42:19 - INFO - __main__ - ['equivalent']
06/22/2022 04:42:19 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:42:19 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:42:19 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:42:20 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 04:42:25 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:42:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:42:25 - INFO - __main__ - Starting training!
06/22/2022 04:42:27 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_13_0.2_8_predictions.txt
06/22/2022 04:42:27 - INFO - __main__ - ACC on test data: 0.6201
06/22/2022 04:42:28 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.2, bsz=8, dev_performance=0.5625, test_performance=0.6200980392156863
06/22/2022 04:42:28 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.5, bsz=8 ...
06/22/2022 04:42:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:42:29 - INFO - __main__ - Printing 3 examples
06/22/2022 04:42:29 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/22/2022 04:42:29 - INFO - __main__ - ['equivalent']
06/22/2022 04:42:29 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/22/2022 04:42:29 - INFO - __main__ - ['equivalent']
06/22/2022 04:42:29 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/22/2022 04:42:29 - INFO - __main__ - ['equivalent']
06/22/2022 04:42:29 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:42:29 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:42:29 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:42:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:42:29 - INFO - __main__ - Printing 3 examples
06/22/2022 04:42:29 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/22/2022 04:42:29 - INFO - __main__ - ['equivalent']
06/22/2022 04:42:29 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/22/2022 04:42:29 - INFO - __main__ - ['equivalent']
06/22/2022 04:42:29 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/22/2022 04:42:29 - INFO - __main__ - ['equivalent']
06/22/2022 04:42:29 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:42:29 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:42:29 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:42:34 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:42:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:42:34 - INFO - __main__ - Starting training!
06/22/2022 04:42:36 - INFO - __main__ - Step 10 Global step 10 Train loss 3.34 on epoch=4
06/22/2022 04:42:37 - INFO - __main__ - Step 20 Global step 20 Train loss 2.03 on epoch=9
06/22/2022 04:42:38 - INFO - __main__ - Step 30 Global step 30 Train loss 1.38 on epoch=14
06/22/2022 04:42:39 - INFO - __main__ - Step 40 Global step 40 Train loss 0.92 on epoch=19
06/22/2022 04:42:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.68 on epoch=24
06/22/2022 04:42:41 - INFO - __main__ - Global step 50 Train loss 1.67 ACC 0.53125 on epoch=24
06/22/2022 04:42:41 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.53125 on epoch=24, global_step=50
06/22/2022 04:42:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.56 on epoch=29
06/22/2022 04:42:44 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=34
06/22/2022 04:42:45 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=39
06/22/2022 04:42:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
06/22/2022 04:42:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=49
06/22/2022 04:42:48 - INFO - __main__ - Global step 100 Train loss 0.46 ACC 0.59375 on epoch=49
06/22/2022 04:42:48 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=49, global_step=100
06/22/2022 04:42:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.38 on epoch=54
06/22/2022 04:42:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=59
06/22/2022 04:42:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.34 on epoch=64
06/22/2022 04:42:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.29 on epoch=69
06/22/2022 04:42:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=74
06/22/2022 04:42:55 - INFO - __main__ - Global step 150 Train loss 0.34 ACC 0.65625 on epoch=74
06/22/2022 04:42:55 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=74, global_step=150
06/22/2022 04:42:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.34 on epoch=79
06/22/2022 04:42:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.30 on epoch=84
06/22/2022 04:42:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=89
06/22/2022 04:43:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=94
06/22/2022 04:43:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.23 on epoch=99
06/22/2022 04:43:01 - INFO - __main__ - Global step 200 Train loss 0.27 ACC 0.59375 on epoch=99
06/22/2022 04:43:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.30 on epoch=104
06/22/2022 04:43:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=109
06/22/2022 04:43:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=114
06/22/2022 04:43:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=119
06/22/2022 04:43:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=124
06/22/2022 04:43:08 - INFO - __main__ - Global step 250 Train loss 0.27 ACC 0.625 on epoch=124
06/22/2022 04:43:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=129
06/22/2022 04:43:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=134
06/22/2022 04:43:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=139
06/22/2022 04:43:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.20 on epoch=144
06/22/2022 04:43:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=149
06/22/2022 04:43:15 - INFO - __main__ - Global step 300 Train loss 0.22 ACC 0.5625 on epoch=149
06/22/2022 04:43:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.18 on epoch=154
06/22/2022 04:43:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=159
06/22/2022 04:43:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.19 on epoch=164
06/22/2022 04:43:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.17 on epoch=169
06/22/2022 04:43:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.13 on epoch=174
06/22/2022 04:43:22 - INFO - __main__ - Global step 350 Train loss 0.17 ACC 0.59375 on epoch=174
06/22/2022 04:43:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=179
06/22/2022 04:43:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=184
06/22/2022 04:43:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.14 on epoch=189
06/22/2022 04:43:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.09 on epoch=194
06/22/2022 04:43:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.08 on epoch=199
06/22/2022 04:43:28 - INFO - __main__ - Global step 400 Train loss 0.12 ACC 0.59375 on epoch=199
06/22/2022 04:43:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
06/22/2022 04:43:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.06 on epoch=209
06/22/2022 04:43:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.04 on epoch=214
06/22/2022 04:43:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.04 on epoch=219
06/22/2022 04:43:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.06 on epoch=224
06/22/2022 04:43:35 - INFO - __main__ - Global step 450 Train loss 0.06 ACC 0.625 on epoch=224
06/22/2022 04:43:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.04 on epoch=229
06/22/2022 04:43:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.05 on epoch=234
06/22/2022 04:43:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.03 on epoch=239
06/22/2022 04:43:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
06/22/2022 04:43:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.05 on epoch=249
06/22/2022 04:43:42 - INFO - __main__ - Global step 500 Train loss 0.04 ACC 0.59375 on epoch=249
06/22/2022 04:43:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.04 on epoch=254
06/22/2022 04:43:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
06/22/2022 04:43:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
06/22/2022 04:43:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.01 on epoch=269
06/22/2022 04:43:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
06/22/2022 04:43:48 - INFO - __main__ - Global step 550 Train loss 0.02 ACC 0.625 on epoch=274
06/22/2022 04:43:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
06/22/2022 04:43:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
06/22/2022 04:43:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
06/22/2022 04:43:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
06/22/2022 04:43:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
06/22/2022 04:43:55 - INFO - __main__ - Global step 600 Train loss 0.01 ACC 0.59375 on epoch=299
06/22/2022 04:43:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
06/22/2022 04:43:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
06/22/2022 04:43:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
06/22/2022 04:44:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
06/22/2022 04:44:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
06/22/2022 04:44:02 - INFO - __main__ - Global step 650 Train loss 0.01 ACC 0.59375 on epoch=324
06/22/2022 04:44:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
06/22/2022 04:44:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
06/22/2022 04:44:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
06/22/2022 04:44:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
06/22/2022 04:44:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
06/22/2022 04:44:08 - INFO - __main__ - Global step 700 Train loss 0.01 ACC 0.625 on epoch=349
06/22/2022 04:44:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
06/22/2022 04:44:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
06/22/2022 04:44:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
06/22/2022 04:44:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/22/2022 04:44:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
06/22/2022 04:44:15 - INFO - __main__ - Global step 750 Train loss 0.01 ACC 0.59375 on epoch=374
06/22/2022 04:44:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
06/22/2022 04:44:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
06/22/2022 04:44:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
06/22/2022 04:44:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
06/22/2022 04:44:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
06/22/2022 04:44:22 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.59375 on epoch=399
06/22/2022 04:44:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
06/22/2022 04:44:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
06/22/2022 04:44:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
06/22/2022 04:44:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
06/22/2022 04:44:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
06/22/2022 04:44:28 - INFO - __main__ - Global step 850 Train loss 0.00 ACC 0.59375 on epoch=424
06/22/2022 04:44:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
06/22/2022 04:44:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
06/22/2022 04:44:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
06/22/2022 04:44:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
06/22/2022 04:44:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
06/22/2022 04:44:35 - INFO - __main__ - Global step 900 Train loss 0.00 ACC 0.59375 on epoch=449
06/22/2022 04:44:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
06/22/2022 04:44:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/22/2022 04:44:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/22/2022 04:44:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
06/22/2022 04:44:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/22/2022 04:44:42 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.625 on epoch=474
06/22/2022 04:44:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
06/22/2022 04:44:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
06/22/2022 04:44:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
06/22/2022 04:44:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/22/2022 04:44:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/22/2022 04:44:49 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.59375 on epoch=499
06/22/2022 04:44:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/22/2022 04:44:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
06/22/2022 04:44:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/22/2022 04:44:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/22/2022 04:44:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/22/2022 04:44:55 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.59375 on epoch=524
06/22/2022 04:44:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/22/2022 04:44:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/22/2022 04:44:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/22/2022 04:45:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
06/22/2022 04:45:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/22/2022 04:45:02 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.59375 on epoch=549
06/22/2022 04:45:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 04:45:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/22/2022 04:45:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/22/2022 04:45:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/22/2022 04:45:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/22/2022 04:45:09 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.59375 on epoch=574
06/22/2022 04:45:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/22/2022 04:45:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/22/2022 04:45:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/22/2022 04:45:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 04:45:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/22/2022 04:45:15 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.59375 on epoch=599
06/22/2022 04:45:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/22/2022 04:45:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/22/2022 04:45:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 04:45:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/22/2022 04:45:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 04:45:22 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.59375 on epoch=624
06/22/2022 04:45:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 04:45:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 04:45:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 04:45:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 04:45:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 04:45:29 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.59375 on epoch=649
06/22/2022 04:45:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 04:45:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 04:45:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 04:45:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 04:45:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 04:45:35 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.59375 on epoch=674
06/22/2022 04:45:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 04:45:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 04:45:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 04:45:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 04:45:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 04:45:42 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.59375 on epoch=699
06/22/2022 04:45:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 04:45:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 04:45:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 04:45:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 04:45:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 04:45:49 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.625 on epoch=724
06/22/2022 04:45:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/22/2022 04:45:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 04:45:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 04:45:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 04:45:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 04:45:56 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.625 on epoch=749
06/22/2022 04:45:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 04:45:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 04:45:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 04:46:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 04:46:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 04:46:02 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.59375 on epoch=774
06/22/2022 04:46:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 04:46:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 04:46:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 04:46:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 04:46:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 04:46:09 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.5625 on epoch=799
06/22/2022 04:46:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 04:46:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 04:46:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 04:46:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 04:46:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 04:46:16 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.59375 on epoch=824
06/22/2022 04:46:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 04:46:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 04:46:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 04:46:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 04:46:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 04:46:22 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.625 on epoch=849
06/22/2022 04:46:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 04:46:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 04:46:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 04:46:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 04:46:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 04:46:29 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.59375 on epoch=874
06/22/2022 04:46:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 04:46:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 04:46:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 04:46:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 04:46:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 04:46:36 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.59375 on epoch=899
06/22/2022 04:46:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 04:46:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 04:46:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 04:46:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/22/2022 04:46:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 04:46:42 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.59375 on epoch=924
06/22/2022 04:46:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 04:46:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 04:46:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 04:46:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 04:46:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 04:46:49 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.625 on epoch=949
06/22/2022 04:46:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 04:46:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 04:46:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 04:46:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 04:46:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 04:46:56 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.59375 on epoch=974
06/22/2022 04:46:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 04:46:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 04:46:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 04:47:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 04:47:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 04:47:03 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.625 on epoch=999
06/22/2022 04:47:03 - INFO - __main__ - save last model!
06/22/2022 04:47:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 04:47:03 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 04:47:03 - INFO - __main__ - Printing 3 examples
06/22/2022 04:47:03 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 04:47:03 - INFO - __main__ - ['equivalent']
06/22/2022 04:47:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 04:47:03 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:47:03 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 04:47:03 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:47:03 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:47:03 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:47:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:47:03 - INFO - __main__ - Printing 3 examples
06/22/2022 04:47:03 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/22/2022 04:47:03 - INFO - __main__ - ['equivalent']
06/22/2022 04:47:03 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/22/2022 04:47:03 - INFO - __main__ - ['equivalent']
06/22/2022 04:47:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/22/2022 04:47:03 - INFO - __main__ - ['equivalent']
06/22/2022 04:47:03 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:47:03 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:47:03 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:47:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:47:03 - INFO - __main__ - Printing 3 examples
06/22/2022 04:47:03 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/22/2022 04:47:03 - INFO - __main__ - ['equivalent']
06/22/2022 04:47:03 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/22/2022 04:47:03 - INFO - __main__ - ['equivalent']
06/22/2022 04:47:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/22/2022 04:47:03 - INFO - __main__ - ['equivalent']
06/22/2022 04:47:03 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:47:03 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:47:03 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 04:47:03 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:47:08 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:47:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:47:09 - INFO - __main__ - Starting training!
06/22/2022 04:47:11 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_21_0.5_8_predictions.txt
06/22/2022 04:47:11 - INFO - __main__ - ACC on test data: 0.5539
06/22/2022 04:47:11 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.5, bsz=8, dev_performance=0.65625, test_performance=0.553921568627451
06/22/2022 04:47:11 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.4, bsz=8 ...
06/22/2022 04:47:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:47:12 - INFO - __main__ - Printing 3 examples
06/22/2022 04:47:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/22/2022 04:47:12 - INFO - __main__ - ['equivalent']
06/22/2022 04:47:12 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/22/2022 04:47:12 - INFO - __main__ - ['equivalent']
06/22/2022 04:47:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/22/2022 04:47:12 - INFO - __main__ - ['equivalent']
06/22/2022 04:47:12 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:47:12 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:47:12 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:47:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:47:12 - INFO - __main__ - Printing 3 examples
06/22/2022 04:47:12 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/22/2022 04:47:12 - INFO - __main__ - ['equivalent']
06/22/2022 04:47:12 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/22/2022 04:47:12 - INFO - __main__ - ['equivalent']
06/22/2022 04:47:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/22/2022 04:47:12 - INFO - __main__ - ['equivalent']
06/22/2022 04:47:12 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:47:12 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:47:12 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:47:18 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:47:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:47:18 - INFO - __main__ - Starting training!
06/22/2022 04:47:19 - INFO - __main__ - Step 10 Global step 10 Train loss 3.57 on epoch=4
06/22/2022 04:47:21 - INFO - __main__ - Step 20 Global step 20 Train loss 2.38 on epoch=9
06/22/2022 04:47:22 - INFO - __main__ - Step 30 Global step 30 Train loss 1.69 on epoch=14
06/22/2022 04:47:23 - INFO - __main__ - Step 40 Global step 40 Train loss 1.20 on epoch=19
06/22/2022 04:47:24 - INFO - __main__ - Step 50 Global step 50 Train loss 0.73 on epoch=24
06/22/2022 04:47:25 - INFO - __main__ - Global step 50 Train loss 1.91 ACC 0.5 on epoch=24
06/22/2022 04:47:25 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/22/2022 04:47:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=29
06/22/2022 04:47:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=34
06/22/2022 04:47:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=39
06/22/2022 04:47:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.38 on epoch=44
06/22/2022 04:47:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=49
06/22/2022 04:47:32 - INFO - __main__ - Global step 100 Train loss 0.50 ACC 0.59375 on epoch=49
06/22/2022 04:47:32 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=49, global_step=100
06/22/2022 04:47:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=54
06/22/2022 04:47:34 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=59
06/22/2022 04:47:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=64
06/22/2022 04:47:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.35 on epoch=69
06/22/2022 04:47:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.26 on epoch=74
06/22/2022 04:47:38 - INFO - __main__ - Global step 150 Train loss 0.35 ACC 0.59375 on epoch=74
06/22/2022 04:47:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.33 on epoch=79
06/22/2022 04:47:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.30 on epoch=84
06/22/2022 04:47:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.34 on epoch=89
06/22/2022 04:47:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=94
06/22/2022 04:47:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
06/22/2022 04:47:45 - INFO - __main__ - Global step 200 Train loss 0.30 ACC 0.65625 on epoch=99
06/22/2022 04:47:45 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=99, global_step=200
06/22/2022 04:47:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.28 on epoch=104
06/22/2022 04:47:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.31 on epoch=109
06/22/2022 04:47:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
06/22/2022 04:47:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=119
06/22/2022 04:47:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=124
06/22/2022 04:47:52 - INFO - __main__ - Global step 250 Train loss 0.29 ACC 0.65625 on epoch=124
06/22/2022 04:47:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
06/22/2022 04:47:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=134
06/22/2022 04:47:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=139
06/22/2022 04:47:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
06/22/2022 04:47:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.21 on epoch=149
06/22/2022 04:47:58 - INFO - __main__ - Global step 300 Train loss 0.23 ACC 0.59375 on epoch=149
06/22/2022 04:48:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.19 on epoch=154
06/22/2022 04:48:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=159
06/22/2022 04:48:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.18 on epoch=164
06/22/2022 04:48:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.19 on epoch=169
06/22/2022 04:48:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.19 on epoch=174
06/22/2022 04:48:05 - INFO - __main__ - Global step 350 Train loss 0.19 ACC 0.625 on epoch=174
06/22/2022 04:48:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.19 on epoch=179
06/22/2022 04:48:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.19 on epoch=184
06/22/2022 04:48:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.18 on epoch=189
06/22/2022 04:48:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.17 on epoch=194
06/22/2022 04:48:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.12 on epoch=199
06/22/2022 04:48:12 - INFO - __main__ - Global step 400 Train loss 0.17 ACC 0.625 on epoch=199
06/22/2022 04:48:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
06/22/2022 04:48:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.09 on epoch=209
06/22/2022 04:48:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
06/22/2022 04:48:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.07 on epoch=219
06/22/2022 04:48:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
06/22/2022 04:48:18 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.59375 on epoch=224
06/22/2022 04:48:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.06 on epoch=229
06/22/2022 04:48:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
06/22/2022 04:48:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.06 on epoch=239
06/22/2022 04:48:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
06/22/2022 04:48:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.04 on epoch=249
06/22/2022 04:48:25 - INFO - __main__ - Global step 500 Train loss 0.06 ACC 0.59375 on epoch=249
06/22/2022 04:48:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
06/22/2022 04:48:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
06/22/2022 04:48:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
06/22/2022 04:48:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=269
06/22/2022 04:48:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.02 on epoch=274
06/22/2022 04:48:32 - INFO - __main__ - Global step 550 Train loss 0.03 ACC 0.59375 on epoch=274
06/22/2022 04:48:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
06/22/2022 04:48:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
06/22/2022 04:48:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.04 on epoch=289
06/22/2022 04:48:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
06/22/2022 04:48:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
06/22/2022 04:48:39 - INFO - __main__ - Global step 600 Train loss 0.02 ACC 0.5625 on epoch=299
06/22/2022 04:48:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=304
06/22/2022 04:48:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
06/22/2022 04:48:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
06/22/2022 04:48:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
06/22/2022 04:48:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/22/2022 04:48:45 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.59375 on epoch=324
06/22/2022 04:48:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
06/22/2022 04:48:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
06/22/2022 04:48:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
06/22/2022 04:48:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
06/22/2022 04:48:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
06/22/2022 04:48:52 - INFO - __main__ - Global step 700 Train loss 0.01 ACC 0.625 on epoch=349
06/22/2022 04:48:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
06/22/2022 04:48:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
06/22/2022 04:48:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/22/2022 04:48:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/22/2022 04:48:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
06/22/2022 04:48:59 - INFO - __main__ - Global step 750 Train loss 0.01 ACC 0.625 on epoch=374
06/22/2022 04:49:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
06/22/2022 04:49:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/22/2022 04:49:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
06/22/2022 04:49:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/22/2022 04:49:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
06/22/2022 04:49:05 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.625 on epoch=399
06/22/2022 04:49:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
06/22/2022 04:49:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/22/2022 04:49:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/22/2022 04:49:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
06/22/2022 04:49:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/22/2022 04:49:12 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.59375 on epoch=424
06/22/2022 04:49:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/22/2022 04:49:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
06/22/2022 04:49:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
06/22/2022 04:49:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
06/22/2022 04:49:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
06/22/2022 04:49:19 - INFO - __main__ - Global step 900 Train loss 0.00 ACC 0.59375 on epoch=449
06/22/2022 04:49:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
06/22/2022 04:49:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/22/2022 04:49:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
06/22/2022 04:49:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
06/22/2022 04:49:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/22/2022 04:49:25 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.59375 on epoch=474
06/22/2022 04:49:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
06/22/2022 04:49:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/22/2022 04:49:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/22/2022 04:49:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/22/2022 04:49:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/22/2022 04:49:32 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.625 on epoch=499
06/22/2022 04:49:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/22/2022 04:49:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/22/2022 04:49:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/22/2022 04:49:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
06/22/2022 04:49:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
06/22/2022 04:49:39 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.5625 on epoch=524
06/22/2022 04:49:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/22/2022 04:49:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/22/2022 04:49:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/22/2022 04:49:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/22/2022 04:49:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/22/2022 04:49:45 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.59375 on epoch=549
06/22/2022 04:49:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 04:49:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/22/2022 04:49:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/22/2022 04:49:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/22/2022 04:49:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/22/2022 04:49:52 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.59375 on epoch=574
06/22/2022 04:49:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/22/2022 04:49:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/22/2022 04:49:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/22/2022 04:49:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 04:49:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
06/22/2022 04:49:59 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.59375 on epoch=599
06/22/2022 04:50:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
06/22/2022 04:50:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/22/2022 04:50:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 04:50:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/22/2022 04:50:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 04:50:05 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.59375 on epoch=624
06/22/2022 04:50:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 04:50:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 04:50:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 04:50:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 04:50:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 04:50:12 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.59375 on epoch=649
06/22/2022 04:50:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 04:50:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 04:50:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 04:50:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 04:50:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 04:50:19 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.625 on epoch=674
06/22/2022 04:50:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 04:50:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 04:50:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 04:50:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 04:50:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 04:50:25 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.59375 on epoch=699
06/22/2022 04:50:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 04:50:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 04:50:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 04:50:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
06/22/2022 04:50:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 04:50:32 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.625 on epoch=724
06/22/2022 04:50:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/22/2022 04:50:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 04:50:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 04:50:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
06/22/2022 04:50:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 04:50:39 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.5625 on epoch=749
06/22/2022 04:50:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 04:50:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 04:50:42 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 04:50:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 04:50:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 04:50:45 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.5625 on epoch=774
06/22/2022 04:50:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 04:50:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/22/2022 04:50:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 04:50:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 04:50:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 04:50:52 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.59375 on epoch=799
06/22/2022 04:50:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 04:50:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 04:50:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 04:50:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 04:50:58 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 04:50:59 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.5625 on epoch=824
06/22/2022 04:51:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 04:51:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 04:51:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 04:51:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 04:51:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 04:51:06 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.5625 on epoch=849
06/22/2022 04:51:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 04:51:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 04:51:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 04:51:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 04:51:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 04:51:12 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.59375 on epoch=874
06/22/2022 04:51:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 04:51:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
06/22/2022 04:51:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 04:51:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 04:51:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 04:51:19 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.5625 on epoch=899
06/22/2022 04:51:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 04:51:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 04:51:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 04:51:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 04:51:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
06/22/2022 04:51:26 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.5625 on epoch=924
06/22/2022 04:51:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 04:51:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 04:51:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 04:51:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 04:51:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 04:51:32 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.5625 on epoch=949
06/22/2022 04:51:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 04:51:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 04:51:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 04:51:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 04:51:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 04:51:39 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.5625 on epoch=974
06/22/2022 04:51:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 04:51:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 04:51:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 04:51:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 04:51:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 04:51:46 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.59375 on epoch=999
06/22/2022 04:51:46 - INFO - __main__ - save last model!
06/22/2022 04:51:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 04:51:46 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 04:51:46 - INFO - __main__ - Printing 3 examples
06/22/2022 04:51:46 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 04:51:46 - INFO - __main__ - ['equivalent']
06/22/2022 04:51:46 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 04:51:46 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:51:46 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 04:51:46 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:51:46 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:51:46 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:51:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:51:46 - INFO - __main__ - Printing 3 examples
06/22/2022 04:51:46 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/22/2022 04:51:46 - INFO - __main__ - ['equivalent']
06/22/2022 04:51:46 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/22/2022 04:51:46 - INFO - __main__ - ['equivalent']
06/22/2022 04:51:46 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/22/2022 04:51:46 - INFO - __main__ - ['equivalent']
06/22/2022 04:51:46 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:51:46 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:51:46 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:51:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:51:46 - INFO - __main__ - Printing 3 examples
06/22/2022 04:51:46 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/22/2022 04:51:46 - INFO - __main__ - ['equivalent']
06/22/2022 04:51:46 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/22/2022 04:51:46 - INFO - __main__ - ['equivalent']
06/22/2022 04:51:46 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/22/2022 04:51:46 - INFO - __main__ - ['equivalent']
06/22/2022 04:51:46 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:51:46 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:51:46 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 04:51:46 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:51:52 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:51:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:51:52 - INFO - __main__ - Starting training!
06/22/2022 04:51:54 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_21_0.4_8_predictions.txt
06/22/2022 04:51:54 - INFO - __main__ - ACC on test data: 0.5662
06/22/2022 04:51:54 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.4, bsz=8, dev_performance=0.65625, test_performance=0.5661764705882353
06/22/2022 04:51:54 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.3, bsz=8 ...
06/22/2022 04:51:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:51:55 - INFO - __main__ - Printing 3 examples
06/22/2022 04:51:55 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/22/2022 04:51:55 - INFO - __main__ - ['equivalent']
06/22/2022 04:51:55 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/22/2022 04:51:55 - INFO - __main__ - ['equivalent']
06/22/2022 04:51:55 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/22/2022 04:51:55 - INFO - __main__ - ['equivalent']
06/22/2022 04:51:55 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:51:55 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:51:55 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:51:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:51:55 - INFO - __main__ - Printing 3 examples
06/22/2022 04:51:55 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/22/2022 04:51:55 - INFO - __main__ - ['equivalent']
06/22/2022 04:51:55 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/22/2022 04:51:55 - INFO - __main__ - ['equivalent']
06/22/2022 04:51:55 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/22/2022 04:51:55 - INFO - __main__ - ['equivalent']
06/22/2022 04:51:55 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:51:55 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:51:55 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:52:01 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:52:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:52:01 - INFO - __main__ - Starting training!
06/22/2022 04:52:02 - INFO - __main__ - Step 10 Global step 10 Train loss 3.42 on epoch=4
06/22/2022 04:52:04 - INFO - __main__ - Step 20 Global step 20 Train loss 2.85 on epoch=9
06/22/2022 04:52:05 - INFO - __main__ - Step 30 Global step 30 Train loss 2.17 on epoch=14
06/22/2022 04:52:06 - INFO - __main__ - Step 40 Global step 40 Train loss 1.59 on epoch=19
06/22/2022 04:52:07 - INFO - __main__ - Step 50 Global step 50 Train loss 1.26 on epoch=24
06/22/2022 04:52:08 - INFO - __main__ - Global step 50 Train loss 2.26 ACC 0.03125 on epoch=24
06/22/2022 04:52:08 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.03125 on epoch=24, global_step=50
06/22/2022 04:52:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.97 on epoch=29
06/22/2022 04:52:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.80 on epoch=34
06/22/2022 04:52:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.64 on epoch=39
06/22/2022 04:52:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=44
06/22/2022 04:52:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.58 on epoch=49
06/22/2022 04:52:15 - INFO - __main__ - Global step 100 Train loss 0.71 ACC 0.59375 on epoch=49
06/22/2022 04:52:15 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.59375 on epoch=49, global_step=100
06/22/2022 04:52:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=54
06/22/2022 04:52:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=59
06/22/2022 04:52:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.34 on epoch=64
06/22/2022 04:52:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.35 on epoch=69
06/22/2022 04:52:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=74
06/22/2022 04:52:21 - INFO - __main__ - Global step 150 Train loss 0.41 ACC 0.625 on epoch=74
06/22/2022 04:52:21 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=74, global_step=150
06/22/2022 04:52:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=79
06/22/2022 04:52:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=84
06/22/2022 04:52:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.31 on epoch=89
06/22/2022 04:52:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
06/22/2022 04:52:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
06/22/2022 04:52:28 - INFO - __main__ - Global step 200 Train loss 0.38 ACC 0.5 on epoch=99
06/22/2022 04:52:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.35 on epoch=104
06/22/2022 04:52:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=109
06/22/2022 04:52:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=114
06/22/2022 04:52:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.29 on epoch=119
06/22/2022 04:52:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=124
06/22/2022 04:52:35 - INFO - __main__ - Global step 250 Train loss 0.32 ACC 0.75 on epoch=124
06/22/2022 04:52:35 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.75 on epoch=124, global_step=250
06/22/2022 04:52:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=129
06/22/2022 04:52:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=134
06/22/2022 04:52:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=139
06/22/2022 04:52:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
06/22/2022 04:52:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
06/22/2022 04:52:41 - INFO - __main__ - Global step 300 Train loss 0.30 ACC 0.625 on epoch=149
06/22/2022 04:52:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
06/22/2022 04:52:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
06/22/2022 04:52:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=164
06/22/2022 04:52:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
06/22/2022 04:52:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=174
06/22/2022 04:52:48 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.625 on epoch=174
06/22/2022 04:52:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.31 on epoch=179
06/22/2022 04:52:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.20 on epoch=184
06/22/2022 04:52:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.15 on epoch=189
06/22/2022 04:52:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=194
06/22/2022 04:52:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=199
06/22/2022 04:52:55 - INFO - __main__ - Global step 400 Train loss 0.21 ACC 0.53125 on epoch=199
06/22/2022 04:52:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=204
06/22/2022 04:52:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=209
06/22/2022 04:52:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.18 on epoch=214
06/22/2022 04:53:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.16 on epoch=219
06/22/2022 04:53:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=224
06/22/2022 04:53:01 - INFO - __main__ - Global step 450 Train loss 0.18 ACC 0.59375 on epoch=224
06/22/2022 04:53:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=229
06/22/2022 04:53:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.10 on epoch=234
06/22/2022 04:53:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.14 on epoch=239
06/22/2022 04:53:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=244
06/22/2022 04:53:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=249
06/22/2022 04:53:08 - INFO - __main__ - Global step 500 Train loss 0.12 ACC 0.5625 on epoch=249
06/22/2022 04:53:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
06/22/2022 04:53:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.13 on epoch=259
06/22/2022 04:53:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.09 on epoch=264
06/22/2022 04:53:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=269
06/22/2022 04:53:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.08 on epoch=274
06/22/2022 04:53:15 - INFO - __main__ - Global step 550 Train loss 0.10 ACC 0.5625 on epoch=274
06/22/2022 04:53:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
06/22/2022 04:53:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.09 on epoch=284
06/22/2022 04:53:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
06/22/2022 04:53:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
06/22/2022 04:53:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
06/22/2022 04:53:21 - INFO - __main__ - Global step 600 Train loss 0.07 ACC 0.5625 on epoch=299
06/22/2022 04:53:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=304
06/22/2022 04:53:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=309
06/22/2022 04:53:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/22/2022 04:53:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.05 on epoch=319
06/22/2022 04:53:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=324
06/22/2022 04:53:28 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.5625 on epoch=324
06/22/2022 04:53:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=329
06/22/2022 04:53:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
06/22/2022 04:53:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
06/22/2022 04:53:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
06/22/2022 04:53:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=349
06/22/2022 04:53:35 - INFO - __main__ - Global step 700 Train loss 0.05 ACC 0.5625 on epoch=349
06/22/2022 04:53:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
06/22/2022 04:53:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
06/22/2022 04:53:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=364
06/22/2022 04:53:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
06/22/2022 04:53:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=374
06/22/2022 04:53:41 - INFO - __main__ - Global step 750 Train loss 0.04 ACC 0.5625 on epoch=374
06/22/2022 04:53:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/22/2022 04:53:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=384
06/22/2022 04:53:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=389
06/22/2022 04:53:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=394
06/22/2022 04:53:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
06/22/2022 04:53:48 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.5625 on epoch=399
06/22/2022 04:53:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
06/22/2022 04:53:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=409
06/22/2022 04:53:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
06/22/2022 04:53:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
06/22/2022 04:53:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/22/2022 04:53:55 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.53125 on epoch=424
06/22/2022 04:53:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
06/22/2022 04:53:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/22/2022 04:53:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
06/22/2022 04:54:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
06/22/2022 04:54:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/22/2022 04:54:02 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.5625 on epoch=449
06/22/2022 04:54:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
06/22/2022 04:54:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
06/22/2022 04:54:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
06/22/2022 04:54:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/22/2022 04:54:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/22/2022 04:54:08 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.5625 on epoch=474
06/22/2022 04:54:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/22/2022 04:54:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
06/22/2022 04:54:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/22/2022 04:54:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/22/2022 04:54:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
06/22/2022 04:54:15 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.5625 on epoch=499
06/22/2022 04:54:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/22/2022 04:54:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/22/2022 04:54:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
06/22/2022 04:54:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/22/2022 04:54:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
06/22/2022 04:54:21 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.5625 on epoch=524
06/22/2022 04:54:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/22/2022 04:54:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/22/2022 04:54:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/22/2022 04:54:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/22/2022 04:54:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/22/2022 04:54:28 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.5625 on epoch=549
06/22/2022 04:54:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 04:54:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/22/2022 04:54:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 04:54:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/22/2022 04:54:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/22/2022 04:54:35 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.5625 on epoch=574
06/22/2022 04:54:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/22/2022 04:54:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/22/2022 04:54:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/22/2022 04:54:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 04:54:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
06/22/2022 04:54:42 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.5625 on epoch=599
06/22/2022 04:54:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/22/2022 04:54:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
06/22/2022 04:54:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
06/22/2022 04:54:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/22/2022 04:54:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/22/2022 04:54:48 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.53125 on epoch=624
06/22/2022 04:54:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
06/22/2022 04:54:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 04:54:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 04:54:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/22/2022 04:54:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 04:54:55 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.5625 on epoch=649
06/22/2022 04:54:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 04:54:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/22/2022 04:54:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 04:55:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 04:55:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/22/2022 04:55:02 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.59375 on epoch=674
06/22/2022 04:55:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/22/2022 04:55:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/22/2022 04:55:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
06/22/2022 04:55:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 04:55:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/22/2022 04:55:09 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.5625 on epoch=699
06/22/2022 04:55:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 04:55:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 04:55:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
06/22/2022 04:55:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 04:55:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 04:55:15 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.5625 on epoch=724
06/22/2022 04:55:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 04:55:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 04:55:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 04:55:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 04:55:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 04:55:22 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.5625 on epoch=749
06/22/2022 04:55:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 04:55:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 04:55:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 04:55:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
06/22/2022 04:55:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
06/22/2022 04:55:29 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.625 on epoch=774
06/22/2022 04:55:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/22/2022 04:55:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 04:55:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
06/22/2022 04:55:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 04:55:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 04:55:35 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.5625 on epoch=799
06/22/2022 04:55:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 04:55:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 04:55:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 04:55:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 04:55:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
06/22/2022 04:55:42 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.59375 on epoch=824
06/22/2022 04:55:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 04:55:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 04:55:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 04:55:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/22/2022 04:55:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=849
06/22/2022 04:55:49 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.59375 on epoch=849
06/22/2022 04:55:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 04:55:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 04:55:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 04:55:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 04:55:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 04:55:55 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.59375 on epoch=874
06/22/2022 04:55:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 04:55:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 04:55:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/22/2022 04:56:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 04:56:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 04:56:02 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.5625 on epoch=899
06/22/2022 04:56:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=904
06/22/2022 04:56:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 04:56:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 04:56:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=919
06/22/2022 04:56:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 04:56:09 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.5625 on epoch=924
06/22/2022 04:56:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 04:56:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 04:56:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 04:56:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 04:56:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
06/22/2022 04:56:15 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.59375 on epoch=949
06/22/2022 04:56:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 04:56:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 04:56:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 04:56:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 04:56:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 04:56:22 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.59375 on epoch=974
06/22/2022 04:56:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 04:56:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 04:56:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 04:56:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
06/22/2022 04:56:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 04:56:29 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.59375 on epoch=999
06/22/2022 04:56:29 - INFO - __main__ - save last model!
06/22/2022 04:56:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 04:56:29 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 04:56:29 - INFO - __main__ - Printing 3 examples
06/22/2022 04:56:29 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 04:56:29 - INFO - __main__ - ['equivalent']
06/22/2022 04:56:29 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 04:56:29 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:56:29 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 04:56:29 - INFO - __main__ - ['not_equivalent']
06/22/2022 04:56:29 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:56:29 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:56:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:56:29 - INFO - __main__ - Printing 3 examples
06/22/2022 04:56:29 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/22/2022 04:56:29 - INFO - __main__ - ['equivalent']
06/22/2022 04:56:29 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/22/2022 04:56:29 - INFO - __main__ - ['equivalent']
06/22/2022 04:56:30 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/22/2022 04:56:30 - INFO - __main__ - ['equivalent']
06/22/2022 04:56:30 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:56:30 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:56:30 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:56:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:56:30 - INFO - __main__ - Printing 3 examples
06/22/2022 04:56:30 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/22/2022 04:56:30 - INFO - __main__ - ['equivalent']
06/22/2022 04:56:30 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/22/2022 04:56:30 - INFO - __main__ - ['equivalent']
06/22/2022 04:56:30 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/22/2022 04:56:30 - INFO - __main__ - ['equivalent']
06/22/2022 04:56:30 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:56:30 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:56:30 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 04:56:30 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:56:35 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:56:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:56:35 - INFO - __main__ - Starting training!
06/22/2022 04:56:38 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_21_0.3_8_predictions.txt
06/22/2022 04:56:38 - INFO - __main__ - ACC on test data: 0.5613
06/22/2022 04:56:38 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.3, bsz=8, dev_performance=0.75, test_performance=0.5612745098039216
06/22/2022 04:56:38 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.2, bsz=8 ...
06/22/2022 04:56:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:56:39 - INFO - __main__ - Printing 3 examples
06/22/2022 04:56:39 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/22/2022 04:56:39 - INFO - __main__ - ['equivalent']
06/22/2022 04:56:39 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/22/2022 04:56:39 - INFO - __main__ - ['equivalent']
06/22/2022 04:56:39 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/22/2022 04:56:39 - INFO - __main__ - ['equivalent']
06/22/2022 04:56:39 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:56:39 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:56:39 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 04:56:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 04:56:39 - INFO - __main__ - Printing 3 examples
06/22/2022 04:56:39 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/22/2022 04:56:39 - INFO - __main__ - ['equivalent']
06/22/2022 04:56:39 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/22/2022 04:56:39 - INFO - __main__ - ['equivalent']
06/22/2022 04:56:39 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/22/2022 04:56:39 - INFO - __main__ - ['equivalent']
06/22/2022 04:56:39 - INFO - __main__ - Tokenizing Input ...
06/22/2022 04:56:39 - INFO - __main__ - Tokenizing Output ...
06/22/2022 04:56:39 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 04:56:44 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 04:56:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 04:56:44 - INFO - __main__ - Starting training!
06/22/2022 04:56:46 - INFO - __main__ - Step 10 Global step 10 Train loss 3.60 on epoch=4
06/22/2022 04:56:47 - INFO - __main__ - Step 20 Global step 20 Train loss 3.14 on epoch=9
06/22/2022 04:56:48 - INFO - __main__ - Step 30 Global step 30 Train loss 2.47 on epoch=14
06/22/2022 04:56:49 - INFO - __main__ - Step 40 Global step 40 Train loss 2.23 on epoch=19
06/22/2022 04:56:51 - INFO - __main__ - Step 50 Global step 50 Train loss 1.87 on epoch=24
06/22/2022 04:56:51 - INFO - __main__ - Global step 50 Train loss 2.66 ACC 0.0 on epoch=24
06/22/2022 04:56:51 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/22/2022 04:56:52 - INFO - __main__ - Step 60 Global step 60 Train loss 1.52 on epoch=29
06/22/2022 04:56:54 - INFO - __main__ - Step 70 Global step 70 Train loss 1.31 on epoch=34
06/22/2022 04:56:55 - INFO - __main__ - Step 80 Global step 80 Train loss 1.11 on epoch=39
06/22/2022 04:56:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.95 on epoch=44
06/22/2022 04:56:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.79 on epoch=49
06/22/2022 04:56:58 - INFO - __main__ - Global step 100 Train loss 1.14 ACC 0.4375 on epoch=49
06/22/2022 04:56:58 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.4375 on epoch=49, global_step=100
06/22/2022 04:56:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.68 on epoch=54
06/22/2022 04:57:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.66 on epoch=59
06/22/2022 04:57:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.69 on epoch=64
06/22/2022 04:57:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=69
06/22/2022 04:57:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=74
06/22/2022 04:57:05 - INFO - __main__ - Global step 150 Train loss 0.62 ACC 0.5 on epoch=74
06/22/2022 04:57:05 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=74, global_step=150
06/22/2022 04:57:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=79
06/22/2022 04:57:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=84
06/22/2022 04:57:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
06/22/2022 04:57:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=94
06/22/2022 04:57:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.33 on epoch=99
06/22/2022 04:57:11 - INFO - __main__ - Global step 200 Train loss 0.43 ACC 0.5625 on epoch=99
06/22/2022 04:57:11 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=99, global_step=200
06/22/2022 04:57:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=104
06/22/2022 04:57:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=109
06/22/2022 04:57:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
06/22/2022 04:57:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=119
06/22/2022 04:57:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.30 on epoch=124
06/22/2022 04:57:18 - INFO - __main__ - Global step 250 Train loss 0.38 ACC 0.59375 on epoch=124
06/22/2022 04:57:18 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=124, global_step=250
06/22/2022 04:57:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.33 on epoch=129
06/22/2022 04:57:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=134
06/22/2022 04:57:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=139
06/22/2022 04:57:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=144
06/22/2022 04:57:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
06/22/2022 04:57:25 - INFO - __main__ - Global step 300 Train loss 0.32 ACC 0.5625 on epoch=149
06/22/2022 04:57:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=154
06/22/2022 04:57:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.30 on epoch=159
06/22/2022 04:57:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.30 on epoch=164
06/22/2022 04:57:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
06/22/2022 04:57:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.28 on epoch=174
06/22/2022 04:57:31 - INFO - __main__ - Global step 350 Train loss 0.30 ACC 0.5625 on epoch=174
06/22/2022 04:57:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.30 on epoch=179
06/22/2022 04:57:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
06/22/2022 04:57:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=189
06/22/2022 04:57:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
06/22/2022 04:57:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=199
06/22/2022 04:57:38 - INFO - __main__ - Global step 400 Train loss 0.28 ACC 0.5625 on epoch=199
06/22/2022 04:57:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
06/22/2022 04:57:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=209
06/22/2022 04:57:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.29 on epoch=214
06/22/2022 04:57:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=219
06/22/2022 04:57:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=224
06/22/2022 04:57:45 - INFO - __main__ - Global step 450 Train loss 0.28 ACC 0.6875 on epoch=224
06/22/2022 04:57:45 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.6875 on epoch=224, global_step=450
06/22/2022 04:57:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
06/22/2022 04:57:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=234
06/22/2022 04:57:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
06/22/2022 04:57:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
06/22/2022 04:57:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
06/22/2022 04:57:52 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.625 on epoch=249
06/22/2022 04:57:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
06/22/2022 04:57:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=259
06/22/2022 04:57:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=264
06/22/2022 04:57:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=269
06/22/2022 04:57:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
06/22/2022 04:57:58 - INFO - __main__ - Global step 550 Train loss 0.25 ACC 0.625 on epoch=274
06/22/2022 04:58:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=279
06/22/2022 04:58:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=284
06/22/2022 04:58:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=289
06/22/2022 04:58:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=294
06/22/2022 04:58:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=299
06/22/2022 04:58:05 - INFO - __main__ - Global step 600 Train loss 0.19 ACC 0.65625 on epoch=299
06/22/2022 04:58:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=304
06/22/2022 04:58:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=309
06/22/2022 04:58:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.14 on epoch=314
06/22/2022 04:58:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.14 on epoch=319
06/22/2022 04:58:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=324
06/22/2022 04:58:12 - INFO - __main__ - Global step 650 Train loss 0.16 ACC 0.5625 on epoch=324
06/22/2022 04:58:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=329
06/22/2022 04:58:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=334
06/22/2022 04:58:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=339
06/22/2022 04:58:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=344
06/22/2022 04:58:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=349
06/22/2022 04:58:19 - INFO - __main__ - Global step 700 Train loss 0.15 ACC 0.65625 on epoch=349
06/22/2022 04:58:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=354
06/22/2022 04:58:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.15 on epoch=359
06/22/2022 04:58:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=364
06/22/2022 04:58:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=369
06/22/2022 04:58:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=374
06/22/2022 04:58:25 - INFO - __main__ - Global step 750 Train loss 0.14 ACC 0.5625 on epoch=374
06/22/2022 04:58:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=379
06/22/2022 04:58:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=384
06/22/2022 04:58:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=389
06/22/2022 04:58:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=394
06/22/2022 04:58:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=399
06/22/2022 04:58:32 - INFO - __main__ - Global step 800 Train loss 0.13 ACC 0.625 on epoch=399
06/22/2022 04:58:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=404
06/22/2022 04:58:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=409
06/22/2022 04:58:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=414
06/22/2022 04:58:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=419
06/22/2022 04:58:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=424
06/22/2022 04:58:39 - INFO - __main__ - Global step 850 Train loss 0.09 ACC 0.625 on epoch=424
06/22/2022 04:58:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
06/22/2022 04:58:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=434
06/22/2022 04:58:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
06/22/2022 04:58:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=444
06/22/2022 04:58:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
06/22/2022 04:58:45 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.625 on epoch=449
06/22/2022 04:58:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=454
06/22/2022 04:58:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
06/22/2022 04:58:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
06/22/2022 04:58:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=469
06/22/2022 04:58:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=474
06/22/2022 04:58:52 - INFO - __main__ - Global step 950 Train loss 0.06 ACC 0.625 on epoch=474
06/22/2022 04:58:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
06/22/2022 04:58:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=484
06/22/2022 04:58:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=489
06/22/2022 04:58:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
06/22/2022 04:58:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
06/22/2022 04:58:59 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.65625 on epoch=499
06/22/2022 04:59:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
06/22/2022 04:59:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=509
06/22/2022 04:59:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
06/22/2022 04:59:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
06/22/2022 04:59:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
06/22/2022 04:59:05 - INFO - __main__ - Global step 1050 Train loss 0.05 ACC 0.625 on epoch=524
06/22/2022 04:59:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=529
06/22/2022 04:59:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=534
06/22/2022 04:59:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
06/22/2022 04:59:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
06/22/2022 04:59:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
06/22/2022 04:59:12 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.625 on epoch=549
06/22/2022 04:59:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=554
06/22/2022 04:59:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
06/22/2022 04:59:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
06/22/2022 04:59:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
06/22/2022 04:59:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
06/22/2022 04:59:19 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.625 on epoch=574
06/22/2022 04:59:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/22/2022 04:59:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
06/22/2022 04:59:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
06/22/2022 04:59:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
06/22/2022 04:59:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
06/22/2022 04:59:26 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.625 on epoch=599
06/22/2022 04:59:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/22/2022 04:59:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
06/22/2022 04:59:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/22/2022 04:59:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=619
06/22/2022 04:59:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/22/2022 04:59:32 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.59375 on epoch=624
06/22/2022 04:59:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
06/22/2022 04:59:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
06/22/2022 04:59:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/22/2022 04:59:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
06/22/2022 04:59:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/22/2022 04:59:39 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.625 on epoch=649
06/22/2022 04:59:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
06/22/2022 04:59:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
06/22/2022 04:59:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/22/2022 04:59:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/22/2022 04:59:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/22/2022 04:59:46 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.625 on epoch=674
06/22/2022 04:59:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/22/2022 04:59:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
06/22/2022 04:59:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
06/22/2022 04:59:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
06/22/2022 04:59:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/22/2022 04:59:52 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.625 on epoch=699
06/22/2022 04:59:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 04:59:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/22/2022 04:59:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/22/2022 04:59:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
06/22/2022 04:59:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 04:59:59 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.625 on epoch=724
06/22/2022 05:00:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
06/22/2022 05:00:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
06/22/2022 05:00:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
06/22/2022 05:00:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
06/22/2022 05:00:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 05:00:06 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.625 on epoch=749
06/22/2022 05:00:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/22/2022 05:00:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/22/2022 05:00:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/22/2022 05:00:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/22/2022 05:00:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/22/2022 05:00:13 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.59375 on epoch=774
06/22/2022 05:00:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
06/22/2022 05:00:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/22/2022 05:00:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/22/2022 05:00:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
06/22/2022 05:00:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
06/22/2022 05:00:19 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.625 on epoch=799
06/22/2022 05:00:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/22/2022 05:00:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
06/22/2022 05:00:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/22/2022 05:00:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
06/22/2022 05:00:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 05:00:26 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.625 on epoch=824
06/22/2022 05:00:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/22/2022 05:00:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/22/2022 05:00:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 05:00:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/22/2022 05:00:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/22/2022 05:00:33 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.625 on epoch=849
06/22/2022 05:00:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/22/2022 05:00:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 05:00:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=864
06/22/2022 05:00:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/22/2022 05:00:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=874
06/22/2022 05:00:39 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.625 on epoch=874
06/22/2022 05:00:41 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/22/2022 05:00:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
06/22/2022 05:00:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
06/22/2022 05:00:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 05:00:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/22/2022 05:00:46 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.625 on epoch=899
06/22/2022 05:00:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=904
06/22/2022 05:00:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 05:00:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/22/2022 05:00:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
06/22/2022 05:00:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 05:00:53 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.59375 on epoch=924
06/22/2022 05:00:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/22/2022 05:00:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/22/2022 05:00:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 05:00:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 05:00:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 05:00:59 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.59375 on epoch=949
06/22/2022 05:01:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/22/2022 05:01:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
06/22/2022 05:01:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/22/2022 05:01:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/22/2022 05:01:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/22/2022 05:01:06 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.59375 on epoch=974
06/22/2022 05:01:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/22/2022 05:01:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 05:01:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 05:01:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 05:01:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=999
06/22/2022 05:01:13 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.59375 on epoch=999
06/22/2022 05:01:13 - INFO - __main__ - save last model!
06/22/2022 05:01:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 05:01:13 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 05:01:13 - INFO - __main__ - Printing 3 examples
06/22/2022 05:01:13 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 05:01:13 - INFO - __main__ - ['equivalent']
06/22/2022 05:01:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 05:01:13 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:01:13 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 05:01:13 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:01:13 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:01:13 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:01:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:01:13 - INFO - __main__ - Printing 3 examples
06/22/2022 05:01:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/22/2022 05:01:13 - INFO - __main__ - ['equivalent']
06/22/2022 05:01:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/22/2022 05:01:13 - INFO - __main__ - ['equivalent']
06/22/2022 05:01:13 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/22/2022 05:01:13 - INFO - __main__ - ['equivalent']
06/22/2022 05:01:13 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:01:13 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:01:13 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:01:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:01:13 - INFO - __main__ - Printing 3 examples
06/22/2022 05:01:13 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/22/2022 05:01:13 - INFO - __main__ - ['equivalent']
06/22/2022 05:01:13 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/22/2022 05:01:13 - INFO - __main__ - ['equivalent']
06/22/2022 05:01:13 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/22/2022 05:01:13 - INFO - __main__ - ['equivalent']
06/22/2022 05:01:13 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:01:13 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:01:13 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:01:13 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 05:01:19 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:01:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:01:19 - INFO - __main__ - Starting training!
06/22/2022 05:01:21 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_21_0.2_8_predictions.txt
06/22/2022 05:01:21 - INFO - __main__ - ACC on test data: 0.5833
06/22/2022 05:01:22 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.2, bsz=8, dev_performance=0.6875, test_performance=0.5833333333333334
06/22/2022 05:01:22 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.5, bsz=8 ...
06/22/2022 05:01:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:01:22 - INFO - __main__ - Printing 3 examples
06/22/2022 05:01:22 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/22/2022 05:01:22 - INFO - __main__ - ['equivalent']
06/22/2022 05:01:22 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/22/2022 05:01:22 - INFO - __main__ - ['equivalent']
06/22/2022 05:01:22 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/22/2022 05:01:22 - INFO - __main__ - ['equivalent']
06/22/2022 05:01:22 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:01:22 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:01:23 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:01:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:01:23 - INFO - __main__ - Printing 3 examples
06/22/2022 05:01:23 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/22/2022 05:01:23 - INFO - __main__ - ['equivalent']
06/22/2022 05:01:23 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/22/2022 05:01:23 - INFO - __main__ - ['equivalent']
06/22/2022 05:01:23 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/22/2022 05:01:23 - INFO - __main__ - ['equivalent']
06/22/2022 05:01:23 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:01:23 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:01:23 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:01:28 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:01:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:01:28 - INFO - __main__ - Starting training!
06/22/2022 05:01:30 - INFO - __main__ - Step 10 Global step 10 Train loss 3.16 on epoch=4
06/22/2022 05:01:31 - INFO - __main__ - Step 20 Global step 20 Train loss 1.97 on epoch=9
06/22/2022 05:01:32 - INFO - __main__ - Step 30 Global step 30 Train loss 1.33 on epoch=14
06/22/2022 05:01:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.90 on epoch=19
06/22/2022 05:01:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.69 on epoch=24
06/22/2022 05:01:35 - INFO - __main__ - Global step 50 Train loss 1.61 ACC 0.53125 on epoch=24
06/22/2022 05:01:35 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.53125 on epoch=24, global_step=50
06/22/2022 05:01:36 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=29
06/22/2022 05:01:38 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=34
06/22/2022 05:01:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=39
06/22/2022 05:01:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=44
06/22/2022 05:01:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=49
06/22/2022 05:01:42 - INFO - __main__ - Global step 100 Train loss 0.45 ACC 0.53125 on epoch=49
06/22/2022 05:01:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.33 on epoch=54
06/22/2022 05:01:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.34 on epoch=59
06/22/2022 05:01:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=64
06/22/2022 05:01:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.35 on epoch=69
06/22/2022 05:01:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.26 on epoch=74
06/22/2022 05:01:49 - INFO - __main__ - Global step 150 Train loss 0.32 ACC 0.40625 on epoch=74
06/22/2022 05:01:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=79
06/22/2022 05:01:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=84
06/22/2022 05:01:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=89
06/22/2022 05:01:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=94
06/22/2022 05:01:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=99
06/22/2022 05:01:55 - INFO - __main__ - Global step 200 Train loss 0.27 ACC 0.40625 on epoch=99
06/22/2022 05:01:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.17 on epoch=104
06/22/2022 05:01:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.19 on epoch=109
06/22/2022 05:01:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.19 on epoch=114
06/22/2022 05:02:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.16 on epoch=119
06/22/2022 05:02:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.19 on epoch=124
06/22/2022 05:02:02 - INFO - __main__ - Global step 250 Train loss 0.18 ACC 0.4375 on epoch=124
06/22/2022 05:02:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.18 on epoch=129
06/22/2022 05:02:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.16 on epoch=134
06/22/2022 05:02:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.11 on epoch=139
06/22/2022 05:02:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.10 on epoch=144
06/22/2022 05:02:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.13 on epoch=149
06/22/2022 05:02:09 - INFO - __main__ - Global step 300 Train loss 0.13 ACC 0.4375 on epoch=149
06/22/2022 05:02:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.13 on epoch=154
06/22/2022 05:02:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.10 on epoch=159
06/22/2022 05:02:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.11 on epoch=164
06/22/2022 05:02:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.10 on epoch=169
06/22/2022 05:02:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.09 on epoch=174
06/22/2022 05:02:16 - INFO - __main__ - Global step 350 Train loss 0.11 ACC 0.4375 on epoch=174
06/22/2022 05:02:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.06 on epoch=179
06/22/2022 05:02:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.07 on epoch=184
06/22/2022 05:02:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.10 on epoch=189
06/22/2022 05:02:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.08 on epoch=194
06/22/2022 05:02:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.04 on epoch=199
06/22/2022 05:02:22 - INFO - __main__ - Global step 400 Train loss 0.07 ACC 0.46875 on epoch=199
06/22/2022 05:02:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.03 on epoch=204
06/22/2022 05:02:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.05 on epoch=209
06/22/2022 05:02:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
06/22/2022 05:02:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.07 on epoch=219
06/22/2022 05:02:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.05 on epoch=224
06/22/2022 05:02:29 - INFO - __main__ - Global step 450 Train loss 0.06 ACC 0.46875 on epoch=224
06/22/2022 05:02:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.04 on epoch=229
06/22/2022 05:02:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.03 on epoch=234
06/22/2022 05:02:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.04 on epoch=239
06/22/2022 05:02:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.04 on epoch=244
06/22/2022 05:02:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.03 on epoch=249
06/22/2022 05:02:36 - INFO - __main__ - Global step 500 Train loss 0.04 ACC 0.40625 on epoch=249
06/22/2022 05:02:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
06/22/2022 05:02:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
06/22/2022 05:02:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.05 on epoch=264
06/22/2022 05:02:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
06/22/2022 05:02:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=274
06/22/2022 05:02:43 - INFO - __main__ - Global step 550 Train loss 0.06 ACC 0.40625 on epoch=274
06/22/2022 05:02:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
06/22/2022 05:02:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
06/22/2022 05:02:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
06/22/2022 05:02:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
06/22/2022 05:02:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
06/22/2022 05:02:49 - INFO - __main__ - Global step 600 Train loss 0.03 ACC 0.40625 on epoch=299
06/22/2022 05:02:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
06/22/2022 05:02:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
06/22/2022 05:02:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/22/2022 05:02:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
06/22/2022 05:02:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
06/22/2022 05:02:56 - INFO - __main__ - Global step 650 Train loss 0.03 ACC 0.375 on epoch=324
06/22/2022 05:02:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
06/22/2022 05:02:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
06/22/2022 05:03:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
06/22/2022 05:03:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
06/22/2022 05:03:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
06/22/2022 05:03:03 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.46875 on epoch=349
06/22/2022 05:03:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
06/22/2022 05:03:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
06/22/2022 05:03:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/22/2022 05:03:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/22/2022 05:03:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
06/22/2022 05:03:10 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.5 on epoch=374
06/22/2022 05:03:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
06/22/2022 05:03:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/22/2022 05:03:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
06/22/2022 05:03:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/22/2022 05:03:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
06/22/2022 05:03:16 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.40625 on epoch=399
06/22/2022 05:03:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
06/22/2022 05:03:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
06/22/2022 05:03:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
06/22/2022 05:03:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
06/22/2022 05:03:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
06/22/2022 05:03:23 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.4375 on epoch=424
06/22/2022 05:03:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/22/2022 05:03:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=434
06/22/2022 05:03:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
06/22/2022 05:03:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=444
06/22/2022 05:03:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/22/2022 05:03:30 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.5 on epoch=449
06/22/2022 05:03:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/22/2022 05:03:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=459
06/22/2022 05:03:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
06/22/2022 05:03:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/22/2022 05:03:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/22/2022 05:03:37 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.5 on epoch=474
06/22/2022 05:03:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
06/22/2022 05:03:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/22/2022 05:03:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
06/22/2022 05:03:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
06/22/2022 05:03:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/22/2022 05:03:43 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.5 on epoch=499
06/22/2022 05:03:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/22/2022 05:03:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/22/2022 05:03:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
06/22/2022 05:03:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=519
06/22/2022 05:03:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
06/22/2022 05:03:50 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.5 on epoch=524
06/22/2022 05:03:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/22/2022 05:03:52 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/22/2022 05:03:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
06/22/2022 05:03:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/22/2022 05:03:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
06/22/2022 05:03:57 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.46875 on epoch=549
06/22/2022 05:03:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/22/2022 05:03:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
06/22/2022 05:04:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 05:04:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/22/2022 05:04:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/22/2022 05:04:03 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.46875 on epoch=574
06/22/2022 05:04:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/22/2022 05:04:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
06/22/2022 05:04:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
06/22/2022 05:04:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=594
06/22/2022 05:04:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/22/2022 05:04:10 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.46875 on epoch=599
06/22/2022 05:04:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/22/2022 05:04:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/22/2022 05:04:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/22/2022 05:04:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/22/2022 05:04:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/22/2022 05:04:17 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.46875 on epoch=624
06/22/2022 05:04:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/22/2022 05:04:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
06/22/2022 05:04:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 05:04:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 05:04:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 05:04:24 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.46875 on epoch=649
06/22/2022 05:04:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/22/2022 05:04:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/22/2022 05:04:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/22/2022 05:04:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 05:04:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/22/2022 05:04:30 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.5 on epoch=674
06/22/2022 05:04:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 05:04:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 05:04:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 05:04:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 05:04:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 05:04:37 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.5 on epoch=699
06/22/2022 05:04:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 05:04:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/22/2022 05:04:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
06/22/2022 05:04:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 05:04:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
06/22/2022 05:04:44 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.46875 on epoch=724
06/22/2022 05:04:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=729
06/22/2022 05:04:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
06/22/2022 05:04:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 05:04:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 05:04:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/22/2022 05:04:51 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.5 on epoch=749
06/22/2022 05:04:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
06/22/2022 05:04:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 05:04:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 05:04:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/22/2022 05:04:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 05:04:58 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.5 on epoch=774
06/22/2022 05:04:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 05:05:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 05:05:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 05:05:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=794
06/22/2022 05:05:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 05:05:04 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.5 on epoch=799
06/22/2022 05:05:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 05:05:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 05:05:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/22/2022 05:05:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 05:05:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 05:05:11 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.5 on epoch=824
06/22/2022 05:05:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/22/2022 05:05:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 05:05:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/22/2022 05:05:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 05:05:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 05:05:18 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.5 on epoch=849
06/22/2022 05:05:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 05:05:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 05:05:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/22/2022 05:05:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 05:05:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 05:05:24 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.4375 on epoch=874
06/22/2022 05:05:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/22/2022 05:05:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 05:05:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 05:05:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=894
06/22/2022 05:05:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=899
06/22/2022 05:05:31 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.4375 on epoch=899
06/22/2022 05:05:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 05:05:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/22/2022 05:05:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
06/22/2022 05:05:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
06/22/2022 05:05:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/22/2022 05:05:38 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.53125 on epoch=924
06/22/2022 05:05:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 05:05:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 05:05:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
06/22/2022 05:05:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/22/2022 05:05:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 05:05:45 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.4375 on epoch=949
06/22/2022 05:05:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 05:05:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 05:05:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/22/2022 05:05:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 05:05:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
06/22/2022 05:05:51 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.40625 on epoch=974
06/22/2022 05:05:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 05:05:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 05:05:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 05:05:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
06/22/2022 05:05:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
06/22/2022 05:05:58 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.4375 on epoch=999
06/22/2022 05:05:58 - INFO - __main__ - save last model!
06/22/2022 05:05:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 05:05:58 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 05:05:58 - INFO - __main__ - Printing 3 examples
06/22/2022 05:05:58 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 05:05:58 - INFO - __main__ - ['equivalent']
06/22/2022 05:05:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 05:05:58 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:05:58 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 05:05:58 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:05:58 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:05:58 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:05:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:05:59 - INFO - __main__ - Printing 3 examples
06/22/2022 05:05:59 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/22/2022 05:05:59 - INFO - __main__ - ['equivalent']
06/22/2022 05:05:59 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/22/2022 05:05:59 - INFO - __main__ - ['equivalent']
06/22/2022 05:05:59 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/22/2022 05:05:59 - INFO - __main__ - ['equivalent']
06/22/2022 05:05:59 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:05:59 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:05:59 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:05:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:05:59 - INFO - __main__ - Printing 3 examples
06/22/2022 05:05:59 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/22/2022 05:05:59 - INFO - __main__ - ['equivalent']
06/22/2022 05:05:59 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/22/2022 05:05:59 - INFO - __main__ - ['equivalent']
06/22/2022 05:05:59 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/22/2022 05:05:59 - INFO - __main__ - ['equivalent']
06/22/2022 05:05:59 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:05:59 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:05:59 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 05:05:59 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:06:04 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:06:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:06:04 - INFO - __main__ - Starting training!
06/22/2022 05:06:07 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_42_0.5_8_predictions.txt
06/22/2022 05:06:07 - INFO - __main__ - ACC on test data: 0.4289
06/22/2022 05:06:07 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.5, bsz=8, dev_performance=0.53125, test_performance=0.42892156862745096
06/22/2022 05:06:07 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.4, bsz=8 ...
06/22/2022 05:06:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:06:08 - INFO - __main__ - Printing 3 examples
06/22/2022 05:06:08 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/22/2022 05:06:08 - INFO - __main__ - ['equivalent']
06/22/2022 05:06:08 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/22/2022 05:06:08 - INFO - __main__ - ['equivalent']
06/22/2022 05:06:08 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/22/2022 05:06:08 - INFO - __main__ - ['equivalent']
06/22/2022 05:06:08 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:06:08 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:06:08 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:06:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:06:08 - INFO - __main__ - Printing 3 examples
06/22/2022 05:06:08 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/22/2022 05:06:08 - INFO - __main__ - ['equivalent']
06/22/2022 05:06:08 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/22/2022 05:06:08 - INFO - __main__ - ['equivalent']
06/22/2022 05:06:08 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/22/2022 05:06:08 - INFO - __main__ - ['equivalent']
06/22/2022 05:06:08 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:06:08 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:06:08 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:06:13 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:06:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:06:14 - INFO - __main__ - Starting training!
06/22/2022 05:06:15 - INFO - __main__ - Step 10 Global step 10 Train loss 3.32 on epoch=4
06/22/2022 05:06:16 - INFO - __main__ - Step 20 Global step 20 Train loss 2.24 on epoch=9
06/22/2022 05:06:18 - INFO - __main__ - Step 30 Global step 30 Train loss 1.58 on epoch=14
06/22/2022 05:06:19 - INFO - __main__ - Step 40 Global step 40 Train loss 1.08 on epoch=19
06/22/2022 05:06:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.90 on epoch=24
06/22/2022 05:06:21 - INFO - __main__ - Global step 50 Train loss 1.82 ACC 0.40625 on epoch=24
06/22/2022 05:06:21 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.40625 on epoch=24, global_step=50
06/22/2022 05:06:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.70 on epoch=29
06/22/2022 05:06:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=34
06/22/2022 05:06:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=39
06/22/2022 05:06:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=44
06/22/2022 05:06:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=49
06/22/2022 05:06:27 - INFO - __main__ - Global step 100 Train loss 0.53 ACC 0.46875 on epoch=49
06/22/2022 05:06:27 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.46875 on epoch=49, global_step=100
06/22/2022 05:06:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=54
06/22/2022 05:06:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.35 on epoch=59
06/22/2022 05:06:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=64
06/22/2022 05:06:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=69
06/22/2022 05:06:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.32 on epoch=74
06/22/2022 05:06:34 - INFO - __main__ - Global step 150 Train loss 0.34 ACC 0.46875 on epoch=74
06/22/2022 05:06:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
06/22/2022 05:06:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=84
06/22/2022 05:06:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.22 on epoch=89
06/22/2022 05:06:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.24 on epoch=94
06/22/2022 05:06:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=99
06/22/2022 05:06:41 - INFO - __main__ - Global step 200 Train loss 0.26 ACC 0.46875 on epoch=99
06/22/2022 05:06:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.19 on epoch=104
06/22/2022 05:06:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.18 on epoch=109
06/22/2022 05:06:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.18 on epoch=114
06/22/2022 05:06:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.20 on epoch=119
06/22/2022 05:06:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.18 on epoch=124
06/22/2022 05:06:48 - INFO - __main__ - Global step 250 Train loss 0.19 ACC 0.53125 on epoch=124
06/22/2022 05:06:48 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.53125 on epoch=124, global_step=250
06/22/2022 05:06:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.15 on epoch=129
06/22/2022 05:06:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.13 on epoch=134
06/22/2022 05:06:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.14 on epoch=139
06/22/2022 05:06:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.14 on epoch=144
06/22/2022 05:06:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.14 on epoch=149
06/22/2022 05:06:54 - INFO - __main__ - Global step 300 Train loss 0.14 ACC 0.53125 on epoch=149
06/22/2022 05:06:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.09 on epoch=154
06/22/2022 05:06:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.13 on epoch=159
06/22/2022 05:06:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.07 on epoch=164
06/22/2022 05:06:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.15 on epoch=169
06/22/2022 05:07:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.08 on epoch=174
06/22/2022 05:07:01 - INFO - __main__ - Global step 350 Train loss 0.10 ACC 0.5625 on epoch=174
06/22/2022 05:07:01 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=174, global_step=350
06/22/2022 05:07:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.08 on epoch=179
06/22/2022 05:07:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.09 on epoch=184
06/22/2022 05:07:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.09 on epoch=189
06/22/2022 05:07:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.07 on epoch=194
06/22/2022 05:07:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.06 on epoch=199
06/22/2022 05:07:08 - INFO - __main__ - Global step 400 Train loss 0.08 ACC 0.53125 on epoch=199
06/22/2022 05:07:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.07 on epoch=204
06/22/2022 05:07:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.06 on epoch=209
06/22/2022 05:07:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.06 on epoch=214
06/22/2022 05:07:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.04 on epoch=219
06/22/2022 05:07:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.04 on epoch=224
06/22/2022 05:07:14 - INFO - __main__ - Global step 450 Train loss 0.05 ACC 0.5 on epoch=224
06/22/2022 05:07:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=229
06/22/2022 05:07:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.04 on epoch=234
06/22/2022 05:07:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
06/22/2022 05:07:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
06/22/2022 05:07:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.08 on epoch=249
06/22/2022 05:07:21 - INFO - __main__ - Global step 500 Train loss 0.06 ACC 0.53125 on epoch=249
06/22/2022 05:07:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
06/22/2022 05:07:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
06/22/2022 05:07:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
06/22/2022 05:07:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.04 on epoch=269
06/22/2022 05:07:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
06/22/2022 05:07:28 - INFO - __main__ - Global step 550 Train loss 0.05 ACC 0.4375 on epoch=274
06/22/2022 05:07:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
06/22/2022 05:07:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
06/22/2022 05:07:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
06/22/2022 05:07:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=294
06/22/2022 05:07:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
06/22/2022 05:07:35 - INFO - __main__ - Global step 600 Train loss 0.04 ACC 0.4375 on epoch=299
06/22/2022 05:07:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=304
06/22/2022 05:07:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
06/22/2022 05:07:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
06/22/2022 05:07:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
06/22/2022 05:07:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/22/2022 05:07:41 - INFO - __main__ - Global step 650 Train loss 0.03 ACC 0.46875 on epoch=324
06/22/2022 05:07:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
06/22/2022 05:07:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
06/22/2022 05:07:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
06/22/2022 05:07:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
06/22/2022 05:07:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
06/22/2022 05:07:48 - INFO - __main__ - Global step 700 Train loss 0.02 ACC 0.4375 on epoch=349
06/22/2022 05:07:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
06/22/2022 05:07:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
06/22/2022 05:07:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/22/2022 05:07:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
06/22/2022 05:07:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
06/22/2022 05:07:55 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.4375 on epoch=374
06/22/2022 05:07:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
06/22/2022 05:07:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
06/22/2022 05:07:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
06/22/2022 05:08:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
06/22/2022 05:08:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=399
06/22/2022 05:08:02 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.4375 on epoch=399
06/22/2022 05:08:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
06/22/2022 05:08:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=409
06/22/2022 05:08:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
06/22/2022 05:08:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
06/22/2022 05:08:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
06/22/2022 05:08:08 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.46875 on epoch=424
06/22/2022 05:08:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=429
06/22/2022 05:08:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
06/22/2022 05:08:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
06/22/2022 05:08:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
06/22/2022 05:08:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
06/22/2022 05:08:15 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.46875 on epoch=449
06/22/2022 05:08:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
06/22/2022 05:08:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/22/2022 05:08:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/22/2022 05:08:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/22/2022 05:08:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
06/22/2022 05:08:22 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.46875 on epoch=474
06/22/2022 05:08:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/22/2022 05:08:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/22/2022 05:08:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/22/2022 05:08:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/22/2022 05:08:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/22/2022 05:08:29 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.46875 on epoch=499
06/22/2022 05:08:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/22/2022 05:08:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/22/2022 05:08:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/22/2022 05:08:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/22/2022 05:08:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/22/2022 05:08:35 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.46875 on epoch=524
06/22/2022 05:08:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/22/2022 05:08:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
06/22/2022 05:08:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
06/22/2022 05:08:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
06/22/2022 05:08:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=549
06/22/2022 05:08:42 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.46875 on epoch=549
06/22/2022 05:08:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/22/2022 05:08:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
06/22/2022 05:08:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/22/2022 05:08:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/22/2022 05:08:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/22/2022 05:08:49 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.375 on epoch=574
06/22/2022 05:08:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/22/2022 05:08:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=584
06/22/2022 05:08:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/22/2022 05:08:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
06/22/2022 05:08:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
06/22/2022 05:08:56 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.4375 on epoch=599
06/22/2022 05:08:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
06/22/2022 05:08:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
06/22/2022 05:08:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
06/22/2022 05:09:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/22/2022 05:09:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/22/2022 05:09:02 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.4375 on epoch=624
06/22/2022 05:09:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/22/2022 05:09:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=634
06/22/2022 05:09:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
06/22/2022 05:09:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
06/22/2022 05:09:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 05:09:09 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.46875 on epoch=649
06/22/2022 05:09:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/22/2022 05:09:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
06/22/2022 05:09:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/22/2022 05:09:14 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/22/2022 05:09:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 05:09:16 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.46875 on epoch=674
06/22/2022 05:09:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/22/2022 05:09:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/22/2022 05:09:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
06/22/2022 05:09:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/22/2022 05:09:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=699
06/22/2022 05:09:22 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.4375 on epoch=699
06/22/2022 05:09:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 05:09:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 05:09:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 05:09:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 05:09:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 05:09:29 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.40625 on epoch=724
06/22/2022 05:09:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
06/22/2022 05:09:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=734
06/22/2022 05:09:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 05:09:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 05:09:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/22/2022 05:09:36 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.4375 on epoch=749
06/22/2022 05:09:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/22/2022 05:09:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 05:09:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 05:09:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 05:09:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/22/2022 05:09:43 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.40625 on epoch=774
06/22/2022 05:09:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/22/2022 05:09:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/22/2022 05:09:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/22/2022 05:09:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/22/2022 05:09:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 05:09:49 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.4375 on epoch=799
06/22/2022 05:09:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
06/22/2022 05:09:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 05:09:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/22/2022 05:09:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 05:09:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=824
06/22/2022 05:09:56 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.4375 on epoch=824
06/22/2022 05:09:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
06/22/2022 05:09:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
06/22/2022 05:10:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 05:10:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 05:10:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/22/2022 05:10:03 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.4375 on epoch=849
06/22/2022 05:10:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 05:10:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 05:10:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/22/2022 05:10:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
06/22/2022 05:10:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 05:10:09 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.4375 on epoch=874
06/22/2022 05:10:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/22/2022 05:10:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 05:10:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
06/22/2022 05:10:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/22/2022 05:10:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 05:10:16 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.4375 on epoch=899
06/22/2022 05:10:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/22/2022 05:10:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/22/2022 05:10:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 05:10:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=919
06/22/2022 05:10:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/22/2022 05:10:23 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.40625 on epoch=924
06/22/2022 05:10:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
06/22/2022 05:10:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=934
06/22/2022 05:10:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 05:10:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 05:10:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 05:10:30 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.40625 on epoch=949
06/22/2022 05:10:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 05:10:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 05:10:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 05:10:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
06/22/2022 05:10:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 05:10:36 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.4375 on epoch=974
06/22/2022 05:10:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
06/22/2022 05:10:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 05:10:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 05:10:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=994
06/22/2022 05:10:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/22/2022 05:10:43 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.4375 on epoch=999
06/22/2022 05:10:43 - INFO - __main__ - save last model!
06/22/2022 05:10:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 05:10:43 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 05:10:43 - INFO - __main__ - Printing 3 examples
06/22/2022 05:10:43 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 05:10:43 - INFO - __main__ - ['equivalent']
06/22/2022 05:10:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 05:10:43 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:10:43 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 05:10:43 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:10:43 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:10:43 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:10:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:10:44 - INFO - __main__ - Printing 3 examples
06/22/2022 05:10:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/22/2022 05:10:44 - INFO - __main__ - ['equivalent']
06/22/2022 05:10:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/22/2022 05:10:44 - INFO - __main__ - ['equivalent']
06/22/2022 05:10:44 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/22/2022 05:10:44 - INFO - __main__ - ['equivalent']
06/22/2022 05:10:44 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:10:44 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:10:44 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:10:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:10:44 - INFO - __main__ - Printing 3 examples
06/22/2022 05:10:44 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/22/2022 05:10:44 - INFO - __main__ - ['equivalent']
06/22/2022 05:10:44 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/22/2022 05:10:44 - INFO - __main__ - ['equivalent']
06/22/2022 05:10:44 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/22/2022 05:10:44 - INFO - __main__ - ['equivalent']
06/22/2022 05:10:44 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:10:44 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:10:44 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:10:44 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 05:10:49 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:10:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:10:49 - INFO - __main__ - Starting training!
06/22/2022 05:10:52 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_42_0.4_8_predictions.txt
06/22/2022 05:10:52 - INFO - __main__ - ACC on test data: 0.4877
06/22/2022 05:10:52 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.4, bsz=8, dev_performance=0.5625, test_performance=0.4877450980392157
06/22/2022 05:10:52 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.3, bsz=8 ...
06/22/2022 05:10:53 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:10:53 - INFO - __main__ - Printing 3 examples
06/22/2022 05:10:53 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/22/2022 05:10:53 - INFO - __main__ - ['equivalent']
06/22/2022 05:10:53 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/22/2022 05:10:53 - INFO - __main__ - ['equivalent']
06/22/2022 05:10:53 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/22/2022 05:10:53 - INFO - __main__ - ['equivalent']
06/22/2022 05:10:53 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:10:53 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:10:53 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:10:53 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:10:53 - INFO - __main__ - Printing 3 examples
06/22/2022 05:10:53 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/22/2022 05:10:53 - INFO - __main__ - ['equivalent']
06/22/2022 05:10:53 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/22/2022 05:10:53 - INFO - __main__ - ['equivalent']
06/22/2022 05:10:53 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/22/2022 05:10:53 - INFO - __main__ - ['equivalent']
06/22/2022 05:10:53 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:10:53 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:10:53 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:10:58 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:10:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:10:59 - INFO - __main__ - Starting training!
06/22/2022 05:11:00 - INFO - __main__ - Step 10 Global step 10 Train loss 3.43 on epoch=4
06/22/2022 05:11:01 - INFO - __main__ - Step 20 Global step 20 Train loss 2.68 on epoch=9
06/22/2022 05:11:03 - INFO - __main__ - Step 30 Global step 30 Train loss 2.07 on epoch=14
06/22/2022 05:11:04 - INFO - __main__ - Step 40 Global step 40 Train loss 1.69 on epoch=19
06/22/2022 05:11:05 - INFO - __main__ - Step 50 Global step 50 Train loss 1.26 on epoch=24
06/22/2022 05:11:06 - INFO - __main__ - Global step 50 Train loss 2.23 ACC 0.125 on epoch=24
06/22/2022 05:11:06 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.125 on epoch=24, global_step=50
06/22/2022 05:11:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.90 on epoch=29
06/22/2022 05:11:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.74 on epoch=34
06/22/2022 05:11:09 - INFO - __main__ - Step 80 Global step 80 Train loss 0.66 on epoch=39
06/22/2022 05:11:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.63 on epoch=44
06/22/2022 05:11:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.58 on epoch=49
06/22/2022 05:11:12 - INFO - __main__ - Global step 100 Train loss 0.71 ACC 0.5 on epoch=49
06/22/2022 05:11:13 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.5 on epoch=49, global_step=100
06/22/2022 05:11:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=54
06/22/2022 05:11:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=59
06/22/2022 05:11:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=64
06/22/2022 05:11:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=69
06/22/2022 05:11:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
06/22/2022 05:11:19 - INFO - __main__ - Global step 150 Train loss 0.43 ACC 0.53125 on epoch=74
06/22/2022 05:11:19 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=74, global_step=150
06/22/2022 05:11:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=79
06/22/2022 05:11:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=84
06/22/2022 05:11:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
06/22/2022 05:11:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.33 on epoch=94
06/22/2022 05:11:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=99
06/22/2022 05:11:26 - INFO - __main__ - Global step 200 Train loss 0.37 ACC 0.5 on epoch=99
06/22/2022 05:11:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.34 on epoch=104
06/22/2022 05:11:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
06/22/2022 05:11:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.34 on epoch=114
06/22/2022 05:11:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.36 on epoch=119
06/22/2022 05:11:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=124
06/22/2022 05:11:33 - INFO - __main__ - Global step 250 Train loss 0.34 ACC 0.65625 on epoch=124
06/22/2022 05:11:33 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.65625 on epoch=124, global_step=250
06/22/2022 05:11:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.30 on epoch=129
06/22/2022 05:11:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=134
06/22/2022 05:11:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=139
06/22/2022 05:11:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
06/22/2022 05:11:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
06/22/2022 05:11:40 - INFO - __main__ - Global step 300 Train loss 0.29 ACC 0.5625 on epoch=149
06/22/2022 05:11:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=154
06/22/2022 05:11:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
06/22/2022 05:11:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
06/22/2022 05:11:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=169
06/22/2022 05:11:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=174
06/22/2022 05:11:46 - INFO - __main__ - Global step 350 Train loss 0.29 ACC 0.59375 on epoch=174
06/22/2022 05:11:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
06/22/2022 05:11:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=184
06/22/2022 05:11:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
06/22/2022 05:11:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=194
06/22/2022 05:11:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
06/22/2022 05:11:53 - INFO - __main__ - Global step 400 Train loss 0.26 ACC 0.5625 on epoch=199
06/22/2022 05:11:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
06/22/2022 05:11:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
06/22/2022 05:11:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
06/22/2022 05:11:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=219
06/22/2022 05:11:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=224
06/22/2022 05:12:00 - INFO - __main__ - Global step 450 Train loss 0.23 ACC 0.46875 on epoch=224
06/22/2022 05:12:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=229
06/22/2022 05:12:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=234
06/22/2022 05:12:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=239
06/22/2022 05:12:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=244
06/22/2022 05:12:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=249
06/22/2022 05:12:07 - INFO - __main__ - Global step 500 Train loss 0.19 ACC 0.5 on epoch=249
06/22/2022 05:12:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.12 on epoch=254
06/22/2022 05:12:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.12 on epoch=259
06/22/2022 05:12:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
06/22/2022 05:12:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=269
06/22/2022 05:12:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.09 on epoch=274
06/22/2022 05:12:13 - INFO - __main__ - Global step 550 Train loss 0.10 ACC 0.375 on epoch=274
06/22/2022 05:12:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=279
06/22/2022 05:12:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.10 on epoch=284
06/22/2022 05:12:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
06/22/2022 05:12:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=294
06/22/2022 05:12:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
06/22/2022 05:12:20 - INFO - __main__ - Global step 600 Train loss 0.08 ACC 0.4375 on epoch=299
06/22/2022 05:12:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
06/22/2022 05:12:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=309
06/22/2022 05:12:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
06/22/2022 05:12:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
06/22/2022 05:12:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=324
06/22/2022 05:12:27 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.46875 on epoch=324
06/22/2022 05:12:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
06/22/2022 05:12:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.07 on epoch=334
06/22/2022 05:12:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
06/22/2022 05:12:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
06/22/2022 05:12:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
06/22/2022 05:12:34 - INFO - __main__ - Global step 700 Train loss 0.05 ACC 0.46875 on epoch=349
06/22/2022 05:12:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=354
06/22/2022 05:12:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=359
06/22/2022 05:12:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
06/22/2022 05:12:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
06/22/2022 05:12:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
06/22/2022 05:12:40 - INFO - __main__ - Global step 750 Train loss 0.05 ACC 0.46875 on epoch=374
06/22/2022 05:12:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
06/22/2022 05:12:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
06/22/2022 05:12:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=389
06/22/2022 05:12:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
06/22/2022 05:12:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
06/22/2022 05:12:47 - INFO - __main__ - Global step 800 Train loss 0.04 ACC 0.5 on epoch=399
06/22/2022 05:12:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
06/22/2022 05:12:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
06/22/2022 05:12:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
06/22/2022 05:12:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
06/22/2022 05:12:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
06/22/2022 05:12:54 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.5 on epoch=424
06/22/2022 05:12:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
06/22/2022 05:12:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
06/22/2022 05:12:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/22/2022 05:12:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
06/22/2022 05:13:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
06/22/2022 05:13:00 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.5 on epoch=449
06/22/2022 05:13:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=454
06/22/2022 05:13:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
06/22/2022 05:13:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
06/22/2022 05:13:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
06/22/2022 05:13:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
06/22/2022 05:13:07 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.53125 on epoch=474
06/22/2022 05:13:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
06/22/2022 05:13:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/22/2022 05:13:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/22/2022 05:13:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=494
06/22/2022 05:13:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
06/22/2022 05:13:14 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.5 on epoch=499
06/22/2022 05:13:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
06/22/2022 05:13:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/22/2022 05:13:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
06/22/2022 05:13:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/22/2022 05:13:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
06/22/2022 05:13:21 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.5 on epoch=524
06/22/2022 05:13:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
06/22/2022 05:13:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
06/22/2022 05:13:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/22/2022 05:13:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
06/22/2022 05:13:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/22/2022 05:13:27 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.53125 on epoch=549
06/22/2022 05:13:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/22/2022 05:13:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/22/2022 05:13:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
06/22/2022 05:13:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
06/22/2022 05:13:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/22/2022 05:13:34 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.5 on epoch=574
06/22/2022 05:13:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=579
06/22/2022 05:13:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=584
06/22/2022 05:13:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
06/22/2022 05:13:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/22/2022 05:13:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
06/22/2022 05:13:41 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.53125 on epoch=599
06/22/2022 05:13:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
06/22/2022 05:13:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/22/2022 05:13:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/22/2022 05:13:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
06/22/2022 05:13:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=624
06/22/2022 05:13:47 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.5625 on epoch=624
06/22/2022 05:13:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
06/22/2022 05:13:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/22/2022 05:13:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/22/2022 05:13:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/22/2022 05:13:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
06/22/2022 05:13:54 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.5625 on epoch=649
06/22/2022 05:13:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
06/22/2022 05:13:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
06/22/2022 05:13:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
06/22/2022 05:13:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/22/2022 05:14:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/22/2022 05:14:01 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.59375 on epoch=674
06/22/2022 05:14:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 05:14:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/22/2022 05:14:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
06/22/2022 05:14:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/22/2022 05:14:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/22/2022 05:14:08 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.5625 on epoch=699
06/22/2022 05:14:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 05:14:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
06/22/2022 05:14:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/22/2022 05:14:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=719
06/22/2022 05:14:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/22/2022 05:14:14 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.59375 on epoch=724
06/22/2022 05:14:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
06/22/2022 05:14:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=734
06/22/2022 05:14:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=739
06/22/2022 05:14:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/22/2022 05:14:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/22/2022 05:14:21 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.59375 on epoch=749
06/22/2022 05:14:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/22/2022 05:14:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/22/2022 05:14:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/22/2022 05:14:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/22/2022 05:14:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
06/22/2022 05:14:28 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.59375 on epoch=774
06/22/2022 05:14:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
06/22/2022 05:14:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/22/2022 05:14:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/22/2022 05:14:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
06/22/2022 05:14:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
06/22/2022 05:14:35 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.53125 on epoch=799
06/22/2022 05:14:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=804
06/22/2022 05:14:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/22/2022 05:14:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/22/2022 05:14:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=819
06/22/2022 05:14:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
06/22/2022 05:14:41 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.5625 on epoch=824
06/22/2022 05:14:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 05:14:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
06/22/2022 05:14:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=839
06/22/2022 05:14:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/22/2022 05:14:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 05:14:48 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.53125 on epoch=849
06/22/2022 05:14:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/22/2022 05:14:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/22/2022 05:14:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 05:14:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 05:14:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/22/2022 05:14:55 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.53125 on epoch=874
06/22/2022 05:14:56 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
06/22/2022 05:14:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
06/22/2022 05:14:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 05:15:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
06/22/2022 05:15:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/22/2022 05:15:02 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.53125 on epoch=899
06/22/2022 05:15:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
06/22/2022 05:15:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/22/2022 05:15:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
06/22/2022 05:15:06 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=919
06/22/2022 05:15:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=924
06/22/2022 05:15:08 - INFO - __main__ - Global step 1850 Train loss 0.03 ACC 0.53125 on epoch=924
06/22/2022 05:15:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 05:15:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 05:15:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=939
06/22/2022 05:15:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=944
06/22/2022 05:15:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=949
06/22/2022 05:15:15 - INFO - __main__ - Global step 1900 Train loss 0.04 ACC 0.4375 on epoch=949
06/22/2022 05:15:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=954
06/22/2022 05:15:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/22/2022 05:15:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=964
06/22/2022 05:15:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=969
06/22/2022 05:15:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
06/22/2022 05:15:22 - INFO - __main__ - Global step 1950 Train loss 0.04 ACC 0.4375 on epoch=974
06/22/2022 05:15:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/22/2022 05:15:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/22/2022 05:15:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=989
06/22/2022 05:15:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 05:15:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/22/2022 05:15:28 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.5 on epoch=999
06/22/2022 05:15:28 - INFO - __main__ - save last model!
06/22/2022 05:15:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 05:15:29 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 05:15:29 - INFO - __main__ - Printing 3 examples
06/22/2022 05:15:29 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 05:15:29 - INFO - __main__ - ['equivalent']
06/22/2022 05:15:29 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 05:15:29 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:15:29 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 05:15:29 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:15:29 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:15:29 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:15:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:15:29 - INFO - __main__ - Printing 3 examples
06/22/2022 05:15:29 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/22/2022 05:15:29 - INFO - __main__ - ['equivalent']
06/22/2022 05:15:29 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/22/2022 05:15:29 - INFO - __main__ - ['equivalent']
06/22/2022 05:15:29 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/22/2022 05:15:29 - INFO - __main__ - ['equivalent']
06/22/2022 05:15:29 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:15:29 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:15:29 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:15:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:15:29 - INFO - __main__ - Printing 3 examples
06/22/2022 05:15:29 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/22/2022 05:15:29 - INFO - __main__ - ['equivalent']
06/22/2022 05:15:29 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/22/2022 05:15:29 - INFO - __main__ - ['equivalent']
06/22/2022 05:15:29 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/22/2022 05:15:29 - INFO - __main__ - ['equivalent']
06/22/2022 05:15:29 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:15:29 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:15:29 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 05:15:29 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:15:34 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:15:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:15:35 - INFO - __main__ - Starting training!
06/22/2022 05:15:37 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_42_0.3_8_predictions.txt
06/22/2022 05:15:37 - INFO - __main__ - ACC on test data: 0.5074
06/22/2022 05:15:37 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.3, bsz=8, dev_performance=0.65625, test_performance=0.5073529411764706
06/22/2022 05:15:37 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.2, bsz=8 ...
06/22/2022 05:15:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:15:38 - INFO - __main__ - Printing 3 examples
06/22/2022 05:15:38 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/22/2022 05:15:38 - INFO - __main__ - ['equivalent']
06/22/2022 05:15:38 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/22/2022 05:15:38 - INFO - __main__ - ['equivalent']
06/22/2022 05:15:38 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/22/2022 05:15:38 - INFO - __main__ - ['equivalent']
06/22/2022 05:15:38 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:15:38 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:15:38 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:15:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:15:38 - INFO - __main__ - Printing 3 examples
06/22/2022 05:15:38 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/22/2022 05:15:38 - INFO - __main__ - ['equivalent']
06/22/2022 05:15:38 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/22/2022 05:15:38 - INFO - __main__ - ['equivalent']
06/22/2022 05:15:38 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/22/2022 05:15:38 - INFO - __main__ - ['equivalent']
06/22/2022 05:15:38 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:15:39 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:15:39 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:15:44 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:15:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:15:44 - INFO - __main__ - Starting training!
06/22/2022 05:15:45 - INFO - __main__ - Step 10 Global step 10 Train loss 3.55 on epoch=4
06/22/2022 05:15:47 - INFO - __main__ - Step 20 Global step 20 Train loss 2.78 on epoch=9
06/22/2022 05:15:48 - INFO - __main__ - Step 30 Global step 30 Train loss 2.55 on epoch=14
06/22/2022 05:15:49 - INFO - __main__ - Step 40 Global step 40 Train loss 2.10 on epoch=19
06/22/2022 05:15:50 - INFO - __main__ - Step 50 Global step 50 Train loss 1.72 on epoch=24
06/22/2022 05:15:51 - INFO - __main__ - Global step 50 Train loss 2.54 ACC 0.0 on epoch=24
06/22/2022 05:15:51 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/22/2022 05:15:52 - INFO - __main__ - Step 60 Global step 60 Train loss 1.51 on epoch=29
06/22/2022 05:15:53 - INFO - __main__ - Step 70 Global step 70 Train loss 1.14 on epoch=34
06/22/2022 05:15:55 - INFO - __main__ - Step 80 Global step 80 Train loss 1.07 on epoch=39
06/22/2022 05:15:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.87 on epoch=44
06/22/2022 05:15:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.75 on epoch=49
06/22/2022 05:15:58 - INFO - __main__ - Global step 100 Train loss 1.07 ACC 0.53125 on epoch=49
06/22/2022 05:15:58 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.53125 on epoch=49, global_step=100
06/22/2022 05:15:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.69 on epoch=54
06/22/2022 05:16:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.67 on epoch=59
06/22/2022 05:16:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.63 on epoch=64
06/22/2022 05:16:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=69
06/22/2022 05:16:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=74
06/22/2022 05:16:04 - INFO - __main__ - Global step 150 Train loss 0.61 ACC 0.5 on epoch=74
06/22/2022 05:16:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=79
06/22/2022 05:16:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=84
06/22/2022 05:16:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=89
06/22/2022 05:16:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=94
06/22/2022 05:16:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=99
06/22/2022 05:16:11 - INFO - __main__ - Global step 200 Train loss 0.40 ACC 0.5 on epoch=99
06/22/2022 05:16:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=104
06/22/2022 05:16:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.30 on epoch=109
06/22/2022 05:16:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=114
06/22/2022 05:16:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=119
06/22/2022 05:16:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=124
06/22/2022 05:16:18 - INFO - __main__ - Global step 250 Train loss 0.36 ACC 0.5625 on epoch=124
06/22/2022 05:16:18 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=124, global_step=250
06/22/2022 05:16:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=129
06/22/2022 05:16:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=134
06/22/2022 05:16:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.29 on epoch=139
06/22/2022 05:16:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.30 on epoch=144
06/22/2022 05:16:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=149
06/22/2022 05:16:24 - INFO - __main__ - Global step 300 Train loss 0.31 ACC 0.40625 on epoch=149
06/22/2022 05:16:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
06/22/2022 05:16:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
06/22/2022 05:16:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
06/22/2022 05:16:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=169
06/22/2022 05:16:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=174
06/22/2022 05:16:31 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.4375 on epoch=174
06/22/2022 05:16:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=179
06/22/2022 05:16:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=184
06/22/2022 05:16:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=189
06/22/2022 05:16:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=194
06/22/2022 05:16:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=199
06/22/2022 05:16:38 - INFO - __main__ - Global step 400 Train loss 0.24 ACC 0.4375 on epoch=199
06/22/2022 05:16:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=204
06/22/2022 05:16:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=209
06/22/2022 05:16:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.17 on epoch=214
06/22/2022 05:16:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
06/22/2022 05:16:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=224
06/22/2022 05:16:44 - INFO - __main__ - Global step 450 Train loss 0.16 ACC 0.40625 on epoch=224
06/22/2022 05:16:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.16 on epoch=229
06/22/2022 05:16:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=234
06/22/2022 05:16:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.17 on epoch=239
06/22/2022 05:16:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
06/22/2022 05:16:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=249
06/22/2022 05:16:51 - INFO - __main__ - Global step 500 Train loss 0.17 ACC 0.5 on epoch=249
06/22/2022 05:16:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.15 on epoch=254
06/22/2022 05:16:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=259
06/22/2022 05:16:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=264
06/22/2022 05:16:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.16 on epoch=269
06/22/2022 05:16:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.12 on epoch=274
06/22/2022 05:16:58 - INFO - __main__ - Global step 550 Train loss 0.14 ACC 0.5 on epoch=274
06/22/2022 05:16:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.11 on epoch=279
06/22/2022 05:17:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=284
06/22/2022 05:17:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.11 on epoch=289
06/22/2022 05:17:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
06/22/2022 05:17:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=299
06/22/2022 05:17:04 - INFO - __main__ - Global step 600 Train loss 0.11 ACC 0.46875 on epoch=299
06/22/2022 05:17:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=304
06/22/2022 05:17:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=309
06/22/2022 05:17:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=314
06/22/2022 05:17:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.13 on epoch=319
06/22/2022 05:17:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=324
06/22/2022 05:17:11 - INFO - __main__ - Global step 650 Train loss 0.13 ACC 0.5 on epoch=324
06/22/2022 05:17:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.09 on epoch=329
06/22/2022 05:17:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=334
06/22/2022 05:17:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=339
06/22/2022 05:17:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=344
06/22/2022 05:17:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=349
06/22/2022 05:17:18 - INFO - __main__ - Global step 700 Train loss 0.11 ACC 0.5 on epoch=349
06/22/2022 05:17:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.08 on epoch=354
06/22/2022 05:17:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=359
06/22/2022 05:17:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=364
06/22/2022 05:17:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
06/22/2022 05:17:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=374
06/22/2022 05:17:25 - INFO - __main__ - Global step 750 Train loss 0.09 ACC 0.4375 on epoch=374
06/22/2022 05:17:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
06/22/2022 05:17:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.09 on epoch=384
06/22/2022 05:17:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=389
06/22/2022 05:17:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=394
06/22/2022 05:17:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=399
06/22/2022 05:17:31 - INFO - __main__ - Global step 800 Train loss 0.07 ACC 0.40625 on epoch=399
06/22/2022 05:17:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
06/22/2022 05:17:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=409
06/22/2022 05:17:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=414
06/22/2022 05:17:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=419
06/22/2022 05:17:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
06/22/2022 05:17:38 - INFO - __main__ - Global step 850 Train loss 0.08 ACC 0.46875 on epoch=424
06/22/2022 05:17:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
06/22/2022 05:17:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
06/22/2022 05:17:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=439
06/22/2022 05:17:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=444
06/22/2022 05:17:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
06/22/2022 05:17:45 - INFO - __main__ - Global step 900 Train loss 0.05 ACC 0.40625 on epoch=449
06/22/2022 05:17:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
06/22/2022 05:17:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
06/22/2022 05:17:48 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
06/22/2022 05:17:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
06/22/2022 05:17:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=474
06/22/2022 05:17:51 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.4375 on epoch=474
06/22/2022 05:17:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=479
06/22/2022 05:17:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=484
06/22/2022 05:17:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=489
06/22/2022 05:17:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
06/22/2022 05:17:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
06/22/2022 05:17:58 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.4375 on epoch=499
06/22/2022 05:17:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=504
06/22/2022 05:18:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
06/22/2022 05:18:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
06/22/2022 05:18:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
06/22/2022 05:18:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
06/22/2022 05:18:05 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.40625 on epoch=524
06/22/2022 05:18:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
06/22/2022 05:18:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
06/22/2022 05:18:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
06/22/2022 05:18:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
06/22/2022 05:18:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
06/22/2022 05:18:11 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.375 on epoch=549
06/22/2022 05:18:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=554
06/22/2022 05:18:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
06/22/2022 05:18:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 05:18:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
06/22/2022 05:18:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/22/2022 05:18:18 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.34375 on epoch=574
06/22/2022 05:18:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
06/22/2022 05:18:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
06/22/2022 05:18:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=589
06/22/2022 05:18:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=594
06/22/2022 05:18:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=599
06/22/2022 05:18:25 - INFO - __main__ - Global step 1200 Train loss 0.05 ACC 0.4375 on epoch=599
06/22/2022 05:18:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
06/22/2022 05:18:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=609
06/22/2022 05:18:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
06/22/2022 05:18:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/22/2022 05:18:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
06/22/2022 05:18:31 - INFO - __main__ - Global step 1250 Train loss 0.04 ACC 0.46875 on epoch=624
06/22/2022 05:18:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
06/22/2022 05:18:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
06/22/2022 05:18:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
06/22/2022 05:18:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
06/22/2022 05:18:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
06/22/2022 05:18:38 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.34375 on epoch=649
06/22/2022 05:18:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
06/22/2022 05:18:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
06/22/2022 05:18:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
06/22/2022 05:18:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
06/22/2022 05:18:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
06/22/2022 05:18:45 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.40625 on epoch=674
06/22/2022 05:18:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/22/2022 05:18:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
06/22/2022 05:18:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=689
06/22/2022 05:18:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
06/22/2022 05:18:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=699
06/22/2022 05:18:52 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.40625 on epoch=699
06/22/2022 05:18:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 05:18:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
06/22/2022 05:18:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=714
06/22/2022 05:18:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/22/2022 05:18:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
06/22/2022 05:18:58 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.375 on epoch=724
06/22/2022 05:18:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
06/22/2022 05:19:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
06/22/2022 05:19:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
06/22/2022 05:19:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=744
06/22/2022 05:19:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/22/2022 05:19:05 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.46875 on epoch=749
06/22/2022 05:19:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
06/22/2022 05:19:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
06/22/2022 05:19:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
06/22/2022 05:19:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/22/2022 05:19:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/22/2022 05:19:12 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.4375 on epoch=774
06/22/2022 05:19:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/22/2022 05:19:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/22/2022 05:19:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=789
06/22/2022 05:19:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
06/22/2022 05:19:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=799
06/22/2022 05:19:18 - INFO - __main__ - Global step 1600 Train loss 0.04 ACC 0.375 on epoch=799
06/22/2022 05:19:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=804
06/22/2022 05:19:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
06/22/2022 05:19:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
06/22/2022 05:19:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
06/22/2022 05:19:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
06/22/2022 05:19:25 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.375 on epoch=824
06/22/2022 05:19:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/22/2022 05:19:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
06/22/2022 05:19:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
06/22/2022 05:19:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/22/2022 05:19:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/22/2022 05:19:32 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.375 on epoch=849
06/22/2022 05:19:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=854
06/22/2022 05:19:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
06/22/2022 05:19:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=864
06/22/2022 05:19:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
06/22/2022 05:19:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/22/2022 05:19:38 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.40625 on epoch=874
06/22/2022 05:19:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
06/22/2022 05:19:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
06/22/2022 05:19:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/22/2022 05:19:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=894
06/22/2022 05:19:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/22/2022 05:19:45 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.4375 on epoch=899
06/22/2022 05:19:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
06/22/2022 05:19:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=909
06/22/2022 05:19:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
06/22/2022 05:19:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/22/2022 05:19:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=924
06/22/2022 05:19:52 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.4375 on epoch=924
06/22/2022 05:19:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/22/2022 05:19:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=934
06/22/2022 05:19:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/22/2022 05:19:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/22/2022 05:19:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
06/22/2022 05:19:59 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.4375 on epoch=949
06/22/2022 05:20:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=954
06/22/2022 05:20:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/22/2022 05:20:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/22/2022 05:20:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=969
06/22/2022 05:20:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/22/2022 05:20:05 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.4375 on epoch=974
06/22/2022 05:20:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
06/22/2022 05:20:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/22/2022 05:20:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=989
06/22/2022 05:20:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 05:20:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
06/22/2022 05:20:12 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.46875 on epoch=999
06/22/2022 05:20:12 - INFO - __main__ - save last model!
06/22/2022 05:20:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 05:20:12 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 05:20:12 - INFO - __main__ - Printing 3 examples
06/22/2022 05:20:12 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 05:20:12 - INFO - __main__ - ['equivalent']
06/22/2022 05:20:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 05:20:12 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:20:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 05:20:12 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:20:12 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:20:12 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:20:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:20:13 - INFO - __main__ - Printing 3 examples
06/22/2022 05:20:13 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/22/2022 05:20:13 - INFO - __main__ - ['equivalent']
06/22/2022 05:20:13 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/22/2022 05:20:13 - INFO - __main__ - ['equivalent']
06/22/2022 05:20:13 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/22/2022 05:20:13 - INFO - __main__ - ['equivalent']
06/22/2022 05:20:13 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:20:13 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:20:13 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:20:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:20:13 - INFO - __main__ - Printing 3 examples
06/22/2022 05:20:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/22/2022 05:20:13 - INFO - __main__ - ['equivalent']
06/22/2022 05:20:13 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/22/2022 05:20:13 - INFO - __main__ - ['equivalent']
06/22/2022 05:20:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/22/2022 05:20:13 - INFO - __main__ - ['equivalent']
06/22/2022 05:20:13 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:20:13 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:20:13 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:20:13 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 05:20:18 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:20:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:20:18 - INFO - __main__ - Starting training!
06/22/2022 05:20:21 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_42_0.2_8_predictions.txt
06/22/2022 05:20:21 - INFO - __main__ - ACC on test data: 0.4534
06/22/2022 05:20:21 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.2, bsz=8, dev_performance=0.5625, test_performance=0.4534313725490196
06/22/2022 05:20:21 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.5, bsz=8 ...
06/22/2022 05:20:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:20:22 - INFO - __main__ - Printing 3 examples
06/22/2022 05:20:22 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/22/2022 05:20:22 - INFO - __main__ - ['equivalent']
06/22/2022 05:20:22 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/22/2022 05:20:22 - INFO - __main__ - ['equivalent']
06/22/2022 05:20:22 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/22/2022 05:20:22 - INFO - __main__ - ['equivalent']
06/22/2022 05:20:22 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:20:22 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:20:22 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:20:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:20:22 - INFO - __main__ - Printing 3 examples
06/22/2022 05:20:22 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/22/2022 05:20:22 - INFO - __main__ - ['equivalent']
06/22/2022 05:20:22 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/22/2022 05:20:22 - INFO - __main__ - ['equivalent']
06/22/2022 05:20:22 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/22/2022 05:20:22 - INFO - __main__ - ['equivalent']
06/22/2022 05:20:22 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:20:22 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:20:22 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:20:27 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:20:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:20:28 - INFO - __main__ - Starting training!
06/22/2022 05:20:29 - INFO - __main__ - Step 10 Global step 10 Train loss 3.44 on epoch=4
06/22/2022 05:20:30 - INFO - __main__ - Step 20 Global step 20 Train loss 2.11 on epoch=9
06/22/2022 05:20:32 - INFO - __main__ - Step 30 Global step 30 Train loss 1.39 on epoch=14
06/22/2022 05:20:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.89 on epoch=19
06/22/2022 05:20:34 - INFO - __main__ - Step 50 Global step 50 Train loss 0.64 on epoch=24
06/22/2022 05:20:35 - INFO - __main__ - Global step 50 Train loss 1.69 ACC 0.53125 on epoch=24
06/22/2022 05:20:35 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.53125 on epoch=24, global_step=50
06/22/2022 05:20:36 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=29
06/22/2022 05:20:37 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=34
06/22/2022 05:20:38 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=39
06/22/2022 05:20:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=44
06/22/2022 05:20:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=49
06/22/2022 05:20:41 - INFO - __main__ - Global step 100 Train loss 0.42 ACC 0.59375 on epoch=49
06/22/2022 05:20:42 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=49, global_step=100
06/22/2022 05:20:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.33 on epoch=54
06/22/2022 05:20:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
06/22/2022 05:20:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=64
06/22/2022 05:20:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.32 on epoch=69
06/22/2022 05:20:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=74
06/22/2022 05:20:48 - INFO - __main__ - Global step 150 Train loss 0.33 ACC 0.46875 on epoch=74
06/22/2022 05:20:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
06/22/2022 05:20:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=84
06/22/2022 05:20:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=89
06/22/2022 05:20:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
06/22/2022 05:20:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=99
06/22/2022 05:20:55 - INFO - __main__ - Global step 200 Train loss 0.28 ACC 0.34375 on epoch=99
06/22/2022 05:20:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=104
06/22/2022 05:20:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.23 on epoch=109
06/22/2022 05:20:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=114
06/22/2022 05:21:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.20 on epoch=119
06/22/2022 05:21:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=124
06/22/2022 05:21:02 - INFO - __main__ - Global step 250 Train loss 0.24 ACC 0.53125 on epoch=124
06/22/2022 05:21:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.18 on epoch=129
06/22/2022 05:21:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.19 on epoch=134
06/22/2022 05:21:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.18 on epoch=139
06/22/2022 05:21:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.16 on epoch=144
06/22/2022 05:21:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=149
06/22/2022 05:21:08 - INFO - __main__ - Global step 300 Train loss 0.18 ACC 0.375 on epoch=149
06/22/2022 05:21:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.17 on epoch=154
06/22/2022 05:21:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.12 on epoch=159
06/22/2022 05:21:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.17 on epoch=164
06/22/2022 05:21:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
06/22/2022 05:21:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.07 on epoch=174
06/22/2022 05:21:15 - INFO - __main__ - Global step 350 Train loss 0.13 ACC 0.40625 on epoch=174
06/22/2022 05:21:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.10 on epoch=179
06/22/2022 05:21:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
06/22/2022 05:21:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.07 on epoch=189
06/22/2022 05:21:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.07 on epoch=194
06/22/2022 05:21:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.15 on epoch=199
06/22/2022 05:21:22 - INFO - __main__ - Global step 400 Train loss 0.10 ACC 0.5 on epoch=199
06/22/2022 05:21:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
06/22/2022 05:21:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.08 on epoch=209
06/22/2022 05:21:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
06/22/2022 05:21:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.04 on epoch=219
06/22/2022 05:21:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.07 on epoch=224
06/22/2022 05:21:29 - INFO - __main__ - Global step 450 Train loss 0.07 ACC 0.5 on epoch=224
06/22/2022 05:21:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.03 on epoch=229
06/22/2022 05:21:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
06/22/2022 05:21:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.03 on epoch=239
06/22/2022 05:21:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
06/22/2022 05:21:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.04 on epoch=249
06/22/2022 05:21:35 - INFO - __main__ - Global step 500 Train loss 0.05 ACC 0.46875 on epoch=249
06/22/2022 05:21:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=254
06/22/2022 05:21:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
06/22/2022 05:21:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=264
06/22/2022 05:21:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=269
06/22/2022 05:21:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
06/22/2022 05:21:42 - INFO - __main__ - Global step 550 Train loss 0.06 ACC 0.46875 on epoch=274
06/22/2022 05:21:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
06/22/2022 05:21:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
06/22/2022 05:21:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=289
06/22/2022 05:21:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
06/22/2022 05:21:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
06/22/2022 05:21:49 - INFO - __main__ - Global step 600 Train loss 0.03 ACC 0.53125 on epoch=299
06/22/2022 05:21:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
06/22/2022 05:21:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
06/22/2022 05:21:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/22/2022 05:21:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
06/22/2022 05:21:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
06/22/2022 05:21:56 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.46875 on epoch=324
06/22/2022 05:21:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=329
06/22/2022 05:21:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
06/22/2022 05:21:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
06/22/2022 05:22:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
06/22/2022 05:22:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=349
06/22/2022 05:22:02 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.40625 on epoch=349
06/22/2022 05:22:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
06/22/2022 05:22:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
06/22/2022 05:22:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
06/22/2022 05:22:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/22/2022 05:22:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
06/22/2022 05:22:09 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.4375 on epoch=374
06/22/2022 05:22:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/22/2022 05:22:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
06/22/2022 05:22:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
06/22/2022 05:22:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=394
06/22/2022 05:22:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
06/22/2022 05:22:16 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.46875 on epoch=399
06/22/2022 05:22:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
06/22/2022 05:22:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/22/2022 05:22:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
06/22/2022 05:22:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
06/22/2022 05:22:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/22/2022 05:22:22 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.53125 on epoch=424
06/22/2022 05:22:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
06/22/2022 05:22:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
06/22/2022 05:22:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/22/2022 05:22:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
06/22/2022 05:22:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
06/22/2022 05:22:29 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.59375 on epoch=449
06/22/2022 05:22:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/22/2022 05:22:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
06/22/2022 05:22:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=464
06/22/2022 05:22:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
06/22/2022 05:22:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/22/2022 05:22:36 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.53125 on epoch=474
06/22/2022 05:22:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/22/2022 05:22:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
06/22/2022 05:22:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/22/2022 05:22:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
06/22/2022 05:22:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/22/2022 05:22:43 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.46875 on epoch=499
06/22/2022 05:22:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/22/2022 05:22:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/22/2022 05:22:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/22/2022 05:22:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
06/22/2022 05:22:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/22/2022 05:22:49 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.5 on epoch=524
06/22/2022 05:22:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/22/2022 05:22:52 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/22/2022 05:22:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/22/2022 05:22:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/22/2022 05:22:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/22/2022 05:22:56 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.5 on epoch=549
06/22/2022 05:22:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
06/22/2022 05:22:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
06/22/2022 05:22:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
06/22/2022 05:23:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/22/2022 05:23:02 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/22/2022 05:23:02 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.4375 on epoch=574
06/22/2022 05:23:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/22/2022 05:23:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/22/2022 05:23:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
06/22/2022 05:23:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 05:23:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/22/2022 05:23:09 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.40625 on epoch=599
06/22/2022 05:23:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
06/22/2022 05:23:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/22/2022 05:23:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 05:23:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/22/2022 05:23:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 05:23:16 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.5 on epoch=624
06/22/2022 05:23:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 05:23:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 05:23:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
06/22/2022 05:23:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 05:23:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 05:23:22 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.5 on epoch=649
06/22/2022 05:23:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/22/2022 05:23:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 05:23:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 05:23:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
06/22/2022 05:23:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 05:23:29 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.5 on epoch=674
06/22/2022 05:23:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/22/2022 05:23:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/22/2022 05:23:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=689
06/22/2022 05:23:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 05:23:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 05:23:36 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.4375 on epoch=699
06/22/2022 05:23:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 05:23:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 05:23:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 05:23:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 05:23:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 05:23:43 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.4375 on epoch=724
06/22/2022 05:23:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 05:23:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
06/22/2022 05:23:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 05:23:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/22/2022 05:23:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
06/22/2022 05:23:49 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.46875 on epoch=749
06/22/2022 05:23:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 05:23:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 05:23:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
06/22/2022 05:23:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 05:23:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 05:23:56 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.4375 on epoch=774
06/22/2022 05:23:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 05:23:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 05:24:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 05:24:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 05:24:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
06/22/2022 05:24:03 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.40625 on epoch=799
06/22/2022 05:24:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 05:24:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/22/2022 05:24:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/22/2022 05:24:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/22/2022 05:24:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
06/22/2022 05:24:10 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.4375 on epoch=824
06/22/2022 05:24:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/22/2022 05:24:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 05:24:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 05:24:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/22/2022 05:24:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/22/2022 05:24:16 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.46875 on epoch=849
06/22/2022 05:24:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 05:24:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 05:24:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/22/2022 05:24:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 05:24:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 05:24:23 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.46875 on epoch=874
06/22/2022 05:24:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 05:24:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
06/22/2022 05:24:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=889
06/22/2022 05:24:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 05:24:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 05:24:30 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.4375 on epoch=899
06/22/2022 05:24:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 05:24:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 05:24:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 05:24:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 05:24:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 05:24:37 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.46875 on epoch=924
06/22/2022 05:24:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 05:24:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 05:24:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 05:24:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 05:24:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 05:24:43 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.4375 on epoch=949
06/22/2022 05:24:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 05:24:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 05:24:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/22/2022 05:24:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 05:24:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 05:24:50 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.46875 on epoch=974
06/22/2022 05:24:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
06/22/2022 05:24:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=984
06/22/2022 05:24:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=989
06/22/2022 05:24:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/22/2022 05:24:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
06/22/2022 05:24:57 - INFO - __main__ - Global step 2000 Train loss 0.04 ACC 0.46875 on epoch=999
06/22/2022 05:24:57 - INFO - __main__ - save last model!
06/22/2022 05:24:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 05:24:57 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 05:24:57 - INFO - __main__ - Printing 3 examples
06/22/2022 05:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 05:24:57 - INFO - __main__ - ['equivalent']
06/22/2022 05:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 05:24:57 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 05:24:57 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:24:57 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:24:57 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:24:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:24:57 - INFO - __main__ - Printing 3 examples
06/22/2022 05:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/22/2022 05:24:57 - INFO - __main__ - ['equivalent']
06/22/2022 05:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/22/2022 05:24:57 - INFO - __main__ - ['equivalent']
06/22/2022 05:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/22/2022 05:24:57 - INFO - __main__ - ['equivalent']
06/22/2022 05:24:57 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:24:57 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:24:57 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:24:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:24:57 - INFO - __main__ - Printing 3 examples
06/22/2022 05:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/22/2022 05:24:57 - INFO - __main__ - ['equivalent']
06/22/2022 05:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/22/2022 05:24:57 - INFO - __main__ - ['equivalent']
06/22/2022 05:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/22/2022 05:24:57 - INFO - __main__ - ['equivalent']
06/22/2022 05:24:57 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:24:57 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:24:57 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:24:57 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 05:25:03 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:25:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:25:03 - INFO - __main__ - Starting training!
06/22/2022 05:25:06 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_87_0.5_8_predictions.txt
06/22/2022 05:25:06 - INFO - __main__ - ACC on test data: 0.5221
06/22/2022 05:25:06 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.5, bsz=8, dev_performance=0.59375, test_performance=0.5220588235294118
06/22/2022 05:25:06 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.4, bsz=8 ...
06/22/2022 05:25:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:25:07 - INFO - __main__ - Printing 3 examples
06/22/2022 05:25:07 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/22/2022 05:25:07 - INFO - __main__ - ['equivalent']
06/22/2022 05:25:07 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/22/2022 05:25:07 - INFO - __main__ - ['equivalent']
06/22/2022 05:25:07 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/22/2022 05:25:07 - INFO - __main__ - ['equivalent']
06/22/2022 05:25:07 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:25:07 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:25:07 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:25:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:25:07 - INFO - __main__ - Printing 3 examples
06/22/2022 05:25:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/22/2022 05:25:07 - INFO - __main__ - ['equivalent']
06/22/2022 05:25:07 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/22/2022 05:25:07 - INFO - __main__ - ['equivalent']
06/22/2022 05:25:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/22/2022 05:25:07 - INFO - __main__ - ['equivalent']
06/22/2022 05:25:07 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:25:07 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:25:07 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:25:12 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:25:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:25:12 - INFO - __main__ - Starting training!
06/22/2022 05:25:14 - INFO - __main__ - Step 10 Global step 10 Train loss 3.34 on epoch=4
06/22/2022 05:25:15 - INFO - __main__ - Step 20 Global step 20 Train loss 2.36 on epoch=9
06/22/2022 05:25:16 - INFO - __main__ - Step 30 Global step 30 Train loss 1.61 on epoch=14
06/22/2022 05:25:18 - INFO - __main__ - Step 40 Global step 40 Train loss 1.16 on epoch=19
06/22/2022 05:25:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.87 on epoch=24
06/22/2022 05:25:19 - INFO - __main__ - Global step 50 Train loss 1.87 ACC 0.625 on epoch=24
06/22/2022 05:25:19 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.625 on epoch=24, global_step=50
06/22/2022 05:25:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.66 on epoch=29
06/22/2022 05:25:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=34
06/22/2022 05:25:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=39
06/22/2022 05:25:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=44
06/22/2022 05:25:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=49
06/22/2022 05:25:26 - INFO - __main__ - Global step 100 Train loss 0.51 ACC 0.5 on epoch=49
06/22/2022 05:25:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=54
06/22/2022 05:25:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.35 on epoch=59
06/22/2022 05:25:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=64
06/22/2022 05:25:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.31 on epoch=69
06/22/2022 05:25:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.32 on epoch=74
06/22/2022 05:25:33 - INFO - __main__ - Global step 150 Train loss 0.33 ACC 0.4375 on epoch=74
06/22/2022 05:25:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
06/22/2022 05:25:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=84
06/22/2022 05:25:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
06/22/2022 05:25:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
06/22/2022 05:25:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
06/22/2022 05:25:40 - INFO - __main__ - Global step 200 Train loss 0.31 ACC 0.46875 on epoch=99
06/22/2022 05:25:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=104
06/22/2022 05:25:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.30 on epoch=109
06/22/2022 05:25:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=114
06/22/2022 05:25:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
06/22/2022 05:25:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=124
06/22/2022 05:25:46 - INFO - __main__ - Global step 250 Train loss 0.28 ACC 0.375 on epoch=124
06/22/2022 05:25:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.21 on epoch=129
06/22/2022 05:25:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.20 on epoch=134
06/22/2022 05:25:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=139
06/22/2022 05:25:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.18 on epoch=144
06/22/2022 05:25:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.14 on epoch=149
06/22/2022 05:25:53 - INFO - __main__ - Global step 300 Train loss 0.19 ACC 0.40625 on epoch=149
06/22/2022 05:25:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.14 on epoch=154
06/22/2022 05:25:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.14 on epoch=159
06/22/2022 05:25:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.13 on epoch=164
06/22/2022 05:25:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.11 on epoch=169
06/22/2022 05:25:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.16 on epoch=174
06/22/2022 05:26:00 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.46875 on epoch=174
06/22/2022 05:26:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.06 on epoch=179
06/22/2022 05:26:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.06 on epoch=184
06/22/2022 05:26:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.07 on epoch=189
06/22/2022 05:26:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.06 on epoch=194
06/22/2022 05:26:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.08 on epoch=199
06/22/2022 05:26:07 - INFO - __main__ - Global step 400 Train loss 0.07 ACC 0.5 on epoch=199
06/22/2022 05:26:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
06/22/2022 05:26:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.05 on epoch=209
06/22/2022 05:26:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.05 on epoch=214
06/22/2022 05:26:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.06 on epoch=219
06/22/2022 05:26:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.08 on epoch=224
06/22/2022 05:26:13 - INFO - __main__ - Global step 450 Train loss 0.07 ACC 0.4375 on epoch=224
06/22/2022 05:26:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=229
06/22/2022 05:26:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.06 on epoch=234
06/22/2022 05:26:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.08 on epoch=239
06/22/2022 05:26:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
06/22/2022 05:26:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.02 on epoch=249
06/22/2022 05:26:20 - INFO - __main__ - Global step 500 Train loss 0.06 ACC 0.40625 on epoch=249
06/22/2022 05:26:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
06/22/2022 05:26:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
06/22/2022 05:26:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
06/22/2022 05:26:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=269
06/22/2022 05:26:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
06/22/2022 05:26:27 - INFO - __main__ - Global step 550 Train loss 0.04 ACC 0.4375 on epoch=274
06/22/2022 05:26:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=279
06/22/2022 05:26:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
06/22/2022 05:26:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
06/22/2022 05:26:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
06/22/2022 05:26:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
06/22/2022 05:26:34 - INFO - __main__ - Global step 600 Train loss 0.04 ACC 0.4375 on epoch=299
06/22/2022 05:26:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
06/22/2022 05:26:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
06/22/2022 05:26:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/22/2022 05:26:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
06/22/2022 05:26:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/22/2022 05:26:40 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.5 on epoch=324
06/22/2022 05:26:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
06/22/2022 05:26:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
06/22/2022 05:26:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=339
06/22/2022 05:26:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
06/22/2022 05:26:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
06/22/2022 05:26:47 - INFO - __main__ - Global step 700 Train loss 0.02 ACC 0.46875 on epoch=349
06/22/2022 05:26:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
06/22/2022 05:26:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
06/22/2022 05:26:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
06/22/2022 05:26:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
06/22/2022 05:26:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
06/22/2022 05:26:54 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.4375 on epoch=374
06/22/2022 05:26:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
06/22/2022 05:26:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=384
06/22/2022 05:26:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
06/22/2022 05:26:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/22/2022 05:27:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
06/22/2022 05:27:01 - INFO - __main__ - Global step 800 Train loss 0.05 ACC 0.5 on epoch=399
06/22/2022 05:27:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
06/22/2022 05:27:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
06/22/2022 05:27:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
06/22/2022 05:27:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
06/22/2022 05:27:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/22/2022 05:27:07 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.53125 on epoch=424
06/22/2022 05:27:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/22/2022 05:27:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
06/22/2022 05:27:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
06/22/2022 05:27:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=444
06/22/2022 05:27:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/22/2022 05:27:14 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.4375 on epoch=449
06/22/2022 05:27:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/22/2022 05:27:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/22/2022 05:27:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/22/2022 05:27:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/22/2022 05:27:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
06/22/2022 05:27:21 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.53125 on epoch=474
06/22/2022 05:27:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/22/2022 05:27:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/22/2022 05:27:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
06/22/2022 05:27:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
06/22/2022 05:27:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/22/2022 05:27:28 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.53125 on epoch=499
06/22/2022 05:27:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/22/2022 05:27:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/22/2022 05:27:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/22/2022 05:27:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/22/2022 05:27:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/22/2022 05:27:34 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.53125 on epoch=524
06/22/2022 05:27:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/22/2022 05:27:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
06/22/2022 05:27:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/22/2022 05:27:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
06/22/2022 05:27:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=549
06/22/2022 05:27:41 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.53125 on epoch=549
06/22/2022 05:27:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 05:27:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
06/22/2022 05:27:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 05:27:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=569
06/22/2022 05:27:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/22/2022 05:27:48 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.4375 on epoch=574
06/22/2022 05:27:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/22/2022 05:27:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/22/2022 05:27:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/22/2022 05:27:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 05:27:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/22/2022 05:27:55 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.46875 on epoch=599
06/22/2022 05:27:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/22/2022 05:27:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/22/2022 05:27:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 05:27:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
06/22/2022 05:28:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 05:28:01 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.53125 on epoch=624
06/22/2022 05:28:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
06/22/2022 05:28:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 05:28:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
06/22/2022 05:28:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/22/2022 05:28:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 05:28:08 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.53125 on epoch=649
06/22/2022 05:28:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/22/2022 05:28:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 05:28:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 05:28:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 05:28:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/22/2022 05:28:15 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.53125 on epoch=674
06/22/2022 05:28:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/22/2022 05:28:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/22/2022 05:28:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
06/22/2022 05:28:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 05:28:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/22/2022 05:28:21 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.5625 on epoch=699
06/22/2022 05:28:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 05:28:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 05:28:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 05:28:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 05:28:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/22/2022 05:28:28 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.53125 on epoch=724
06/22/2022 05:28:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 05:28:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 05:28:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 05:28:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 05:28:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 05:28:35 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.53125 on epoch=749
06/22/2022 05:28:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 05:28:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 05:28:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 05:28:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 05:28:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 05:28:42 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.59375 on epoch=774
06/22/2022 05:28:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 05:28:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=784
06/22/2022 05:28:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 05:28:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 05:28:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 05:28:48 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.59375 on epoch=799
06/22/2022 05:28:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 05:28:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 05:28:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/22/2022 05:28:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 05:28:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 05:28:55 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.46875 on epoch=824
06/22/2022 05:28:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 05:28:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 05:28:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 05:29:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 05:29:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/22/2022 05:29:02 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.59375 on epoch=849
06/22/2022 05:29:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 05:29:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 05:29:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 05:29:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 05:29:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/22/2022 05:29:09 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.53125 on epoch=874
06/22/2022 05:29:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 05:29:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 05:29:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 05:29:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 05:29:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 05:29:15 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.59375 on epoch=899
06/22/2022 05:29:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
06/22/2022 05:29:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 05:29:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 05:29:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 05:29:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
06/22/2022 05:29:22 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.53125 on epoch=924
06/22/2022 05:29:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/22/2022 05:29:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
06/22/2022 05:29:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 05:29:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 05:29:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 05:29:29 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.53125 on epoch=949
06/22/2022 05:29:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 05:29:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 05:29:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 05:29:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=969
06/22/2022 05:29:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 05:29:36 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.5 on epoch=974
06/22/2022 05:29:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 05:29:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 05:29:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 05:29:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 05:29:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 05:29:42 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.53125 on epoch=999
06/22/2022 05:29:42 - INFO - __main__ - save last model!
06/22/2022 05:29:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 05:29:42 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 05:29:42 - INFO - __main__ - Printing 3 examples
06/22/2022 05:29:42 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 05:29:42 - INFO - __main__ - ['equivalent']
06/22/2022 05:29:42 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 05:29:42 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:29:42 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 05:29:42 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:29:42 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:29:43 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:29:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:29:43 - INFO - __main__ - Printing 3 examples
06/22/2022 05:29:43 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/22/2022 05:29:43 - INFO - __main__ - ['equivalent']
06/22/2022 05:29:43 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/22/2022 05:29:43 - INFO - __main__ - ['equivalent']
06/22/2022 05:29:43 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/22/2022 05:29:43 - INFO - __main__ - ['equivalent']
06/22/2022 05:29:43 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:29:43 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:29:43 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:29:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:29:43 - INFO - __main__ - Printing 3 examples
06/22/2022 05:29:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/22/2022 05:29:43 - INFO - __main__ - ['equivalent']
06/22/2022 05:29:43 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/22/2022 05:29:43 - INFO - __main__ - ['equivalent']
06/22/2022 05:29:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/22/2022 05:29:43 - INFO - __main__ - ['equivalent']
06/22/2022 05:29:43 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:29:43 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:29:43 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:29:43 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 05:29:48 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:29:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:29:49 - INFO - __main__ - Starting training!
06/22/2022 05:29:51 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_87_0.4_8_predictions.txt
06/22/2022 05:29:51 - INFO - __main__ - ACC on test data: 0.5196
06/22/2022 05:29:51 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.4, bsz=8, dev_performance=0.625, test_performance=0.5196078431372549
06/22/2022 05:29:51 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.3, bsz=8 ...
06/22/2022 05:29:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:29:52 - INFO - __main__ - Printing 3 examples
06/22/2022 05:29:52 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/22/2022 05:29:52 - INFO - __main__ - ['equivalent']
06/22/2022 05:29:52 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/22/2022 05:29:52 - INFO - __main__ - ['equivalent']
06/22/2022 05:29:52 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/22/2022 05:29:52 - INFO - __main__ - ['equivalent']
06/22/2022 05:29:52 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:29:52 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:29:52 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:29:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:29:52 - INFO - __main__ - Printing 3 examples
06/22/2022 05:29:52 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/22/2022 05:29:52 - INFO - __main__ - ['equivalent']
06/22/2022 05:29:52 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/22/2022 05:29:52 - INFO - __main__ - ['equivalent']
06/22/2022 05:29:52 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/22/2022 05:29:52 - INFO - __main__ - ['equivalent']
06/22/2022 05:29:52 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:29:52 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:29:52 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:29:57 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:29:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:29:58 - INFO - __main__ - Starting training!
06/22/2022 05:29:59 - INFO - __main__ - Step 10 Global step 10 Train loss 3.38 on epoch=4
06/22/2022 05:30:00 - INFO - __main__ - Step 20 Global step 20 Train loss 2.59 on epoch=9
06/22/2022 05:30:02 - INFO - __main__ - Step 30 Global step 30 Train loss 1.83 on epoch=14
06/22/2022 05:30:03 - INFO - __main__ - Step 40 Global step 40 Train loss 1.58 on epoch=19
06/22/2022 05:30:04 - INFO - __main__ - Step 50 Global step 50 Train loss 1.22 on epoch=24
06/22/2022 05:30:05 - INFO - __main__ - Global step 50 Train loss 2.12 ACC 0.09375 on epoch=24
06/22/2022 05:30:05 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.09375 on epoch=24, global_step=50
06/22/2022 05:30:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.91 on epoch=29
06/22/2022 05:30:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.76 on epoch=34
06/22/2022 05:30:09 - INFO - __main__ - Step 80 Global step 80 Train loss 0.65 on epoch=39
06/22/2022 05:30:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=44
06/22/2022 05:30:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=49
06/22/2022 05:30:12 - INFO - __main__ - Global step 100 Train loss 0.67 ACC 0.5 on epoch=49
06/22/2022 05:30:12 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.5 on epoch=49, global_step=100
06/22/2022 05:30:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=54
06/22/2022 05:30:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=59
06/22/2022 05:30:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=64
06/22/2022 05:30:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.37 on epoch=69
06/22/2022 05:30:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
06/22/2022 05:30:18 - INFO - __main__ - Global step 150 Train loss 0.41 ACC 0.53125 on epoch=74
06/22/2022 05:30:18 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=74, global_step=150
06/22/2022 05:30:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.35 on epoch=79
06/22/2022 05:30:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=84
06/22/2022 05:30:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
06/22/2022 05:30:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
06/22/2022 05:30:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
06/22/2022 05:30:25 - INFO - __main__ - Global step 200 Train loss 0.33 ACC 0.4375 on epoch=99
06/22/2022 05:30:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=104
06/22/2022 05:30:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=109
06/22/2022 05:30:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=114
06/22/2022 05:30:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=119
06/22/2022 05:30:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=124
06/22/2022 05:30:32 - INFO - __main__ - Global step 250 Train loss 0.30 ACC 0.53125 on epoch=124
06/22/2022 05:30:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=129
06/22/2022 05:30:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=134
06/22/2022 05:30:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.31 on epoch=139
06/22/2022 05:30:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=144
06/22/2022 05:30:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=149
06/22/2022 05:30:38 - INFO - __main__ - Global step 300 Train loss 0.33 ACC 0.5 on epoch=149
06/22/2022 05:30:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
06/22/2022 05:30:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
06/22/2022 05:30:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=164
06/22/2022 05:30:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
06/22/2022 05:30:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
06/22/2022 05:30:45 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.53125 on epoch=174
06/22/2022 05:30:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
06/22/2022 05:30:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=184
06/22/2022 05:30:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=189
06/22/2022 05:30:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
06/22/2022 05:30:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=199
06/22/2022 05:30:52 - INFO - __main__ - Global step 400 Train loss 0.23 ACC 0.5625 on epoch=199
06/22/2022 05:30:52 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=199, global_step=400
06/22/2022 05:30:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=204
06/22/2022 05:30:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.17 on epoch=209
06/22/2022 05:30:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.16 on epoch=214
06/22/2022 05:30:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.15 on epoch=219
06/22/2022 05:30:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=224
06/22/2022 05:30:59 - INFO - __main__ - Global step 450 Train loss 0.16 ACC 0.46875 on epoch=224
06/22/2022 05:31:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.17 on epoch=229
06/22/2022 05:31:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.09 on epoch=234
06/22/2022 05:31:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.12 on epoch=239
06/22/2022 05:31:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=244
06/22/2022 05:31:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=249
06/22/2022 05:31:05 - INFO - __main__ - Global step 500 Train loss 0.12 ACC 0.40625 on epoch=249
06/22/2022 05:31:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
06/22/2022 05:31:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=259
06/22/2022 05:31:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=264
06/22/2022 05:31:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.08 on epoch=269
06/22/2022 05:31:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
06/22/2022 05:31:12 - INFO - __main__ - Global step 550 Train loss 0.08 ACC 0.46875 on epoch=274
06/22/2022 05:31:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
06/22/2022 05:31:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.04 on epoch=284
06/22/2022 05:31:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.03 on epoch=289
06/22/2022 05:31:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
06/22/2022 05:31:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
06/22/2022 05:31:19 - INFO - __main__ - Global step 600 Train loss 0.05 ACC 0.53125 on epoch=299
06/22/2022 05:31:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=304
06/22/2022 05:31:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
06/22/2022 05:31:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=314
06/22/2022 05:31:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
06/22/2022 05:31:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.08 on epoch=324
06/22/2022 05:31:25 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.53125 on epoch=324
06/22/2022 05:31:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
06/22/2022 05:31:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
06/22/2022 05:31:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
06/22/2022 05:31:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=344
06/22/2022 05:31:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
06/22/2022 05:31:32 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.53125 on epoch=349
06/22/2022 05:31:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
06/22/2022 05:31:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
06/22/2022 05:31:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=364
06/22/2022 05:31:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
06/22/2022 05:31:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
06/22/2022 05:31:39 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.5 on epoch=374
06/22/2022 05:31:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/22/2022 05:31:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
06/22/2022 05:31:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
06/22/2022 05:31:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
06/22/2022 05:31:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
06/22/2022 05:31:46 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.5 on epoch=399
06/22/2022 05:31:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
06/22/2022 05:31:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
06/22/2022 05:31:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/22/2022 05:31:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
06/22/2022 05:31:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/22/2022 05:31:52 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.53125 on epoch=424
06/22/2022 05:31:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/22/2022 05:31:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
06/22/2022 05:31:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
06/22/2022 05:31:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
06/22/2022 05:31:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
06/22/2022 05:31:59 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.53125 on epoch=449
06/22/2022 05:32:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
06/22/2022 05:32:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/22/2022 05:32:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/22/2022 05:32:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/22/2022 05:32:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/22/2022 05:32:06 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.53125 on epoch=474
06/22/2022 05:32:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/22/2022 05:32:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
06/22/2022 05:32:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/22/2022 05:32:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/22/2022 05:32:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
06/22/2022 05:32:12 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.53125 on epoch=499
06/22/2022 05:32:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/22/2022 05:32:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/22/2022 05:32:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
06/22/2022 05:32:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
06/22/2022 05:32:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
06/22/2022 05:32:19 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.53125 on epoch=524
06/22/2022 05:32:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
06/22/2022 05:32:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
06/22/2022 05:32:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/22/2022 05:32:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/22/2022 05:32:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
06/22/2022 05:32:26 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.53125 on epoch=549
06/22/2022 05:32:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
06/22/2022 05:32:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
06/22/2022 05:32:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 05:32:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
06/22/2022 05:32:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/22/2022 05:32:32 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.46875 on epoch=574
06/22/2022 05:32:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/22/2022 05:32:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/22/2022 05:32:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
06/22/2022 05:32:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
06/22/2022 05:32:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/22/2022 05:32:39 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.5 on epoch=599
06/22/2022 05:32:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=604
06/22/2022 05:32:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=609
06/22/2022 05:32:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/22/2022 05:32:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
06/22/2022 05:32:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
06/22/2022 05:32:46 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.46875 on epoch=624
06/22/2022 05:32:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/22/2022 05:32:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/22/2022 05:32:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/22/2022 05:32:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 05:32:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 05:32:53 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.53125 on epoch=649
06/22/2022 05:32:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 05:32:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 05:32:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 05:32:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 05:32:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/22/2022 05:32:59 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.5 on epoch=674
06/22/2022 05:33:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/22/2022 05:33:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 05:33:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 05:33:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/22/2022 05:33:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=699
06/22/2022 05:33:06 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.53125 on epoch=699
06/22/2022 05:33:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 05:33:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/22/2022 05:33:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 05:33:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/22/2022 05:33:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 05:33:13 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.46875 on epoch=724
06/22/2022 05:33:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=729
06/22/2022 05:33:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 05:33:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 05:33:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/22/2022 05:33:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
06/22/2022 05:33:19 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.5 on epoch=749
06/22/2022 05:33:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
06/22/2022 05:33:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
06/22/2022 05:33:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 05:33:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 05:33:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 05:33:26 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.5 on epoch=774
06/22/2022 05:33:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 05:33:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/22/2022 05:33:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 05:33:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/22/2022 05:33:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
06/22/2022 05:33:33 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.46875 on epoch=799
06/22/2022 05:33:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
06/22/2022 05:33:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 05:33:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
06/22/2022 05:33:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
06/22/2022 05:33:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 05:33:39 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.46875 on epoch=824
06/22/2022 05:33:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/22/2022 05:33:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/22/2022 05:33:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 05:33:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 05:33:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
06/22/2022 05:33:46 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.46875 on epoch=849
06/22/2022 05:33:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
06/22/2022 05:33:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 05:33:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 05:33:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 05:33:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 05:33:53 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.5 on epoch=874
06/22/2022 05:33:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 05:33:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
06/22/2022 05:33:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 05:33:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 05:33:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/22/2022 05:34:00 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.5 on epoch=899
06/22/2022 05:34:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/22/2022 05:34:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 05:34:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 05:34:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
06/22/2022 05:34:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 05:34:06 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.5 on epoch=924
06/22/2022 05:34:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=929
06/22/2022 05:34:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/22/2022 05:34:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 05:34:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=944
06/22/2022 05:34:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 05:34:13 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.5 on epoch=949
06/22/2022 05:34:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 05:34:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 05:34:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 05:34:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 05:34:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 05:34:20 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.5 on epoch=974
06/22/2022 05:34:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
06/22/2022 05:34:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 05:34:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/22/2022 05:34:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 05:34:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 05:34:26 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.46875 on epoch=999
06/22/2022 05:34:26 - INFO - __main__ - save last model!
06/22/2022 05:34:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 05:34:26 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 05:34:26 - INFO - __main__ - Printing 3 examples
06/22/2022 05:34:26 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 05:34:26 - INFO - __main__ - ['equivalent']
06/22/2022 05:34:26 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 05:34:26 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:34:26 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 05:34:26 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:34:26 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:34:27 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:34:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:34:27 - INFO - __main__ - Printing 3 examples
06/22/2022 05:34:27 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/22/2022 05:34:27 - INFO - __main__ - ['equivalent']
06/22/2022 05:34:27 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/22/2022 05:34:27 - INFO - __main__ - ['equivalent']
06/22/2022 05:34:27 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/22/2022 05:34:27 - INFO - __main__ - ['equivalent']
06/22/2022 05:34:27 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:34:27 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:34:27 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:34:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:34:27 - INFO - __main__ - Printing 3 examples
06/22/2022 05:34:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/22/2022 05:34:27 - INFO - __main__ - ['equivalent']
06/22/2022 05:34:27 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/22/2022 05:34:27 - INFO - __main__ - ['equivalent']
06/22/2022 05:34:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/22/2022 05:34:27 - INFO - __main__ - ['equivalent']
06/22/2022 05:34:27 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:34:27 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:34:27 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:34:27 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 05:34:32 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:34:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:34:32 - INFO - __main__ - Starting training!
06/22/2022 05:34:35 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_87_0.3_8_predictions.txt
06/22/2022 05:34:35 - INFO - __main__ - ACC on test data: 0.5613
06/22/2022 05:34:35 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.3, bsz=8, dev_performance=0.5625, test_performance=0.5612745098039216
06/22/2022 05:34:35 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.2, bsz=8 ...
06/22/2022 05:34:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:34:36 - INFO - __main__ - Printing 3 examples
06/22/2022 05:34:36 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/22/2022 05:34:36 - INFO - __main__ - ['equivalent']
06/22/2022 05:34:36 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/22/2022 05:34:36 - INFO - __main__ - ['equivalent']
06/22/2022 05:34:36 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/22/2022 05:34:36 - INFO - __main__ - ['equivalent']
06/22/2022 05:34:36 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:34:36 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:34:36 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 05:34:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 05:34:36 - INFO - __main__ - Printing 3 examples
06/22/2022 05:34:36 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/22/2022 05:34:36 - INFO - __main__ - ['equivalent']
06/22/2022 05:34:36 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/22/2022 05:34:36 - INFO - __main__ - ['equivalent']
06/22/2022 05:34:36 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/22/2022 05:34:36 - INFO - __main__ - ['equivalent']
06/22/2022 05:34:36 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:34:36 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:34:36 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 05:34:41 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 05:34:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 05:34:42 - INFO - __main__ - Starting training!
06/22/2022 05:34:43 - INFO - __main__ - Step 10 Global step 10 Train loss 3.52 on epoch=4
06/22/2022 05:34:44 - INFO - __main__ - Step 20 Global step 20 Train loss 3.03 on epoch=9
06/22/2022 05:34:46 - INFO - __main__ - Step 30 Global step 30 Train loss 2.47 on epoch=14
06/22/2022 05:34:47 - INFO - __main__ - Step 40 Global step 40 Train loss 2.02 on epoch=19
06/22/2022 05:34:48 - INFO - __main__ - Step 50 Global step 50 Train loss 1.74 on epoch=24
06/22/2022 05:34:49 - INFO - __main__ - Global step 50 Train loss 2.56 ACC 0.0 on epoch=24
06/22/2022 05:34:49 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/22/2022 05:34:50 - INFO - __main__ - Step 60 Global step 60 Train loss 1.52 on epoch=29
06/22/2022 05:34:51 - INFO - __main__ - Step 70 Global step 70 Train loss 1.23 on epoch=34
06/22/2022 05:34:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=39
06/22/2022 05:34:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.92 on epoch=44
06/22/2022 05:34:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.70 on epoch=49
06/22/2022 05:34:55 - INFO - __main__ - Global step 100 Train loss 1.06 ACC 0.59375 on epoch=49
06/22/2022 05:34:55 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.59375 on epoch=49, global_step=100
06/22/2022 05:34:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.65 on epoch=54
06/22/2022 05:34:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=59
06/22/2022 05:34:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=64
06/22/2022 05:35:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=69
06/22/2022 05:35:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=74
06/22/2022 05:35:02 - INFO - __main__ - Global step 150 Train loss 0.55 ACC 0.46875 on epoch=74
06/22/2022 05:35:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=79
06/22/2022 05:35:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=84
06/22/2022 05:35:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
06/22/2022 05:35:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.36 on epoch=94
06/22/2022 05:35:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=99
06/22/2022 05:35:09 - INFO - __main__ - Global step 200 Train loss 0.41 ACC 0.53125 on epoch=99
06/22/2022 05:35:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=104
06/22/2022 05:35:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=109
06/22/2022 05:35:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
06/22/2022 05:35:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.33 on epoch=119
06/22/2022 05:35:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=124
06/22/2022 05:35:16 - INFO - __main__ - Global step 250 Train loss 0.38 ACC 0.46875 on epoch=124
06/22/2022 05:35:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=129
06/22/2022 05:35:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=134
06/22/2022 05:35:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.34 on epoch=139
06/22/2022 05:35:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.30 on epoch=144
06/22/2022 05:35:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.34 on epoch=149
06/22/2022 05:35:22 - INFO - __main__ - Global step 300 Train loss 0.32 ACC 0.5 on epoch=149
06/22/2022 05:35:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=154
06/22/2022 05:35:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.29 on epoch=159
06/22/2022 05:35:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=164
06/22/2022 05:35:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
06/22/2022 05:35:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.29 on epoch=174
06/22/2022 05:35:29 - INFO - __main__ - Global step 350 Train loss 0.30 ACC 0.46875 on epoch=174
06/22/2022 05:35:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
06/22/2022 05:35:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=184
06/22/2022 05:35:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.27 on epoch=189
06/22/2022 05:35:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=194
06/22/2022 05:35:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.31 on epoch=199
06/22/2022 05:35:36 - INFO - __main__ - Global step 400 Train loss 0.30 ACC 0.3125 on epoch=199
06/22/2022 05:35:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
06/22/2022 05:35:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=209
06/22/2022 05:35:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=214
06/22/2022 05:35:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
06/22/2022 05:35:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/22/2022 05:35:42 - INFO - __main__ - Global step 450 Train loss 0.25 ACC 0.53125 on epoch=224
06/22/2022 05:35:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=229
06/22/2022 05:35:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=234
06/22/2022 05:35:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=239
06/22/2022 05:35:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=244
06/22/2022 05:35:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
06/22/2022 05:35:49 - INFO - __main__ - Global step 500 Train loss 0.26 ACC 0.5 on epoch=249
06/22/2022 05:35:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
06/22/2022 05:35:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
06/22/2022 05:35:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
06/22/2022 05:35:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=269
06/22/2022 05:35:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=274
06/22/2022 05:35:56 - INFO - __main__ - Global step 550 Train loss 0.21 ACC 0.375 on epoch=274
06/22/2022 05:35:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.18 on epoch=279
06/22/2022 05:35:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.17 on epoch=284
06/22/2022 05:36:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=289
06/22/2022 05:36:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=294
06/22/2022 05:36:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=299
06/22/2022 05:36:03 - INFO - __main__ - Global step 600 Train loss 0.16 ACC 0.46875 on epoch=299
06/22/2022 05:36:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=304
06/22/2022 05:36:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=309
06/22/2022 05:36:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
06/22/2022 05:36:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.13 on epoch=319
06/22/2022 05:36:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=324
06/22/2022 05:36:10 - INFO - __main__ - Global step 650 Train loss 0.15 ACC 0.5 on epoch=324
06/22/2022 05:36:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=329
06/22/2022 05:36:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=334
06/22/2022 05:36:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=339
06/22/2022 05:36:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=344
06/22/2022 05:36:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=349
06/22/2022 05:36:16 - INFO - __main__ - Global step 700 Train loss 0.13 ACC 0.46875 on epoch=349
06/22/2022 05:36:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=354
06/22/2022 05:36:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=359
06/22/2022 05:36:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=364
06/22/2022 05:36:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=369
06/22/2022 05:36:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=374
06/22/2022 05:36:23 - INFO - __main__ - Global step 750 Train loss 0.09 ACC 0.5 on epoch=374
06/22/2022 05:36:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=379
06/22/2022 05:36:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
06/22/2022 05:36:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
06/22/2022 05:36:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
06/22/2022 05:36:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
06/22/2022 05:36:30 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.5 on epoch=399
06/22/2022 05:36:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
06/22/2022 05:36:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=409
06/22/2022 05:36:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=414
06/22/2022 05:36:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
06/22/2022 05:36:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/22/2022 05:36:36 - INFO - __main__ - Global step 850 Train loss 0.06 ACC 0.53125 on epoch=424
06/22/2022 05:36:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=429
06/22/2022 05:36:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
06/22/2022 05:36:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=439
06/22/2022 05:36:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
06/22/2022 05:36:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=449
06/22/2022 05:36:43 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.46875 on epoch=449
06/22/2022 05:36:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
06/22/2022 05:36:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=459
06/22/2022 05:36:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=464
06/22/2022 05:36:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
06/22/2022 05:36:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
06/22/2022 05:36:50 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.53125 on epoch=474
06/22/2022 05:36:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
06/22/2022 05:36:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/22/2022 05:36:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
06/22/2022 05:36:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
06/22/2022 05:36:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
06/22/2022 05:36:57 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.5 on epoch=499
06/22/2022 05:36:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
06/22/2022 05:36:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
06/22/2022 05:37:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
06/22/2022 05:37:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=519
06/22/2022 05:37:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
06/22/2022 05:37:03 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.5 on epoch=524
06/22/2022 05:37:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/22/2022 05:37:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
06/22/2022 05:37:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
06/22/2022 05:37:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=544
06/22/2022 05:37:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=549
06/22/2022 05:37:10 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.5 on epoch=549
06/22/2022 05:37:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
06/22/2022 05:37:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=559
06/22/2022 05:37:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=564
06/22/2022 05:37:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
06/22/2022 05:37:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
06/22/2022 05:37:17 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.53125 on epoch=574
06/22/2022 05:37:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
06/22/2022 05:37:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
06/22/2022 05:37:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
06/22/2022 05:37:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
06/22/2022 05:37:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
06/22/2022 05:37:24 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.53125 on epoch=599
06/22/2022 05:37:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
06/22/2022 05:37:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
06/22/2022 05:37:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
06/22/2022 05:37:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/22/2022 05:37:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/22/2022 05:37:30 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.53125 on epoch=624
06/22/2022 05:37:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
06/22/2022 05:37:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/22/2022 05:37:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/22/2022 05:37:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/22/2022 05:37:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
06/22/2022 05:37:37 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.5625 on epoch=649
06/22/2022 05:37:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/22/2022 05:37:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
06/22/2022 05:37:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/22/2022 05:37:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
06/22/2022 05:37:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
06/22/2022 05:37:44 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.5625 on epoch=674
06/22/2022 05:37:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
06/22/2022 05:37:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/22/2022 05:37:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
06/22/2022 05:37:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
06/22/2022 05:37:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/22/2022 05:37:50 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.53125 on epoch=699
06/22/2022 05:37:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 05:37:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
06/22/2022 05:37:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/22/2022 05:37:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/22/2022 05:37:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/22/2022 05:37:57 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.53125 on epoch=724
06/22/2022 05:37:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
06/22/2022 05:38:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 05:38:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
06/22/2022 05:38:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/22/2022 05:38:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
06/22/2022 05:38:04 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.5 on epoch=749
06/22/2022 05:38:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/22/2022 05:38:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 05:38:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
06/22/2022 05:38:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/22/2022 05:38:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/22/2022 05:38:11 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.53125 on epoch=774
06/22/2022 05:38:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/22/2022 05:38:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/22/2022 05:38:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/22/2022 05:38:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
06/22/2022 05:38:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
06/22/2022 05:38:17 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.53125 on epoch=799
06/22/2022 05:38:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/22/2022 05:38:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/22/2022 05:38:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/22/2022 05:38:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
06/22/2022 05:38:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
06/22/2022 05:38:24 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.53125 on epoch=824
06/22/2022 05:38:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/22/2022 05:38:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/22/2022 05:38:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 05:38:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/22/2022 05:38:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
06/22/2022 05:38:31 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.53125 on epoch=849
06/22/2022 05:38:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/22/2022 05:38:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=859
06/22/2022 05:38:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/22/2022 05:38:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 05:38:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/22/2022 05:38:38 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.53125 on epoch=874
06/22/2022 05:38:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/22/2022 05:38:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
06/22/2022 05:38:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
06/22/2022 05:38:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/22/2022 05:38:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 05:38:44 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.5 on epoch=899
06/22/2022 05:38:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=904
06/22/2022 05:38:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=909
06/22/2022 05:38:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 05:38:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 05:38:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
06/22/2022 05:38:51 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.53125 on epoch=924
06/22/2022 05:38:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/22/2022 05:38:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=934
06/22/2022 05:38:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/22/2022 05:38:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/22/2022 05:38:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 05:38:58 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.53125 on epoch=949
06/22/2022 05:38:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
06/22/2022 05:39:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/22/2022 05:39:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
06/22/2022 05:39:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/22/2022 05:39:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
06/22/2022 05:39:05 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.5 on epoch=974
06/22/2022 05:39:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
06/22/2022 05:39:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/22/2022 05:39:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/22/2022 05:39:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 05:39:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=999
06/22/2022 05:39:11 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.46875 on epoch=999
06/22/2022 05:39:11 - INFO - __main__ - save last model!
06/22/2022 05:39:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 05:39:11 - INFO - __main__ - Start tokenizing ... 408 instances
06/22/2022 05:39:11 - INFO - __main__ - Printing 3 examples
06/22/2022 05:39:11 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/22/2022 05:39:11 - INFO - __main__ - ['equivalent']
06/22/2022 05:39:11 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/22/2022 05:39:11 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:39:11 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/22/2022 05:39:11 - INFO - __main__ - ['not_equivalent']
06/22/2022 05:39:11 - INFO - __main__ - Tokenizing Input ...
06/22/2022 05:39:12 - INFO - __main__ - Tokenizing Output ...
06/22/2022 05:39:12 - INFO - __main__ - Loaded 408 examples from test data
06/22/2022 05:39:20 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_87_0.2_8_predictions.txt
06/22/2022 05:39:20 - INFO - __main__ - ACC on test data: 0.5637
06/22/2022 05:39:20 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.2, bsz=8, dev_performance=0.59375, test_performance=0.5637254901960784
06/24/2022 04:44:29 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-multitask-nopara2para-5e-1-4-20', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-nopara2para-5e-1-4-20-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='4,5')
06/24/2022 04:44:29 - INFO - __main__ - models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc
06/24/2022 04:44:29 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-multitask-nopara2para-5e-1-4-20', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-nopara2para-5e-1-4-20-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='4,5')
06/24/2022 04:44:29 - INFO - __main__ - models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc
06/24/2022 04:44:31 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/24/2022 04:44:31 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/24/2022 04:44:31 - INFO - __main__ - args.device: cuda:0
06/24/2022 04:44:31 - INFO - __main__ - Using 2 gpus
06/24/2022 04:44:31 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/24/2022 04:44:31 - INFO - __main__ - args.device: cuda:1
06/24/2022 04:44:31 - INFO - __main__ - Using 2 gpus
06/24/2022 04:44:31 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/24/2022 04:44:35 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.5, bsz=8 ...
06/24/2022 04:44:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:44:36 - INFO - __main__ - Printing 3 examples
06/24/2022 04:44:36 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 04:44:36 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:44:36 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 04:44:36 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:44:36 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 04:44:36 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:44:36 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:44:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:44:36 - INFO - __main__ - Printing 3 examples
06/24/2022 04:44:36 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 04:44:36 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:44:36 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 04:44:36 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:44:36 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 04:44:36 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:44:36 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:44:36 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:44:36 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:44:36 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 04:44:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:44:36 - INFO - __main__ - Printing 3 examples
06/24/2022 04:44:36 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 04:44:36 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:44:36 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 04:44:36 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:44:36 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 04:44:36 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:44:36 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:44:36 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 04:44:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:44:36 - INFO - __main__ - Printing 3 examples
06/24/2022 04:44:36 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 04:44:36 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:44:36 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 04:44:36 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:44:36 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 04:44:36 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:44:36 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:44:36 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:44:36 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:44:36 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 04:44:36 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 04:44:42 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 04:44:42 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 04:44:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 04:44:42 - INFO - __main__ - Starting training!
06/24/2022 04:44:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 04:44:48 - INFO - __main__ - Starting training!
06/24/2022 04:44:50 - INFO - __main__ - Step 10 Global step 10 Train loss 2.97 on epoch=4
06/24/2022 04:44:51 - INFO - __main__ - Step 20 Global step 20 Train loss 1.79 on epoch=9
06/24/2022 04:44:52 - INFO - __main__ - Step 30 Global step 30 Train loss 1.15 on epoch=14
06/24/2022 04:44:54 - INFO - __main__ - Step 40 Global step 40 Train loss 0.75 on epoch=19
06/24/2022 04:44:55 - INFO - __main__ - Step 50 Global step 50 Train loss 0.62 on epoch=24
06/24/2022 04:44:55 - INFO - __main__ - Global step 50 Train loss 1.46 ACC 0.46875 on epoch=24
06/24/2022 04:44:55 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.46875 on epoch=24, global_step=50
06/24/2022 04:44:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.52 on epoch=29
06/24/2022 04:44:58 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=34
06/24/2022 04:44:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.36 on epoch=39
06/24/2022 04:45:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.33 on epoch=44
06/24/2022 04:45:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.32 on epoch=49
06/24/2022 04:45:02 - INFO - __main__ - Global step 100 Train loss 0.39 ACC 0.46875 on epoch=49
06/24/2022 04:45:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.33 on epoch=54
06/24/2022 04:45:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=59
06/24/2022 04:45:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=64
06/24/2022 04:45:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
06/24/2022 04:45:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=74
06/24/2022 04:45:09 - INFO - __main__ - Global step 150 Train loss 0.28 ACC 0.53125 on epoch=74
06/24/2022 04:45:09 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.53125 on epoch=74, global_step=150
06/24/2022 04:45:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=79
06/24/2022 04:45:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
06/24/2022 04:45:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=89
06/24/2022 04:45:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
06/24/2022 04:45:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
06/24/2022 04:45:16 - INFO - __main__ - Global step 200 Train loss 0.28 ACC 0.5625 on epoch=99
06/24/2022 04:45:16 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=99, global_step=200
06/24/2022 04:45:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
06/24/2022 04:45:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=109
06/24/2022 04:45:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.20 on epoch=114
06/24/2022 04:45:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
06/24/2022 04:45:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.20 on epoch=124
06/24/2022 04:45:23 - INFO - __main__ - Global step 250 Train loss 0.23 ACC 0.46875 on epoch=124
06/24/2022 04:45:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
06/24/2022 04:45:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.21 on epoch=134
06/24/2022 04:45:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=139
06/24/2022 04:45:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.17 on epoch=144
06/24/2022 04:45:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=149
06/24/2022 04:45:29 - INFO - __main__ - Global step 300 Train loss 0.21 ACC 0.59375 on epoch=149
06/24/2022 04:45:29 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=149, global_step=300
06/24/2022 04:45:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.18 on epoch=154
06/24/2022 04:45:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.19 on epoch=159
06/24/2022 04:45:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.15 on epoch=164
06/24/2022 04:45:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.16 on epoch=169
06/24/2022 04:45:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.19 on epoch=174
06/24/2022 04:45:36 - INFO - __main__ - Global step 350 Train loss 0.18 ACC 0.53125 on epoch=174
06/24/2022 04:45:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.19 on epoch=179
06/24/2022 04:45:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.18 on epoch=184
06/24/2022 04:45:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.16 on epoch=189
06/24/2022 04:45:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.17 on epoch=194
06/24/2022 04:45:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.14 on epoch=199
06/24/2022 04:45:43 - INFO - __main__ - Global step 400 Train loss 0.17 ACC 0.6875 on epoch=199
06/24/2022 04:45:43 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.6875 on epoch=199, global_step=400
06/24/2022 04:45:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
06/24/2022 04:45:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=209
06/24/2022 04:45:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
06/24/2022 04:45:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.12 on epoch=219
06/24/2022 04:45:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=224
06/24/2022 04:45:50 - INFO - __main__ - Global step 450 Train loss 0.13 ACC 0.5625 on epoch=224
06/24/2022 04:45:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.10 on epoch=229
06/24/2022 04:45:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.16 on epoch=234
06/24/2022 04:45:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.12 on epoch=239
06/24/2022 04:45:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.09 on epoch=244
06/24/2022 04:45:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=249
06/24/2022 04:45:56 - INFO - __main__ - Global step 500 Train loss 0.12 ACC 0.71875 on epoch=249
06/24/2022 04:45:56 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=249, global_step=500
06/24/2022 04:45:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=254
06/24/2022 04:45:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.09 on epoch=259
06/24/2022 04:46:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
06/24/2022 04:46:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.07 on epoch=269
06/24/2022 04:46:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=274
06/24/2022 04:46:03 - INFO - __main__ - Global step 550 Train loss 0.09 ACC 0.75 on epoch=274
06/24/2022 04:46:03 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.75 on epoch=274, global_step=550
06/24/2022 04:46:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
06/24/2022 04:46:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=284
06/24/2022 04:46:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
06/24/2022 04:46:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
06/24/2022 04:46:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
06/24/2022 04:46:10 - INFO - __main__ - Global step 600 Train loss 0.05 ACC 0.71875 on epoch=299
06/24/2022 04:46:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
06/24/2022 04:46:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
06/24/2022 04:46:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/24/2022 04:46:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
06/24/2022 04:46:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
06/24/2022 04:46:17 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.75 on epoch=324
06/24/2022 04:46:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
06/24/2022 04:46:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
06/24/2022 04:46:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
06/24/2022 04:46:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
06/24/2022 04:46:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
06/24/2022 04:46:23 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.75 on epoch=349
06/24/2022 04:46:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
06/24/2022 04:46:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
06/24/2022 04:46:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
06/24/2022 04:46:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/24/2022 04:46:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
06/24/2022 04:46:30 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.6875 on epoch=374
06/24/2022 04:46:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/24/2022 04:46:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/24/2022 04:46:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
06/24/2022 04:46:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/24/2022 04:46:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
06/24/2022 04:46:37 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.75 on epoch=399
06/24/2022 04:46:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
06/24/2022 04:46:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/24/2022 04:46:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
06/24/2022 04:46:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
06/24/2022 04:46:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
06/24/2022 04:46:44 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.65625 on epoch=424
06/24/2022 04:46:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/24/2022 04:46:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
06/24/2022 04:46:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
06/24/2022 04:46:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
06/24/2022 04:46:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/24/2022 04:46:50 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.78125 on epoch=449
06/24/2022 04:46:50 - INFO - __main__ - Saving model with best ACC: 0.75 -> 0.78125 on epoch=449, global_step=900
06/24/2022 04:46:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/24/2022 04:46:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/24/2022 04:46:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/24/2022 04:46:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/24/2022 04:46:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
06/24/2022 04:46:57 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.625 on epoch=474
06/24/2022 04:46:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/24/2022 04:47:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
06/24/2022 04:47:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/24/2022 04:47:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/24/2022 04:47:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/24/2022 04:47:04 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.65625 on epoch=499
06/24/2022 04:47:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/24/2022 04:47:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/24/2022 04:47:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/24/2022 04:47:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/24/2022 04:47:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/24/2022 04:47:10 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.78125 on epoch=524
06/24/2022 04:47:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/24/2022 04:47:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/24/2022 04:47:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
06/24/2022 04:47:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/24/2022 04:47:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/24/2022 04:47:17 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.71875 on epoch=549
06/24/2022 04:47:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 04:47:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/24/2022 04:47:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/24/2022 04:47:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/24/2022 04:47:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/24/2022 04:47:24 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.71875 on epoch=574
06/24/2022 04:47:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/24/2022 04:47:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/24/2022 04:47:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/24/2022 04:47:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/24/2022 04:47:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/24/2022 04:47:31 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.71875 on epoch=599
06/24/2022 04:47:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/24/2022 04:47:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/24/2022 04:47:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 04:47:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/24/2022 04:47:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 04:47:37 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.71875 on epoch=624
06/24/2022 04:47:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 04:47:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 04:47:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 04:47:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/24/2022 04:47:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 04:47:44 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.71875 on epoch=649
06/24/2022 04:47:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 04:47:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 04:47:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 04:47:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 04:47:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/24/2022 04:47:51 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.65625 on epoch=674
06/24/2022 04:47:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 04:47:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 04:47:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 04:47:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 04:47:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 04:47:57 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.71875 on epoch=699
06/24/2022 04:47:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 04:48:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 04:48:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/24/2022 04:48:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 04:48:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/24/2022 04:48:04 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.65625 on epoch=724
06/24/2022 04:48:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 04:48:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 04:48:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
06/24/2022 04:48:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 04:48:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
06/24/2022 04:48:11 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.75 on epoch=749
06/24/2022 04:48:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 04:48:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 04:48:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
06/24/2022 04:48:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 04:48:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 04:48:17 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.71875 on epoch=774
06/24/2022 04:48:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 04:48:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 04:48:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 04:48:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 04:48:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=799
06/24/2022 04:48:24 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.75 on epoch=799
06/24/2022 04:48:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 04:48:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 04:48:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 04:48:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 04:48:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 04:48:31 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.6875 on epoch=824
06/24/2022 04:48:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 04:48:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 04:48:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/24/2022 04:48:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 04:48:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 04:48:38 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.71875 on epoch=849
06/24/2022 04:48:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 04:48:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/24/2022 04:48:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 04:48:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 04:48:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 04:48:44 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.75 on epoch=874
06/24/2022 04:48:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 04:48:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 04:48:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 04:48:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 04:48:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 04:48:51 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.75 on epoch=899
06/24/2022 04:48:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 04:48:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 04:48:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/24/2022 04:48:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 04:48:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 04:48:58 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.78125 on epoch=924
06/24/2022 04:48:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 04:49:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 04:49:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 04:49:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 04:49:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 04:49:04 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.78125 on epoch=949
06/24/2022 04:49:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 04:49:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 04:49:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 04:49:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 04:49:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 04:49:11 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.78125 on epoch=974
06/24/2022 04:49:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 04:49:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 04:49:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
06/24/2022 04:49:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 04:49:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 04:49:18 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.65625 on epoch=999
06/24/2022 04:49:18 - INFO - __main__ - save last model!
06/24/2022 04:49:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 04:49:18 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 04:49:18 - INFO - __main__ - Printing 3 examples
06/24/2022 04:49:18 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 04:49:18 - INFO - __main__ - ['equivalent']
06/24/2022 04:49:18 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 04:49:18 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:18 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 04:49:18 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:49:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:49:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:49:18 - INFO - __main__ - Printing 3 examples
06/24/2022 04:49:18 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 04:49:18 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:18 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 04:49:18 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:18 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 04:49:18 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:49:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:49:18 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 04:49:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:49:18 - INFO - __main__ - Printing 3 examples
06/24/2022 04:49:18 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 04:49:18 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:18 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 04:49:18 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:18 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 04:49:18 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:49:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:49:18 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 04:49:18 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 04:49:24 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 04:49:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 04:49:24 - INFO - __main__ - Starting training!
06/24/2022 04:49:26 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_100_0.5_8_predictions.txt
06/24/2022 04:49:26 - INFO - __main__ - ACC on test data: 0.6373
06/24/2022 04:49:26 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.5, bsz=8, dev_performance=0.78125, test_performance=0.6372549019607843
06/24/2022 04:49:26 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.4, bsz=8 ...
06/24/2022 04:49:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:49:27 - INFO - __main__ - Printing 3 examples
06/24/2022 04:49:27 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 04:49:27 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 04:49:27 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:27 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 04:49:27 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:49:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:49:27 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 04:49:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:49:27 - INFO - __main__ - Printing 3 examples
06/24/2022 04:49:27 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 04:49:27 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:27 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 04:49:27 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 04:49:27 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:49:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:49:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:49:27 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 04:49:33 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 04:49:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 04:49:34 - INFO - __main__ - Starting training!
06/24/2022 04:49:35 - INFO - __main__ - Step 10 Global step 10 Train loss 3.21 on epoch=4
06/24/2022 04:49:36 - INFO - __main__ - Step 20 Global step 20 Train loss 2.15 on epoch=9
06/24/2022 04:49:38 - INFO - __main__ - Step 30 Global step 30 Train loss 1.49 on epoch=14
06/24/2022 04:49:39 - INFO - __main__ - Step 40 Global step 40 Train loss 1.05 on epoch=19
06/24/2022 04:49:40 - INFO - __main__ - Step 50 Global step 50 Train loss 0.78 on epoch=24
06/24/2022 04:49:41 - INFO - __main__ - Global step 50 Train loss 1.74 ACC 0.46875 on epoch=24
06/24/2022 04:49:41 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.46875 on epoch=24, global_step=50
06/24/2022 04:49:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.70 on epoch=29
06/24/2022 04:49:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=34
06/24/2022 04:49:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=39
06/24/2022 04:49:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=44
06/24/2022 04:49:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=49
06/24/2022 04:49:48 - INFO - __main__ - Global step 100 Train loss 0.51 ACC 0.4375 on epoch=49
06/24/2022 04:49:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=54
06/24/2022 04:49:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.35 on epoch=59
06/24/2022 04:49:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=64
06/24/2022 04:49:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.33 on epoch=69
06/24/2022 04:49:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.29 on epoch=74
06/24/2022 04:49:55 - INFO - __main__ - Global step 150 Train loss 0.34 ACC 0.53125 on epoch=74
06/24/2022 04:49:55 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.53125 on epoch=74, global_step=150
06/24/2022 04:49:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.35 on epoch=79
06/24/2022 04:49:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
06/24/2022 04:49:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=89
06/24/2022 04:50:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=94
06/24/2022 04:50:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.30 on epoch=99
06/24/2022 04:50:01 - INFO - __main__ - Global step 200 Train loss 0.31 ACC 0.4375 on epoch=99
06/24/2022 04:50:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=104
06/24/2022 04:50:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.32 on epoch=109
06/24/2022 04:50:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=114
06/24/2022 04:50:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=119
06/24/2022 04:50:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=124
06/24/2022 04:50:08 - INFO - __main__ - Global step 250 Train loss 0.28 ACC 0.46875 on epoch=124
06/24/2022 04:50:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=129
06/24/2022 04:50:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.28 on epoch=134
06/24/2022 04:50:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=139
06/24/2022 04:50:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=144
06/24/2022 04:50:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.18 on epoch=149
06/24/2022 04:50:15 - INFO - __main__ - Global step 300 Train loss 0.24 ACC 0.5 on epoch=149
06/24/2022 04:50:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
06/24/2022 04:50:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
06/24/2022 04:50:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=164
06/24/2022 04:50:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=169
06/24/2022 04:50:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.20 on epoch=174
06/24/2022 04:50:22 - INFO - __main__ - Global step 350 Train loss 0.21 ACC 0.5 on epoch=174
06/24/2022 04:50:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.20 on epoch=179
06/24/2022 04:50:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.17 on epoch=184
06/24/2022 04:50:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=189
06/24/2022 04:50:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.18 on epoch=194
06/24/2022 04:50:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
06/24/2022 04:50:29 - INFO - __main__ - Global step 400 Train loss 0.20 ACC 0.59375 on epoch=199
06/24/2022 04:50:29 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=199, global_step=400
06/24/2022 04:50:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=204
06/24/2022 04:50:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=209
06/24/2022 04:50:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.18 on epoch=214
06/24/2022 04:50:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.14 on epoch=219
06/24/2022 04:50:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=224
06/24/2022 04:50:36 - INFO - __main__ - Global step 450 Train loss 0.17 ACC 0.5625 on epoch=224
06/24/2022 04:50:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.18 on epoch=229
06/24/2022 04:50:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.12 on epoch=234
06/24/2022 04:50:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.17 on epoch=239
06/24/2022 04:50:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.12 on epoch=244
06/24/2022 04:50:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.14 on epoch=249
06/24/2022 04:50:42 - INFO - __main__ - Global step 500 Train loss 0.15 ACC 0.71875 on epoch=249
06/24/2022 04:50:42 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.71875 on epoch=249, global_step=500
06/24/2022 04:50:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=254
06/24/2022 04:50:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.12 on epoch=259
06/24/2022 04:50:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=264
06/24/2022 04:50:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=269
06/24/2022 04:50:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.12 on epoch=274
06/24/2022 04:50:49 - INFO - __main__ - Global step 550 Train loss 0.12 ACC 0.65625 on epoch=274
06/24/2022 04:50:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=279
06/24/2022 04:50:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=284
06/24/2022 04:50:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
06/24/2022 04:50:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.07 on epoch=294
06/24/2022 04:50:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.08 on epoch=299
06/24/2022 04:50:56 - INFO - __main__ - Global step 600 Train loss 0.08 ACC 0.6875 on epoch=299
06/24/2022 04:50:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=304
06/24/2022 04:50:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=309
06/24/2022 04:51:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=314
06/24/2022 04:51:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
06/24/2022 04:51:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=324
06/24/2022 04:51:03 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.71875 on epoch=324
06/24/2022 04:51:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
06/24/2022 04:51:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
06/24/2022 04:51:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=339
06/24/2022 04:51:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
06/24/2022 04:51:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
06/24/2022 04:51:10 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.6875 on epoch=349
06/24/2022 04:51:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=354
06/24/2022 04:51:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
06/24/2022 04:51:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=364
06/24/2022 04:51:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
06/24/2022 04:51:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
06/24/2022 04:51:17 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.65625 on epoch=374
06/24/2022 04:51:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/24/2022 04:51:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/24/2022 04:51:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
06/24/2022 04:51:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/24/2022 04:51:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
06/24/2022 04:51:24 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.75 on epoch=399
06/24/2022 04:51:24 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.75 on epoch=399, global_step=800
06/24/2022 04:51:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
06/24/2022 04:51:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
06/24/2022 04:51:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/24/2022 04:51:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
06/24/2022 04:51:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/24/2022 04:51:30 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.6875 on epoch=424
06/24/2022 04:51:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/24/2022 04:51:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/24/2022 04:51:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
06/24/2022 04:51:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
06/24/2022 04:51:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/24/2022 04:51:37 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.6875 on epoch=449
06/24/2022 04:51:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/24/2022 04:51:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/24/2022 04:51:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/24/2022 04:51:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/24/2022 04:51:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
06/24/2022 04:51:44 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.65625 on epoch=474
06/24/2022 04:51:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/24/2022 04:51:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/24/2022 04:51:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/24/2022 04:51:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/24/2022 04:51:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
06/24/2022 04:51:51 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.6875 on epoch=499
06/24/2022 04:51:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/24/2022 04:51:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/24/2022 04:51:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/24/2022 04:51:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/24/2022 04:51:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/24/2022 04:51:58 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.75 on epoch=524
06/24/2022 04:51:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/24/2022 04:52:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/24/2022 04:52:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
06/24/2022 04:52:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
06/24/2022 04:52:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/24/2022 04:52:05 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.65625 on epoch=549
06/24/2022 04:52:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 04:52:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/24/2022 04:52:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/24/2022 04:52:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/24/2022 04:52:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/24/2022 04:52:11 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.65625 on epoch=574
06/24/2022 04:52:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/24/2022 04:52:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/24/2022 04:52:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/24/2022 04:52:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 04:52:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/24/2022 04:52:18 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.625 on epoch=599
06/24/2022 04:52:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/24/2022 04:52:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/24/2022 04:52:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 04:52:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/24/2022 04:52:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/24/2022 04:52:25 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.75 on epoch=624
06/24/2022 04:52:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/24/2022 04:52:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
06/24/2022 04:52:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 04:52:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 04:52:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 04:52:32 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.8125 on epoch=649
06/24/2022 04:52:32 - INFO - __main__ - Saving model with best ACC: 0.75 -> 0.8125 on epoch=649, global_step=1300
06/24/2022 04:52:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 04:52:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/24/2022 04:52:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 04:52:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 04:52:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 04:52:39 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.75 on epoch=674
06/24/2022 04:52:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 04:52:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 04:52:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 04:52:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 04:52:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 04:52:46 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.6875 on epoch=699
06/24/2022 04:52:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 04:52:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 04:52:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 04:52:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 04:52:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/24/2022 04:52:52 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.78125 on epoch=724
06/24/2022 04:52:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 04:52:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 04:52:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 04:52:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 04:52:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 04:52:59 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.6875 on epoch=749
06/24/2022 04:53:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 04:53:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 04:53:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 04:53:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 04:53:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/24/2022 04:53:06 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.71875 on epoch=774
06/24/2022 04:53:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 04:53:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 04:53:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/24/2022 04:53:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/24/2022 04:53:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 04:53:13 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.65625 on epoch=799
06/24/2022 04:53:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/24/2022 04:53:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
06/24/2022 04:53:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 04:53:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 04:53:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 04:53:20 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.6875 on epoch=824
06/24/2022 04:53:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 04:53:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 04:53:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 04:53:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 04:53:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 04:53:26 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.65625 on epoch=849
06/24/2022 04:53:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 04:53:29 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 04:53:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 04:53:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 04:53:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 04:53:33 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.6875 on epoch=874
06/24/2022 04:53:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 04:53:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 04:53:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 04:53:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 04:53:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 04:53:40 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.625 on epoch=899
06/24/2022 04:53:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 04:53:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 04:53:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 04:53:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 04:53:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 04:53:47 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.65625 on epoch=924
06/24/2022 04:53:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 04:53:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/24/2022 04:53:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 04:53:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 04:53:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 04:53:54 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.625 on epoch=949
06/24/2022 04:53:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 04:53:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/24/2022 04:53:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/24/2022 04:53:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 04:54:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 04:54:01 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.71875 on epoch=974
06/24/2022 04:54:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 04:54:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 04:54:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 04:54:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 04:54:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 04:54:07 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.75 on epoch=999
06/24/2022 04:54:07 - INFO - __main__ - save last model!
06/24/2022 04:54:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 04:54:07 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 04:54:07 - INFO - __main__ - Printing 3 examples
06/24/2022 04:54:07 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 04:54:07 - INFO - __main__ - ['equivalent']
06/24/2022 04:54:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 04:54:07 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:07 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 04:54:07 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:54:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:54:08 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 04:54:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:54:08 - INFO - __main__ - Printing 3 examples
06/24/2022 04:54:08 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 04:54:08 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:08 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 04:54:08 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:08 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 04:54:08 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:54:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:54:09 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 04:54:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:54:09 - INFO - __main__ - Printing 3 examples
06/24/2022 04:54:09 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 04:54:09 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:09 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 04:54:09 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:09 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 04:54:09 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:09 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:54:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:54:09 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 04:54:15 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 04:54:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 04:54:15 - INFO - __main__ - Starting training!
06/24/2022 04:54:16 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_100_0.4_8_predictions.txt
06/24/2022 04:54:16 - INFO - __main__ - ACC on test data: 0.5858
06/24/2022 04:54:16 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.4, bsz=8, dev_performance=0.8125, test_performance=0.5857843137254902
06/24/2022 04:54:16 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.3, bsz=8 ...
06/24/2022 04:54:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:54:17 - INFO - __main__ - Printing 3 examples
06/24/2022 04:54:17 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 04:54:17 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:17 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 04:54:17 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:17 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 04:54:17 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:17 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:54:17 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:54:17 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 04:54:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:54:17 - INFO - __main__ - Printing 3 examples
06/24/2022 04:54:17 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 04:54:17 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:17 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 04:54:17 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:17 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 04:54:17 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:54:17 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:54:17 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:54:17 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 04:54:23 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 04:54:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 04:54:23 - INFO - __main__ - Starting training!
06/24/2022 04:54:25 - INFO - __main__ - Step 10 Global step 10 Train loss 3.28 on epoch=4
06/24/2022 04:54:26 - INFO - __main__ - Step 20 Global step 20 Train loss 2.36 on epoch=9
06/24/2022 04:54:28 - INFO - __main__ - Step 30 Global step 30 Train loss 1.90 on epoch=14
06/24/2022 04:54:29 - INFO - __main__ - Step 40 Global step 40 Train loss 1.44 on epoch=19
06/24/2022 04:54:30 - INFO - __main__ - Step 50 Global step 50 Train loss 1.12 on epoch=24
06/24/2022 04:54:31 - INFO - __main__ - Global step 50 Train loss 2.02 ACC 0.15625 on epoch=24
06/24/2022 04:54:31 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
06/24/2022 04:54:32 - INFO - __main__ - Step 60 Global step 60 Train loss 0.87 on epoch=29
06/24/2022 04:54:33 - INFO - __main__ - Step 70 Global step 70 Train loss 0.69 on epoch=34
06/24/2022 04:54:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=39
06/24/2022 04:54:36 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=44
06/24/2022 04:54:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=49
06/24/2022 04:54:38 - INFO - __main__ - Global step 100 Train loss 0.64 ACC 0.53125 on epoch=49
06/24/2022 04:54:38 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.53125 on epoch=49, global_step=100
06/24/2022 04:54:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=54
06/24/2022 04:54:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=59
06/24/2022 04:54:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=64
06/24/2022 04:54:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.38 on epoch=69
06/24/2022 04:54:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.33 on epoch=74
06/24/2022 04:54:45 - INFO - __main__ - Global step 150 Train loss 0.39 ACC 0.5 on epoch=74
06/24/2022 04:54:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=79
06/24/2022 04:54:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=84
06/24/2022 04:54:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.30 on epoch=89
06/24/2022 04:54:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.31 on epoch=94
06/24/2022 04:54:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.29 on epoch=99
06/24/2022 04:54:52 - INFO - __main__ - Global step 200 Train loss 0.34 ACC 0.59375 on epoch=99
06/24/2022 04:54:52 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=99, global_step=200
06/24/2022 04:54:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.30 on epoch=104
06/24/2022 04:54:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.31 on epoch=109
06/24/2022 04:54:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
06/24/2022 04:54:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.31 on epoch=119
06/24/2022 04:54:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=124
06/24/2022 04:54:59 - INFO - __main__ - Global step 250 Train loss 0.29 ACC 0.65625 on epoch=124
06/24/2022 04:54:59 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=124, global_step=250
06/24/2022 04:55:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=129
06/24/2022 04:55:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=134
06/24/2022 04:55:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.29 on epoch=139
06/24/2022 04:55:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.30 on epoch=144
06/24/2022 04:55:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
06/24/2022 04:55:06 - INFO - __main__ - Global step 300 Train loss 0.29 ACC 0.5625 on epoch=149
06/24/2022 04:55:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=154
06/24/2022 04:55:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
06/24/2022 04:55:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
06/24/2022 04:55:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=169
06/24/2022 04:55:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
06/24/2022 04:55:13 - INFO - __main__ - Global step 350 Train loss 0.24 ACC 0.4375 on epoch=174
06/24/2022 04:55:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
06/24/2022 04:55:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=184
06/24/2022 04:55:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.27 on epoch=189
06/24/2022 04:55:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=194
06/24/2022 04:55:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=199
06/24/2022 04:55:20 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.5 on epoch=199
06/24/2022 04:55:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.19 on epoch=204
06/24/2022 04:55:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
06/24/2022 04:55:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.20 on epoch=214
06/24/2022 04:55:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=219
06/24/2022 04:55:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=224
06/24/2022 04:55:27 - INFO - __main__ - Global step 450 Train loss 0.21 ACC 0.46875 on epoch=224
06/24/2022 04:55:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=229
06/24/2022 04:55:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=234
06/24/2022 04:55:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
06/24/2022 04:55:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
06/24/2022 04:55:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=249
06/24/2022 04:55:34 - INFO - __main__ - Global step 500 Train loss 0.22 ACC 0.625 on epoch=249
06/24/2022 04:55:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=254
06/24/2022 04:55:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=259
06/24/2022 04:55:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=264
06/24/2022 04:55:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=269
06/24/2022 04:55:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=274
06/24/2022 04:55:41 - INFO - __main__ - Global step 550 Train loss 0.17 ACC 0.625 on epoch=274
06/24/2022 04:55:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=279
06/24/2022 04:55:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=284
06/24/2022 04:55:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=289
06/24/2022 04:55:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.17 on epoch=294
06/24/2022 04:55:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=299
06/24/2022 04:55:48 - INFO - __main__ - Global step 600 Train loss 0.17 ACC 0.6875 on epoch=299
06/24/2022 04:55:48 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=299, global_step=600
06/24/2022 04:55:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=304
06/24/2022 04:55:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=309
06/24/2022 04:55:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.14 on epoch=314
06/24/2022 04:55:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=319
06/24/2022 04:55:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=324
06/24/2022 04:55:55 - INFO - __main__ - Global step 650 Train loss 0.14 ACC 0.59375 on epoch=324
06/24/2022 04:55:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=329
06/24/2022 04:55:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=334
06/24/2022 04:55:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=339
06/24/2022 04:56:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=344
06/24/2022 04:56:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=349
06/24/2022 04:56:02 - INFO - __main__ - Global step 700 Train loss 0.12 ACC 0.6875 on epoch=349
06/24/2022 04:56:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=354
06/24/2022 04:56:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=359
06/24/2022 04:56:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=364
06/24/2022 04:56:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=369
06/24/2022 04:56:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=374
06/24/2022 04:56:09 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.6875 on epoch=374
06/24/2022 04:56:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=379
06/24/2022 04:56:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=384
06/24/2022 04:56:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=389
06/24/2022 04:56:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=394
06/24/2022 04:56:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=399
06/24/2022 04:56:16 - INFO - __main__ - Global step 800 Train loss 0.07 ACC 0.71875 on epoch=399
06/24/2022 04:56:16 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=399, global_step=800
06/24/2022 04:56:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
06/24/2022 04:56:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
06/24/2022 04:56:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=414
06/24/2022 04:56:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
06/24/2022 04:56:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/24/2022 04:56:23 - INFO - __main__ - Global step 850 Train loss 0.05 ACC 0.6875 on epoch=424
06/24/2022 04:56:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
06/24/2022 04:56:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
06/24/2022 04:56:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
06/24/2022 04:56:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
06/24/2022 04:56:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
06/24/2022 04:56:30 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.78125 on epoch=449
06/24/2022 04:56:30 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.78125 on epoch=449, global_step=900
06/24/2022 04:56:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
06/24/2022 04:56:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
06/24/2022 04:56:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
06/24/2022 04:56:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/24/2022 04:56:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
06/24/2022 04:56:37 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.6875 on epoch=474
06/24/2022 04:56:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/24/2022 04:56:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/24/2022 04:56:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
06/24/2022 04:56:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/24/2022 04:56:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
06/24/2022 04:56:44 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.65625 on epoch=499
06/24/2022 04:56:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
06/24/2022 04:56:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
06/24/2022 04:56:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
06/24/2022 04:56:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/24/2022 04:56:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/24/2022 04:56:51 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.65625 on epoch=524
06/24/2022 04:56:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/24/2022 04:56:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/24/2022 04:56:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/24/2022 04:56:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
06/24/2022 04:56:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/24/2022 04:56:58 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.71875 on epoch=549
06/24/2022 04:56:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
06/24/2022 04:57:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/24/2022 04:57:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 04:57:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/24/2022 04:57:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/24/2022 04:57:05 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.6875 on epoch=574
06/24/2022 04:57:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/24/2022 04:57:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/24/2022 04:57:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/24/2022 04:57:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/24/2022 04:57:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/24/2022 04:57:12 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.71875 on epoch=599
06/24/2022 04:57:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/24/2022 04:57:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/24/2022 04:57:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/24/2022 04:57:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/24/2022 04:57:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/24/2022 04:57:19 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.6875 on epoch=624
06/24/2022 04:57:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/24/2022 04:57:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/24/2022 04:57:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/24/2022 04:57:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 04:57:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 04:57:25 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.6875 on epoch=649
06/24/2022 04:57:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/24/2022 04:57:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/24/2022 04:57:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 04:57:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
06/24/2022 04:57:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/24/2022 04:57:32 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.71875 on epoch=674
06/24/2022 04:57:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 04:57:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/24/2022 04:57:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/24/2022 04:57:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 04:57:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 04:57:39 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.625 on epoch=699
06/24/2022 04:57:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 04:57:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
06/24/2022 04:57:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 04:57:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 04:57:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 04:57:46 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.6875 on epoch=724
06/24/2022 04:57:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 04:57:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 04:57:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 04:57:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 04:57:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 04:57:53 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.6875 on epoch=749
06/24/2022 04:57:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 04:57:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 04:57:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/24/2022 04:57:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 04:58:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 04:58:00 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.75 on epoch=774
06/24/2022 04:58:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 04:58:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 04:58:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 04:58:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
06/24/2022 04:58:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 04:58:07 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.78125 on epoch=799
06/24/2022 04:58:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 04:58:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 04:58:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 04:58:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 04:58:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 04:58:14 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.75 on epoch=824
06/24/2022 04:58:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 04:58:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 04:58:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 04:58:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 04:58:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 04:58:21 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.78125 on epoch=849
06/24/2022 04:58:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 04:58:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/24/2022 04:58:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 04:58:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/24/2022 04:58:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 04:58:28 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.71875 on epoch=874
06/24/2022 04:58:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
06/24/2022 04:58:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 04:58:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=889
06/24/2022 04:58:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 04:58:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 04:58:35 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.78125 on epoch=899
06/24/2022 04:58:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 04:58:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 04:58:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 04:58:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=919
06/24/2022 04:58:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 04:58:42 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.75 on epoch=924
06/24/2022 04:58:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 04:58:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 04:58:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 04:58:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 04:58:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 04:58:49 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.78125 on epoch=949
06/24/2022 04:58:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 04:58:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 04:58:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 04:58:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 04:58:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 04:58:56 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.71875 on epoch=974
06/24/2022 04:58:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/24/2022 04:58:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 04:59:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/24/2022 04:59:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 04:59:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 04:59:03 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.71875 on epoch=999
06/24/2022 04:59:03 - INFO - __main__ - save last model!
06/24/2022 04:59:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 04:59:03 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 04:59:03 - INFO - __main__ - Printing 3 examples
06/24/2022 04:59:03 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 04:59:03 - INFO - __main__ - ['equivalent']
06/24/2022 04:59:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 04:59:03 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:03 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 04:59:03 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:59:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:59:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:59:03 - INFO - __main__ - Printing 3 examples
06/24/2022 04:59:03 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 04:59:03 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 04:59:03 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:03 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 04:59:03 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:59:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:59:03 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 04:59:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:59:03 - INFO - __main__ - Printing 3 examples
06/24/2022 04:59:03 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 04:59:03 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:03 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 04:59:03 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 04:59:03 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:59:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:59:03 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 04:59:03 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 04:59:09 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 04:59:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 04:59:10 - INFO - __main__ - Starting training!
06/24/2022 04:59:11 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_100_0.3_8_predictions.txt
06/24/2022 04:59:11 - INFO - __main__ - ACC on test data: 0.6201
06/24/2022 04:59:12 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.3, bsz=8, dev_performance=0.78125, test_performance=0.6200980392156863
06/24/2022 04:59:12 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.2, bsz=8 ...
06/24/2022 04:59:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:59:13 - INFO - __main__ - Printing 3 examples
06/24/2022 04:59:13 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 04:59:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 04:59:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:13 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 04:59:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:13 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:59:13 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:59:13 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 04:59:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:59:13 - INFO - __main__ - Printing 3 examples
06/24/2022 04:59:13 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 04:59:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:13 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 04:59:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 04:59:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 04:59:13 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:59:13 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:59:13 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 04:59:18 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 04:59:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 04:59:18 - INFO - __main__ - Starting training!
06/24/2022 04:59:20 - INFO - __main__ - Step 10 Global step 10 Train loss 3.37 on epoch=4
06/24/2022 04:59:21 - INFO - __main__ - Step 20 Global step 20 Train loss 2.82 on epoch=9
06/24/2022 04:59:23 - INFO - __main__ - Step 30 Global step 30 Train loss 2.28 on epoch=14
06/24/2022 04:59:24 - INFO - __main__ - Step 40 Global step 40 Train loss 1.94 on epoch=19
06/24/2022 04:59:25 - INFO - __main__ - Step 50 Global step 50 Train loss 1.56 on epoch=24
06/24/2022 04:59:26 - INFO - __main__ - Global step 50 Train loss 2.39 ACC 0.0 on epoch=24
06/24/2022 04:59:26 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 04:59:27 - INFO - __main__ - Step 60 Global step 60 Train loss 1.29 on epoch=29
06/24/2022 04:59:28 - INFO - __main__ - Step 70 Global step 70 Train loss 1.24 on epoch=34
06/24/2022 04:59:30 - INFO - __main__ - Step 80 Global step 80 Train loss 1.02 on epoch=39
06/24/2022 04:59:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=44
06/24/2022 04:59:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.68 on epoch=49
06/24/2022 04:59:33 - INFO - __main__ - Global step 100 Train loss 1.01 ACC 0.46875 on epoch=49
06/24/2022 04:59:33 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.46875 on epoch=49, global_step=100
06/24/2022 04:59:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.64 on epoch=54
06/24/2022 04:59:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.65 on epoch=59
06/24/2022 04:59:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.66 on epoch=64
06/24/2022 04:59:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=69
06/24/2022 04:59:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=74
06/24/2022 04:59:40 - INFO - __main__ - Global step 150 Train loss 0.60 ACC 0.5 on epoch=74
06/24/2022 04:59:40 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=74, global_step=150
06/24/2022 04:59:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=79
06/24/2022 04:59:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=84
06/24/2022 04:59:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=89
06/24/2022 04:59:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
06/24/2022 04:59:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=99
06/24/2022 04:59:47 - INFO - __main__ - Global step 200 Train loss 0.41 ACC 0.59375 on epoch=99
06/24/2022 04:59:47 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=99, global_step=200
06/24/2022 04:59:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=104
06/24/2022 04:59:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=109
06/24/2022 04:59:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
06/24/2022 04:59:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=119
06/24/2022 04:59:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=124
06/24/2022 04:59:54 - INFO - __main__ - Global step 250 Train loss 0.37 ACC 0.5625 on epoch=124
06/24/2022 04:59:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=129
06/24/2022 04:59:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=134
06/24/2022 04:59:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=139
06/24/2022 04:59:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=144
06/24/2022 05:00:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=149
06/24/2022 05:00:01 - INFO - __main__ - Global step 300 Train loss 0.33 ACC 0.53125 on epoch=149
06/24/2022 05:00:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
06/24/2022 05:00:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=159
06/24/2022 05:00:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=164
06/24/2022 05:00:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
06/24/2022 05:00:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=174
06/24/2022 05:00:07 - INFO - __main__ - Global step 350 Train loss 0.30 ACC 0.59375 on epoch=174
06/24/2022 05:00:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
06/24/2022 05:00:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=184
06/24/2022 05:00:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
06/24/2022 05:00:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=194
06/24/2022 05:00:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
06/24/2022 05:00:14 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.59375 on epoch=199
06/24/2022 05:00:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=204
06/24/2022 05:00:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
06/24/2022 05:00:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=214
06/24/2022 05:00:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=219
06/24/2022 05:00:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/24/2022 05:00:21 - INFO - __main__ - Global step 450 Train loss 0.27 ACC 0.59375 on epoch=224
06/24/2022 05:00:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=229
06/24/2022 05:00:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=234
06/24/2022 05:00:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=239
06/24/2022 05:00:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
06/24/2022 05:00:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
06/24/2022 05:00:28 - INFO - __main__ - Global step 500 Train loss 0.24 ACC 0.53125 on epoch=249
06/24/2022 05:00:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
06/24/2022 05:00:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
06/24/2022 05:00:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=264
06/24/2022 05:00:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=269
06/24/2022 05:00:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=274
06/24/2022 05:00:35 - INFO - __main__ - Global step 550 Train loss 0.23 ACC 0.53125 on epoch=274
06/24/2022 05:00:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=279
06/24/2022 05:00:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
06/24/2022 05:00:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=289
06/24/2022 05:00:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=294
06/24/2022 05:00:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=299
06/24/2022 05:00:42 - INFO - __main__ - Global step 600 Train loss 0.22 ACC 0.65625 on epoch=299
06/24/2022 05:00:42 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=299, global_step=600
06/24/2022 05:00:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=304
06/24/2022 05:00:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=309
06/24/2022 05:00:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
06/24/2022 05:00:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=319
06/24/2022 05:00:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=324
06/24/2022 05:00:49 - INFO - __main__ - Global step 650 Train loss 0.20 ACC 0.53125 on epoch=324
06/24/2022 05:00:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=329
06/24/2022 05:00:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=334
06/24/2022 05:00:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=339
06/24/2022 05:00:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=344
06/24/2022 05:00:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=349
06/24/2022 05:00:56 - INFO - __main__ - Global step 700 Train loss 0.18 ACC 0.5 on epoch=349
06/24/2022 05:00:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=354
06/24/2022 05:00:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=359
06/24/2022 05:01:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=364
06/24/2022 05:01:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=369
06/24/2022 05:01:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=374
06/24/2022 05:01:03 - INFO - __main__ - Global step 750 Train loss 0.16 ACC 0.5625 on epoch=374
06/24/2022 05:01:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.17 on epoch=379
06/24/2022 05:01:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.17 on epoch=384
06/24/2022 05:01:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=389
06/24/2022 05:01:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=394
06/24/2022 05:01:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=399
06/24/2022 05:01:10 - INFO - __main__ - Global step 800 Train loss 0.15 ACC 0.5625 on epoch=399
06/24/2022 05:01:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=404
06/24/2022 05:01:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=409
06/24/2022 05:01:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=414
06/24/2022 05:01:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=419
06/24/2022 05:01:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=424
06/24/2022 05:01:17 - INFO - __main__ - Global step 850 Train loss 0.12 ACC 0.53125 on epoch=424
06/24/2022 05:01:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=429
06/24/2022 05:01:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=434
06/24/2022 05:01:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=439
06/24/2022 05:01:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=444
06/24/2022 05:01:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=449
06/24/2022 05:01:24 - INFO - __main__ - Global step 900 Train loss 0.10 ACC 0.53125 on epoch=449
06/24/2022 05:01:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=454
06/24/2022 05:01:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=459
06/24/2022 05:01:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=464
06/24/2022 05:01:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
06/24/2022 05:01:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=474
06/24/2022 05:01:31 - INFO - __main__ - Global step 950 Train loss 0.08 ACC 0.53125 on epoch=474
06/24/2022 05:01:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
06/24/2022 05:01:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=484
06/24/2022 05:01:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=489
06/24/2022 05:01:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=494
06/24/2022 05:01:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=499
06/24/2022 05:01:38 - INFO - __main__ - Global step 1000 Train loss 0.07 ACC 0.5 on epoch=499
06/24/2022 05:01:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=504
06/24/2022 05:01:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
06/24/2022 05:01:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
06/24/2022 05:01:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
06/24/2022 05:01:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=524
06/24/2022 05:01:45 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.59375 on epoch=524
06/24/2022 05:01:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
06/24/2022 05:01:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=534
06/24/2022 05:01:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
06/24/2022 05:01:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=544
06/24/2022 05:01:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
06/24/2022 05:01:51 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.5625 on epoch=549
06/24/2022 05:01:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=554
06/24/2022 05:01:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
06/24/2022 05:01:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=564
06/24/2022 05:01:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=569
06/24/2022 05:01:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
06/24/2022 05:01:58 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.5625 on epoch=574
06/24/2022 05:02:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/24/2022 05:02:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
06/24/2022 05:02:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=589
06/24/2022 05:02:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
06/24/2022 05:02:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
06/24/2022 05:02:05 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.59375 on epoch=599
06/24/2022 05:02:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
06/24/2022 05:02:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
06/24/2022 05:02:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=614
06/24/2022 05:02:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/24/2022 05:02:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/24/2022 05:02:12 - INFO - __main__ - Global step 1250 Train loss 0.04 ACC 0.625 on epoch=624
06/24/2022 05:02:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
06/24/2022 05:02:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
06/24/2022 05:02:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
06/24/2022 05:02:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
06/24/2022 05:02:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=649
06/24/2022 05:02:19 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.59375 on epoch=649
06/24/2022 05:02:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
06/24/2022 05:02:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
06/24/2022 05:02:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=664
06/24/2022 05:02:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=669
06/24/2022 05:02:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/24/2022 05:02:26 - INFO - __main__ - Global step 1350 Train loss 0.04 ACC 0.46875 on epoch=674
06/24/2022 05:02:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/24/2022 05:02:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
06/24/2022 05:02:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
06/24/2022 05:02:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
06/24/2022 05:02:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/24/2022 05:02:33 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.59375 on epoch=699
06/24/2022 05:02:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 05:02:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
06/24/2022 05:02:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=714
06/24/2022 05:02:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/24/2022 05:02:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
06/24/2022 05:02:40 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.5625 on epoch=724
06/24/2022 05:02:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
06/24/2022 05:02:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
06/24/2022 05:02:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
06/24/2022 05:02:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/24/2022 05:02:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
06/24/2022 05:02:47 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.65625 on epoch=749
06/24/2022 05:02:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/24/2022 05:02:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
06/24/2022 05:02:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
06/24/2022 05:02:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/24/2022 05:02:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/24/2022 05:02:54 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.59375 on epoch=774
06/24/2022 05:02:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
06/24/2022 05:02:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=784
06/24/2022 05:02:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
06/24/2022 05:02:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/24/2022 05:03:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
06/24/2022 05:03:01 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.59375 on epoch=799
06/24/2022 05:03:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/24/2022 05:03:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/24/2022 05:03:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/24/2022 05:03:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/24/2022 05:03:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
06/24/2022 05:03:08 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.59375 on epoch=824
06/24/2022 05:03:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/24/2022 05:03:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
06/24/2022 05:03:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/24/2022 05:03:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=844
06/24/2022 05:03:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/24/2022 05:03:15 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.5625 on epoch=849
06/24/2022 05:03:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/24/2022 05:03:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
06/24/2022 05:03:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
06/24/2022 05:03:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/24/2022 05:03:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/24/2022 05:03:22 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.59375 on epoch=874
06/24/2022 05:03:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/24/2022 05:03:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
06/24/2022 05:03:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
06/24/2022 05:03:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/24/2022 05:03:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/24/2022 05:03:29 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.59375 on epoch=899
06/24/2022 05:03:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/24/2022 05:03:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/24/2022 05:03:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/24/2022 05:03:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/24/2022 05:03:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 05:03:36 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.5625 on epoch=924
06/24/2022 05:03:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=929
06/24/2022 05:03:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/24/2022 05:03:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 05:03:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 05:03:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
06/24/2022 05:03:42 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.5625 on epoch=949
06/24/2022 05:03:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/24/2022 05:03:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/24/2022 05:03:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/24/2022 05:03:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/24/2022 05:03:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 05:03:49 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.59375 on epoch=974
06/24/2022 05:03:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/24/2022 05:03:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
06/24/2022 05:03:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/24/2022 05:03:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/24/2022 05:03:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/24/2022 05:03:56 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.6875 on epoch=999
06/24/2022 05:03:56 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=999, global_step=2000
06/24/2022 05:03:56 - INFO - __main__ - save last model!
06/24/2022 05:03:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:03:56 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 05:03:56 - INFO - __main__ - Printing 3 examples
06/24/2022 05:03:56 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 05:03:56 - INFO - __main__ - ['equivalent']
06/24/2022 05:03:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 05:03:56 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:03:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 05:03:56 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:03:56 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:03:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:03:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:03:57 - INFO - __main__ - Printing 3 examples
06/24/2022 05:03:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 05:03:57 - INFO - __main__ - ['equivalent']
06/24/2022 05:03:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 05:03:57 - INFO - __main__ - ['equivalent']
06/24/2022 05:03:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 05:03:57 - INFO - __main__ - ['equivalent']
06/24/2022 05:03:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:03:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:03:57 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:03:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:03:57 - INFO - __main__ - Printing 3 examples
06/24/2022 05:03:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 05:03:57 - INFO - __main__ - ['equivalent']
06/24/2022 05:03:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 05:03:57 - INFO - __main__ - ['equivalent']
06/24/2022 05:03:57 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 05:03:57 - INFO - __main__ - ['equivalent']
06/24/2022 05:03:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:03:57 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 05:03:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:03:57 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:04:02 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:04:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:04:03 - INFO - __main__ - Starting training!
06/24/2022 05:04:05 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_100_0.2_8_predictions.txt
06/24/2022 05:04:05 - INFO - __main__ - ACC on test data: 0.6275
06/24/2022 05:04:05 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.2, bsz=8, dev_performance=0.6875, test_performance=0.6274509803921569
06/24/2022 05:04:05 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.5, bsz=8 ...
06/24/2022 05:04:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:04:06 - INFO - __main__ - Printing 3 examples
06/24/2022 05:04:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 05:04:06 - INFO - __main__ - ['equivalent']
06/24/2022 05:04:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 05:04:06 - INFO - __main__ - ['equivalent']
06/24/2022 05:04:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 05:04:06 - INFO - __main__ - ['equivalent']
06/24/2022 05:04:06 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:04:06 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:04:06 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:04:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:04:06 - INFO - __main__ - Printing 3 examples
06/24/2022 05:04:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 05:04:06 - INFO - __main__ - ['equivalent']
06/24/2022 05:04:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 05:04:06 - INFO - __main__ - ['equivalent']
06/24/2022 05:04:06 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 05:04:06 - INFO - __main__ - ['equivalent']
06/24/2022 05:04:06 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:04:06 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:04:06 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:04:12 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:04:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:04:12 - INFO - __main__ - Starting training!
06/24/2022 05:04:13 - INFO - __main__ - Step 10 Global step 10 Train loss 3.32 on epoch=4
06/24/2022 05:04:15 - INFO - __main__ - Step 20 Global step 20 Train loss 2.24 on epoch=9
06/24/2022 05:04:16 - INFO - __main__ - Step 30 Global step 30 Train loss 1.41 on epoch=14
06/24/2022 05:04:17 - INFO - __main__ - Step 40 Global step 40 Train loss 0.93 on epoch=19
06/24/2022 05:04:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.69 on epoch=24
06/24/2022 05:04:19 - INFO - __main__ - Global step 50 Train loss 1.72 ACC 0.5 on epoch=24
06/24/2022 05:04:19 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/24/2022 05:04:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.60 on epoch=29
06/24/2022 05:04:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.57 on epoch=34
06/24/2022 05:04:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=39
06/24/2022 05:04:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.37 on epoch=44
06/24/2022 05:04:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.34 on epoch=49
06/24/2022 05:04:26 - INFO - __main__ - Global step 100 Train loss 0.46 ACC 0.40625 on epoch=49
06/24/2022 05:04:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
06/24/2022 05:04:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
06/24/2022 05:04:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=64
06/24/2022 05:04:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.35 on epoch=69
06/24/2022 05:04:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.33 on epoch=74
06/24/2022 05:04:33 - INFO - __main__ - Global step 150 Train loss 0.36 ACC 0.5 on epoch=74
06/24/2022 05:04:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.35 on epoch=79
06/24/2022 05:04:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
06/24/2022 05:04:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.30 on epoch=89
06/24/2022 05:04:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
06/24/2022 05:04:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=99
06/24/2022 05:04:40 - INFO - __main__ - Global step 200 Train loss 0.30 ACC 0.5 on epoch=99
06/24/2022 05:04:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.34 on epoch=104
06/24/2022 05:04:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.32 on epoch=109
06/24/2022 05:04:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
06/24/2022 05:04:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
06/24/2022 05:04:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.33 on epoch=124
06/24/2022 05:04:47 - INFO - __main__ - Global step 250 Train loss 0.30 ACC 0.53125 on epoch=124
06/24/2022 05:04:47 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=124, global_step=250
06/24/2022 05:04:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=129
06/24/2022 05:04:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.24 on epoch=134
06/24/2022 05:04:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=139
06/24/2022 05:04:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=144
06/24/2022 05:04:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.28 on epoch=149
06/24/2022 05:04:54 - INFO - __main__ - Global step 300 Train loss 0.26 ACC 0.5 on epoch=149
06/24/2022 05:04:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=154
06/24/2022 05:04:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
06/24/2022 05:04:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
06/24/2022 05:04:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
06/24/2022 05:05:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
06/24/2022 05:05:01 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.5 on epoch=174
06/24/2022 05:05:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=179
06/24/2022 05:05:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=184
06/24/2022 05:05:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=189
06/24/2022 05:05:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=194
06/24/2022 05:05:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.18 on epoch=199
06/24/2022 05:05:08 - INFO - __main__ - Global step 400 Train loss 0.21 ACC 0.5625 on epoch=199
06/24/2022 05:05:08 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=199, global_step=400
06/24/2022 05:05:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.18 on epoch=204
06/24/2022 05:05:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=209
06/24/2022 05:05:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.15 on epoch=214
06/24/2022 05:05:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=219
06/24/2022 05:05:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.17 on epoch=224
06/24/2022 05:05:15 - INFO - __main__ - Global step 450 Train loss 0.18 ACC 0.59375 on epoch=224
06/24/2022 05:05:15 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=224, global_step=450
06/24/2022 05:05:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.17 on epoch=229
06/24/2022 05:05:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=234
06/24/2022 05:05:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
06/24/2022 05:05:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.09 on epoch=244
06/24/2022 05:05:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=249
06/24/2022 05:05:22 - INFO - __main__ - Global step 500 Train loss 0.13 ACC 0.5 on epoch=249
06/24/2022 05:05:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.11 on epoch=254
06/24/2022 05:05:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=259
06/24/2022 05:05:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
06/24/2022 05:05:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=269
06/24/2022 05:05:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.10 on epoch=274
06/24/2022 05:05:29 - INFO - __main__ - Global step 550 Train loss 0.10 ACC 0.53125 on epoch=274
06/24/2022 05:05:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.11 on epoch=279
06/24/2022 05:05:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=284
06/24/2022 05:05:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.08 on epoch=289
06/24/2022 05:05:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
06/24/2022 05:05:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.10 on epoch=299
06/24/2022 05:05:36 - INFO - __main__ - Global step 600 Train loss 0.09 ACC 0.53125 on epoch=299
06/24/2022 05:05:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
06/24/2022 05:05:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=309
06/24/2022 05:05:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/24/2022 05:05:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
06/24/2022 05:05:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
06/24/2022 05:05:43 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.5 on epoch=324
06/24/2022 05:05:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
06/24/2022 05:05:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
06/24/2022 05:05:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
06/24/2022 05:05:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
06/24/2022 05:05:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
06/24/2022 05:05:50 - INFO - __main__ - Global step 700 Train loss 0.02 ACC 0.5625 on epoch=349
06/24/2022 05:05:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
06/24/2022 05:05:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
06/24/2022 05:05:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/24/2022 05:05:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/24/2022 05:05:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
06/24/2022 05:05:57 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.65625 on epoch=374
06/24/2022 05:05:57 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=374, global_step=750
06/24/2022 05:05:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/24/2022 05:05:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/24/2022 05:06:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
06/24/2022 05:06:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/24/2022 05:06:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
06/24/2022 05:06:04 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.59375 on epoch=399
06/24/2022 05:06:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
06/24/2022 05:06:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/24/2022 05:06:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/24/2022 05:06:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
06/24/2022 05:06:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
06/24/2022 05:06:11 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.625 on epoch=424
06/24/2022 05:06:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
06/24/2022 05:06:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/24/2022 05:06:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
06/24/2022 05:06:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
06/24/2022 05:06:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/24/2022 05:06:18 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.625 on epoch=449
06/24/2022 05:06:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
06/24/2022 05:06:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/24/2022 05:06:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/24/2022 05:06:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/24/2022 05:06:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/24/2022 05:06:25 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.625 on epoch=474
06/24/2022 05:06:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
06/24/2022 05:06:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
06/24/2022 05:06:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
06/24/2022 05:06:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/24/2022 05:06:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/24/2022 05:06:32 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.65625 on epoch=499
06/24/2022 05:06:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/24/2022 05:06:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
06/24/2022 05:06:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/24/2022 05:06:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/24/2022 05:06:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/24/2022 05:06:39 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.625 on epoch=524
06/24/2022 05:06:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/24/2022 05:06:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/24/2022 05:06:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/24/2022 05:06:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/24/2022 05:06:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/24/2022 05:06:46 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.59375 on epoch=549
06/24/2022 05:06:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 05:06:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/24/2022 05:06:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/24/2022 05:06:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/24/2022 05:06:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/24/2022 05:06:53 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.5625 on epoch=574
06/24/2022 05:06:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/24/2022 05:06:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/24/2022 05:06:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/24/2022 05:06:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 05:06:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/24/2022 05:07:00 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.59375 on epoch=599
06/24/2022 05:07:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/24/2022 05:07:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/24/2022 05:07:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 05:07:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/24/2022 05:07:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/24/2022 05:07:07 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.59375 on epoch=624
06/24/2022 05:07:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 05:07:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 05:07:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 05:07:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
06/24/2022 05:07:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 05:07:14 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.625 on epoch=649
06/24/2022 05:07:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 05:07:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 05:07:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 05:07:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 05:07:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 05:07:21 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.625 on epoch=674
06/24/2022 05:07:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 05:07:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 05:07:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 05:07:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 05:07:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 05:07:28 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.59375 on epoch=699
06/24/2022 05:07:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 05:07:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 05:07:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 05:07:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 05:07:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 05:07:35 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.59375 on epoch=724
06/24/2022 05:07:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 05:07:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 05:07:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 05:07:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 05:07:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 05:07:42 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.59375 on epoch=749
06/24/2022 05:07:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 05:07:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 05:07:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 05:07:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/24/2022 05:07:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 05:07:49 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.59375 on epoch=774
06/24/2022 05:07:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 05:07:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 05:07:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 05:07:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 05:07:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 05:07:56 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.625 on epoch=799
06/24/2022 05:07:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 05:07:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 05:08:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 05:08:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 05:08:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 05:08:03 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.625 on epoch=824
06/24/2022 05:08:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 05:08:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 05:08:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 05:08:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 05:08:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 05:08:10 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.625 on epoch=849
06/24/2022 05:08:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 05:08:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 05:08:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 05:08:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 05:08:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 05:08:17 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.625 on epoch=874
06/24/2022 05:08:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 05:08:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 05:08:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 05:08:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 05:08:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 05:08:24 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.625 on epoch=899
06/24/2022 05:08:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 05:08:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 05:08:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 05:08:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 05:08:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 05:08:31 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.59375 on epoch=924
06/24/2022 05:08:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 05:08:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 05:08:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 05:08:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 05:08:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 05:08:38 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.59375 on epoch=949
06/24/2022 05:08:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
06/24/2022 05:08:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 05:08:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 05:08:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 05:08:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 05:08:45 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.625 on epoch=974
06/24/2022 05:08:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 05:08:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 05:08:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 05:08:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 05:08:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 05:08:52 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.625 on epoch=999
06/24/2022 05:08:52 - INFO - __main__ - save last model!
06/24/2022 05:08:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:08:53 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 05:08:53 - INFO - __main__ - Printing 3 examples
06/24/2022 05:08:53 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 05:08:53 - INFO - __main__ - ['equivalent']
06/24/2022 05:08:53 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 05:08:53 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:08:53 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 05:08:53 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:08:53 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:08:53 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:08:53 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:08:53 - INFO - __main__ - Printing 3 examples
06/24/2022 05:08:53 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 05:08:53 - INFO - __main__ - ['equivalent']
06/24/2022 05:08:53 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 05:08:53 - INFO - __main__ - ['equivalent']
06/24/2022 05:08:53 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 05:08:53 - INFO - __main__ - ['equivalent']
06/24/2022 05:08:53 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:08:53 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:08:53 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:08:53 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:08:53 - INFO - __main__ - Printing 3 examples
06/24/2022 05:08:53 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 05:08:53 - INFO - __main__ - ['equivalent']
06/24/2022 05:08:53 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 05:08:53 - INFO - __main__ - ['equivalent']
06/24/2022 05:08:53 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 05:08:53 - INFO - __main__ - ['equivalent']
06/24/2022 05:08:53 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:08:53 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:08:53 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:08:53 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 05:08:58 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:08:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:08:59 - INFO - __main__ - Starting training!
06/24/2022 05:09:01 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_13_0.5_8_predictions.txt
06/24/2022 05:09:01 - INFO - __main__ - ACC on test data: 0.5368
06/24/2022 05:09:02 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.5, bsz=8, dev_performance=0.65625, test_performance=0.5367647058823529
06/24/2022 05:09:02 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.4, bsz=8 ...
06/24/2022 05:09:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:09:03 - INFO - __main__ - Printing 3 examples
06/24/2022 05:09:03 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 05:09:03 - INFO - __main__ - ['equivalent']
06/24/2022 05:09:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 05:09:03 - INFO - __main__ - ['equivalent']
06/24/2022 05:09:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 05:09:03 - INFO - __main__ - ['equivalent']
06/24/2022 05:09:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:09:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:09:03 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:09:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:09:03 - INFO - __main__ - Printing 3 examples
06/24/2022 05:09:03 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 05:09:03 - INFO - __main__ - ['equivalent']
06/24/2022 05:09:03 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 05:09:03 - INFO - __main__ - ['equivalent']
06/24/2022 05:09:03 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 05:09:03 - INFO - __main__ - ['equivalent']
06/24/2022 05:09:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:09:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:09:03 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:09:08 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:09:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:09:09 - INFO - __main__ - Starting training!
06/24/2022 05:09:10 - INFO - __main__ - Step 10 Global step 10 Train loss 3.42 on epoch=4
06/24/2022 05:09:11 - INFO - __main__ - Step 20 Global step 20 Train loss 2.39 on epoch=9
06/24/2022 05:09:13 - INFO - __main__ - Step 30 Global step 30 Train loss 1.76 on epoch=14
06/24/2022 05:09:14 - INFO - __main__ - Step 40 Global step 40 Train loss 1.23 on epoch=19
06/24/2022 05:09:15 - INFO - __main__ - Step 50 Global step 50 Train loss 0.88 on epoch=24
06/24/2022 05:09:16 - INFO - __main__ - Global step 50 Train loss 1.93 ACC 0.59375 on epoch=24
06/24/2022 05:09:16 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.59375 on epoch=24, global_step=50
06/24/2022 05:09:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.65 on epoch=29
06/24/2022 05:09:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=34
06/24/2022 05:09:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=39
06/24/2022 05:09:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=44
06/24/2022 05:09:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=49
06/24/2022 05:09:23 - INFO - __main__ - Global step 100 Train loss 0.52 ACC 0.46875 on epoch=49
06/24/2022 05:09:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=54
06/24/2022 05:09:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.36 on epoch=59
06/24/2022 05:09:27 - INFO - __main__ - Step 130 Global step 130 Train loss 0.41 on epoch=64
06/24/2022 05:09:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.32 on epoch=69
06/24/2022 05:09:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=74
06/24/2022 05:09:30 - INFO - __main__ - Global step 150 Train loss 0.37 ACC 0.5 on epoch=74
06/24/2022 05:09:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=79
06/24/2022 05:09:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
06/24/2022 05:09:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.34 on epoch=89
06/24/2022 05:09:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
06/24/2022 05:09:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.29 on epoch=99
06/24/2022 05:09:37 - INFO - __main__ - Global step 200 Train loss 0.32 ACC 0.5 on epoch=99
06/24/2022 05:09:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.29 on epoch=104
06/24/2022 05:09:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
06/24/2022 05:09:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.29 on epoch=114
06/24/2022 05:09:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.31 on epoch=119
06/24/2022 05:09:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.33 on epoch=124
06/24/2022 05:09:44 - INFO - __main__ - Global step 250 Train loss 0.31 ACC 0.53125 on epoch=124
06/24/2022 05:09:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=129
06/24/2022 05:09:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
06/24/2022 05:09:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=139
06/24/2022 05:09:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=144
06/24/2022 05:09:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=149
06/24/2022 05:09:51 - INFO - __main__ - Global step 300 Train loss 0.29 ACC 0.5 on epoch=149
06/24/2022 05:09:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
06/24/2022 05:09:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
06/24/2022 05:09:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=164
06/24/2022 05:09:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
06/24/2022 05:09:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
06/24/2022 05:09:58 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.5 on epoch=174
06/24/2022 05:09:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
06/24/2022 05:10:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=184
06/24/2022 05:10:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
06/24/2022 05:10:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
06/24/2022 05:10:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
06/24/2022 05:10:05 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.46875 on epoch=199
06/24/2022 05:10:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=204
06/24/2022 05:10:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=209
06/24/2022 05:10:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
06/24/2022 05:10:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.29 on epoch=219
06/24/2022 05:10:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=224
06/24/2022 05:10:12 - INFO - __main__ - Global step 450 Train loss 0.24 ACC 0.625 on epoch=224
06/24/2022 05:10:12 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=224, global_step=450
06/24/2022 05:10:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.19 on epoch=229
06/24/2022 05:10:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=234
06/24/2022 05:10:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
06/24/2022 05:10:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=244
06/24/2022 05:10:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=249
06/24/2022 05:10:19 - INFO - __main__ - Global step 500 Train loss 0.19 ACC 0.53125 on epoch=249
06/24/2022 05:10:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.17 on epoch=254
06/24/2022 05:10:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=259
06/24/2022 05:10:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=264
06/24/2022 05:10:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=269
06/24/2022 05:10:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=274
06/24/2022 05:10:26 - INFO - __main__ - Global step 550 Train loss 0.16 ACC 0.5 on epoch=274
06/24/2022 05:10:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=279
06/24/2022 05:10:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=284
06/24/2022 05:10:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=289
06/24/2022 05:10:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=294
06/24/2022 05:10:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=299
06/24/2022 05:10:33 - INFO - __main__ - Global step 600 Train loss 0.16 ACC 0.5625 on epoch=299
06/24/2022 05:10:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.11 on epoch=304
06/24/2022 05:10:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=309
06/24/2022 05:10:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=314
06/24/2022 05:10:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=319
06/24/2022 05:10:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=324
06/24/2022 05:10:40 - INFO - __main__ - Global step 650 Train loss 0.09 ACC 0.53125 on epoch=324
06/24/2022 05:10:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=329
06/24/2022 05:10:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
06/24/2022 05:10:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
06/24/2022 05:10:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
06/24/2022 05:10:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
06/24/2022 05:10:47 - INFO - __main__ - Global step 700 Train loss 0.06 ACC 0.53125 on epoch=349
06/24/2022 05:10:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
06/24/2022 05:10:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
06/24/2022 05:10:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
06/24/2022 05:10:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
06/24/2022 05:10:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
06/24/2022 05:10:54 - INFO - __main__ - Global step 750 Train loss 0.05 ACC 0.46875 on epoch=374
06/24/2022 05:10:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
06/24/2022 05:10:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
06/24/2022 05:10:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
06/24/2022 05:10:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
06/24/2022 05:11:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=399
06/24/2022 05:11:01 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.5 on epoch=399
06/24/2022 05:11:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
06/24/2022 05:11:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
06/24/2022 05:11:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
06/24/2022 05:11:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
06/24/2022 05:11:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/24/2022 05:11:08 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.53125 on epoch=424
06/24/2022 05:11:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
06/24/2022 05:11:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
06/24/2022 05:11:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
06/24/2022 05:11:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
06/24/2022 05:11:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/24/2022 05:11:15 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.53125 on epoch=449
06/24/2022 05:11:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/24/2022 05:11:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/24/2022 05:11:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/24/2022 05:11:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
06/24/2022 05:11:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
06/24/2022 05:11:22 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.5625 on epoch=474
06/24/2022 05:11:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/24/2022 05:11:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/24/2022 05:11:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
06/24/2022 05:11:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
06/24/2022 05:11:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/24/2022 05:11:29 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.53125 on epoch=499
06/24/2022 05:11:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/24/2022 05:11:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/24/2022 05:11:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/24/2022 05:11:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/24/2022 05:11:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/24/2022 05:11:36 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.53125 on epoch=524
06/24/2022 05:11:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/24/2022 05:11:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/24/2022 05:11:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/24/2022 05:11:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
06/24/2022 05:11:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/24/2022 05:11:43 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.5625 on epoch=549
06/24/2022 05:11:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/24/2022 05:11:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/24/2022 05:11:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 05:11:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/24/2022 05:11:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/24/2022 05:11:50 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.59375 on epoch=574
06/24/2022 05:11:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/24/2022 05:11:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/24/2022 05:11:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
06/24/2022 05:11:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 05:11:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/24/2022 05:11:57 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.5625 on epoch=599
06/24/2022 05:11:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/24/2022 05:12:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/24/2022 05:12:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/24/2022 05:12:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/24/2022 05:12:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 05:12:04 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.59375 on epoch=624
06/24/2022 05:12:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 05:12:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 05:12:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 05:12:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 05:12:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 05:12:11 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.5625 on epoch=649
06/24/2022 05:12:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/24/2022 05:12:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 05:12:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 05:12:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/24/2022 05:12:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 05:12:18 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.625 on epoch=674
06/24/2022 05:12:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
06/24/2022 05:12:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/24/2022 05:12:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 05:12:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 05:12:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 05:12:25 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.5625 on epoch=699
06/24/2022 05:12:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 05:12:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/24/2022 05:12:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 05:12:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 05:12:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 05:12:32 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.625 on epoch=724
06/24/2022 05:12:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 05:12:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 05:12:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 05:12:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 05:12:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 05:12:39 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.5625 on epoch=749
06/24/2022 05:12:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 05:12:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 05:12:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 05:12:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 05:12:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 05:12:46 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.59375 on epoch=774
06/24/2022 05:12:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 05:12:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 05:12:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 05:12:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 05:12:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 05:12:53 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.59375 on epoch=799
06/24/2022 05:12:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 05:12:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 05:12:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 05:12:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 05:13:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 05:13:00 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.5625 on epoch=824
06/24/2022 05:13:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 05:13:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
06/24/2022 05:13:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 05:13:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 05:13:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 05:13:07 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.59375 on epoch=849
06/24/2022 05:13:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 05:13:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
06/24/2022 05:13:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 05:13:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 05:13:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
06/24/2022 05:13:14 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.53125 on epoch=874
06/24/2022 05:13:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/24/2022 05:13:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 05:13:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 05:13:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 05:13:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 05:13:21 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.53125 on epoch=899
06/24/2022 05:13:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 05:13:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 05:13:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 05:13:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 05:13:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 05:13:28 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.53125 on epoch=924
06/24/2022 05:13:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 05:13:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/24/2022 05:13:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
06/24/2022 05:13:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 05:13:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 05:13:35 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.625 on epoch=949
06/24/2022 05:13:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 05:13:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 05:13:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 05:13:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 05:13:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 05:13:42 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.59375 on epoch=974
06/24/2022 05:13:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 05:13:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 05:13:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 05:13:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 05:13:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 05:13:49 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.5 on epoch=999
06/24/2022 05:13:49 - INFO - __main__ - save last model!
06/24/2022 05:13:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:13:49 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 05:13:49 - INFO - __main__ - Printing 3 examples
06/24/2022 05:13:49 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 05:13:49 - INFO - __main__ - ['equivalent']
06/24/2022 05:13:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 05:13:49 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:13:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 05:13:49 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:13:49 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:13:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:13:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:13:49 - INFO - __main__ - Printing 3 examples
06/24/2022 05:13:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 05:13:49 - INFO - __main__ - ['equivalent']
06/24/2022 05:13:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 05:13:49 - INFO - __main__ - ['equivalent']
06/24/2022 05:13:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 05:13:49 - INFO - __main__ - ['equivalent']
06/24/2022 05:13:49 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:13:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:13:49 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:13:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:13:49 - INFO - __main__ - Printing 3 examples
06/24/2022 05:13:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 05:13:50 - INFO - __main__ - ['equivalent']
06/24/2022 05:13:50 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 05:13:50 - INFO - __main__ - ['equivalent']
06/24/2022 05:13:50 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 05:13:50 - INFO - __main__ - ['equivalent']
06/24/2022 05:13:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:13:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:13:50 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:13:50 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 05:13:56 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:13:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:13:56 - INFO - __main__ - Starting training!
06/24/2022 05:13:57 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_13_0.4_8_predictions.txt
06/24/2022 05:13:57 - INFO - __main__ - ACC on test data: 0.6593
06/24/2022 05:13:58 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.4, bsz=8, dev_performance=0.625, test_performance=0.6593137254901961
06/24/2022 05:13:58 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.3, bsz=8 ...
06/24/2022 05:13:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:13:58 - INFO - __main__ - Printing 3 examples
06/24/2022 05:13:58 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 05:13:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:13:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 05:13:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:13:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 05:13:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:13:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:13:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:13:59 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:13:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:13:59 - INFO - __main__ - Printing 3 examples
06/24/2022 05:13:59 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 05:13:59 - INFO - __main__ - ['equivalent']
06/24/2022 05:13:59 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 05:13:59 - INFO - __main__ - ['equivalent']
06/24/2022 05:13:59 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 05:13:59 - INFO - __main__ - ['equivalent']
06/24/2022 05:13:59 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:13:59 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:13:59 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:14:04 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:14:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:14:04 - INFO - __main__ - Starting training!
06/24/2022 05:14:06 - INFO - __main__ - Step 10 Global step 10 Train loss 3.39 on epoch=4
06/24/2022 05:14:07 - INFO - __main__ - Step 20 Global step 20 Train loss 2.66 on epoch=9
06/24/2022 05:14:08 - INFO - __main__ - Step 30 Global step 30 Train loss 2.19 on epoch=14
06/24/2022 05:14:10 - INFO - __main__ - Step 40 Global step 40 Train loss 1.67 on epoch=19
06/24/2022 05:14:11 - INFO - __main__ - Step 50 Global step 50 Train loss 1.36 on epoch=24
06/24/2022 05:14:12 - INFO - __main__ - Global step 50 Train loss 2.25 ACC 0.28125 on epoch=24
06/24/2022 05:14:12 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.28125 on epoch=24, global_step=50
06/24/2022 05:14:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.86 on epoch=29
06/24/2022 05:14:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.85 on epoch=34
06/24/2022 05:14:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.71 on epoch=39
06/24/2022 05:14:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
06/24/2022 05:14:18 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=49
06/24/2022 05:14:19 - INFO - __main__ - Global step 100 Train loss 0.68 ACC 0.5625 on epoch=49
06/24/2022 05:14:19 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.5625 on epoch=49, global_step=100
06/24/2022 05:14:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=54
06/24/2022 05:14:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=59
06/24/2022 05:14:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=64
06/24/2022 05:14:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=69
06/24/2022 05:14:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=74
06/24/2022 05:14:26 - INFO - __main__ - Global step 150 Train loss 0.47 ACC 0.59375 on epoch=74
06/24/2022 05:14:26 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=74, global_step=150
06/24/2022 05:14:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=79
06/24/2022 05:14:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=84
06/24/2022 05:14:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=89
06/24/2022 05:14:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
06/24/2022 05:14:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=99
06/24/2022 05:14:33 - INFO - __main__ - Global step 200 Train loss 0.37 ACC 0.40625 on epoch=99
06/24/2022 05:14:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.35 on epoch=104
06/24/2022 05:14:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
06/24/2022 05:14:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.33 on epoch=114
06/24/2022 05:14:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=119
06/24/2022 05:14:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.33 on epoch=124
06/24/2022 05:14:40 - INFO - __main__ - Global step 250 Train loss 0.34 ACC 0.5 on epoch=124
06/24/2022 05:14:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=129
06/24/2022 05:14:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=134
06/24/2022 05:14:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=139
06/24/2022 05:14:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=144
06/24/2022 05:14:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
06/24/2022 05:14:47 - INFO - __main__ - Global step 300 Train loss 0.34 ACC 0.53125 on epoch=149
06/24/2022 05:14:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=154
06/24/2022 05:14:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.30 on epoch=159
06/24/2022 05:14:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
06/24/2022 05:14:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
06/24/2022 05:14:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=174
06/24/2022 05:14:54 - INFO - __main__ - Global step 350 Train loss 0.28 ACC 0.53125 on epoch=174
06/24/2022 05:14:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=179
06/24/2022 05:14:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.28 on epoch=184
06/24/2022 05:14:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=189
06/24/2022 05:14:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=194
06/24/2022 05:15:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=199
06/24/2022 05:15:01 - INFO - __main__ - Global step 400 Train loss 0.30 ACC 0.5 on epoch=199
06/24/2022 05:15:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=204
06/24/2022 05:15:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=209
06/24/2022 05:15:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=214
06/24/2022 05:15:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
06/24/2022 05:15:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
06/24/2022 05:15:08 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.5625 on epoch=224
06/24/2022 05:15:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.30 on epoch=229
06/24/2022 05:15:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=234
06/24/2022 05:15:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=239
06/24/2022 05:15:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=244
06/24/2022 05:15:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
06/24/2022 05:15:15 - INFO - __main__ - Global step 500 Train loss 0.28 ACC 0.5625 on epoch=249
06/24/2022 05:15:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=254
06/24/2022 05:15:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
06/24/2022 05:15:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
06/24/2022 05:15:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=269
06/24/2022 05:15:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=274
06/24/2022 05:15:22 - INFO - __main__ - Global step 550 Train loss 0.24 ACC 0.59375 on epoch=274
06/24/2022 05:15:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
06/24/2022 05:15:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=284
06/24/2022 05:15:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=289
06/24/2022 05:15:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
06/24/2022 05:15:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
06/24/2022 05:15:29 - INFO - __main__ - Global step 600 Train loss 0.25 ACC 0.625 on epoch=299
06/24/2022 05:15:29 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=299, global_step=600
06/24/2022 05:15:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=304
06/24/2022 05:15:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=309
06/24/2022 05:15:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
06/24/2022 05:15:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=319
06/24/2022 05:15:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=324
06/24/2022 05:15:36 - INFO - __main__ - Global step 650 Train loss 0.20 ACC 0.625 on epoch=324
06/24/2022 05:15:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=329
06/24/2022 05:15:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=334
06/24/2022 05:15:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=339
06/24/2022 05:15:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=344
06/24/2022 05:15:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=349
06/24/2022 05:15:43 - INFO - __main__ - Global step 700 Train loss 0.18 ACC 0.59375 on epoch=349
06/24/2022 05:15:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=354
06/24/2022 05:15:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=359
06/24/2022 05:15:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=364
06/24/2022 05:15:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=369
06/24/2022 05:15:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=374
06/24/2022 05:15:50 - INFO - __main__ - Global step 750 Train loss 0.14 ACC 0.625 on epoch=374
06/24/2022 05:15:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=379
06/24/2022 05:15:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=384
06/24/2022 05:15:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=389
06/24/2022 05:15:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=394
06/24/2022 05:15:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=399
06/24/2022 05:15:57 - INFO - __main__ - Global step 800 Train loss 0.13 ACC 0.65625 on epoch=399
06/24/2022 05:15:57 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=399, global_step=800
06/24/2022 05:15:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=404
06/24/2022 05:15:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=409
06/24/2022 05:16:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=414
06/24/2022 05:16:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=419
06/24/2022 05:16:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=424
06/24/2022 05:16:04 - INFO - __main__ - Global step 850 Train loss 0.08 ACC 0.625 on epoch=424
06/24/2022 05:16:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=429
06/24/2022 05:16:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=434
06/24/2022 05:16:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=439
06/24/2022 05:16:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
06/24/2022 05:16:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=449
06/24/2022 05:16:11 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.59375 on epoch=449
06/24/2022 05:16:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
06/24/2022 05:16:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
06/24/2022 05:16:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=464
06/24/2022 05:16:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/24/2022 05:16:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
06/24/2022 05:16:18 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.59375 on epoch=474
06/24/2022 05:16:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=479
06/24/2022 05:16:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/24/2022 05:16:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
06/24/2022 05:16:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
06/24/2022 05:16:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
06/24/2022 05:16:25 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.53125 on epoch=499
06/24/2022 05:16:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
06/24/2022 05:16:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
06/24/2022 05:16:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/24/2022 05:16:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
06/24/2022 05:16:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
06/24/2022 05:16:32 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.59375 on epoch=524
06/24/2022 05:16:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/24/2022 05:16:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/24/2022 05:16:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/24/2022 05:16:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
06/24/2022 05:16:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/24/2022 05:16:39 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.625 on epoch=549
06/24/2022 05:16:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/24/2022 05:16:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
06/24/2022 05:16:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
06/24/2022 05:16:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/24/2022 05:16:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/24/2022 05:16:46 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.625 on epoch=574
06/24/2022 05:16:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
06/24/2022 05:16:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
06/24/2022 05:16:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/24/2022 05:16:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
06/24/2022 05:16:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/24/2022 05:16:53 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.5625 on epoch=599
06/24/2022 05:16:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/24/2022 05:16:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/24/2022 05:16:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/24/2022 05:16:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/24/2022 05:16:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 05:17:00 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.59375 on epoch=624
06/24/2022 05:17:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/24/2022 05:17:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 05:17:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/24/2022 05:17:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/24/2022 05:17:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 05:17:07 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.59375 on epoch=649
06/24/2022 05:17:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
06/24/2022 05:17:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/24/2022 05:17:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 05:17:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/24/2022 05:17:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/24/2022 05:17:14 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.5625 on epoch=674
06/24/2022 05:17:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
06/24/2022 05:17:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 05:17:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/24/2022 05:17:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 05:17:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 05:17:21 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.53125 on epoch=699
06/24/2022 05:17:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 05:17:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 05:17:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/24/2022 05:17:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 05:17:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
06/24/2022 05:17:28 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.59375 on epoch=724
06/24/2022 05:17:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/24/2022 05:17:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 05:17:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/24/2022 05:17:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/24/2022 05:17:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 05:17:35 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.53125 on epoch=749
06/24/2022 05:17:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/24/2022 05:17:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/24/2022 05:17:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/24/2022 05:17:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 05:17:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 05:17:42 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.53125 on epoch=774
06/24/2022 05:17:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 05:17:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/24/2022 05:17:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 05:17:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 05:17:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 05:17:49 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.5625 on epoch=799
06/24/2022 05:17:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/24/2022 05:17:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 05:17:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 05:17:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 05:17:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 05:17:56 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.53125 on epoch=824
06/24/2022 05:17:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 05:17:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/24/2022 05:17:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=839
06/24/2022 05:18:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 05:18:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 05:18:03 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.53125 on epoch=849
06/24/2022 05:18:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 05:18:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 05:18:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/24/2022 05:18:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 05:18:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 05:18:09 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.53125 on epoch=874
06/24/2022 05:18:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 05:18:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 05:18:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 05:18:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 05:18:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 05:18:16 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.65625 on epoch=899
06/24/2022 05:18:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 05:18:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 05:18:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 05:18:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 05:18:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 05:18:23 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.5625 on epoch=924
06/24/2022 05:18:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 05:18:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 05:18:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 05:18:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 05:18:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 05:18:30 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.5625 on epoch=949
06/24/2022 05:18:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 05:18:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 05:18:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 05:18:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 05:18:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 05:18:37 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.625 on epoch=974
06/24/2022 05:18:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 05:18:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 05:18:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/24/2022 05:18:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 05:18:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 05:18:44 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.65625 on epoch=999
06/24/2022 05:18:44 - INFO - __main__ - save last model!
06/24/2022 05:18:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:18:44 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 05:18:44 - INFO - __main__ - Printing 3 examples
06/24/2022 05:18:44 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 05:18:44 - INFO - __main__ - ['equivalent']
06/24/2022 05:18:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 05:18:44 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:18:44 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 05:18:44 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:18:44 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:18:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:18:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:18:45 - INFO - __main__ - Printing 3 examples
06/24/2022 05:18:45 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 05:18:45 - INFO - __main__ - ['equivalent']
06/24/2022 05:18:45 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 05:18:45 - INFO - __main__ - ['equivalent']
06/24/2022 05:18:45 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 05:18:45 - INFO - __main__ - ['equivalent']
06/24/2022 05:18:45 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:18:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:18:45 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:18:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:18:45 - INFO - __main__ - Printing 3 examples
06/24/2022 05:18:45 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 05:18:45 - INFO - __main__ - ['equivalent']
06/24/2022 05:18:45 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 05:18:45 - INFO - __main__ - ['equivalent']
06/24/2022 05:18:45 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 05:18:45 - INFO - __main__ - ['equivalent']
06/24/2022 05:18:45 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:18:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:18:45 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:18:45 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 05:18:50 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:18:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:18:51 - INFO - __main__ - Starting training!
06/24/2022 05:18:53 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_13_0.3_8_predictions.txt
06/24/2022 05:18:53 - INFO - __main__ - ACC on test data: 0.5588
06/24/2022 05:18:53 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.3, bsz=8, dev_performance=0.65625, test_performance=0.5588235294117647
06/24/2022 05:18:53 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.2, bsz=8 ...
06/24/2022 05:18:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:18:54 - INFO - __main__ - Printing 3 examples
06/24/2022 05:18:54 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 05:18:54 - INFO - __main__ - ['equivalent']
06/24/2022 05:18:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 05:18:54 - INFO - __main__ - ['equivalent']
06/24/2022 05:18:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 05:18:54 - INFO - __main__ - ['equivalent']
06/24/2022 05:18:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:18:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:18:54 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:18:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:18:54 - INFO - __main__ - Printing 3 examples
06/24/2022 05:18:54 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 05:18:54 - INFO - __main__ - ['equivalent']
06/24/2022 05:18:54 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 05:18:54 - INFO - __main__ - ['equivalent']
06/24/2022 05:18:54 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 05:18:54 - INFO - __main__ - ['equivalent']
06/24/2022 05:18:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:18:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:18:54 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:19:00 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:19:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:19:00 - INFO - __main__ - Starting training!
06/24/2022 05:19:02 - INFO - __main__ - Step 10 Global step 10 Train loss 3.74 on epoch=4
06/24/2022 05:19:03 - INFO - __main__ - Step 20 Global step 20 Train loss 3.28 on epoch=9
06/24/2022 05:19:05 - INFO - __main__ - Step 30 Global step 30 Train loss 2.52 on epoch=14
06/24/2022 05:19:06 - INFO - __main__ - Step 40 Global step 40 Train loss 2.21 on epoch=19
06/24/2022 05:19:07 - INFO - __main__ - Step 50 Global step 50 Train loss 1.72 on epoch=24
06/24/2022 05:19:08 - INFO - __main__ - Global step 50 Train loss 2.69 ACC 0.0 on epoch=24
06/24/2022 05:19:08 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 05:19:09 - INFO - __main__ - Step 60 Global step 60 Train loss 1.51 on epoch=29
06/24/2022 05:19:10 - INFO - __main__ - Step 70 Global step 70 Train loss 1.35 on epoch=34
06/24/2022 05:19:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.98 on epoch=39
06/24/2022 05:19:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.88 on epoch=44
06/24/2022 05:19:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.78 on epoch=49
06/24/2022 05:19:15 - INFO - __main__ - Global step 100 Train loss 1.10 ACC 0.375 on epoch=49
06/24/2022 05:19:15 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.375 on epoch=49, global_step=100
06/24/2022 05:19:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.62 on epoch=54
06/24/2022 05:19:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.68 on epoch=59
06/24/2022 05:19:19 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=64
06/24/2022 05:19:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=69
06/24/2022 05:19:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=74
06/24/2022 05:19:22 - INFO - __main__ - Global step 150 Train loss 0.60 ACC 0.53125 on epoch=74
06/24/2022 05:19:22 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.53125 on epoch=74, global_step=150
06/24/2022 05:19:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=79
06/24/2022 05:19:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=84
06/24/2022 05:19:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=89
06/24/2022 05:19:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=94
06/24/2022 05:19:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=99
06/24/2022 05:19:29 - INFO - __main__ - Global step 200 Train loss 0.45 ACC 0.53125 on epoch=99
06/24/2022 05:19:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=104
06/24/2022 05:19:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=109
06/24/2022 05:19:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
06/24/2022 05:19:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=119
06/24/2022 05:19:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=124
06/24/2022 05:19:36 - INFO - __main__ - Global step 250 Train loss 0.41 ACC 0.46875 on epoch=124
06/24/2022 05:19:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=129
06/24/2022 05:19:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=134
06/24/2022 05:19:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=139
06/24/2022 05:19:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=144
06/24/2022 05:19:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.33 on epoch=149
06/24/2022 05:19:43 - INFO - __main__ - Global step 300 Train loss 0.39 ACC 0.40625 on epoch=149
06/24/2022 05:19:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=154
06/24/2022 05:19:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=159
06/24/2022 05:19:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=164
06/24/2022 05:19:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=169
06/24/2022 05:19:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.32 on epoch=174
06/24/2022 05:19:50 - INFO - __main__ - Global step 350 Train loss 0.35 ACC 0.5 on epoch=174
06/24/2022 05:19:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=179
06/24/2022 05:19:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=184
06/24/2022 05:19:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.32 on epoch=189
06/24/2022 05:19:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=194
06/24/2022 05:19:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.31 on epoch=199
06/24/2022 05:19:57 - INFO - __main__ - Global step 400 Train loss 0.33 ACC 0.5 on epoch=199
06/24/2022 05:19:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=204
06/24/2022 05:20:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.30 on epoch=209
06/24/2022 05:20:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=214
06/24/2022 05:20:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=219
06/24/2022 05:20:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
06/24/2022 05:20:04 - INFO - __main__ - Global step 450 Train loss 0.30 ACC 0.5 on epoch=224
06/24/2022 05:20:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=229
06/24/2022 05:20:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=234
06/24/2022 05:20:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=239
06/24/2022 05:20:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=244
06/24/2022 05:20:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=249
06/24/2022 05:20:11 - INFO - __main__ - Global step 500 Train loss 0.30 ACC 0.46875 on epoch=249
06/24/2022 05:20:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
06/24/2022 05:20:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=259
06/24/2022 05:20:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.29 on epoch=264
06/24/2022 05:20:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=269
06/24/2022 05:20:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=274
06/24/2022 05:20:18 - INFO - __main__ - Global step 550 Train loss 0.27 ACC 0.5 on epoch=274
06/24/2022 05:20:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
06/24/2022 05:20:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=284
06/24/2022 05:20:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=289
06/24/2022 05:20:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=294
06/24/2022 05:20:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.28 on epoch=299
06/24/2022 05:20:25 - INFO - __main__ - Global step 600 Train loss 0.27 ACC 0.5 on epoch=299
06/24/2022 05:20:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=304
06/24/2022 05:20:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=309
06/24/2022 05:20:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=314
06/24/2022 05:20:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=319
06/24/2022 05:20:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=324
06/24/2022 05:20:32 - INFO - __main__ - Global step 650 Train loss 0.25 ACC 0.53125 on epoch=324
06/24/2022 05:20:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=329
06/24/2022 05:20:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=334
06/24/2022 05:20:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
06/24/2022 05:20:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=344
06/24/2022 05:20:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=349
06/24/2022 05:20:39 - INFO - __main__ - Global step 700 Train loss 0.26 ACC 0.46875 on epoch=349
06/24/2022 05:20:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=354
06/24/2022 05:20:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=359
06/24/2022 05:20:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=364
06/24/2022 05:20:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=369
06/24/2022 05:20:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=374
06/24/2022 05:20:46 - INFO - __main__ - Global step 750 Train loss 0.26 ACC 0.53125 on epoch=374
06/24/2022 05:20:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=379
06/24/2022 05:20:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.27 on epoch=384
06/24/2022 05:20:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
06/24/2022 05:20:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=394
06/24/2022 05:20:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=399
06/24/2022 05:20:53 - INFO - __main__ - Global step 800 Train loss 0.24 ACC 0.5625 on epoch=399
06/24/2022 05:20:53 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=399, global_step=800
06/24/2022 05:20:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=404
06/24/2022 05:20:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=409
06/24/2022 05:20:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=414
06/24/2022 05:20:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=419
06/24/2022 05:20:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=424
06/24/2022 05:21:00 - INFO - __main__ - Global step 850 Train loss 0.23 ACC 0.5 on epoch=424
06/24/2022 05:21:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=429
06/24/2022 05:21:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.27 on epoch=434
06/24/2022 05:21:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=439
06/24/2022 05:21:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=444
06/24/2022 05:21:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=449
06/24/2022 05:21:07 - INFO - __main__ - Global step 900 Train loss 0.23 ACC 0.53125 on epoch=449
06/24/2022 05:21:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=454
06/24/2022 05:21:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=459
06/24/2022 05:21:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=464
06/24/2022 05:21:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.19 on epoch=469
06/24/2022 05:21:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=474
06/24/2022 05:21:14 - INFO - __main__ - Global step 950 Train loss 0.21 ACC 0.46875 on epoch=474
06/24/2022 05:21:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=479
06/24/2022 05:21:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=484
06/24/2022 05:21:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=489
06/24/2022 05:21:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=494
06/24/2022 05:21:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=499
06/24/2022 05:21:21 - INFO - __main__ - Global step 1000 Train loss 0.17 ACC 0.46875 on epoch=499
06/24/2022 05:21:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=504
06/24/2022 05:21:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=509
06/24/2022 05:21:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=514
06/24/2022 05:21:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.14 on epoch=519
06/24/2022 05:21:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=524
06/24/2022 05:21:28 - INFO - __main__ - Global step 1050 Train loss 0.16 ACC 0.53125 on epoch=524
06/24/2022 05:21:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=529
06/24/2022 05:21:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=534
06/24/2022 05:21:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=539
06/24/2022 05:21:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=544
06/24/2022 05:21:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=549
06/24/2022 05:21:35 - INFO - __main__ - Global step 1100 Train loss 0.11 ACC 0.53125 on epoch=549
06/24/2022 05:21:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=554
06/24/2022 05:21:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=559
06/24/2022 05:21:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=564
06/24/2022 05:21:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=569
06/24/2022 05:21:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=574
06/24/2022 05:21:42 - INFO - __main__ - Global step 1150 Train loss 0.10 ACC 0.53125 on epoch=574
06/24/2022 05:21:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=579
06/24/2022 05:21:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=584
06/24/2022 05:21:46 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=589
06/24/2022 05:21:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=594
06/24/2022 05:21:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=599
06/24/2022 05:21:49 - INFO - __main__ - Global step 1200 Train loss 0.09 ACC 0.53125 on epoch=599
06/24/2022 05:21:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
06/24/2022 05:21:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=609
06/24/2022 05:21:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
06/24/2022 05:21:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=619
06/24/2022 05:21:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
06/24/2022 05:21:56 - INFO - __main__ - Global step 1250 Train loss 0.07 ACC 0.53125 on epoch=624
06/24/2022 05:21:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=629
06/24/2022 05:21:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=634
06/24/2022 05:22:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=639
06/24/2022 05:22:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
06/24/2022 05:22:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
06/24/2022 05:22:03 - INFO - __main__ - Global step 1300 Train loss 0.05 ACC 0.53125 on epoch=649
06/24/2022 05:22:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=654
06/24/2022 05:22:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
06/24/2022 05:22:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=664
06/24/2022 05:22:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
06/24/2022 05:22:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=674
06/24/2022 05:22:10 - INFO - __main__ - Global step 1350 Train loss 0.06 ACC 0.53125 on epoch=674
06/24/2022 05:22:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
06/24/2022 05:22:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
06/24/2022 05:22:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=689
06/24/2022 05:22:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
06/24/2022 05:22:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=699
06/24/2022 05:22:17 - INFO - __main__ - Global step 1400 Train loss 0.04 ACC 0.53125 on epoch=699
06/24/2022 05:22:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
06/24/2022 05:22:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
06/24/2022 05:22:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
06/24/2022 05:22:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
06/24/2022 05:22:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
06/24/2022 05:22:24 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.5625 on epoch=724
06/24/2022 05:22:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
06/24/2022 05:22:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=734
06/24/2022 05:22:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
06/24/2022 05:22:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=744
06/24/2022 05:22:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
06/24/2022 05:22:31 - INFO - __main__ - Global step 1500 Train loss 0.04 ACC 0.5 on epoch=749
06/24/2022 05:22:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
06/24/2022 05:22:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
06/24/2022 05:22:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
06/24/2022 05:22:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/24/2022 05:22:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=774
06/24/2022 05:22:37 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.5625 on epoch=774
06/24/2022 05:22:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/24/2022 05:22:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
06/24/2022 05:22:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
06/24/2022 05:22:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
06/24/2022 05:22:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
06/24/2022 05:22:44 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.53125 on epoch=799
06/24/2022 05:22:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
06/24/2022 05:22:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
06/24/2022 05:22:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/24/2022 05:22:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
06/24/2022 05:22:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
06/24/2022 05:22:51 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.53125 on epoch=824
06/24/2022 05:22:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
06/24/2022 05:22:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/24/2022 05:22:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/24/2022 05:22:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/24/2022 05:22:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
06/24/2022 05:22:58 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.5625 on epoch=849
06/24/2022 05:23:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/24/2022 05:23:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/24/2022 05:23:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/24/2022 05:23:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
06/24/2022 05:23:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
06/24/2022 05:23:05 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.53125 on epoch=874
06/24/2022 05:23:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/24/2022 05:23:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=884
06/24/2022 05:23:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
06/24/2022 05:23:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
06/24/2022 05:23:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/24/2022 05:23:12 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.5625 on epoch=899
06/24/2022 05:23:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/24/2022 05:23:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=909
06/24/2022 05:23:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/24/2022 05:23:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/24/2022 05:23:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/24/2022 05:23:19 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.53125 on epoch=924
06/24/2022 05:23:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/24/2022 05:23:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/24/2022 05:23:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/24/2022 05:23:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
06/24/2022 05:23:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 05:23:26 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.5625 on epoch=949
06/24/2022 05:23:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
06/24/2022 05:23:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
06/24/2022 05:23:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/24/2022 05:23:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/24/2022 05:23:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/24/2022 05:23:33 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.53125 on epoch=974
06/24/2022 05:23:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/24/2022 05:23:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 05:23:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/24/2022 05:23:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 05:23:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/24/2022 05:23:40 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.53125 on epoch=999
06/24/2022 05:23:40 - INFO - __main__ - save last model!
06/24/2022 05:23:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:23:40 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 05:23:40 - INFO - __main__ - Printing 3 examples
06/24/2022 05:23:40 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 05:23:40 - INFO - __main__ - ['equivalent']
06/24/2022 05:23:40 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 05:23:40 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:23:40 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 05:23:40 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:23:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:23:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:23:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:23:41 - INFO - __main__ - Printing 3 examples
06/24/2022 05:23:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 05:23:41 - INFO - __main__ - ['equivalent']
06/24/2022 05:23:41 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 05:23:41 - INFO - __main__ - ['equivalent']
06/24/2022 05:23:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 05:23:41 - INFO - __main__ - ['equivalent']
06/24/2022 05:23:41 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:23:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:23:41 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 05:23:41 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:23:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:23:41 - INFO - __main__ - Printing 3 examples
06/24/2022 05:23:41 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 05:23:41 - INFO - __main__ - ['equivalent']
06/24/2022 05:23:41 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 05:23:41 - INFO - __main__ - ['equivalent']
06/24/2022 05:23:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 05:23:41 - INFO - __main__ - ['equivalent']
06/24/2022 05:23:41 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:23:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:23:41 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:23:47 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:23:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:23:47 - INFO - __main__ - Starting training!
06/24/2022 05:23:49 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_13_0.2_8_predictions.txt
06/24/2022 05:23:49 - INFO - __main__ - ACC on test data: 0.6201
06/24/2022 05:23:49 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.2, bsz=8, dev_performance=0.5625, test_performance=0.6200980392156863
06/24/2022 05:23:49 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.5, bsz=8 ...
06/24/2022 05:23:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:23:50 - INFO - __main__ - Printing 3 examples
06/24/2022 05:23:50 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 05:23:50 - INFO - __main__ - ['equivalent']
06/24/2022 05:23:50 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 05:23:50 - INFO - __main__ - ['equivalent']
06/24/2022 05:23:50 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 05:23:50 - INFO - __main__ - ['equivalent']
06/24/2022 05:23:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:23:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:23:50 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:23:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:23:50 - INFO - __main__ - Printing 3 examples
06/24/2022 05:23:50 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 05:23:50 - INFO - __main__ - ['equivalent']
06/24/2022 05:23:50 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 05:23:50 - INFO - __main__ - ['equivalent']
06/24/2022 05:23:50 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 05:23:50 - INFO - __main__ - ['equivalent']
06/24/2022 05:23:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:23:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:23:50 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:23:56 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:23:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:23:56 - INFO - __main__ - Starting training!
06/24/2022 05:23:57 - INFO - __main__ - Step 10 Global step 10 Train loss 3.34 on epoch=4
06/24/2022 05:23:59 - INFO - __main__ - Step 20 Global step 20 Train loss 2.03 on epoch=9
06/24/2022 05:24:00 - INFO - __main__ - Step 30 Global step 30 Train loss 1.38 on epoch=14
06/24/2022 05:24:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.92 on epoch=19
06/24/2022 05:24:02 - INFO - __main__ - Step 50 Global step 50 Train loss 0.68 on epoch=24
06/24/2022 05:24:03 - INFO - __main__ - Global step 50 Train loss 1.67 ACC 0.53125 on epoch=24
06/24/2022 05:24:03 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.53125 on epoch=24, global_step=50
06/24/2022 05:24:04 - INFO - __main__ - Step 60 Global step 60 Train loss 0.56 on epoch=29
06/24/2022 05:24:06 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=34
06/24/2022 05:24:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=39
06/24/2022 05:24:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
06/24/2022 05:24:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=49
06/24/2022 05:24:10 - INFO - __main__ - Global step 100 Train loss 0.46 ACC 0.59375 on epoch=49
06/24/2022 05:24:10 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=49, global_step=100
06/24/2022 05:24:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.38 on epoch=54
06/24/2022 05:24:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=59
06/24/2022 05:24:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.34 on epoch=64
06/24/2022 05:24:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.29 on epoch=69
06/24/2022 05:24:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=74
06/24/2022 05:24:17 - INFO - __main__ - Global step 150 Train loss 0.34 ACC 0.65625 on epoch=74
06/24/2022 05:24:17 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=74, global_step=150
06/24/2022 05:24:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.34 on epoch=79
06/24/2022 05:24:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.30 on epoch=84
06/24/2022 05:24:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=89
06/24/2022 05:24:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=94
06/24/2022 05:24:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.23 on epoch=99
06/24/2022 05:24:24 - INFO - __main__ - Global step 200 Train loss 0.27 ACC 0.59375 on epoch=99
06/24/2022 05:24:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.30 on epoch=104
06/24/2022 05:24:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=109
06/24/2022 05:24:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=114
06/24/2022 05:24:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=119
06/24/2022 05:24:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=124
06/24/2022 05:24:31 - INFO - __main__ - Global step 250 Train loss 0.27 ACC 0.625 on epoch=124
06/24/2022 05:24:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=129
06/24/2022 05:24:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=134
06/24/2022 05:24:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=139
06/24/2022 05:24:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.20 on epoch=144
06/24/2022 05:24:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=149
06/24/2022 05:24:38 - INFO - __main__ - Global step 300 Train loss 0.22 ACC 0.5625 on epoch=149
06/24/2022 05:24:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.18 on epoch=154
06/24/2022 05:24:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=159
06/24/2022 05:24:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.19 on epoch=164
06/24/2022 05:24:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.17 on epoch=169
06/24/2022 05:24:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.13 on epoch=174
06/24/2022 05:24:45 - INFO - __main__ - Global step 350 Train loss 0.17 ACC 0.59375 on epoch=174
06/24/2022 05:24:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=179
06/24/2022 05:24:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=184
06/24/2022 05:24:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.14 on epoch=189
06/24/2022 05:24:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.09 on epoch=194
06/24/2022 05:24:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.08 on epoch=199
06/24/2022 05:24:51 - INFO - __main__ - Global step 400 Train loss 0.12 ACC 0.59375 on epoch=199
06/24/2022 05:24:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
06/24/2022 05:24:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.06 on epoch=209
06/24/2022 05:24:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.04 on epoch=214
06/24/2022 05:24:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.04 on epoch=219
06/24/2022 05:24:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.06 on epoch=224
06/24/2022 05:24:58 - INFO - __main__ - Global step 450 Train loss 0.06 ACC 0.625 on epoch=224
06/24/2022 05:25:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.04 on epoch=229
06/24/2022 05:25:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.05 on epoch=234
06/24/2022 05:25:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.03 on epoch=239
06/24/2022 05:25:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
06/24/2022 05:25:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.05 on epoch=249
06/24/2022 05:25:05 - INFO - __main__ - Global step 500 Train loss 0.04 ACC 0.59375 on epoch=249
06/24/2022 05:25:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.04 on epoch=254
06/24/2022 05:25:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
06/24/2022 05:25:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
06/24/2022 05:25:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.01 on epoch=269
06/24/2022 05:25:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
06/24/2022 05:25:12 - INFO - __main__ - Global step 550 Train loss 0.02 ACC 0.625 on epoch=274
06/24/2022 05:25:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
06/24/2022 05:25:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
06/24/2022 05:25:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
06/24/2022 05:25:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
06/24/2022 05:25:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
06/24/2022 05:25:19 - INFO - __main__ - Global step 600 Train loss 0.01 ACC 0.59375 on epoch=299
06/24/2022 05:25:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
06/24/2022 05:25:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
06/24/2022 05:25:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
06/24/2022 05:25:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
06/24/2022 05:25:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
06/24/2022 05:25:26 - INFO - __main__ - Global step 650 Train loss 0.01 ACC 0.59375 on epoch=324
06/24/2022 05:25:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
06/24/2022 05:25:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
06/24/2022 05:25:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
06/24/2022 05:25:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
06/24/2022 05:25:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
06/24/2022 05:25:33 - INFO - __main__ - Global step 700 Train loss 0.01 ACC 0.625 on epoch=349
06/24/2022 05:25:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
06/24/2022 05:25:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
06/24/2022 05:25:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
06/24/2022 05:25:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/24/2022 05:25:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
06/24/2022 05:25:40 - INFO - __main__ - Global step 750 Train loss 0.01 ACC 0.59375 on epoch=374
06/24/2022 05:25:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
06/24/2022 05:25:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
06/24/2022 05:25:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
06/24/2022 05:25:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
06/24/2022 05:25:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
06/24/2022 05:25:47 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.59375 on epoch=399
06/24/2022 05:25:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
06/24/2022 05:25:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
06/24/2022 05:25:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
06/24/2022 05:25:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
06/24/2022 05:25:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
06/24/2022 05:25:54 - INFO - __main__ - Global step 850 Train loss 0.00 ACC 0.59375 on epoch=424
06/24/2022 05:25:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
06/24/2022 05:25:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
06/24/2022 05:25:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
06/24/2022 05:25:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
06/24/2022 05:26:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
06/24/2022 05:26:01 - INFO - __main__ - Global step 900 Train loss 0.00 ACC 0.59375 on epoch=449
06/24/2022 05:26:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
06/24/2022 05:26:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/24/2022 05:26:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/24/2022 05:26:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
06/24/2022 05:26:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/24/2022 05:26:08 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.625 on epoch=474
06/24/2022 05:26:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
06/24/2022 05:26:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
06/24/2022 05:26:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
06/24/2022 05:26:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/24/2022 05:26:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/24/2022 05:26:15 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.59375 on epoch=499
06/24/2022 05:26:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/24/2022 05:26:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
06/24/2022 05:26:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/24/2022 05:26:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/24/2022 05:26:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/24/2022 05:26:22 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.59375 on epoch=524
06/24/2022 05:26:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/24/2022 05:26:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/24/2022 05:26:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/24/2022 05:26:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
06/24/2022 05:26:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/24/2022 05:26:29 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.59375 on epoch=549
06/24/2022 05:26:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 05:26:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/24/2022 05:26:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/24/2022 05:26:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/24/2022 05:26:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/24/2022 05:26:35 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.59375 on epoch=574
06/24/2022 05:26:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/24/2022 05:26:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/24/2022 05:26:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/24/2022 05:26:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 05:26:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/24/2022 05:26:42 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.59375 on epoch=599
06/24/2022 05:26:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/24/2022 05:26:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/24/2022 05:26:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 05:26:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/24/2022 05:26:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 05:26:49 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.59375 on epoch=624
06/24/2022 05:26:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 05:26:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 05:26:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 05:26:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 05:26:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 05:26:56 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.59375 on epoch=649
06/24/2022 05:26:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 05:26:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 05:27:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 05:27:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 05:27:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 05:27:03 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.59375 on epoch=674
06/24/2022 05:27:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 05:27:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 05:27:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 05:27:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 05:27:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 05:27:10 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.59375 on epoch=699
06/24/2022 05:27:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 05:27:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 05:27:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 05:27:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 05:27:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 05:27:17 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.625 on epoch=724
06/24/2022 05:27:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/24/2022 05:27:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 05:27:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 05:27:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 05:27:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 05:27:24 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.625 on epoch=749
06/24/2022 05:27:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 05:27:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 05:27:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 05:27:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 05:27:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 05:27:31 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.59375 on epoch=774
06/24/2022 05:27:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 05:27:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 05:27:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 05:27:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 05:27:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 05:27:38 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.5625 on epoch=799
06/24/2022 05:27:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 05:27:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 05:27:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 05:27:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 05:27:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 05:27:45 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.59375 on epoch=824
06/24/2022 05:27:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 05:27:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 05:27:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 05:27:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 05:27:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 05:27:52 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.625 on epoch=849
06/24/2022 05:27:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 05:27:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 05:27:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 05:27:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 05:27:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 05:27:58 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.59375 on epoch=874
06/24/2022 05:28:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 05:28:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 05:28:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 05:28:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 05:28:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 05:28:05 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.59375 on epoch=899
06/24/2022 05:28:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 05:28:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 05:28:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 05:28:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/24/2022 05:28:12 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 05:28:12 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.59375 on epoch=924
06/24/2022 05:28:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 05:28:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 05:28:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 05:28:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 05:28:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 05:28:19 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.625 on epoch=949
06/24/2022 05:28:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 05:28:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 05:28:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 05:28:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 05:28:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 05:28:26 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.59375 on epoch=974
06/24/2022 05:28:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 05:28:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 05:28:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 05:28:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 05:28:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 05:28:33 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.625 on epoch=999
06/24/2022 05:28:33 - INFO - __main__ - save last model!
06/24/2022 05:28:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:28:33 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 05:28:33 - INFO - __main__ - Printing 3 examples
06/24/2022 05:28:33 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 05:28:33 - INFO - __main__ - ['equivalent']
06/24/2022 05:28:33 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 05:28:33 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:28:33 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 05:28:33 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:28:33 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:28:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:28:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:28:34 - INFO - __main__ - Printing 3 examples
06/24/2022 05:28:34 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 05:28:34 - INFO - __main__ - ['equivalent']
06/24/2022 05:28:34 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 05:28:34 - INFO - __main__ - ['equivalent']
06/24/2022 05:28:34 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 05:28:34 - INFO - __main__ - ['equivalent']
06/24/2022 05:28:34 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:28:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:28:34 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:28:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:28:34 - INFO - __main__ - Printing 3 examples
06/24/2022 05:28:34 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 05:28:34 - INFO - __main__ - ['equivalent']
06/24/2022 05:28:34 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 05:28:34 - INFO - __main__ - ['equivalent']
06/24/2022 05:28:34 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 05:28:34 - INFO - __main__ - ['equivalent']
06/24/2022 05:28:34 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:28:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:28:34 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:28:34 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 05:28:39 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:28:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:28:39 - INFO - __main__ - Starting training!
06/24/2022 05:28:42 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_21_0.5_8_predictions.txt
06/24/2022 05:28:42 - INFO - __main__ - ACC on test data: 0.5539
06/24/2022 05:28:42 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.5, bsz=8, dev_performance=0.65625, test_performance=0.553921568627451
06/24/2022 05:28:42 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.4, bsz=8 ...
06/24/2022 05:28:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:28:43 - INFO - __main__ - Printing 3 examples
06/24/2022 05:28:43 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 05:28:43 - INFO - __main__ - ['equivalent']
06/24/2022 05:28:43 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 05:28:43 - INFO - __main__ - ['equivalent']
06/24/2022 05:28:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 05:28:43 - INFO - __main__ - ['equivalent']
06/24/2022 05:28:43 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:28:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:28:43 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:28:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:28:43 - INFO - __main__ - Printing 3 examples
06/24/2022 05:28:43 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 05:28:43 - INFO - __main__ - ['equivalent']
06/24/2022 05:28:43 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 05:28:43 - INFO - __main__ - ['equivalent']
06/24/2022 05:28:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 05:28:43 - INFO - __main__ - ['equivalent']
06/24/2022 05:28:43 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:28:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:28:43 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:28:48 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:28:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:28:48 - INFO - __main__ - Starting training!
06/24/2022 05:28:50 - INFO - __main__ - Step 10 Global step 10 Train loss 3.57 on epoch=4
06/24/2022 05:28:51 - INFO - __main__ - Step 20 Global step 20 Train loss 2.38 on epoch=9
06/24/2022 05:28:53 - INFO - __main__ - Step 30 Global step 30 Train loss 1.69 on epoch=14
06/24/2022 05:28:54 - INFO - __main__ - Step 40 Global step 40 Train loss 1.20 on epoch=19
06/24/2022 05:28:55 - INFO - __main__ - Step 50 Global step 50 Train loss 0.73 on epoch=24
06/24/2022 05:28:55 - INFO - __main__ - Global step 50 Train loss 1.91 ACC 0.5 on epoch=24
06/24/2022 05:28:56 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
06/24/2022 05:28:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=29
06/24/2022 05:28:58 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=34
06/24/2022 05:28:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=39
06/24/2022 05:29:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.38 on epoch=44
06/24/2022 05:29:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=49
06/24/2022 05:29:02 - INFO - __main__ - Global step 100 Train loss 0.50 ACC 0.59375 on epoch=49
06/24/2022 05:29:02 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=49, global_step=100
06/24/2022 05:29:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=54
06/24/2022 05:29:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=59
06/24/2022 05:29:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=64
06/24/2022 05:29:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.35 on epoch=69
06/24/2022 05:29:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.26 on epoch=74
06/24/2022 05:29:09 - INFO - __main__ - Global step 150 Train loss 0.35 ACC 0.59375 on epoch=74
06/24/2022 05:29:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.33 on epoch=79
06/24/2022 05:29:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.30 on epoch=84
06/24/2022 05:29:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.34 on epoch=89
06/24/2022 05:29:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=94
06/24/2022 05:29:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
06/24/2022 05:29:16 - INFO - __main__ - Global step 200 Train loss 0.30 ACC 0.65625 on epoch=99
06/24/2022 05:29:16 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=99, global_step=200
06/24/2022 05:29:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.28 on epoch=104
06/24/2022 05:29:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.31 on epoch=109
06/24/2022 05:29:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
06/24/2022 05:29:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=119
06/24/2022 05:29:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=124
06/24/2022 05:29:23 - INFO - __main__ - Global step 250 Train loss 0.29 ACC 0.65625 on epoch=124
06/24/2022 05:29:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
06/24/2022 05:29:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=134
06/24/2022 05:29:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=139
06/24/2022 05:29:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
06/24/2022 05:29:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.21 on epoch=149
06/24/2022 05:29:30 - INFO - __main__ - Global step 300 Train loss 0.23 ACC 0.59375 on epoch=149
06/24/2022 05:29:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.19 on epoch=154
06/24/2022 05:29:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=159
06/24/2022 05:29:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.18 on epoch=164
06/24/2022 05:29:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.19 on epoch=169
06/24/2022 05:29:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.19 on epoch=174
06/24/2022 05:29:36 - INFO - __main__ - Global step 350 Train loss 0.19 ACC 0.625 on epoch=174
06/24/2022 05:29:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.19 on epoch=179
06/24/2022 05:29:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.19 on epoch=184
06/24/2022 05:29:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.18 on epoch=189
06/24/2022 05:29:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.17 on epoch=194
06/24/2022 05:29:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.12 on epoch=199
06/24/2022 05:29:43 - INFO - __main__ - Global step 400 Train loss 0.17 ACC 0.625 on epoch=199
06/24/2022 05:29:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
06/24/2022 05:29:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.09 on epoch=209
06/24/2022 05:29:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
06/24/2022 05:29:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.07 on epoch=219
06/24/2022 05:29:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
06/24/2022 05:29:50 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.59375 on epoch=224
06/24/2022 05:29:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.06 on epoch=229
06/24/2022 05:29:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
06/24/2022 05:29:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.06 on epoch=239
06/24/2022 05:29:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
06/24/2022 05:29:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.04 on epoch=249
06/24/2022 05:29:57 - INFO - __main__ - Global step 500 Train loss 0.06 ACC 0.59375 on epoch=249
06/24/2022 05:29:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
06/24/2022 05:29:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
06/24/2022 05:30:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
06/24/2022 05:30:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=269
06/24/2022 05:30:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.02 on epoch=274
06/24/2022 05:30:04 - INFO - __main__ - Global step 550 Train loss 0.03 ACC 0.59375 on epoch=274
06/24/2022 05:30:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
06/24/2022 05:30:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
06/24/2022 05:30:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.04 on epoch=289
06/24/2022 05:30:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
06/24/2022 05:30:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
06/24/2022 05:30:11 - INFO - __main__ - Global step 600 Train loss 0.02 ACC 0.5625 on epoch=299
06/24/2022 05:30:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=304
06/24/2022 05:30:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
06/24/2022 05:30:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
06/24/2022 05:30:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
06/24/2022 05:30:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/24/2022 05:30:17 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.59375 on epoch=324
06/24/2022 05:30:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
06/24/2022 05:30:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
06/24/2022 05:30:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
06/24/2022 05:30:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
06/24/2022 05:30:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
06/24/2022 05:30:24 - INFO - __main__ - Global step 700 Train loss 0.01 ACC 0.625 on epoch=349
06/24/2022 05:30:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
06/24/2022 05:30:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
06/24/2022 05:30:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/24/2022 05:30:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/24/2022 05:30:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
06/24/2022 05:30:31 - INFO - __main__ - Global step 750 Train loss 0.01 ACC 0.625 on epoch=374
06/24/2022 05:30:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
06/24/2022 05:30:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/24/2022 05:30:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
06/24/2022 05:30:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/24/2022 05:30:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
06/24/2022 05:30:38 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.625 on epoch=399
06/24/2022 05:30:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
06/24/2022 05:30:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/24/2022 05:30:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/24/2022 05:30:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
06/24/2022 05:30:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/24/2022 05:30:45 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.59375 on epoch=424
06/24/2022 05:30:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/24/2022 05:30:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
06/24/2022 05:30:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
06/24/2022 05:30:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
06/24/2022 05:30:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
06/24/2022 05:30:52 - INFO - __main__ - Global step 900 Train loss 0.00 ACC 0.59375 on epoch=449
06/24/2022 05:30:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
06/24/2022 05:30:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/24/2022 05:30:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
06/24/2022 05:30:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
06/24/2022 05:30:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/24/2022 05:30:58 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.59375 on epoch=474
06/24/2022 05:31:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
06/24/2022 05:31:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/24/2022 05:31:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/24/2022 05:31:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/24/2022 05:31:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/24/2022 05:31:05 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.625 on epoch=499
06/24/2022 05:31:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/24/2022 05:31:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/24/2022 05:31:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/24/2022 05:31:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
06/24/2022 05:31:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
06/24/2022 05:31:12 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.5625 on epoch=524
06/24/2022 05:31:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/24/2022 05:31:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/24/2022 05:31:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/24/2022 05:31:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/24/2022 05:31:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/24/2022 05:31:19 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.59375 on epoch=549
06/24/2022 05:31:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 05:31:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/24/2022 05:31:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/24/2022 05:31:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/24/2022 05:31:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/24/2022 05:31:26 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.59375 on epoch=574
06/24/2022 05:31:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/24/2022 05:31:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/24/2022 05:31:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/24/2022 05:31:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 05:31:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
06/24/2022 05:31:33 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.59375 on epoch=599
06/24/2022 05:31:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
06/24/2022 05:31:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/24/2022 05:31:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 05:31:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/24/2022 05:31:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 05:31:40 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.59375 on epoch=624
06/24/2022 05:31:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 05:31:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 05:31:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 05:31:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 05:31:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 05:31:46 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.59375 on epoch=649
06/24/2022 05:31:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 05:31:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 05:31:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 05:31:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 05:31:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 05:31:53 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.625 on epoch=674
06/24/2022 05:31:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 05:31:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 05:31:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 05:31:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 05:32:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 05:32:00 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.59375 on epoch=699
06/24/2022 05:32:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 05:32:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 05:32:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 05:32:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
06/24/2022 05:32:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 05:32:07 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.625 on epoch=724
06/24/2022 05:32:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/24/2022 05:32:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 05:32:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 05:32:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
06/24/2022 05:32:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 05:32:14 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.5625 on epoch=749
06/24/2022 05:32:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 05:32:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 05:32:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 05:32:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 05:32:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 05:32:21 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.5625 on epoch=774
06/24/2022 05:32:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 05:32:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/24/2022 05:32:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 05:32:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 05:32:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 05:32:28 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.59375 on epoch=799
06/24/2022 05:32:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 05:32:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 05:32:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 05:32:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 05:32:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 05:32:35 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.5625 on epoch=824
06/24/2022 05:32:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 05:32:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 05:32:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 05:32:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 05:32:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 05:32:42 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.5625 on epoch=849
06/24/2022 05:32:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 05:32:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 05:32:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 05:32:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 05:32:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 05:32:49 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.59375 on epoch=874
06/24/2022 05:32:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 05:32:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
06/24/2022 05:32:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 05:32:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 05:32:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 05:32:56 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.5625 on epoch=899
06/24/2022 05:32:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 05:32:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 05:32:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 05:33:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 05:33:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
06/24/2022 05:33:02 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.5625 on epoch=924
06/24/2022 05:33:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 05:33:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 05:33:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 05:33:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 05:33:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 05:33:09 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.5625 on epoch=949
06/24/2022 05:33:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 05:33:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 05:33:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 05:33:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 05:33:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 05:33:16 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.5625 on epoch=974
06/24/2022 05:33:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 05:33:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 05:33:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 05:33:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 05:33:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 05:33:23 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.59375 on epoch=999
06/24/2022 05:33:23 - INFO - __main__ - save last model!
06/24/2022 05:33:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:33:23 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 05:33:23 - INFO - __main__ - Printing 3 examples
06/24/2022 05:33:23 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 05:33:23 - INFO - __main__ - ['equivalent']
06/24/2022 05:33:23 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 05:33:23 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:33:23 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 05:33:23 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:33:23 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:33:23 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:33:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:33:23 - INFO - __main__ - Printing 3 examples
06/24/2022 05:33:23 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 05:33:23 - INFO - __main__ - ['equivalent']
06/24/2022 05:33:23 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 05:33:23 - INFO - __main__ - ['equivalent']
06/24/2022 05:33:23 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 05:33:23 - INFO - __main__ - ['equivalent']
06/24/2022 05:33:23 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:33:23 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:33:24 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:33:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:33:24 - INFO - __main__ - Printing 3 examples
06/24/2022 05:33:24 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 05:33:24 - INFO - __main__ - ['equivalent']
06/24/2022 05:33:24 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 05:33:24 - INFO - __main__ - ['equivalent']
06/24/2022 05:33:24 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 05:33:24 - INFO - __main__ - ['equivalent']
06/24/2022 05:33:24 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:33:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:33:24 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 05:33:24 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:33:30 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:33:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:33:30 - INFO - __main__ - Starting training!
06/24/2022 05:33:31 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_21_0.4_8_predictions.txt
06/24/2022 05:33:31 - INFO - __main__ - ACC on test data: 0.5662
06/24/2022 05:33:32 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.4, bsz=8, dev_performance=0.65625, test_performance=0.5661764705882353
06/24/2022 05:33:32 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.3, bsz=8 ...
06/24/2022 05:33:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:33:32 - INFO - __main__ - Printing 3 examples
06/24/2022 05:33:32 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 05:33:32 - INFO - __main__ - ['equivalent']
06/24/2022 05:33:32 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 05:33:32 - INFO - __main__ - ['equivalent']
06/24/2022 05:33:32 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 05:33:32 - INFO - __main__ - ['equivalent']
06/24/2022 05:33:32 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:33:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:33:32 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:33:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:33:32 - INFO - __main__ - Printing 3 examples
06/24/2022 05:33:32 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 05:33:32 - INFO - __main__ - ['equivalent']
06/24/2022 05:33:32 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 05:33:32 - INFO - __main__ - ['equivalent']
06/24/2022 05:33:32 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 05:33:32 - INFO - __main__ - ['equivalent']
06/24/2022 05:33:32 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:33:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:33:33 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:33:38 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:33:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:33:38 - INFO - __main__ - Starting training!
06/24/2022 05:33:40 - INFO - __main__ - Step 10 Global step 10 Train loss 3.42 on epoch=4
06/24/2022 05:33:41 - INFO - __main__ - Step 20 Global step 20 Train loss 2.85 on epoch=9
06/24/2022 05:33:43 - INFO - __main__ - Step 30 Global step 30 Train loss 2.17 on epoch=14
06/24/2022 05:33:44 - INFO - __main__ - Step 40 Global step 40 Train loss 1.59 on epoch=19
06/24/2022 05:33:45 - INFO - __main__ - Step 50 Global step 50 Train loss 1.26 on epoch=24
06/24/2022 05:33:46 - INFO - __main__ - Global step 50 Train loss 2.26 ACC 0.03125 on epoch=24
06/24/2022 05:33:46 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.03125 on epoch=24, global_step=50
06/24/2022 05:33:47 - INFO - __main__ - Step 60 Global step 60 Train loss 0.97 on epoch=29
06/24/2022 05:33:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.80 on epoch=34
06/24/2022 05:33:49 - INFO - __main__ - Step 80 Global step 80 Train loss 0.64 on epoch=39
06/24/2022 05:33:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=44
06/24/2022 05:33:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.58 on epoch=49
06/24/2022 05:33:53 - INFO - __main__ - Global step 100 Train loss 0.71 ACC 0.59375 on epoch=49
06/24/2022 05:33:53 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.59375 on epoch=49, global_step=100
06/24/2022 05:33:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=54
06/24/2022 05:33:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=59
06/24/2022 05:33:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.34 on epoch=64
06/24/2022 05:33:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.35 on epoch=69
06/24/2022 05:33:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=74
06/24/2022 05:33:59 - INFO - __main__ - Global step 150 Train loss 0.41 ACC 0.625 on epoch=74
06/24/2022 05:34:00 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=74, global_step=150
06/24/2022 05:34:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=79
06/24/2022 05:34:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=84
06/24/2022 05:34:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.31 on epoch=89
06/24/2022 05:34:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
06/24/2022 05:34:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
06/24/2022 05:34:06 - INFO - __main__ - Global step 200 Train loss 0.38 ACC 0.5 on epoch=99
06/24/2022 05:34:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.35 on epoch=104
06/24/2022 05:34:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=109
06/24/2022 05:34:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=114
06/24/2022 05:34:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.29 on epoch=119
06/24/2022 05:34:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=124
06/24/2022 05:34:13 - INFO - __main__ - Global step 250 Train loss 0.32 ACC 0.75 on epoch=124
06/24/2022 05:34:13 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.75 on epoch=124, global_step=250
06/24/2022 05:34:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=129
06/24/2022 05:34:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=134
06/24/2022 05:34:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=139
06/24/2022 05:34:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
06/24/2022 05:34:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
06/24/2022 05:34:20 - INFO - __main__ - Global step 300 Train loss 0.30 ACC 0.625 on epoch=149
06/24/2022 05:34:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
06/24/2022 05:34:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
06/24/2022 05:34:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=164
06/24/2022 05:34:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
06/24/2022 05:34:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=174
06/24/2022 05:34:27 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.625 on epoch=174
06/24/2022 05:34:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.31 on epoch=179
06/24/2022 05:34:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.20 on epoch=184
06/24/2022 05:34:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.15 on epoch=189
06/24/2022 05:34:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=194
06/24/2022 05:34:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=199
06/24/2022 05:34:34 - INFO - __main__ - Global step 400 Train loss 0.21 ACC 0.53125 on epoch=199
06/24/2022 05:34:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=204
06/24/2022 05:34:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=209
06/24/2022 05:34:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.18 on epoch=214
06/24/2022 05:34:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.16 on epoch=219
06/24/2022 05:34:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=224
06/24/2022 05:34:40 - INFO - __main__ - Global step 450 Train loss 0.18 ACC 0.59375 on epoch=224
06/24/2022 05:34:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=229
06/24/2022 05:34:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.10 on epoch=234
06/24/2022 05:34:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.14 on epoch=239
06/24/2022 05:34:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=244
06/24/2022 05:34:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=249
06/24/2022 05:34:47 - INFO - __main__ - Global step 500 Train loss 0.12 ACC 0.5625 on epoch=249
06/24/2022 05:34:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
06/24/2022 05:34:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.13 on epoch=259
06/24/2022 05:34:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.09 on epoch=264
06/24/2022 05:34:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=269
06/24/2022 05:34:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.08 on epoch=274
06/24/2022 05:34:54 - INFO - __main__ - Global step 550 Train loss 0.10 ACC 0.5625 on epoch=274
06/24/2022 05:34:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
06/24/2022 05:34:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.09 on epoch=284
06/24/2022 05:34:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
06/24/2022 05:34:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
06/24/2022 05:35:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
06/24/2022 05:35:01 - INFO - __main__ - Global step 600 Train loss 0.07 ACC 0.5625 on epoch=299
06/24/2022 05:35:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=304
06/24/2022 05:35:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=309
06/24/2022 05:35:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/24/2022 05:35:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.05 on epoch=319
06/24/2022 05:35:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=324
06/24/2022 05:35:08 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.5625 on epoch=324
06/24/2022 05:35:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=329
06/24/2022 05:35:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
06/24/2022 05:35:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
06/24/2022 05:35:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
06/24/2022 05:35:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=349
06/24/2022 05:35:14 - INFO - __main__ - Global step 700 Train loss 0.05 ACC 0.5625 on epoch=349
06/24/2022 05:35:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
06/24/2022 05:35:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
06/24/2022 05:35:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=364
06/24/2022 05:35:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
06/24/2022 05:35:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=374
06/24/2022 05:35:21 - INFO - __main__ - Global step 750 Train loss 0.04 ACC 0.5625 on epoch=374
06/24/2022 05:35:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/24/2022 05:35:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=384
06/24/2022 05:35:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=389
06/24/2022 05:35:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=394
06/24/2022 05:35:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
06/24/2022 05:35:28 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.5625 on epoch=399
06/24/2022 05:35:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
06/24/2022 05:35:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=409
06/24/2022 05:35:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
06/24/2022 05:35:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
06/24/2022 05:35:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/24/2022 05:35:35 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.53125 on epoch=424
06/24/2022 05:35:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
06/24/2022 05:35:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/24/2022 05:35:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
06/24/2022 05:35:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
06/24/2022 05:35:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/24/2022 05:35:42 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.5625 on epoch=449
06/24/2022 05:35:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
06/24/2022 05:35:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
06/24/2022 05:35:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
06/24/2022 05:35:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/24/2022 05:35:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/24/2022 05:35:48 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.5625 on epoch=474
06/24/2022 05:35:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/24/2022 05:35:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
06/24/2022 05:35:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/24/2022 05:35:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/24/2022 05:35:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
06/24/2022 05:35:55 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.5625 on epoch=499
06/24/2022 05:35:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/24/2022 05:35:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/24/2022 05:35:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
06/24/2022 05:36:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/24/2022 05:36:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
06/24/2022 05:36:02 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.5625 on epoch=524
06/24/2022 05:36:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/24/2022 05:36:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/24/2022 05:36:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/24/2022 05:36:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/24/2022 05:36:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/24/2022 05:36:09 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.5625 on epoch=549
06/24/2022 05:36:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 05:36:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/24/2022 05:36:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 05:36:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/24/2022 05:36:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/24/2022 05:36:16 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.5625 on epoch=574
06/24/2022 05:36:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/24/2022 05:36:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/24/2022 05:36:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/24/2022 05:36:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 05:36:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
06/24/2022 05:36:22 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.5625 on epoch=599
06/24/2022 05:36:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/24/2022 05:36:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
06/24/2022 05:36:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
06/24/2022 05:36:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/24/2022 05:36:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/24/2022 05:36:29 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.53125 on epoch=624
06/24/2022 05:36:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
06/24/2022 05:36:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 05:36:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 05:36:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/24/2022 05:36:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 05:36:36 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.5625 on epoch=649
06/24/2022 05:36:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 05:36:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/24/2022 05:36:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 05:36:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 05:36:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/24/2022 05:36:43 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.59375 on epoch=674
06/24/2022 05:36:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/24/2022 05:36:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/24/2022 05:36:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
06/24/2022 05:36:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 05:36:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/24/2022 05:36:50 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.5625 on epoch=699
06/24/2022 05:36:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 05:36:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 05:36:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
06/24/2022 05:36:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 05:36:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 05:36:56 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.5625 on epoch=724
06/24/2022 05:36:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 05:36:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 05:37:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 05:37:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 05:37:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 05:37:03 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.5625 on epoch=749
06/24/2022 05:37:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 05:37:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 05:37:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 05:37:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
06/24/2022 05:37:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
06/24/2022 05:37:10 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.625 on epoch=774
06/24/2022 05:37:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/24/2022 05:37:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 05:37:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
06/24/2022 05:37:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 05:37:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 05:37:17 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.5625 on epoch=799
06/24/2022 05:37:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 05:37:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 05:37:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 05:37:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 05:37:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
06/24/2022 05:37:24 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.59375 on epoch=824
06/24/2022 05:37:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 05:37:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 05:37:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 05:37:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/24/2022 05:37:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=849
06/24/2022 05:37:31 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.59375 on epoch=849
06/24/2022 05:37:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 05:37:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 05:37:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 05:37:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 05:37:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 05:37:38 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.59375 on epoch=874
06/24/2022 05:37:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 05:37:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 05:37:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/24/2022 05:37:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 05:37:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 05:37:44 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.5625 on epoch=899
06/24/2022 05:37:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=904
06/24/2022 05:37:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 05:37:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 05:37:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=919
06/24/2022 05:37:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 05:37:51 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.5625 on epoch=924
06/24/2022 05:37:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 05:37:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 05:37:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 05:37:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 05:37:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
06/24/2022 05:37:58 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.59375 on epoch=949
06/24/2022 05:37:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 05:38:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 05:38:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 05:38:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 05:38:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 05:38:05 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.59375 on epoch=974
06/24/2022 05:38:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 05:38:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 05:38:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 05:38:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
06/24/2022 05:38:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 05:38:12 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.59375 on epoch=999
06/24/2022 05:38:12 - INFO - __main__ - save last model!
06/24/2022 05:38:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:38:12 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 05:38:12 - INFO - __main__ - Printing 3 examples
06/24/2022 05:38:12 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 05:38:12 - INFO - __main__ - ['equivalent']
06/24/2022 05:38:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 05:38:12 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:38:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 05:38:12 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:38:12 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:38:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:38:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:38:12 - INFO - __main__ - Printing 3 examples
06/24/2022 05:38:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 05:38:12 - INFO - __main__ - ['equivalent']
06/24/2022 05:38:12 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 05:38:12 - INFO - __main__ - ['equivalent']
06/24/2022 05:38:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 05:38:12 - INFO - __main__ - ['equivalent']
06/24/2022 05:38:12 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:38:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:38:12 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:38:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:38:12 - INFO - __main__ - Printing 3 examples
06/24/2022 05:38:12 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 05:38:12 - INFO - __main__ - ['equivalent']
06/24/2022 05:38:12 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 05:38:12 - INFO - __main__ - ['equivalent']
06/24/2022 05:38:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 05:38:12 - INFO - __main__ - ['equivalent']
06/24/2022 05:38:12 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:38:12 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 05:38:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:38:13 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:38:18 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:38:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:38:19 - INFO - __main__ - Starting training!
06/24/2022 05:38:20 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_21_0.3_8_predictions.txt
06/24/2022 05:38:20 - INFO - __main__ - ACC on test data: 0.5613
06/24/2022 05:38:20 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.3, bsz=8, dev_performance=0.75, test_performance=0.5612745098039216
06/24/2022 05:38:20 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.2, bsz=8 ...
06/24/2022 05:38:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:38:21 - INFO - __main__ - Printing 3 examples
06/24/2022 05:38:21 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 05:38:21 - INFO - __main__ - ['equivalent']
06/24/2022 05:38:21 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 05:38:21 - INFO - __main__ - ['equivalent']
06/24/2022 05:38:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 05:38:21 - INFO - __main__ - ['equivalent']
06/24/2022 05:38:21 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:38:21 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:38:21 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:38:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:38:21 - INFO - __main__ - Printing 3 examples
06/24/2022 05:38:21 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 05:38:21 - INFO - __main__ - ['equivalent']
06/24/2022 05:38:21 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 05:38:21 - INFO - __main__ - ['equivalent']
06/24/2022 05:38:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 05:38:21 - INFO - __main__ - ['equivalent']
06/24/2022 05:38:21 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:38:21 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:38:21 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:38:27 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:38:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:38:28 - INFO - __main__ - Starting training!
06/24/2022 05:38:29 - INFO - __main__ - Step 10 Global step 10 Train loss 3.60 on epoch=4
06/24/2022 05:38:30 - INFO - __main__ - Step 20 Global step 20 Train loss 3.14 on epoch=9
06/24/2022 05:38:31 - INFO - __main__ - Step 30 Global step 30 Train loss 2.47 on epoch=14
06/24/2022 05:38:33 - INFO - __main__ - Step 40 Global step 40 Train loss 2.23 on epoch=19
06/24/2022 05:38:34 - INFO - __main__ - Step 50 Global step 50 Train loss 1.87 on epoch=24
06/24/2022 05:38:35 - INFO - __main__ - Global step 50 Train loss 2.66 ACC 0.0 on epoch=24
06/24/2022 05:38:35 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 05:38:36 - INFO - __main__ - Step 60 Global step 60 Train loss 1.52 on epoch=29
06/24/2022 05:38:37 - INFO - __main__ - Step 70 Global step 70 Train loss 1.31 on epoch=34
06/24/2022 05:38:38 - INFO - __main__ - Step 80 Global step 80 Train loss 1.11 on epoch=39
06/24/2022 05:38:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.95 on epoch=44
06/24/2022 05:38:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.79 on epoch=49
06/24/2022 05:38:41 - INFO - __main__ - Global step 100 Train loss 1.14 ACC 0.4375 on epoch=49
06/24/2022 05:38:41 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.4375 on epoch=49, global_step=100
06/24/2022 05:38:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.68 on epoch=54
06/24/2022 05:38:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.66 on epoch=59
06/24/2022 05:38:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.69 on epoch=64
06/24/2022 05:38:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=69
06/24/2022 05:38:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=74
06/24/2022 05:38:48 - INFO - __main__ - Global step 150 Train loss 0.62 ACC 0.5 on epoch=74
06/24/2022 05:38:48 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=74, global_step=150
06/24/2022 05:38:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=79
06/24/2022 05:38:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=84
06/24/2022 05:38:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
06/24/2022 05:38:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=94
06/24/2022 05:38:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.33 on epoch=99
06/24/2022 05:38:55 - INFO - __main__ - Global step 200 Train loss 0.43 ACC 0.5625 on epoch=99
06/24/2022 05:38:55 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=99, global_step=200
06/24/2022 05:38:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=104
06/24/2022 05:38:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=109
06/24/2022 05:38:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
06/24/2022 05:39:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=119
06/24/2022 05:39:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.30 on epoch=124
06/24/2022 05:39:02 - INFO - __main__ - Global step 250 Train loss 0.38 ACC 0.59375 on epoch=124
06/24/2022 05:39:02 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=124, global_step=250
06/24/2022 05:39:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.33 on epoch=129
06/24/2022 05:39:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=134
06/24/2022 05:39:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=139
06/24/2022 05:39:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=144
06/24/2022 05:39:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
06/24/2022 05:39:08 - INFO - __main__ - Global step 300 Train loss 0.32 ACC 0.5625 on epoch=149
06/24/2022 05:39:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=154
06/24/2022 05:39:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.30 on epoch=159
06/24/2022 05:39:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.30 on epoch=164
06/24/2022 05:39:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
06/24/2022 05:39:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.28 on epoch=174
06/24/2022 05:39:15 - INFO - __main__ - Global step 350 Train loss 0.30 ACC 0.5625 on epoch=174
06/24/2022 05:39:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.30 on epoch=179
06/24/2022 05:39:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
06/24/2022 05:39:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=189
06/24/2022 05:39:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
06/24/2022 05:39:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=199
06/24/2022 05:39:22 - INFO - __main__ - Global step 400 Train loss 0.28 ACC 0.5625 on epoch=199
06/24/2022 05:39:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
06/24/2022 05:39:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=209
06/24/2022 05:39:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.29 on epoch=214
06/24/2022 05:39:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=219
06/24/2022 05:39:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=224
06/24/2022 05:39:29 - INFO - __main__ - Global step 450 Train loss 0.28 ACC 0.6875 on epoch=224
06/24/2022 05:39:29 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.6875 on epoch=224, global_step=450
06/24/2022 05:39:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
06/24/2022 05:39:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=234
06/24/2022 05:39:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
06/24/2022 05:39:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
06/24/2022 05:39:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
06/24/2022 05:39:35 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.625 on epoch=249
06/24/2022 05:39:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
06/24/2022 05:39:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=259
06/24/2022 05:39:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=264
06/24/2022 05:39:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=269
06/24/2022 05:39:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
06/24/2022 05:39:42 - INFO - __main__ - Global step 550 Train loss 0.25 ACC 0.625 on epoch=274
06/24/2022 05:39:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=279
06/24/2022 05:39:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=284
06/24/2022 05:39:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=289
06/24/2022 05:39:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=294
06/24/2022 05:39:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=299
06/24/2022 05:39:49 - INFO - __main__ - Global step 600 Train loss 0.19 ACC 0.65625 on epoch=299
06/24/2022 05:39:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=304
06/24/2022 05:39:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=309
06/24/2022 05:39:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.14 on epoch=314
06/24/2022 05:39:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.14 on epoch=319
06/24/2022 05:39:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=324
06/24/2022 05:39:56 - INFO - __main__ - Global step 650 Train loss 0.16 ACC 0.5625 on epoch=324
06/24/2022 05:39:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=329
06/24/2022 05:39:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=334
06/24/2022 05:39:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=339
06/24/2022 05:40:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=344
06/24/2022 05:40:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=349
06/24/2022 05:40:02 - INFO - __main__ - Global step 700 Train loss 0.15 ACC 0.65625 on epoch=349
06/24/2022 05:40:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=354
06/24/2022 05:40:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.15 on epoch=359
06/24/2022 05:40:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=364
06/24/2022 05:40:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=369
06/24/2022 05:40:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=374
06/24/2022 05:40:09 - INFO - __main__ - Global step 750 Train loss 0.14 ACC 0.5625 on epoch=374
06/24/2022 05:40:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=379
06/24/2022 05:40:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=384
06/24/2022 05:40:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=389
06/24/2022 05:40:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=394
06/24/2022 05:40:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=399
06/24/2022 05:40:16 - INFO - __main__ - Global step 800 Train loss 0.13 ACC 0.625 on epoch=399
06/24/2022 05:40:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=404
06/24/2022 05:40:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=409
06/24/2022 05:40:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=414
06/24/2022 05:40:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=419
06/24/2022 05:40:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=424
06/24/2022 05:40:22 - INFO - __main__ - Global step 850 Train loss 0.09 ACC 0.625 on epoch=424
06/24/2022 05:40:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
06/24/2022 05:40:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=434
06/24/2022 05:40:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
06/24/2022 05:40:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=444
06/24/2022 05:40:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
06/24/2022 05:40:29 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.625 on epoch=449
06/24/2022 05:40:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=454
06/24/2022 05:40:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
06/24/2022 05:40:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
06/24/2022 05:40:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=469
06/24/2022 05:40:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=474
06/24/2022 05:40:36 - INFO - __main__ - Global step 950 Train loss 0.06 ACC 0.625 on epoch=474
06/24/2022 05:40:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
06/24/2022 05:40:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=484
06/24/2022 05:40:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=489
06/24/2022 05:40:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
06/24/2022 05:40:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
06/24/2022 05:40:43 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.65625 on epoch=499
06/24/2022 05:40:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
06/24/2022 05:40:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=509
06/24/2022 05:40:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
06/24/2022 05:40:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
06/24/2022 05:40:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
06/24/2022 05:40:49 - INFO - __main__ - Global step 1050 Train loss 0.05 ACC 0.625 on epoch=524
06/24/2022 05:40:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=529
06/24/2022 05:40:52 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=534
06/24/2022 05:40:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
06/24/2022 05:40:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
06/24/2022 05:40:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
06/24/2022 05:40:56 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.625 on epoch=549
06/24/2022 05:40:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=554
06/24/2022 05:40:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
06/24/2022 05:41:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
06/24/2022 05:41:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
06/24/2022 05:41:02 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
06/24/2022 05:41:03 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.625 on epoch=574
06/24/2022 05:41:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/24/2022 05:41:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
06/24/2022 05:41:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
06/24/2022 05:41:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
06/24/2022 05:41:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
06/24/2022 05:41:09 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.625 on epoch=599
06/24/2022 05:41:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/24/2022 05:41:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
06/24/2022 05:41:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/24/2022 05:41:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=619
06/24/2022 05:41:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/24/2022 05:41:16 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.59375 on epoch=624
06/24/2022 05:41:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
06/24/2022 05:41:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
06/24/2022 05:41:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/24/2022 05:41:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
06/24/2022 05:41:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/24/2022 05:41:23 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.625 on epoch=649
06/24/2022 05:41:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
06/24/2022 05:41:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
06/24/2022 05:41:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/24/2022 05:41:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/24/2022 05:41:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/24/2022 05:41:30 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.625 on epoch=674
06/24/2022 05:41:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/24/2022 05:41:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
06/24/2022 05:41:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
06/24/2022 05:41:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
06/24/2022 05:41:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/24/2022 05:41:36 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.625 on epoch=699
06/24/2022 05:41:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 05:41:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/24/2022 05:41:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/24/2022 05:41:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
06/24/2022 05:41:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 05:41:43 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.625 on epoch=724
06/24/2022 05:41:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
06/24/2022 05:41:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
06/24/2022 05:41:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
06/24/2022 05:41:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
06/24/2022 05:41:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 05:41:50 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.625 on epoch=749
06/24/2022 05:41:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/24/2022 05:41:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/24/2022 05:41:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/24/2022 05:41:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/24/2022 05:41:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/24/2022 05:41:57 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.59375 on epoch=774
06/24/2022 05:41:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
06/24/2022 05:41:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/24/2022 05:42:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/24/2022 05:42:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
06/24/2022 05:42:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
06/24/2022 05:42:03 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.625 on epoch=799
06/24/2022 05:42:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/24/2022 05:42:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
06/24/2022 05:42:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/24/2022 05:42:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
06/24/2022 05:42:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 05:42:10 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.625 on epoch=824
06/24/2022 05:42:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/24/2022 05:42:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/24/2022 05:42:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 05:42:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/24/2022 05:42:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/24/2022 05:42:17 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.625 on epoch=849
06/24/2022 05:42:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/24/2022 05:42:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 05:42:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=864
06/24/2022 05:42:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/24/2022 05:42:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=874
06/24/2022 05:42:24 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.625 on epoch=874
06/24/2022 05:42:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/24/2022 05:42:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
06/24/2022 05:42:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
06/24/2022 05:42:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 05:42:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/24/2022 05:42:30 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.625 on epoch=899
06/24/2022 05:42:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=904
06/24/2022 05:42:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 05:42:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/24/2022 05:42:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
06/24/2022 05:42:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 05:42:37 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.59375 on epoch=924
06/24/2022 05:42:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/24/2022 05:42:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/24/2022 05:42:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 05:42:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 05:42:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 05:42:44 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.59375 on epoch=949
06/24/2022 05:42:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/24/2022 05:42:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
06/24/2022 05:42:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/24/2022 05:42:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/24/2022 05:42:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/24/2022 05:42:51 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.59375 on epoch=974
06/24/2022 05:42:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/24/2022 05:42:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 05:42:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 05:42:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 05:42:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=999
06/24/2022 05:42:58 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.59375 on epoch=999
06/24/2022 05:42:58 - INFO - __main__ - save last model!
06/24/2022 05:42:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:42:58 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 05:42:58 - INFO - __main__ - Printing 3 examples
06/24/2022 05:42:58 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 05:42:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:42:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 05:42:58 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:42:58 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 05:42:58 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:42:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:42:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:42:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:42:58 - INFO - __main__ - Printing 3 examples
06/24/2022 05:42:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 05:42:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:42:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 05:42:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:42:58 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 05:42:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:42:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:42:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:42:58 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:42:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:42:58 - INFO - __main__ - Printing 3 examples
06/24/2022 05:42:58 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 05:42:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:42:58 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 05:42:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:42:58 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 05:42:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:42:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:42:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:42:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:42:58 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 05:43:03 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:43:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:43:04 - INFO - __main__ - Starting training!
06/24/2022 05:43:06 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_21_0.2_8_predictions.txt
06/24/2022 05:43:06 - INFO - __main__ - ACC on test data: 0.5833
06/24/2022 05:43:06 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.2, bsz=8, dev_performance=0.6875, test_performance=0.5833333333333334
06/24/2022 05:43:06 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.5, bsz=8 ...
06/24/2022 05:43:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:43:07 - INFO - __main__ - Printing 3 examples
06/24/2022 05:43:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 05:43:07 - INFO - __main__ - ['equivalent']
06/24/2022 05:43:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 05:43:07 - INFO - __main__ - ['equivalent']
06/24/2022 05:43:07 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 05:43:07 - INFO - __main__ - ['equivalent']
06/24/2022 05:43:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:43:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:43:07 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:43:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:43:07 - INFO - __main__ - Printing 3 examples
06/24/2022 05:43:07 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 05:43:07 - INFO - __main__ - ['equivalent']
06/24/2022 05:43:07 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 05:43:07 - INFO - __main__ - ['equivalent']
06/24/2022 05:43:07 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 05:43:07 - INFO - __main__ - ['equivalent']
06/24/2022 05:43:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:43:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:43:07 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:43:13 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:43:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:43:13 - INFO - __main__ - Starting training!
06/24/2022 05:43:15 - INFO - __main__ - Step 10 Global step 10 Train loss 3.16 on epoch=4
06/24/2022 05:43:16 - INFO - __main__ - Step 20 Global step 20 Train loss 1.97 on epoch=9
06/24/2022 05:43:17 - INFO - __main__ - Step 30 Global step 30 Train loss 1.33 on epoch=14
06/24/2022 05:43:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.90 on epoch=19
06/24/2022 05:43:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.69 on epoch=24
06/24/2022 05:43:20 - INFO - __main__ - Global step 50 Train loss 1.61 ACC 0.53125 on epoch=24
06/24/2022 05:43:20 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.53125 on epoch=24, global_step=50
06/24/2022 05:43:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=29
06/24/2022 05:43:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=34
06/24/2022 05:43:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=39
06/24/2022 05:43:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=44
06/24/2022 05:43:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=49
06/24/2022 05:43:27 - INFO - __main__ - Global step 100 Train loss 0.45 ACC 0.53125 on epoch=49
06/24/2022 05:43:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.33 on epoch=54
06/24/2022 05:43:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.34 on epoch=59
06/24/2022 05:43:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=64
06/24/2022 05:43:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.35 on epoch=69
06/24/2022 05:43:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.26 on epoch=74
06/24/2022 05:43:34 - INFO - __main__ - Global step 150 Train loss 0.32 ACC 0.40625 on epoch=74
06/24/2022 05:43:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=79
06/24/2022 05:43:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=84
06/24/2022 05:43:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=89
06/24/2022 05:43:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=94
06/24/2022 05:43:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=99
06/24/2022 05:43:41 - INFO - __main__ - Global step 200 Train loss 0.27 ACC 0.40625 on epoch=99
06/24/2022 05:43:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.17 on epoch=104
06/24/2022 05:43:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.19 on epoch=109
06/24/2022 05:43:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.19 on epoch=114
06/24/2022 05:43:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.16 on epoch=119
06/24/2022 05:43:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.19 on epoch=124
06/24/2022 05:43:47 - INFO - __main__ - Global step 250 Train loss 0.18 ACC 0.4375 on epoch=124
06/24/2022 05:43:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.18 on epoch=129
06/24/2022 05:43:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.16 on epoch=134
06/24/2022 05:43:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.11 on epoch=139
06/24/2022 05:43:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.10 on epoch=144
06/24/2022 05:43:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.13 on epoch=149
06/24/2022 05:43:54 - INFO - __main__ - Global step 300 Train loss 0.13 ACC 0.4375 on epoch=149
06/24/2022 05:43:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.13 on epoch=154
06/24/2022 05:43:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.10 on epoch=159
06/24/2022 05:43:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.11 on epoch=164
06/24/2022 05:43:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.10 on epoch=169
06/24/2022 05:44:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.09 on epoch=174
06/24/2022 05:44:01 - INFO - __main__ - Global step 350 Train loss 0.11 ACC 0.4375 on epoch=174
06/24/2022 05:44:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.06 on epoch=179
06/24/2022 05:44:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.07 on epoch=184
06/24/2022 05:44:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.10 on epoch=189
06/24/2022 05:44:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.08 on epoch=194
06/24/2022 05:44:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.04 on epoch=199
06/24/2022 05:44:08 - INFO - __main__ - Global step 400 Train loss 0.07 ACC 0.46875 on epoch=199
06/24/2022 05:44:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.03 on epoch=204
06/24/2022 05:44:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.05 on epoch=209
06/24/2022 05:44:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
06/24/2022 05:44:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.07 on epoch=219
06/24/2022 05:44:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.05 on epoch=224
06/24/2022 05:44:15 - INFO - __main__ - Global step 450 Train loss 0.06 ACC 0.46875 on epoch=224
06/24/2022 05:44:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.04 on epoch=229
06/24/2022 05:44:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.03 on epoch=234
06/24/2022 05:44:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.04 on epoch=239
06/24/2022 05:44:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.04 on epoch=244
06/24/2022 05:44:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.03 on epoch=249
06/24/2022 05:44:22 - INFO - __main__ - Global step 500 Train loss 0.04 ACC 0.40625 on epoch=249
06/24/2022 05:44:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
06/24/2022 05:44:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
06/24/2022 05:44:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.05 on epoch=264
06/24/2022 05:44:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
06/24/2022 05:44:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=274
06/24/2022 05:44:29 - INFO - __main__ - Global step 550 Train loss 0.06 ACC 0.40625 on epoch=274
06/24/2022 05:44:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
06/24/2022 05:44:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
06/24/2022 05:44:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
06/24/2022 05:44:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
06/24/2022 05:44:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
06/24/2022 05:44:35 - INFO - __main__ - Global step 600 Train loss 0.03 ACC 0.40625 on epoch=299
06/24/2022 05:44:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
06/24/2022 05:44:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
06/24/2022 05:44:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/24/2022 05:44:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
06/24/2022 05:44:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
06/24/2022 05:44:42 - INFO - __main__ - Global step 650 Train loss 0.03 ACC 0.375 on epoch=324
06/24/2022 05:44:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
06/24/2022 05:44:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
06/24/2022 05:44:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
06/24/2022 05:44:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
06/24/2022 05:44:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
06/24/2022 05:44:49 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.46875 on epoch=349
06/24/2022 05:44:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
06/24/2022 05:44:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
06/24/2022 05:44:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/24/2022 05:44:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/24/2022 05:44:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
06/24/2022 05:44:56 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.5 on epoch=374
06/24/2022 05:44:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
06/24/2022 05:44:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/24/2022 05:45:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
06/24/2022 05:45:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/24/2022 05:45:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
06/24/2022 05:45:03 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.40625 on epoch=399
06/24/2022 05:45:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
06/24/2022 05:45:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
06/24/2022 05:45:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
06/24/2022 05:45:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
06/24/2022 05:45:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
06/24/2022 05:45:10 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.4375 on epoch=424
06/24/2022 05:45:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/24/2022 05:45:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=434
06/24/2022 05:45:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
06/24/2022 05:45:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=444
06/24/2022 05:45:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/24/2022 05:45:16 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.5 on epoch=449
06/24/2022 05:45:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/24/2022 05:45:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=459
06/24/2022 05:45:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
06/24/2022 05:45:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/24/2022 05:45:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/24/2022 05:45:23 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.5 on epoch=474
06/24/2022 05:45:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
06/24/2022 05:45:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/24/2022 05:45:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
06/24/2022 05:45:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
06/24/2022 05:45:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/24/2022 05:45:30 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.5 on epoch=499
06/24/2022 05:45:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/24/2022 05:45:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/24/2022 05:45:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
06/24/2022 05:45:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=519
06/24/2022 05:45:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
06/24/2022 05:45:37 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.5 on epoch=524
06/24/2022 05:45:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/24/2022 05:45:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/24/2022 05:45:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
06/24/2022 05:45:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/24/2022 05:45:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
06/24/2022 05:45:44 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.46875 on epoch=549
06/24/2022 05:45:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/24/2022 05:45:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
06/24/2022 05:45:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 05:45:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/24/2022 05:45:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/24/2022 05:45:51 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.46875 on epoch=574
06/24/2022 05:45:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/24/2022 05:45:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
06/24/2022 05:45:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
06/24/2022 05:45:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=594
06/24/2022 05:45:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/24/2022 05:45:58 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.46875 on epoch=599
06/24/2022 05:45:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/24/2022 05:46:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/24/2022 05:46:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/24/2022 05:46:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/24/2022 05:46:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/24/2022 05:46:04 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.46875 on epoch=624
06/24/2022 05:46:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/24/2022 05:46:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
06/24/2022 05:46:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 05:46:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 05:46:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 05:46:11 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.46875 on epoch=649
06/24/2022 05:46:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/24/2022 05:46:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/24/2022 05:46:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/24/2022 05:46:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 05:46:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/24/2022 05:46:18 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.5 on epoch=674
06/24/2022 05:46:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 05:46:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 05:46:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 05:46:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 05:46:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 05:46:25 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.5 on epoch=699
06/24/2022 05:46:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 05:46:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/24/2022 05:46:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
06/24/2022 05:46:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 05:46:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
06/24/2022 05:46:32 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.46875 on epoch=724
06/24/2022 05:46:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=729
06/24/2022 05:46:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
06/24/2022 05:46:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 05:46:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 05:46:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/24/2022 05:46:39 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.5 on epoch=749
06/24/2022 05:46:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
06/24/2022 05:46:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 05:46:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 05:46:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/24/2022 05:46:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 05:46:46 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.5 on epoch=774
06/24/2022 05:46:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 05:46:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 05:46:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 05:46:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=794
06/24/2022 05:46:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 05:46:53 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.5 on epoch=799
06/24/2022 05:46:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 05:46:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 05:46:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/24/2022 05:46:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 05:46:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 05:47:00 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.5 on epoch=824
06/24/2022 05:47:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/24/2022 05:47:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 05:47:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/24/2022 05:47:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 05:47:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 05:47:06 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.5 on epoch=849
06/24/2022 05:47:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 05:47:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 05:47:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/24/2022 05:47:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 05:47:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 05:47:13 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.4375 on epoch=874
06/24/2022 05:47:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/24/2022 05:47:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 05:47:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 05:47:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=894
06/24/2022 05:47:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=899
06/24/2022 05:47:20 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.4375 on epoch=899
06/24/2022 05:47:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 05:47:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/24/2022 05:47:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
06/24/2022 05:47:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
06/24/2022 05:47:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/24/2022 05:47:27 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.53125 on epoch=924
06/24/2022 05:47:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 05:47:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 05:47:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
06/24/2022 05:47:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/24/2022 05:47:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 05:47:34 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.4375 on epoch=949
06/24/2022 05:47:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 05:47:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 05:47:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/24/2022 05:47:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 05:47:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
06/24/2022 05:47:41 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.40625 on epoch=974
06/24/2022 05:47:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 05:47:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 05:47:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 05:47:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
06/24/2022 05:47:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
06/24/2022 05:47:48 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.4375 on epoch=999
06/24/2022 05:47:48 - INFO - __main__ - save last model!
06/24/2022 05:47:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:47:48 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 05:47:48 - INFO - __main__ - Printing 3 examples
06/24/2022 05:47:48 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 05:47:48 - INFO - __main__ - ['equivalent']
06/24/2022 05:47:48 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 05:47:48 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:47:48 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 05:47:48 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:47:48 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:47:48 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:47:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:47:49 - INFO - __main__ - Printing 3 examples
06/24/2022 05:47:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 05:47:49 - INFO - __main__ - ['equivalent']
06/24/2022 05:47:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 05:47:49 - INFO - __main__ - ['equivalent']
06/24/2022 05:47:49 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 05:47:49 - INFO - __main__ - ['equivalent']
06/24/2022 05:47:49 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:47:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:47:49 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:47:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:47:49 - INFO - __main__ - Printing 3 examples
06/24/2022 05:47:49 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 05:47:49 - INFO - __main__ - ['equivalent']
06/24/2022 05:47:49 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 05:47:49 - INFO - __main__ - ['equivalent']
06/24/2022 05:47:49 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 05:47:49 - INFO - __main__ - ['equivalent']
06/24/2022 05:47:49 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:47:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:47:49 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:47:49 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 05:47:55 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:47:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:47:55 - INFO - __main__ - Starting training!
06/24/2022 05:47:57 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_42_0.5_8_predictions.txt
06/24/2022 05:47:57 - INFO - __main__ - ACC on test data: 0.4289
06/24/2022 05:47:57 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.5, bsz=8, dev_performance=0.53125, test_performance=0.42892156862745096
06/24/2022 05:47:57 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.4, bsz=8 ...
06/24/2022 05:47:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:47:58 - INFO - __main__ - Printing 3 examples
06/24/2022 05:47:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 05:47:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:47:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 05:47:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:47:58 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 05:47:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:47:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:47:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:47:58 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:47:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:47:58 - INFO - __main__ - Printing 3 examples
06/24/2022 05:47:58 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 05:47:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:47:58 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 05:47:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:47:58 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 05:47:58 - INFO - __main__ - ['equivalent']
06/24/2022 05:47:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:47:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:47:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:48:04 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:48:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:48:04 - INFO - __main__ - Starting training!
06/24/2022 05:48:06 - INFO - __main__ - Step 10 Global step 10 Train loss 3.32 on epoch=4
06/24/2022 05:48:07 - INFO - __main__ - Step 20 Global step 20 Train loss 2.24 on epoch=9
06/24/2022 05:48:08 - INFO - __main__ - Step 30 Global step 30 Train loss 1.58 on epoch=14
06/24/2022 05:48:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.08 on epoch=19
06/24/2022 05:48:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.90 on epoch=24
06/24/2022 05:48:11 - INFO - __main__ - Global step 50 Train loss 1.82 ACC 0.40625 on epoch=24
06/24/2022 05:48:11 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.40625 on epoch=24, global_step=50
06/24/2022 05:48:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.70 on epoch=29
06/24/2022 05:48:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=34
06/24/2022 05:48:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=39
06/24/2022 05:48:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=44
06/24/2022 05:48:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=49
06/24/2022 05:48:18 - INFO - __main__ - Global step 100 Train loss 0.53 ACC 0.46875 on epoch=49
06/24/2022 05:48:18 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.46875 on epoch=49, global_step=100
06/24/2022 05:48:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=54
06/24/2022 05:48:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.35 on epoch=59
06/24/2022 05:48:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=64
06/24/2022 05:48:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=69
06/24/2022 05:48:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.32 on epoch=74
06/24/2022 05:48:25 - INFO - __main__ - Global step 150 Train loss 0.34 ACC 0.46875 on epoch=74
06/24/2022 05:48:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
06/24/2022 05:48:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=84
06/24/2022 05:48:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.22 on epoch=89
06/24/2022 05:48:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.24 on epoch=94
06/24/2022 05:48:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=99
06/24/2022 05:48:32 - INFO - __main__ - Global step 200 Train loss 0.26 ACC 0.46875 on epoch=99
06/24/2022 05:48:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.19 on epoch=104
06/24/2022 05:48:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.18 on epoch=109
06/24/2022 05:48:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.18 on epoch=114
06/24/2022 05:48:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.20 on epoch=119
06/24/2022 05:48:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.18 on epoch=124
06/24/2022 05:48:39 - INFO - __main__ - Global step 250 Train loss 0.19 ACC 0.53125 on epoch=124
06/24/2022 05:48:39 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.53125 on epoch=124, global_step=250
06/24/2022 05:48:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.15 on epoch=129
06/24/2022 05:48:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.13 on epoch=134
06/24/2022 05:48:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.14 on epoch=139
06/24/2022 05:48:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.14 on epoch=144
06/24/2022 05:48:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.14 on epoch=149
06/24/2022 05:48:45 - INFO - __main__ - Global step 300 Train loss 0.14 ACC 0.53125 on epoch=149
06/24/2022 05:48:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.09 on epoch=154
06/24/2022 05:48:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.13 on epoch=159
06/24/2022 05:48:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.07 on epoch=164
06/24/2022 05:48:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.15 on epoch=169
06/24/2022 05:48:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.08 on epoch=174
06/24/2022 05:48:52 - INFO - __main__ - Global step 350 Train loss 0.10 ACC 0.5625 on epoch=174
06/24/2022 05:48:52 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=174, global_step=350
06/24/2022 05:48:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.08 on epoch=179
06/24/2022 05:48:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.09 on epoch=184
06/24/2022 05:48:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.09 on epoch=189
06/24/2022 05:48:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.07 on epoch=194
06/24/2022 05:48:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.06 on epoch=199
06/24/2022 05:48:59 - INFO - __main__ - Global step 400 Train loss 0.08 ACC 0.53125 on epoch=199
06/24/2022 05:49:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.07 on epoch=204
06/24/2022 05:49:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.06 on epoch=209
06/24/2022 05:49:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.06 on epoch=214
06/24/2022 05:49:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.04 on epoch=219
06/24/2022 05:49:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.04 on epoch=224
06/24/2022 05:49:06 - INFO - __main__ - Global step 450 Train loss 0.05 ACC 0.5 on epoch=224
06/24/2022 05:49:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=229
06/24/2022 05:49:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.04 on epoch=234
06/24/2022 05:49:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
06/24/2022 05:49:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
06/24/2022 05:49:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.08 on epoch=249
06/24/2022 05:49:13 - INFO - __main__ - Global step 500 Train loss 0.06 ACC 0.53125 on epoch=249
06/24/2022 05:49:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
06/24/2022 05:49:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
06/24/2022 05:49:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
06/24/2022 05:49:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.04 on epoch=269
06/24/2022 05:49:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
06/24/2022 05:49:20 - INFO - __main__ - Global step 550 Train loss 0.05 ACC 0.4375 on epoch=274
06/24/2022 05:49:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
06/24/2022 05:49:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
06/24/2022 05:49:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
06/24/2022 05:49:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=294
06/24/2022 05:49:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
06/24/2022 05:49:26 - INFO - __main__ - Global step 600 Train loss 0.04 ACC 0.4375 on epoch=299
06/24/2022 05:49:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=304
06/24/2022 05:49:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
06/24/2022 05:49:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
06/24/2022 05:49:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
06/24/2022 05:49:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/24/2022 05:49:33 - INFO - __main__ - Global step 650 Train loss 0.03 ACC 0.46875 on epoch=324
06/24/2022 05:49:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
06/24/2022 05:49:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
06/24/2022 05:49:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
06/24/2022 05:49:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
06/24/2022 05:49:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
06/24/2022 05:49:40 - INFO - __main__ - Global step 700 Train loss 0.02 ACC 0.4375 on epoch=349
06/24/2022 05:49:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
06/24/2022 05:49:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
06/24/2022 05:49:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/24/2022 05:49:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
06/24/2022 05:49:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
06/24/2022 05:49:47 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.4375 on epoch=374
06/24/2022 05:49:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
06/24/2022 05:49:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
06/24/2022 05:49:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
06/24/2022 05:49:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
06/24/2022 05:49:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=399
06/24/2022 05:49:54 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.4375 on epoch=399
06/24/2022 05:49:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
06/24/2022 05:49:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=409
06/24/2022 05:49:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
06/24/2022 05:49:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
06/24/2022 05:50:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
06/24/2022 05:50:01 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.46875 on epoch=424
06/24/2022 05:50:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=429
06/24/2022 05:50:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
06/24/2022 05:50:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
06/24/2022 05:50:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
06/24/2022 05:50:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
06/24/2022 05:50:08 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.46875 on epoch=449
06/24/2022 05:50:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
06/24/2022 05:50:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/24/2022 05:50:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/24/2022 05:50:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/24/2022 05:50:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
06/24/2022 05:50:14 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.46875 on epoch=474
06/24/2022 05:50:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/24/2022 05:50:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/24/2022 05:50:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/24/2022 05:50:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/24/2022 05:50:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/24/2022 05:50:21 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.46875 on epoch=499
06/24/2022 05:50:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/24/2022 05:50:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/24/2022 05:50:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/24/2022 05:50:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/24/2022 05:50:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/24/2022 05:50:28 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.46875 on epoch=524
06/24/2022 05:50:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/24/2022 05:50:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
06/24/2022 05:50:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
06/24/2022 05:50:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
06/24/2022 05:50:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=549
06/24/2022 05:50:35 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.46875 on epoch=549
06/24/2022 05:50:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/24/2022 05:50:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
06/24/2022 05:50:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/24/2022 05:50:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/24/2022 05:50:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/24/2022 05:50:42 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.375 on epoch=574
06/24/2022 05:50:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/24/2022 05:50:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=584
06/24/2022 05:50:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/24/2022 05:50:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
06/24/2022 05:50:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
06/24/2022 05:50:49 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.4375 on epoch=599
06/24/2022 05:50:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
06/24/2022 05:50:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
06/24/2022 05:50:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
06/24/2022 05:50:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/24/2022 05:50:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/24/2022 05:50:55 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.4375 on epoch=624
06/24/2022 05:50:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/24/2022 05:50:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=634
06/24/2022 05:50:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
06/24/2022 05:51:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
06/24/2022 05:51:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 05:51:02 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.46875 on epoch=649
06/24/2022 05:51:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/24/2022 05:51:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
06/24/2022 05:51:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/24/2022 05:51:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/24/2022 05:51:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 05:51:09 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.46875 on epoch=674
06/24/2022 05:51:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/24/2022 05:51:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/24/2022 05:51:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
06/24/2022 05:51:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/24/2022 05:51:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=699
06/24/2022 05:51:16 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.4375 on epoch=699
06/24/2022 05:51:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 05:51:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 05:51:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 05:51:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 05:51:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 05:51:23 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.40625 on epoch=724
06/24/2022 05:51:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
06/24/2022 05:51:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=734
06/24/2022 05:51:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 05:51:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 05:51:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/24/2022 05:51:30 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.4375 on epoch=749
06/24/2022 05:51:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/24/2022 05:51:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 05:51:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 05:51:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 05:51:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/24/2022 05:51:36 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.40625 on epoch=774
06/24/2022 05:51:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/24/2022 05:51:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/24/2022 05:51:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/24/2022 05:51:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/24/2022 05:51:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 05:51:43 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.4375 on epoch=799
06/24/2022 05:51:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
06/24/2022 05:51:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 05:51:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/24/2022 05:51:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 05:51:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=824
06/24/2022 05:51:51 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.4375 on epoch=824
06/24/2022 05:51:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
06/24/2022 05:51:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
06/24/2022 05:51:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 05:51:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 05:51:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/24/2022 05:51:57 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.4375 on epoch=849
06/24/2022 05:51:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 05:52:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 05:52:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/24/2022 05:52:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
06/24/2022 05:52:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 05:52:04 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.4375 on epoch=874
06/24/2022 05:52:05 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/24/2022 05:52:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 05:52:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
06/24/2022 05:52:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/24/2022 05:52:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 05:52:11 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.4375 on epoch=899
06/24/2022 05:52:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/24/2022 05:52:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/24/2022 05:52:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 05:52:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=919
06/24/2022 05:52:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/24/2022 05:52:18 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.40625 on epoch=924
06/24/2022 05:52:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
06/24/2022 05:52:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=934
06/24/2022 05:52:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 05:52:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 05:52:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 05:52:25 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.40625 on epoch=949
06/24/2022 05:52:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 05:52:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 05:52:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 05:52:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
06/24/2022 05:52:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 05:52:31 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.4375 on epoch=974
06/24/2022 05:52:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
06/24/2022 05:52:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 05:52:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 05:52:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=994
06/24/2022 05:52:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/24/2022 05:52:38 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.4375 on epoch=999
06/24/2022 05:52:38 - INFO - __main__ - save last model!
06/24/2022 05:52:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:52:38 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 05:52:38 - INFO - __main__ - Printing 3 examples
06/24/2022 05:52:38 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 05:52:38 - INFO - __main__ - ['equivalent']
06/24/2022 05:52:38 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 05:52:38 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:52:38 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 05:52:38 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:52:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:52:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:52:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:52:39 - INFO - __main__ - Printing 3 examples
06/24/2022 05:52:39 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 05:52:39 - INFO - __main__ - ['equivalent']
06/24/2022 05:52:39 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 05:52:39 - INFO - __main__ - ['equivalent']
06/24/2022 05:52:39 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 05:52:39 - INFO - __main__ - ['equivalent']
06/24/2022 05:52:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:52:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:52:39 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:52:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:52:39 - INFO - __main__ - Printing 3 examples
06/24/2022 05:52:39 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 05:52:39 - INFO - __main__ - ['equivalent']
06/24/2022 05:52:39 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 05:52:39 - INFO - __main__ - ['equivalent']
06/24/2022 05:52:39 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 05:52:39 - INFO - __main__ - ['equivalent']
06/24/2022 05:52:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:52:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:52:39 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:52:39 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 05:52:44 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:52:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:52:45 - INFO - __main__ - Starting training!
06/24/2022 05:52:47 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_42_0.4_8_predictions.txt
06/24/2022 05:52:47 - INFO - __main__ - ACC on test data: 0.4877
06/24/2022 05:52:47 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.4, bsz=8, dev_performance=0.5625, test_performance=0.4877450980392157
06/24/2022 05:52:47 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.3, bsz=8 ...
06/24/2022 05:52:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:52:48 - INFO - __main__ - Printing 3 examples
06/24/2022 05:52:48 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 05:52:48 - INFO - __main__ - ['equivalent']
06/24/2022 05:52:48 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 05:52:48 - INFO - __main__ - ['equivalent']
06/24/2022 05:52:48 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 05:52:48 - INFO - __main__ - ['equivalent']
06/24/2022 05:52:48 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:52:48 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:52:48 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:52:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:52:48 - INFO - __main__ - Printing 3 examples
06/24/2022 05:52:48 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 05:52:48 - INFO - __main__ - ['equivalent']
06/24/2022 05:52:48 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 05:52:48 - INFO - __main__ - ['equivalent']
06/24/2022 05:52:48 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 05:52:48 - INFO - __main__ - ['equivalent']
06/24/2022 05:52:48 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:52:48 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:52:48 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:52:54 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:52:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:52:54 - INFO - __main__ - Starting training!
06/24/2022 05:52:56 - INFO - __main__ - Step 10 Global step 10 Train loss 3.43 on epoch=4
06/24/2022 05:52:57 - INFO - __main__ - Step 20 Global step 20 Train loss 2.68 on epoch=9
06/24/2022 05:52:58 - INFO - __main__ - Step 30 Global step 30 Train loss 2.07 on epoch=14
06/24/2022 05:53:00 - INFO - __main__ - Step 40 Global step 40 Train loss 1.69 on epoch=19
06/24/2022 05:53:01 - INFO - __main__ - Step 50 Global step 50 Train loss 1.26 on epoch=24
06/24/2022 05:53:02 - INFO - __main__ - Global step 50 Train loss 2.23 ACC 0.125 on epoch=24
06/24/2022 05:53:02 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.125 on epoch=24, global_step=50
06/24/2022 05:53:03 - INFO - __main__ - Step 60 Global step 60 Train loss 0.90 on epoch=29
06/24/2022 05:53:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.74 on epoch=34
06/24/2022 05:53:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.66 on epoch=39
06/24/2022 05:53:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.63 on epoch=44
06/24/2022 05:53:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.58 on epoch=49
06/24/2022 05:53:08 - INFO - __main__ - Global step 100 Train loss 0.71 ACC 0.5 on epoch=49
06/24/2022 05:53:09 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.5 on epoch=49, global_step=100
06/24/2022 05:53:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=54
06/24/2022 05:53:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=59
06/24/2022 05:53:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=64
06/24/2022 05:53:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=69
06/24/2022 05:53:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
06/24/2022 05:53:15 - INFO - __main__ - Global step 150 Train loss 0.43 ACC 0.53125 on epoch=74
06/24/2022 05:53:15 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=74, global_step=150
06/24/2022 05:53:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=79
06/24/2022 05:53:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=84
06/24/2022 05:53:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
06/24/2022 05:53:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.33 on epoch=94
06/24/2022 05:53:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=99
06/24/2022 05:53:22 - INFO - __main__ - Global step 200 Train loss 0.37 ACC 0.5 on epoch=99
06/24/2022 05:53:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.34 on epoch=104
06/24/2022 05:53:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
06/24/2022 05:53:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.34 on epoch=114
06/24/2022 05:53:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.36 on epoch=119
06/24/2022 05:53:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=124
06/24/2022 05:53:29 - INFO - __main__ - Global step 250 Train loss 0.34 ACC 0.65625 on epoch=124
06/24/2022 05:53:29 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.65625 on epoch=124, global_step=250
06/24/2022 05:53:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.30 on epoch=129
06/24/2022 05:53:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=134
06/24/2022 05:53:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=139
06/24/2022 05:53:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
06/24/2022 05:53:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
06/24/2022 05:53:36 - INFO - __main__ - Global step 300 Train loss 0.29 ACC 0.5625 on epoch=149
06/24/2022 05:53:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=154
06/24/2022 05:53:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
06/24/2022 05:53:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
06/24/2022 05:53:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=169
06/24/2022 05:53:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=174
06/24/2022 05:53:43 - INFO - __main__ - Global step 350 Train loss 0.29 ACC 0.59375 on epoch=174
06/24/2022 05:53:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
06/24/2022 05:53:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=184
06/24/2022 05:53:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
06/24/2022 05:53:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=194
06/24/2022 05:53:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
06/24/2022 05:53:49 - INFO - __main__ - Global step 400 Train loss 0.26 ACC 0.5625 on epoch=199
06/24/2022 05:53:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
06/24/2022 05:53:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
06/24/2022 05:53:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
06/24/2022 05:53:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=219
06/24/2022 05:53:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=224
06/24/2022 05:53:56 - INFO - __main__ - Global step 450 Train loss 0.23 ACC 0.46875 on epoch=224
06/24/2022 05:53:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=229
06/24/2022 05:53:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=234
06/24/2022 05:54:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=239
06/24/2022 05:54:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=244
06/24/2022 05:54:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=249
06/24/2022 05:54:03 - INFO - __main__ - Global step 500 Train loss 0.19 ACC 0.5 on epoch=249
06/24/2022 05:54:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.12 on epoch=254
06/24/2022 05:54:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.12 on epoch=259
06/24/2022 05:54:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
06/24/2022 05:54:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=269
06/24/2022 05:54:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.09 on epoch=274
06/24/2022 05:54:10 - INFO - __main__ - Global step 550 Train loss 0.10 ACC 0.375 on epoch=274
06/24/2022 05:54:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=279
06/24/2022 05:54:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.10 on epoch=284
06/24/2022 05:54:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
06/24/2022 05:54:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=294
06/24/2022 05:54:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
06/24/2022 05:54:17 - INFO - __main__ - Global step 600 Train loss 0.08 ACC 0.4375 on epoch=299
06/24/2022 05:54:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
06/24/2022 05:54:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=309
06/24/2022 05:54:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
06/24/2022 05:54:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
06/24/2022 05:54:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=324
06/24/2022 05:54:23 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.46875 on epoch=324
06/24/2022 05:54:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
06/24/2022 05:54:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.07 on epoch=334
06/24/2022 05:54:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
06/24/2022 05:54:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
06/24/2022 05:54:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
06/24/2022 05:54:30 - INFO - __main__ - Global step 700 Train loss 0.05 ACC 0.46875 on epoch=349
06/24/2022 05:54:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=354
06/24/2022 05:54:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=359
06/24/2022 05:54:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
06/24/2022 05:54:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
06/24/2022 05:54:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
06/24/2022 05:54:37 - INFO - __main__ - Global step 750 Train loss 0.05 ACC 0.46875 on epoch=374
06/24/2022 05:54:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
06/24/2022 05:54:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
06/24/2022 05:54:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=389
06/24/2022 05:54:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
06/24/2022 05:54:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
06/24/2022 05:54:44 - INFO - __main__ - Global step 800 Train loss 0.04 ACC 0.5 on epoch=399
06/24/2022 05:54:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
06/24/2022 05:54:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
06/24/2022 05:54:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
06/24/2022 05:54:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
06/24/2022 05:54:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
06/24/2022 05:54:51 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.5 on epoch=424
06/24/2022 05:54:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
06/24/2022 05:54:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
06/24/2022 05:54:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/24/2022 05:54:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
06/24/2022 05:54:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
06/24/2022 05:54:57 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.5 on epoch=449
06/24/2022 05:54:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=454
06/24/2022 05:55:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
06/24/2022 05:55:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
06/24/2022 05:55:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
06/24/2022 05:55:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
06/24/2022 05:55:04 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.53125 on epoch=474
06/24/2022 05:55:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
06/24/2022 05:55:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/24/2022 05:55:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/24/2022 05:55:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=494
06/24/2022 05:55:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
06/24/2022 05:55:11 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.5 on epoch=499
06/24/2022 05:55:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
06/24/2022 05:55:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/24/2022 05:55:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
06/24/2022 05:55:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/24/2022 05:55:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
06/24/2022 05:55:18 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.5 on epoch=524
06/24/2022 05:55:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
06/24/2022 05:55:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
06/24/2022 05:55:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/24/2022 05:55:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
06/24/2022 05:55:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/24/2022 05:55:25 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.53125 on epoch=549
06/24/2022 05:55:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/24/2022 05:55:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/24/2022 05:55:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
06/24/2022 05:55:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
06/24/2022 05:55:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/24/2022 05:55:31 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.5 on epoch=574
06/24/2022 05:55:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=579
06/24/2022 05:55:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=584
06/24/2022 05:55:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
06/24/2022 05:55:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/24/2022 05:55:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
06/24/2022 05:55:38 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.53125 on epoch=599
06/24/2022 05:55:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
06/24/2022 05:55:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/24/2022 05:55:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/24/2022 05:55:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
06/24/2022 05:55:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=624
06/24/2022 05:55:45 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.5625 on epoch=624
06/24/2022 05:55:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
06/24/2022 05:55:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/24/2022 05:55:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/24/2022 05:55:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/24/2022 05:55:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
06/24/2022 05:55:52 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.5625 on epoch=649
06/24/2022 05:55:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
06/24/2022 05:55:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
06/24/2022 05:55:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
06/24/2022 05:55:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/24/2022 05:55:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/24/2022 05:55:59 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.59375 on epoch=674
06/24/2022 05:56:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 05:56:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/24/2022 05:56:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
06/24/2022 05:56:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/24/2022 05:56:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/24/2022 05:56:05 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.5625 on epoch=699
06/24/2022 05:56:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 05:56:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
06/24/2022 05:56:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/24/2022 05:56:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=719
06/24/2022 05:56:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/24/2022 05:56:12 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.59375 on epoch=724
06/24/2022 05:56:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
06/24/2022 05:56:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=734
06/24/2022 05:56:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=739
06/24/2022 05:56:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/24/2022 05:56:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/24/2022 05:56:19 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.59375 on epoch=749
06/24/2022 05:56:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/24/2022 05:56:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/24/2022 05:56:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/24/2022 05:56:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/24/2022 05:56:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
06/24/2022 05:56:26 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.59375 on epoch=774
06/24/2022 05:56:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
06/24/2022 05:56:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/24/2022 05:56:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/24/2022 05:56:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
06/24/2022 05:56:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
06/24/2022 05:56:33 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.53125 on epoch=799
06/24/2022 05:56:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=804
06/24/2022 05:56:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/24/2022 05:56:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/24/2022 05:56:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=819
06/24/2022 05:56:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
06/24/2022 05:56:39 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.5625 on epoch=824
06/24/2022 05:56:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 05:56:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
06/24/2022 05:56:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=839
06/24/2022 05:56:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/24/2022 05:56:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 05:56:46 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.53125 on epoch=849
06/24/2022 05:56:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/24/2022 05:56:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/24/2022 05:56:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 05:56:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 05:56:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/24/2022 05:56:53 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.53125 on epoch=874
06/24/2022 05:56:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
06/24/2022 05:56:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
06/24/2022 05:56:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 05:56:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
06/24/2022 05:56:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/24/2022 05:57:00 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.53125 on epoch=899
06/24/2022 05:57:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
06/24/2022 05:57:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/24/2022 05:57:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
06/24/2022 05:57:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=919
06/24/2022 05:57:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=924
06/24/2022 05:57:07 - INFO - __main__ - Global step 1850 Train loss 0.03 ACC 0.53125 on epoch=924
06/24/2022 05:57:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 05:57:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 05:57:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=939
06/24/2022 05:57:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=944
06/24/2022 05:57:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=949
06/24/2022 05:57:14 - INFO - __main__ - Global step 1900 Train loss 0.04 ACC 0.4375 on epoch=949
06/24/2022 05:57:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=954
06/24/2022 05:57:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/24/2022 05:57:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=964
06/24/2022 05:57:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=969
06/24/2022 05:57:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
06/24/2022 05:57:21 - INFO - __main__ - Global step 1950 Train loss 0.04 ACC 0.4375 on epoch=974
06/24/2022 05:57:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/24/2022 05:57:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/24/2022 05:57:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=989
06/24/2022 05:57:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 05:57:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/24/2022 05:57:27 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.5 on epoch=999
06/24/2022 05:57:27 - INFO - __main__ - save last model!
06/24/2022 05:57:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:57:27 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 05:57:27 - INFO - __main__ - Printing 3 examples
06/24/2022 05:57:27 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 05:57:27 - INFO - __main__ - ['equivalent']
06/24/2022 05:57:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 05:57:27 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:57:27 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 05:57:27 - INFO - __main__ - ['not_equivalent']
06/24/2022 05:57:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:57:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:57:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:57:28 - INFO - __main__ - Printing 3 examples
06/24/2022 05:57:28 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 05:57:28 - INFO - __main__ - ['equivalent']
06/24/2022 05:57:28 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 05:57:28 - INFO - __main__ - ['equivalent']
06/24/2022 05:57:28 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 05:57:28 - INFO - __main__ - ['equivalent']
06/24/2022 05:57:28 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:57:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:57:28 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:57:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:57:28 - INFO - __main__ - Printing 3 examples
06/24/2022 05:57:28 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 05:57:28 - INFO - __main__ - ['equivalent']
06/24/2022 05:57:28 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 05:57:28 - INFO - __main__ - ['equivalent']
06/24/2022 05:57:28 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 05:57:28 - INFO - __main__ - ['equivalent']
06/24/2022 05:57:28 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:57:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:57:28 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:57:28 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 05:57:33 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:57:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:57:34 - INFO - __main__ - Starting training!
06/24/2022 05:57:36 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_42_0.3_8_predictions.txt
06/24/2022 05:57:36 - INFO - __main__ - ACC on test data: 0.5074
06/24/2022 05:57:37 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.3, bsz=8, dev_performance=0.65625, test_performance=0.5073529411764706
06/24/2022 05:57:37 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.2, bsz=8 ...
06/24/2022 05:57:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:57:37 - INFO - __main__ - Printing 3 examples
06/24/2022 05:57:37 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 05:57:37 - INFO - __main__ - ['equivalent']
06/24/2022 05:57:37 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 05:57:37 - INFO - __main__ - ['equivalent']
06/24/2022 05:57:37 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 05:57:37 - INFO - __main__ - ['equivalent']
06/24/2022 05:57:37 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:57:37 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:57:37 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 05:57:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:57:37 - INFO - __main__ - Printing 3 examples
06/24/2022 05:57:37 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 05:57:37 - INFO - __main__ - ['equivalent']
06/24/2022 05:57:37 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 05:57:37 - INFO - __main__ - ['equivalent']
06/24/2022 05:57:37 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 05:57:37 - INFO - __main__ - ['equivalent']
06/24/2022 05:57:37 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:57:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:57:38 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:57:43 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:57:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:57:44 - INFO - __main__ - Starting training!
06/24/2022 05:57:45 - INFO - __main__ - Step 10 Global step 10 Train loss 3.55 on epoch=4
06/24/2022 05:57:46 - INFO - __main__ - Step 20 Global step 20 Train loss 2.78 on epoch=9
06/24/2022 05:57:48 - INFO - __main__ - Step 30 Global step 30 Train loss 2.55 on epoch=14
06/24/2022 05:57:49 - INFO - __main__ - Step 40 Global step 40 Train loss 2.10 on epoch=19
06/24/2022 05:57:50 - INFO - __main__ - Step 50 Global step 50 Train loss 1.72 on epoch=24
06/24/2022 05:57:51 - INFO - __main__ - Global step 50 Train loss 2.54 ACC 0.0 on epoch=24
06/24/2022 05:57:51 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 05:57:52 - INFO - __main__ - Step 60 Global step 60 Train loss 1.51 on epoch=29
06/24/2022 05:57:53 - INFO - __main__ - Step 70 Global step 70 Train loss 1.14 on epoch=34
06/24/2022 05:57:55 - INFO - __main__ - Step 80 Global step 80 Train loss 1.07 on epoch=39
06/24/2022 05:57:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.87 on epoch=44
06/24/2022 05:57:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.75 on epoch=49
06/24/2022 05:57:58 - INFO - __main__ - Global step 100 Train loss 1.07 ACC 0.53125 on epoch=49
06/24/2022 05:57:58 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.53125 on epoch=49, global_step=100
06/24/2022 05:57:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.69 on epoch=54
06/24/2022 05:58:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.67 on epoch=59
06/24/2022 05:58:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.63 on epoch=64
06/24/2022 05:58:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=69
06/24/2022 05:58:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=74
06/24/2022 05:58:05 - INFO - __main__ - Global step 150 Train loss 0.61 ACC 0.5 on epoch=74
06/24/2022 05:58:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=79
06/24/2022 05:58:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=84
06/24/2022 05:58:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=89
06/24/2022 05:58:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=94
06/24/2022 05:58:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=99
06/24/2022 05:58:11 - INFO - __main__ - Global step 200 Train loss 0.40 ACC 0.5 on epoch=99
06/24/2022 05:58:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=104
06/24/2022 05:58:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.30 on epoch=109
06/24/2022 05:58:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=114
06/24/2022 05:58:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=119
06/24/2022 05:58:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=124
06/24/2022 05:58:18 - INFO - __main__ - Global step 250 Train loss 0.36 ACC 0.5625 on epoch=124
06/24/2022 05:58:18 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=124, global_step=250
06/24/2022 05:58:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=129
06/24/2022 05:58:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=134
06/24/2022 05:58:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.29 on epoch=139
06/24/2022 05:58:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.30 on epoch=144
06/24/2022 05:58:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=149
06/24/2022 05:58:25 - INFO - __main__ - Global step 300 Train loss 0.31 ACC 0.40625 on epoch=149
06/24/2022 05:58:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
06/24/2022 05:58:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
06/24/2022 05:58:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
06/24/2022 05:58:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=169
06/24/2022 05:58:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=174
06/24/2022 05:58:32 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.4375 on epoch=174
06/24/2022 05:58:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=179
06/24/2022 05:58:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=184
06/24/2022 05:58:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=189
06/24/2022 05:58:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=194
06/24/2022 05:58:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=199
06/24/2022 05:58:39 - INFO - __main__ - Global step 400 Train loss 0.24 ACC 0.4375 on epoch=199
06/24/2022 05:58:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=204
06/24/2022 05:58:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=209
06/24/2022 05:58:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.17 on epoch=214
06/24/2022 05:58:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
06/24/2022 05:58:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=224
06/24/2022 05:58:45 - INFO - __main__ - Global step 450 Train loss 0.16 ACC 0.40625 on epoch=224
06/24/2022 05:58:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.16 on epoch=229
06/24/2022 05:58:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=234
06/24/2022 05:58:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.17 on epoch=239
06/24/2022 05:58:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
06/24/2022 05:58:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=249
06/24/2022 05:58:52 - INFO - __main__ - Global step 500 Train loss 0.17 ACC 0.5 on epoch=249
06/24/2022 05:58:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.15 on epoch=254
06/24/2022 05:58:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=259
06/24/2022 05:58:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=264
06/24/2022 05:58:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.16 on epoch=269
06/24/2022 05:58:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.12 on epoch=274
06/24/2022 05:58:59 - INFO - __main__ - Global step 550 Train loss 0.14 ACC 0.5 on epoch=274
06/24/2022 05:59:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.11 on epoch=279
06/24/2022 05:59:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=284
06/24/2022 05:59:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.11 on epoch=289
06/24/2022 05:59:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
06/24/2022 05:59:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=299
06/24/2022 05:59:06 - INFO - __main__ - Global step 600 Train loss 0.11 ACC 0.46875 on epoch=299
06/24/2022 05:59:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=304
06/24/2022 05:59:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=309
06/24/2022 05:59:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=314
06/24/2022 05:59:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.13 on epoch=319
06/24/2022 05:59:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=324
06/24/2022 05:59:13 - INFO - __main__ - Global step 650 Train loss 0.13 ACC 0.5 on epoch=324
06/24/2022 05:59:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.09 on epoch=329
06/24/2022 05:59:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=334
06/24/2022 05:59:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=339
06/24/2022 05:59:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=344
06/24/2022 05:59:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=349
06/24/2022 05:59:19 - INFO - __main__ - Global step 700 Train loss 0.11 ACC 0.5 on epoch=349
06/24/2022 05:59:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.08 on epoch=354
06/24/2022 05:59:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=359
06/24/2022 05:59:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=364
06/24/2022 05:59:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
06/24/2022 05:59:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=374
06/24/2022 05:59:26 - INFO - __main__ - Global step 750 Train loss 0.09 ACC 0.4375 on epoch=374
06/24/2022 05:59:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
06/24/2022 05:59:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.09 on epoch=384
06/24/2022 05:59:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=389
06/24/2022 05:59:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=394
06/24/2022 05:59:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=399
06/24/2022 05:59:33 - INFO - __main__ - Global step 800 Train loss 0.07 ACC 0.40625 on epoch=399
06/24/2022 05:59:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
06/24/2022 05:59:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=409
06/24/2022 05:59:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=414
06/24/2022 05:59:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=419
06/24/2022 05:59:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
06/24/2022 05:59:40 - INFO - __main__ - Global step 850 Train loss 0.08 ACC 0.46875 on epoch=424
06/24/2022 05:59:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
06/24/2022 05:59:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
06/24/2022 05:59:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=439
06/24/2022 05:59:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=444
06/24/2022 05:59:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
06/24/2022 05:59:47 - INFO - __main__ - Global step 900 Train loss 0.05 ACC 0.40625 on epoch=449
06/24/2022 05:59:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
06/24/2022 05:59:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
06/24/2022 05:59:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
06/24/2022 05:59:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
06/24/2022 05:59:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=474
06/24/2022 05:59:54 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.4375 on epoch=474
06/24/2022 05:59:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=479
06/24/2022 05:59:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=484
06/24/2022 05:59:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=489
06/24/2022 05:59:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
06/24/2022 06:00:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
06/24/2022 06:00:00 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.4375 on epoch=499
06/24/2022 06:00:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=504
06/24/2022 06:00:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
06/24/2022 06:00:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
06/24/2022 06:00:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
06/24/2022 06:00:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
06/24/2022 06:00:07 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.40625 on epoch=524
06/24/2022 06:00:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
06/24/2022 06:00:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
06/24/2022 06:00:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
06/24/2022 06:00:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
06/24/2022 06:00:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
06/24/2022 06:00:14 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.375 on epoch=549
06/24/2022 06:00:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=554
06/24/2022 06:00:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
06/24/2022 06:00:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 06:00:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
06/24/2022 06:00:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/24/2022 06:00:21 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.34375 on epoch=574
06/24/2022 06:00:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
06/24/2022 06:00:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
06/24/2022 06:00:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=589
06/24/2022 06:00:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=594
06/24/2022 06:00:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=599
06/24/2022 06:00:28 - INFO - __main__ - Global step 1200 Train loss 0.05 ACC 0.4375 on epoch=599
06/24/2022 06:00:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
06/24/2022 06:00:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=609
06/24/2022 06:00:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
06/24/2022 06:00:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/24/2022 06:00:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
06/24/2022 06:00:35 - INFO - __main__ - Global step 1250 Train loss 0.04 ACC 0.46875 on epoch=624
06/24/2022 06:00:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
06/24/2022 06:00:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
06/24/2022 06:00:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
06/24/2022 06:00:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
06/24/2022 06:00:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
06/24/2022 06:00:41 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.34375 on epoch=649
06/24/2022 06:00:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
06/24/2022 06:00:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
06/24/2022 06:00:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
06/24/2022 06:00:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
06/24/2022 06:00:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
06/24/2022 06:00:48 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.40625 on epoch=674
06/24/2022 06:00:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/24/2022 06:00:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
06/24/2022 06:00:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=689
06/24/2022 06:00:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
06/24/2022 06:00:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=699
06/24/2022 06:00:55 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.40625 on epoch=699
06/24/2022 06:00:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 06:00:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
06/24/2022 06:00:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=714
06/24/2022 06:01:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/24/2022 06:01:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
06/24/2022 06:01:02 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.375 on epoch=724
06/24/2022 06:01:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
06/24/2022 06:01:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
06/24/2022 06:01:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
06/24/2022 06:01:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=744
06/24/2022 06:01:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/24/2022 06:01:09 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.46875 on epoch=749
06/24/2022 06:01:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
06/24/2022 06:01:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
06/24/2022 06:01:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
06/24/2022 06:01:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/24/2022 06:01:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/24/2022 06:01:16 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.4375 on epoch=774
06/24/2022 06:01:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/24/2022 06:01:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/24/2022 06:01:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=789
06/24/2022 06:01:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
06/24/2022 06:01:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=799
06/24/2022 06:01:22 - INFO - __main__ - Global step 1600 Train loss 0.04 ACC 0.375 on epoch=799
06/24/2022 06:01:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=804
06/24/2022 06:01:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
06/24/2022 06:01:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
06/24/2022 06:01:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
06/24/2022 06:01:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
06/24/2022 06:01:29 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.375 on epoch=824
06/24/2022 06:01:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/24/2022 06:01:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
06/24/2022 06:01:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
06/24/2022 06:01:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/24/2022 06:01:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/24/2022 06:01:36 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.375 on epoch=849
06/24/2022 06:01:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=854
06/24/2022 06:01:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
06/24/2022 06:01:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=864
06/24/2022 06:01:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
06/24/2022 06:01:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/24/2022 06:01:43 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.40625 on epoch=874
06/24/2022 06:01:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
06/24/2022 06:01:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
06/24/2022 06:01:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/24/2022 06:01:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=894
06/24/2022 06:01:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/24/2022 06:01:50 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.4375 on epoch=899
06/24/2022 06:01:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
06/24/2022 06:01:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=909
06/24/2022 06:01:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
06/24/2022 06:01:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/24/2022 06:01:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=924
06/24/2022 06:01:57 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.4375 on epoch=924
06/24/2022 06:01:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/24/2022 06:01:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=934
06/24/2022 06:02:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/24/2022 06:02:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/24/2022 06:02:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
06/24/2022 06:02:04 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.4375 on epoch=949
06/24/2022 06:02:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=954
06/24/2022 06:02:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/24/2022 06:02:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/24/2022 06:02:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=969
06/24/2022 06:02:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/24/2022 06:02:10 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.4375 on epoch=974
06/24/2022 06:02:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
06/24/2022 06:02:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/24/2022 06:02:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=989
06/24/2022 06:02:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 06:02:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
06/24/2022 06:02:17 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.46875 on epoch=999
06/24/2022 06:02:17 - INFO - __main__ - save last model!
06/24/2022 06:02:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 06:02:17 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 06:02:17 - INFO - __main__ - Printing 3 examples
06/24/2022 06:02:17 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 06:02:17 - INFO - __main__ - ['equivalent']
06/24/2022 06:02:17 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 06:02:17 - INFO - __main__ - ['not_equivalent']
06/24/2022 06:02:17 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 06:02:17 - INFO - __main__ - ['not_equivalent']
06/24/2022 06:02:17 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:02:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:02:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:02:18 - INFO - __main__ - Printing 3 examples
06/24/2022 06:02:18 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 06:02:18 - INFO - __main__ - ['equivalent']
06/24/2022 06:02:18 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 06:02:18 - INFO - __main__ - ['equivalent']
06/24/2022 06:02:18 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 06:02:18 - INFO - __main__ - ['equivalent']
06/24/2022 06:02:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:02:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:02:18 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 06:02:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:02:18 - INFO - __main__ - Printing 3 examples
06/24/2022 06:02:18 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 06:02:18 - INFO - __main__ - ['equivalent']
06/24/2022 06:02:18 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 06:02:18 - INFO - __main__ - ['equivalent']
06/24/2022 06:02:18 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 06:02:18 - INFO - __main__ - ['equivalent']
06/24/2022 06:02:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:02:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:02:18 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 06:02:18 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 06:02:24 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 06:02:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 06:02:24 - INFO - __main__ - Starting training!
06/24/2022 06:02:26 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_42_0.2_8_predictions.txt
06/24/2022 06:02:26 - INFO - __main__ - ACC on test data: 0.4534
06/24/2022 06:02:26 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.2, bsz=8, dev_performance=0.5625, test_performance=0.4534313725490196
06/24/2022 06:02:26 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.5, bsz=8 ...
06/24/2022 06:02:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:02:27 - INFO - __main__ - Printing 3 examples
06/24/2022 06:02:27 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 06:02:27 - INFO - __main__ - ['equivalent']
06/24/2022 06:02:27 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 06:02:27 - INFO - __main__ - ['equivalent']
06/24/2022 06:02:27 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 06:02:27 - INFO - __main__ - ['equivalent']
06/24/2022 06:02:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:02:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:02:27 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 06:02:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:02:27 - INFO - __main__ - Printing 3 examples
06/24/2022 06:02:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 06:02:27 - INFO - __main__ - ['equivalent']
06/24/2022 06:02:27 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 06:02:27 - INFO - __main__ - ['equivalent']
06/24/2022 06:02:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 06:02:27 - INFO - __main__ - ['equivalent']
06/24/2022 06:02:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:02:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:02:27 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 06:02:33 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 06:02:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 06:02:33 - INFO - __main__ - Starting training!
06/24/2022 06:02:35 - INFO - __main__ - Step 10 Global step 10 Train loss 3.44 on epoch=4
06/24/2022 06:02:36 - INFO - __main__ - Step 20 Global step 20 Train loss 2.11 on epoch=9
06/24/2022 06:02:37 - INFO - __main__ - Step 30 Global step 30 Train loss 1.39 on epoch=14
06/24/2022 06:02:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.89 on epoch=19
06/24/2022 06:02:40 - INFO - __main__ - Step 50 Global step 50 Train loss 0.64 on epoch=24
06/24/2022 06:02:40 - INFO - __main__ - Global step 50 Train loss 1.69 ACC 0.53125 on epoch=24
06/24/2022 06:02:40 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.53125 on epoch=24, global_step=50
06/24/2022 06:02:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=29
06/24/2022 06:02:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=34
06/24/2022 06:02:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=39
06/24/2022 06:02:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=44
06/24/2022 06:02:46 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=49
06/24/2022 06:02:47 - INFO - __main__ - Global step 100 Train loss 0.42 ACC 0.59375 on epoch=49
06/24/2022 06:02:47 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=49, global_step=100
06/24/2022 06:02:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.33 on epoch=54
06/24/2022 06:02:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
06/24/2022 06:02:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=64
06/24/2022 06:02:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.32 on epoch=69
06/24/2022 06:02:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=74
06/24/2022 06:02:54 - INFO - __main__ - Global step 150 Train loss 0.33 ACC 0.46875 on epoch=74
06/24/2022 06:02:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
06/24/2022 06:02:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=84
06/24/2022 06:02:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=89
06/24/2022 06:02:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
06/24/2022 06:03:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=99
06/24/2022 06:03:01 - INFO - __main__ - Global step 200 Train loss 0.28 ACC 0.34375 on epoch=99
06/24/2022 06:03:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=104
06/24/2022 06:03:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.23 on epoch=109
06/24/2022 06:03:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=114
06/24/2022 06:03:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.20 on epoch=119
06/24/2022 06:03:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=124
06/24/2022 06:03:07 - INFO - __main__ - Global step 250 Train loss 0.24 ACC 0.53125 on epoch=124
06/24/2022 06:03:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.18 on epoch=129
06/24/2022 06:03:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.19 on epoch=134
06/24/2022 06:03:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.18 on epoch=139
06/24/2022 06:03:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.16 on epoch=144
06/24/2022 06:03:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=149
06/24/2022 06:03:14 - INFO - __main__ - Global step 300 Train loss 0.18 ACC 0.375 on epoch=149
06/24/2022 06:03:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.17 on epoch=154
06/24/2022 06:03:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.12 on epoch=159
06/24/2022 06:03:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.17 on epoch=164
06/24/2022 06:03:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
06/24/2022 06:03:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.07 on epoch=174
06/24/2022 06:03:21 - INFO - __main__ - Global step 350 Train loss 0.13 ACC 0.40625 on epoch=174
06/24/2022 06:03:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.10 on epoch=179
06/24/2022 06:03:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
06/24/2022 06:03:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.07 on epoch=189
06/24/2022 06:03:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.07 on epoch=194
06/24/2022 06:03:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.15 on epoch=199
06/24/2022 06:03:27 - INFO - __main__ - Global step 400 Train loss 0.10 ACC 0.5 on epoch=199
06/24/2022 06:03:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
06/24/2022 06:03:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.08 on epoch=209
06/24/2022 06:03:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
06/24/2022 06:03:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.04 on epoch=219
06/24/2022 06:03:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.07 on epoch=224
06/24/2022 06:03:34 - INFO - __main__ - Global step 450 Train loss 0.07 ACC 0.5 on epoch=224
06/24/2022 06:03:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.03 on epoch=229
06/24/2022 06:03:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
06/24/2022 06:03:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.03 on epoch=239
06/24/2022 06:03:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
06/24/2022 06:03:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.04 on epoch=249
06/24/2022 06:03:41 - INFO - __main__ - Global step 500 Train loss 0.05 ACC 0.46875 on epoch=249
06/24/2022 06:03:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=254
06/24/2022 06:03:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
06/24/2022 06:03:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=264
06/24/2022 06:03:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=269
06/24/2022 06:03:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
06/24/2022 06:03:48 - INFO - __main__ - Global step 550 Train loss 0.06 ACC 0.46875 on epoch=274
06/24/2022 06:03:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
06/24/2022 06:03:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
06/24/2022 06:03:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=289
06/24/2022 06:03:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
06/24/2022 06:03:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
06/24/2022 06:03:54 - INFO - __main__ - Global step 600 Train loss 0.03 ACC 0.53125 on epoch=299
06/24/2022 06:03:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
06/24/2022 06:03:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
06/24/2022 06:03:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/24/2022 06:03:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
06/24/2022 06:04:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
06/24/2022 06:04:01 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.46875 on epoch=324
06/24/2022 06:04:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=329
06/24/2022 06:04:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
06/24/2022 06:04:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
06/24/2022 06:04:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
06/24/2022 06:04:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=349
06/24/2022 06:04:08 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.40625 on epoch=349
06/24/2022 06:04:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
06/24/2022 06:04:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
06/24/2022 06:04:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
06/24/2022 06:04:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/24/2022 06:04:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
06/24/2022 06:04:15 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.4375 on epoch=374
06/24/2022 06:04:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/24/2022 06:04:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
06/24/2022 06:04:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
06/24/2022 06:04:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=394
06/24/2022 06:04:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
06/24/2022 06:04:21 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.46875 on epoch=399
06/24/2022 06:04:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
06/24/2022 06:04:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/24/2022 06:04:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
06/24/2022 06:04:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
06/24/2022 06:04:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/24/2022 06:04:28 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.53125 on epoch=424
06/24/2022 06:04:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
06/24/2022 06:04:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
06/24/2022 06:04:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/24/2022 06:04:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
06/24/2022 06:04:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
06/24/2022 06:04:35 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.59375 on epoch=449
06/24/2022 06:04:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/24/2022 06:04:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
06/24/2022 06:04:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=464
06/24/2022 06:04:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
06/24/2022 06:04:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/24/2022 06:04:42 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.53125 on epoch=474
06/24/2022 06:04:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/24/2022 06:04:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
06/24/2022 06:04:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/24/2022 06:04:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
06/24/2022 06:04:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/24/2022 06:04:48 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.46875 on epoch=499
06/24/2022 06:04:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/24/2022 06:04:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/24/2022 06:04:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/24/2022 06:04:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
06/24/2022 06:04:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/24/2022 06:04:55 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.5 on epoch=524
06/24/2022 06:04:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/24/2022 06:04:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/24/2022 06:04:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/24/2022 06:05:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/24/2022 06:05:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/24/2022 06:05:02 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.5 on epoch=549
06/24/2022 06:05:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
06/24/2022 06:05:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
06/24/2022 06:05:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
06/24/2022 06:05:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/24/2022 06:05:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/24/2022 06:05:09 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.4375 on epoch=574
06/24/2022 06:05:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/24/2022 06:05:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/24/2022 06:05:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
06/24/2022 06:05:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 06:05:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/24/2022 06:05:15 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.40625 on epoch=599
06/24/2022 06:05:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
06/24/2022 06:05:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/24/2022 06:05:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 06:05:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/24/2022 06:05:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 06:05:22 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.5 on epoch=624
06/24/2022 06:05:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 06:05:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 06:05:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
06/24/2022 06:05:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 06:05:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 06:05:29 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.5 on epoch=649
06/24/2022 06:05:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/24/2022 06:05:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 06:05:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 06:05:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
06/24/2022 06:05:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 06:05:36 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.5 on epoch=674
06/24/2022 06:05:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/24/2022 06:05:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/24/2022 06:05:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=689
06/24/2022 06:05:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 06:05:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 06:05:42 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.4375 on epoch=699
06/24/2022 06:05:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 06:05:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 06:05:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 06:05:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 06:05:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 06:05:49 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.4375 on epoch=724
06/24/2022 06:05:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 06:05:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
06/24/2022 06:05:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 06:05:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/24/2022 06:05:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
06/24/2022 06:05:56 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.46875 on epoch=749
06/24/2022 06:05:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 06:05:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 06:05:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
06/24/2022 06:06:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 06:06:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 06:06:02 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.4375 on epoch=774
06/24/2022 06:06:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 06:06:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 06:06:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 06:06:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 06:06:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
06/24/2022 06:06:09 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.40625 on epoch=799
06/24/2022 06:06:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 06:06:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/24/2022 06:06:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/24/2022 06:06:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/24/2022 06:06:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
06/24/2022 06:06:16 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.4375 on epoch=824
06/24/2022 06:06:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/24/2022 06:06:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 06:06:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 06:06:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/24/2022 06:06:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/24/2022 06:06:23 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.46875 on epoch=849
06/24/2022 06:06:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 06:06:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 06:06:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/24/2022 06:06:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 06:06:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 06:06:29 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.46875 on epoch=874
06/24/2022 06:06:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 06:06:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
06/24/2022 06:06:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=889
06/24/2022 06:06:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 06:06:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 06:06:36 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.4375 on epoch=899
06/24/2022 06:06:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 06:06:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 06:06:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 06:06:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 06:06:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 06:06:43 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.46875 on epoch=924
06/24/2022 06:06:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 06:06:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 06:06:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 06:06:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 06:06:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 06:06:50 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.4375 on epoch=949
06/24/2022 06:06:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 06:06:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 06:06:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/24/2022 06:06:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 06:06:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 06:06:56 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.46875 on epoch=974
06/24/2022 06:06:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
06/24/2022 06:06:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=984
06/24/2022 06:07:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=989
06/24/2022 06:07:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/24/2022 06:07:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
06/24/2022 06:07:03 - INFO - __main__ - Global step 2000 Train loss 0.04 ACC 0.46875 on epoch=999
06/24/2022 06:07:03 - INFO - __main__ - save last model!
06/24/2022 06:07:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 06:07:03 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 06:07:03 - INFO - __main__ - Printing 3 examples
06/24/2022 06:07:03 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 06:07:03 - INFO - __main__ - ['equivalent']
06/24/2022 06:07:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 06:07:03 - INFO - __main__ - ['not_equivalent']
06/24/2022 06:07:03 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 06:07:03 - INFO - __main__ - ['not_equivalent']
06/24/2022 06:07:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:07:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:07:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:07:04 - INFO - __main__ - Printing 3 examples
06/24/2022 06:07:04 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 06:07:04 - INFO - __main__ - ['equivalent']
06/24/2022 06:07:04 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 06:07:04 - INFO - __main__ - ['equivalent']
06/24/2022 06:07:04 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 06:07:04 - INFO - __main__ - ['equivalent']
06/24/2022 06:07:04 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:07:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:07:04 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 06:07:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:07:04 - INFO - __main__ - Printing 3 examples
06/24/2022 06:07:04 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 06:07:04 - INFO - __main__ - ['equivalent']
06/24/2022 06:07:04 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 06:07:04 - INFO - __main__ - ['equivalent']
06/24/2022 06:07:04 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 06:07:04 - INFO - __main__ - ['equivalent']
06/24/2022 06:07:04 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:07:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:07:04 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 06:07:04 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 06:07:10 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 06:07:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 06:07:10 - INFO - __main__ - Starting training!
06/24/2022 06:07:12 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_87_0.5_8_predictions.txt
06/24/2022 06:07:12 - INFO - __main__ - ACC on test data: 0.5221
06/24/2022 06:07:12 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.5, bsz=8, dev_performance=0.59375, test_performance=0.5220588235294118
06/24/2022 06:07:12 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.4, bsz=8 ...
06/24/2022 06:07:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:07:13 - INFO - __main__ - Printing 3 examples
06/24/2022 06:07:13 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 06:07:13 - INFO - __main__ - ['equivalent']
06/24/2022 06:07:13 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 06:07:13 - INFO - __main__ - ['equivalent']
06/24/2022 06:07:13 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 06:07:13 - INFO - __main__ - ['equivalent']
06/24/2022 06:07:13 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:07:13 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:07:13 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 06:07:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:07:13 - INFO - __main__ - Printing 3 examples
06/24/2022 06:07:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 06:07:13 - INFO - __main__ - ['equivalent']
06/24/2022 06:07:13 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 06:07:13 - INFO - __main__ - ['equivalent']
06/24/2022 06:07:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 06:07:13 - INFO - __main__ - ['equivalent']
06/24/2022 06:07:13 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:07:13 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:07:13 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 06:07:19 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 06:07:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 06:07:19 - INFO - __main__ - Starting training!
06/24/2022 06:07:20 - INFO - __main__ - Step 10 Global step 10 Train loss 3.34 on epoch=4
06/24/2022 06:07:22 - INFO - __main__ - Step 20 Global step 20 Train loss 2.36 on epoch=9
06/24/2022 06:07:23 - INFO - __main__ - Step 30 Global step 30 Train loss 1.61 on epoch=14
06/24/2022 06:07:24 - INFO - __main__ - Step 40 Global step 40 Train loss 1.16 on epoch=19
06/24/2022 06:07:25 - INFO - __main__ - Step 50 Global step 50 Train loss 0.87 on epoch=24
06/24/2022 06:07:26 - INFO - __main__ - Global step 50 Train loss 1.87 ACC 0.625 on epoch=24
06/24/2022 06:07:26 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.625 on epoch=24, global_step=50
06/24/2022 06:07:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.66 on epoch=29
06/24/2022 06:07:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=34
06/24/2022 06:07:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=39
06/24/2022 06:07:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=44
06/24/2022 06:07:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=49
06/24/2022 06:07:33 - INFO - __main__ - Global step 100 Train loss 0.51 ACC 0.5 on epoch=49
06/24/2022 06:07:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=54
06/24/2022 06:07:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.35 on epoch=59
06/24/2022 06:07:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=64
06/24/2022 06:07:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.31 on epoch=69
06/24/2022 06:07:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.32 on epoch=74
06/24/2022 06:07:40 - INFO - __main__ - Global step 150 Train loss 0.33 ACC 0.4375 on epoch=74
06/24/2022 06:07:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
06/24/2022 06:07:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=84
06/24/2022 06:07:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
06/24/2022 06:07:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
06/24/2022 06:07:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
06/24/2022 06:07:46 - INFO - __main__ - Global step 200 Train loss 0.31 ACC 0.46875 on epoch=99
06/24/2022 06:07:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=104
06/24/2022 06:07:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.30 on epoch=109
06/24/2022 06:07:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=114
06/24/2022 06:07:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
06/24/2022 06:07:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=124
06/24/2022 06:07:53 - INFO - __main__ - Global step 250 Train loss 0.28 ACC 0.375 on epoch=124
06/24/2022 06:07:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.21 on epoch=129
06/24/2022 06:07:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.20 on epoch=134
06/24/2022 06:07:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=139
06/24/2022 06:07:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.18 on epoch=144
06/24/2022 06:07:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.14 on epoch=149
06/24/2022 06:08:00 - INFO - __main__ - Global step 300 Train loss 0.19 ACC 0.40625 on epoch=149
06/24/2022 06:08:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.14 on epoch=154
06/24/2022 06:08:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.14 on epoch=159
06/24/2022 06:08:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.13 on epoch=164
06/24/2022 06:08:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.11 on epoch=169
06/24/2022 06:08:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.16 on epoch=174
06/24/2022 06:08:07 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.46875 on epoch=174
06/24/2022 06:08:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.06 on epoch=179
06/24/2022 06:08:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.06 on epoch=184
06/24/2022 06:08:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.07 on epoch=189
06/24/2022 06:08:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.06 on epoch=194
06/24/2022 06:08:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.08 on epoch=199
06/24/2022 06:08:13 - INFO - __main__ - Global step 400 Train loss 0.07 ACC 0.5 on epoch=199
06/24/2022 06:08:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
06/24/2022 06:08:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.05 on epoch=209
06/24/2022 06:08:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.05 on epoch=214
06/24/2022 06:08:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.06 on epoch=219
06/24/2022 06:08:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.08 on epoch=224
06/24/2022 06:08:20 - INFO - __main__ - Global step 450 Train loss 0.07 ACC 0.4375 on epoch=224
06/24/2022 06:08:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=229
06/24/2022 06:08:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.06 on epoch=234
06/24/2022 06:08:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.08 on epoch=239
06/24/2022 06:08:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
06/24/2022 06:08:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.02 on epoch=249
06/24/2022 06:08:27 - INFO - __main__ - Global step 500 Train loss 0.06 ACC 0.40625 on epoch=249
06/24/2022 06:08:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
06/24/2022 06:08:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
06/24/2022 06:08:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
06/24/2022 06:08:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=269
06/24/2022 06:08:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
06/24/2022 06:08:33 - INFO - __main__ - Global step 550 Train loss 0.04 ACC 0.4375 on epoch=274
06/24/2022 06:08:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=279
06/24/2022 06:08:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
06/24/2022 06:08:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
06/24/2022 06:08:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
06/24/2022 06:08:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
06/24/2022 06:08:40 - INFO - __main__ - Global step 600 Train loss 0.04 ACC 0.4375 on epoch=299
06/24/2022 06:08:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
06/24/2022 06:08:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
06/24/2022 06:08:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/24/2022 06:08:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
06/24/2022 06:08:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/24/2022 06:08:47 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.5 on epoch=324
06/24/2022 06:08:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
06/24/2022 06:08:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
06/24/2022 06:08:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=339
06/24/2022 06:08:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
06/24/2022 06:08:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
06/24/2022 06:08:54 - INFO - __main__ - Global step 700 Train loss 0.02 ACC 0.46875 on epoch=349
06/24/2022 06:08:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
06/24/2022 06:08:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
06/24/2022 06:08:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
06/24/2022 06:08:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
06/24/2022 06:09:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
06/24/2022 06:09:00 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.4375 on epoch=374
06/24/2022 06:09:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
06/24/2022 06:09:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=384
06/24/2022 06:09:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
06/24/2022 06:09:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/24/2022 06:09:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
06/24/2022 06:09:07 - INFO - __main__ - Global step 800 Train loss 0.05 ACC 0.5 on epoch=399
06/24/2022 06:09:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
06/24/2022 06:09:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
06/24/2022 06:09:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
06/24/2022 06:09:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
06/24/2022 06:09:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/24/2022 06:09:14 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.53125 on epoch=424
06/24/2022 06:09:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/24/2022 06:09:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
06/24/2022 06:09:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
06/24/2022 06:09:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=444
06/24/2022 06:09:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/24/2022 06:09:21 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.4375 on epoch=449
06/24/2022 06:09:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/24/2022 06:09:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/24/2022 06:09:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/24/2022 06:09:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/24/2022 06:09:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
06/24/2022 06:09:27 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.53125 on epoch=474
06/24/2022 06:09:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/24/2022 06:09:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/24/2022 06:09:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
06/24/2022 06:09:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
06/24/2022 06:09:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/24/2022 06:09:34 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.53125 on epoch=499
06/24/2022 06:09:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/24/2022 06:09:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/24/2022 06:09:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/24/2022 06:09:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/24/2022 06:09:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/24/2022 06:09:41 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.53125 on epoch=524
06/24/2022 06:09:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/24/2022 06:09:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
06/24/2022 06:09:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/24/2022 06:09:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
06/24/2022 06:09:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=549
06/24/2022 06:09:48 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.53125 on epoch=549
06/24/2022 06:09:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 06:09:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
06/24/2022 06:09:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 06:09:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=569
06/24/2022 06:09:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/24/2022 06:09:54 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.4375 on epoch=574
06/24/2022 06:09:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/24/2022 06:09:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/24/2022 06:09:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/24/2022 06:09:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 06:10:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/24/2022 06:10:01 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.46875 on epoch=599
06/24/2022 06:10:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/24/2022 06:10:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/24/2022 06:10:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 06:10:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
06/24/2022 06:10:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 06:10:08 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.53125 on epoch=624
06/24/2022 06:10:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
06/24/2022 06:10:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 06:10:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
06/24/2022 06:10:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/24/2022 06:10:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 06:10:15 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.53125 on epoch=649
06/24/2022 06:10:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/24/2022 06:10:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 06:10:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 06:10:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 06:10:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/24/2022 06:10:21 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.53125 on epoch=674
06/24/2022 06:10:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/24/2022 06:10:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/24/2022 06:10:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
06/24/2022 06:10:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 06:10:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/24/2022 06:10:28 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.5625 on epoch=699
06/24/2022 06:10:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 06:10:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 06:10:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 06:10:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 06:10:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/24/2022 06:10:35 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.53125 on epoch=724
06/24/2022 06:10:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 06:10:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 06:10:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 06:10:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 06:10:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 06:10:42 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.53125 on epoch=749
06/24/2022 06:10:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 06:10:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 06:10:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 06:10:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 06:10:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 06:10:48 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.59375 on epoch=774
06/24/2022 06:10:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 06:10:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=784
06/24/2022 06:10:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 06:10:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 06:10:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 06:10:55 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.59375 on epoch=799
06/24/2022 06:10:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 06:10:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 06:10:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/24/2022 06:11:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 06:11:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 06:11:02 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.46875 on epoch=824
06/24/2022 06:11:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 06:11:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 06:11:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 06:11:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 06:11:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/24/2022 06:11:09 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.59375 on epoch=849
06/24/2022 06:11:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 06:11:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 06:11:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 06:11:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 06:11:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/24/2022 06:11:16 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.53125 on epoch=874
06/24/2022 06:11:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 06:11:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 06:11:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 06:11:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 06:11:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 06:11:22 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.59375 on epoch=899
06/24/2022 06:11:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
06/24/2022 06:11:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 06:11:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 06:11:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 06:11:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
06/24/2022 06:11:29 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.53125 on epoch=924
06/24/2022 06:11:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/24/2022 06:11:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
06/24/2022 06:11:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 06:11:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 06:11:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 06:11:36 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.53125 on epoch=949
06/24/2022 06:11:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 06:11:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 06:11:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 06:11:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=969
06/24/2022 06:11:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 06:11:43 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.5 on epoch=974
06/24/2022 06:11:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 06:11:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 06:11:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 06:11:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 06:11:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 06:11:50 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.53125 on epoch=999
06/24/2022 06:11:50 - INFO - __main__ - save last model!
06/24/2022 06:11:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 06:11:50 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 06:11:50 - INFO - __main__ - Printing 3 examples
06/24/2022 06:11:50 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 06:11:50 - INFO - __main__ - ['equivalent']
06/24/2022 06:11:50 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 06:11:50 - INFO - __main__ - ['not_equivalent']
06/24/2022 06:11:50 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 06:11:50 - INFO - __main__ - ['not_equivalent']
06/24/2022 06:11:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:11:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:11:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:11:50 - INFO - __main__ - Printing 3 examples
06/24/2022 06:11:50 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 06:11:50 - INFO - __main__ - ['equivalent']
06/24/2022 06:11:50 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 06:11:50 - INFO - __main__ - ['equivalent']
06/24/2022 06:11:50 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 06:11:50 - INFO - __main__ - ['equivalent']
06/24/2022 06:11:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:11:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:11:50 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 06:11:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:11:50 - INFO - __main__ - Printing 3 examples
06/24/2022 06:11:50 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 06:11:50 - INFO - __main__ - ['equivalent']
06/24/2022 06:11:50 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 06:11:50 - INFO - __main__ - ['equivalent']
06/24/2022 06:11:50 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 06:11:50 - INFO - __main__ - ['equivalent']
06/24/2022 06:11:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:11:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:11:50 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 06:11:50 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 06:11:56 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 06:11:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 06:11:56 - INFO - __main__ - Starting training!
06/24/2022 06:11:58 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_87_0.4_8_predictions.txt
06/24/2022 06:11:58 - INFO - __main__ - ACC on test data: 0.5196
06/24/2022 06:11:58 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.4, bsz=8, dev_performance=0.625, test_performance=0.5196078431372549
06/24/2022 06:11:58 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.3, bsz=8 ...
06/24/2022 06:11:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:11:59 - INFO - __main__ - Printing 3 examples
06/24/2022 06:11:59 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 06:11:59 - INFO - __main__ - ['equivalent']
06/24/2022 06:11:59 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 06:11:59 - INFO - __main__ - ['equivalent']
06/24/2022 06:11:59 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 06:11:59 - INFO - __main__ - ['equivalent']
06/24/2022 06:11:59 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:11:59 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:11:59 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 06:11:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:11:59 - INFO - __main__ - Printing 3 examples
06/24/2022 06:11:59 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 06:11:59 - INFO - __main__ - ['equivalent']
06/24/2022 06:11:59 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 06:11:59 - INFO - __main__ - ['equivalent']
06/24/2022 06:11:59 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 06:11:59 - INFO - __main__ - ['equivalent']
06/24/2022 06:11:59 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:11:59 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:11:59 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 06:12:05 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 06:12:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 06:12:05 - INFO - __main__ - Starting training!
06/24/2022 06:12:07 - INFO - __main__ - Step 10 Global step 10 Train loss 3.38 on epoch=4
06/24/2022 06:12:08 - INFO - __main__ - Step 20 Global step 20 Train loss 2.59 on epoch=9
06/24/2022 06:12:09 - INFO - __main__ - Step 30 Global step 30 Train loss 1.83 on epoch=14
06/24/2022 06:12:11 - INFO - __main__ - Step 40 Global step 40 Train loss 1.58 on epoch=19
06/24/2022 06:12:12 - INFO - __main__ - Step 50 Global step 50 Train loss 1.22 on epoch=24
06/24/2022 06:12:13 - INFO - __main__ - Global step 50 Train loss 2.12 ACC 0.09375 on epoch=24
06/24/2022 06:12:13 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.09375 on epoch=24, global_step=50
06/24/2022 06:12:14 - INFO - __main__ - Step 60 Global step 60 Train loss 0.91 on epoch=29
06/24/2022 06:12:15 - INFO - __main__ - Step 70 Global step 70 Train loss 0.76 on epoch=34
06/24/2022 06:12:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.65 on epoch=39
06/24/2022 06:12:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=44
06/24/2022 06:12:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=49
06/24/2022 06:12:19 - INFO - __main__ - Global step 100 Train loss 0.67 ACC 0.5 on epoch=49
06/24/2022 06:12:20 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.5 on epoch=49, global_step=100
06/24/2022 06:12:21 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=54
06/24/2022 06:12:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=59
06/24/2022 06:12:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=64
06/24/2022 06:12:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.37 on epoch=69
06/24/2022 06:12:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
06/24/2022 06:12:26 - INFO - __main__ - Global step 150 Train loss 0.41 ACC 0.53125 on epoch=74
06/24/2022 06:12:26 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=74, global_step=150
06/24/2022 06:12:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.35 on epoch=79
06/24/2022 06:12:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=84
06/24/2022 06:12:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
06/24/2022 06:12:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
06/24/2022 06:12:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
06/24/2022 06:12:33 - INFO - __main__ - Global step 200 Train loss 0.33 ACC 0.4375 on epoch=99
06/24/2022 06:12:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=104
06/24/2022 06:12:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=109
06/24/2022 06:12:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=114
06/24/2022 06:12:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=119
06/24/2022 06:12:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=124
06/24/2022 06:12:40 - INFO - __main__ - Global step 250 Train loss 0.30 ACC 0.53125 on epoch=124
06/24/2022 06:12:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=129
06/24/2022 06:12:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=134
06/24/2022 06:12:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.31 on epoch=139
06/24/2022 06:12:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=144
06/24/2022 06:12:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=149
06/24/2022 06:12:47 - INFO - __main__ - Global step 300 Train loss 0.33 ACC 0.5 on epoch=149
06/24/2022 06:12:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
06/24/2022 06:12:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
06/24/2022 06:12:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=164
06/24/2022 06:12:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
06/24/2022 06:12:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
06/24/2022 06:12:53 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.53125 on epoch=174
06/24/2022 06:12:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
06/24/2022 06:12:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=184
06/24/2022 06:12:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=189
06/24/2022 06:12:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
06/24/2022 06:13:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=199
06/24/2022 06:13:00 - INFO - __main__ - Global step 400 Train loss 0.23 ACC 0.5625 on epoch=199
06/24/2022 06:13:00 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=199, global_step=400
06/24/2022 06:13:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=204
06/24/2022 06:13:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.17 on epoch=209
06/24/2022 06:13:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.16 on epoch=214
06/24/2022 06:13:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.15 on epoch=219
06/24/2022 06:13:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=224
06/24/2022 06:13:07 - INFO - __main__ - Global step 450 Train loss 0.16 ACC 0.46875 on epoch=224
06/24/2022 06:13:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.17 on epoch=229
06/24/2022 06:13:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.09 on epoch=234
06/24/2022 06:13:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.12 on epoch=239
06/24/2022 06:13:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=244
06/24/2022 06:13:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=249
06/24/2022 06:13:14 - INFO - __main__ - Global step 500 Train loss 0.12 ACC 0.40625 on epoch=249
06/24/2022 06:13:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
06/24/2022 06:13:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=259
06/24/2022 06:13:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=264
06/24/2022 06:13:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.08 on epoch=269
06/24/2022 06:13:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
06/24/2022 06:13:21 - INFO - __main__ - Global step 550 Train loss 0.08 ACC 0.46875 on epoch=274
06/24/2022 06:13:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
06/24/2022 06:13:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.04 on epoch=284
06/24/2022 06:13:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.03 on epoch=289
06/24/2022 06:13:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
06/24/2022 06:13:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
06/24/2022 06:13:27 - INFO - __main__ - Global step 600 Train loss 0.05 ACC 0.53125 on epoch=299
06/24/2022 06:13:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=304
06/24/2022 06:13:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
06/24/2022 06:13:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=314
06/24/2022 06:13:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
06/24/2022 06:13:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.08 on epoch=324
06/24/2022 06:13:34 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.53125 on epoch=324
06/24/2022 06:13:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
06/24/2022 06:13:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
06/24/2022 06:13:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
06/24/2022 06:13:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=344
06/24/2022 06:13:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
06/24/2022 06:13:41 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.53125 on epoch=349
06/24/2022 06:13:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
06/24/2022 06:13:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
06/24/2022 06:13:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=364
06/24/2022 06:13:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
06/24/2022 06:13:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
06/24/2022 06:13:48 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.5 on epoch=374
06/24/2022 06:13:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/24/2022 06:13:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
06/24/2022 06:13:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
06/24/2022 06:13:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
06/24/2022 06:13:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
06/24/2022 06:13:55 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.5 on epoch=399
06/24/2022 06:13:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
06/24/2022 06:13:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
06/24/2022 06:13:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/24/2022 06:14:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
06/24/2022 06:14:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/24/2022 06:14:01 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.53125 on epoch=424
06/24/2022 06:14:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/24/2022 06:14:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
06/24/2022 06:14:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
06/24/2022 06:14:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
06/24/2022 06:14:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
06/24/2022 06:14:08 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.53125 on epoch=449
06/24/2022 06:14:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
06/24/2022 06:14:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/24/2022 06:14:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/24/2022 06:14:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/24/2022 06:14:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/24/2022 06:14:15 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.53125 on epoch=474
06/24/2022 06:14:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/24/2022 06:14:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
06/24/2022 06:14:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/24/2022 06:14:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/24/2022 06:14:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
06/24/2022 06:14:22 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.53125 on epoch=499
06/24/2022 06:14:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/24/2022 06:14:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/24/2022 06:14:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
06/24/2022 06:14:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
06/24/2022 06:14:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
06/24/2022 06:14:29 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.53125 on epoch=524
06/24/2022 06:14:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
06/24/2022 06:14:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
06/24/2022 06:14:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/24/2022 06:14:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/24/2022 06:14:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
06/24/2022 06:14:35 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.53125 on epoch=549
06/24/2022 06:14:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
06/24/2022 06:14:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
06/24/2022 06:14:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 06:14:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
06/24/2022 06:14:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/24/2022 06:14:42 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.46875 on epoch=574
06/24/2022 06:14:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/24/2022 06:14:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/24/2022 06:14:46 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
06/24/2022 06:14:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
06/24/2022 06:14:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/24/2022 06:14:49 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.5 on epoch=599
06/24/2022 06:14:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=604
06/24/2022 06:14:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=609
06/24/2022 06:14:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/24/2022 06:14:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
06/24/2022 06:14:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
06/24/2022 06:14:56 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.46875 on epoch=624
06/24/2022 06:14:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/24/2022 06:14:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/24/2022 06:14:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/24/2022 06:15:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 06:15:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 06:15:02 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.53125 on epoch=649
06/24/2022 06:15:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 06:15:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 06:15:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 06:15:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 06:15:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/24/2022 06:15:09 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.5 on epoch=674
06/24/2022 06:15:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/24/2022 06:15:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 06:15:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 06:15:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/24/2022 06:15:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=699
06/24/2022 06:15:16 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.53125 on epoch=699
06/24/2022 06:15:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 06:15:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/24/2022 06:15:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 06:15:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/24/2022 06:15:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 06:15:23 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.46875 on epoch=724
06/24/2022 06:15:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=729
06/24/2022 06:15:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 06:15:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 06:15:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/24/2022 06:15:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
06/24/2022 06:15:30 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.5 on epoch=749
06/24/2022 06:15:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
06/24/2022 06:15:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
06/24/2022 06:15:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 06:15:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 06:15:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 06:15:37 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.5 on epoch=774
06/24/2022 06:15:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 06:15:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/24/2022 06:15:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 06:15:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/24/2022 06:15:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
06/24/2022 06:15:44 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.46875 on epoch=799
06/24/2022 06:15:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
06/24/2022 06:15:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 06:15:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
06/24/2022 06:15:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
06/24/2022 06:15:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 06:15:50 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.46875 on epoch=824
06/24/2022 06:15:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/24/2022 06:15:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/24/2022 06:15:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 06:15:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 06:15:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
06/24/2022 06:15:57 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.46875 on epoch=849
06/24/2022 06:15:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
06/24/2022 06:16:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 06:16:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 06:16:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 06:16:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 06:16:04 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.5 on epoch=874
06/24/2022 06:16:05 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 06:16:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
06/24/2022 06:16:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 06:16:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 06:16:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/24/2022 06:16:11 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.5 on epoch=899
06/24/2022 06:16:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/24/2022 06:16:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 06:16:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 06:16:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
06/24/2022 06:16:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 06:16:18 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.5 on epoch=924
06/24/2022 06:16:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=929
06/24/2022 06:16:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/24/2022 06:16:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 06:16:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=944
06/24/2022 06:16:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 06:16:24 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.5 on epoch=949
06/24/2022 06:16:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 06:16:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 06:16:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 06:16:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 06:16:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 06:16:31 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.5 on epoch=974
06/24/2022 06:16:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
06/24/2022 06:16:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 06:16:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/24/2022 06:16:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 06:16:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 06:16:38 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.46875 on epoch=999
06/24/2022 06:16:38 - INFO - __main__ - save last model!
06/24/2022 06:16:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 06:16:38 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 06:16:38 - INFO - __main__ - Printing 3 examples
06/24/2022 06:16:38 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 06:16:38 - INFO - __main__ - ['equivalent']
06/24/2022 06:16:38 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 06:16:38 - INFO - __main__ - ['not_equivalent']
06/24/2022 06:16:38 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 06:16:38 - INFO - __main__ - ['not_equivalent']
06/24/2022 06:16:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:16:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:16:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:16:39 - INFO - __main__ - Printing 3 examples
06/24/2022 06:16:39 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 06:16:39 - INFO - __main__ - ['equivalent']
06/24/2022 06:16:39 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 06:16:39 - INFO - __main__ - ['equivalent']
06/24/2022 06:16:39 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 06:16:39 - INFO - __main__ - ['equivalent']
06/24/2022 06:16:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:16:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:16:39 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 06:16:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:16:39 - INFO - __main__ - Printing 3 examples
06/24/2022 06:16:39 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 06:16:39 - INFO - __main__ - ['equivalent']
06/24/2022 06:16:39 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 06:16:39 - INFO - __main__ - ['equivalent']
06/24/2022 06:16:39 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 06:16:39 - INFO - __main__ - ['equivalent']
06/24/2022 06:16:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:16:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:16:39 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 06:16:39 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 06:16:45 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 06:16:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 06:16:45 - INFO - __main__ - Starting training!
06/24/2022 06:16:47 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_87_0.3_8_predictions.txt
06/24/2022 06:16:47 - INFO - __main__ - ACC on test data: 0.5613
06/24/2022 06:16:47 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.3, bsz=8, dev_performance=0.5625, test_performance=0.5612745098039216
06/24/2022 06:16:47 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.2, bsz=8 ...
06/24/2022 06:16:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:16:48 - INFO - __main__ - Printing 3 examples
06/24/2022 06:16:48 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 06:16:48 - INFO - __main__ - ['equivalent']
06/24/2022 06:16:48 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 06:16:48 - INFO - __main__ - ['equivalent']
06/24/2022 06:16:48 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 06:16:48 - INFO - __main__ - ['equivalent']
06/24/2022 06:16:48 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:16:48 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:16:48 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 06:16:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:16:48 - INFO - __main__ - Printing 3 examples
06/24/2022 06:16:48 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 06:16:48 - INFO - __main__ - ['equivalent']
06/24/2022 06:16:48 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 06:16:48 - INFO - __main__ - ['equivalent']
06/24/2022 06:16:48 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 06:16:48 - INFO - __main__ - ['equivalent']
06/24/2022 06:16:48 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:16:48 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:16:48 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 06:16:54 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 06:16:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 06:16:54 - INFO - __main__ - Starting training!
06/24/2022 06:16:55 - INFO - __main__ - Step 10 Global step 10 Train loss 3.52 on epoch=4
06/24/2022 06:16:56 - INFO - __main__ - Step 20 Global step 20 Train loss 3.03 on epoch=9
06/24/2022 06:16:58 - INFO - __main__ - Step 30 Global step 30 Train loss 2.47 on epoch=14
06/24/2022 06:16:59 - INFO - __main__ - Step 40 Global step 40 Train loss 2.02 on epoch=19
06/24/2022 06:17:00 - INFO - __main__ - Step 50 Global step 50 Train loss 1.74 on epoch=24
06/24/2022 06:17:01 - INFO - __main__ - Global step 50 Train loss 2.56 ACC 0.0 on epoch=24
06/24/2022 06:17:01 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 06:17:02 - INFO - __main__ - Step 60 Global step 60 Train loss 1.52 on epoch=29
06/24/2022 06:17:03 - INFO - __main__ - Step 70 Global step 70 Train loss 1.23 on epoch=34
06/24/2022 06:17:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=39
06/24/2022 06:17:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.92 on epoch=44
06/24/2022 06:17:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.70 on epoch=49
06/24/2022 06:17:08 - INFO - __main__ - Global step 100 Train loss 1.06 ACC 0.59375 on epoch=49
06/24/2022 06:17:08 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.59375 on epoch=49, global_step=100
06/24/2022 06:17:09 - INFO - __main__ - Step 110 Global step 110 Train loss 0.65 on epoch=54
06/24/2022 06:17:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=59
06/24/2022 06:17:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=64
06/24/2022 06:17:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=69
06/24/2022 06:17:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=74
06/24/2022 06:17:14 - INFO - __main__ - Global step 150 Train loss 0.55 ACC 0.46875 on epoch=74
06/24/2022 06:17:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=79
06/24/2022 06:17:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=84
06/24/2022 06:17:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
06/24/2022 06:17:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.36 on epoch=94
06/24/2022 06:17:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=99
06/24/2022 06:17:21 - INFO - __main__ - Global step 200 Train loss 0.41 ACC 0.53125 on epoch=99
06/24/2022 06:17:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=104
06/24/2022 06:17:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=109
06/24/2022 06:17:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
06/24/2022 06:17:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.33 on epoch=119
06/24/2022 06:17:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=124
06/24/2022 06:17:28 - INFO - __main__ - Global step 250 Train loss 0.38 ACC 0.46875 on epoch=124
06/24/2022 06:17:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=129
06/24/2022 06:17:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=134
06/24/2022 06:17:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.34 on epoch=139
06/24/2022 06:17:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.30 on epoch=144
06/24/2022 06:17:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.34 on epoch=149
06/24/2022 06:17:35 - INFO - __main__ - Global step 300 Train loss 0.32 ACC 0.5 on epoch=149
06/24/2022 06:17:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=154
06/24/2022 06:17:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.29 on epoch=159
06/24/2022 06:17:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=164
06/24/2022 06:17:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
06/24/2022 06:17:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.29 on epoch=174
06/24/2022 06:17:42 - INFO - __main__ - Global step 350 Train loss 0.30 ACC 0.46875 on epoch=174
06/24/2022 06:17:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
06/24/2022 06:17:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=184
06/24/2022 06:17:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.27 on epoch=189
06/24/2022 06:17:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=194
06/24/2022 06:17:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.31 on epoch=199
06/24/2022 06:17:48 - INFO - __main__ - Global step 400 Train loss 0.30 ACC 0.3125 on epoch=199
06/24/2022 06:17:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
06/24/2022 06:17:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=209
06/24/2022 06:17:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=214
06/24/2022 06:17:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
06/24/2022 06:17:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/24/2022 06:17:55 - INFO - __main__ - Global step 450 Train loss 0.25 ACC 0.53125 on epoch=224
06/24/2022 06:17:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=229
06/24/2022 06:17:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=234
06/24/2022 06:17:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=239
06/24/2022 06:18:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=244
06/24/2022 06:18:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
06/24/2022 06:18:02 - INFO - __main__ - Global step 500 Train loss 0.26 ACC 0.5 on epoch=249
06/24/2022 06:18:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
06/24/2022 06:18:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
06/24/2022 06:18:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
06/24/2022 06:18:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=269
06/24/2022 06:18:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=274
06/24/2022 06:18:09 - INFO - __main__ - Global step 550 Train loss 0.21 ACC 0.375 on epoch=274
06/24/2022 06:18:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.18 on epoch=279
06/24/2022 06:18:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.17 on epoch=284
06/24/2022 06:18:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=289
06/24/2022 06:18:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=294
06/24/2022 06:18:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=299
06/24/2022 06:18:16 - INFO - __main__ - Global step 600 Train loss 0.16 ACC 0.46875 on epoch=299
06/24/2022 06:18:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=304
06/24/2022 06:18:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=309
06/24/2022 06:18:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
06/24/2022 06:18:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.13 on epoch=319
06/24/2022 06:18:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=324
06/24/2022 06:18:22 - INFO - __main__ - Global step 650 Train loss 0.15 ACC 0.5 on epoch=324
06/24/2022 06:18:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=329
06/24/2022 06:18:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=334
06/24/2022 06:18:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=339
06/24/2022 06:18:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=344
06/24/2022 06:18:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=349
06/24/2022 06:18:29 - INFO - __main__ - Global step 700 Train loss 0.13 ACC 0.46875 on epoch=349
06/24/2022 06:18:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=354
06/24/2022 06:18:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=359
06/24/2022 06:18:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=364
06/24/2022 06:18:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=369
06/24/2022 06:18:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=374
06/24/2022 06:18:36 - INFO - __main__ - Global step 750 Train loss 0.09 ACC 0.5 on epoch=374
06/24/2022 06:18:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=379
06/24/2022 06:18:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
06/24/2022 06:18:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
06/24/2022 06:18:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
06/24/2022 06:18:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
06/24/2022 06:18:42 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.5 on epoch=399
06/24/2022 06:18:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
06/24/2022 06:18:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=409
06/24/2022 06:18:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=414
06/24/2022 06:18:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
06/24/2022 06:18:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/24/2022 06:18:49 - INFO - __main__ - Global step 850 Train loss 0.06 ACC 0.53125 on epoch=424
06/24/2022 06:18:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=429
06/24/2022 06:18:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
06/24/2022 06:18:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=439
06/24/2022 06:18:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
06/24/2022 06:18:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=449
06/24/2022 06:18:56 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.46875 on epoch=449
06/24/2022 06:18:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
06/24/2022 06:18:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=459
06/24/2022 06:19:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=464
06/24/2022 06:19:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
06/24/2022 06:19:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
06/24/2022 06:19:03 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.53125 on epoch=474
06/24/2022 06:19:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
06/24/2022 06:19:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/24/2022 06:19:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
06/24/2022 06:19:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
06/24/2022 06:19:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
06/24/2022 06:19:09 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.5 on epoch=499
06/24/2022 06:19:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
06/24/2022 06:19:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
06/24/2022 06:19:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
06/24/2022 06:19:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=519
06/24/2022 06:19:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
06/24/2022 06:19:16 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.5 on epoch=524
06/24/2022 06:19:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/24/2022 06:19:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
06/24/2022 06:19:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
06/24/2022 06:19:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=544
06/24/2022 06:19:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=549
06/24/2022 06:19:23 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.5 on epoch=549
06/24/2022 06:19:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
06/24/2022 06:19:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=559
06/24/2022 06:19:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=564
06/24/2022 06:19:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
06/24/2022 06:19:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
06/24/2022 06:19:30 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.53125 on epoch=574
06/24/2022 06:19:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
06/24/2022 06:19:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
06/24/2022 06:19:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
06/24/2022 06:19:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
06/24/2022 06:19:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
06/24/2022 06:19:36 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.53125 on epoch=599
06/24/2022 06:19:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
06/24/2022 06:19:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
06/24/2022 06:19:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
06/24/2022 06:19:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/24/2022 06:19:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/24/2022 06:19:43 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.53125 on epoch=624
06/24/2022 06:19:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
06/24/2022 06:19:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/24/2022 06:19:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/24/2022 06:19:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/24/2022 06:19:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
06/24/2022 06:19:50 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.5625 on epoch=649
06/24/2022 06:19:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/24/2022 06:19:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
06/24/2022 06:19:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/24/2022 06:19:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
06/24/2022 06:19:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
06/24/2022 06:19:57 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.5625 on epoch=674
06/24/2022 06:19:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
06/24/2022 06:19:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/24/2022 06:20:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
06/24/2022 06:20:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
06/24/2022 06:20:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/24/2022 06:20:04 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.53125 on epoch=699
06/24/2022 06:20:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 06:20:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
06/24/2022 06:20:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/24/2022 06:20:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/24/2022 06:20:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/24/2022 06:20:10 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.53125 on epoch=724
06/24/2022 06:20:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
06/24/2022 06:20:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 06:20:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
06/24/2022 06:20:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/24/2022 06:20:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
06/24/2022 06:20:17 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.5 on epoch=749
06/24/2022 06:20:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/24/2022 06:20:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 06:20:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
06/24/2022 06:20:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/24/2022 06:20:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/24/2022 06:20:24 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.53125 on epoch=774
06/24/2022 06:20:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/24/2022 06:20:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/24/2022 06:20:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/24/2022 06:20:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
06/24/2022 06:20:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
06/24/2022 06:20:31 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.53125 on epoch=799
06/24/2022 06:20:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/24/2022 06:20:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/24/2022 06:20:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/24/2022 06:20:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
06/24/2022 06:20:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
06/24/2022 06:20:37 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.53125 on epoch=824
06/24/2022 06:20:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/24/2022 06:20:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/24/2022 06:20:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 06:20:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/24/2022 06:20:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
06/24/2022 06:20:44 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.53125 on epoch=849
06/24/2022 06:20:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/24/2022 06:20:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=859
06/24/2022 06:20:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/24/2022 06:20:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 06:20:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/24/2022 06:20:51 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.53125 on epoch=874
06/24/2022 06:20:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/24/2022 06:20:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
06/24/2022 06:20:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
06/24/2022 06:20:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/24/2022 06:20:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 06:20:58 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.5 on epoch=899
06/24/2022 06:20:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=904
06/24/2022 06:21:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=909
06/24/2022 06:21:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 06:21:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 06:21:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
06/24/2022 06:21:04 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.53125 on epoch=924
06/24/2022 06:21:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/24/2022 06:21:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=934
06/24/2022 06:21:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/24/2022 06:21:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/24/2022 06:21:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 06:21:11 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.53125 on epoch=949
06/24/2022 06:21:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
06/24/2022 06:21:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/24/2022 06:21:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
06/24/2022 06:21:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/24/2022 06:21:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
06/24/2022 06:21:18 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.5 on epoch=974
06/24/2022 06:21:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
06/24/2022 06:21:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/24/2022 06:21:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/24/2022 06:21:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 06:21:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=999
06/24/2022 06:21:25 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.46875 on epoch=999
06/24/2022 06:21:25 - INFO - __main__ - save last model!
06/24/2022 06:21:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 06:21:25 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 06:21:25 - INFO - __main__ - Printing 3 examples
06/24/2022 06:21:25 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 06:21:25 - INFO - __main__ - ['equivalent']
06/24/2022 06:21:25 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 06:21:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 06:21:25 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 06:21:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 06:21:25 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:21:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:21:25 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 06:21:33 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-glue-mrpc/glue-mrpc_16_87_0.2_8_predictions.txt
06/24/2022 06:21:33 - INFO - __main__ - ACC on test data: 0.5637
06/24/2022 06:21:33 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.2, bsz=8, dev_performance=0.59375, test_performance=0.5637254901960784
